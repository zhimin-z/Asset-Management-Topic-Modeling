[
    {
        "Answerer_created_time":1462822911288,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5460.0,
        "Answerer_view_count":588.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>While training a job on a SageMaker instance using H2o AutoML a message \"This H2OFrame is empty\" has come up after running the code, what should I do to fix the problem?<\/p>\n\n<pre><code>\/opt\/ml\/input\/config\/hyperparameters.json\nAll Parameters:\n{'nfolds': '5', 'training': \"{'classification': 'true', 'target': 'y'}\", 'max_runtime_secs': '3600'}\n\/opt\/ml\/input\/config\/resourceconfig.json\nAll Resources:\n{'current_host': 'algo-1', 'hosts': ['algo-1'], 'network_interface_name': 'eth0'}\nWaiting until DNS resolves: 1\n10.0.182.83\nStarting up H2O-3\nCreating Connection to H2O-3\nAttempt 0: H2O-3 not running yet...\nConnecting to H2O server at http:\/\/127.0.0.1:54321... successful.\n-------------------------- ----------------------------------------\n\n-------------------------- ----------------------------------------\nBeginning Model Training\nParse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100%\nClassification - If you want to do a regression instead, set \"classification\":\"false\" in \"training\" params, inhyperparamters.json\nConverting specified columns to categorical values:\n[]\nAutoML progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100%\nThis H2OFrame is empty.\nException during training: Argument `model` should be a ModelBase, got NoneType None\nTraceback (most recent call last):\nFile \"\/opt\/program\/train\", line 138, in _train_model\nh2o.save_model(aml.leader, path=model_path)\nFile \"\/root\/.local\/lib\/python3.7\/site-packages\/h2o\/h2o.py\", line 969, in save_model\nassert_is_type(model, ModelBase)\nFile \"\/root\/.local\/lib\/python3.7\/site-packages\/h2o\/utils\/typechecks.py\", line 457, in assert_is_type\nskip_frames=skip_frames)\nh2o.exceptions.H2OTypeError: Argument `model` should be a ModelBase, got NoneType None\nH2O session _sid_8aba closed.\n<\/code><\/pre>\n\n<p>I'm wondering if it's a problem because of the max_runtime_secs, my data has around 500 rows and 250000 columns.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1568737272183,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1568738644663,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57978333",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":12.3,
        "Challenge_reading_time":26.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"What should I do when H2O AutoML returns \"H2OFrame is empty\"?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":517.0,
        "Challenge_word_count":197,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509012479112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>thanks @Marcel Mendes Reis for following up on your solution in the comments. I will repost here for others to easily find:<\/p>\n\n<p><em>I realized the issue was due to the max_runtime. When I trained the model with more time I didn't have the problem.<\/em> <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":3.22,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been following along with this really helpful XGBoost tutorial on Medium (code used towards bottom of article): <a href=\"https:\/\/medium.com\/analytics-vidhya\/random-forest-and-xgboost-on-amazon-sagemaker-and-aws-lambda-29abd9467795\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/analytics-vidhya\/random-forest-and-xgboost-on-amazon-sagemaker-and-aws-lambda-29abd9467795<\/a>.<\/p>\n<p>To-date, I've been able to get data appropriately formatted for ML purposes, a model created based on training data, and then test data fed through the model to give useful results.<\/p>\n<p>Whenever I leave and come back to work more on the model or feed in new test data however, I find I need to re-run all model creation steps in order to make any further predictions. Instead I would like to just call my already created model endpoint based on the Image_URI and feed in new data.<\/p>\n<p>Current steps performed:<\/p>\n<p>Model Training<\/p>\n<pre><code>xgb = sagemaker.estimator.Estimator(containers[my_region],\n                                    role, \n                                    train_instance_count=1, \n                                    train_instance_type='ml.m4.xlarge',\n                                    output_path='s3:\/\/{}\/{}\/output'.format(bucket_name, prefix),\n                                    sagemaker_session=sess)\nxgb.set_hyperparameters(eta=0.06,\n                        alpha=0.8,\n                        lambda_bias=0.8,\n                        gamma=50,\n                        min_child_weight=6,\n                        subsample=0.5,\n                        silent=0,\n                        early_stopping_rounds=5,\n                        objective='reg:linear',\n                        num_round=1000)\n\nxgb.fit({'train': s3_input_train})\n\nxgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>Evaluation<\/p>\n<pre><code>test_data_array = test_data.drop([ 'price','id','sqft_above','date'], axis=1).values #load the data into an array\n\nxgb_predictor.serializer = csv_serializer # set the serializer type\n\npredictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\npredictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\nprint(predictions_array.shape)\n\nfrom sklearn.metrics import r2_score\nprint(&quot;R2 score : %.2f&quot; % r2_score(test_data['price'],predictions_array))\n<\/code><\/pre>\n<p>It seems that this particular line:<\/p>\n<pre><code>predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n<\/code><\/pre>\n<p>needs to be re-written in order to not reference xgb.predictor but instead reference the model location.<\/p>\n<p>I have tried the following<\/p>\n<pre><code>trained_model = sagemaker.model.Model(\n    model_data='s3:\/\/{}\/{}\/output\/xgboost-2020-11-10-00-00\/output\/model.tar.gz'.format(bucket_name, prefix),\n    image_uri='XXXXXXXXXX.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest',\n    role=role)  # your role here; could be different name\n\ntrained_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>and then replaced<\/p>\n<pre><code>xgb_predictor.serializer = csv_serializer # set the serializer type\npredictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>trained_model.serializer = csv_serializer # set the serializer type\npredictions = trained_model.predict(test_data_array).decode('utf-8') # predict!\n<\/code><\/pre>\n<p>but I get the following error:<\/p>\n<pre><code>AttributeError: 'Model' object has no attribute 'predict'\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605059871707,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64779388",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.5,
        "Challenge_reading_time":43.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":null,
        "Challenge_title":"How to invoke Sagemaker XGBoost endpoint post model creation?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1242.0,
        "Challenge_word_count":272,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1532706046196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":191.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>that's a good question :) I agree, many of the official tutorials tend to show the full train-to-invoke pipeline and don't emphasize enough that each step can be done separately. In your specific case, when you want to invoke an already-deployed endpoint, you can either: (A) use the invoke API call in one of the numerous SDKs (example in <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker-runtime\/invoke-endpoint.html\" rel=\"nofollow noreferrer\">CLI<\/a>, <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint\" rel=\"nofollow noreferrer\">boto3<\/a>) or (B) or instantiate a <code>predictor<\/code> with the high-level Python SDK, either the generic <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"nofollow noreferrer\"><code>sagemaker.model.Model<\/code><\/a> class or its XGBoost-specific child: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/xgboost\/xgboost.html#sagemaker.xgboost.model.XGBoostPredictor\" rel=\"nofollow noreferrer\"><code>sagemaker.xgboost.model.XGBoostPredictor<\/code><\/a> as illustrated below:<\/p>\n<pre><code>from sagemaker.xgboost.model import XGBoostPredictor\n    \npredictor = XGBoostPredictor(endpoint_name='your-endpoint')\npredictor.predict('&lt;payload&gt;')\n<\/code><\/pre>\n<p>similar question <a href=\"https:\/\/stackoverflow.com\/questions\/56255154\/how-to-use-a-pretrained-model-from-s3-to-predict-some-data\/56277411#56277411\">How to use a pretrained model from s3 to predict some data?<\/a><\/p>\n<p>Note:<\/p>\n<ul>\n<li>If you want the <code>model.deploy()<\/code> call to return a predictor, your model must be instantiated with a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"nofollow noreferrer\"><code>predictor_cls<\/code><\/a>. This is optional, you can also first deploy a model, and then invoke it as a separate step with the above technique<\/li>\n<li>Endpoints create charges even if you don't invoke them; they are charged per uptime. So if you don't need an always-on endpoint, don't hesitate to shut it down to minimize costs.<\/li>\n<\/ul>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":17.5,
        "Solution_reading_time":28.73,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":196.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi, here is the details of my issue.  <br \/>\nI want to execute a distributed training run with the Tensorflow framework and Horovod.  <br \/>\nTo do this, I've configured a environment called &quot;tf_env&quot; as follow :<\/p>\n<pre><code># Create the environment : the dependencies are in the .yml file\ntf_env = Environment.from_conda_specification(name=&quot;tensorflow_environment&quot;, file_path=&quot;experiments\/package-list.yml&quot;)\n\n# Register the environment\ntf_env.register(workspace=ws)\n\n# Specify a GPU base image\ntf_env.docker.enabled = True\ntf_env.docker.base_image = 'mcr.microsoft.com\/azureml\/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04'\n<\/code><\/pre>\n<p>Where my &quot;package-list.yml&quot; contains all the dependencies my &quot;train_script.py&quot; requires.  <br \/>\nI've defined my ScriptConfigRun as follow :<\/p>\n<pre><code>arguments = [\n    (... other arguments ...)\n    &quot;--ds&quot;,  images_ds.as_mount()\n]\n\nsrc = ScriptRunConfig(\n    source_directory=&quot;experiments&quot;,\n    script='train_script.py',\n    arguments=arguments,\n    compute_target=compute_target,\n    environment=tf_env,\n    distributed_job_config=MpiConfiguration(node_count=2)\n)\n<\/code><\/pre>\n<p>Then, when I want to submit the run :<\/p>\n<pre><code>run = best_model_experiment.submit(config=src)\n<\/code><\/pre>\n<p>... it raises this error I don't understand :<\/p>\n<pre><code>ExperimentExecutionException: ExperimentExecutionException:\n    Message: {\n    &quot;error_details&quot;: {\n        &quot;componentName&quot;: &quot;execution&quot;,\n        &quot;correlation&quot;: {\n            &quot;operation&quot;: &quot;***&quot;,\n            &quot;request&quot;: &quot;***&quot;\n        },\n        &quot;environment&quot;: &quot;westeurope&quot;,\n        &quot;error&quot;: {\n            &quot;code&quot;: &quot;UserError&quot;,\n            &quot;message&quot;: &quot;Error when parsing request; unable to deserialize request body&quot;\n        },\n        &quot;location&quot;: &quot;westeurope&quot;,\n        &quot;time&quot;: &quot;***&quot;\n    },\n    &quot;status_code&quot;: 400,\n    &quot;url&quot;: &quot;https:\/\/westeurope.experiments.azureml.net\/execution\/v1.0\/subscriptions\/***\/resourceGroups\/***\/providers\/Microsoft.MachineLearningServices\/workspaces\/***\/experiments\/experiment\/snapshotrun?runId=experiment***&quot;\n}\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;{\\n    \\&quot;error_details\\&quot;: {\\n        \\&quot;componentName\\&quot;: \\&quot;execution\\&quot;,\\n        \\&quot;correlation\\&quot;: {\\n            \\&quot;operation\\&quot;: \\&quot;***\\&quot;,\\n            \\&quot;request\\&quot;: \\&quot;***\\&quot;\\n        },\\n        \\&quot;environment\\&quot;: \\&quot;westeurope\\&quot;,\\n        \\&quot;error\\&quot;: {\\n            \\&quot;code\\&quot;: \\&quot;UserError\\&quot;,\\n            \\&quot;message\\&quot;: \\&quot;Error when parsing request; unable to deserialize request body\\&quot;\\n        },\\n        \\&quot;location\\&quot;: \\&quot;westeurope\\&quot;,\\n        \\&quot;time\\&quot;: \\&quot;***\\&quot;\\n    },\\n    \\&quot;status_code\\&quot;: 400,\\n    \\&quot;url\\&quot;: \\&quot;https:\/\/westeurope.experiments.azureml.net\/execution\/v1.0\/subscriptions\/***\/resourceGroups\/***\/providers\/Microsoft.MachineLearningServices\/workspaces\/***\/experiments\/experiment\/snapshotrun?runId=experiment_***\\&quot;\\n}&quot;\n    }\n}\n<\/code><\/pre>\n<p>Could you please help me decrypt this error ?  <br \/>\nThank you.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619892981027,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/379458\/azure-machine-learning-experimentexecutionexceptio",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":24.5,
        "Challenge_reading_time":44.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning ExperimentExecutionException while submitting a distributed training run !",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":216,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Issue solved ! I've given a list in arguments to argparse so it could'nt deserialized the object.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":1.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1324808381143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":9050.0,
        "Answerer_view_count":1750.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using SageMaker v2.29.2 and Tensorflow v2.3.2 I'm trying to implement distributed training as explained in the following blogpost:<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23<\/a><\/p>\n<p>However I'm having difficulties importing the smdistributed script.<\/p>\n<p>Here is my code:<\/p>\n<pre><code>import tensorflow as tf\nimport smdistributed.modelparallel.tensorflow as smp\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;temp.py&quot;, line 2, in &lt;module&gt;\n    import smdistributed.modelparallel.tensorflow as smp\nModuleNotFoundError: No module named 'smdistributed'\n<\/code><\/pre>\n<p>What am I missing?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1615901050403,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1615921589292,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66656120",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":21.2,
        "Challenge_reading_time":13.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker TF 2.3 distributed training",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":383.0,
        "Challenge_word_count":73,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324808381143,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":9050.0,
        "Poster_view_count":1750.0,
        "Solution_body":"<p>smdistributed is only available on the SageMaker containers. It is supported for specific TensorFlow versions and you must add:<\/p>\n<pre><code>distribution={'smdistributed': {\n            'dataparallel': {\n                'enabled': True\n            }\n        }}\n<\/code><\/pre>\n<p>On the estimator code in order to enable it<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.4,
        "Solution_reading_time":3.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1376999872723,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Malaysia",
        "Answerer_reputation_count":998.0,
        "Answerer_view_count":136.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I create an forecasting experiment using R engine. My data source is pivoted, hence I need to pass row by row.\nThe output works great with single row prediction. But when I try to populate multiple lines, it still gives single row output - for the first record only.<\/p>\n\n<p>I'm trying to loop my result as follows :<\/p>\n\n<pre><code># Map 1-based optional input ports to variables\ndataset1 &lt;- maml.mapInputPort(1) # class: data.frame\n\nlibrary(forecast)\nlibrary(reshape)\nlibrary(dplyr)\nlibrary(zoo)\n#exclude non required columns\nmy.ds &lt;- dataset1[, -c(4,5,6)]\n# set the CIs we want to use here, so we can reuse this vector\ncis &lt;- c(80, 95)\n\nfor (i in 1:nrow(my.ds)) {\nmy.start &lt;- my.ds[i,c(3)]\nmy.product &lt;- my.ds[i, \"Product\"]\nmy.location &lt;- my.ds[i, \"Location\"]\nmy.result &lt;- melt(my.ds[i,], id = c(\"Product\",\"Location\"))\nmy.ts &lt;- ts(my.result$value, frequency=52, start=c(my.start,1))\n# generate the forecast using those ci levels\nf &lt;- forecast(na.interp(my.ts), h=52, level=cis)\n# make a data frame containing the forecast information, including the index\nz &lt;- as.data.frame(cbind(seq(1:52),\n                       f$mean,\n                       Reduce(cbind, lapply(seq_along(cis), function(i) cbind(f$lower[,i], f$upper[,i])))))\n# give the columns better names\nnames(z) &lt;- c(\"index\", \"mean\", paste(rep(c(\"lower\", \"upper\"), times = length(cis)), rep(cis, each = 2), sep = \".\"))\n# manipulate the results as you describe\nzw &lt;- z %&gt;%\n# keep only the variable you want and its index\nmutate(sssf = upper.95 - mean) %&gt;%\nselect(index, mean, sssf) %&gt;%\n# add product and location info\nmutate(product = my.product,\n       location = my.location) %&gt;%\n# rearrange columns so it's easier to read\nselect(product, location, index, mean, sssf)\nzw &lt;- melt(zw, id.vars = c(\"product\", \"location\", \"index\"), measure.vars = c(\"mean\",\"sssf\"))\ndata.set &lt;- cast(zw, product + location ~ index + variable, value = \"value\")\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"data.set\");\n}\n<\/code><\/pre>\n\n<p>This is design of my experiment :\n<a href=\"https:\/\/i.stack.imgur.com\/6lYd1.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6lYd1.png\" alt=\"experiment\"><\/a><\/p>\n\n<p>And this is how sample <a href=\"https:\/\/www.dropbox.com\/s\/xgfc7pnyy29frid\/dhf-00009E850%20-%20Copy.csv?dl=0\" rel=\"nofollow noreferrer\" title=\"input file\">input<\/a> looks like :<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/QlRiE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QlRiE.png\" alt=\"Sample input\"><\/a><\/p>\n\n<p>I'm testing using the Excel test workbook downloaded from experiment site.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1480051198437,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1480086026020,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40798184",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":33.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Batch Run - Single Output",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":195.0,
        "Challenge_word_count":302,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1376999872723,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Malaysia",
        "Poster_reputation_count":998.0,
        "Poster_view_count":136.0,
        "Solution_body":"<p>I figured out the problem :<\/p>\n\n<pre><code>{\n...\nds &lt;- cast(zw, product + location ~ index + variable, value = \"value\")\ndata.set &lt;- rbind(data.set, ds)\n}\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"data.set\");\n<\/code><\/pre>\n\n<p>I should be merging the rows and then output outside of the loop.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.9,
        "Solution_reading_time":4.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nGetting errors with the new Sagemaker Async Operators that I don't get with the traditional ones. I'm using a personal Access Key, Secret, and Session Token as I did with the non async operators for auth.\r\n\r\n```\r\nbotocore.exceptions.ClientError: An error occurred (UnrecognizedClientException) when calling the DescribeTrainingJob operation: The security token included in the request is invalid.\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\nUse the SageMaker async operators with user Access Key, Secret, and Session Token\r\n\r\n**Expected behavior**\r\nExpect it to not have auth\/token errors.\r\n\r\n\r\n**Additional context**\r\nWhen I switch back to the traditional operators in the same dag with the same auth creds it works fine.\r\n\r\n\r\n@kentdanas also had similar issues and her auth was setup a little different.",
        "Challenge_closed_time":1666859.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666713848000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/725",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":10.75,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":22.0,
        "Challenge_repo_issue_count":807.0,
        "Challenge_repo_star_count":97.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Token error with Sagemaker Async Operators",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":127,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The issue is with the session token is not considered while the secrete and access key is given in the connection proper field, not in the extra config",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":1.82,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1476806455803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Holzkirchen, Deutschland",
        "Answerer_reputation_count":3068.0,
        "Answerer_view_count":386.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>An Azure Data Factory pipeline for updating a trained ML model returns this error:<\/p>\n\n<pre><code>HTTP 404. The resource you are looking for (or one of its dependencies) could have been removed, had its name changed, or is temporarily unavailable. Please review the following URL and make sure that it is spelled correctly.\nRequested URL: \/workspaces\/xxxx\/webservices\/xxxx\/endpoints\/update\n\nDiagnostic details: job ID xxxx. Endpoint https:\/\/services.azureml.net\/workspaces\/xxxx\/webservices\/xxxx\/endpoints\/update.\n<\/code><\/pre>\n\n<p>I don't even want to think about why it returned a HTML document...\nI am 100% sure that the endpoint exists and the key provided is correct.\nSo what is my mistake?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1503048195263,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1503233060163,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45753090",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":9.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML endpoint 404 error",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":494.0,
        "Challenge_word_count":99,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1476806455803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Holzkirchen, Deutschland",
        "Poster_reputation_count":3068.0,
        "Poster_view_count":386.0,
        "Solution_body":"<p>Deleting and creating the endpoint again fixed it.<\/p>\n\n<p>Microsoft...<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>This may be a very basic question (I imagine it is)<\/p>\n<pre><code>estimator = PyTorch(entry_point='train.py',\n                   source_dir = 'code',\n                    role = role,\n                   framework_version = '1.5.0',\n                   py_version = 'py3',\n                   instance_count = 2,\n                   instance_type = 'ml.g4dn.2xlarge',\n                   hyperparameters={&quot;epochs&quot;: 2,\n                                     &quot;num_labels&quot;: 46,\n                                     &quot;backend&quot;: &quot;gloo&quot;,    \n                                    },\n                   profiler_config=profiler_config,\n                    debugger_hook_config=debugger_hook_config,\n                    rules=rules\n                   )\n<\/code><\/pre>\n<p>I declare my estimator as above, and put this into training using fit().<br \/>\nI have done several of these on my sagemaker, and there are several training jobs in the aws training job log.<br \/>\nBut they all appear in the form 'pytorch-training-2021 ....'. <br \/>\nIs there anyway I could declare the name of the training job like 'custom-model-xgboost-ver1' ?<br \/>\nI thought it would be possible as one of the parameter of estimator, but i couldn't find it.<\/p>\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626249953723,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68374280",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.3,
        "Challenge_reading_time":12.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Assigning name to AWS SageMaker Training job",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":445.0,
        "Challenge_word_count":120,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600718448276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seoul, South Korea",
        "Poster_reputation_count":69.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>When you call <code>fit()<\/code> you can pass this parameter <code>job_name=yourJobName<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.3,
        "Solution_reading_time":1.35,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1467237684900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":613.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying out the sample notebooks in AWS Sagemaker, currently in the mxnet mnist example which demonstrates bringing your own code. The entry point parameter passed in when instantiating an estimator instance, only mentions the source file (mnist.py) and not a method name or any other point inside the source file.<\/p>\n\n<p>So how does aws sagemaker figure out which method to send the training data to? <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1519726485703,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1519802441487,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49006174",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":6.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How is the entry point to the code specified in AWS sagemaker bring your own code?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":3327.0,
        "Challenge_word_count":82,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1450260166772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1587.0,
        "Poster_view_count":540.0,
        "Solution_body":"<p>Your python script should implement a few methods like train, model_fn, transform_fn, input_fn etc. SagaMaker would call appropriate method when needed. <\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet-training-inference-code-template.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet-training-inference-code-template.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":25.9,
        "Solution_reading_time":5.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":73.4173611111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi! \n\nI'm using the [Feature Store Spark connector](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-ingestion-spark-connector-setup.html) to ingest data into the Sagemaker Feature Store and when we try to ingest data to a Feature Group with the online store enabled, the data is duplicated.\nIn the image bellow, \"customer_id\" is the ID feature, \"date_ref\" the event column. All the features are equal for the same ID and EventTime column, except the \"api_invocation_time\".\n\n ![Duplicated data](\/media\/postImages\/original\/IMhOq2A8JdTzm0WS21rLjyLQ)\n\nIf the feature group doesn't have the online store enabled, we ingest the data directly to the offline store without issues. But when we use the \"Ingest by default\" option in the connector (not specifying the \"target_stores\" in the connector, uses the PutRecord API), the data ingested is duplicated:\n\n```\nparams = {\n    \"input_data_frame\":dataframe,\n    \"feature_group_arn\": feature_group_arn            \n}\n\nif not online_store_enabled:\n    params[\"target_stores\"] = [\"OfflineStore\"]\n    logger.info(f\"Ingesting data to the offline store\")\n\npyspark_connector.ingest_data(**params)\nlogger.info(\"Finished the ingestion!\")\n\nfailed_records = pyspark_connector.get_failed_stream_ingestion_data_frame()\n```\n\nHow can I solve this issue using the connector?\n\n**EDIT:**\n\nApparently, the problem is in the \"get_failed_stream ingestion data frame\" method. This method, instead of just returning a dataframe, ingests the data again before returning the failed records. Removing the method from the ingestion pipeline resolves the issue, although we lose a form of validation.",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677163363698,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1677519025372,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUDdzO0n0cRKm5fLSuPdE3Qg\/sagemaker-feature-store-spark-connector-is-duplicating-data",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":21.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker Feature Store Spark connector is duplicating data",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":89.0,
        "Challenge_word_count":198,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This issue should be patched in 1.1.1. If you upgrade from 1.1.0, get_failed_stream_ingestion_data_frame should no longer trigger any re-computation now.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1677783327872,
        "Solution_link_count":0.0,
        "Solution_readability":4.7,
        "Solution_reading_time":1.97,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>When I'm submitting my experiment fom notebook, experiment is queing for a long time then I get as error:  <\/p>\n<p>AzureMLCompute job failed.  <br \/>\nClusterIdentityNotFound: Identity of the specified   <br \/>\nmanaged compute &lt;hidden cluster location&gt; is not found  <\/p>\n<p>I've updated all azure ml packages and restarted cluster, deleted, recreating, ... Nothing seems to be working.  <\/p>\n<p>What Should I do?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633943984543,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/585373\/clusteridentitynotfound-when-submitting-experiment",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.6,
        "Challenge_reading_time":5.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"ClusterIdentityNotFound when submitting experiment.",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":63,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, are you by any chance using a low priority VM? If so, can you try selecting 'dedicated' as priority to verify? Also, ensure that you are following the steps outlined in this <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-attach-compute-studio#amlcompute\">document<\/a> for creating a compute cluster. In the advanced settings, ensure to assign a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-attach-compute-studio#managed-identity\">managed identity<\/a> and specify a system-assigned identity or user-assigned identity.<\/p>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information provided helps. Thanks.*<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.5,
        "Solution_reading_time":9.33,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":70.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I understand the azure machine learning studio (classic) version using Anaconda distribution but my question is where would the python modules like pandas\/tensorflow are installed when using <strong>IPython interface of Azure ML<\/strong>. Is this on AML studio itself or in azure blob (AML studio uses blob as backend store)? <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578440246977,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1578440835316,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59637596",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":5.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Where does Python modules installed on Azure Machine Learning Studio",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":136.0,
        "Challenge_word_count":59,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1500744375327,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":255.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>Here is my screenshots for tabs <code>EXPERIMENTS<\/code> and <code>NOTEBOOKS<\/code> in Azure Machine Learning Studio (classic), as the figures below.<\/p>\n\n<p>Fig 1. I created a <code>Excute Python Script<\/code> module with the code to print the <code>sys.path<\/code> and the real path of <code>pandas<\/code> installed.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/KdkJa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KdkJa.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 2. The <code>View output log<\/code> page of the code in Fig 1 shows <code>EXPERIMENTS<\/code> is a runtime of Anaconda on Windows. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/zo9te.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zo9te.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 3. I created a notebook named <code>demo<\/code> and run the same code as Fig 1, the result shows <code>NOTEBOOKS<\/code> is a runtime of Anaconda on Linux, even the notenooks url is started with <code>notebooks.azure.com<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/uAGRk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uAGRk.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 4. I used different commands like <code>lsb_release -a<\/code>, <code>fdisk -l<\/code>, <code>lsdev<\/code>, <code>ls \/dev<\/code>, <code>df -a<\/code> to try to see the Linux version and its disk or partition information, the result shows it's a Ubuntu Linux container.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/R4wC6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/R4wC6.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Other infomation what you want to know, you can try to check by yourself.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":10.7,
        "Solution_reading_time":22.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":187.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1533754693910,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":801.0,
        "Answerer_view_count":80.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<ol>\n<li><p>task : object_detection<\/p>\n<\/li>\n<li><p>environment: AWS sagemaker<\/p>\n<\/li>\n<li><p>instance type: 'ml.p2.xlarge' | num_instances = 1<\/p>\n<\/li>\n<li><p>Main file to be run: <a href=\"https:\/\/github.com\/tensorflow\/models\/blob\/master\/research\/object_detection\/model_main_tf2.py\" rel=\"nofollow noreferrer\">original<\/a><\/p>\n<\/li>\n<li><p>Problematic code segment from the main file:<\/p>\n<pre><code>    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n    FLAGS.tpu_name)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n    elif FLAGS.num_workers &gt; 1:\n        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n    else:\n        strategy = tf.compat.v2.distribute.MirroredStrategy()\n<\/code><\/pre>\n<\/li>\n<li><p>Problem : Can't find the proper value to be given as <code>tpu_name<\/code> argument.<\/p>\n<\/li>\n<li><p>My research on the problem:<\/p>\n<\/li>\n<\/ol>\n<p>According to the tensorflow documentation in <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/cluster_resolver\/TPUClusterResolver\" rel=\"nofollow noreferrer\">tf.distribute.cluster_resolver.TPUClusterResolver<\/a>, it says that this resolver works only on Google Cloud platform.<\/p>\n<blockquote>\n<p>This is an implementation of cluster resolvers for the Google Cloud\nTPU service.<\/p>\n<p>TPUClusterResolver supports the following distinct environments:\nGoogle Compute Engine Google Kubernetes Engine Google internal<\/p>\n<p>It can be passed into tf.distribute.TPUStrategy to support TF2\ntraining on Cloud TPUs.<\/p>\n<\/blockquote>\n<p>But from <a href=\"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/39721\" rel=\"nofollow noreferrer\">this issue in github<\/a>, I found out that a similar code also works in Azure.<\/p>\n<ol start=\"8\">\n<li>My question :<\/li>\n<\/ol>\n<p>Is there a way I can bypass this resolver and initialize my tpu in <strong>sagemaker<\/strong> ?<\/p>\n<p>Even better, if I can find a way to insert the name or url of sagemaker gpu to the resolver and initiate it from there ?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609059338347,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1609063955520,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65464181",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":15.6,
        "Challenge_reading_time":28.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"An alternative to tf.distribute.cluster_resolver.TPUClusterResolver( tpu_name) to be used in Sagemaker?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":355.0,
        "Challenge_word_count":210,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1517147266416,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>Let me clarify some confusion here. TPUs are only offered on Google Cloud and the <code>TPUClusterResolver<\/code> implementation queries GCP APIs to get the cluster config for the TPU node. Thus, no you can't use <code>TPUClusterResolver<\/code> with AWS sagemaker, but you should try it out with TPUs on GCP instead or try find some other documentation on Sagemaker's end on how they enable cluster resolving on their end (if they do).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.2,
        "Solution_reading_time":5.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hello all!<\/p>\n<p>I am running a lot of runs per day (~2K sometimes) and have been encountering some strange errors in a handful of my runs. I am doing this on a large computation cluster, so to avoid putting too much strain on the network for every run I<\/p>\n<ol>\n<li>set wandb to run offline (<code>export WANDB_MODE=\"offline\"<\/code>)<\/li>\n<li>set the <code>WANDB_DIR<\/code> to be a tmp directory (<code>WANDB_DIR=$(mktemp -d)<\/code>)<\/li>\n<li>Run my run as normal (runs are relatively short often taking ~2-20 minutes)<\/li>\n<li>Sync my wandb runs (<code>wandb sync $WANDB_DIR\/wandb\/offline*<\/code>)<\/li>\n<li>Clean up my tmpdir (<code>rm -rf $WANDB_DIR <\/code>)<\/li>\n<\/ol>\n<p>The full script is below:<\/p>\n<pre><code class=\"lang-auto\">my_config= # some config unique to this run\nexport WANDB_MODE=\"offline\"\nexport WANDB_DIR=$(mktemp -d)\npython train.py --config $my_config \nwandb sync $WANDB_DIR\/wandb\/offline*\nrm -rf $WANDB_DIR \n\n<\/code><\/pre>\n<p>In 99% of runs this works totally fine, however in a handful I get messages like:<\/p>\n<pre><code class=\"lang-auto\">Syncing: https:\/\/wandb.ai\/some_run ... wandb: WARNING .wandb file is incomplete (invalid padding), be sure to sync this run again once it's finished\ndone.\n<\/code><\/pre>\n<p>If I actually <em>look<\/em> at <code>some_run<\/code>, it seems totally normal and I don\u2019t see any missing data. Furthermore the <code>wandb sync<\/code> command returns 0 exit code so I would assume all is well despite the error message. But the existence of the error is concerning and I am not sure the best way to deal with it or if it needs to be delt with at all. I am grateful for any advice people have!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1680179826946,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/warning-wandb-file-is-incomplete-invalid-padding\/4153",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.7,
        "Challenge_reading_time":21.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"WARNING .wandb file is incomplete (invalid padding)",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":245,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/evanv\">@evanv<\/a> , apologies for the delay.<\/p>\n<p>The <code>invalid padding<\/code> error occurs when  wandb tries to read data from your run file, but the file may not be  in an expected format. This could happen for a variety of reasons including file corruption, or issues when the file is read. When wandb scans your file and finds a discrepency with the format, it raises a warning informing you to  <code>sync this run again once it's finished<\/code> as precautionary measure. If it successfully synced the first time around, then great, if not try again.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":7.39,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":94.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1501163272143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I wonder if it's possible to export my model to a json file, so I can do some kind of versioning.<\/p>\n\n<p>Building up a model with Azure Machine Learning Studio is easy, but I need to save the previous version anytime I do an update.<\/p>\n\n<p>It's possible to do this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544660891693,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53753367",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":3.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to export my azure machine learning model to json",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":445.0,
        "Challenge_word_count":59,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458558039430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Trieste, Province of Trieste, Italy",
        "Poster_reputation_count":1657.0,
        "Poster_view_count":164.0,
        "Solution_body":"<p>In Azure ML Studio, the versioning is available as Run History: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/manage-experiment-iterations\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/manage-experiment-iterations<\/a>\nRegards,\nJaya<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":30.3,
        "Solution_reading_time":4.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I need to run XGBoost inferences on 15MM samples (3.9Gb when stored as csv). Since Batch transform does not seem to work on such large batches (max payload 100MB) I split my input file into 646 files, each around 6Mb, stored in S3. I am running the code below:\n\n    transformer = XGB.transformer(\n        instance_count=2, instance_type='ml.c5.9xlarge',\n        output_path='s3:\/\/xxxxxxxxxxxxx\/sagemaker\/recsys\/xgbtransform\/',\n        max_payload=100)\n\n    transformer.transform(\n        data='s3:\/\/xxxxxxxxxxxxx\/sagemaker\/recsys\/testchunks\/',\n        split_type='Line')\n\nBut the job fails - Sagemaker tells \"ClientError: Too many objects failed. See logs for more information\" and cloudwatch logs show:\n\n    Bad HTTP status returned from invoke: 415\n    'NoneType' object has no attribute 'lower'\n\nDid I forget something in my batch transform settings?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1532625720000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668556023886,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUr4Vq95ScROqSguzxNQYDOg\/sagemaker-batch-transform-415-error",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":10.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker batch transform 415 error",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":568.0,
        "Challenge_word_count":105,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This indicates that the algorithm thinks it has been passed bad data. Perhaps a problem with your splitting?\n\nI would suggest two things: \n\n1. Try running the algorithm on the original data using the `\"SplitType\": \"Line\"` and `\"BatchStrategy\": \"MultiRecord\"` arguments and see if you have better luck.\n2. Look in the cloudwatch logs for your run and see if  there's any helpful information about what the algorithm didn't like. You can find these in the log group \"\/aws\/sagemaker\/TransformJobs\" in the log stream that begins with your job name.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925589052,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":6.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":7,
        "Challenge_body":"### What steps did you take:\r\n[A clear and concise description of what the bug is.]\r\n\r\nI am use the re usable Sagemaker Components for building kubeflow pipelines.\r\n\r\nsagemaker_train_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/train\/component.yaml')\r\nsagemaker_model_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/model\/component.yaml')\r\nsagemaker_deploy_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/deploy\/component.yaml')\r\n\r\nWhen i am trying to update the endpoint that already exists \r\n\r\npiece of code i used to update the endpoint.\r\n\r\n**#deploy the pipeline\r\nprediction = sagemaker_deploy_op(\r\n        region=aws_region,\r\n        endpoint_name='Endpoint-price-prediction-model',\r\n        endpoint_config_name='EndpointConfig-price-prediction-model',\r\n        update_endpoint=True,\r\n        model_name_1 = create_model.output,\r\n        instance_type_1='ml.m5.large'\r\n    )\r\n# compiling the pipeline\r\nkfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')**\r\n\r\n\r\n### What happened:\r\nI am getting this error \r\nTypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\nI think while compile the pipeline kfp is throwing this error.can you suggest me or help me out in this\r\n\r\n\r\nTraceback (most recent call last):\r\n--\r\n414 | File \"pipeline.py\", line 94, in <module>\r\n415 | kfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')\r\n416 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 920, in compile\r\n417 | self._create_and_write_workflow(\r\n418 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 972, in _create_and_write_workflow\r\n419 | workflow = self._create_workflow(\r\n420 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 813, in _create_workflow\r\n421 | pipeline_func(*args_list)\r\n422 | File \"pipeline.py\", line 85, in car_price_prediction\r\n423 | prediction = sagemaker_deploy_op(\r\n424 | TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\n\r\n\r\n### What did you expect to happen:\r\nto update the endpoint without any issue\r\n### Environment:\r\n<!-- Please fill in those that seem relevant. -->\r\nusing kfp 1.1.2\r\nsagemaker 2.1.0\r\n\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\n<!-- If you are not sure, here's [an introduction of all options](https:\/\/www.kubeflow.org\/docs\/pipelines\/installation\/overview\/). -->\r\n\r\nKFP version: <!-- If you are not sure, build commit shows on bottom of KFP UI left sidenav. -->\r\n\r\nKFP SDK version: <!-- Please attach the output of this shell command: $pip list | grep kfp -->\r\nkfp-1.1.2.tar.gz \r\n\r\n### Anything else you would like to add:\r\n[Miscellaneous information that will assist in solving the issue.]\r\n\r\nPlease help me out \r\n\r\n\/kind bug\r\n<!-- Please include labels by uncommenting them to help us better triage issues, choose from the following -->\r\n<!--\r\n\/\/ \/area frontend\r\n\/\/ \/area backend\r\n\/\/ \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Challenge_closed_time":1611093.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607683045000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/4888",
        "Challenge_link_count":4,
        "Challenge_participation_count":7,
        "Challenge_readability":14.3,
        "Challenge_reading_time":43.94,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":null,
        "Challenge_title":"TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":304,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"\/assign @mameshini \r\n\/assign @PatrickXYS \r\n\r\nDo you mind taking a look? Thanks @numerology Thanks!\r\n\r\n@akartsky @RedbackThomson Can you take a look?  Hi @jchaudari, \r\nThanks for reporting the issue, we are taking a look at it. \r\n\r\nThanks,\r\nMeghna Hi @jchaudari, \r\nAre you certain you are using the latest version of the components ? The attached yaml files show that you are using version 0.3.0 of the image which is very old. This feature was added more recently in version 0.9.0 - \r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/Changelog.md. \r\n\r\nCould you please try with the newer version and let us know if that fixes your issue ?\r\nThanks,\r\nMeghna Baijal If there aren't any further issues, we'll close this by the end of the week. Otherwise, let us know. \/close @akartsky: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/4888#issuecomment-763167821):\n\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":9.7,
        "Solution_reading_time":16.34,
        "Solution_score_count":null,
        "Solution_sentence_count":15.0,
        "Solution_word_count":158.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Newbie data scientist here, I am just starting my way in Azure, is there any I should start NLP? Any trained model or code sample? Thank you for any idea<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1667251332713,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1069986\/trained-nlp-model-snippet",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.0,
        "Challenge_reading_time":2.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Trained NLP model snippet",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":33,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=12b79ebe-b30f-4203-a714-62377c3d557b\">@jackson schmidt  <\/a>     <\/p>\n<p>Sorry I have not heard from you. I have done some researches around NLP in Azure. This can be done by two ways -    <\/p>\n<ol>\n<li> Azure Machine Learning Python SDK\/ ML CLI extension    <br \/>\nWe don't have any trained model you can use in Azure ML but you do have the SDK supporting you to train your model    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-nlp-models?tabs=cli\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-nlp-models?tabs=cli<\/a>    <\/li>\n<li> NLP Server     <br \/>\nApache Spark is a parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications. Azure Synapse Analytics, Azure HDInsight, and Azure Databricks offer access to Spark and take advantage of its processing power.    <\/li>\n<\/ol>\n<p>For customized NLP workloads, Spark NLP serves as an efficient framework for processing a large amount of text. This open-source NLP library provides Python, Java, and Scala libraries that offer the full functionality of traditional NLP libraries such as spaCy, NLTK, Stanford CoreNLP, and Open NLP. Spark NLP also offers functionality such as spell checking, sentiment analysis, and document classification. Spark NLP improves on previous efforts by providing state-of-the-art accuracy, speed, and scalability.    <\/p>\n<p><img src=\"https:\/\/learn.microsoft.com\/en-us\/azure\/architecture\/data-guide\/images\/natural-language-processing-functionality.png\" alt=\"natural-language-processing-functionality.png\" \/>    <\/p>\n<p>The NLP Server is available in Azure Marketplace. To explore large-scale custom NLP in Azure, see NLP Server - <a href=\"https:\/\/azuremarketplace.microsoft.com\/en-US\/marketplace\/apps\/johnsnowlabsinc1646051154808.nlp_server?ocid=gtmrewards_whatsnewblog_nlp_server_040622\">https:\/\/azuremarketplace.microsoft.com\/en-US\/marketplace\/apps\/johnsnowlabsinc1646051154808.nlp_server?ocid=gtmrewards_whatsnewblog_nlp_server_040622<\/a>    <\/p>\n<ol start=\"3\">\n<li> Azure Language Service    <br \/>\nThough we don't have trained model in Azure ML, but we do have REST APIs you can use for Text Analytics, Sentiment Analytics and so on functions for NLP, I would suggest you to check on the document, it may help you achieve your bussiness goals.    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/language-service\/\">https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/language-service\/<\/a>    <\/li>\n<\/ol>\n<p>I hope those information helps. Please let me know if you have any questions regarding to any of above.     <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.6,
        "Solution_reading_time":36.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":311.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've a azure synapse analytics workspace in region North Europe, as the region has hardware Accelerated pools, GPU base pools so to say. But i don't see the packages setting.     <br \/>\nhere is the comparison for 2 workspace, 1 in north Europe and other one in West Europe.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209418-screenshot-2022-06-08-at-120209.png?platform=QnA\" alt=\"209418-screenshot-2022-06-08-at-120209.png\" \/> vs <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209494-screenshot-2022-06-08-at-120550.png?platform=QnA\" alt=\"209494-screenshot-2022-06-08-at-120550.png\" \/>    <\/p>\n<p>Even the package setting in the Workspace itself is disabled for me: here is the screenshot.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209434-screenshot-2022-06-08-at-120143.png?platform=QnA\" alt=\"209434-screenshot-2022-06-08-at-120143.png\" \/>    <\/p>\n<p>I've 2 questions in this reagrd:     <\/p>\n<ul>\n<li> Am I missing any configuration for the GPU pool or this feature is not released?    <\/li>\n<li> Is there any alternate way to install a package? <code>pip install<\/code> or <code>pip3 install<\/code> are not working.      <\/li>\n<\/ul>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1654683175267,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/881432\/how-to-install-python-package-in-hardware-accelera",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":16.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"How to install python package in hardware accelerated GPU spark pool ?",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":134,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=f236076b-e057-4c60-b0fd-0068a6492053\">@Prateek Narula  <\/a>,    <\/p>\n<p>Thanks for the question and using MS Q&amp;A platform.    <\/p>\n<blockquote>\n<p>(UPDATE:6\/10\/2022): Unfortunately, we do not have Library Management (Package) support for GPU spark pools in Azure Synapse Analytics.    <\/p>\n<\/blockquote>\n<p>---------------------------------------------------    <\/p>\n<p>As per the repro, I had noticed similar behaviour.     <\/p>\n<blockquote>\n<p>Looks like packages are only supported for Node size family: &quot;Memory Optimized&quot; - let me get a confirmation from the product team.    <\/p>\n<\/blockquote>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209808-synape-gpu.gif?platform=QnA\" alt=\"209808-synape-gpu.gif\" \/>    <\/p>\n<p>We are reaching out to internal team to get more information related to this issue and will get back to you as soon as we have an update.    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/support\/community-champions-program\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":12.6,
        "Solution_reading_time":26.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":213.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1658238093960,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Does anyone know a repo that shows what a simple HelloWorld java or scala code would look like to build the jar that could be executed using the AWS SageMaker <strong>SparkJarProcessing<\/strong> class?<\/p>\n<p>Readthedocs (<a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_processing\/spark_distributed_data_processing\/sagemaker-spark-processing.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_processing\/spark_distributed_data_processing\/sagemaker-spark-processing.html<\/a>) mentions:<\/p>\n<p>&quot;In the next example, you\u2019ll take a Spark application jar (located in .\/code\/spark-test-app.jar)...&quot;<\/p>\n<p>My question is how does the source code look like for this jar (spark-test-app.jar)?<\/p>\n<p>I tried building a simple Java project jar<\/p>\n<p>src&gt;com.test&gt;HW.java:<\/p>\n<pre><code>\npublic class HW {\n    public static void main(String[] args) {\n        System.out.printf(&quot;hello world!&quot;);\n    }\n}\n<\/code><\/pre>\n<p>and running it inside SageMaker Notebook conda_python3 kernel using<\/p>\n<pre><code>from sagemaker.spark.processing import SparkJarProcessor\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\nprint(role)\n\nspark_processor = SparkJarProcessor(\n    base_job_name=&quot;sm-spark-java&quot;,\n    framework_version=&quot;3.1&quot;,\n    role=role,\n    instance_count=2,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    max_runtime_in_seconds=1200,\n)\n\nspark_processor.run(\n    submit_app=&quot;.\/SparkJarProcessing-1.0-SNAPSHOT.jar&quot;,\n    submit_class=&quot;com.test.HW&quot;,\n    arguments=[&quot;--input&quot;, &quot;abc&quot;],\n    logs=True,\n)\n<\/code><\/pre>\n<p>But end up getting an error:\nCould not execute HW class.<\/p>\n<p>Any sample source code for <strong>spark-test-app.jar<\/strong> would be highly appreciated!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657772745600,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72975189",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.3,
        "Challenge_reading_time":25.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"How to build a simple spark-test-app.jar to test AWS SageMaker SparkJarProcessing",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":49.0,
        "Challenge_word_count":152,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600969850140,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":51.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>To answer your question, the source code of that class looks like:<\/p>\n<pre><code>package com.amazonaws.sagemaker.spark.test;\n\nimport java.lang.invoke.SerializedLambda;\nimport org.apache.commons.cli.CommandLineParser;\nimport org.apache.commons.cli.ParseException;\nimport org.apache.commons.cli.HelpFormatter;\nimport org.apache.commons.cli.Option;\nimport org.apache.commons.cli.BasicParser;\nimport org.apache.commons.cli.Options;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.commons.cli.CommandLine;\nimport org.apache.spark.sql.types.DataTypes;\nimport org.apache.commons.lang3.StringUtils;\nimport java.util.List;\nimport org.apache.spark.sql.SparkSession;\n\npublic class HelloJavaSparkApp\n{\n    public static void main(final String[] args) {\n        System.out.println(&quot;Hello World, this is Java-Spark!&quot;);\n        final CommandLine parsedArgs = parseArgs(args);\n        final String inputPath = parsedArgs.getOptionValue(&quot;input&quot;);\n        final String outputPath = parsedArgs.getOptionValue(&quot;output&quot;);\n        final SparkSession spark = SparkSession.builder().appName(&quot;Hello Spark App&quot;).getOrCreate();\n        System.out.println(&quot;Got a Spark session with version: &quot; + spark.version());\n        System.out.println(&quot;Reading input from: &quot; + inputPath);\n        final Dataset salesDF = spark.read().json(inputPath);\n        salesDF.printSchema();\n        salesDF.createOrReplaceTempView(&quot;sales&quot;);\n        final Dataset topDF = spark.sql(&quot;SELECT date, sale FROM sales WHERE sale &gt; 750 SORT BY sale DESC&quot;);\n        topDF.show();\n        final Dataset avgDF = salesDF.groupBy(&quot;date&quot;, new String[0]).avg(new String[0]).orderBy(&quot;date&quot;, new String[0]);\n        System.out.println(&quot;Collected average sales: &quot; + StringUtils.join((Object[])new List[] { avgDF.collectAsList() }));\n        spark.sqlContext().udf().register(&quot;double&quot;, n -&gt; n + n, DataTypes.LongType);\n        final Dataset saleDoubleDF = salesDF.selectExpr(new String[] { &quot;date&quot;, &quot;sale&quot;, &quot;double(sale) as sale_double&quot; }).orderBy(&quot;date&quot;, new String[] { &quot;sale&quot; });\n        saleDoubleDF.show();\n        System.out.println(&quot;Writing output to: &quot; + outputPath);\n        saleDoubleDF.coalesce(1).write().json(outputPath);\n        spark.stop();\n    }\n    \n    private static CommandLine parseArgs(final String[] args) {\n        final Options options = new Options();\n        final CommandLineParser parser = (CommandLineParser)new BasicParser();\n        final Option input = new Option(&quot;i&quot;, &quot;input&quot;, true, &quot;input path&quot;);\n        input.setRequired(true);\n        options.addOption(input);\n        final Option output = new Option(&quot;o&quot;, &quot;output&quot;, true, &quot;output path&quot;);\n        output.setRequired(true);\n        options.addOption(output);\n        try {\n            return parser.parse(options, args);\n        }\n        catch (ParseException e) {\n            new HelpFormatter().printHelp(&quot;HelloScalaSparkApp --input \/opt\/ml\/input\/foo --output \/opt\/ml\/output\/bar&quot;, options);\n            throw new RuntimeException((Throwable)e);\n        }\n    }\n}\n<\/code><\/pre>\n<p>At the same time, I have created a simple example that shows how to run an hello world app <a href=\"https:\/\/github.com\/giuseppeporcelli\/sagemaker-misc-examples\/blob\/main\/spark-jar-example.ipynb\" rel=\"nofollow noreferrer\">here<\/a>. Please note that I have run that example on Amazon SageMaker Studio Notebooks, using the Data Science 1.0 kernel.<\/p>\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.6,
        "Solution_reading_time":44.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":42.0,
        "Solution_word_count":253.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In a batched trial pipeline see <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/196\" rel=\"nofollow noopener\">196<\/a>, how can we run as fast as possible parallelizing everything we can ?<\/p>\n<p>Example Pipeline:<\/p>\n<pre><code>operation_1: # has no dependencies\n\noperation_2: \n    requires:\n        - operation: operation_1\n    flags-dest: globals\n    flags-import:\n        - some_param\n\noperation_3: \n    requires:\n        - operation: operation_2\n    flags-dest: globals\n    flags-import:\n        - some_other_param\n\npipeline:\n  steps:\n    - run: operation_1\n    - run: operation_2\n      flags:\n        some_param: [a, b]\n    - run: operation_3\n      flags:\n        some_other_param: [1, 2, 3]\n<\/code><\/pre>\n<pre><code># create 6 queues as we have 6 batch trials that can be done in parallel (a1, a2, a3, b1, b2, b3, c1, c2, c3)\n# run this command 6 times\nguild run queue --background \n\nguild run pipeline --stage\n<\/code><\/pre>\n<p>For some reason this leads to out of sequence events happening, like operation 2 being run before operation 1 resulting in an error. Is this the correct way to do this?<\/p>\n<p>Further, is there a good way of timing this to sanity check parallel works faster i.e time batched trials run?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591810114957,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/my.guild.ai\/t\/parallel-batch-trial-pipeline\/142",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.2,
        "Challenge_reading_time":14.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Parallel batch trial pipeline",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":391.0,
        "Challenge_word_count":156,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Don\u2019t use staged runs for this. Just run your pipelines in the background. At some point Guild will be optimized for parallel runs and you won\u2019t have to think about this (as you say - we want to run everything as fast as possible). But at the moment, you need to use parallel OS processes to manage parallel runs.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.4,
        "Solution_reading_time":3.85,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":58.0,
        "Tool":"Guild AI"
    },
    {
        "Answerer_created_time":1513106638900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":276.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":272.3565766667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>As in the documentation \/ tutorial mentioned, we can call <code>Estimator.fit()<\/code> to start Training Job. <\/p>\n\n<p>Required parameter for the method would be the <code>inputs<\/code> that is s3 \/ file reference to the Training File. Example:<\/p>\n\n<pre><code>estimator.fit({'train':'s3:\/\/my-bucket\/training_data})\n<\/code><\/pre>\n\n<p><strong>training-script.py<\/strong><\/p>\n\n<pre><code>parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n<\/code><\/pre>\n\n<p>I would expect <code>os.environ['SM_CHANNEL_TRAIN']<\/code> to be the S3 path. But instead, it returns <code>\/opt\/ml\/input\/data\/train<\/code>.<\/p>\n\n<p>Anyone know why?<\/p>\n\n<p><strong>Update<\/strong><\/p>\n\n<p>I also tried to call estimator.fit('s3:\/\/my-bucket\/training_data'). \nAnd somehow training instance didn't get the SM_CHANNEL_TRAIN Environment Variables. In fact, I didn't see the s3 URI in Environment Variables at all.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565949262600,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1566221662207,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57522553",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.3,
        "Challenge_reading_time":13.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker Estimator.fit() didn't pass the 'train' input to the Training instance",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1411.0,
        "Challenge_word_count":98,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506308535436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Yogyakarta, Indonesia",
        "Poster_reputation_count":3575.0,
        "Poster_view_count":274.0,
        "Solution_body":"<p>When running training jobs in SageMaker the S3 URL containing your training data provided ends up being copied into the docker container (aka training job) from the specified url. Thus the environment variable SM_CHANNEL_TRAIN is pointing to the local path of the training data that was copied from the S3 URL provided.<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html#SageMaker-CreateTrainingJob-request-InputDataConfig\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html#SageMaker-CreateTrainingJob-request-InputDataConfig<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1567202145883,
        "Solution_link_count":2.0,
        "Solution_readability":20.6,
        "Solution_reading_time":8.4,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646758945343,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been trying to train a BertSequenceForClassification Model using AWS Sagemaker. i'm using hugging face estimators. but I keep getting the error: <code>RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 11.17 GiB total capacity; 10.73 GiB already allocated; 87.88 MiB free; 10.77 GiB reserved in total by PyTorch)<\/code> the same code runs fine on my laptop.<\/p>\n<ol>\n<li>how do I check what is occupying that 10GB of memory? my dataset is pretty small (68kb), so is my batch size (8) and epochs (1). When I run nvidia-smi, i can only see &quot;No processes running&quot; and the GPU memory usage is zero. When I run <code>print(torch.cuda.memory_summary(device=None, abbreviated=False))<\/code> from within my training script (right before it throws the error) it prints<\/li>\n<\/ol>\n<pre><code>|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| Allocations           |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Active allocs         |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| GPU reserved segments |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|===========================================================================|\n<\/code><\/pre>\n<p>but I have no idea what it means or how to interpret it<\/p>\n<ol start=\"2\">\n<li>when i run <code>!df -h<\/code> I can see:<\/li>\n<\/ol>\n<pre><code>Filesystem      Size  Used Avail Use% Mounted on\ndevtmpfs         30G   72K   30G   1% \/dev\ntmpfs            30G     0   30G   0% \/dev\/shm\n\/dev\/xvda1      109G   93G   16G  86% \/\n\/dev\/xvdf       196G   61M  186G   1% \/home\/ec2-user\/SageMaker\n<\/code><\/pre>\n<p>how is this memory different from the GPU? if theres 200GB in \/dev\/xvdf is there anyway I can just use that..? in my test script I tried<br \/>\n<code>model = BertForSequenceClassification.from_pretrained(args.model_name,num_labels=args.num_labels).to(&quot;cpu&quot;)<\/code>\nbut that just gives the same error<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646759194503,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1646767755128,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71398882",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":43.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"CUDA: RuntimeError: CUDA out of memory - BERT sagemaker",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1983.0,
        "Challenge_word_count":456,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597997723910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>A <code>CUDA out of memory<\/code> error indicates that your GPU RAM (Random access memory) is full. This is different from the storage on your device (which is the info you get following the <code>df -h<\/code> command).<\/p>\n<p>This memory is occupied by the model that you load into GPU memory, which is independent of  your dataset size. The GPU memory required by the model is at least twice the actual size of the model, but most likely closer to 4 times (initial weights, checkpoint, gradients, optimizer states, etc).<\/p>\n<p>Things you can try:<\/p>\n<ul>\n<li>Provision an instance with more GPU memory<\/li>\n<li>Decrease batch size<\/li>\n<li>Use a different (smaller) model<\/li>\n<\/ul>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1646760473952,
        "Solution_link_count":0.0,
        "Solution_readability":10.5,
        "Solution_reading_time":8.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":108.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>During the preparation for a training (in <code>prepare_data<\/code> in pytorch lightning) I either create or update local data (download, prepare different encodings). I then create a W&amp;B artifact and wait for the upload to be complete. Later in the code (in <code>setup()<\/code> in pytorch lightning) I use the data. Strictly speaking, this is not necessary, because I have the files locally, but I want to track the usage of the data (and the IDs of the data used for training, validation, \u2026). I added the <code>wait()<\/code> statement, because wandb would download the previous version (v=n-1) of the data \/without the enoding just added). In mode <code>ONLINE<\/code> this works nicely. However, in mode <code>DISABLED<\/code> I get this error: <code>ValueError: Cannot call wait on an artifact before it has been logged or in offline mode<\/code>. How am I supposed to handle <code>wait()<\/code>in order to have it work in all modes? (it would be nice if <code>wait()<\/code> would do it).<\/p>\n<p>This is the sample code:<\/p>\n<pre><code class=\"lang-python\"># Upload the data\nartifact = wandb.Artifact(name=..., type=...)\nartifact.description = ...\nartifact.metadata = ...\nartifact.add_file(local_path=...)\nwandb.run.log_artifact(artifact)\nartifact.save()  # I think I don't need this, playing around because of this issue\nartifact.wait()\n<\/code><\/pre>\n<pre><code class=\"lang-python\"># Use (Download) the data\nartifact = wandb.run.use_artifact(artifact_or_name=... + \":latest\")\nartifact_entry = artifact.get_path(...)\nartifact_entry.download(root=...)\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655133484499,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-deal-with-artifact-wait-when-running-in-mode-disabled\/2607",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.1,
        "Challenge_reading_time":20.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"How to deal with artifact.wait() when running in mode \"DISABLED\"",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":210,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/hogru\">@hogru<\/a>, sorry about the late response. Runs have a <code>disabled<\/code> attribute. Here is a code snippet you can use:<\/p>\n<pre><code class=\"lang-auto\">run = wandb.init(mode=\"disabled\")\nif run.disabled:\n    \/\/ your code\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":3.6,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1586454383232,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":44.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":87.4057897222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am running the mlflow registry using <code>mlflow server<\/code> (<a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/model-registry.html<\/a>). The server runs fine. If the server crashes for any reason it restart automatically. But for the time of restart the server is not available.<\/p>\n\n<p>Is it possible to run multiple isntances in parallel behind a load balancer? Is this safe or could it be possible that there are any inconsistencies?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588079911073,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61481147",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":7.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFlow Registry high availability",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":501.0,
        "Challenge_word_count":67,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378563249260,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":470.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>Yes, it's possible to have multiple instances of MLflow Tracker Service running behind a load balancer.<\/p>\n\n<p>Because the Tracking server is stateless, you could have multiple instances log to a replicated primary DB as a store. A second hot standby can take over if the primary fails.<\/p>\n\n<p>As for the documentation in how to set up replicated instances of your backend store will vary on which one you elect to use, we cannot definitely document all different scenarios and their configurations.<\/p>\n\n<p>I would check the respective documentation of your backend DB and load balancer for how to federate requests to multiple instances of an MLflow tracking server, how to failover to a hot standby or replicated DB, or how to configure a hot-standby replicated DB instance.<\/p>\n\n<p>The short of it: MLflow tracking server is stateless.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1588394571916,
        "Solution_link_count":0.0,
        "Solution_readability":12.1,
        "Solution_reading_time":10.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":135.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Good morning,  <br \/>\nI have a a dataset that consist of 99000 (256 x 256 pixels) images. I am trying to use this dataset to training a generative advesarial network (GAN) for at least a 1,000 epoch.   <br \/>\nCurrently, I am using a standard_NC24r (24 cores, 224 GB RAM, 1440 GB disk) GPU  (4 x NVIDIA Tesla K80) cluster but the training is slow. It takes about 3000 seconds to train 1 epoch. This implies it would take at least a month to complete training.  <br \/>\nIs a cluster that I can used to speed up training?  <\/p>\n<p>Thanks for your help in advance  <\/p>\n<p>Many thanks  <\/p>\n<p>Roland<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1633703536287,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/583349\/best-compute-cluster-for-training-large-image-data",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.4,
        "Challenge_reading_time":7.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Best compute cluster for training large image datasets !",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=5554be0b-de3f-4849-9c04-ab53140c4523\">@Okwen, Roland T  <\/a> Thanks, Instead of bigger machines with more memory, there are techniques to be used with Aml Compute for larger datasets. The <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/how-to-use-azureml\/machine-learning-pipelines\/parallel-run\">Parallel Run<\/a> Step is an AzureML Pipeline Step which enables parallel processing or data partitions across multiple workers on multiple nodes. PRS (ParallelRunStep) is designed for embarrassingly parallel workload, e.g. train many models, batch inference, etc.    <\/p>\n<p>Also look into using some of the curated images provided for compute clusters.    <br \/>\nSpecifically look into the DASK image.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/resource-curated-environments\">Curated environments - Azure Machine Learning | Microsoft Learn<\/a>    <\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.2,
        "Solution_reading_time":12.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1640956373383,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":309.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":168.3167494445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an XGBoost model currently in production using AWS sagemaker and making real time inferences. After a while, I would like to update the model with a newer one trained on more data and keep everything as is (e.g. same endpoint, same inference procedure, so really no changes aside from the model itself)<\/p>\n<p>The current deployment procedure is the following :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.xgboost.model import XGBoostModel\nfrom sagemaker.xgboost.model import XGBoostPredictor\n\nxgboost_model = XGBoostModel(\n    model_data = &lt;S3 url&gt;,\n    role = &lt;sagemaker role&gt;,\n    entry_point = 'inference.py',\n    source_dir = 'src',\n    code_location = &lt;S3 url of other dependencies&gt;\n    framework_version='1.5-1',\n    name = model_name)\n\nxgboost_model.deploy(\n    instance_type='ml.c5.large',\n    initial_instance_count=1,\n    endpoint_name = model_name)\n<\/code><\/pre>\n<p>Now that I updated the model a few weeks later, I would like to re-deploy it. I am aware that the <code>.deploy()<\/code> method creates an endpoint and an endpoint configuration so it does it all. I cannot simply re-run my script again since I would encounter an error.<\/p>\n<p>In previous versions of sagemaker I could have updated the model with an extra argument passed to the <code>.deploy()<\/code> method called <code>update_endpoint = True<\/code>. In sagemaker &gt;=2.0 this is a no-op. Now, in sagemaker &gt;= 2.0, I need to use the predictor object as stated in the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html\" rel=\"nofollow noreferrer\">documentation<\/a>. So I try the following :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predictor = XGBoostPredictor(model_name)\npredictor.update_endpoint(model_name= model_name)\n<\/code><\/pre>\n<p>Which actually updates the endpoint according to a new endpoint configuration. However, I do not know what it is updating... I do not specify in the above 2 lines of code that we need to considering the new <code>xgboost_model<\/code> trained on more data...  so where do I tell the update to take a more recent model?<\/p>\n<p>Thank you!<\/p>\n<p><strong>Update<\/strong><\/p>\n<p>I believe that I need to be looking at production variants as stated in their documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-ab-testing.html\" rel=\"nofollow noreferrer\">here<\/a>. However, their whole tutorial is based on the amazon sdk for python (boto3) which has artifacts that are hard to manage when I have difference entry points for each model variant (e.g. different <code>inference.py<\/code> scripts).<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663233254363,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1663319367852,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73728499",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":11.5,
        "Challenge_reading_time":33.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":null,
        "Challenge_title":"How to update an existing model in AWS sagemaker >= 2.0",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":41.0,
        "Challenge_word_count":333,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1640956373383,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":309.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Since I found an answer to my own question I will post it here for those who encounter the same problem.<\/p>\n<p>I ended up re-coding all my deployment script using the boto3 SDK rather than the sagemaker SDK (or a mix of both as some documentation suggest).<\/p>\n<p>Here's the whole script that shows how to create a sagemaker model object, an endpoint configuration and an endpoint to deploy the model on for the first time. In addition, it shows what to do how to update the endpoint with a newer model (which was my main question)<\/p>\n<p>Here's the code to do all 3 in case you want to bring your own model and update it safely in production using sagemaker :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\nimport time\nfrom datetime import datetime\nfrom sagemaker import image_uris\nfrom fileManager import *  # this is a local script for helper functions\n\n# name of zipped model and zipped inference code\nCODE_TAR = 'your_inference_code_and_other_artifacts.tar.gz'\nMODEL_TAR = 'your_saved_xgboost_model.tar.gz'\n\n# sagemaker params\nsmClient = boto3.client('sagemaker')\nsmRole = &lt;your_sagemaker_role&gt;\nbucket = sagemaker.Session().default_bucket()\n\n# deploy algorithm\nclass Deployer:\n\n    def __init__(self, modelName, deployRetrained=False):\n        self.modelName=modelName\n        self.deployRetrained = deployRetrained\n        self.prefix = &lt;S3_model_path_prefix&gt;\n    \n    def deploy(self):\n        '''\n        Main method to create a sagemaker model, create an endpoint configuration and deploy the model. If deployRetrained\n        param is set to True, this method will update an already existing endpoint.\n        '''\n        # define model name and endpoint name to be used for model deployment\/update\n        model_name = self.modelName + &lt;any_suffix&gt;\n        endpoint_config_name = self.modelName + '-%s' %datetime.now().strftime('%Y-%m-%d-%HH%M')\n        endpoint_name = self.modelName\n        \n        # deploy model for the first time\n        if not self.deployRetrained:\n            print('Deploying for the first time')\n\n            # here you should copy and zip the model dependencies that you may have (such as preprocessors, inference code, config code...)\n            # mine were zipped into the file called CODE_TAR\n\n            # upload model and model artifacts needed for inference to S3\n            uploadFile(list_files=[MODEL_TAR, CODE_TAR], prefix = self.prefix)\n\n            # create sagemaker model and endpoint configuration\n            self.createSagemakerModel(model_name)\n            self.createEndpointConfig(endpoint_config_name, model_name)\n\n            # deploy model and wait while endpoint is being created\n            self.createEndpoint(endpoint_name, endpoint_config_name)\n            self.waitWhileCreating(endpoint_name)\n        \n        # update model\n        else:\n            print('Updating existing model')\n\n            # upload model and model artifacts needed for inference (here the old ones are replaced)\n            # make sure to make a backup in S3 if you would like to keep the older models\n            # we replace the old ones and keep the same names to avoid having to recreate a sagemaker model with a different name for the update!\n            uploadFile(list_files=[MODEL_TAR, CODE_TAR], prefix = self.prefix)\n\n            # create a new endpoint config that takes the new model\n            self.createEndpointConfig(endpoint_config_name, model_name)\n\n            # update endpoint\n            self.updateEndpoint(endpoint_name, endpoint_config_name)\n\n            # wait while endpoint updates then delete outdated endpoint config once it is InService\n            self.waitWhileCreating(endpoint_name)\n            self.deleteOutdatedEndpointConfig(model_name, endpoint_config_name)\n\n    def createSagemakerModel(self, model_name):\n        ''' \n        Create a new sagemaker Model object with an xgboost container and an entry point for inference using boto3 API\n        '''\n        # Retrieve that inference image (container)\n        docker_container = image_uris.retrieve(region=region, framework='xgboost', version='1.5-1')\n\n        # Relative S3 path to pre-trained model to create S3 model URI\n        model_s3_key = f'{self.prefix}\/'+ MODEL_TAR\n\n        # Combine bucket name, model file name, and relate S3 path to create S3 model URI\n        model_url = f's3:\/\/{bucket}\/{model_s3_key}'\n\n        # S3 path to the necessary inference code\n        code_url = f's3:\/\/{bucket}\/{self.prefix}\/{CODE_TAR}'\n        \n        # Create a sagemaker Model object with all its artifacts\n        smClient.create_model(\n            ModelName = model_name,\n            ExecutionRoleArn = smRole,\n            PrimaryContainer = {\n                'Image': docker_container,\n                'ModelDataUrl': model_url,\n                'Environment': {\n                    'SAGEMAKER_PROGRAM': 'inference.py', #inference.py is at the root of my zipped CODE_TAR\n                    'SAGEMAKER_SUBMIT_DIRECTORY': code_url,\n                }\n            }\n        )\n    \n    def createEndpointConfig(self, endpoint_config_name, model_name):\n        ''' \n        Create an endpoint configuration (only for boto3 sdk procedure) and set production variants parameters.\n        Each retraining procedure will induce a new variant name based on the endpoint configuration name.\n        '''\n        smClient.create_endpoint_config(\n            EndpointConfigName=endpoint_config_name,\n            ProductionVariants=[\n                {\n                    'VariantName': endpoint_config_name,\n                    'ModelName': model_name,\n                    'InstanceType': INSTANCE_TYPE,\n                    'InitialInstanceCount': 1\n                }\n            ]\n        )\n\n    def createEndpoint(self, endpoint_name, endpoint_config_name):\n        '''\n        Deploy the model to an endpoint\n        '''\n        smClient.create_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=endpoint_config_name)\n    \n    def deleteOutdatedEndpointConfig(self, name_check, current_endpoint_config):\n        '''\n        Automatically detect and delete endpoint configurations that contain a string 'name_check'. This method can be used\n        after a retrain procedure to delete all previous endpoint configurations but keep the current one named 'current_endpoint_config'.\n        '''\n        # get a list of all available endpoint configurations\n        all_configs = smClient.list_endpoint_configs()['EndpointConfigs']\n\n        # loop over the names of endpoint configs\n        names_list = []\n        for config_dict in all_configs:\n            endpoint_config_name = config_dict['EndpointConfigName']\n\n            # get only endpoint configs that contain name_check in them and save names to a list\n            if name_check in endpoint_config_name:\n                names_list.append(endpoint_config_name)\n        \n        # remove the current endpoint configuration from the list (we do not want to detele this one since it is live)\n        names_list.remove(current_endpoint_config)\n\n        for name in names_list:\n            try:\n                smClient.delete_endpoint_config(EndpointConfigName=name)\n                print('Deleted endpoint configuration for %s' %name)\n            except:\n                print('INFO : No endpoint configuration was found for %s' %endpoint_config_name)\n\n    def updateEndpoint(self, endpoint_name, endpoint_config_name):\n        ''' \n        Update existing endpoint with a new retrained model\n        '''\n        smClient.update_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=endpoint_config_name,\n            RetainAllVariantProperties=True)\n    \n    def waitWhileCreating(self, endpoint_name):\n        ''' \n        While the endpoint is being created or updated sleep for 60 seconds.\n        '''\n        # wait while creating or updating endpoint\n        status = smClient.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n        print('Status: %s' %status)\n        while status != 'InService' and status !='Failed':\n            time.sleep(60)\n            status = smClient.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n            print('Status: %s' %status)\n        \n        # in case of a deployment failure raise an error\n        if status == 'Failed':\n            raise ValueError('Endpoint failed to deploy')\n\nif __name__==&quot;__main__&quot;:\n    deployer = Deployer('churnmodel', deployRetrained=True)\n    deployer.deploy()\n<\/code><\/pre>\n<p>Final comments :<\/p>\n<ul>\n<li><p>The sagemaker <a href=\"https:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/realtime-endpoints-deployment.html\" rel=\"nofollow noreferrer\">documentation<\/a> mentions all this but fails to state that you can provide an 'entry_point' to the <code>create_model<\/code> method as well as a 'source_dir' for inference dependencies (e.g. normalization artifacts). It can be done as seen in <code>PrimaryContainer<\/code> argument.<\/p>\n<\/li>\n<li><p>my <code>fileManager.py<\/code> script just contains basic functions to make tar files, upload and download to and from my S3 paths. To simplify the class, I have not included them in.<\/p>\n<\/li>\n<li><p>The method deleteOutdatedEndpointConfig may seem like an overkill with an unnecessary loop and checks, I do so because I have multiple endpoint configurations to handle and wanted to remove the ones that weren't live AND contain the string <code>name_check<\/code> (I do not know the exact name of the configuration since there is a datetime suffix). Feel free to simplify it or remove it all together if you feel like it.<\/p>\n<\/li>\n<\/ul>\n<p>Hope it helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1663925308150,
        "Solution_link_count":1.0,
        "Solution_readability":14.3,
        "Solution_reading_time":106.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":57.0,
        "Solution_word_count":924.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1621409485092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":3609.0,
        "Answerer_view_count":2438.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run this 10 line .ipynb file from Google Colab in Microsoft Azure Machine Learning Studio<\/p>\n<p><a href=\"https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing\" rel=\"nofollow noreferrer\">https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing<\/a><\/p>\n<p>When I get to this step:<\/p>\n<pre><code>import gradio as gr\nimport tensorflow as tf\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n<\/code><\/pre>\n<p>I get this error:<\/p>\n<pre><code>ContextualVersionConflict: (Flask 1.0.3 (\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages), Requirement.parse('Flask&gt;=1.1.1'), {'gradio'})\n<\/code><\/pre>\n<p>I tried to install the Flask 1.1.1 version but I get more errors. Any idea what I should do to get past this step in Azure ML Studio?<\/p>\n<pre><code>!pip install \u2013force-reinstall Flask==1.1.1\n\/\/ More errors\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630103458310,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1630103807136,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68959934",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":13.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Contextual version conflict error, Microsoft Azure Machine Learning Studio",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":96,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1630103231523,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>The issue is because <code>gradio<\/code> package using existing Flask package (version 1.0.3). But as your application required Flask&gt;=1.1.1, therefore it is showing error. You need to uninstall the existing Flask package and then install the latest required version.<\/p>\n<p>To uninstall the existing package:\n<code>!pip uninstall Flask -y<\/code><\/p>\n<p>To install latest package:\n<code>!pip install Flask&gt;=1.1.1<\/code><\/p>\n<p><strong>Then, make sure to restart your runtime to pick up the new Flask using the Runtime -&gt; Restart runtime menu.<\/strong><\/p>\n<p>Finally, import gradio.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":7.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":77.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1577353307072,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":491.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to load files as a dataset in the GUI of Azure ML Studio. These parquet files have been created through Spark.<\/p>\n<p>In my folder, Spark creates files such as &quot;_SUCCESS&quot; or &quot;_committed_8998000&quot;.<\/p>\n<p>Azure ML Studio is not able to read them or ignore them and tells me:<\/p>\n<pre><code>The provided file(s) have invalid byte(s) for the specified file encoding.\n{\n  &quot;message&quot;: &quot; &quot;\n}\n<\/code><\/pre>\n<p>I selected &quot;Ignore unmatched files path&quot; and yet, it still does not work.<\/p>\n<p>If I remove the &quot;_SUCCESS&quot; and other Spark files, it works.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601468292143,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64137409",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.0,
        "Challenge_reading_time":9.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I create an Azure dataset in Azure ML studio (through the GUI) from a parquet file created with Azure Spark",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":178.0,
        "Challenge_word_count":109,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423640080283,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lyon, France",
        "Poster_reputation_count":457.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>Thanks for the feedback. You can use globing in path. e.g. path = '**\/*.parquet' to select only the parquet files<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.4,
        "Solution_reading_time":1.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1434117836363,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Toronto, ON, Canada",
        "Answerer_reputation_count":78.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have installed sagemaker using <code>sc.install_pypi_package(&quot;sagemaker==2.5.1&quot;)<\/code>. However, I get the following error when I try to import sagemaker and it is pointing to python2.7.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yfln3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yfln3.png\" alt=\"cannot import name git_utils\" \/><\/a><\/p>\n<p>I checked my EMR master node running pyspark and the version there is pyspark 2.4.5 running python 3.7.6.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hyctk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hyctk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So then I tried to upgrade the python version of my spark context but it says<\/p>\n<blockquote>\n<p>&quot;ValueError: Package already installed for current Spark context!&quot;<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/XBg3E.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XBg3E.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So I thought lemme try uninstalling python2.7 from spark context and that does not let me do it, saying<\/p>\n<blockquote>\n<p>&quot;Not uninstalling python at \/usr\/lib64\/python2.7\/lib-dynload, outside\nenvironment \/tmp\/1598628537004-0&quot;<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ktlfO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ktlfO.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What am I doing wrong? I believe the sagemaker import is failing due to spark context referring python2.7. How do I fix this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598630122480,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1598631210847,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63637178",
        "Challenge_link_count":8,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":22.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"Spark is running python 3.7.6 but spark context is showing python 2.7. How to fix using the spark context?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":855.0,
        "Challenge_word_count":178,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1434117836363,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":78.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Referred to this <a href=\"https:\/\/aws.amazon.com\/blogs\/big-data\/install-python-libraries-on-a-running-cluster-with-emr-notebooks\/\" rel=\"nofollow noreferrer\">link<\/a> and updated the python version of spark context to python3. This fixes the issue:<\/p>\n<pre><code>%%configure -f\n{ &quot;conf&quot;:{\n          &quot;spark.pyspark.python&quot;: &quot;python3&quot;,\n          &quot;spark.pyspark.virtualenv.enabled&quot;: &quot;true&quot;,\n          &quot;spark.pyspark.virtualenv.type&quot;:&quot;native&quot;,\n          &quot;spark.pyspark.virtualenv.bin.path&quot;:&quot;\/usr\/bin\/virtualenv&quot;\n         }\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":22.2,
        "Solution_reading_time":8.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I\u2019ve recently started working with azure for ML and am trying to use machine learning service workspace.\nI\u2019ve set up a workspace with the compute set to NC6s-V2 machines since I need train a NN using images on GPU. <\/p>\n\n<p>The issue is that the training still happens on the CPU \u2013 the logs say it\u2019s not able to find CUDA. Here\u2019s the warning log when running my script.\nAny clues how to solve this issue?<\/p>\n\n<p>I\u2019ve also mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. <\/p>\n\n<p>Here's my code for the estimator,<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>script_params = {\n         '--input_data_folder': ds.path('dataset').as_mount(),\n         '--zip_file_name': 'train.zip',\n         '--run_mode': 'train'\n    }\n\n\nest = Estimator(source_directory='.\/scripts',\n                     script_params=script_params,\n                     compute_target=compute_target,\n                     entry_script='main.py',\n                     conda_packages=['scikit-image', 'keras', 'tqdm', 'pillow', 'matplotlib', 'scipy', 'tensorflow-gpu']\n                     )\n\nrun = exp.submit(config=est)\n\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n\n<p>The compute target was made as per the sample code on github:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>compute_name = \"P100-NC6s-V2\"\ncompute_min_nodes = 0\ncompute_max_nodes = 4\n\nvm_size = \"STANDARD_NC6S_V2\"\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n                                                                min_nodes=compute_min_nodes,\n                                                                max_nodes=compute_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(\n        ws, compute_name, provisioning_config)\n\n    # can poll for a minimum number of nodes and for a specific timeout.\n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(\n        show_output=True, min_node_count=None, timeout_in_minutes=20)\n\n    # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())\n\n<\/code><\/pre>\n\n<p>This is the warning with which it fails to use the GPU:<\/p>\n\n<pre><code>2019-08-12 14:50:16.961247: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x55a7ce570830 executing computations on platform Host. Devices:\n2019-08-12 14:50:16.961278: I tensorflow\/compiler\/xla\/service\/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n2019-08-12 14:50:16.971025: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/azureml-envs\/azureml_5fdf05c5671519f307e0f43128b8610e\/lib:\n2019-08-12 14:50:16.971054: E tensorflow\/stream_executor\/cuda\/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n2019-08-12 14:50:16.971081: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971089: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:176] hostname: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971164: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\n2019-08-12 14:50:16.971202: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:204] kernel reported version is: 418.40.4\nDevice mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n2019-08-12 14:50:16.973301: I tensorflow\/core\/common_runtime\/direct_session.cc:296] Device mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n\n<\/code><\/pre>\n\n<p>It's currently using the CPU as per the logs. Any clues how to resolve the issue here?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565670597500,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1565670701316,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57471129",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":14.8,
        "Challenge_reading_time":57.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to use GPU to train a NN model in azure machine learning service using P100-NC6s-V2 compute. Fails wth CUDA error",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1402.0,
        "Challenge_word_count":396,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408145271463,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":65.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Instead of base Estimator, you can use the Tensorflow Estimator with Keras and other libraries layered on top. That way you don't have to worry about setting up and configuring the GPU libraries, as the Tensorflow Estimator uses a Docker image with GPU libraries pre-configured. <\/p>\n\n<p>See here for documentation:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.tensorflow?view=azure-ml-py\" rel=\"nofollow noreferrer\">API Reference<\/a> You can use <code>conda_packages<\/code> argument to specify additional libraries. Also set argument <code>use_gpu = True<\/code>.<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training-with-deep-learning\/train-hyperparameter-tune-deploy-with-keras\/train-hyperparameter-tune-deploy-with-keras.ipynb\" rel=\"nofollow noreferrer\">Example Notebook<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.2,
        "Solution_reading_time":11.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":74.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1335447186710,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Egypt",
        "Answerer_reputation_count":1972.0,
        "Answerer_view_count":547.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a simple requirement, I need to run sagemaker prediction inside a spark job<\/p>\n<p>am trying to run the below<\/p>\n<pre><code>ENDPOINT_NAME = &quot;MY-ENDPOINT_NAME&quot;\nfrom sagemaker_pyspark import SageMakerModel\nfrom sagemaker_pyspark import EndpointCreationPolicy\nfrom sagemaker_pyspark.transformation.serializers import ProtobufRequestRowSerializer\nfrom sagemaker_pyspark.transformation.deserializers import ProtobufResponseRowDeserializer\n\nattachedModel = SageMakerModel(\n    existingEndpointName=ENDPOINT_NAME,\n    endpointCreationPolicy=EndpointCreationPolicy.DO_NOT_CREATE,\n    endpointInstanceType=None,  # Required\n    endpointInitialInstanceCount=None,  # Required\n    requestRowSerializer=ProtobufRequestRowSerializer(\n        featuresColumnName=&quot;featureCol&quot;\n    ),  # Optional: already default value\n    responseRowDeserializer= ProtobufResponseRowDeserializer(schema=ouput_schema),\n)\n\ntransformedData2 = attachedModel.transform(df)\ntransformedData2.show()\n<\/code><\/pre>\n<p>I get the following error <code>TypeError: 'JavaPackage' object is not callable<\/code><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662667047127,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73654460",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":29.4,
        "Challenge_reading_time":14.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"how to use sagemaker inside pyspark",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":18.0,
        "Challenge_word_count":75,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1335447186710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Egypt",
        "Poster_reputation_count":1972.0,
        "Poster_view_count":547.0,
        "Solution_body":"<p>this was solved by ...<\/p>\n<pre><code>classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars())\nconf = SparkConf() \\\n    .set(&quot;spark.driver.extraClassPath&quot;, classpath)\nsc = SparkContext(conf=conf)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.5,
        "Solution_reading_time":3.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":14.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a simple linear learner in AWS SageMaker with MXNet. I have never worked with SageMaker or MXNet previously. Fitting the model gives runtime error as follows and shuts the instance:<\/p>\n\n<blockquote>\n  <p>UnexpectedStatusException: Error for Training job\n  linear-learner-2020-02-11-06-13-22-712: Failed. Reason: ClientError:\n  Unable to read data channel 'train'. Requested content-type is\n  'application\/x-recordio-protobuf'. Please verify the data matches the\n  requested content-type. (caused by MXNetError)<\/p>\n<\/blockquote>\n\n<p>I think that the data should be converted to protobuf format before passing as training data. Could someone please explain to me what is the correct format for MXNet models? What is the best way to convert a simple data frame into protobuf?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581404906080,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60163614",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"What is correct input for mxnet's linear learner in AWS SageMaker?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":617.0,
        "Challenge_word_count":120,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1490866954400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Ahmedabad, Gujarat, India",
        "Poster_reputation_count":834.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p><a href=\"https:\/\/github.com\/awslabs\/fraud-detection-using-machine-learning\/blob\/master\/source\/notebooks\/sagemaker_fraud_detection.ipynb\" rel=\"nofollow noreferrer\">This end-to-end demo<\/a> shows usage of Linear Learner from input data pre-processed in <code>pandas<\/code> dataframes and then converted to protobuf using the SDK. But note that:<\/p>\n\n<ul>\n<li>There is no need to use protobuf, you can also pass csv data with the target variable on the first column of the files, as <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html#ll-input_output\" rel=\"nofollow noreferrer\">indicated here<\/a>.<\/li>\n<li>There is no need to know MXNet in order to use the SageMaker Linear Learner, just use the SDK of your choice, bring data to S3, and orchestrate training and inference :)<\/li>\n<\/ul>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.1,
        "Solution_reading_time":10.56,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":93.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>we have also found this example of using <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-use-databricks-as-compute-target.ipynb\">Databricks as a Compute Target for an Azure Machine Learning Pipeline<\/a>.  <\/p>\n<p>However, we want to use an existing Databricks Cluster as compute target within Azure Machine Learning Studio for our Azure Machine Learning Pipeline.  <br \/>\nCould you help us in accomplishing this, please?  <\/p>\n<p>With best regards  <br \/>\nAlex  <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654614267930,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/880189\/connecting-to-an-existing-databricks-cluster-in-am",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":8.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Connecting to an existing Databricks Cluster in AMLS",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":69,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@AlexanderPakakis-0994 Are you looking at adding the cluster from the UI of ML studio rather than using the SDK as mentioned in the notebook you referenced?    <br \/>\nIf Yes, you need to add the same attached compute.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209283-image.png?platform=QnA\" alt=\"209283-image.png\" \/>    <\/p>\n<p>Once you select Azure Databricks the following option to add the existing databricks workspace is seen.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209260-image.png?platform=QnA\" alt=\"209260-image.png\" \/>    <\/p>\n<p>I hope this helps!!    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":10.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.2,
        "Solution_reading_time":13.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<h1>Difficulty in understanding<\/h1>\n<p>Q2) How to download a file from S3?<\/p>\n<p><strong>From<\/strong>  <a href=\"https:\/\/medium.com\/akeneo-labs\/machine-learning-workflow-with-sagemaker-b83b293337ff\" rel=\"nofollow noreferrer\">The Machine Learning Workflow with SageMaker<\/a><\/p>\n<p>And also why are we using this piece of code?<\/p>\n<p><code>estimator.fit(train_data_location)<\/code><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568926981497,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58018893",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":6.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How do S3 file download and estimator.fit() work in this blog post?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":220.0,
        "Challenge_word_count":43,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1556451987416,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":1309.0,
        "Poster_view_count":288.0,
        "Solution_body":"<h2>Downloading a file from S3:<\/h2>\n\n<p>This code block in the Q2 section defines the function that downloads a file from S3. The user instantiates an S3 client, and then passes the S3 URL along to the <code>s3.Bucket.download_file()<\/code> method.<\/p>\n\n<pre><code>def download_from_s3(url):\n    \"\"\"ex: url = s3:\/\/sagemakerbucketname\/data\/validation.tfrecords\"\"\"\n    url_parts = url.split(\"\/\")  # =&gt; ['s3:', '', 'sagemakerbucketname', 'data', ...\n    bucket_name = url_parts[2]\n    key = os.path.join(*url_parts[3:])\n    filename = url_parts[-1]\n    if not os.path.exists(filename):\n        try:\n            # Create an S3 client\n            s3 = boto3.resource('s3')\n            print('Downloading {} to {}'.format(url, filename))\n            s3.Bucket(bucket_name).download_file(key, filename)\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] == \"404\":\n                print('The object {} does not exist in bucket {}'.format(\n                    key, bucket_name))\n            else:\n                raise\n<\/code><\/pre>\n\n<h2>Estimator.fit() explanation:<\/h2>\n\n<p>The <code>estimator.fit(train_data_location)<\/code> line is what initiates the training process with SageMaker. When run, SageMaker will provision the necessary infrastructure, fetch the data from the location the user designated (here, <code>train_data_location<\/code> which is a path to Amazon S3) and distribute it amongst the training cluster, carry out the training process, return the resulting model, and tear down the training infrastructure. <\/p>\n\n<p>You can find the result of this training job in the SageMaker console.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":19.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":166.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Amazon SageMaker to train a model with a lot of data. \nThis takes a lot of time - hours or even days. During this time, I would like be able to query the trainer and see its current status, particularly:<\/p>\n\n<ul>\n<li>How many iterations it already did, and how many iterations it still needs to do? (the training algorithm is deep learning - it is based on iterations).<\/li>\n<li>How much time does it need to complete the training?<\/li>\n<li>Ideally, I would like to classify a test-sample using the model of the current iteration, to see its current performance.<\/li>\n<\/ul>\n\n<p>One way to do this is to explicitly tell the trainer to print debug messages after each iteration. However, these messages will be availble only at the console from which I run the trainer. Since training takes so much time, I would like to be able to query the trainer status remotely, from different computers.<\/p>\n\n<p>Is there a way to remotely query the status of a running trainer?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1537104812430,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1537105180103,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52354671",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.3,
        "Challenge_reading_time":12.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Watching over SageMaker while it is training",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":504.0,
        "Challenge_word_count":174,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1309774923240,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":31050.0,
        "Poster_view_count":2297.0,
        "Solution_body":"<p>All logs are available in Amazon Cloudwatch. You can query CloudWatch programmatically or via an API to parse the logs.<\/p>\n\n<p>Are you using built-in algorithms or a Framework like MXNet or TensorFlow? For TensorFlow you can monitor your job with <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_resnet_cifar10_with_tensorboard\/tensorflow_resnet_cifar10_with_tensorboard.ipynb\" rel=\"nofollow noreferrer\">TensorBoard<\/a>.<\/p>\n\n<p>Additionally, you can see high level job status using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_DescribeTrainingJob.html\" rel=\"nofollow noreferrer\">describe training job<\/a> API call:<\/p>\n\n<pre><code>import sagemaker\nsm_client = sagemaker.Session().sagemaker_client\nprint(sm_client.describe_training_job(TrainingJobName='You job name here'))\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.7,
        "Solution_reading_time":11.66,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have not seen any doc talking about this topic, is this supported in Microsoft Machine Learning? Is this a good plan if anyone has tried? <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661358044013,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/980372\/time-series-training-for-anomal-detect",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":2.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Time series training for anomal detect",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":32,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=033f3419-75e1-402b-b1ac-8869dd655829\">@minhoo lee  <\/a>     <\/p>\n<p>Thanks for using Microsoft Q&amp;A platform, Azure Machine Learning Serivce support time series training - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-forecast\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-forecast<\/a>    <\/p>\n<p>You can check above document to see how to set up a quick model.    <\/p>\n<p>But for Anomaly Dectection, I think Anomaly Detector API is a better choice for you - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/anomaly-detector\/\">https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/anomaly-detector\/<\/a>    <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.5,
        "Solution_reading_time":11.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":81.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1393524211332,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":745.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to make transfer learning method on MXNet on Sagemaker instance. Train and serve start locally without any problem and I'm using that python code to predict:<\/p>\n\n<pre><code>def predict_mx(net, fname):\n    with open(fname, 'rb') as f:\n      img = image.imdecode(f.read())\n      plt.imshow(img.asnumpy())\n      plt.show()\n    data = transform(img, -1, test_augs)\n    plt.imshow(data.transpose((1,2,0)).asnumpy()\/255)\n    plt.show()\n    data = data.expand_dims(axis=0)\n    return net.predict(data.asnumpy().tolist())\n<\/code><\/pre>\n\n<p>I checked <code>data.asnumpy().tolist()<\/code> that is ok and pyplot draw images (firts is the original image, the second is the resized image). But <code>net.predict<\/code> raise an error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nJSONDecodeError                           Traceback (most recent call last)\n&lt;ipython-input-171-ea0f1f5bdc72&gt; in &lt;module&gt;()\n----&gt; 1 predict_mx(predictor.predict, '.\/data2\/burgers-imgnet\/00103785.jpg')\n\n&lt;ipython-input-170-150a72b14997&gt; in predict_mx(net, fname)\n     30     plt.show()\n     31     data = data.expand_dims(axis=0)\n---&gt; 32     return net(data.asnumpy().tolist())\n     33 \n\n~\/Projects\/Lab\/ML\/AWS\/v\/lib64\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data)\n     89         if self.deserializer is not None:\n     90             # It's the deserializer's responsibility to close the stream\n---&gt; 91             return self.deserializer(response_body, response['ContentType'])\n     92         data = response_body.read()\n     93         response_body.close()\n\n~\/Projects\/Lab\/ML\/AWS\/v\/lib64\/python3.6\/site-packages\/sagemaker\/predictor.py in __call__(self, stream, content_type)\n    290         \"\"\"\n    291         try:\n--&gt; 292             return json.load(codecs.getreader('utf-8')(stream))\n    293         finally:\n    294             stream.close()\n\n\/usr\/lib64\/python3.6\/json\/__init__.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    297         cls=cls, object_hook=object_hook,\n    298         parse_float=parse_float, parse_int=parse_int,\n--&gt; 299         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n    300 \n    301 \n\n\/usr\/lib64\/python3.6\/json\/__init__.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    352             parse_int is None and parse_float is None and\n    353             parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354         return _default_decoder.decode(s)\n    355     if cls is None:\n    356         cls = JSONDecoder\n\n\/usr\/lib64\/python3.6\/json\/decoder.py in decode(self, s, _w)\n    337 \n    338         \"\"\"\n--&gt; 339         obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n    340         end = _w(s, end).end()\n    341         if end != len(s):\n\n\/usr\/lib64\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n    355             obj, end = self.scan_once(s, idx)\n    356         except StopIteration as err:\n--&gt; 357             raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n    358         return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n<\/code><\/pre>\n\n<p>I tried to json.dumps my data, and there is no problem with that.<\/p>\n\n<p>Note that I didn't deployed the service on AWS yet, I want to be able to test the model and prediction locally before to make a larger train and to serve it later.<\/p>\n\n<p>Thanks for your help<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":4,
        "Challenge_created_time":1528098536343,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50675708",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":10.9,
        "Challenge_reading_time":41.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker Predict on local instance, JSON Error",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1358.0,
        "Challenge_word_count":332,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340280616088,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Laval, France",
        "Poster_reputation_count":2835.0,
        "Poster_view_count":281.0,
        "Solution_body":"<p>The call to <strong>net.predict<\/strong> is working fine. <\/p>\n\n<p>It seems that you are using the SageMaker Python SDK <strong>predict_fn<\/strong> for hosting. After the <strong>predict_fn<\/strong> is invoked, the MXNet container will try to serialize your prediction to JSON before sending it back to the client. You can see code that does that here: <a href=\"https:\/\/github.com\/aws\/sagemaker-mxnet-container\/blob\/master\/src\/mxnet_container\/serve\/transformer.py#L132\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-mxnet-container\/blob\/master\/src\/mxnet_container\/serve\/transformer.py#L132<\/a><\/p>\n\n<p>The container is failing to serialize because <strong>net.predict<\/strong> does not return a serializable object. You can solve this issue by returning a list instead:<\/p>\n\n<pre><code>return net.predict(data.asnumpy().tolist()).asnumpy().tolist()\n<\/code><\/pre>\n\n<p>Another alternative is to use a <strong>transform_fn<\/strong> instead of <strong>prediction_fn<\/strong> so you can handle the output serialization yourself. You can see an example of a <strong>transform_fn<\/strong> here <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/e93eff66626c0ab1f292048451c4c3ac7c39a121\/examples\/cli\/host\/script.py#L41\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/e93eff66626c0ab1f292048451c4c3ac7c39a121\/examples\/cli\/host\/script.py#L41<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":16.4,
        "Solution_reading_time":18.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":114.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":1.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI have created a simple notebook on SageMaker Studio using the Image \"Tensorflow 2.6 Python 3.8 GPU optimized\". But when I try to run simple statement viz. \"import tensorflow\", I am getting the error \"no module named 'tensorflow'\".\n\nI tried to install 'tensorflow' package using pip from the terminal attached to the image. But it shows the message \"requirement already satisfied\".\n\nAm I missing anything here? Please help.\n\nThanks in advance, \nPraveen",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640585801507,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1668460271272,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUN63fjMWsT5uVUNn2AIsmhw\/sagemaker-studio-notebook-no-module-named-tensorflow-when-chosen-image-type-tensorflow-2-6-python-3-8-gpu-optimized",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.2,
        "Challenge_reading_time":7.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker Studio notebook - no module named 'tensorflow' when chosen image type \"Tensorflow 2.6 Python 3.8 GPU optimized\"",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":358.0,
        "Challenge_word_count":89,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"From the question, I understand that you are trying to use a TensorFlow 2.6 Python 3.8 kernel in SageMaker Studio, but you are unable to import tensorflow.\n\nThe service team are aware of this issue and are actively working on a fix.\n\nMitigation Option\n\nA)  If your use case is version flexible, version other than 2.6 should work.\n \nB) If not, you can try the following as workaround \n\n1. Open a notebook using a Tensorflow 2.6 Python 3.8 kernel\n2. Execute the following line in a notebook cell:\n!sed -i 's|^ *\"python\",|  \"\/usr\/local\/bin\/python\",|g' \/usr\/local\/share\/jupyter\/kernels\/python3\/kernel.json\n3. Stop the kernel\n4. Re-attach the kernel to your notebook.\n\nHope it helps!",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1640721332675,
        "Solution_link_count":0.0,
        "Solution_readability":4.9,
        "Solution_reading_time":8.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":108.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1343828614448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seville, Spain",
        "Answerer_reputation_count":359.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an inference pipeline with some PythonScriptStep with a ParallelRunStep in the middle. Everything works fine except for the fact that all mini batches are run on one node during the ParallelRunStep, no matter how many nodes I put in the <code>node_count<\/code> config argument.<\/p>\n<p>All the nodes seem to be up and running in the cluster, and according to the logs the <code>init()<\/code> function has been run on them multiple times. Diving into the logs I can see in <strong>sys\/error\/10.0.0.*<\/strong> that all the workers except the one that is working are saying:<\/p>\n<p><code>FileNotFoundError: [Errno 2] No such file or directory: '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/virtualstage\/azureml\/c36eb050-adc9-4c34-8a33-5f6d42dcb19c\/wd\/tmp8_txakpm\/bg.png'<\/code><\/p>\n<p><strong>bg.png<\/strong> happens to be a side argument created in a previous PythonScriptStep that I'm passing to the ParallelRunStep:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>bg_file = PipelineData('bg',  datastore=data_store)\nbg_file_ds = bg_file.as_dataset()\nbg_file_named = bg_file_ds.as_named_input(&quot;bg&quot;)\nbg_file_dw = bg_file_named.as_download()\n\n...\n\nparallelrun_step = ParallelRunStep(\n    name='batch-inference',\n    parallel_run_config=parallel_run_config,\n    inputs=[frames_data_named.as_download()],\n    arguments=[&quot;--bg_folder&quot;, bg_file_dw],\n    side_inputs=[bg_file_dw],\n    output=inference_frames_ds,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>What's happening here? Why the side argument seems to be available only in one worker while it fails in the others?<\/p>\n<p>BTW I found <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/957\" rel=\"nofollow noreferrer\">this<\/a> similar but unresolved question.<\/p>\n<p>Any help is much appreciated, thanks!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619386109403,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67258465",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":23.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureML ParallelRunStep runs only on one node",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":244.0,
        "Challenge_word_count":186,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343828614448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seville, Spain",
        "Poster_reputation_count":359.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>Apparently you need to specify a local mount path to use side_inputs in more than one node:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>bg_file_named = bg_file_ds.as_named_input(f&quot;bg&quot;)\nbg_file_mnt = bg_file_named.as_mount(f&quot;\/tmp\/{str(uuid.uuid4())}&quot;)\n\n...\n\nparallelrun_step = ParallelRunStep(\n    name='batch-inference',\n    parallel_run_config=parallel_run_config,\n    inputs=[frames_data_named.as_download()],\n    arguments=[&quot;--bg_folder&quot;, bg_file_mnt],\n    side_inputs=[bg_file_mnt],\n    output=inference_frames_ds,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>Sources:<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-parallel-run-step\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-parallel-run-step<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/18355\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/18355<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":31.7,
        "Solution_reading_time":13.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1629385138956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":395.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to <a href=\"https:\/\/spacy.io\/usage\/training\" rel=\"nofollow noreferrer\">train a spaCy model<\/a> , but turning the code into a Vertex AI Pipeline <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro?hl=en#:%7E:text=Step%201%3A%20Create%20a%20Python%20function%20based%20component\" rel=\"nofollow noreferrer\">Component<\/a>. My current code is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;train.yaml&quot;\n)\ndef train(train_name: str, dev_name: str) -&gt; NamedTuple(&quot;output&quot;, [(&quot;model_path&quot;, str)]):\n    &quot;&quot;&quot;\n    Trains a spacy model\n    \n    Parameters:\n    ----------\n    train_name : Name of the spaCy &quot;train&quot; set, used for model training.\n    dev_name: Name of the spaCy &quot;dev&quot; set, , used for model training.\n    \n    Returns:\n    -------\n    output : Destination path of the saved model.\n    &quot;&quot;&quot;\n    import spacy\n    import subprocess\n    \n    spacy.require_gpu()  # &lt;=== IMAGE FAILS TO BE COMPILED HERE\n    \n    # NOTE: The remaining code has already been tested and proven to be functional.\n    #       It has been edited since the project is private.\n    \n    # Presets for training\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;init&quot;, &quot;fill-config&quot;, &quot;gcs\/secret_path_to_config\/base_config.cfg&quot;, &quot;config.cfg&quot;])\n\n    # Training model\n    location = &quot;gcs\/secret_model_destination_path\/TestModel&quot;\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;train&quot;, &quot;config.cfg&quot;,\n                    &quot;--output&quot;, location,\n                    &quot;--paths.train&quot;, &quot;gcs\/secret_bucket\/secret_path\/{}.spacy&quot;.format(train_name),\n                    &quot;--paths.dev&quot;, &quot;gcs\/secret_bucket\/secret_path\/{}.spacy&quot;.format(dev_name),\n                    &quot;--gpu-id&quot;, &quot;0&quot;])\n    \n    return (location,)\n<\/code><\/pre>\n<p>The Vertex AI Logs display the following as main cause of the failure:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bRotZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bRotZ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The libraries are successfully installed, and yet I feel like there is some missing library \/ setting (as I know by <a href=\"https:\/\/dev.to\/davidgerva\/spacy-3-on-a-google-cloud-compute-instance-to-train-a-ner-transformer-model-23hf\" rel=\"nofollow noreferrer\">experience<\/a>); however I don't know  how to make it &quot;Python-based Vertex AI Components Compatible&quot;. BTW, the use of GPU is <strong>mandatory<\/strong> in my code.<\/p>\n<p>Any ideas?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650978304193,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1651090705967,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72014493",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":16.7,
        "Challenge_reading_time":37.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"Training spaCy model as a Vertex AI Pipeline \"Component\"",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":234.0,
        "Challenge_word_count":227,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1629385138956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":395.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>After some rehearsals, I think I have figured out what my code was missing. Actually, the <code>train<\/code> component definition was correct (with some minor tweaks relative to what was originally posted); however <strong>the pipeline was missing the GPU definition<\/strong>. I will first include a dummy example code, which trains a NER model using spaCy, and orchestrates everything via Vertex AI Pipeline:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output, OutputPath, InputPath\nfrom datetime import datetime\nfrom google.cloud import aiplatform\nfrom typing import NamedTuple\n\n\n# Component definition\n\n@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;generate.yaml&quot;\n)\ndef generate_spacy_file(train_path: OutputPath(), dev_path: OutputPath()):\n    &quot;&quot;&quot;\n    Generates a small, dummy 'train.spacy' &amp; 'dev.spacy' file\n    \n    Returns:\n    -------\n    train_path : Relative location in GCS, for the &quot;train.spacy&quot; file.\n    dev_path: Relative location in GCS, for the &quot;dev.spacy&quot; file.\n    &quot;&quot;&quot;\n    import spacy\n    from spacy.training import Example\n    from spacy.tokens import DocBin\n\n    td = [    # Train (dummy) dataset, in 'spacy V2 presentation'\n              (&quot;Walmart is a leading e-commerce company&quot;, {&quot;entities&quot;: [(0, 7, &quot;ORG&quot;)]}),\n              (&quot;I reached Chennai yesterday.&quot;, {&quot;entities&quot;: [(19, 28, &quot;GPE&quot;)]}),\n              (&quot;I recently ordered a book from Amazon&quot;, {&quot;entities&quot;: [(24,32, &quot;ORG&quot;)]}),\n              (&quot;I was driving a BMW&quot;, {&quot;entities&quot;: [(16,19, &quot;PRODUCT&quot;)]}),\n              (&quot;I ordered this from ShopClues&quot;, {&quot;entities&quot;: [(20,29, &quot;ORG&quot;)]}),\n              (&quot;Fridge can be ordered in Amazon &quot;, {&quot;entities&quot;: [(0,6, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a new Washer&quot;, {&quot;entities&quot;: [(16,22, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a old table&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a fancy dress&quot;, {&quot;entities&quot;: [(18,23, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a camera&quot;, {&quot;entities&quot;: [(12,18, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a tent for our trip&quot;, {&quot;entities&quot;: [(12,16, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a screwdriver from our neighbour&quot;, {&quot;entities&quot;: [(12,22, &quot;PRODUCT&quot;)]}),\n              (&quot;I repaired my computer&quot;, {&quot;entities&quot;: [(15,23, &quot;PRODUCT&quot;)]}),\n              (&quot;I got my clock fixed&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n              (&quot;I got my truck fixed&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n    ]\n    \n    dd = [    # Development (dummy) dataset (CV), in 'spacy V2 presentation'\n              (&quot;Flipkart started it's journey from zero&quot;, {&quot;entities&quot;: [(0,8, &quot;ORG&quot;)]}),\n              (&quot;I recently ordered from Max&quot;, {&quot;entities&quot;: [(24,27, &quot;ORG&quot;)]}),\n              (&quot;Flipkart is recognized as leader in market&quot;,{&quot;entities&quot;: [(0,8, &quot;ORG&quot;)]}),\n              (&quot;I recently ordered from Swiggy&quot;, {&quot;entities&quot;: [(24,29, &quot;ORG&quot;)]})\n    ]\n\n    \n    # Converting Train &amp; Development datasets, from 'spaCy V2' to 'spaCy V3'\n    nlp = spacy.blank(&quot;en&quot;)\n    db_train = DocBin()\n    db_dev = DocBin()\n\n    for text, annotations in td:\n        example = Example.from_dict(nlp.make_doc(text), annotations)\n        db_train.add(example.reference)\n        \n    for text, annotations in dd:\n        example = Example.from_dict(nlp.make_doc(text), annotations)\n        db_dev.add(example.reference)\n    \n    db_train.to_disk(train_path + &quot;.spacy&quot;)  # &lt;== Obtaining and storing &quot;train.spacy&quot;\n    db_dev.to_disk(dev_path + &quot;.spacy&quot;)      # &lt;== Obtaining and storing &quot;dev.spacy&quot;\n    \n\n# ----------------------- ORIGINALLY POSTED CODE -----------------------\n\n@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;train.yaml&quot;\n)\ndef train(train_path: InputPath(), dev_path: InputPath(), output_path: OutputPath()):\n    &quot;&quot;&quot;\n    Trains a spacy model\n    \n    Parameters:\n    ----------\n    train_path : Relative location in GCS, for the &quot;train.spacy&quot; file.\n    dev_path: Relative location in GCS, for the &quot;dev.spacy&quot; file.\n    \n    Returns:\n    -------\n    output : Destination path of the saved model.\n    &quot;&quot;&quot;\n    import spacy\n    import subprocess\n    \n    spacy.require_gpu()  # &lt;=== IMAGE NOW MANAGES TO GET BUILT!\n\n    # Presets for training\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;init&quot;, &quot;fill-config&quot;, &quot;gcs\/secret_path_to_config\/base_config.cfg&quot;, &quot;config.cfg&quot;])\n\n    # Training model\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;train&quot;, &quot;config.cfg&quot;,\n                    &quot;--output&quot;, output_path,\n                    &quot;--paths.train&quot;, &quot;{}.spacy&quot;.format(train_path),\n                    &quot;--paths.dev&quot;, &quot;{}.spacy&quot;.format(dev_path),\n                    &quot;--gpu-id&quot;, &quot;0&quot;])\n\n# ----------------------------------------------------------------------\n    \n\n# Pipeline definition\n\n@pipeline(\n    pipeline_root=PIPELINE_ROOT,\n    name=&quot;spacy-dummy-pipeline&quot;,\n)\ndef spacy_pipeline():\n    &quot;&quot;&quot;\n    Builds a custom pipeline\n    &quot;&quot;&quot;\n    # Generating dummy &quot;train.spacy&quot; + &quot;dev.spacy&quot;\n    train_dev_sets = generate_spacy_file()\n    # With the output of the previous component, train a spaCy modeL    \n    model = train(\n        train_dev_sets.outputs[&quot;train_path&quot;],\n        train_dev_sets.outputs[&quot;dev_path&quot;]\n    \n    # ------ !!! THIS SECTION DOES THE TRICK !!! ------\n    ).add_node_selector_constraint(\n        label_name=&quot;cloud.google.com\/gke-accelerator&quot;,\n        value=&quot;NVIDIA_TESLA_T4&quot;\n    ).set_gpu_limit(1).set_memory_limit('32G')\n    # -------------------------------------------------\n\n# Pipeline compilation   \n\ncompiler.Compiler().compile(\n    pipeline_func=spacy_pipeline, package_path=&quot;pipeline_spacy_job.json&quot;\n)\n\n\n# Pipeline run\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun = aiplatform.PipelineJob(  # Include your own naming here\n    display_name=&quot;spacy-dummy-pipeline&quot;,\n    template_path=&quot;pipeline_spacy_job.json&quot;,\n    job_id=&quot;ml-pipeline-spacydummy-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={},\n    enable_caching=True,\n)\n\n\n# Pipeline gets submitted\n\nrun.submit()\n<\/code><\/pre>\n<p>Now, the explanation; according to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/machine-types\" rel=\"nofollow noreferrer\">Google<\/a>:<\/p>\n<blockquote>\n<p>By default, the component will run on as a Vertex AI CustomJob using an e2-standard-4 machine, with 4 core CPUs and 16GB memory.<\/p>\n<\/blockquote>\n<p>Therefore, when the <code>train<\/code> component gets compiled, it fails as &quot;<em>it was not seeing any GPU available as resource<\/em>&quot;; in the same link however, all the available settings for both CPU and GPU are mentioned. In my case as you can see, I set <code>train<\/code> component to run under ONE (1) <code>NVIDIA_TESLA_T4<\/code> GPU card, and I also increased my CPU memory, to 32GB. With these modifications, the resulting pipeline looks as follows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s0I31.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s0I31.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>And as you can see, it gets compiled successfully, as well as trains (and eventually obtains) a functional spaCy model. From here, you can tweak this code, to fit your own needs.<\/p>\n<p>I hope this helps to anyone who might be interested.<\/p>\n<p>Thank you.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":15.7,
        "Solution_reading_time":104.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":50.0,
        "Solution_word_count":686.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hey there,  <\/p>\n<p>I was wondering, whether it is possible to connect your local computer as a compute target to the workspace and then access it as a compute target for AutoML and the Designer in the ML Studio (instead of a compute cluster)?  <br \/>\nI have read through the documentation and I feel like if this is possible, it is not very well-documented.  <\/p>\n<p>Thanks in advance!  <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624632132377,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/452250\/attaching-local-computer-to-ml-studio-and-use-it-w",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":5.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Attaching local computer to ML Studio and use it with Azure AutoML and Azure Designer",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":83,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. You can use <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-target#train\">local compute<\/a> for model training\/deployment including automl. However, you cannot attach it directly in Designer or ML Studio interface. You can only attach it from your local environment. Hope this helps!    <\/p>\n",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":4.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":41.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to deploy sklearn model in sagemaker. I created a training script.<\/p>\n\n<p>scripPath=' sklearn.py'<\/p>\n\n<p><code>sklearn=SKLearn(entry_point=scripPath,\n                                 train_instance_type='ml.m5.xlarge',\n                                   role=role,                  output_path='s3:\/\/{}\/{}\/output'.format(bucket,prefix), sagemaker_session=session)\nsklearn.fit({\"train-dir' : train_input})<\/code><\/p>\n\n<p>When I deploy it\n<code>predictor=sklearn.deploy(initial_count=1,instance_type='ml.m5.xlarge')<\/code><\/p>\n\n<p>It throws,\n<code>Clienterror: An error occured when calling the CreateModel operation:Could not find model data at s3:\/\/tree\/sklearn\/output\/model.tar.gz<\/code><\/p>\n\n<p>Can anyone say how to solve this issue?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1560943786943,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56666667",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":9.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Clienterror: An error occured when calling the CreateModel operation",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1033.0,
        "Challenge_word_count":60,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1560085651596,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":155.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>When deploying models, SageMaker looks up S3 to find your trained model artifact. It seems that there is no trained model artifact at <code>s3:\/\/tree\/sklearn\/output\/model.tar.gz<\/code>. Make sure to persist your model artifact in your training script at the appropriate local location in docker which is <code>\/opt\/ml\/model<\/code>.\nfor example, in your training script this could look like:<\/p>\n\n<pre><code>joblib.dump(model, \/opt\/ml\/model\/mymodel.joblib)\n<\/code><\/pre>\n\n<p>After training, SageMaker will copy the content of <code>\/opt\/ml\/model<\/code> to s3 at the <code>output_path<\/code> location.<\/p>\n\n<p>If you deploy in the same session a <code>model.deploy()<\/code> will map automatically to the artifact path. If you want to deploy a model that you trained elsewhere, possibly during a different session or in a different hardware, you need to explicitly instantiate a model before deploying<\/p>\n\n<pre><code>from sagemaker.sklearn.model import SKLearnModel\n\nmodel = SKLearnModel(\n    model_data='s3:\/\/...model.tar.gz',  # your artifact\n    role=get_execution_role(),\n    entry_point='script.py')  # script containing inference functions\n\nmodel.deploy(\n    instance_type='ml.m5.xlarge',\n    initial_instance_count=1,\n    endpoint_name='your_endpoint_name')\n<\/code><\/pre>\n\n<p>See more about Sklearn in SageMaker here <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.7,
        "Solution_reading_time":19.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":150.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1468179475927,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation_count":795.0,
        "Answerer_view_count":210.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to test Azure Machine Learning Studio. <\/p>\n\n<p>I want to use TensorFlow, but it is not installed on Jupyter notebook.<\/p>\n\n<p>How can I use some machine learning libraries like TensorFlow, Theano, Keras,... on the notebook?<\/p>\n\n<p>I tried this:<\/p>\n\n<pre><code>!pip install tensorflow \n<\/code><\/pre>\n\n<p>But, I got error as below:<\/p>\n\n<pre><code>Collecting tensorflow\n  Downloading tensorflow-0.12.0rc0-cp34-cp34m-manylinux1_x86_64.whl (43.1MB)\n    100% |################################| 43.1MB 27kB\/s \nCollecting protobuf==3.1.0 (from tensorflow)\n  Downloading protobuf-3.1.0-py2.py3-none-any.whl (339kB)\n    100% |################################| 348kB 3.7MB\/s \nCollecting six&gt;=1.10.0 (from tensorflow)\n  Downloading six-1.10.0-py2.py3-none-any.whl\nRequirement already satisfied: numpy&gt;=1.11.0 in \/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages (from tensorflow)\nRequirement already satisfied: wheel&gt;=0.26 in \/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages (from tensorflow)\nRequirement already satisfied: setuptools in \/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/setuptools-27.2.0-py3.4.egg (from protobuf==3.1.0-&gt;tensorflow)\nInstalling collected packages: six, protobuf, tensorflow\n  Found existing installation: six 1.9.0\n    DEPRECATION: Uninstalling a distutils installed project (six) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n    Uninstalling six-1.9.0:\n      Successfully uninstalled six-1.9.0\n  Rolling back uninstall of six\nException:\nTraceback (most recent call last):\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/basecommand.py\", line 215, in main\n    status = self.run(options, args)\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/commands\/install.py\", line 342, in run\n    prefix=options.prefix_path,\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/req\/req_set.py\", line 784, in install\n    **kwargs\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/req\/req_install.py\", line 851, in install\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/req\/req_install.py\", line 1064, in move_wheel_files\n    isolated=self.isolated,\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/wheel.py\", line 345, in move_wheel_files\n    clobber(source, lib_dir, True)\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/wheel.py\", line 329, in clobber\n    os.utime(destfile, (st.st_atime, st.st_mtime))\nPermissionError: [Errno 1] Operation not permitted\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1481173386563,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41032108",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":36.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":null,
        "Challenge_title":"How to install TensorFlow in jupyter notebook on Azure Machine Learning Studio",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":2886.0,
        "Challenge_word_count":235,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1279636903496,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":1540.0,
        "Poster_view_count":404.0,
        "Solution_body":"<p>As you noticed, the active user doesn't have permissions to write to the <code>site-packages<\/code> directory in Azure Machine Learning Studio notebooks. You could try installing the package to another directory where you do have write permissions (like the default working directory) and importing from there, but I recommend the following lower-hassle option.<\/p>\n\n<p><a href=\"https:\/\/notebooks.azure.com\" rel=\"nofollow noreferrer\">Azure Notebooks<\/a> is a separate Jupyter Notebook service that will allow you to install tensorflow, theano, and keras. Like the notebooks in AML Studio, these notebooks will persist in your account. The primary downside is that if you want to access your workspace through e.g. the Python <code>azureml<\/code> package, you'll need to <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\" rel=\"nofollow noreferrer\">provide your workspace id\/authorization token<\/a> to set up the connection. (In Azure ML Studio, those values are loaded automatically from the current workspace.) Otherwise I believe Azure Notebooks can do everything you are used to doing inside AML Studio only.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.3,
        "Solution_reading_time":14.6,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":150.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"## What\r\n\r\nWhen loading configs from wandb, the resulting HParams objects are not correct. This can be seen when attempting to load the model checkpoint with the given parameters (failure), or when comparing the object with the info panel for the run on wandb.\r\n\r\n## How to Reproduce\r\n\r\nLoad the configs:\r\n\r\n```python\r\nfrom wavenet import utils, model, train\r\n\r\nrun_path = 'purzelrakete\/feldberlin-wavenet\/21ei0tqc'\r\np, ptrain = utils.load_wandb_cfg(run_path)\r\np, ptrain = model.HParams(**p), train.HParams(**ptrain)\r\n```\r\n\r\nValidate against the run [on wandb](https:\/\/wandb.ai\/purzelrakete\/feldberlin-wavenet\/runs\/21ei0tqc\/overview?workspace=user-purzelrakete)\r\n\r\n## Acceptance Criteria\r\n\r\n- [x] Bug has been understood and fixed\r\n- [x] The same config given above can be loaded and is correct",
        "Challenge_closed_time":1624287.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624097966000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/feldberlin\/wavenet\/issues\/5",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":10.49,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":35.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Loading configs from wandb yields incorrect parameters",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":99,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"I'm not sure if I'm doing something wrong, I'm using mlflow instead of tensorboard as a logger. I've used the defaults i.e.\r\n\r\n```\r\nmlflow = loggers.MLFlowLogger()\r\ntrainer = pl.Trainer.from_argparse_args(args, logger=mlflow)\r\n```\r\n\r\nI'm ending up with the following folder structure\r\n\r\n\\mlflow\r\n\\mlflow\\1\r\n\\mlflow\\1\\\\{guid}\\artifacts\r\n\\mlflow\\1\\\\{guid}\\metrics\r\n\\mlflow\\1\\\\{guid}\\params\r\n\\mlflow\\1\\\\{guid}\\meta.yaml\r\n**\\1\\\\{guid}\\checkpoints**\r\n\r\ni.e. the checkpoints are in the wrong location, they should be in the `\\mlflow` folder. \r\n\r\nPerhaps this is an mlflow rather than pytorch-lightning issue? \r\n\r\nI'm using pytorch-lightning 0.8.5 on macos running in python 3.7.6\r\n",
        "Challenge_closed_time":1597488.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597273128000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/2939",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.5,
        "Challenge_reading_time":8.83,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"mlflow checkpoints in the wrong location ",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":82,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@david-waterworth mind try the latest 0.9rc12? It was fixed here: #2502 \r\nThe checkpoints subfolder will go here: `mlflow\\1{guid}\\checkpoints`, is that what you want @david-waterworth ?\r\n Thanks @awaelchli  yes that's what I want - thanks!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":2.95,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":32.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"if in setup log_plot set True then it is giving error in self._mlflow_log_model() as \r\nfor plot in log_plots:\r\nTypeError: 'bool' object is not iterable",
        "Challenge_closed_time":1635811.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634814038000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1736",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":7.1,
        "Challenge_reading_time":2.45,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Issue with Mlflow Timeseries_beta branch",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":29,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm moving my first steps in <code>amazon sagemaker<\/code>. I'm using script mode to train a classification algorithm. Training is fine, however I'm not able to do incremental training. I want to train again the same model with new data. Here what I did. This is my script:<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker.tensorflow import TensorFlow\nfrom sagemaker import get_execution_role\n\nbucket = 'sagemaker-blablabla'\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\n\ns3_output_location = 's3:\/\/{}'.format(bucket)\n\ntf_estimator = TensorFlow(entry_point='main.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          framework_version='1.12', \n                          py_version='py3',\n                          output_path=s3_output_location)\n\ninputs = {'train': train_data, 'test': validation_data}\ntf_estimator.fit(inputs)\n<\/code><\/pre>\n\n<p>The entry point is my custom keras code, which I adapted to receive arguments from the script.\nNow the training is successfully completed and I have in my s3 bucket the model.tar.gz. I want to train again, but it's not clear to me how to do it.. I tried this<\/p>\n\n<pre><code>trained_model = 's3:\/\/sagemaker-blablabla\/sagemaker-tensorflow-scriptmode-2019-11-27-12-01-42-300\/output\/model.tar.gz'\n\ntf_estimator = sagemaker.estimator.Estimator(image_name='blablabla-west-1.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.12-gpu-py3', \n                                              role=get_execution_role(),\n                                              train_instance_count=1, \n                                              train_instance_type='ml.p2.xlarge',\n                                              output_path=s3_output_location,\n                                              model_uri = trained_model)\n\ninputs = {'train': train_data, 'test': validation_data}\n\ntf_estimator.fit(inputs)\n<\/code><\/pre>\n\n<p>Doesn't work. Firstly, I don't know how to retrieve the training image name (for this I looked for it in the <code>aws<\/code> console, but I guess there should be a smarter solution), second this code throws an exception about the entry point but it is my understanding that I shouldn't need it when I do incremental learning with a ready image.\nI'm surely missing something important, any help? Thank you!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574941643827,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1574944370140,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59088199",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":27.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":null,
        "Challenge_title":"incremental training on custom code in amazon sagemaker",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":621.0,
        "Challenge_word_count":224,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416346350292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation_count":2302.0,
        "Poster_view_count":227.0,
        "Solution_body":"<p>Incremental training is a native feature for the built-in <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/now-easily-perform-incremental-learning-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">Image Classifier and Object Detector<\/a>. For custom code, it is the developer responsibility to write the incremental training logic and to verify its validity. Here is a possible path:<\/p>\n\n<ol>\n<li>use one of the data channels passed in the <code>fit<\/code> to load a model state (artifact to fine-tune)<\/li>\n<li>in your code, check if the model state channel is filled\nwith artifacts. If it is, instantiate a model from that state\nand continue training. This is framework specific and you may to take\nnecessary precautions to avoid forgetting previous learnings.<\/li>\n<\/ol>\n\n<p>Some frameworks provide better support for incremental learning that others. For example some sklearn models provide an <a href=\"https:\/\/scikit-learn.org\/0.15\/modules\/scaling_strategies.html#incremental-learning\" rel=\"nofollow noreferrer\">incremental_fit<\/a> method. For DL frameworks it is technically very easy to continue training from a checkpoint, but if new data is very different from previously-seen data this may lead your model to forget previous learnings.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.1,
        "Solution_reading_time":16.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":157.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1365624651363,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":335.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have created two models in azure ml studio and i want to download those models.<\/p>\n\n<p>Is it possible to download train and score models from azure ml studio?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1482218722783,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41236871",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":5.9,
        "Challenge_reading_time":2.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How to download the trained models from Azure machine studio?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":7873.0,
        "Challenge_word_count":38,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1482218454343,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":73.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Models can be trained, scored, saved, and run in AzureML studio, but can't downloaded to your local machine. There's no way to do anything with a model outside of AzureML.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":2.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to read a json file from S3 into a sagemaker notebook.<\/p>\n\n<p>I can do this with pandas with this code, and this works without error :<\/p>\n\n<pre><code>import json\nimport pandas as pd\nimport boto3\n\n\nprefix_source = 'folder'\n\ns3 = boto3.resource('s3')\nmy_bucket_source = s3.Bucket('bucket_source')\n\nfor obj in my_bucket_source.objects.filter(Prefix=prefix_source):\n        data_location = 's3:\/\/{}\/{}'.format(obj.bucket_name, obj.key)\n        data = pd.read_json(data_location, lines = True )\n        display(data.head())\n<\/code><\/pre>\n\n<p>but I don't want to use pandas, I want to use Python <\/p>\n\n<p>I tried this code<\/p>\n\n<pre><code>for obj in my_bucket_source.objects.filter(Prefix=prefix_source):\n        data_location = 's3:\/\/{}\/{}'.format(obj.bucket_name, obj.key)\n        with open(data_location, 'r') as f:\n            array = json.load(f)\n            display(array) \n<\/code><\/pre>\n\n<p>I got this error :<\/p>\n\n<p><strong>IOError: [Errno 2] No such file or directory<\/strong><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1555517547827,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55731954",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":12.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"read json file with Python from S3 into sagemaker notebook",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":7094.0,
        "Challenge_word_count":110,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1445771281667,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":737.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>Json.load() expect a local file system path \"\/...\", not an \"s3:\/\/\" URI.<br>\nSee answer here: <a href=\"https:\/\/stackoverflow.com\/a\/47121263\">https:\/\/stackoverflow.com\/a\/47121263<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.9,
        "Solution_reading_time":2.51,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":16.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <br \/>\nI am using the example provided in the Machine Learning Studio Docs for extracting Health Entities from a given string.  <br \/>\nThe code is shown below.  <\/p>\n<p>My question is: what is the easiest way to <strong>convert the output result into JSON format<\/strong>?  <\/p>\n<pre><code>from azure.core.credentials import AzureKeyCredential\nfrom azure.ai.textanalytics import TextAnalyticsClient\nimport json\n\ncredential = AzureKeyCredential(&quot;**********************************&quot;)\nendpoint=&quot;https:\/\/eastus.api.cognitive.microsoft.com\/&quot;\n\ntext_analytics_client = TextAnalyticsClient(endpoint, credential)\n\ndocuments = [&quot;Subject is taking 100mg of ibuprofen twice daily&quot;]\n\npoller = text_analytics_client.begin_analyze_healthcare_entities(documents)\nresult = poller.result()\n\ndocs = [doc for doc in result if not doc.is_error]\n\nprint(&quot;Results of Healthcare Entities Analysis:&quot;)\nfor idx, doc in enumerate(docs):\n    for entity in doc.entities:\n        print(&quot;Entity: {}&quot;.format(entity.text))\n        print(&quot;...Normalized Text: {}&quot;.format(entity.normalized_text))\n        print(&quot;...Category: {}&quot;.format(entity.category))\n        print(&quot;...Subcategory: {}&quot;.format(entity.subcategory))\n        print(&quot;...Offset: {}&quot;.format(entity.offset))\n        print(&quot;...Confidence score: {}&quot;.format(entity.confidence_score))\n        if entity.data_sources is not None:\n            print(&quot;...Data Sources:&quot;)\n            for data_source in entity.data_sources:\n                print(&quot;......Entity ID: {}&quot;.format(data_source.entity_id))\n                print(&quot;......Name: {}&quot;.format(data_source.name))\n        if entity.assertion is not None:\n            print(&quot;...Assertion:&quot;)\n            print(&quot;......Conditionality: {}&quot;.format(entity.assertion.conditionality))\n            print(&quot;......Certainty: {}&quot;.format(entity.assertion.certainty))\n            print(&quot;......Association: {}&quot;.format(entity.assertion.association))\n        for relation in doc.entity_relations:\n            print(&quot;Relation of type: {} has the following roles&quot;.format(relation.relation_type))\n        for role in relation.roles:\n            print(&quot;...Role '{}' with entity '{}'&quot;.format(role.name, role.entity.text))\n    print(&quot;------------------------------------------&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1650967532993,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/826603\/converting-textanalytics-result-to-json-format",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":21.2,
        "Challenge_reading_time":30.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":null,
        "Challenge_title":"Converting textanalytics result to JSON Format",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":167,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=c7e28bb4-3bff-4f66-9bdf-9c63a80436b1\">@KA  <\/a> The result does not seem to be directly serializable to JSON. I found a library <a href=\"https:\/\/pypi.org\/project\/jsons\/\">JSONS<\/a> that can do the heavy lifting if you are using python 3.5 or higher.     <\/p>\n<p>Install jsons    <\/p>\n<pre><code>pip install jsons  \n<\/code><\/pre>\n<p>Import JSONS and using jsons.dump() on docs object.    <\/p>\n<pre><code>import jsons #import in the import section  \nprint(jsons.dump(docs)) #Printing the json after docs is created  \n<\/code><\/pre>\n<p>This should give a file of this format in this case. Uploaded the file in .txt format since JSON files cannot be uploaded on Q&amp;A, download the file and rename it to .json     <br \/>\nI hope this helps!!    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n<p><a href=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/196624-health.txt?platform=QnA\">196624-health.txt<\/a>    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":10.3,
        "Solution_reading_time":16.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":135.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi, I am training my models via Azure Machine Learning.<\/p>\n<p>On other day, my training is running with GPU support, however today I found my training is running on a CPU.  <br \/>\nI'm not modified training environment, only training script was modified.  <br \/>\nMy computing cluster is NC6v3 - have a GPU.<\/p>\n<p>I investigate a situation, and I found training script is running on PyTorch 1.6.0.  <br \/>\nOn other day, it ran on Pytorch 1.8.1.  <br \/>\nI think my &quot;don't use GPU&quot; problem is caused by the situation that CUDA toolkit version is not suitable for Pytorch version.<\/p>\n<p>Then, I output a installed package to the log.  <br \/>\nThe log says 'Pytorch 1.8.1 was installed, however uses 1.6.0'.  <br \/>\nI confused by this weird circumstances.  <br \/>\nCan someone tell me the solution?<\/p>\n<p>&lt;My code snippet&gt;  <br \/>\n&lt;&lt;conda_dependencies.yaml&gt;&gt;<\/p>\n<p>channels:  <\/p>\n<ul>\n<li> conda-forge  <\/li>\n<li> pytorch  <\/li>\n<li> nvidia  <br \/>\ndependencies:  <\/li>\n<li> python=3.8.10  <\/li>\n<li> mesa-libgl-cos6-x86_64  <\/li>\n<li> cudatoolkit=11.1  <\/li>\n<li> pytorch==1.8.1  <\/li>\n<li> torchvision==0.9.1  <\/li>\n<li> tqdm  <\/li>\n<li> scikit-learn  <\/li>\n<li> matplotlib  <\/li>\n<li> pandas  <\/li>\n<li> pip &lt; 20.3  <\/li>\n<li> pip:  <\/li>\n<li> azureml-defaults  <\/li>\n<li> opencv-python-headless  <\/li>\n<li> pillow==8.2.0<\/li>\n<\/ul>\n<p>&lt;&lt;Environment definition&gt;&gt;  <br \/>\nenvironment_definition_file = experiment_dir \/ 'conda_dependencies.yaml'  <br \/>\nenvironment_name = 'pytorch-1.8.1-gpu'  <br \/>\nbase_image_name = 'mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04'  <br \/>\nenvironment = Environment.from_docker_image(environment_name, base_image_name, conda_specification = environment_definition_file)  <br \/>\ndocker_run_config = DockerConfiguration(use_docker=True)<\/p>\n<p>script_run_config = ScriptRunConfig(  <br \/>\nsource_directory = experiment_dir,  <br \/>\nscript = SCRIPT_FILE_NAME,  <br \/>\narguments = arguments,  <br \/>\ncompute_target = compute_target,  <br \/>\ndocker_runtime_config = docker_run_config,  <br \/>\nenvironment = environment)<\/p>\n<p>&lt;&lt;Output a log in the training script&gt;&gt;  <br \/>\nimport torch  <br \/>\nimport pip<\/p>\n<p>pip.main(['list'])  <br \/>\nprint(f'PyTorch version: {torch.<strong>version<\/strong>}')<\/p>\n<p>&lt;My logs&gt;  <br \/>\nPackage Version<\/p>\n<hr \/>\n<p>adal 1.2.7  <br \/>\napplicationinsights 0.11.10  <br \/>\n(omission)  <br \/>\ntorch 1.8.1  <br \/>\ntorchvision 0.9.0a0  <br \/>\n(omission)<\/p>\n<p>PyTorch version: 1.6.0<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629119160593,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/515579\/azure-machine-learning-uses-invalid-pytorch-versio",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":32.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning - Uses invalid Pytorch version when training",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":285,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. These are the <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.pytorch?view=azure-ml-py\">supported versions<\/a> for PyTorch. Please refer to this document for creating a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-pytorch#create-a-custom-environment\">custom environment<\/a>. As shown, you'll need to use versions &lt;= 1.6.0. Hope this helps.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.0,
        "Solution_reading_time":6.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":37.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1393579668636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":1450.0,
        "Answerer_view_count":162.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to understand what is the optimal way in Kedro to convert Spark dataframe coming out of one node into Pandas required as input for another node without creating a redundant conversion step.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573500781437,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1573553002123,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58807540",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"How to convert Spark data frame to Pandas and back in Kedro?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":800.0,
        "Challenge_word_count":45,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1393579668636,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":1450.0,
        "Poster_view_count":162.0,
        "Solution_body":"<p>Kedro currently supports 2 strategies for that:<\/p>\n\n<h3>Using <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/04_data_catalog.html#transcoding-datasets\" rel=\"nofollow noreferrer\">Transcoding<\/a> feature<\/h3>\n\n<p>This requires one to define two <code>DataCatalog<\/code> entries for the same dataset, working with the same file in a common format (Parquet, JSON, CSV, etc.), in your <code>catalog.yml<\/code>:<\/p>\n\n<pre><code>my_dataframe@spark:\n  type: kedro.contrib.io.pyspark.SparkDataSet\n  filepath: data\/02_intermediate\/data.parquet\n\nmy_dataframe@pandas:\n  type: ParquetLocalDataSet\n  filepath: data\/02_intermediate\/data.parquet\n<\/code><\/pre>\n\n<p>And then use them in the pipeline like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>Pipeline([\n    node(my_func1, \"spark_input\", \"my_dataframe@spark\"),\n    node(my_func2, \"my_dataframe@pandas\", \"output\"),\n])\n<\/code><\/pre>\n\n<p>In this case, <code>kedro<\/code> understands that <code>my_dataframe<\/code> is the same dataset in both cases and resolves the node execution order properly. At the same time, <code>kedro<\/code> would use the <code>SparkDataSet<\/code> implementation for saving and <code>ParquetLocalDataSet<\/code> for loading, so the first node should output <code>pyspark.sql.DataFrame<\/code>, while the second node would receive a <code>pandas.Dataframe<\/code>.<\/p>\n\n<h3>Using <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/kedro.contrib.decorators.pandas_to_spark.html\" rel=\"nofollow noreferrer\">Pandas to Spark<\/a> and <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/kedro.contrib.decorators.spark_to_pandas.html\" rel=\"nofollow noreferrer\">Spark to Pandas<\/a> node decorators<\/h3>\n\n<p><strong>Note:<\/strong> <code>Spark &lt;-&gt; Pandas<\/code> in-memory conversion is <a href=\"https:\/\/stackoverflow.com\/a\/47536675\/3364156\">notorious<\/a> for its memory demands, so this is a viable option only if the dataframe is known to be small.<\/p>\n\n<p>One can decorate the node as per the docs:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from spark import get_spark\nfrom kedro.contrib.decorators import pandas_to_spark\n\n@pandas_to_spark(spark_session)\ndef my_func3(data):\n    data.show() # data is pyspark.sql.DataFrame\n<\/code><\/pre>\n\n<p>Or even the whole pipeline:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>Pipeline([\n    node(my_func4, \"pandas_input\", \"some_output\"),\n    ...\n]).decorate(pandas_to_spark)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1573501243163,
        "Solution_link_count":4.0,
        "Solution_readability":16.3,
        "Solution_reading_time":31.94,
        "Solution_score_count":3.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":207.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\n\nI've just trained a churn prediction model with XGBoost algorithm, based on the SageMaker example notebooks.  I've created SageMaker batch transformation jobs using this model using input from CSV file with multiple records, however the output file is a single record CSV containing all the inferences in a single comma separated row.  The result is that I'm not able to use the \"Join source\" feature with \"Input  - Merge input data with job output\" since the input and output files must match the number of records. I've tried with different batch job configurations but I always get the same single line output file.\n\nDo you know if is there any configuration that allows me to merge input and output in order to have a direct association between an input column with its inference result? Is this a restriction from the XGBoost algorithm built-in implementation?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599771185000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668594259996,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUYz7Bz_5sTmG0uBaqlt7J_g\/xgboost-sagemaker-batch-transform-job-output-in-multiple-lines",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":11.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"xgboost sagemaker batch transform job output in multiple lines",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":410.0,
        "Challenge_word_count":152,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Sounds like a configuration issue, this algorithm should be able to output proper output CSVs.\n\nAre you using `accept=\"text\/csv\"` and `assemble_with=\"Line\"` on your `Transformer`? Is your `strategy` set to `SingleRecord` or `MultiRecord`?\n\nAnd `split_type=\"Line\"`, `content_type=\"text\/csv\"` on the `.transform()` call?\n\nI have had custom algorithms accidentally output row vectors instead of column vectors for multi-record batches in the past (because they gave a 1D output which the default serializer interpreted as a row), but not built-in algorithms.\n\nDropping to `SingleRecord` could be a last resort (forcing Batch Transform itself to handle the serialization), but would decrease efficiency\/speed.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1607690229838,
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":8.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":96.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run tensorboard from my Jupyter notebook in Sagemaker. The below is my code:<\/p>\n<pre><code>import tensorflow as tf\nimport datetime, os\n\n%load_ext tensorboard\nlogs_base_dir = &quot;.\/logs&quot;\nos.makedirs(logs_base_dir)\n\n!tensorboard --logdir=data\/ --host localhost --port=8080\n<\/code><\/pre>\n<p>The output I get looks fine:\n<code>TensorBoard 1.14.0 at http:\/\/localhost:8080\/ (Press CTRL+C to quit)<\/code>\nbut when I click on the link, I'm taken to a page with ERR_CONNECTION_REFUSED.<\/p>\n<p>Does anyone have suggestions about what to try next? Thanks so much!<\/p>\n<p>Tensorflow: 1.14\nPython: 2<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1623953190457,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1623996319287,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68024476",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":7.1,
        "Challenge_reading_time":8.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Running Tensorboard in Jupyter in Sagemaker: this site can't be reached",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":286.0,
        "Challenge_word_count":88,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340392287000,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1843.0,
        "Poster_view_count":87.0,
        "Solution_body":"<p>Based on the comments it seems to me that you are trying to open the wrong URL. If I understand you question, you are not running in a local environment, so you can not open <code>localhost<\/code>. The right URL for <code>sagemaker<\/code> from the docs is <code>https:\/\/&lt;notebook instance hostname&gt;\/proxy\/6006\/<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.5,
        "Solution_reading_time":4.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":48.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1365101584443,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Munich, Germany",
        "Answerer_reputation_count":7203.0,
        "Answerer_view_count":445.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created a stepfunction, the definition for this statemachine below (<code>step-function.json<\/code>) is used in terraform (using the syntax in this page:<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html<\/a>)<\/p>\n<p>The first time if I execute this statemachine, it will create a SageMaker batch transform job named <code>example-jobname<\/code>, but I need to exeucute this statemachine everyday, then it will give me error <code>&quot;error&quot;: &quot;SageMaker.ResourceInUseException&quot;, &quot;cause&quot;: &quot;Job name must be unique within an AWS account and region, and a job with this name already exists <\/code>.<\/p>\n<p>The cause is because the job name is hard-coded as <code>example-jobname<\/code> so if the state machine gets executed after the first time, since the job name needs to be unique, the task will fail, just wondering how I can add a string (something like ExecutionId at the end of the job name). Here's what I have tried:<\/p>\n<ol>\n<li><p>I added <code>&quot;executionId.$&quot;: &quot;States.Format('somestring {}', $$.Execution.Id)&quot;<\/code> in the <code>Parameters<\/code> section in the json file, but when I execute the task I got error <code> &quot;error&quot;: &quot;States.Runtime&quot;, &quot;cause&quot;: &quot;An error occurred while executing the state 'SageMaker CreateTransformJob' (entered at the event id #2). The Parameters '{\\&quot;BatchStrategy\\&quot;:\\&quot;SingleRecord\\&quot;,..............\\&quot;executionId\\&quot;:\\&quot;somestring arn:aws:states:us-east-1:xxxxx:execution:xxxxx-state-machine:xxxxxxxx72950\\&quot;}' could not be used to start the Task: [The field \\&quot;executionId\\&quot; is not supported by Step Functions]&quot;}<\/code><\/p>\n<\/li>\n<li><p>I modified the jobname in the json file to  <code>&quot;TransformJobName&quot;: &quot;example-jobname-States.Format('somestring {}', $$.Execution.Id)&quot;,<\/code>, when I execute the statemachine, it gave me error: <code>&quot;error&quot;: &quot;SageMaker.AmazonSageMakerException&quot;, &quot;cause&quot;: &quot;2 validation errors detected: Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}; Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must have length less than or equal to 63<\/code><\/p>\n<\/li>\n<\/ol>\n<p>I really run out of ideas, can someone help please? Many thanks.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610635167150,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1611071837132,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65721061",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":16.2,
        "Challenge_reading_time":36.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"How to parse stepfunction executionId to SageMaker batch transform job name?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1209.0,
        "Challenge_word_count":288,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>So as per the <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/sample-train-model.html#sample-train-model-code-examples\" rel=\"nofollow noreferrer\">documentation<\/a>, we should be passing the parameters in the following format<\/p>\n<pre><code>        &quot;Parameters&quot;: {\n            &quot;ModelName.$&quot;: &quot;$$.Execution.Name&quot;,  \n            ....\n        },\n<\/code><\/pre>\n<p>If you take a close look this is something missing from your definition, So your step function definition should be something like below:<\/p>\n<p>either<\/p>\n<pre><code>      &quot;TransformJobName.$&quot;: &quot;$$.Execution.Id&quot;,\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code>      &quot;TransformJobName.$: &quot;States.Format('mytransformjob{}', $$.Execution.Id)&quot;\n<\/code><\/pre>\n<p>full State machine definition:<\/p>\n<pre><code>    {\n        &quot;Comment&quot;: &quot;Defines the statemachine.&quot;,\n        &quot;StartAt&quot;: &quot;Generate Random String&quot;,\n        &quot;States&quot;: {\n            &quot;Generate Random String&quot;: {\n                &quot;Type&quot;: &quot;Task&quot;,\n                &quot;Resource&quot;: &quot;arn:aws:lambda:eu-central-1:1234567890:function:randomstring&quot;,\n                &quot;ResultPath&quot;: &quot;$.executionid&quot;,\n                &quot;Parameters&quot;: {\n                &quot;executionId.$&quot;: &quot;$$.Execution.Id&quot;\n                },\n                &quot;Next&quot;: &quot;SageMaker CreateTransformJob&quot;\n            },\n        &quot;SageMaker CreateTransformJob&quot;: {\n            &quot;Type&quot;: &quot;Task&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:states:::sagemaker:createTransformJob.sync&quot;,\n            &quot;Parameters&quot;: {\n            &quot;BatchStrategy&quot;: &quot;SingleRecord&quot;,\n            &quot;DataProcessing&quot;: {\n                &quot;InputFilter&quot;: &quot;$&quot;,\n                &quot;JoinSource&quot;: &quot;Input&quot;,\n                &quot;OutputFilter&quot;: &quot;xxx&quot;\n            },\n            &quot;Environment&quot;: {\n                &quot;SAGEMAKER_MODEL_SERVER_TIMEOUT&quot;: &quot;300&quot;\n            },\n            &quot;MaxConcurrentTransforms&quot;: 100,\n            &quot;MaxPayloadInMB&quot;: 1,\n            &quot;ModelName&quot;: &quot;${model_name}&quot;,\n            &quot;TransformInput&quot;: {\n                &quot;DataSource&quot;: {\n                    &quot;S3DataSource&quot;: {\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3Uri&quot;: &quot;${s3_input_path}&quot;\n                    }\n                },\n                &quot;ContentType&quot;: &quot;application\/jsonlines&quot;,\n                &quot;CompressionType&quot;: &quot;Gzip&quot;,\n                &quot;SplitType&quot;: &quot;Line&quot;\n            },\n            &quot;TransformJobName.$&quot;: &quot;$.executionid&quot;,\n            &quot;TransformOutput&quot;: {\n                &quot;S3OutputPath&quot;: &quot;${s3_output_path}&quot;,\n                &quot;Accept&quot;: &quot;application\/jsonlines&quot;,\n                &quot;AssembleWith&quot;: &quot;Line&quot;\n            },    \n            &quot;TransformResources&quot;: {\n                &quot;InstanceType&quot;: &quot;xxx&quot;,\n                &quot;InstanceCount&quot;: 1\n            }\n        },\n            &quot;End&quot;: true\n        }\n        }\n    }\n<\/code><\/pre>\n<p>In the above definition the lambda could be a function which parses the execution id arn which I am passing via the parameters section:<\/p>\n<pre><code> def lambda_handler(event, context):\n    return(event.get('executionId').split(':')[-1])\n<\/code><\/pre>\n<p>Or if you dont wanna pass the execution id , it can simply return the random string like<\/p>\n<pre><code> import string\n def lambda_handler(event, context):\n    return(string.ascii_uppercase + string.digits)\n<\/code><\/pre>\n<p>you can generate all kinds of random string or do generate anything in the lambda and pass that to the transform job name.<\/p>",
        "Solution_comment_count":23.0,
        "Solution_last_edit_time":1610641519768,
        "Solution_link_count":1.0,
        "Solution_readability":24.6,
        "Solution_reading_time":43.95,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":220.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi Google Community,\n\nI was wondering, has anyone been able to successfully train and deploy a custom trained scikit-learn classification model and deploy it to a vertex endpoint with the feature attribution through the explain endpoint working?\n\nEvery time i define my instances, predictions and explanation_spec while uploading my model, i get errors on the endpoint for the :explain method. Specifically, i get '400 bad request' with no information on why it was a bad request.\n\nI am using the v1beta1 ai platform python SDK and also am using a custom basic serving container. The custom container works for :predict but :explain does not work. Is there some example code out there? Is\u00a0scikit-learn not supported for feature attribution?\u00a0\n\nThanks! Ryan",
        "Challenge_closed_time":1657817.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656581520000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-explain-with-a-custom-trained-scikit-learn\/m-p\/436711#M397",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.6,
        "Challenge_reading_time":10.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Vertex AI explain with a custom trained scikit-learn classification model",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":431.0,
        "Challenge_word_count":130,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"For anyone looking back on this, i was able to use the following notebook to solve my problem. It seems we need to use encoding BAG_OF_FEATURES. I am not to sure why this is required, but it seems to have done the trick for me.\n\nhttps:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/ml_ops\/stage4...\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.3,
        "Solution_reading_time":4.58,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":51.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\n*Description:*\r\n\r\nAn apt-get error is seen in `sagemaker-local-test` builds as below. This is because `apt-get` process is already running and in active state.\r\n\r\n```\r\nE: Could not get lock \/var\/lib\/dpkg\/lock-frontend - open (11: Resource temporarily unavailable)\r\n--\r\n294 | E: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), is another process using it?\r\n```\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1598551.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597888000000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/deep-learning-containers\/issues\/517",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.0,
        "Challenge_reading_time":5.41,
        "Challenge_repo_contributor_count":100.0,
        "Challenge_repo_fork_count":316.0,
        "Challenge_repo_issue_count":2511.0,
        "Challenge_repo_star_count":579.0,
        "Challenge_repo_watch_count":38.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"[bug] apt-get failure in sagemaker-local-test builds",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a pytorch model that i have tested as a real-time endpoint in sagemaker, now i want to test it with batch inference. I am using jsonl data, and setting up a batch transform job as documented in aws documentation, in addition, i'm using my own inference.py (see sample below). I'm getting a json decode error inside the input_fn , function, when i try =&gt; json.loads(request_body).<\/p>\n<p>the error is =&gt; raise JSONDecodeError(&quot;Extra data&quot;, s, end)<\/p>\n<p>has anyone tried this? I sucessfully tested this model and json input with a real time endpoint in sagemaker, but now i'm trying to switch to batch and it is erroring it out.<\/p>\n<p>inference.py<\/p>\n<pre><code>def model_fn(model_dir):\n   ....\n\n\ndef input_fn(request_body, request_content_type):\n    data = json.loads(request_body)\n    return data\n\ndef predict_fn(data, model)\n  ...\n<\/code><\/pre>\n<p>set up for batch job via lambda<\/p>\n<pre><code>response = client.create_transform_job(\n    TransformJobName='some-job',\n    ModelName='mypytorchmodel',\n    ModelClientConfig={\n        'InvocationsTimeoutInSeconds': 3600,\n        'InvocationsMaxRetries': 1\n    },\n    BatchStrategy='MultiRecord',\n    TransformInput={\n        'DataSource': {\n            'S3DataSource': {\n                'S3DataType': 'S3Prefix',\n                'S3Uri': 's3:\/\/inputpath'\n            }\n        },\n        'ContentType': 'application\/json',\n        'SplitType': 'Line'\n    },\n    TransformOutput={\n        'S3OutputPath': 's3:\/\/outputpath',\n        'Accept': 'application\/json',\n        'AssembleWith': 'Line',\n    },\n    TransformResources={\n        'InstanceType': 'ml.g4dn.xlarge'\n        'InstanceCount': 1\n    }\n)\n<\/code><\/pre>\n<p>input file<\/p>\n<pre><code>{&quot;input&quot; : &quot;input line one&quot;}\n{&quot;input&quot; : &quot;input line two&quot;}\n{&quot;input&quot; : &quot;input line three&quot;}\n{&quot;input&quot; : &quot;input line four&quot;}\n{&quot;input&quot; : &quot;input line five&quot;}\n...\n\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661702506533,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73520188",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":23.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"How to get batch predictions with jsonl data in sagemaker?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":52.0,
        "Challenge_word_count":195,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1584308275360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":365.0,
        "Poster_view_count":94.0,
        "Solution_body":"<p>What is your client side code where you are invoking the endpoint? You should also be properly serializing the data on the client side and handling it in your inference script. Example:<\/p>\n<pre><code>import json\ndata = json.loads(json.dumps(request_body))\npayload = json.dumps(data)\nresponse = client.invoke_endpoint(\n    EndpointName=endpoint_name,\n    ContentType=content_type,\n    Body=payload)\nresult = json.loads(response['Body'].read().decode())['Output']\nresult\n<\/code><\/pre>\n<p>Make sure to also specify your content_type appropriately &quot;application\/jsonlines&quot;.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.7,
        "Solution_reading_time":7.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to work on different models on a small piece of ML project which needs to work on azure platform and get the score.py with all the values. It is getting not a single library issue, but getting multiple <strong>Module errors<\/strong> and <strong>Attribute errors<\/strong>. I am using latest SDK version only, but I am not sure, where I am going side path.<\/p>\n<p>Any previous observations on this error?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660210046063,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73318372",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":6.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"several dependency errors causing in Azure AutoML while running model",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":29.0,
        "Challenge_word_count":79,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651094469216,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p>The compatibility break is there for the newer version of the packages based on the current version of <strong>SDK<\/strong>. If the current SDK version is <strong>1.13.0<\/strong> and above, previous versions of packages are not in working stage. The compatibility issue is raising because of support of packages from SDK for different versions. It differs from version-to-version package support from <strong>SDK<\/strong>.<\/p>\n<p>Because of this we are getting Module not found,  <code>ImportError and AttributeError<\/code>.<\/p>\n<p>This solution depends on the AutoML SDK training version.<\/p>\n<ul>\n<li>If you are using 1.13.0 above version of SDK, update the versions of pandas to 0.25.1 and scikit-learn to 0.22.1<\/li>\n<\/ul>\n<p>Using the following command in  <code>BASH<\/code>  to upgrade the versions.<\/p>\n<pre><code>pip install \u2013upgrade pandas==0.25.1\n\npip install \u2013upgrade sickit-learn==0.22.1\n\n<\/code><\/pre>\n<p>The generic syntax for upgrading is:<\/p>\n<pre><code>pip install \u2013upgrade package_name==version\n\n<\/code><\/pre>\n<ul>\n<li>If the error occurs in AutoML Configuration file, then need to upgrade that also.<\/li>\n<li>But it is suggestable to uninstall and reinstall  <code>AutoMLConfig<\/code>.<\/li>\n<\/ul>\n<pre><code>pip uninstall azureml-train automl\n\n<\/code><\/pre>\n<p>Then reinstall using the below code,<\/p>\n<pre><code>pip install azureml-train automl\n\n<\/code><\/pre>\n<p>If you are using windows operating system, then install  <a href=\"https:\/\/docs.conda.io\/en\/latest\/miniconda.html\" rel=\"nofollow noreferrer\">Miniconda<\/a>.<\/p>\n<p>If you are a linux user, then using sudo or conda syntaxes for the same operation.<\/p>\n<p>Some of the advanced libraries of computer vision supportive like TensorFlow will be installed by default. Then we need to install them from dependencies.<\/p>\n<blockquote>\n<pre><code>azureml.core.runconfig import RunConfiguration from\nazureml.core.conda_dependencies import CondaDependencies run_config =\nRunConfiguration() run_config.environment.python.conda_dependencies =\nCondaDependencies.create(conda_packages=['tensorflow==1.12.0']) \n\n<\/code><\/pre>\n<\/blockquote>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-auto-ml#tensorflow\" rel=\"nofollow noreferrer\">Documentation<\/a>  credit to @Larry Franks.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.9,
        "Solution_reading_time":29.75,
        "Solution_score_count":1.0,
        "Solution_sentence_count":25.0,
        "Solution_word_count":249.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How do I get args like epochs to show up in the UI configuration panel under hyperparameters? I want to be able to change number of epochs and learning rate from within the UI.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1629097990217,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68798737",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":2.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"ClearML How to get configurable hyperparameters?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":110.0,
        "Challenge_word_count":39,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1616008398583,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Islamabad, Pakistan",
        "Poster_reputation_count":3.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You can use <code>argparse<\/code> - ClearML will auto-magically log all parameters in the task's configuration section (under hyper-parameters section) - see <a href=\"https:\/\/github.com\/allegroai\/clearml\/blob\/master\/examples\/frameworks\/keras\/keras_tensorboard.py#L56\" rel=\"nofollow noreferrer\">this<\/a> example. You can also just connect any dictionary (see <a href=\"https:\/\/github.com\/allegroai\/clearml\/blob\/master\/examples\/frameworks\/ignite\/cifar_ignite.py#L23\" rel=\"nofollow noreferrer\">this<\/a> example)<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":23.4,
        "Solution_reading_time":6.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1599390178756,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":628.2919663889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm working in sage maker studio, and I have a single instance running one computationally intensive task:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/IntzJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IntzJ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It appears that the kernel running my task is maxed out, but the actual instance is only using a small amount of its resources. Is there some sort of throttling occurring? Can I configure this so that more of the instance is utilized?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1627529779217,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68569742",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":7.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Sage Maker Studio CPU Usage",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":946.0,
        "Challenge_word_count":74,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545360696800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Earth",
        "Poster_reputation_count":1011.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>Your ml.c5.xlarge instance comes with 4 vCPU. However, Python only uses a single CPU by default. (Source: <a href=\"https:\/\/stackoverflow.com\/questions\/64121703\/can-i-apply-multithreading-for-computationally-intensive-task-in-python\">Can I apply multithreading for computationally intensive task in python?<\/a>)<\/p>\n<p>As a result, the overall CPU utilization of your ml.c5.xlarge instance is low. To utilize all the vCPUs, you can try multiprocessing.<\/p>\n<p>The examples below are performed using a 2 vCPU + 4 GiB instance.<\/p>\n<p>In the first picture, multiprocessing is not set up. The instance CPU utilization peaks at around 50%.<\/p>\n<p>single processing:<br \/>\n<img src=\"https:\/\/i.stack.imgur.com\/lcn8K.png\" alt=\"single processing\" \/><\/p>\n<p>In the second picture, I created 50 processes to be run simultaneously. The instance CPU utilization rises to 100% immediately.<\/p>\n<p>multiprocessing:<br \/>\n<img src=\"https:\/\/i.stack.imgur.com\/tk65n.png\" alt=\"multiprocessing\" \/><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1629791630296,
        "Solution_link_count":3.0,
        "Solution_readability":12.1,
        "Solution_reading_time":12.79,
        "Solution_score_count":3.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":111.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"For uploading data to AWS Neptune we use `NeptuneCSVPublisher`, which internally uses `NeptuneBulkLoaderApi`. The current configuration uses config key `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME`, which provides name of IAM role for the loader to be able to use S3 and Neptune. The issue is that `NeptuneBulkLoaderApi` constructs IAM role ARN from name as follows: \r\n\r\n```python\r\naccount_id = self.session.client('sts').get_caller_identity()['Account']\r\nself.iam_role_arn = f'arn:aws:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nwhereas, [second element of ARN aka partition](https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws-arns-and-namespaces.html) can be currently:\r\n* `aws` -AWS Regions\r\n* `aws-cn` - China Regions\r\n* `aws-us-gov` - AWS GovCloud (US) Regions\r\n\r\nSince we use Amundsen also in AWS China, the above ARN is not valid. \r\n\r\n## Expected Behavior\r\n\r\nIAM role ARN either takes into account AWS partition or there is a possibility of passing IAM role ARN instead of name directly.\r\n\r\n## Current Behavior\r\n\r\nIAM role ARN is constructed incorrectly outside of AWS Global.\r\n\r\n## Possible Solutions\r\n\r\nIAM role ARN should take partition into account. There are two solutions:\r\n1. Add partition into current code\r\n2. Add option of passing IAM role ARN directly which supersedes IAM role name \r\n\r\n### Solution 1\r\n\r\nSince I didn't know or found any good way to get the AWS partition, we can use caller identity and ARN there to get the partition, e.g.:\r\n\r\n```python\r\nidentity = self.session.client('sts').get_caller_identity()\r\naccount_id = identity['Account']\r\npartition = identity['Arn'].split(':')[1]\r\nself.iam_role_arn = f'arn:{partition}:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nThis is smaller fix but it is a bit hacky and I'm not sure it'll work in all situation, but it should I guess.\r\n\r\n### Solution 2\r\n\r\nAdd config key `NeptuneCSVPublisher.AWS_IAM_ROLE_ARN` which either supersedes `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME` in a way that in constructor we would have something like:\r\n\r\n```python\r\nif iam_role_arn:\r\n    self.iam_role_arn = iam_role_arn\r\nelse:\r\n   ...\r\n   self.iam_role_arn = f'arn:{partition}:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nOr even replace `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME` with `NeptuneCSVPublisher.AWS_IAM_ROLE_ARN`, which is IMO cleaner, but would be not backward compatible. \r\n\r\n## Steps to Reproduce\r\nDeploy Amundsen in AWS China with Neptune and try to use Databuilder to upload CSV data from S3. \r\n\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\nCurrently we are unable to load data into Neptune as the IAM role ARN setting is hidden and we get an error:\r\n\r\n```\r\n[ERROR] Exception: Failed to load csv. Response: {'detailedMessage': \"Failed to start new load from the source s3:\/\/amundsenBucket\/amundsen\/2021_08_10_01_01_28. Couldn't find the aws credential for iam_role_arn: arn:aws:iam::111111111:role\/RoleForNeptune111111-2222\", 'code': 'InvalidParameterException', 'requestId': 'xxx'}\r\nTraceback (most recent call last):\r\n\u00a0\u00a0File \"\/var\/task\/ctw\/jobs\/synchronize_redshift_metadata.py\", line 49, in lambda_handler\r\n\u00a0\u00a0\u00a0\u00a0redshift_to_neptune_job.launch()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/job\/job.py\", line 76, in launch\r\n\u00a0\u00a0\u00a0\u00a0raise e\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/job\/job.py\", line 72, in launch\r\n\u00a0\u00a0\u00a0\u00a0self.publisher.publish()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/base_publisher.py\", line 40, in publish\r\n\u00a0\u00a0\u00a0\u00a0raise e\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/base_publisher.py\", line 37, in publish\r\n\u00a0\u00a0\u00a0\u00a0self.publish_impl()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/neptune_csv_publisher.py\", line 109, in publish_impl\r\n\u00a0\u00a0\u00a0\u00a0raise Exception(\"Failed to load csv. Response: {0}\".format(str(bulk_upload_response)))\r\n```\r\n\r\n## Your Environment\r\n* Amunsen version used: `amundsen-databuilder==4.3.1`\r\n* Data warehouse stores: AWS Neptune\r\n* Deployment (k8s or native): AWS Step Functions (k8s for backend but unrelated for now)\r\n* Link to your fork or repository:",
        "Challenge_closed_time":1671067.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628591009000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1430",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":49.7,
        "Challenge_repo_contributor_count":207.0,
        "Challenge_repo_fork_count":890.0,
        "Challenge_repo_issue_count":2023.0,
        "Challenge_repo_star_count":3674.0,
        "Challenge_repo_watch_count":245.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":null,
        "Challenge_title":"Databuilder `NeptuneBulkLoaderApi` constructs wrong IAM role ARN for AWS other than global",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":441,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.2,
        "Solution_reading_time":1.69,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":24.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I am trying to do some inference on some CSV file saved on s3 using BatchTransform with `strategy ='MultiRecord'` and `assemble_with='Line'`. The same system works with `strategy ='SingleRecord'`, however I need it to be as efficient as possible. The main issue comes when I switch to MultiRecord with a small csv composed of two columns, both texts.\nWith a `max_payload = 6`, the process is succesfull with a CSV of 181 samples (609.7KB), but with the same CSV with 182 samples (621.6KB), the process fails with a `\"message\": \"Worker died.\"`. I imagined it had something to do with the memory limit of the instance I am using, so I switched to a `ml.m5.2xlarge` with 32GB of memory. \nWhen I switch to a `max_payload = 7`, suddenly the process works with the 182 samples of CSV (621.6KB), but it fails with anything bigger than that. Any ideas of what could be causing this?\n\nThe logs look like this\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,218 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 0%| | 0.00\/8.68M [00:00<?, ?B\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,327 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 0%| | 20.0k\/8.68M [00:00<00:47, 191kB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,434 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 1%| | 100k\/8.68M [00:00<00:17, 526kB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,543 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 3%|\u258e | 228k\/8.68M [00:00<00:10, 841kB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,651 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 6%|\u258c | 509k\/8.68M [00:00<00:05, 1.56MB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,759 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 12%|\u2588\u258f | 1.04M\/8.68M [00:00<00:02, 2.91MB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,867 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 25%|\u2588\u2588\u258d | 2.14M\/8.68M [00:00<00:01, 5.54MB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,975 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 50%|\u2588\u2588\u2588\u2588\u2589 | 4.31M\/8.68M [00:00<00:00, 10.6MB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,976 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 8.65M\/8.68M [00:00<00:00, 20.7MB\/s]\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,129 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8.68M\/8.68M [00:00<00:00, 10.5MB\/s]\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,129 [WARN ] W-9007-model_1.0-stderr MODEL_LOG -\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,131 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 0%| | 0.00\/615 [00:00<?, ?B\/s]\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,894 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13647\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,895 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13459\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,895 [INFO ] W-9006-model_1.0 TS_METRICS - W-9006-model_1.0.ms:15108|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,896 [INFO ] W-9006-model_1.0 TS_METRICS - WorkerThreadTime.ms:56|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,897 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1684489793897\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,898 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13705\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,898 [INFO ] W-9007-model_1.0 TS_METRICS - W-9007-model_1.0.ms:15111|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,899 [INFO ] W-9007-model_1.0 TS_METRICS - WorkerThreadTime.ms:42|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,896 [INFO ] W-9005-model_1.0 TS_METRICS - W-9005-model_1.0.ms:15111|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,902 [INFO ] W-9005-model_1.0 TS_METRICS - WorkerThreadTime.ms:108|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,906 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13399\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,906 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:15148|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,907 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:73|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,907 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Backend received inference at: 1684489793\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:53,989 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13784\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:53,991 [INFO ] W-9002-model_1.0 TS_METRICS - W-9002-model_1.0.ms:15231|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:53,992 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:56|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,009 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13663\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,009 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:15249|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,010 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:103|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,038 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13293\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,038 [INFO ] W-9003-model_1.0 TS_METRICS - W-9003-model_1.0.ms:15277|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,039 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:52|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,187 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13751\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,187 [INFO ] W-9004-model_1.0 TS_METRICS - W-9004-model_1.0.ms:15426|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,188 [INFO ] W-9004-model_1.0 TS_METRICS - WorkerThreadTime.ms:87|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,971 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:66.7|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,972 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:46.74238204956055|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,972 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:9.122749328613281|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,973 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:16.3|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,973 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:17504.14453125|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,974 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:13734.4453125|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,974 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:44.8|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,909 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60000\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,910 [ERROR] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Number or consecutive unsuccessful inference 1\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,910 [ERROR] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\n\n2023-05-19T12:50:54.009+03:00\torg.pytorch.serve.wlm.WorkerInitializationException: Backend worker did not respond in given time\n\n2023-05-19T12:50:54.009+03:00\t#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:199) [model-server.jar:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.lang.Thread.run(Thread.java:829) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,945 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_MODEL_LOADED\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,946 [INFO ] W-9006-model_1.0 ACCESS_LOG - \/169.254.255.130:54478 \"POST \/invocations HTTP\/1.1\" 500 72597\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,947 [INFO ] W-9006-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489779\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,947 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,948 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,949 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,995 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1684489853995\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,997 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Backend received inference at: 1684489853\n\n2023-05-19T12:50:55.010+03:00\t2023-05-19T09:50:54,024 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout\n\n2023-05-19T12:50:55.010+03:00\t2023-05-19T09:50:54,024 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr\n\n2023-05-19T12:50:57.010+03:00\t2023-05-19T09:50:56,227 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: \/home\/model-server\/tmp\/.ts.sock.9006\n\n2023-05-19T12:50:57.010+03:00\t2023-05-19T09:50:56,235 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded \/opt\/conda\/lib\/python3.8\/site-packages\/ts\/configs\/metrics.yaml.\n\n2023-05-19T12:50:57.010+03:00\t2023-05-19T09:50:56,235 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]428\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,236 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: \/home\/model-server\/tmp\/.ts.sock.9006\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,235 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started.\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,237 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,238 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: \/home\/model-server\/tmp\/.ts.sock.9006.\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,238 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1684489856238\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,269 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n\n2023-05-19T12:51:07.014+03:00\t2023-05-19T09:51:06,351 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 10083\n\n2023-05-19T12:51:07.014+03:00\t2023-05-19T09:51:06,351 [INFO ] W-9006-model_1.0 TS_METRICS - W-9006-model_1.0.ms:87564|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489866\n\n2023-05-19T12:51:07.014+03:00\t2023-05-19T09:51:06,352 [INFO ] W-9006-model_1.0 TS_METRICS - WorkerThreadTime.ms:30|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489866\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,971 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,972 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:46.74235534667969|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,972 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:9.12277603149414|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,972 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:16.3|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,973 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:17432.91796875|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.024+03:00\t2023-05-19T09:51:39,974 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:13805.67578125|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.024+03:00\t2023-05-19T09:51:39,974 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:45.0|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:54.028+03:00\t2023-05-19T09:51:53,996 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60000\n\n2023-05-19T12:51:54.028+03:00\t2023-05-19T09:51:53,997 [ERROR] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Number or consecutive unsuccessful inference 2\n\n2023-05-19T12:51:54.028+03:00\t2023-05-19T09:51:53,997 [ERROR] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\n\n2023-05-19T12:51:54.028+03:00\torg.pytorch.serve.wlm.WorkerInitializationException: Backend worker did not respond in given time\n\n2023-05-19T12:51:54.028+03:00\t#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:199) [model-server.jar:?]\n\n2023-05-19T12:51:54.028+03:00\t#011at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684490308406,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1684836402615,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUAI7z2SCLT06bUOu1ULKV-Q\/sagemaker-batch-transform-multirecord-fail-with-csv-as-input",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":205.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":183,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker Batch Transform MultiRecord Fail with CSV as Input",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":65.0,
        "Challenge_word_count":837,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"There could be two possible reasons for the error message : \"message\": \"Worker died.\".\n\n1. This is commonly noticed when the instance exhausts the Memory usage. (This can be verified if you can check the Cloudwatch Metrics for the job run.)\n2. Model server timeouts during the job. \n\nYou can either use a larger instance for the job or try to set the higher value of following environment variables - worker and timeout in your script. \n\n\tmodel_server_workers = int(os.environ.get(_params.MODEL_SERVER_WORKERS_ENV, num_cpus())) \n\tmodel_server_timeout = int(os.environ.get(_params.MODEL_SERVER_TIMEOUT_ENV, <Enter value here>))",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1684555351304,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":7.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":82.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to read a csv file on an s3 bucket (for which the sagemaker notebook has full access to) into a spark dataframe however I am hitting the following issue where <code>sagemaker-spark_2.11-spark_2.2.0-1.1.1.jar<\/code> can't be found. Any tips on how to resolve this is appreciate!<\/p>\n\n<pre><code>bucket = \"mybucket\"\nprefix = \"folder\/file.csv\"\ndf = spark.read.csv(\"s3:\/\/{}\/{}\/\".format(bucket,prefix))\n\nPy4JJavaError: An error occurred while calling o388.csv.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Error reading configuration file\nat java.util.ServiceLoader.fail(ServiceLoader.java:232)\nat java.util.ServiceLoader.parse(ServiceLoader.java:309)\nat java.util.ServiceLoader.access$200(ServiceLoader.java:185)\nat java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:357)\nat java.util.ServiceLoader$LazyIterator.hasNext(ServiceLoader.java:393)\nat java.util.ServiceLoader$1.hasNext(ServiceLoader.java:474)\nat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)\nat scala.collection.Iterator$class.foreach(Iterator.scala:893)\nat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\nat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\nat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\nat scala.collection.TraversableLike$class.filterImpl(TraversableLike.scala:247)\nat scala.collection.TraversableLike$class.filter(TraversableLike.scala:259)\nat scala.collection.AbstractTraversable.filter(Traversable.scala:104)\nat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:614)\nat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190)\nat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:596)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:498)\nat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\nat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\nat py4j.Gateway.invoke(Gateway.java:282)\nat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\nat py4j.commands.CallCommand.execute(CallCommand.java:79)\nat py4j.GatewayConnection.run(GatewayConnection.java:238)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker_pyspark\/jars\/sagemaker-spark_2.11-spark_2.2.0-1.1.1.jar (No such file or directory)\n    at java.util.zip.ZipFile.open(Native Method)\n    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:219)\n    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:149)\n    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:166)\n    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:103)\n    at sun.net.www.protocol.jar.URLJarFile.&lt;init&gt;(URLJarFile.java:93)\n    at sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:69)\n    at sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:84)\n    at sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:122)\n    at sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:150)\n    at java.net.URL.openStream(URL.java:1045)\n    at java.util.ServiceLoader.parse(ServiceLoader.java:304)\n    ... 26 more\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1532387294417,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1532387921180,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51488308",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":34.1,
        "Challenge_reading_time":49.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":null,
        "Challenge_title":"Failing to read data from s3 to a spark dataframe in Sagemaker",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":2283.0,
        "Challenge_word_count":169,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1444524456120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Los Angeles, CA, USA",
        "Poster_reputation_count":973.0,
        "Poster_view_count":82.0,
        "Solution_body":"<p>(Making comment to the original question as answer)<\/p>\n\n<p>It looks like a jupyter kernel issue. I had a similar issue and I used <code>Sparkmagic (pyspark)<\/code> kernel instead of <code>Sparkmagic (pyspark3)<\/code> and it is working fine. Follow instructions on this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\" rel=\"nofollow noreferrer\">blog<\/a> and see if it helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":5.93,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":48.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>As part of our MLOps flow, we need to retrain a machine learning model using the AML designer, and then update the AKS webservice with the new machine learning model (+ a couple of other supplementary training artifacts), also from the designer.  <\/p>\n<p>We have built an inference pipeline to do this, and are able to run it manually. However, the solution requirements require this process to be automated. We have previously successfully automated this through the python SDK and the akswebservice.update method, but this solution has a hard requirement to use the designer only (custom python code blocks would be allowed, however).  <\/p>\n<p>Is there a way, using any Azure services (eg Azure Data Factory, Azure DevOps), that we can kick off a designer real time inference update pipeline immediately after its associated training pipeline finishes executing, in order to get the latest model version into the webservice, without any manual intervention? To be clear though, manual intervention is acceptable to build the initial inference pipeline for version 1, but not on the retraining cycle.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620652503773,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/389170\/azure-ml-designer-automatically-update-aks-webserv",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":14.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Designer: Automatically Update AKS Webservice After Training",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":183,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. Currently, you can only use the Azure Machine Learning SDK to automatically <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-update-web-service\">update the web service<\/a>. I'm inquiring from the product team whether there are plans to support this scenario (will share updates accordingly). Hope this helps.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.1,
        "Solution_reading_time":4.89,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Enchanter v0.7.0 raise `COMET WARNING: log_asset_data(..., file_name=...) is deprecated; use log_asset_data(..., name=...)` when using Context API\r\n\r\n## Expected behavior\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n- Enchanter version: v0.7.0\r\n- Python version: ?\r\n- OS: Linux\r\n- (Optional) Other libraries and their versions: Google Colab with GPU\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## Reproducible examples (optional)\r\n\r\n```python\r\nrunner = ClassificationRunner(\r\n    net, optimizer, criterion, Experiment()\r\n)\r\n\r\nwith runner:\r\n    runner.scaler = torch.cuda.amp.GradScaler()\r\n\r\n    runner.add_loader(\"train\", trainloader)\r\n    runner.add_loader(\"test\", testloader)\r\n    runner.train_config(epochs=20)\r\n\r\n    runner.run()\r\n```\r\n\r\n## Additional context (optional)\r\n\r\n<!-- Please add any other context or screenshots about the problem here. -->",
        "Challenge_closed_time":1600153.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600151670000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/khirotaka\/enchanter\/issues\/129",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":211.0,
        "Challenge_repo_star_count":7.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"COMET WARNING: log_asset_data(..., file_name=...) is deprecated; use log_asset_data(..., name=...)",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":109,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Issue-Label Bot is automatically applying the label `bug` to this issue, with a confidence of 0.69. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \n\n Links: [app homepage](https:\/\/github.com\/marketplace\/issue-label-bot), [dashboard](https:\/\/mlbot.net\/data\/khirotaka\/enchanter) and [code](https:\/\/github.com\/hamelsmu\/MLapp) for this bot.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.3,
        "Solution_reading_time":4.88,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":38.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":8,
        "Challenge_body":"<p>Is it possible to parse additional arguments to the program when running the wandb agent command from the command line?<\/p>\n<p>For example, suppose I have a script <code>train.py<\/code> that takes a <code>--gpu_idx<\/code> argument to specify the GPU index, and I want to run the script with different GPUs using the WandB agent. Can I pass the <code>--gpu_idx<\/code> argument as a key-value pair when running the <code>wandb agent<\/code> command?<\/p>\n<p><code>wandb agent &lt;ID&gt;  --gpu_idx 1<\/code><\/p>\n<p>In the training script, I have something like:<\/p>\n<pre><code class=\"lang-auto\">import wandb\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--gpu_idx', type=int, default=1)\nargs = parser.parse_args()\n\nwandb.init()\nwandb.config.update(args)\n\n# train model with the learning rate\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1678133398859,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/parse-additional-arguments-to-the-program-when-running-the-wandb-agent-command-from-the-command-line\/4010",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":10.2,
        "Challenge_reading_time":11.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Parse additional arguments to the program when running the wandb agent command from the command line",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":348.0,
        "Challenge_word_count":117,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/liu97\">@liu97<\/a> thanks for the additional context. In that case you won\u2019t be able to do this as you will be using the same <code>yaml<\/code> for all agents. What you could do though in a multi-gpu environment is to specify the GPU as follows:<\/p>\n<pre><code class=\"lang-auto\">CUDA_VISIBLE_DEVICES=0 wandb agent sweep_ID\n<\/code><\/pre>\n<p>Would this work for you? Please also check <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/parallelize-agents#parallelize-on-a-multi-gpu-machine\">this docs page<\/a> for more information.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.7,
        "Solution_reading_time":7.26,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":67.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to use this <a href=\"https:\/\/github.com\/amogh147\/binance_takeHome_gemini_amogh\" rel=\"nofollow noreferrer\">repo<\/a> and I have created and activated a virtualenv and installed the required dependencies.<\/p>\n<p>I get an error when I run pytest.<\/p>\n<p>And under the file binance_cdk\/app.py it describes the following tasks:<\/p>\n<h1>App (PSVM method) entry point of the program.<\/h1>\n<h1>Note:<\/h1>\n<p>Steps tp setup CDK:<\/p>\n<ol>\n<li>install npm<\/li>\n<li>cdk -init (creates an empty project)<\/li>\n<li>Add in your infrastructure code.<\/li>\n<li>Run CDK synth<\/li>\n<li>CDK bootstrap &lt;aws_account&gt;\/<\/li>\n<li>Run CDK deploy ---&gt; This creates a cloudformation .yml file and the aws resources will be created as per the mentioned stack.<\/li>\n<\/ol>\n<p>I'm stuck on step 3, what do I add in this infrastructure code, and if I want to use this on amazon sagemaker which I am not familiar with, do I even bother doing this on my local terminal, or do I do the whole process regardless on sagemaker?<\/p>\n<p>Thank you in advance for your time and answers !<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655801685383,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72697889",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":13.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"How to deploy AWS using CDK, sagemaker?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":100.0,
        "Challenge_word_count":161,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604312740476,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":141.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>The infrastructure code is the Python code that you want to write for the resources you want to provision with SageMaker. In the example you provided for example the infra code they have is creating a Lambda function. You can do this locally on your machine, the question is what do you want to achieve with SageMaker? If you want to create an endpoint then following the CDK Python docs with SageMaker to identify the steps for creating an endpoint. Here's two guides, the first is an introduction to the AWS CDK and getting started. The second is an example of using the CDK with SageMaker to create an endpoint for  inference.<\/p>\n<p>CDK Python Starter: <a href=\"https:\/\/towardsdatascience.com\/build-your-first-aws-cdk-project-18b1fee2ed2d\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/build-your-first-aws-cdk-project-18b1fee2ed2d<\/a>\nCDK SageMaker Example: <a href=\"https:\/\/github.com\/philschmid\/cdk-samples\/tree\/master\/sagemaker-serverless-huggingface-endpoint\" rel=\"nofollow noreferrer\">https:\/\/github.com\/philschmid\/cdk-samples\/tree\/master\/sagemaker-serverless-huggingface-endpoint<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.7,
        "Solution_reading_time":14.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":126.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi,\r\nI was trying to follow this documentation: https:\/\/azure.microsoft.com\/en-us\/services\/open-datasets\/catalog\/noaa-integrated-surface-data\/ (Go to \"Data access\" tab)to use opendatasets module to access historical weather data. But it gives me the error message `No name 'opendatasets' in module 'azureml'`. \r\nI tried `pip install azureml-sdk[opendatasets]` as well, it shows `WARNING: azureml-sdk 1.0.55 does not provide the extra 'opendatasets'`.\r\nDo you know how to use the opendatasets module in azureml?\r\n\r\nThanks!",
        "Challenge_closed_time":1565217.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565216029000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/518",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":7.24,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"No name 'opendatasets' in module 'azureml' Error",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":71,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Find the solution, maybe because `opendatasets` is a preview module, so it is not included in azureml sdk yet. You can download through pip `pip install azureml-opendatasets` in your env. > pip install azureml-opendatasets\r\n\r\nThanks, was looking for the solution, this worked !! However, I had another error \" [WinError 5] Access is denied:\" This was solved by adding --user at the end of your command.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":4.91,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"### System Info\n\n```shell\nThis was verified today on a fresh SageMaker Studio instance running in us-west-2.\r\n\r\nIt's not a Transformer issue, but as sacremoses is a dependency, this is likely to break 'pip install transformers' on SageMaker Studio at some point.\n```\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1) Open an SM Studio notebook\r\n\r\n2) Run the following cell:\r\n```\r\n%%sh\r\npip install \"sacremoses>=0.0.50\"\r\n```\r\n\r\nThe obvious workaround for now is\r\n```\r\npip install \"sacremoses==0.0.49\"\r\n```\r\n\r\n\n\n### Expected behavior\n\n```shell\nsacremoses should install without error.\n```\n",
        "Challenge_closed_time":1654502.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651754767000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17096",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.3,
        "Challenge_reading_time":10.49,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17219.0,
        "Challenge_repo_issue_count":20692.0,
        "Challenge_repo_star_count":76135.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"pip install \"sacremoses>=0.0.50\" breaks on SageMaker Studio",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":115,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for the issue @juliensimon, this should be fixed by https:\/\/github.com\/huggingface\/transformers\/pull\/17049. It will be in the next release which should drop early next week. This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored. Should be fixed now!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.5,
        "Solution_reading_time":6.85,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":73.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I launched an Autopilot job in SageMaker Studio, and now I'm trying to figure out how to compare autoML iterations. Is there a way to list them, see their metrics, and see the configuration of the best job?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601332827000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667925765776,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU7p9B6zcpSIeTG4MbzrSjKA\/how-do-you-analyze-autopilot-results-in-amazon-sagemaker-studio",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":3.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How do you analyze Autopilot results in Amazon SageMaker Studio?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":91.0,
        "Challenge_word_count":47,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Watch the [Choose and deploy the best model](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/autopilot-videos.html#autopilot-video-choose-and-deploy-the-best-model) video tutorial in the SageMaker developer guide. The video shows how to use SageMaker Autopilot to visualize and compare model metrics.\n\nFor more SageMaker Autopilot tutorials, see [Videos: Use Autopilot to automate and explore the machine learning process](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/autopilot-videos.html).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925562588,
        "Solution_link_count":2.0,
        "Solution_readability":18.4,
        "Solution_reading_time":6.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1478616262200,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Kolkata, West Bengal, India",
        "Answerer_reputation_count":375.0,
        "Answerer_view_count":69.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to train a PyTorch FLAIR model in AWS Sagemaker.\nWhile doing so getting the following error:<\/p>\n<pre><code>RuntimeError: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 11.17 GiB total capacity; 9.29 GiB already allocated; 7.31 MiB free; 10.80 GiB reserved in total by PyTorch)\n<\/code><\/pre>\n<p>For training I used sagemaker.pytorch.estimator.PyTorch class.<\/p>\n<p>I tried with different variants of instance types from ml.m5, g4dn to p3(even with a 96GB memory one).\nIn the ml.m5 getting the error with CPUmemoryIssue, in g4dn with GPUMemoryIssue and in the P3 getting GPUMemoryIssue mostly because Pytorch is using only one of the GPU of 12GB out of 8*12GB.<\/p>\n<p>Not getting anywhere to complete this training, even in local tried with a CPU machine and got the following error:<\/p>\n<pre><code>RuntimeError: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 67108864 bytes. Buy new RAM!\n<\/code><\/pre>\n<p>The model training script:<\/p>\n<pre><code>    corpus = ClassificationCorpus(data_folder, test_file='..\/data\/exports\/val.csv', train_file='..\/data\/exports\/train.csv')\n                                          \n    print(&quot;finished loading corpus&quot;)\n\n    word_embeddings = [WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n\n    document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n\n    classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n\n    trainer = ModelTrainer(classifier, corpus, optimizer=Adam)\n\n    trainer.train('..\/model_files', max_epochs=12,learning_rate=0.0001, train_with_dev=False, embeddings_storage_mode=&quot;none&quot;)\n<\/code><\/pre>\n<p>P.S.: I was able to train the same architecture with a smaller dataset in my local GPU machine with a 4GB GTX 1650 DDR5 memory and it was really quick.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":4,
        "Challenge_created_time":1597607305160,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1597613347630,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63441299",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":13.2,
        "Challenge_reading_time":25.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"Pytorch CUDA OutOfMemory Error while training",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":2033.0,
        "Challenge_word_count":215,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1478616262200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kolkata, West Bengal, India",
        "Poster_reputation_count":375.0,
        "Poster_view_count":69.0,
        "Solution_body":"<p>Okay, so after 2 days of continuous debugging was able to find out the root cause.\nWhat I understood is Flair does not have any limitation on the sentence length, in the sense the word count, it is taking the highest length sentence as the maximum.\nSo there it was causing issue, as in my case there were few content with 1.5 lakh rows which is too much to load the embedding of into the memory, even a 16GB GPU.\nSo there it was breaking.<\/p>\n<p><strong>To solve this<\/strong>: For content with this much lengthy words, you can take chunk of n words(10K in my case) from these kind of content from any portion(left\/right\/middle anywhere) and trunk the rest, or simply ignore those records for training if it is very minimal in comparative count.<\/p>\n<p>After this I hope you will be able to progress with your training, as it happened in my case.<\/p>\n<p>P.S.: If you are following this thread and face similar issue feel free to comment back so that I can explore and help on your case of the issue.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":12.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":181.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1449207605092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Manila, NCR, Philippines",
        "Answerer_reputation_count":185.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using the following lines of code to specify the desired machine type and accelerator\/GPU on a Kubeflow Pipeline (KFP) that I will be running on a serverless manner through Vertex AI\/Pipelines.<\/p>\n<pre><code>op().\nset_cpu_limit(8).\nset_memory_limit(50G).\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-k80').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>and it works for other GPUs as well i.e. Pascal, Tesla, Volta cards.<\/p>\n<p>However, I can't do the same with the latest accelerator type which is the <code>Tesla A100<\/code> as it requires a special machine type, which is as least an <code>a2-highgpu-1g<\/code>.<\/p>\n<p>How do I make sure that this particular component will run on top of <code>a2-highgpu-1g<\/code> when I run it on Vertex?<\/p>\n<p>If i simply follow the method for older GPUs:<\/p>\n<pre><code>op().\nset_cpu_limit(12). # max for A2-highgpu-1g\nset_memory_limit(85G). # max for A2-highgpu-1g\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>It throws an error when run\/deployed since the machine type that is being spawned is the general type i.e. N1-Highmem-*<\/p>\n<p>Same thing happened when I did not specify the cpu and memory limits, in hope that it will automatically select the right machnie type based on the accelerator constraint.<\/p>\n<pre><code>    op().\n    add_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\n    set_gpu_limit(1)\n<\/code><\/pre>\n<p>Error:\n<code>&quot;NVIDIA_TESLA_A100&quot; is not supported for machine type &quot;n1-highmem-2&quot;,<\/code><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1631772554497,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69203143",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":12.8,
        "Challenge_reading_time":21.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Using Tesla A100 GPU with Kubeflow Pipelines on Vertex AI",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":473.0,
        "Challenge_word_count":197,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449207605092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Manila, NCR, Philippines",
        "Poster_reputation_count":185.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>Currently, GCP don't support A2 Machine type for normal KF Components. A potential workaround right now is to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-job\" rel=\"nofollow noreferrer\"><strong>GCP custom job component<\/strong><\/a> that you can explicitly specify the machine type.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":4.2,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":57.0580416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am execute a python script in Azure machine learning studio. I am including other python scripts and python library, Theano. I can see the Theano get loaded and I got the proper result after script executed. But I saw the error message:<\/p>\n\n<blockquote>\n  <p>WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.<\/p>\n<\/blockquote>\n\n<p>Did anyone know how to solve this problem? Thanks!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539726201673,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52844431",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":9.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Studio execute python script, Theano unable to execute optimized C-implementations (for both CPU and GPU)",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":99.0,
        "Challenge_word_count":112,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1337362023536,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":581.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>I don't think you can fix that - the Python script environment in Azure ML Studio is rather locked down, you can't really configure it (except for choosing from a small selection of Anaconda\/Python versions). <\/p>\n\n<p>You might be better off using the new Azure ML service, which allows you considerably more configuration options (including using GPUs and the like). <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1539931610623,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":4.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No --> Yes\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1. Open remote connection to Azure Machine Learning Compute Instance\r\n\r\nThis does not seem to cause any issues, but it's annoying to see the error message every time.\r\n\r\nAction: azureAccount.onSubscriptionsChanged\r\nError type: REQUEST_SEND_ERROR\r\nError Message: request to redacted:url failed, reason: getaddrinfo ENOTFOUND redacted:idworkspace.westeurope.api.azureml.ms\r\n\r\n\r\nVersion: 0.8.2\r\nOS: linux\r\nOS Release: 5.4.0-1068-azure\r\nProduct: Visual Studio Code\r\nProduct Version: 1.66.1\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nnew t extension.js:2:486489\r\nt.<anonymous> extension.js:2:470040\r\nextension.js:2:2450576\r\nObject.throw extension.js:2:2450681\r\nc extension.js:2:2449471\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":1652117.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649744320000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1541",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.5,
        "Challenge_reading_time":13.57,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Reoccurring error on opening connection to Azure Machine Learning Compute Instance",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":123,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@evakkuri thanks for filing this issue. Is this happening every time you open a remote connection? Are you connecting to Compute Instance through the ML Studio? Currently this happens every time I connect. I'm not connecting via ML Studio, instead through VS Code with the Azure Machine Learning extension. @sevillal Can you please follow up here :) ? @evakkuri we have published version v0.10.0 of the extension. Could you please upgrade and retry to check if you issue is still reproducible? Please reopen this issue if that's the case.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.3,
        "Solution_reading_time":6.61,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I'd like to set up Amazon SageMaker XGBoost to train datasets on multiple machines. Is that possible? If so, how?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583496984000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668057386500,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUOKZq2V_RQaaFzQkapcWpsA\/does-amazon-sagemaker-xgboost-support-parallel-training-across-multiple-machines",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":2.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Does Amazon SageMaker XGBoost support parallel training across multiple machines?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":119.0,
        "Challenge_word_count":29,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes, using Amazon SageMaker hosting with XGBoost allows you to train datasets on multiple machines.\n\nFor more information, see [Docker registry paths and example code](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html) in the Amazon SageMaker developer guide. ",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925571827,
        "Solution_link_count":1.0,
        "Solution_readability":16.7,
        "Solution_reading_time":3.94,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi to all\n\nIm trying to run a procedure looking to reduce the number of features for a model.\n\nThe first try was with google Colab pro+ but it keep crashing and nver run the entire process, then I got a VM\u00a0n1-highmem-8 that has:\u00a0\n\nGPUs1 x NVIDIA Tesla V100\u00a0 +\u00a0\u00a0n1-highmem-8 (vCPUs: 8, RAM: 52GB)\n\nand still not getting the process done.\n\nThe question is how to determin which type of machine should I use? Can I get any metric from the cell that is runing in colab and be able to determin the Type of VM that I need?",
        "Challenge_closed_time":1659349.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658842140000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-determin-which-GCP-VM-do-I-need-for-ML\/m-p\/447075#M448",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.3,
        "Challenge_reading_time":6.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to determin which GCP VM do I need for ML",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":105.0,
        "Challenge_word_count":110,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"There are a few things to take in consideration:\n\nHave you installed all the necessary drivers for the GPU? Here is a complete guide that you can follow.\nI do not see any Python wrapper for CUDA in your code. The way you specify when to use the GPU for specific tasks is through this wrapper, it seems to me that you are using the CPU instead and that is why the task keeps crashing. Now, converting your code to a CUDA version is not a trivial task, and it involves a deeper knowledge on how a GPU works. If you are in a hurry, you could try the Py2CUDA github project, but I would strongly recommend taking a look at the Getting Started Blogs.\u00a0\u00a0\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.0,
        "Solution_reading_time":8.02,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":130.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to use GPT-3 for my application.  I understand MS has licensed GPT-3 from OpenAI, and that there is pricing too.  So how do I get to use GPT-3?  <\/p>\n<p>Chris Powell<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605179070400,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/160489\/gpt-3-access",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.2,
        "Challenge_reading_time":2.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"GPT-3 access",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":34,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=912f72cd-88e9-416f-bbe1-5a7356385053\">@Crispy  <\/a> Thanks for the question, Innovations from our GPT-3 workstreams will be incorporated in later versions of Azure. In the meantime, If you are interested in participation in the OpenAI GPT-3 and Azure Service partnership please fill out this <a href=\"https:\/\/forms.office.com\/Pages\/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRyj5DlT4gqZKgEsfbkRQK5xUQVlSVlJITkxDQkRaOVdESjJGN0dONkQzNy4u\">form<\/a> to submit a request.    <\/p>\n<p>Ignite blog announcement: <a href=\"https:\/\/blogs.microsoft.com\/ai-for-business\/ai-at-scale-ignite\/\">https:\/\/blogs.microsoft.com\/ai-for-business\/ai-at-scale-ignite\/<\/a>    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.0,
        "Solution_reading_time":9.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created an AKS cluster using Azure Machine Learning SDK extension and I attached to the workspace created. When the cluster is created and attached, I doesn't show any error. When I am trying to detach it from workspace, it is not accepting the operations.<\/p>\n<p>I would like to detach the existing AKS cluster from workspace either by program manner, using CLI or even using Azure portal.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659308799773,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73187536",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":5.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Error while detaching AKS cluster through Azure ML SDK extension",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":47.0,
        "Challenge_word_count":76,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651093614703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherland",
        "Poster_reputation_count":19.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>If we are using any <strong>extensions of SDK or Azure CLI<\/strong> for machine learning to detach AKS cluster, it <strong>will not work<\/strong> and it will not get deleted or detached. Instead, we need to use <strong>Azure CLI with AKS<\/strong>. There are two types of implementations we can perform.<\/p>\n<p><strong>Python:<\/strong><\/p>\n<pre><code>Aks_target.detach()\n<\/code><\/pre>\n<p><strong>Azure CLI:<\/strong><\/p>\n<p>Before performing this step, we need to get the details of the working AKS cluster name attached to our workspace. Resource Group details and workspace name<\/p>\n<pre><code>az ml computertarget detach -n youraksname -g yourresourcegroup -w yourworkspacename\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":8.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1657058369727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to perform <strong>distributed training<\/strong> on <strong>Amazon SageMaker<\/strong>. The code is written with <strong>TensorFlow<\/strong> and similar to the following code where I think CPU instance should be enough:\u00a0\n<a href=\"https:\/\/github.com\/horovod\/horovod\/blob\/master\/examples\/tensorflow_word2vec.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/horovod\/horovod\/blob\/master\/examples\/tensorflow_word2vec.py<\/a><\/p>\n<p>Can <strong>Horovod with TensorFlow<\/strong> work on <strong>non-GPU<\/strong> instances in Amazon SageMaker?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662864603780,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73676483",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":8.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Can Horovod with TensorFlow work on non-GPU instances in Amazon SageMaker?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":17.0,
        "Challenge_word_count":54,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>Yeah you should be able to use both CPU's and GPU's with Horovod on Amazon SageMaker. Please follow the below example for the same<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":29.3,
        "Solution_reading_time":6.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1363320186152,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1352.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":9.9320377778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a minimal example of a neural network with a back-propagation trainer, testing it on the IRIS data set. I started of with 7 hidden nodes and it worked well.<\/p>\n\n<p>I lowered the number of nodes in the hidden layer to 1 (expecting it to fail), but was surprised to see that the accuracy went up.<\/p>\n\n<p>I set up the experiment in azure ml, just to validate that it wasn't my code. Same thing there, 98.3333% accuracy with a single hidden node.<\/p>\n\n<p>Can anyone explain to me what is happening here?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1462108714660,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36967126",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.0,
        "Challenge_reading_time":7.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Why do I get good accuracy with IRIS dataset with a single hidden node?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":4488.0,
        "Challenge_word_count":105,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426502047332,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":825.0,
        "Poster_view_count":82.0,
        "Solution_body":"<p>First, it has been well established that a variety of classification models yield incredibly good results on Iris (Iris is very predictable); see <a href=\"http:\/\/lab.fs.uni-lj.si\/lasin\/wp\/IMIT_files\/neural\/doc\/seminar8.pdf\" rel=\"noreferrer\">here<\/a>, for example.<\/p>\n\n<p>Secondly, we can observe that there are relatively few features in the Iris dataset. Moreover, if you look at the <a href=\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/iris\/iris.names\" rel=\"noreferrer\">dataset description<\/a> you can see that two of the features are very highly correlated with the class outcomes.<\/p>\n\n<p>These correlation values are linear, single-feature correlations, which indicates that one can most likely apply a linear model and observe good results. Neural nets are highly nonlinear; they become more and more complex and capture greater and greater nonlinear feature combinations as the number of hidden nodes and hidden layers is increased.<\/p>\n\n<p>Taking these facts into account, that (a) there are few features to begin with and (b) that there are high linear correlations with class, would all point to a less complex, linear function as being the appropriate predictive model-- by using a single hidden node, you are very nearly using a linear model.<\/p>\n\n<p>It can also be noted that, in the absence of any hidden layer (i.e., just input and output nodes), and when the logistic transfer function is used, this is equivalent to logistic regression.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1462144469996,
        "Solution_link_count":2.0,
        "Solution_readability":12.5,
        "Solution_reading_time":18.6,
        "Solution_score_count":6.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":206.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI'm trying this nice SageMaker Autopilot demo https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/autopilot\/autopilot_customer_churn_high_level_with_evaluation.ipynb\n\nAt the beginning of the job, the status is \"InProgress - AnalyzingData\" for several minutes. This is long enough that I'd like to know more about it: what is Autopilot doing when at that status?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596036787000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667925777990,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU8QUiTTSMQ2W2uOgHXC7lqA\/what-is-sagemaker-autopilot-doing-when-in-state-inprogress-analyzingdata",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":5.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"What is SageMaker Autopilot doing when in state \"InProgress - AnalyzingData\" ?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":50.0,
        "Challenge_word_count":53,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"There are some metrics begin collected in this stage. To understanding what is doing is the same as what happens when you're using tensorflow autoML.  There's a deep explanation what is does in our Science page https:\/\/www.amazon.science\/publications\/amazon-sagemaker-autopilot-a-white-box-automl-solution-at-scale",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1609768237540,
        "Solution_link_count":1.0,
        "Solution_readability":14.0,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554425457572,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":63.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Before the AWS Sagemaker batch transform I need to do some transform. is it possible to have an custom script and associate as entry point to BatchTransformer?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645114921967,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71161777",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.3,
        "Challenge_reading_time":2.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker Batch Transform entry point",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":230.0,
        "Challenge_word_count":31,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554425457572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":63.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The inference code and requirement.txt should be stored as part of model.gz while training.  They will be used in the batch transform!!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.1,
        "Solution_reading_time":1.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI have large image dataset stored in a Sagemaker notebook instance, in the file system. I was hoping to learn how I could access this data from outside of that particular notebook instance. I have done quite a bit of researching but can't seem to find much - I am relatively new to this.\n\nI want to be able to access the data in that notebook in a fast manner as I will be using the data to train an AI model. Is there any recommended way to do this? \n\nI originally uploaded the data within that notebook instance to train a model within that instance in exactly the same file system. Note that it is a reasonably large dataset which I had to do some preprocessing on within Sagemaker. \n\nWhat is the best way to store data when using the Sagemaker estimators from training AI models? \n\nMany thanks \n\nTim",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1638914293851,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1668630486323,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU3yXAL7d7Sl--kKO3TTZf1g\/how-to-access-file-system-in-sagemaker-notebook-instance-from-outside-of-that-instance-ie-via-python-sagemaker-estimator-training-call",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":11.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"How to access file system in Sagemaker notebook instance from outside of that instance (ie via Python Sagemaker Estimator training call)",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1538.0,
        "Challenge_word_count":170,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Tim, when you create a sagemaker training job using the estimator, the general best practice is to store your data on S3 and the training job will launch instances as requested by the training job configuration. As now we support fast file mode, which allows faster training job start compared to the file mode (which downloads the data from s3 to the training instance). But when you say you used sagemaker notebook instance to train the model, I assume you were not using SageMaker Training jobs but rather running the notebook (.ipynb) on the SageMaker notebook instance. Please note that as SageMaker is a fully managed service, the notebook instance (also training instances, hosting instances etc.) are launched in the service account, so you will not have directly access to those instance. The SageMaker notebook instance use EBS to store data and the EBS volume is mounted to the \/home\/ec2-user\/SageMaker. Please note that the EBS volume used by a SageMaker notebook instance can only be increased but not decrease. If you want to reduce the EBS volume, you need to create a new notebook instance with a smaller volume and move your data from the previous instance via s3. You will not be able to access that EBS volume from outside of the SageMaker notebook instance. The general best practice is to store large dataset on s3 and only use sample data on the SageMaker notebook instance (reduce the storage). Then use that small amount of sample data to test\/build your code. Then when you are ready to train on the whole dataset, you can launch a SageMaker training job and use the whole dataset stored on s3. Note that, running the training on the whole dataset on a SageMaker notebook instance will require you to use a big instance with enough computing power and also will not be able to perform distributed training with multiple instances. Comparatively, if you run the training job use SageMaker training instances, it gives you more flexibility of choosing the instance type and allow you to run on multiple instances for distributed training. Lastly, once the SageMaker training job is done, all the resources will be terminated which will save cost compared to continue using the big instance with a SageMaker notebook instance. Hope this has helped answer your question",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1638917636668,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":28.01,
        "Solution_score_count":2.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":387.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1535490052056,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>A\/B test feature in SageMaker sounds so intriguing but the more I looked into, the more I am confused whether this is a useful feature. For this to be useful, you need to get the variant assignment data back and join with some internal data to figure out the best performing variant.<\/p>\n\n<p>How is this assignment done? Is it purely random? Or am I supposed to pass some kind of ID (or hashed ID) which can indicate a person or a browser so that the same model is picked up for the same person.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535446293390,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52053776",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.6,
        "Challenge_reading_time":6.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"A\/B Test feature in SageMaker: variant assignment is random?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":614.0,
        "Challenge_word_count":102,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1305269513436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":10317.0,
        "Poster_view_count":595.0,
        "Solution_body":"<blockquote>\n  <p>For this to be useful, you need to get the variant assignment data back and join with some internal data to figure out the best performing variant. <\/p>\n<\/blockquote>\n\n<p>The InvokeEndpoint response includes the \"InvokedProductionVariant\", in order to support the kind of analysis you describe. Details can be found in the API documentation: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html#API_runtime_InvokeEndpoint_ResponseSyntax\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html#API_runtime_InvokeEndpoint_ResponseSyntax<\/a><\/p>\n\n<blockquote>\n  <p>How is this assignment done? Is it purely random? <\/p>\n<\/blockquote>\n\n<p>Traffic is distributed randomly while remaining proportional to the weight of the production variant.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.1,
        "Solution_reading_time":11.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":84.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":11,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nI am training a resnet model on multi core tpus on kaggle. I get this error:\r\n```\r\nDumping Computation:\r\n2021-10-08 23:57:50.220206: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92108 = s32[] constant(0)\r\n2021-10-08 23:57:50.220217: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92110 = pred[] compare(s32[] %constant.92102, s32[] %constant.92108), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220227: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92109 = f32[] constant(1)\r\n2021-10-08 23:57:50.220238: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92111 = f32[] convert(s32[] %constant.92102)\r\n2021-10-08 23:57:50.220248: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92112 = f32[] divide(f32[] %constant.92109, f32[] %convert.92111)\r\n2021-10-08 23:57:50.220260: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92113 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220271: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92114 = f32[] select(pred[] %compare.92110, f32[] %divide.92112, f32[] %constant.92113)\r\n2021-10-08 23:57:50.220281: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92115 = f32[] multiply(f32[] %reduce.92107, f32[] %select.92114)\r\n2021-10-08 23:57:50.220292: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92116 = f32[] convert(f32[] %multiply.92115)\r\n2021-10-08 23:57:50.220302: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134449 = f32[1]{0} reshape(f32[] %convert.92116)\r\n2021-10-08 23:57:50.220312: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92081 = f32[1]{0} reshape(f32[] %p3148.47101)\r\n2021-10-08 23:57:50.220323: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92082 = f32[1]{0} concatenate(f32[1]{0} %reshape.92081), dimensions={0}\r\n2021-10-08 23:57:50.220333: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92083 = f32[] constant(0)\r\n2021-10-08 23:57:50.220343: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92089 = f32[] reduce(f32[1]{0} %concatenate.92082, f32[] %constant.92083), dimensions={0}, to_apply=%AddComputation.92085\r\n2021-10-08 23:57:50.220353: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92084 = s32[] constant(1)\r\n2021-10-08 23:57:50.220364: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92090 = s32[] constant(0)\r\n2021-10-08 23:57:50.220375: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92092 = pred[] compare(s32[] %constant.92084, s32[] %constant.92090), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220387: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92091 = f32[] constant(1)\r\n2021-10-08 23:57:50.220397: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92093 = f32[] convert(s32[] %constant.92084)\r\n2021-10-08 23:57:50.220408: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92094 = f32[] divide(f32[] %constant.92091, f32[] %convert.92093)\r\n2021-10-08 23:57:50.220418: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92095 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220465: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92096 = f32[] select(pred[] %compare.92092, f32[] %divide.92094, f32[] %constant.92095)\r\n2021-10-08 23:57:50.220482: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92097 = f32[] multiply(f32[] %reduce.92089, f32[] %select.92096)\r\n2021-10-08 23:57:50.220494: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92098 = f32[] convert(f32[] %multiply.92097)\r\n2021-10-08 23:57:50.220504: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134450 = f32[1]{0} reshape(f32[] %convert.92098)\r\n2021-10-08 23:57:50.220515: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92063 = f32[1]{0} reshape(f32[] %p3147.47082)\r\n2021-10-08 23:57:50.220525: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92064 = f32[1]{0} concatenate(f32[1]{0} %reshape.92063), dimensions={0}\r\n2021-10-08 23:57:50.220535: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92065 = f32[] constant(0)\r\n2021-10-08 23:57:50.220545: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92071 = f32[] reduce(f32[1]{0} %concatenate.92064, f32[] %constant.92065), dimensions={0}, to_apply=%AddComputation.92067\r\n2021-10-08 23:57:50.220556: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92066 = s32[] constant(1)\r\n2021-10-08 23:57:50.220566: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92072 = s32[] constant(0)\r\n2021-10-08 23:57:50.220576: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92074 = pred[] compare(s32[] %constant.92066, s32[] %constant.92072), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220587: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92073 = f32[] constant(1)\r\n2021-10-08 23:57:50.220598: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92075 = f32[] convert(s32[] %constant.92066)\r\n2021-10-08 23:57:50.220608: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92076 = f32[] divide(f32[] %constant.92073, f32[] %convert.92075)\r\n2021-10-08 23:57:50.220618: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92077 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220629: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92078 = f32[] select(pred[] %compare.92074, f32[] %divide.92076, f32[] %constant.92077)\r\n2021-10-08 23:57:50.220640: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92079 = f32[] multiply(f32[] %reduce.92071, f32[] %select.92078)\r\n2021-10-08 23:57:50.220650: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92080 = f32[] convert(f32[] %multiply.92079)\r\n2021-10-08 23:57:50.220660: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134451 = f32[1]{0} reshape(f32[] %convert.92080)\r\n2021-10-08 23:57:50.220670: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92045 = f32[1]{0} reshape(f32[] %p3146.47063)\r\n2021-10-08 23:57:50.220680: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92046 = f32[1]{0} concatenate(f32[1]{0} %reshape.92045), dimensions={0}\r\n2021-10-08 23:57:50.220691: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92047 = f32[] constant(0)\r\n2021-10-08 23:57:50.220701: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92053 = f32[] reduce(f32[1]{0} %concatenate.92046, f32[] %constant.92047), dimensions={0}, to_apply=%AddComputation.92049\r\n2021-10-08 23:57:50.220711: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92048 = s32[] constant(1)\r\n2021-10-08 23:57:50.220722: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92054 = s32[] constant(0)\r\n2021-10-08 23:57:50.220733: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92056 = pred[] compare(s32[] %constant.92048, s32[] %constant.92054), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220759: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92055 = f32[] constant(1)\r\n2021-10-08 23:57:50.220770: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92057 = f32[] convert(s32[] %constant.92048)\r\n2021-10-08 23:57:50.220781: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92058 = f32[] divide(f32[] %constant.92055, f32[] %convert.92057)\r\n2021-10-08 23:57:50.220792: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92059 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220803: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92060 = f32[] select(pred[] %compare.92056, f32[] %divide.92058, f32[] %constant.92059)\r\n2021-10-08 23:57:50.220813: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92061 = f32[] multiply(f32[] %reduce.92053, f32[] %select.92060)\r\n2021-10-08 23:57:50.220823: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92062 = f32[] convert(f32[] %multiply.92061)\r\n2021-10-08 23:57:50.220833: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134452 = f32[1]{0} reshape(f32[] %convert.92062)\r\n2021-10-08 23:57:50.220843: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92027 = f32[1]{0} reshape(f32[] %p3145.47044)\r\n2021-10-08 23:57:50.220854: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92028 = f32[1]{0} concatenate(f32[1]{0} %reshape.92027), dimensions={0}\r\n2021-10-08 23:57:50.220865: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92029 = f32[] constant(0)\r\n2021-10-08 23:57:50.220876: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92035 = f32[] reduce(f32[1]{0} %concatenate.92028, f32[] %constant.92029), dimensions={0}, to_apply=%AddComputation.92031\r\n2021-10-08 23:57:50.220888: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92030 = s32[] constant(1)\r\n2021-10-08 23:57:50.220899: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92036 = s32[] constant(0)\r\n2021-10-08 23:57:50.220910: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92038 = pred[] compare(s32[] %constant.92030, s32[] %constant.92036), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220921: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92037 = f32[] constant(1)\r\n2021-10-08 23:57:50.220932: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92039 = f32[] convert(s32[] %constant.92030)\r\n2021-10-08 23:57:50.220942: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92040 = f32[] divide(f32[] %constant.92037, f32[] %convert.92039)\r\n2021-10-08 23:57:50.220953: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92041 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220964: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92042 = f32[] select(pred[] %compare.92038, f32[] %divide.92040, f32[] %constant.92041)\r\n2021-10-08 23:57:50.220975: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92043 = f32[] multiply(f32[] %reduce.92035, f32[] %select.92042)\r\n2021-10-08 23:57:50.220986: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92044 = f32[] convert(f32[] %multiply.92043)\r\n```\r\nThis text goes on and on for several pages.\r\n\r\nThe first epoch runs fine at first and just as the validation loop starts, the training crashes and this text is printed as output.\r\n\r\nNote that this only happens when using a logger (wandb or comet.ml) and everything works fine when I do `self.print` or normal `print` as evident in this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-no-logging\/).\r\n\r\n> I have also tried adding very small batch sizes so this probably isn't a memory issue\r\n\r\n### To Reproduce\r\n\r\nSee this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-resnet200d) that uses wandb and [this](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-comet-ml) with comet.ml.\r\n\r\n### Expected behavior\r\n\r\nTraining should run normally with no issues and logging should work.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1+cpu\r\n\t- pytorch-lightning: 1.4.4\r\n\t- tqdm:              4.62.1\r\n\t- pytorch-xla  1.7\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\r\n### Additional context\r\nNone\r\n\n\ncc @kaushikb11 @rohitgr7 @awaelchli @morganmcg1 @AyushExel @borisdayma @scottire",
        "Challenge_closed_time":1642181.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633792312000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/9879",
        "Challenge_link_count":3,
        "Challenge_participation_count":11,
        "Challenge_readability":16.4,
        "Challenge_reading_time":156.3,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":236,
        "Challenge_solved_time":null,
        "Challenge_title":"\"dumps computation\" at the start of validation loop when using wandb\/comet.ml logger during multi-core tpu training",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":786,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks @rusty-electron for opening the issue.\r\n\r\nIs there any more information before the line \"Dumping Computation:\"?  No error output, just the logs from wandb logger and the progressbars created by `tqdm`. Dear @rusty-electron,\r\n\r\nWe are working with the Wandb Team on a large fix. Hopefully it will work for this use-case too.\r\n\r\nWe will keep you updated.\r\n\r\nBest,\r\nT.C @tchaton Thanks for the info. I shall be looking out for the fix. @tchaton Is there an issue to track the Wandb updates? @borisdayma Any idea ?\r\n It's actually a few different PR's ongoing.\r\nI think we should have something next week that will handle these scenarios. This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n @borisdayma Did it end up being an issue on the wandb side? I didn't follow the development lately. If it's still work in progress, could you point us to a PR or issue? Thx in advance <3  We're actually still in the process of updating the way multiprocess is supported.\r\nThere's been good progress, just a few edge cases to handle. This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":16.94,
        "Solution_score_count":null,
        "Solution_sentence_count":23.0,
        "Solution_word_count":238.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I see one document mentioned time series training can be done with AutoML: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-forecast\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-forecast<\/a> is that any sample which from basic build of model? <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1659301076627,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/949086\/time-series-training",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":15.2,
        "Challenge_reading_time":4.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"time series training",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=6ec12307-1a06-4236-b249-3fd890a3f2a1\">@matsuoka  <\/a>     <\/p>\n<p>Thanks for using Microsoft Q&amp;A platform, we don't have any samples for basic build of a model in AutoML, but we do have quick start for how to use time series in AutoML - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-automated-ml-forecast\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-automated-ml-forecast<\/a>    <\/p>\n<p>This is a low code sample for beginning user. Please take a look.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.7,
        "Solution_reading_time":8.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1509672524840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":7512.4283525,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When trying to run a ScriptRunConfig, using :<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>src = ScriptRunConfig(source_directory=project_folder, \n                      script='train.py', \n                      arguments=['--input-data-dir', ds.as_mount(),\n                                 '--reg', '0.99'],\n                      run_config=run_config) \nrun = experiment.submit(config=src)\n<\/code><\/pre>\n\n<p>It doesn't work and breaks with this when I submit the job : <\/p>\n\n<pre><code>... lots of things... and then\nTypeError: Object of type 'DataReference' is not JSON serializable\n<\/code><\/pre>\n\n<p>However if I run it with the Estimator, it works. One of the differences is the fact that with a <code>ScriptRunConfig<\/code> we're using a list for parameters and the other is a dictionary.<\/p>\n\n<p>Thanks for any pointers!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568929720367,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58019308",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":10.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"ScriptRunConfig with datastore reference on AML",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1541.0,
        "Challenge_word_count":92,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1538275960603,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Montreal, QC, Canada",
        "Poster_reputation_count":381.0,
        "Poster_view_count":50.0,
        "Solution_body":"<p>Being able to use <code>DataReference<\/code> in <code>ScriptRunConfig<\/code> is a bit more involved than doing just <code>ds.as_mount()<\/code>. You will need to convert it into a string in <code>arguments<\/code> and then update the <code>RunConfiguration<\/code>'s <code>data_references<\/code> section with the <code>DataReferenceConfiguration<\/code> created from <code>ds<\/code>. Please <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb\" rel=\"nofollow noreferrer\">see here<\/a> for an example notebook on how to do that.<\/p>\n<p>If you are just reading from the input location and not doing any writes to it, please check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-register-datasets\" rel=\"nofollow noreferrer\"><code>Dataset<\/code><\/a>. It allows you to do exactly what you are doing without doing anything extra. <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/datasets-tutorial\/train-with-datasets.ipynb\" rel=\"nofollow noreferrer\">Here is an example notebook<\/a> that shows this in action.<\/p>\n<p>Below is a short version of the notebook<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Dataset\n\n# more imports and code\n\nds = Datastore(workspace, 'mydatastore')\ndataset = Dataset.File.from_files(path=(ds, 'path\/to\/input-data\/within-datastore'))\n\nsrc = ScriptRunConfig(source_directory=project_folder, \n                      script='train.py', \n                      arguments=['--input-data-dir', dataset.as_named_input('input').as_mount(),\n                                 '--reg', '0.99'],\n                      run_config=run_config) \nrun = experiment.submit(config=src)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1595974462436,
        "Solution_link_count":3.0,
        "Solution_readability":17.8,
        "Solution_reading_time":23.02,
        "Solution_score_count":4.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":140.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hey!    <br \/>\nThe VS code integration in the AML ecosystem is great.    <br \/>\nIs it possible to debug using the synapse spark pool as it attached compute (like I am running notebook on it)    <br \/>\nCurrently from the VS code only compute instance is supported .    <br \/>\nAm I missing something?    <\/p>\n<p>Thanks,    <br \/>\nMaya<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669791827530,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1109976\/aml-vs-code-integration-with-synapse-spark-pool-as",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.9,
        "Challenge_reading_time":4.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"AML VS Code integration with synapse spark pool as attached computes",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":64,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=ce96d2b0-635e-4ed7-b685-15e43ebb5007\">@Maya Shauli  <\/a>,    <\/p>\n<p>Thanks for the question and using MS platform.    <\/p>\n<blockquote>\n<p>Unfortunately,  synapse spark pool as it attached compute on Visual Code is no longer supported.    <\/p>\n<\/blockquote>\n<p>Appreciate if you could share the feedback on our <a href=\"https:\/\/feedback.azure.com\/d365community\/idea\/184e6a07-14fa-ec11-a81b-6045bd853c94\">feedback channel<\/a>. Which would be open for the user community to upvote &amp; comment on. This allows our product teams to effectively prioritize your request against our existing feature backlog and gives insight into the potential impact of implementing the suggested feature.    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is jhow you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/support\/community-champions-program\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":12.5,
        "Solution_reading_time":24.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":193.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,    <br \/>\nI created custom Azure AI model what I would like to use in PowerBI.    <br \/>\nWhen I open a dataset in PowerBI and after select the &quot;Azure Machine learning&quot; after the pop-up window is empty but I suppose it should contain my custom model(s).    <br \/>\nI followed the below articles:    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy<\/a>    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/power-bi\/connect-data\/service-aml-integrate\">https:\/\/learn.microsoft.com\/en-us\/power-bi\/connect-data\/service-aml-integrate<\/a>    <\/p>\n<p>Kind regards    <br \/>\nTom    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/162671-powerbi-azure-ai.png?platform=QnA\" alt=\"162671-powerbi-azure-ai.png\" \/>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/162606-azure-ai-model.png?platform=QnA\" alt=\"162606-azure-ai-model.png\" \/>    <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641415247077,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/684845\/why-does-powerbi-not-see-my-custom-azure-ai-model",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":19.6,
        "Challenge_reading_time":14.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Why does PowerBI not see my custom Azure AI model?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The product group for Power Bi actively monitors questions over at    <br \/>\n<a href=\"https:\/\/community.powerbi.com\/\">https:\/\/community.powerbi.com\/<\/a>       <\/p>\n<p>--please don't forget to <code>upvote<\/code> and <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/145510-image.png?platform=QnA\" alt=\"145510-image.png\" \/> if the reply is helpful--    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.8,
        "Solution_reading_time":4.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1491898605956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sydney NSW, Australia",
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The following code snippet is inspired by <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">this<\/a>.<\/p>\n<pre><code>hyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;reg:squarederror&quot;,\n        &quot;num_round&quot;:&quot;10&quot;}\n\noutput_path = 's3:\/\/{}\/{}\/output'.format(s3_bucket_name, s3_prefix)\n\nestimator = sagemaker.estimator.Estimator(image_uri=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;), \n                                          hyperparameters=hyperparameters,\n                                          role=role,\n                                          instance_count=1, \n                                          instance_type='ml.m5.2xlarge', \n                                          volume_size=1, # 1 GB \n                                          output_path=output_path)\n\nestimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n<\/code><\/pre>\n<p>It works fine. I was trying to use:<\/p>\n<pre><code>training_image_name = image_uris.retrieve(framework='xgboost', region=region_name, version='latest')\n<\/code><\/pre>\n<p>instead of:<\/p>\n<pre><code>sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;)\n<\/code><\/pre>\n<p>to (I believe) get hold of the latest training image but reg:squarederror is not supported? Is my code to get hold of the latest image name incorrect?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649933783303,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71870508",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":19.7,
        "Challenge_reading_time":18.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"latest aws xgb image does not support reg:",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":26.0,
        "Challenge_word_count":92,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>Using &quot;latest&quot; it not suggested as per documentation(see note): <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html<\/a><\/p>\n<p>Use specific versions as they are more stable.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":32.0,
        "Solution_reading_time":4.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":8,
        "Challenge_body":"<p>Hi,<br>\nI was just trying to log a 2D bounding box with pixel coordinates, but I keep on running into this error:<br>\n<code>TypeError: Object of type int is not JSON serializable<\/code><\/p>\n<p>The code I used:<\/p>\n<pre><code class=\"lang-auto\">box_data = []\n\nclass_labels = {\n    0: \"face\"\n}\n\nfor (x,y,w,h) in face_rects:\n    frame = cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n\n    midX = int(x+w\/2)\n    midY = int(y+h\/2) \n    box = {\n                \"position\": {\n                    \"middle\": [midX, midY],\n                    \"width\": w,\n                    \"height\": h\n                },\n                \"domain\" : \"pixel\",\n                \"class_id\" : 0\n            }\n    box_data.append(box)\n\npredictions = {\"predictions\": {\n        \"box_data\": box_data,\n        \"class_labels\": class_labels\n    }\n    }\n\nimg = wandb.Image(frame, boxes=predictions)\n<\/code><\/pre>\n<p>It works when I\u2019m using the relational notation instead of pixel values, but I\u2019d rather keep the pixel values for simplicity in the code.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656620400089,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/typeerror-when-uploading-pixel-value-bounding-box\/2684",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":13.8,
        "Challenge_reading_time":11.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"TypeError when uploading pixel value bounding box",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":497.0,
        "Challenge_word_count":104,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey Mohammad,<br>\nactually today I came back to the project and found something interesting about the bounding box.<br>\nActually, it is not outside of the dimensions of the frame set and I can do a little trick to get the right size of the bounding box working:<\/p>\n<pre><code class=\"lang-auto\"> box = {\n                \"position\": {\n                    \"middle\": [midX, midY],\n                    \"width\": (w\/2)+(w\/2),\n                    \"height\": (h\/2)+(h\/2)\n                },\n\n<\/code><\/pre>\n<p>So what is the difference here? After dividing, the coordinates are <code>float<\/code> values and not <code>int<\/code> anymore and this makes it work(can also just cast them). Sounds like a bug and I just created an issue for it: <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/3982\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">[CLI]: Logging bounding boxes only works with float dimensions, not int \u00b7 Issue #3982 \u00b7 wandb\/wandb \u00b7 GitHub<\/a><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.4,
        "Solution_reading_time":10.8,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":117.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":148.3679175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have written a pipeline that I want to run on a remote compute cluster within Azure Machine Learning. My aim is to process a large amount of historical data, and to do this I will need to run the pipeline on a large number of input parameter combinations.<\/p>\n\n<p>Is there a way to restrict the number of nodes that the pipeline uses on the cluster? By default it will use all the nodes available to the cluster, and I would like to restrict it so that it only uses a pre-defined maximum. This allows me to leave the rest of the cluster free for other users.<\/p>\n\n<p>My current code to start the pipeline looks like this:<\/p>\n\n<pre><code># Setup the pipeline\nsteps = [data_import_step] # Contains PythonScriptStep\npipeline = Pipeline(workspace=ws, steps=steps)\npipeline.validate()\n\n# Big long list of historical dates that I want to process data for\ndts = pd.date_range('2019-01-01', '2020-01-01', freq='6H', closed='left')\n# Submit the pipeline job\nfor dt in dts:\n    pipeline_run = Experiment(ws, 'my-pipeline-run').submit(\n        pipeline,\n        pipeline_parameters={\n            'import_datetime': dt.strftime('%Y-%m-%dT%H:00'),\n        }\n    )\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1592308824897,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62407943",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.9,
        "Challenge_reading_time":14.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Restrict the number of nodes used by an Azure Machine Learning pipeine",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":274.0,
        "Challenge_word_count":174,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403698315160,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":1534.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>For me, the killer feature of Azure ML is not having to worry about load balancing like this. Our team has a compute target with <code>max_nodes=100<\/code> for every feature branch and we have <code>Hyperdrive<\/code> pipelines that result in 130 runs for each pipeline.<\/p>\n<p>We can submit multiple <code>PipelineRun<\/code>s back-to-back and the orchestrator does the heavy lifting of queuing, submitting, all the runs so that the <code>PipelineRun<\/code>s execute in the serial order I submitted them, and that the cluster is never overloaded. This works without issue for us 99% of the time.<\/p>\n<p>If what you're looking for is that you'd like the <code>PipelineRun<\/code>s to be executed in parallel, then you should check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-parallel-run-step#build-and-run-the-pipeline-containing-parallelrunstep\" rel=\"nofollow noreferrer\"><code>ParallelRunStep<\/code><\/a>.<\/p>\n<p>Another option is to isolate your computes. You can have up to 200 <code>ComputeTarget<\/code>s per workspace. Two 50-node <code>ComputeTarget<\/code>s cost the same as one 100-node <code>ComputeTarget<\/code>.<\/p>\n<p>On our team, we use <a href=\"https:\/\/www.pygit2.org\/\" rel=\"nofollow noreferrer\"><code>pygit2<\/code><\/a> to have a <code>ComputeTarget<\/code> created for each feature branch, so that, as data scientists, we can be confident that we're not stepping on our coworkers' toes.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1592842949400,
        "Solution_link_count":2.0,
        "Solution_readability":11.9,
        "Solution_reading_time":18.57,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":177.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1509012479112,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Belo Horizonte, MG, Brasil",
        "Answerer_reputation_count":97.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use de AWS SageMaker Training Jobs console to train a model with H2o.AutoMl.<\/p>\n\n<p>I got stuck trying to set up Hyperparameters, specifically setting up the 'training' field.<\/p>\n\n<pre><code>{'classification': true, 'categorical_columns':'', 'target': 'label'}\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9uR6Y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9uR6Y.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I'm trying to set up a classification training job (1\/0), and I believe that everything else on the setup page I can cope, but I don't know how to set up the 'training' field. My data is stored on S3 as a CSV file, as the algorithm requires.<\/p>\n\n<p>My data has around 250000 columns, 4 out of them are categorical, one of them is the target, and the remainder is continuous variables (800 MB)<\/p>\n\n<pre><code>target column name = 'y'\ncategorical columns name = 'SIT','HOL','CTH','YTT'\n<\/code><\/pre>\n\n<p>I hope someone could help me.<\/p>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568683965203,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57966245",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":13.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How to hyperparametrize Amazon SageMaker Training Jobs Console",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":143,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509012479112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>After I asked I came across an explanation from <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/aws_marketplace\/using_algorithms\/automl\/AutoML_-_Train_multiple_models_in_parallel.ipynb\" rel=\"nofollow noreferrer\">SageMaker examples.<\/a><\/p>\n\n<p>{classification': 'true', 'categorical_columns': 'SIT','HOL','CTH','YTT','target': 'y'}.<\/p>\n\n<p>Problem solved!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":31.3,
        "Solution_reading_time":5.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1477057589223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Columbus, OH, United States",
        "Answerer_reputation_count":547.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Azure webjob that is calling a ML training experiment via HttpRequests, leveraging the code generated in the ML webportal:<\/p>\n\n<pre><code>var request = new BatchExecutionRequest()\n            {\n                Inputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"input1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{trainingDataFileName}\"\n                        }\n                    },\n                },\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"output1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = \"azureStorageConnectionString\",\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/Model_2018421.ilearner\"\n                        }\n                    },\n                },\n\n                GlobalParameters = new Dictionary&lt;string, string&gt;()\n                {\n                }\n            };\n<\/code><\/pre>\n\n<p>However, the request fails with the following message:<\/p>\n\n<blockquote>\n  <p>The blob reference:\n  experiments\/experimentId\/TenantId\/Model_2018421.ilearner\n  has an invalid or missing file extension. Supported file extensions\n  for this output type are: \\\\\".csv, .tsv, .arff\\\\\"<\/p>\n<\/blockquote>\n\n<p>I'm pretty confused about this, since it's written right the documentation all over the place that if I'm expecting a trained model to use \".ilearner\" as the file extension for the model.<\/p>\n\n<p>I've seen <a href=\"https:\/\/stackoverflow.com\/questions\/47920098\/use-azure-data-factory-updating-azure-machine-learning-models\">this question<\/a> asking about the same error leveraging the DataFactory, and also <a href=\"https:\/\/datascience.stackexchange.com\/questions\/27397\/azure-machine-learning-model-retraining-problem\">this question on datascience.stackexchange<\/a>. Neither one had any clues, answers, or other follow up.<\/p>\n\n<p>Any insight on what I'm missing would be greatly appreciated!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525714637430,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50219664",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.0,
        "Challenge_reading_time":24.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Experiment Batch Webservice Call Fails with Invalid Output Extension",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":83.0,
        "Challenge_word_count":167,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1477057589223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Columbus, OH, United States",
        "Poster_reputation_count":547.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>For anyone looking for your \"Don't Overthink It\" moment of the day:<\/p>\n\n<p>I needed to provide TWO output blob file references:<\/p>\n\n<pre><code>var request = new BatchExecutionRequest()\n            {\n                Inputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"input1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{trainingDataFileName}.csv\"\n                        }\n                    },\n                },\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"output1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{outputFileNameCsv}.csv\"\n                        }\n                    },\n                    {\n                        \"output2\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{outputFileNameIlearner}.ilearner\"\n                        }\n                    },\n                },\n\n                GlobalParameters = new Dictionary&lt;string, string&gt;()\n                {\n                }\n            };\n<\/code><\/pre>\n\n<p>There's an old saying in American English about not making assumptions, and I assumed the second output was an optional parameter used in batch operations. Since I'm not actually looking for more than one result from each call, I thought I was safe to remove the second output parameter.<\/p>\n\n<p>TL\/DR: Keep all the parameters the webservice portal's \"Consume\" tab generates, and make sure the first one is a .csv file reference.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.0,
        "Solution_reading_time":17.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":130.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I found that I can register the model using Mlflow, but I don't know how to register it in ONNX format.    <br \/>\nI found out that the model is registered using Mlflow.    <br \/>\nBut I don't know how to convert AutoML models to ONNX format and register them with Mlflow.    <\/p>\n<p>from azure.ai.ml import MLClient    <br \/>\nfrom azure.identity import DefaultAzureCredential    <br \/>\nfrom azureml.train.automl import AutoMLConfig    <br \/>\nfrom azureml.core import Workspace, Dataset    <br \/>\nfrom azureml.core.experiment import Experiment    <br \/>\nfrom azureml.core.model import Model    <br \/>\nfrom azureml.core.authentication import ServicePrincipalAuthentication    <br \/>\nfrom azureml.automl.runtime.onnx_convert import OnnxConverter    <br \/>\nfrom random import random    <br \/>\nfrom mlflow.tracking import MlflowClient    <br \/>\nimport mlflow    <br \/>\nimport mlflow.onnx    <br \/>\nimport os    <br \/>\nimport azureml.mlflow    <\/p>\n<p>auth = ServicePrincipalAuthentication(    <br \/>\n    tenant_id=&quot;&quot;,  <br \/>\n    service_principal_id=&quot;&quot;,  <br \/>\n    service_principal_password=&quot;&quot;)  <\/p>\n<p>subscription_id = ''    <br \/>\nresource_group = ''    <br \/>\nworkspace_name = ''    <\/p>\n<p>ml_client = MLClient(credential=auth,    <br \/>\n                    subscription_id=subscription_id,  <br \/>\n                    resource_group_name=resource_group)  <\/p>\n<p>azure_mlflow_uri = ml_client.workspaces.get(workspace_name).mlflow_tracking_uri    <br \/>\nmlflow.set_tracking_uri(azure_mlflow_uri)    <\/p>\n<p>ws = Workspace(subscription_id, resource_group, workspace_name, auth=auth)    <\/p>\n<p>train_data = Dataset.get_by_name(ws, name='iris')    <\/p>\n<p>label = &quot;class&quot;    <\/p>\n<p>automl_settings = {    <br \/>\n    &quot;primary_metric&quot;: 'AUC_weighted',  <br \/>\n    &quot;n_cross_validations&quot;: 2  <br \/>\n    }  <\/p>\n<p>automl_classifier = AutoMLConfig(    <br \/>\n    task='classification',  <br \/>\n    blocked_models=['XGBoostClassifier'],  <br \/>\n    enable_onnx_compatible_models=True,  <br \/>\n    experiment_timeout_minutes=30,  <br \/>\n    training_data=train_data,  <br \/>\n    label_column_name=label,  <br \/>\n    **automl_settings  <br \/>\n    )  <\/p>\n<p>experiment_name = 'experimetn_with_mlflow'    <br \/>\nmlflow.set_experiment(experiment_name)    <br \/>\nexperiment = Experiment(ws, experiment_name)    <\/p>\n<p>with mlflow.start_run() as mlflow_run:    <br \/>\n    mlflow.log_metric(&quot;iris_metric&quot;, random())  <\/p>\n<pre><code>mlflow_run = experiment.submit(automl_classifier, show_output=True)  \n\ndescription = 'iris_Description'  \n\nmodel = mlflow_run.register_model(description=description,  \n                               model_name='iris_Model')  \n\nbest_run, onnx_mdl = mlflow_run.get_output(return_onnx_model=True)  \nonnx_fl_path = &quot;.\/best_model.onnx&quot;  \nOnnxConverter.save_onnx_model(onnx_mdl, onnx_fl_path)  \n\nmodel = Model.register(workspace=ws,  \n                    description=description,  \n                    model_name='iris_onnx_model',  \n                    model_path=onnx_fl_path)  \n\nclient = MlflowClient()  \n\nfinished_mlflow_run = MlflowClient().get_run(mlflow_run.run_id)  \n\nmetrics = finished_mlflow_run.data.metrics  \ntags = finished_mlflow_run.data.tags  \nparams = finished_mlflow_run.data.params  \n\nmodel_path  = &quot;best_model&quot;  \nmodel_uri = 'runs:\/{}\/{}'.format(mlflow_run.run_id, model_path)  \nmlflow.register_model(model_uri, 'iris_onnx_mlflow_model')\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664411309103,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1027830\/i-want-to-register-the-model-learned-by-automl-in",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":17.0,
        "Challenge_reading_time":43.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"I want to register the model learned by AutoML in Azure Machine learning in ONNX format and call it in Azure Synapse Analitics.",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":258,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=cef675e6-0d34-4e7e-873a-aec3478f614f\">@\u4fdd\u53f2 \u7d30\u898b  <\/a> Thanks for the question.  Can you please share document\/sample that you are trying. In order to save trained model download (and score) as the ONNX model you have here a few code <a href=\"https:\/\/github.com\/CESARDELATORRE\/azureml-workshop-2019\/blob\/master\/2-training-inference\/2.3-automl-training\/local-compute\/binayclassification-employee-attrition-autoaml-local-compute.ipynb\">examples<\/a>.    <br \/>\nMLflow model registry will enable Synapse to run ONNX models is in preview.    <\/p>\n<p>Here is the ONNX prediction section in the sample <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/classification-bank-marketing-all-features\/auto-ml-classification-bank-marketing-all-features.ipynb\">notebook<\/a>.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.3,
        "Solution_reading_time":11.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":65.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1534481301200,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Virginia, US",
        "Answerer_reputation_count":7114.0,
        "Answerer_view_count":1111.0,
        "Challenge_adjusted_solved_time":0.1925833333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a bunch of json files that look like this<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n<\/code><\/pre>\n<p>Which I can read in with<\/p>\n<pre><code>f = open(file_name)\ndata = []\nfor line in f:\n   data.append(json.dumps(line))\n<\/code><\/pre>\n<p>But I have another file with output like this<\/p>\n<pre><code>{\n    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]\n    ]\n}\n<\/code><\/pre>\n<p>I.e. the json is formatted over several lines, so I can't simply read the json in line for line. Is there an easy way to parse this? Or do I have to write something that stitches together each json object line by line and the does json.loads?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635629350997,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1635655049276,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69782294",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.6,
        "Challenge_reading_time":17.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker output how to read file with multiple json objects spread out over multiple lines",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":248.0,
        "Challenge_word_count":145,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421343783700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1387.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>Hmm,  as far as I know there's unfortunately no way to load a <a href=\"https:\/\/jsonlines.org\/\" rel=\"nofollow noreferrer\">JSONL<\/a> format data using <code>json.loads<\/code>. One option though, is to come up with a helper function that can convert it to a valid JSON string, as below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import json\n\nstring = &quot;&quot;&quot;\n{\n    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]\n    ]\n}\n&quot;&quot;&quot;\n\n\ndef json_lines_to_json(s: str) -&gt; str:\n    # replace the first occurrence of '{'\n    s = s.replace('{', '[{', 1)\n\n    # replace the last occurrence of '}\n    s = s.rsplit('}', 1)[0] + '}]'\n\n    # now go in and replace all occurrences of '}' immediately followed\n    # by newline with a '},'\n    s = s.replace('}\\n', '},\\n')\n\n    return s\n\n\nprint(json.loads(json_lines_to_json(string)))\n<\/code><\/pre>\n<p>Prints:<\/p>\n<pre><code>[{'predictions': [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]]}, {'predictions': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]}, {'predictions': [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]]}]\n<\/code><\/pre>\n<p><strong>Note:<\/strong> your first example actually doesn't seem like valid JSON (or at least JSON lines from my understanding). In particular, this part appears to be invalid due to a trailing comma after the last array element:<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], ...}\n<\/code><\/pre>\n<p>To ensure it's valid after calling the helper function, you'd also need to remove the trailing commas, so each line is in the below format:<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], ...},\n<\/code><\/pre>\n<hr \/>\n<p>There also appears to be a <a href=\"https:\/\/stackoverflow.com\/questions\/50475635\/loading-jsonl-file-as-json-objects\/50475669\">similar question<\/a> where they suggest splitting on newlines and calling <code>json.loads<\/code> on each line; actually it should be (slightly) less performant to call <code>json.loads<\/code> multiple times on each object, rather than once on the list, as I show below.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from timeit import timeit\nimport json\n\n\nstring = &quot;&quot;&quot;\\\n{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142 ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774 ], &quot;word&quot;: &quot;blah blah blah&quot;}\\\n&quot;&quot;&quot;\n\n\ndef json_lines_to_json(s: str) -&gt; str:\n\n    # Strip newlines from end, then replace all occurrences of '}' followed\n    # by a newline, by a '},' followed by a newline.\n    s = s.rstrip('\\n').replace('}\\n', '},\\n')\n\n    # return string value wrapped in brackets (list)\n    return f'[{s}]'\n\n\nn = 10_000\n\nprint('string replace:        ', timeit(r'json.loads(json_lines_to_json(string))', number=n, globals=globals()))\nprint('json.loads each line:  ', timeit(r'[json.loads(line) for line in string.split(&quot;\\n&quot;)]', number=n, globals=globals()))\n<\/code><\/pre>\n<p>Result:<\/p>\n<pre><code>string replace:         0.07599360000000001\njson.loads each line:   0.1078384\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1635655742576,
        "Solution_link_count":2.0,
        "Solution_readability":9.2,
        "Solution_reading_time":46.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":352.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In colab, whenever we need GPU, we simply click <code>change runtime type<\/code> and change hardware accelarator to <code>GPU<\/code><\/p>\n<p>and cuda becomes available, <code>torch.cuda.is_available()<\/code> is <code>True<\/code><\/p>\n<p>How to do this is AWS sagemaker, i.e. turning on cuda.\nI am new to AWS and trying to train model using pytorch in aws sagemaker, where Pytorch code is first tested in colab environment.<\/p>\n<p>my sagemaker notebook insatnce is <code>ml.t2.medium<\/code><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617348836210,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1617349698827,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66915920",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":6.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Using pytorch cuda in AWS sagemaker notebook instance",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1608.0,
        "Challenge_word_count":73,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1567880532003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lahore, Pakistan",
        "Poster_reputation_count":137.0,
        "Poster_view_count":100.0,
        "Solution_body":"<p>Using AWS Sagemaker you don't need to worry about the GPU, you simply select an instance type with GPU ans Sagemaker will use it. Specifically <code>ml.t2.medium<\/code> doesn't have a GPU but it's anyway not the right way to train a model.\nBasically you have 2 canonical ways to use Sagemaker (look at the documentation and examples please), the first is to use a notebook with a limited computing resource to spin up a training job using a prebuilt image, in that case when you call the estimator you simply specify what <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/\" rel=\"nofollow noreferrer\">instance type<\/a> you want (you'll choose one with GPU, looking at the costs). The second way is to use your own container, push it to ECR and launch a training job from the console, where you specify the instance type.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.6,
        "Solution_reading_time":10.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":134.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1464811778510,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":196.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><a href=\"https:\/\/developer.nvidia.com\/nvidia-triton-inference-server\" rel=\"nofollow noreferrer\">NVIDIA Triton<\/a>\u00a0vs\u00a0<a href=\"https:\/\/pytorch.org\/serve\/\" rel=\"nofollow noreferrer\">TorchServe<\/a>\u00a0for SageMaker inference? When to recommend each?<\/p>\n<p>Both are modern, production grade inference servers. TorchServe is the DLC default inference server for PyTorch models. Triton is also supported for PyTorch inference on SageMaker.<\/p>\n<p>Anyone has a good comparison matrix for both?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663943338403,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73829280",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":7.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"NVIDIA Triton vs TorchServe for SageMaker Inference",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":12.0,
        "Challenge_word_count":57,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>Important notes to add here where both serving stacks differ:<\/p>\n<p>TorchServe does not provide the Instance Groups feature that Triton does (that is, stacking many copies of the same model or even different models onto the same GPU). This is a major advantage for both realtime and batch use-cases, as the performance increase is almost proportional to the model replication count (i.e. 2 copies of the model get you almost twice the throughput and half the latency; check out a BERT benchmark of this here). Hard to match a feature that is almost like having 2+ GPU's for the price of one.\nif you are deploying PyTorch DL models, odds are you often want to accelerate them with GPU's. TensorRT (TRT) is a compiler developed by NVIDIA that automatically quantizes and optimizes your model graph, which represents another huge speed up, depending on GPU architecture and model. It is understandably so probably the best way of automatically optimizing your model to run efficiently on GPU's and make good use of TensorCores. Triton has native integration to run TensorRT engines as they're called (even automatically converting your model to a TRT engine via config file), while TorchServe does not (even though you can use TRT engines with it).\nThere is more parity between both when it comes to other important serving features: both have dynamic batching support, you can define inference DAG's with both (not sure if the latter works with TorchServe on SageMaker without a big hassle), and both support custom code\/handlers instead of just being able to serve a model's forward function.<\/p>\n<p>Finally, MME on GPU (coming shortly) will be based on Triton, which is a valid argument for customers to get familiar with it so that they can quickly leverage this new feature for cost-optimization.<\/p>\n<p>Bottom line I think that Triton is just as easy (if not easier) ot use, a lot more optimized\/integrated for taking full advantage of the underlying hardware (and will be updated to keep being that way as newer GPU architectures are released, enabling an easy move to them), and in general blows TorchServe out of the water performance-wise when its optimization features are used in combination.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.0,
        "Solution_reading_time":27.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":363.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I could really use some help!<\/p>\n\n<p>The company I work for is made up of 52 very different businesses so I can't predict at the company level but instead need to predict business by business then roll up the result to give company wide prediction.<\/p>\n\n<p>I have written an ML model in studio.azureml.net\nIt works great with a 0.947 Coefficient of Determination, but this is for 1 of the businesses.\nI now need to train the model for the other 51.<\/p>\n\n<p>Is there a way to do this in a single ML model rather than having to create 52 very similar models?<\/p>\n\n<p>Any help would be much appreciated !!!<\/p>\n\n<p>Kind Regards\nMartin<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559146852583,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1560930522380,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56364828",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":8.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML - Train a model on segments of the data-set",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":123.0,
        "Challenge_word_count":121,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1300461675980,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":189.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>You can use Ensembles, combining several models to improve predictions. The most direct is stacking when the outputs of all the models are trained on the entire dataset. \nThe method that, I think, corresponds the best to your problem is bagging (bootstrap aggregation). You need to divide the training set into different subsets (each corresponding to a certain business), then train a different model on each subset and combine the result of each classifier. \nAnother way is boosting but it is difficult to implement in Azure ML. \nYou can see an example in <a href=\"https:\/\/gallery.azure.ai\/Experiment\/b6b09fc0c26047e6b4c733ab78a86498\" rel=\"nofollow noreferrer\">Azure ML Gallery<\/a>. <\/p>\n\n<p>Quote from book:<\/p>\n\n<blockquote>\n  <p>Stacking and bagging can be easily implemented in Azure Machine\n  Learning, but other ensemble methods are more difficult. Also, it\n  turns out to be very tedious to implement in Azure Machine Learning an\n  ensemble of, say, more than five models. The experiment is filled with\n  modules and is quite difficult to maintain. Sometimes it is worthwhile\n  to use any ensemble method available in R or Python. Adding more\n  models to an ensemble written in a script can be as trivial as\n  changing a number in the code, instead of copying and pasting modules\n  into the experiment.<\/p>\n<\/blockquote>\n\n<p>You may also have a look at <a href=\"http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\" rel=\"nofollow noreferrer\">sklearn (Python)<\/a> and caret (R) documentation for further details.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.4,
        "Solution_reading_time":18.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":220.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1635045129020,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":53.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm dabbling with ML and was able to take a tutorial and get it to work for my needs.  It's a simple recommender system using TfidfVectorizer and linear_kernel.  I run into a problem with how I go about deploying it through Sagemaker with an end point.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel \nimport json\nimport csv\n\nwith open('data\/big_data.json') as json_file:\n    data = json.load(json_file)\n\nds = pd.DataFrame(data)\n\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(ds['content'])\ncosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n\nresults = {}\n\nfor idx, row in ds.iterrows():\n    similar_indices = cosine_similarities[idx].argsort()[:-100:-1]\n    similar_items = [(cosine_similarities[idx][i], ds['id'][i]) for i in similar_indices]\n\n    results[row['id']] = similar_items[1:]\n\ndef item(id):\n    return ds.loc[ds['id'] == id]['id'].tolist()[0]\n\ndef recommend(item_id, num):\n    print(&quot;Recommending &quot; + str(num) + &quot; products similar to &quot; + item(item_id) + &quot;...&quot;)\n    print(&quot;-------&quot;)\n    recs = results[item_id][:num]\n    for rec in recs:\n        print(&quot;Recommended: &quot; + item(rec[1]) + &quot; (score:&quot; + str(rec[0]) + &quot;)&quot;)\n\nrecommend(item_id='129035', num=5)\n<\/code><\/pre>\n<p>As a starting point I'm not sure if the output from <code>tf.fit_transform(ds['content'])<\/code> is considered the model or the output from <code>linear_kernel(tfidf_matrix, tfidf_matrix)<\/code>.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635046349497,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1635173315200,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69693666",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.2,
        "Challenge_reading_time":21.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"How to Deploy ML Recommender System on AWS",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":63.0,
        "Challenge_word_count":164,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1635045129020,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>I came to the conclusion that I didn't need to deploy this through SageMaker.  Since the final linear_kernel output was a Dictionary I could do quick ID lookups to find correlations.<\/p>\n<p>I have it working on AWS with API Gateway\/Lambda, DynamoDB and an EC2 server to collect, process and plug the data into DynamoDB for fast lookups.  No expensive SageMaker endpoint needed.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":4.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I am doing a comparative analysis of predictive analytics software for my Project. I am looking for approximate lines of code for the Google Vertex AI product.",
        "Challenge_closed_time":1638190.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638115980000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-anyone-tell-what-is-the-approximate-SLOC-of-Google-Vertex-AI\/m-p\/176629#M96",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":3.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Can anyone tell what is the approximate SLOC of Google Vertex AI? For my comparative analysis study.",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":89.0,
        "Challenge_word_count":43,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello,\u00a0\n\nSLOC will be different depending on your specific use case, and you should be in the best position to figure out the approximate SLOC for your scenarios. That being said, you might look into the sample code and notebooks for Vertex AI, the end-to-end machine learning platform on Google Cloud at [1].\n\n[1]\u00a0https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":4.99,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Hello GCP community, I have the following question, I am training in Vertex using a custom container, I am porting pipelines that were in Kubeflow to vertex and using this to train:\n\n\n\nfrom google.cloud import aiplatform\n\njob = aiplatform.CustomContainerTrainingJob(display_name=\"training-job\", container_uri=container_uri)\n# define training code arguments\ntraining_args = [\"--num-epochs\", \"2\", ]\nmodel = job.run(\nreplica_count=1,\nmachine_type=\"n1-standard-8\",\naccelerator_type=\"NVIDIA_TESLA_V100\",\naccelerator_count=1,\nargs=training_args,\nsync=False,\n)\n\nIt looks ok, but here is my question is there anyway in which I can do the training but in a SPOT machine to try to reduce my training costs.\n\nThanks!",
        "Challenge_closed_time":1674130.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1674129120000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Usage-of-spot-machines-while-training-in-Vertex-AI\/m-p\/511862#M1094",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":14.1,
        "Challenge_reading_time":9.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Usage of spot machines while training in Vertex AI",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":370.0,
        "Challenge_word_count":93,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi David\n\nNo unfortunately there is no support for spot \/ preemptible instances with Vertex AI.\u00a0\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":1.54,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is azure working well with Tensorflow framework? I don\u2019t see any document about it. Any help is good.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653989511207,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/871068\/tensorflow-and-azure-machine-learning",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.3,
        "Challenge_reading_time":1.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Tensorflow and Azure machine learning",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":22,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=3d6a9d61-6cf9-45d8-870d-2fbbf147f56d\">@Chungsun  <\/a>    <\/p>\n<p>Welcome to the Microsoft Q&amp;A Platform,    <\/p>\n<p>TensorFlow is supported on Azure Machine Learning:    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-tensorflow\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-tensorflow<\/a>    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/azure-functions\/functions-machine-learning-tensorflow?tabs=bash\">https:\/\/learn.microsoft.com\/en-us\/azure\/azure-functions\/functions-machine-learning-tensorflow?tabs=bash<\/a>    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-keras\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-keras<\/a>    <\/p>\n<p>I hope this helps!      <\/p>\n<p>----------    <\/p>\n<p>Please don\u2019t forget to &quot;<strong>Accept the answer<\/strong>&quot; and \u201c<strong>up-vote<\/strong>\u201d wherever the information provided helps you, this can be beneficial to other community members.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":23.6,
        "Solution_reading_time":14.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":59.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"When running this code https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/commit\/1f67ac4844859e5d60a0f5dba2dbbe8f4c5dbc30 from a colab notebook Wandb views the entire thing as one training session and continue gradient steps indefinitely. Training session should be forced to end when that model stops training not when the meta training loop finishes. Should only be 28 training steps not 80.\r\n<img width=\"1094\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/3865062\/71710653-65567f80-2dcb-11ea-8558-0f3280c4ab7b.png\">\r\n",
        "Challenge_closed_time":1578199.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578034238000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/35",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":9.5,
        "Challenge_reading_time":7.42,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":209.0,
        "Challenge_repo_issue_count":605.0,
        "Challenge_repo_star_count":1230.0,
        "Challenge_repo_watch_count":20.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Wandb bug when running train long",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":59,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Currently working on this should be fixed. Just need to test it So it seems to not be doing steps anymore on the same model run chart, but the runs are still not terminating until the loop ends. Don't really know if this is a problem or not. Going to close this for now Wandb supports up to 50 concurrent runs. So as long as aren't training on more than 50 rivers at a time this shouldn't be an issue. If it becomes one will revert to the subprocess thing but don't want otherwise as with that it doesn't log debugging. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":6.16,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":101.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We have built numerous diagnostic models which can be reduced to equations and code that will allow us to repeat the work. We have the code physically available to us, so it can be installed in our own software.  <\/p>\n<p>Now I would like to use artificial neural networks to build a prediction model. After I build that model, will I be able to take that model and transfer it to our own software environment? My concern is that the prediction model will just be a black box. Thanks<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1591889003457,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/34890\/access-to-neural-network-model",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Access to neural network model",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":93,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a>@OliverBathe-8330<\/a> Please follow the below Deployment scenarios. If possible can you please add more details about the use case.<\/p>\n<p>Option A: Use the DevOps pipeline integration to rollout to production Using same approach as in the <a href=\"https:\/\/github.com\/Microsoft\/MLOpsPython\">MLOps repo<\/a>, set up a <a href=\"https:\/\/github.com\/microsoft\/MLOpsPython\/blob\/master\/docs\/getting_started.md#set-up-build-release-trigger-and-release-multi-stage-pipeline\">release trigger for your DevOps release pipeline<\/a> listening from your dev workspace model registry but then deploy to your production workspace (requires registering again in Prod model registry, call model.deploy() in the Prod workspace<\/p>\n<p>Option B: Use the AML pipeline to rollout to production Following same example as above, add additional PythonScriptStep in your AML pipeline to register and deploy model in the Production workspace<\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where<\/a><\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.3,
        "Solution_reading_time":14.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":110.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1635310523496,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":23.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have type error when I run for training on sagemaker by using xgboost conatiner.\nPlease advise me to fix the issue.<\/p>\n<pre><code>container = 'southeast-2','783357654285.dkr.ecr.ap-southeast-2.amazonaws.com\/sagemaker- xgboost:latest'`\n\ntrain_input = TrainingInput(s3_data='s3:\/\/{}\/train'.format(bucket, prefix), content_type='csv')\nvalidation_input = TrainingInput(s3_data='s3:\/\/{}\/validation\/'.format(bucket, prefix), content_type='csv')\n\nsess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(\ncontainer,\nrole, \ninstance_count=1,\ninstance_type='ml.t2.medium',\noutput_path='s3:\/\/{}\/output'.format(bucket, prefix),\nsagemaker_session=sess\n)\n\nxgb.set_hyperparameters(\nmax_depth=5,\neta=0.1,\ngamma=4,\nmin_child_weight=6,\nsubsample=0.8,\nsilent=0,\nobjective=&quot;binary:logistic&quot;,\nnum_round=25,\n)\n\nxgb.fit({&quot;train&quot;: train_input, &quot;validation&quot;: validation_input})\n<\/code><\/pre>\n<hr \/>\n<p>TypeError                                 Traceback (most recent call last)\n in \n21 )\n22\n---&gt; 23 xgb.fit({&quot;train&quot;: train_input, &quot;validation&quot;: validation_input})<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n685                 * <code>TrialComponentDisplayName<\/code> is used for display in Studio.\n686         &quot;&quot;&quot;\n--&gt; 687         self._prepare_for_training(job_name=job_name)\n688\n689         self.latest_training_job = _TrainingJob.start_new(self, inputs, experiment_config)<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in _prepare_for_training(self, job_name)\n446                 constructor if applicable.\n447         &quot;&quot;&quot;\n--&gt; 448         self._current_job_name = self._get_or_create_name(job_name)\n449\n450         # if output_path was specified we use it otherwise initialize here.<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in _get_or_create_name(self, name)\n435             return name\n436\n--&gt; 437         self._ensure_base_job_name()\n438         return name_from_base(self.base_job_name)\n439<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in _ensure_base_job_name(self)\n420         # honor supplied base_job_name or generate it\n421         if self.base_job_name is None:\n--&gt; 422             self.base_job_name = base_name_from_image(self.training_image_uri())\n423\n424     def _get_or_create_name(self, name=None):<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/utils.py in base_name_from_image(image)\n95         str: Algorithm name, as extracted from the image name.\n96     &quot;&quot;&quot;\n---&gt; 97     m = re.match(&quot;^(.+\/)?([^:\/]+)(:[^:]+)?$&quot;, image)\n98     algo_name = m.group(2) if m else image\n99     return algo_name<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/re.py in match(pattern, string, flags)\n170     &quot;&quot;&quot;Try to apply the pattern at the start of the string, returning\n171     a match object, or None if no match was found.&quot;&quot;&quot;\n--&gt; 172     return _compile(pattern, flags).match(string)\n173\n174 def fullmatch(pattern, string, flags=0):<\/p>\n<p>TypeError: expected string or bytes-like object<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638762253813,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70240640",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":41.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":null,
        "Challenge_title":"xgboost sagemaker train failure",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":130.0,
        "Challenge_word_count":245,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1635310523496,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":3.0,
        "Solution_body":"<pre><code>import sagemaker\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\nfrom sagemaker.session import TrainingInput\nfrom sagemaker import image_uris\nfrom sagemaker.session import Session\n\n# initialize hyperparameters\nhyperparameters = {\n    &quot;max_depth&quot;:&quot;5&quot;,\n    &quot;eta&quot;:&quot;0.1&quot;,\n    &quot;gamma&quot;:&quot;4&quot;,\n    &quot;min_child_weight&quot;:&quot;6&quot;,\n    &quot;subsample&quot;:&quot;0.7&quot;,\n    &quot;objective&quot;:&quot;binary:logistic&quot;,\n    &quot;num_round&quot;:&quot;25&quot;}\n\n# set an output path where the trained model will be saved\nbucket = sagemaker.Session().default_bucket()\noutput_path = 's3:\/\/{}\/{}\/output'.format(bucket, 'rain-xgb-built-in-algo')\n\n\n# this line automatically looks for the XGBoost image URI and builds an \nXGBoost container.\n# specify the repo_version depending on your preference.\nxgboost_container = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, 'ap-southeast- \n2', &quot;1.3-1&quot;)\n\n\n# construct a SageMaker estimator that calls the xgboost-container\nestimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n                                      hyperparameters=hyperparameters,\n                                      role=sagemaker.get_execution_role(),\n                                      instance_count=1, \n                                      instance_type='ml.m5.large', \n                                      volume_size=5, # 5 GB \n                                      output_path=output_path)\n\n\n\n # define the data type and paths to the training and validation datasets\n\n train_input = TrainingInput(&quot;s3:\/\/{}\/{}\/&quot;.format(bucket,'train'), \n content_type='csv')\n validation_input = TrainingInput(&quot;s3:\/\/{}\/{}&quot;.format(bucket,'validation'), \n content_type='csv')\n\n\n # execute the XGBoost training job\n estimator.fit({'train': train_input, 'validation': validation_input})\n<\/code><\/pre>\n<p>I have rewritten as above and could run training.\nthank you !<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":21.5,
        "Solution_reading_time":23.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":132.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm hoping to use SageMaker Training Compiler with a (Hugging Face Trainer API, PyTorch) program split across **multiple .py files** for maintainability. The job needs to run on multiple GPUs (although at the current scale, multi-device single-node would be acceptable).\n\nFollowing [the docs](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/training-compiler-enable.html#training-compiler-enable-pysdk), I added the `distributed_training_launcher.py` launcher script to my `source_dir` bundle, and passed in the true training script via a `training_script` hyperparameter.\n\n...But when the job tries to start, I get:\n\n```\nTraceback (most recent call last):\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\nreturn _run_code(code, main_globals, None,\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 90, in <module>\nmain()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 86, in main\nxmp.spawn(mod._mp_fn, args=(), nprocs=args.num_gpus)\nAttributeError: module 'train' has no attribute '_mp_fn'\n```\n\nAny ideas what might be causing this? Is there some particular limitation or additional requirement for training scripts that are written over multiple files?\n\nI also tried running in single-GPU mode (`p3.2xlarge`) instead - directly calling the train script instead of the distributed launcher - and saw the below error which seems to originate within [TrainingArguments](https:\/\/huggingface.co\/transformers\/v3.0.2\/main_classes\/trainer.html#transformers.TrainingArguments) itself? Not sure why it's trying to call a 'tensorflow\/compiler' compiler when running in PT..?\n\n**EDIT: Turns out the below error can be solved by explicitly setting `n_gpus` as mentioned on the [troubleshooting doc](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/training-compiler-troubleshooting.html#training-compiler-troubleshooting-missing-xla-config) - but that takes me back to the error message above**\n\n```\nFile \"\/opt\/ml\/code\/code\/config.py\", line 124, in __post_init__\nsuper().__post_init__()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 761, in __post_init__\nif is_torch_available() and self.device.type != \"cuda\" and (self.fp16 or self.fp16_full_eval):\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 975, in device\nreturn self._setup_devices\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1754, in __get__\ncached = self.fget(obj)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 918, in _setup_devices\ndevice = xm.xla_device()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 231, in xla_device\ndevices = get_xla_supported_devices(\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 137, in get_xla_supported_devices\nxla_devices = _DEVICES.value\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/utils\/utils.py\", line 32, in value\nself._value = self._gen_fn()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 19, in <lambda>\n_DEVICES = xu.LazyProperty(lambda: torch_xla._XLAC._xla_get_devices())\nRuntimeError: tensorflow\/compiler\/xla\/xla_client\/computation_client.cc:273 : Missing XLA configuration\n```",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639669045329,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667926687612,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUwcM0XER5TcOggtQ_5cfVPw\/multi-file-source-dir-bundle-with-sm-training-compiler-distributed",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":49.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":42,
        "Challenge_solved_time":null,
        "Challenge_title":"Multi-file source_dir bundle with SM Training Compiler (distributed)",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":62.0,
        "Challenge_word_count":326,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Ahh I solved this a while ago and forgot to update -\n\nYes, the training script needs to define a `_mp_fn` (which can just execute the same code as gets run `if __name__ == \"__main__\"`) and number of GPUs (at least the last time I checked - hopefully this could change in future) needs to be explicitly configured.\n\nFor my particular project the fix to enable SMTC on the existing job is available online [here](https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/pull\/14\/commits\/45fa386faa3eee527395251449e6a58e3fb5f13c). For others would also suggest referring to the [official SMTC example notebooks & scripts](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/training-compiler-examples-and-blogs.html)!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1657872107440,
        "Solution_link_count":2.0,
        "Solution_readability":17.9,
        "Solution_reading_time":9.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":85.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1319019150600,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3073.0,
        "Answerer_view_count":341.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use a XGBoost model in Sage Maker and use it to score for a large data stored in S3 using Batch Transform.<\/p>\n<p>I build the model using existing Sagemaker Container as follows:<\/p>\n<pre><code>estimator = sagemaker.estimator.Estimator(image_name=container, \n                                          hyperparameters=hyperparameters,\n                                          role=sagemaker.get_execution_role(),\n                                          train_instance_count=1, \n                                          train_instance_type='ml.m5.2xlarge', \n                                          train_volume_size=5, # 5 GB \n                                          output_path=output_path,\n                                          train_use_spot_instances=True,\n                                          train_max_run=300,\n                                          train_max_wait=600)\n\nestimator.fit({'train': s3_input_train,'validation': s3_input_test})\n<\/code><\/pre>\n<p>The following code is used to do Batch Transform<\/p>\n<pre><code> The location of the test dataset\nbatch_input = 's3:\/\/{}\/{}\/test\/examples'.format(bucket, prefix)\n\n# The location to store the results of the batch transform job\nbatch_output = 's3:\/\/{}\/{}\/batch-inference'.format(bucket, prefix)\n\ntransformer = xgb_model.transformer(instance_count=1, instance_type='ml.m4.xlarge', output_path=batch_output)\n\ntransformer.transform(data=batch_input, data_type='S3Prefix', content_type='text\/csv', split_type='Line')\n\ntransformer.wait()\n<\/code><\/pre>\n<p>The above code works fine in Development environment (Jupyter notebook) when the model is built in Jupyter. However, I would like to deploy the model and call its endpoint for Batch Transform.<\/p>\n<p>Most examples for SageMaker endpoint creation is for scoring on a single data and not for batch transform.<\/p>\n<p>Can someone point to how to deploy and use the endpoints for Batch Transform in SageMaker? Thank you<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603982300667,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64593327",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":21.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS SageMaker Deployment for Batch Transform",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":2988.0,
        "Challenge_word_count":169,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1319019150600,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3073.0,
        "Poster_view_count":341.0,
        "Solution_body":"<p>The following link has an example of how to call a stored model in SageMaker to run Batch Transform job.<\/p>\n<p><a href=\"https:\/\/github.com\/YiranJing\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/xgboost_customer_churn\/customised_xgboost_customer_churn_batch_transform.ipynb\" rel=\"nofollow noreferrer\">Batch Transform Reference<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":23.2,
        "Solution_reading_time":5.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>While distributed dask can be setup manually on AML compute, the process requires lot of configs to be maintained. Is there any native support.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666227934133,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1055350\/ray-dask-native-support-be-added-to-azure-machine",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":2.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"ray+dask native support  be added to Azure Machine Learning",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":32,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=dfa9d536-725c-462d-87c8-47fbafb1a2bc\">@D-0887  <\/a> Thanks for the question. you can do is to setup the compute cluster &amp; compute instance in the same vnet and pip install ray-on-aml. This allows both interactive and job use of Ray and Dask right within Azure ML.    <\/p>\n<p>Here is the document Library to turn Azure ML Compute into Ray and Dask cluster.    <br \/>\n<a href=\"https:\/\/techcommunity.microsoft.com\/t5\/ai-machine-learning-blog\/library-to-turn-azure-ml-compute-into-ray-and-dask-cluster\/ba-p\/3048784\">https:\/\/techcommunity.microsoft.com\/t5\/ai-machine-learning-blog\/library-to-turn-azure-ml-compute-into-ray-and-dask-cluster\/ba-p\/3048784<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":9.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi guys, I'm new to Azure ML. Following the URL below, I tried to run my python script on local machine. By local, I meant exactly Windows on my local physical machine in my house.  But it seems python script 'transform_titanic.py' was executed on Azure.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-set-up-training-targets#local-compute-target\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-set-up-training-targets#local-compute-target<\/a>    <\/p>\n<p>I executed the script below on my local computer, and expected it runs 'transform_titanic.py' on my local computer.    <\/p>\n<pre><code>   from azureml.core import Environment, Experiment, ScriptRunConfig, Workspace  \n   from dotenv import load_dotenv  \n     \n   load_dotenv()  \n     \n   ws = Workspace(  \n       os.environ['SUBSCRIPTION_ID']  \n       os.environ['RESOURCE_GROUP']  \n       os.environ['WORKSPACE_NAME']  \n   )  \n     \n   exp = Experiment(workspace=ws, name='experiment')  \n     \n   env = Environment('user-managed-env')  \n   env.python.user_managed_dependencies = True  \n     \n   script_run_config = ScriptRunConfig(  \n       source_directory='src\/transform',  \n       script='transform_titanic.py',  \n       arguments=['--input_dataset_name1', 'titanic'],  \n   )  \n     \n   script_run_config.run_config.target = 'local'  \n   script_run_config.run_config.environment = env  \n     \n   run = exp.submit(config=script_run_config)  \n   print(run.get_portal_url())  \n   run.wait_for_completion()  \n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600495202147,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/99901\/what-does-local-mean-in-compute-target",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":19.0,
        "Challenge_reading_time":18.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"What does \"local\" mean in compute target?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":114,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Sorry, I found it was run on my local computer. Some artifact created in the script was in C:\\Users{username}\\AppData\\Local\\Temp\\azureml_runs\\local_experiment_XXXXXXXXXX  <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.3,
        "Solution_reading_time":2.32,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1490674180056,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"%Temp%",
        "Answerer_reputation_count":302.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<pre><code>Get-AmlWorkspace : One or more errors occurred.\nAt line:1 char:1\n+ Get-AmlWorkspace\n+ ~~~~~~~~~~~~~~~~\n+ CategoryInfo          : NotSpecified: (:) [Get-AmlWorkspace], \nAggregateException\n+ FullyQualifiedErrorId : \nSystem.AggregateException,AzureML.PowerShell.GetWorkspace\n<\/code><\/pre>\n\n<p>I am trying to use Powershell to connect to Azure ML studio as it looks like an easier way to manage a workspace. I've downloaded the dll file from <a href=\"https:\/\/github.com\/hning86\/azuremlps\" rel=\"nofollow noreferrer\">https:\/\/github.com\/hning86\/azuremlps<\/a> and changed my config.json file, but get the error above if I try to run any AzureML commands. I've unblocked the DLL file and imported the AzureMLPS module, and I can see the module and commands I am trying to use have been imported by doing <code>Get-Module<\/code> and <code>Get-Command<\/code><\/p>\n\n<p>For info I've not used Powershell before.<\/p>\n\n<p>Any suggestions much appreciated!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1499681034153,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45009184",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":12.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Powershell AzureML Get-AmlWorkspace",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":428.0,
        "Challenge_word_count":112,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1397507727100,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":340.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>Have you installed Azure PowerShell Installer on your local machine?\n<strong><a href=\"https:\/\/github.com\/Azure\/azure-powershell\/releases\" rel=\"nofollow noreferrer\">Click here<\/a><\/strong> for more info.<\/p>\n\n<p>Download the latest <strong>Azure PowerShell Installer (4.3.1)<\/strong>, then install on your local machine. Then retry using Azure PowerShell module and commands.<\/p>\n\n<p>I installed mine last May, using Azure PowerShell 4.0.1, and the command Get-AmlWorkspace is working.<\/p>\n\n<pre><code># Set local folder location\nSet-Location -Path \"C:\\Insert here the location of AzureMLPS.dll\"\n\n# Unblock and import Azure Powershell Module (leverages config.json file)\nUnblock-File .\\AzureMLPS.dll\nImport-Module .\\AzureMLPS.dll\n\n# Get Azure ML Workspace info\nGet-AmlWorkspace\n<\/code><\/pre>\n\n<p>The output on my side looks like this:\n<a href=\"https:\/\/i.stack.imgur.com\/mEGeT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mEGeT.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.0,
        "Solution_reading_time":13.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":104.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>On the web\u201c <a href=\"https:\/\/learn.microsoft.com\/en-au\/dotnet\/machine-learning\/how-to-guides\/matchup-app-infer-net\">https:\/\/learn.microsoft.com\/en-au\/dotnet\/machine-learning\/how-to-guides\/matchup-app-infer-net<\/a> \u201dFor Infer.net Probability programming example of. In this example, there are only wins or losses, no draws. Can you give me an example of a draw, which can be used in the machine learning of E-sports Bo 2 or football, thank you<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593357487593,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/40647\/can-you-give-me-an-example-of-a-draw-with-infer-ne",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":6.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Can you give me an example of a draw with Infer.net",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The <a href=\"https:\/\/dotnet.github.io\/infer\/userguide\/Chess%20Analysis.html\">Chess Analysis<\/a> example in the Infer.NET documentation includes draws.  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.8,
        "Solution_reading_time":2.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":12.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1507660761310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Madrid, Espa\u00f1a",
        "Answerer_reputation_count":492.0,
        "Answerer_view_count":22.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have pretty big (~200Gb, ~20M lines) raw jsonl dataset. I need to extract important properties from there and store the intermediate dataset in csv for further conversion into something like HDF5, parquet, etc. Obviously, I can't use <code>JSONDataSet<\/code> for loading raw dataset, because it utilizes <code>pandas.read_json<\/code> under the hood, and using pandas for the dataset of such size sounds like a bad idea. So I'm thinking about reading the raw dataset line by line, process and append processed data line by line to the intermediate dataset.<\/p>\n\n<p>What I can't understand is how to make this compatible with <code>AbstractDataSet<\/code> with its <code>_load<\/code> and <code>_save<\/code> methods.<\/p>\n\n<p>P.S. I understand I can move this out of kedro's context, and introduce preprocessed dataset as a raw one, but that kinda breaks the whole idea of complete pipelines. <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582237049943,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1583417458307,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60329363",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":11.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"How to process huge datasets in kedro",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":656.0,
        "Challenge_word_count":141,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324477592580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1315.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p>Try to use pyspark to leverage lazy evaluation and batch execution. \nSparkDataSet is implemented in kedro.contib.io.spark_data_set<\/p>\n\n<p>Sample catalog config for jsonl:<\/p>\n\n<pre><code>your_dataset_name:   \n  type: kedro.contrib.io.pyspark.SparkDataSet\n  filepath: \"\\file_path\"\n  file_format: json\n  load_args:\n    multiline: True\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1582292099020,
        "Solution_link_count":0.0,
        "Solution_readability":13.5,
        "Solution_reading_time":4.42,
        "Solution_score_count":4.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":32.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":1.2039644444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run below sample code in my notebook, Running on python 3.6 kernel.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-train-models-with-aml\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-train-models-with-aml<\/a>\nDownload the MNIST dataset<\/p>\n\n<p>The following code failed with the attribute error, on line of the following code from azureml.opendatasets import MNIST <\/p>\n\n<pre><code>from azureml.core import Dataset\nfrom azureml.opendatasets import MNIST\n\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1581475311887,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1581477772576,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60180314",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":13.6,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure machine learning failing on sample for training",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":555.0,
        "Challenge_word_count":59,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Prerequisites:\nThe tutorial and accompanying utils.py file is also available on <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/tutorials\" rel=\"nofollow noreferrer\">GitHub<\/a> if you wish to use it on your own <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment#local\" rel=\"nofollow noreferrer\">local environment<\/a>. Run pip install azureml-sdk[notebooks] azureml-opendatasets matplotlib to install dependencies for this tutorial.<\/p>\n\n<p>If you are using older version then upgrade to the latest Azure ML SDK Version 1.0.85.<\/p>\n\n<p>!pip install --upgrade azureml-sdk<\/p>\n\n<pre><code># check core SDK version number\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)\n<\/code><\/pre>\n\n<p>Also <\/p>\n\n<p>!pip install --upgrade azureml-opendataset <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1581482106848,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":10.78,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1345587613323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Ljubljana, Slovenia",
        "Answerer_reputation_count":951.0,
        "Answerer_view_count":74.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>Overview:<\/strong><\/p>\n\n<p>I have a unique Python model where we hold n trained random forest models in a dictionary. I tried to avoid this setup, but for the time being it's necessary. On my local, I can make predictions by passing a dataframe to a predict function and looping through the rows, calling the appropriate model for each row, like rf_models[model].predict().<\/p>\n\n<p>In AzureML I created a toy model that allows me to go:\nWeb Input -> Python Script -> Score Model -> Web output. <\/p>\n\n<p><strong>Challenge:<\/strong><\/p>\n\n<p>I need to be able to call the score_model, or specifically the predict method, from inside the \"Python Script\" function on AzureML so I can deal with the loops and n models stored in the dict. The results, either a JSON or dataframe, would be sent to AzureML's Web Output.<\/p>\n\n<p>I found a link online (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts<\/a>) that got me close, but the example shows a model being trained and used to predict at the same time inside the same Python script, thus calling the predict method as a local variable and not calling a previously trained model. I found only limited documentation online to solve this problem and I could not get the rest of the way there. I'm unsure if this type of customization is not yet available or if I'm completely overlooking some key functionality.<\/p>\n\n<p>Thank you for your assistance.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1502218686207,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45576092",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":20.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureML multiple models stored in dictionary",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":200.0,
        "Challenge_word_count":232,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1493753899063,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":159.0,
        "Poster_view_count":31.0,
        "Solution_body":"<p>Here are two links that might help:<\/p>\n\n<ol>\n<li><a href=\"https:\/\/github.com\/Azure\/Machine-Learning-Operationalization\" rel=\"nofollow noreferrer\">AzureML Operationalization<\/a> <\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/Machine-Learning-Operationalization\/blob\/master\/samples\/python\/tutorials\/realtime\/digit_classification.ipynb\" rel=\"nofollow noreferrer\">Example notebook<\/a> that shows how to publish Python model as a web service. You would do a similar thing, only you would pickle the dictionary of your models instead. <\/li>\n<\/ol>\n\n<p>Note that this functionality is currently in preview mode.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.2,
        "Solution_reading_time":8.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1327481639092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Poznan, Poland",
        "Answerer_reputation_count":2923.0,
        "Answerer_view_count":838.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Within AzureML, I have a CSV file which contains <code>2 columns<\/code> of data with <code>thousands of rows<\/code>. I'm looking to run this entire file as training, and find a pattern between these 2 sets of numbers, for example:<\/p>\n\n<pre><code>x -&gt; y\n\n... 10k x\n<\/code><\/pre>\n\n<p>And after all that training, I'd want to give this one line as the score model, so It'd look like:\nx -> ? (Predict answer from training)\n-- Note, the question mark here wouldn't need to be an exact match, as long as it is somewhat around what that actual number would turn out to be like.<\/p>\n\n<p>Is their a ML method (Inside <code>Azure ML<\/code>) that does such thing? Any points would be great.<\/p>\n\n<p>tl;dr: <code>Finding any type of pattern between 2 numbers (w\/ intense training).<\/code><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1441616495073,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32434805",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":10.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"What would be the best ML method for this use case?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":32.0,
        "Challenge_word_count":137,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416688064183,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":347.0,
        "Poster_view_count":51.0,
        "Solution_body":"<p>Read about <code>linear regression<\/code>. This is answer for your question. And here is the link to Azure ML tutorial <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-create-experiment\/\" rel=\"nofollow\">link<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.8,
        "Solution_reading_time":3.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1579718832727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":149.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have developed keras text classification model. I have preprocessed data(tokenization). I have logged trained model successfully(mlflow.keras.log_model). I have served model using mlflow serve. Now while doing prediction on text data I need to do preprocessing using same tokenizer object used for training.\nHow to preprocess test data and get predictions from served model.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1584090517703,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60667610",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":5.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to deploy mlflow model with data preprocessing(text data)",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1996.0,
        "Challenge_word_count":62,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1498470936987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>You can log a custom python model: \n<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":29.0,
        "Solution_reading_time":3.04,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi there.\nI am working with Vertex AI Jupyterlab Notebook.\nThere were a few such warnings\n\nWARN BlockManager: Block rdd_6_0 already exists on this machine; not re-adding it\nWARN BlockManager: Block rdd_817_0 already exists on this machine; not re-adding it\n\non this as the model was getting trained.\nMay I know if we are safe to ignore this?\nWhat do they mean actually?\nThanks in advance.",
        "Challenge_closed_time":1661250.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659595500000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/WARN-BlockManager-Block-rdd-6-0-already-exists-on-this-machine\/m-p\/450462#M486",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.9,
        "Challenge_reading_time":5.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"WARN BlockManager: Block rdd_6_0 already exists on this machine; not re-adding it",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":135.0,
        "Challenge_word_count":77,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"No, you should not worry since this is just a warning that tells you that those two blocks will not be re added to your notebook.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.6,
        "Solution_reading_time":1.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nAfter Sagemaker workspace stopped automatically, workspace env status is not updated.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Set sagemaker workspace config's AutoStopIdleTimeInMinutes as 10 minutes\r\n2. Create sagemaker workspace and wait for more than 10 minutes,\r\n3. Check sagemaker notebook instances to confirm the instance status is Stopped\r\n4. Check Service Workbench workspace status, it is still \"AVAILABLE\"\r\n\r\n**Expected behavior**\r\n1. Above step 4, workspace status should be \"STOPPED\"\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1657702.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657451336000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/44",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":38.0,
        "Challenge_repo_fork_count":5.0,
        "Challenge_repo_issue_count":119.0,
        "Challenge_repo_star_count":8.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"[Bug] Sagemaker template, after auto stoped, workspace env status is not updated",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":116,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Fixed, refer [commit](https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/commit\/2339efe78d0f4705ef0cd6d2b1c5f06a810e6730) ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":52.8,
        "Solution_reading_time":1.81,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424063473423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":334.0,
        "Answerer_view_count":347.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I need to make some amendments to the keras.json file for a sagemaker notebook instance but I am unsure where it is located. Any help would be greatly appreciated.<\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541760158833,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53224172",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":2.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I update the keras.json file on Sagemaker",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":76.0,
        "Challenge_word_count":38,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1359732456992,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":401.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>Thank you for using Amazon SageMaker! <\/p>\n\n<p>You can find keras.json file located in \/home\/ec2-user\/.keras . <\/p>\n\n<p>Thanks,<br>\nNeelam <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.2,
        "Solution_reading_time":1.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1537550036732,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":83.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Why we need to ML Batch Execution and ML Update resource option in Data factory ? How this can be used to retrain machine learning when updating a blob file ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/8KWH0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8KWH0.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538738198597,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1538846687100,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52664415",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":5.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Why we need ML batch execution and update resource option in azure data factory",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":141.0,
        "Challenge_word_count":52,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1538737895716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":78.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>Ml Batch Execution- to call retraining experiment and get a .ilearner file as output.\nML Update Resource- Use the above .ilearner as input and call patch endpoint of predictive web service to Update resource.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":2.67,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1537462795807,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":99.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I created a custom model and deployed it on sagemaker. I am invoking the endpoint using batch transform jobs. It works if the input file is small, i.e, number of rows in the csv file is less. If I upload a file with around 200000 rows, I am getting this error in the cloudwatch logs.<\/p>\n\n<pre><code>2018-11-21 09:11:52.666476: W external\/org_tensorflow\/tensorflow\/core\/framework\/allocator.cc:113]\nAllocation of 2878368000 exceeds 10% of system memory.\n2018-11-21 09:11:53.166493: W external\/org_tensorflow\/tensorflow\/core\/framework\/allocator.cc:113] \nAllocation of 2878368000 exceeds 10% of system memory.\n[2018-11-21 09:12:02,544] ERROR in serving: &lt;_Rendezvous of RPC that \nterminated with:\n#011status = StatusCode.DEADLINE_EXCEEDED\n#011details = \"Deadline Exceeded\"\n#011debug_error_string = \"\n{\n\"created\": \"@1542791522.543282048\",\n\"description\": \"Error received from peer\",\n\"file\": \"src\/core\/lib\/surface\/call.cc\",\n\"file_line\": 1017,\n\"grpc_message\": \"Deadline Exceeded\",\n\"grpc_status\": 4\n}\n\"\n<\/code><\/pre>\n\n<p>Any ideas what might be going wrong. This is the transform function which I am using to create the transform job.<\/p>\n\n<pre><code>transformer =sagemaker.transformer.Transformer(\nbase_transform_job_name='Batch-Transform',\nmodel_name='sagemaker-tensorflow-2018-11-21-07-58-15-887',\ninstance_count=1,\ninstance_type='ml.m4.xlarge',\noutput_path='s3:\/\/2-n2m-sagemaker-json-output\/out_files\/'\n\n)\ninput_location = 's3:\/\/1-n2m-n2g-csv-input\/smal_sagemaker_sample.csv'\ntransformer.transform(input_location, content_type='text\/csv', split_type='Line')\n<\/code><\/pre>\n\n<p>The .csv file contains 2 columns for first and last name of customer, which I am then preprocessing it in the sagemaker itself using input_fn().<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1542792620897,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1542799396316,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53408927",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.3,
        "Challenge_reading_time":23.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"How to pass a bigger .csv files to amazon sagemaker for predictions using batch transform jobs",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1941.0,
        "Challenge_word_count":187,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1444454434270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":140.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>The error looks to be coming from a GRPC client closing the connection before the server is able to respond. (There looks to be an existing feature request for the sagemaker tensorflow container on <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/issues\/46\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/issues\/46<\/a> to make this timeout configurable)<\/p>\n\n<p>You could try out a few things with the sagemaker Transformer to limit the size of each individual request so that it fits within the timeout:<\/p>\n\n<ul>\n<li>Set a <code>max_payload<\/code> to a smaller value, say 2-3 MB (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">the default is 6 MB<\/a>)<\/li>\n<li>If your instance metrics indicate it has compute \/ memory resources to spare, try <code>max_concurrent_transforms<\/code> > 1 to make use of multiple workers<\/li>\n<li>Split up your csv file into multiple input files. With a bigger dataset, you could also increase the instance count to fan out processing<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.7,
        "Solution_reading_time":14.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":134.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using sagemaker batch transform, with json input files. see below for sample input\/output files. i have custom inference code below, and i'm using json.dumps to return prediction, but it's not returning json. I tried to use =&gt;    &quot;DataProcessing&quot;: {&quot;JoinSource&quot;: &quot;string&quot;,  }, to match input and output. but i'm getting error that &quot;unable to marshall ...&quot; . I think because , the output_fn is returning array of list or just list and not json , that is why it is unable to match input with output.any suggestions on how should i return the data?<\/p>\n<p>infernce code<\/p>\n<pre><code>def model_fn(model_dir):\n...\ndef input_fn(data, content_type):\n...\ndef predict_fn(data, model):\n...\ndef output_fn(prediction, accept):\n    if accept == &quot;application\/json&quot;:\n        return json.dumps(prediction), mimetype=accept)\n    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n<\/code><\/pre>\n<p>input file<\/p>\n<pre><code>{&quot;data&quot; : &quot;input line  one&quot; }\n{&quot;data&quot; : &quot;input line  two&quot; }\n....\n<\/code><\/pre>\n<p>output file<\/p>\n<pre><code>[&quot;output line  one&quot; ]\n[&quot;output line  two&quot; ]\n<\/code><\/pre>\n<pre><code>{\n   &quot;BatchStrategy&quot;: SingleRecord,\n   &quot;DataProcessing&quot;: { \n      &quot;JoinSource&quot;: &quot;string&quot;,\n   },\n   &quot;MaxConcurrentTransforms&quot;: 3,\n   &quot;MaxPayloadInMB&quot;: 6,\n   &quot;ModelClientConfig&quot;: { \n      &quot;InvocationsMaxRetries&quot;: 1,\n      &quot;InvocationsTimeoutInSeconds&quot;: 3600\n   },\n   &quot;ModelName&quot;: &quot;some-model&quot;,\n   &quot;TransformInput&quot;: { \n      &quot;ContentType&quot;: &quot;string&quot;,\n      &quot;DataSource&quot;: { \n         &quot;S3DataSource&quot;: { \n            &quot;S3DataType&quot;: &quot;string&quot;,\n            &quot;S3Uri&quot;: &quot;s3:\/\/bucket-sample&quot;\n         }\n      },\n      &quot;SplitType&quot;: &quot;Line&quot;\n   },\n   &quot;TransformJobName&quot;: &quot;transform-job&quot;\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653780407103,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72419908",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":26.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"How to match input\/output with sagemaker batch transform?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":358.0,
        "Challenge_word_count":178,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1590797441983,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":525.0,
        "Poster_view_count":98.0,
        "Solution_body":"<p><code>json.dumps<\/code> will not convert your array to a dict structure and serialize it to a JSON String.<\/p>\n<p>What data type is <code>prediction<\/code> ? Have you tested making sure <code>prediction<\/code> is a dict?<\/p>\n<p>You can confirm the data type by adding <code>print(type(prediction))<\/code> to see the data type in the CloudWatch Logs.<\/p>\n<p>If prediction is a <code>list<\/code> you can test the following:<\/p>\n<pre><code>def output_fn(prediction, accept):\n    if accept == &quot;application\/json&quot;:\n\n        my_dict = {'output': prediction}\n        return json.dumps(my_dict), mimetype=accept)\n\n    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n<\/code><\/pre>\n<p><code>DataProcessing<\/code> and <code>JoinSource<\/code> are used to associate the data that is relevant to the prediction results in the output. It is not meant to be used to match the input and output format.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":11.87,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":115.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":62.2546263889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong><em>Problem:<\/em><\/strong><\/p>\n\n<p>Jobs repeatedly fail after 5 minutes with the error<\/p>\n\n<blockquote>\n  <p>ClientError: .lst file missing in the train_lst channel.<\/p>\n<\/blockquote>\n\n<p><strong><em>Context:<\/em><\/strong><\/p>\n\n<p>Working within the AWS console, I have a binary classification task of images. I have labeled the classes in their filenames, per a guide.<\/p>\n\n<p>Eventually I started hitting errors that revealed that for this particular algorithm a <code>.lst<\/code> file is required for gathering the labels, since \"Content Type\" is specified as image, which apparently requires a lst file.<\/p>\n\n<p><strong><em>Example Data:<\/em><\/strong><\/p>\n\n<p>I am trying to match the examples I see on <a href=\"https:\/\/stackoverflow.com\/questions\/51670563\/invalid-lst-file-in-sagemaker\">StackOverflow<\/a> and elsewhere online. The current iteration of <code>trn_list.lst<\/code> looks like this:<\/p>\n\n<pre><code>292 \\t 1 \\t dog-292.jpeg\n214 \\t 1 \\t dog-214.jpeg\n290 \\t 0 \\t cat-290.jpeg\n288 \\t 1 \\t dog-288.jpeg\n160 \\t 1 \\t dog-160.jpeg\n18 \\t 0 \\t cat-18.jpeg\n215 \\t 1 \\t dog-215.jpeg\n254 \\t 1 \\t dog-254.jpeg\n53 \\t 1 \\t dog-53.jpeg\n337 \\t 0 \\t cat-337.jpeg\n284 \\t 0 \\t cat-284.jpeg\n177 \\t 1 \\t dog-177.jpeg\n192 \\t 1 \\t dog-192.jpeg\n228 \\t 0 \\t cat-228.jpeg\n305 \\t 0 \\t cat-305.jpeg\n258 \\t 1 \\t dog-258.jpeg\n75 \\t 0 \\t cat-75.jpeg\n148 \\t 0 \\t cat-148.jpeg\n268 \\t 1 \\t dog-268.jpeg\n281 \\t 1 \\t dog-281.jpeg\n24 \\t 1 \\t dog-24.jpeg\n328 \\t 1 \\t dog-328.jpeg\n99 \\t 1 \\t dog-99.jpeg\n<\/code><\/pre>\n\n<p>The bucket has no sub-folders, so I just put the .lst on the <\/p>\n\n<p>In one iteration I allowed my R program that creates the .lst to replace the <code>\\t<\/code> with actual tabs when it writes it out. In other iterations I left the actual delimiters (<code>\\t<\/code>) in there. Didn't seem to affect it (?).<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559761618273,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56466592",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":3.8,
        "Challenge_reading_time":23.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker: ClientError: .lst file missing in the train_lst channel. (customized image classification)",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":608.0,
        "Challenge_word_count":286,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1399301338467,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Columbia, MD, USA",
        "Poster_reputation_count":21413.0,
        "Poster_view_count":6465.0,
        "Solution_body":"<p>When you are using SageMaker training jobs you are actually deploying a Docker image to a cluster of EC2 instances. The Docker has a python file that is running the training code in a similar way that you train it on your machine. In the training code you are referring to local folders when it expects to find the data such as the images to train on and the meta-data to use for that training. <\/p>\n\n<p>The \"magic\" is how to get the data from S3 to be available locally for the training instances. This is done using the definition of the channels in your training job configuration. Each channel definition creates a local folder on the training instance and copies the data from S3 to that local folder. You need to match the names and the S3 location and file formats.<\/p>\n\n<p>Here is the documentation of the definition of a channel in SageMaker: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_Channel.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_Channel.html<\/a><\/p>\n\n<p>For the specific example of the built-in algorithm for image classification and if you use the Image format for training, specify <code>train<\/code>, <code>validation<\/code>, <code>train_lst<\/code>, and <code>validation_lst<\/code> channels as values for the <code>InputDataConfig<\/code> parameter of the <code>CreateTrainingJob<\/code> request. Specify the individual image data (.jpg or .png files) for the train and validation channels. Specify one .lst file in each of the train_lst and validation_lst channels. Set the content type for all four channels to <code>application\/x-image<\/code>.<\/p>\n\n<p>See more details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html#IC-inputoutput\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html#IC-inputoutput<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1559985734928,
        "Solution_link_count":4.0,
        "Solution_readability":12.7,
        "Solution_reading_time":24.03,
        "Solution_score_count":3.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":240.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a python notebook which is initiating a Batch Transform Job in Sagemaker. However, I want to also print the status &quot;Failed&quot;, &quot;In Progress&quot; and &quot;Completed&quot; once the job is complete running. As of now, I am only able to start the Batch Transform Job (rf=random forest) but I am not certain how to get the job status print outs. Can someone help with that given my script below?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/SU0ln.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/SU0ln.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code>rf_transformer = rf.transformer(\n                                instance_count,\n                                instance_type,\n                                strategy=strategy,\n                                output_path=output_path,\n                                max_payload=max_payload)\n\nrf_transformer.transform(\n                                str('s3:\/\/batch_scoring\/rf_output),\n                                content_type='text\/csv',\n                                compression_type='Gzip'\n                         )\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593925160127,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62737008",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":12.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"How to get Sagemaker Batch Transform Job status printed out in my python notebook?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":612.0,
        "Challenge_word_count":106,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1591222877740,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":91.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>You can do it with:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>job_name = rf_transformer.latest_transform_job.name\nrf_transformer.sagemaker_session.describe_transform_job(job_name)['TransformJobStatus']\n<\/code><\/pre>\n<p>You can also use the AWS SDK directly, if you wish:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nsagemaker_client = boto3.client('sagemaker')\nsagemaker_client.describe_transform_job(job_name)['TransformJobStatus']\n<\/code><\/pre>\n<p>API documentation: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTransformJob.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":37.5,
        "Solution_reading_time":10.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1515548304623,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vietnam",
        "Answerer_reputation_count":549.0,
        "Answerer_view_count":68.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to do a neural network for regression analysis using optuna based on <a href=\"https:\/\/dreamer-uma.com\/pytorch-optuna-hyperparameter-tuning\/\" rel=\"nofollow noreferrer\">this site<\/a>.\nI would like to create a model with two 1D data as input and one 1D data as output in batch learning.<\/p>\n<p><code>x<\/code> is the training data and <code>y<\/code> is the teacher data.<\/p>\n<pre><code>class Model(nn.Module):\n    # \u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf(\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u751f\u6210\u6642\u306e\u521d\u671f\u5316)\n    def __init__(self,trial, mid_units1, mid_units2):\n        super(Model, self).__init__()\n        self.linear1 = nn.Linear(2, mid_units1)\n        self.bn1 = nn.BatchNorm1d(mid_units1)\n        self.linear2 = nn.Linear(mid_units1, mid_units2)\n        self.bn2 = nn.BatchNorm1d(mid_units2)\n        self.linear3 = nn.Linear(mid_units2, 1)\n        self.activation = trial_activation(trial)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.bn1(x)\n        x = self.activation(x)\n        x = self.linear2(x)\n\ndevice = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n\nEPOCH = 100\nx = torch.from_numpy(a[0].astype(np.float32)).to(device)\ny = torch.from_numpy(a[1].astype(np.float32)).to(device)\n\ndef train_epoch(model, optimizer, criterion):\n    model.train()\n    optimizer.zero_grad()    # \u52fe\u914d\u60c5\u5831\u30920\u306b\u521d\u671f\u5316\n    y_pred = model(x)                                               # \u4e88\u6e2c\n    loss = criterion(y_pred.reshape(y.shape), y)          # \u640d\u5931\u3092\u8a08\u7b97(shape\u3092\u63c3\u3048\u308b)\n    loss.backward()                                                       # \u52fe\u914d\u306e\u8a08\u7b97\n    optimizer.step()                                                      # \u52fe\u914d\u306e\u66f4\u65b0\n    return loss.item()\n\ndef trial_activation(trial):\n    activation_names = ['ReLU','logsigmoid']\n    activation_name = trial.suggest_categorical('activation', activation_names)\n    if activation_name == activation_names[0]:\n        activation = F.relu\n    else:\n        activation = F.logsigmoid\n    return activation\n\ndef objective(trial):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # \u4e2d\u9593\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\u306e\u8a66\u884c\n    mid_units1 = int(trial.suggest_discrete_uniform(&quot;mid_units1&quot;, 1024*2,1024*4, 64*2))\n    mid_units2 = int(trial.suggest_discrete_uniform(&quot;mid_units2&quot;, 1024, 1024*2, 64*2))\n\n    net = Model(trial, mid_units1, mid_units2).to(device)\n\n    criterion = nn.MSELoss() \n    # \u6700\u9069\u5316\u624b\u6cd5\u306e\u8a66\u884c\n    optimizer = trial_optimizer(trial, net)\n    train_loss = 0\n    for epoch in range(EPOCH):\n        train_loss = train_epoch(net, optimizer, criterion, device)\n    torch.save(net.state_dict(), str(trial.number) + &quot;new1.pth&quot;)\n    return train_loss\n\nstrage_name = &quot;a.sql&quot;\nstudy_name = 'a'\n\nstudy = optuna.create_study(\n    study_name = study_name,\n    storage='sqlite:\/\/\/'  + strage_name, \n    load_if_exists=True,\n    direction='minimize')\nTRIAL_SIZE = 100\n\nstudy.optimize(objective, n_trials=TRIAL_SIZE)\n<\/code><\/pre>\n<p>error message<\/p>\n<pre><code>---&gt; 28     loss = criterion(y_pred.reshape(y.shape), y)          # \u640d\u5931\u3092\u8a08\u7b97(shape\u3092\u63c3\u3048\u308b)\n     29     loss.backward()                                                       # \u52fe\u914d\u306e\u8a08\u7b97\n     30     optimizer.step()                                                      # \u52fe\u914d\u306e\u66f4\u65b0\n\nAttributeError: 'NoneType' object has no attribute 'reshape'\n<\/code><\/pre>\n<p>Because of the above error, I checked the value of <code>y_pred<\/code> and found it to be <code>None<\/code>.<\/p>\n<pre><code>    model.train()\n    optimizer.zero_grad()\n<\/code><\/pre>\n<p>I am thinking that these two lines may be wrong, but I don't know how to solve this problem.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1646559560287,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1646560172363,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71369132",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.2,
        "Challenge_reading_time":39.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":null,
        "Challenge_title":"The pytorch training model cannot be created successfully",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":267,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1643702319420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>With PyTorch, when you call <code>y_pred = model(x)<\/code> that will call the <code>forward<\/code> function which is defined in the <code>Model<\/code> class.<\/p>\n<p>So, <code>y_pred <\/code> will get the result of the <code>forward<\/code> function, in your case, it returns nothing, that's why you get a <code>None<\/code> value. You can change the <code>forward<\/code> function as below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>    def forward(self, x):\n        x = self.linear1(x)\n        x = self.bn1(x)\n        x = self.activation(x)\n        x = self.linear2(x)\n        return x\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":7.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":69.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hey,<\/p>\n<p>i'm right now trying to understand the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/architecture\/example-scenario\/mlops\/mlops-maturity-model\">MLOps Maturity Model<\/a>. The thing i don't understand is the Level 1 (DevOps but not MLOps). Why is the data gathering at this stage automatically, when data is unique for MLOps and is no part of DevOps? And the second thing is, why are the different people not working together at that stage? The DevOps principles say, that the Development and the Operations are working together and communicate with eachother. And in the case of MLOps the Developers are the Data Scientists and the Data Engineers. So why are they siloed from the Software Engineer (Operations)?<\/p>\n<p>Thanks in advance <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1678125232640,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1186987\/mlops-maturity-model",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"MLOps Maturity Model",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":111,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=ee824636-df75-475b-b9cb-ed765083873f\">@MacerPacer  <\/a><\/p>\n<p>Thanks for reaching out to us, below answer is based on some researches I got across the internet, please let me know if you have any point you feel confused.<\/p>\n<p>For your question 1, why is the data gathering at this stage automatically, when data is unique for MLOps and is no part of DevOps?<\/p>\n<p>Level 1 representing organizations that have adopted DevOps practices but have not yet integrated MLOps practices into their workflows. Regarding the data gathering aspect of Level 1, it is important to note that data is a critical component of both DevOps and MLOps. <strong>In DevOps, data is often used for testing and monitoring applications in production, while in MLOps, data is used to train, validate, and test machine learning models.<\/strong> Therefore, it makes sense for organizations at Level 1 to have some data gathering processes in place, even if they have not yet fully integrated MLOps practices into their workflows.<\/p>\n<p>For your question 2, why are the different people not working together at that stage? The DevOps principles say, that the Development and the Operations are working together and communicate with eachother.<\/p>\n<p>Regarding the siloed nature of different teams at Level 1, it is true that DevOps emphasizes collaboration and communication between development and operations teams. However, in the context of MLOps, there may be additional teams involved, <strong>such as data science and data engineering teams, that are responsible for building and managing machine learning models.<\/strong> These teams may have different expertise and tooling requirements than traditional software engineering teams, which can lead to silos. However, as organizations progress through the maturity model, they can work towards breaking down these silos and fostering greater collaboration between different teams involved in the MLOps process.<\/p>\n<p>For your question 3, in the case of MLOps the Developers are the Data Scientists and the Data Engineers. So why are they siloed from the Software Engineer (Operations)?<\/p>\n<p>We have explained some parts of it in the Q2, it is mainly because of the different tooling and process as below.<\/p>\n<p>In the case of MLOps, it is true that developers, specifically data scientists and data engineers, are responsible for <strong>building and managing machine learning models.<\/strong> However, software engineering teams, specifically operations teams, play a critical role in <strong>deploying and managing<\/strong> these models in production environments.<\/p>\n<p>The reason for the potential silos between these teams is that they may have different areas of expertise and use different tools and technologies. For example, data scientists and data engineers may be more familiar with tools such as Jupyter notebooks and data processing frameworks, while operations teams may be more familiar with infrastructure automation tools such as Kubernetes or other approaches.<\/p>\n<p>I hope my answer helps, please take a look and let me know if you have any other question. Happy to help further. Thanks a lot.<\/p>\n<p>Regards,<\/p>\n<p>Yutong<\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.5,
        "Solution_reading_time":41.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":494.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In this <a href=\"https:\/\/github.com\/MicrosoftLearning\/DP100\/blob\/master\/07B%20-%20Creating%20a%20Batch%20Inferencing%20Service.ipynb\">example<\/a>, all data files for the parallel run step are stored in <strong>one<\/strong> folder.    <\/p>\n<p>I also want to create a parallel run step. The task for each of the several <strong>folders<\/strong>, in which the multiple data files are stored, is exactly identical.     <\/p>\n<p>The folders:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/182769-image.png?platform=QnA\" alt=\"182769-image.png\" \/>    <\/p>\n<p>The content of each folder:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/182833-image.png?platform=QnA\" alt=\"182833-image.png\" \/>    <\/p>\n<p>How should I define the <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.parallelrunstep?view=azure-ml-py\">ParallelRunStep<\/a>-class so that the identical task for each folder (here 'a', 'b', 'c', 'd' and 'e') is executed in parallel?    <br \/>\nTwo folders should run simultaneously in parallel.    <\/p>\n<p>Moreover, I would like to ask how to get <strong>only<\/strong> the stored folder names or folder paths from a given directory path of a blob storage container.    <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1647256395817,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/771015\/list-of-folder-names-as-input-for-parallelrunstep",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":12.8,
        "Challenge_reading_time":17.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"list of folder names as input for ParallelRunStep-class",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":134,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@@AlexanderPakakis-0994 Thanks, An Azure ML dataset is just metadata pointing to a path or collection of paths in an Azure storage account. You should first &quot;merge&quot; those datasets into a collection of adjacent folders (e.g. root\/dataset1\/, root\/dataset2\/, ...) and then run PRS against root\/**.<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":3.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1508797229168,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":1115.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":467.3672177778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am able to submit jobs to Azure ML services using a compute cluster. It works well, and the autoscaling combined with good flexibility for custom environments seems to be exactly what I need. However, so far all these jobs seem to only use one compute node of the cluster. Ideally I would like to use multiple nodes for a computation, but all methods that I see rely on rather deep integration with azure ML services.<\/p>\n\n<p>My modelling case is a bit atypical. From previous experiments I identified a group of architectures (pipelines of preprocessing steps + estimators in Scikit-learn) that worked well. \nHyperparameter tuning for one of these estimators can be performed reasonably fast (couple of minutes) with <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV\" rel=\"nofollow noreferrer\">RandomizedSearchCV<\/a>. So it seems less effective to parallelize this step.<\/p>\n\n<p>Now I want to tune and train this entire list of architectures.\nThis should be very easily to parallelize since all architectures can be trained independently. <\/p>\n\n<p>Ideally I would like something like (in pseudocode)<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>tuned = AzurePool.map(tune_model, [model1, model2,...])\n<\/code><\/pre>\n\n<p>However, I could not find any resources on how I could achieve this with an Azure ML Compute cluster.\nAn acceptable alternative would come in the form of a plug-and-play substitute for sklearn's CV-tuning methods, similar to the ones provided in <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV\" rel=\"nofollow noreferrer\">dask<\/a> or <a href=\"https:\/\/databricks.github.io\/spark-sklearn-docs\/#spark_sklearn.GridSearchCV\" rel=\"nofollow noreferrer\">spark<\/a>.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565966913733,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1565987551452,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57526707",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":25.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"How to parallelize work on an Azure ML Service Compute cluster?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":818.0,
        "Challenge_word_count":233,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525187747288,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":1466.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>There are a number of ways you could tackle this with AzureML. The simplest would be to just launch a number of jobs using the AzureML Python SDK (the underlying example is taken from <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4170a394edd36413edebdbab347afb0d833c94ee\/how-to-use-azureml\/training\/train-hyperparameter-tune-deploy-with-sklearn\/train-hyperparameter-tune-deploy-with-sklearn.ipynb\" rel=\"nofollow noreferrer\">here<\/a>)<\/p>\n\n<pre><code>from azureml.train.sklearn import SKLearn\n\nruns = []\n\nfor kernel in ['linear', 'rbf', 'poly', 'sigmoid']:\n    for penalty in [0.5, 1, 1.5]:\n        print ('submitting run for kernel', kernel, 'penalty', penalty)\n        script_params = {\n            '--kernel': kernel,\n            '--penalty': penalty,\n        }\n\n        estimator = SKLearn(source_directory=project_folder, \n                            script_params=script_params,\n                            compute_target=compute_target,\n                            entry_script='train_iris.py',\n                            pip_packages=['joblib==0.13.2'])\n\n        runs.append(experiment.submit(estimator))\n<\/code><\/pre>\n\n<p>The above requires you to factor your training out into a script (or a set of scripts in a folder) along with the python packages required. The above estimator is a convenience wrapper for using Scikit Learn. There are also estimators for Tensorflow, Pytorch, Chainer and a generic one (<code>azureml.train.estimator.Estimator<\/code>) -- they all differ in the Python packages and base docker they use.<\/p>\n\n<p>A second option, if you are actually tuning parameters, is to use the HyperDrive service like so (using the same <code>SKLearn<\/code> Estimator as above):<\/p>\n\n<pre><code>from azureml.train.sklearn import SKLearn\nfrom azureml.train.hyperdrive.runconfig import HyperDriveConfig\nfrom azureml.train.hyperdrive.sampling import RandomParameterSampling\nfrom azureml.train.hyperdrive.run import PrimaryMetricGoal\nfrom azureml.train.hyperdrive.parameter_expressions import choice\n\nestimator = SKLearn(source_directory=project_folder, \n                    script_params=script_params,\n                    compute_target=compute_target,\n                    entry_script='train_iris.py',\n                    pip_packages=['joblib==0.13.2'])\n\nparam_sampling = RandomParameterSampling( {\n    \"--kernel\": choice('linear', 'rbf', 'poly', 'sigmoid'),\n    \"--penalty\": choice(0.5, 1, 1.5)\n    }\n)\n\nhyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n                                         hyperparameter_sampling=param_sampling, \n                                         primary_metric_name='Accuracy',\n                                         primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                                         max_total_runs=12,\n                                         max_concurrent_runs=4)\n\nhyperdrive_run = experiment.submit(hyperdrive_run_config)\n<\/code><\/pre>\n\n<p>Or you could use DASK to schedule the work as you were mentioning. Here is a sample of how to set up DASK on and AzureML Compute Cluster so you can do interactive work on it: <a href=\"https:\/\/github.com\/danielsc\/azureml-and-dask\" rel=\"nofollow noreferrer\">https:\/\/github.com\/danielsc\/azureml-and-dask<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1567670073436,
        "Solution_link_count":3.0,
        "Solution_readability":17.0,
        "Solution_reading_time":36.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":250.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645792458310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run a SageMaker kernel with Python 3.8 in SageMaker Studio, and the notebook appears to use a separate distribution of Python 3.7. The <em>running app<\/em> is indicated as <em>tensorflow-2.6-cpu-py38-ubuntu20.04-v1<\/em>. When I run <code>!python3 -V<\/code> I get <em>Python 3.8.2<\/em>. However, the Python instance inside the notebook is different:<\/p>\n<pre><code>import sys\nsys.version\n<\/code><\/pre>\n<p>gives <code>'3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \\n[GCC 9.4.0]'<\/code><\/p>\n<p>Similarly, running <code>%pip -V<\/code> and <code>%conda info<\/code> indicates Python 3.7.<\/p>\n<p>Also, <code>import tensorflow<\/code> fails, as it isn't preinstalled in the Python environment that the notebook invokes.<\/p>\n<p>I'm running in the <em>eu-west-2<\/em> region. Is there anything I can do to address this short of opening a support ticket?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1640518921197,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70486162",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.0,
        "Challenge_reading_time":12.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Conflicting Python versions in SageMaker Studio notebook with Python 3.8 kernel",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":768.0,
        "Challenge_word_count":123,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1331727483732,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":1060.0,
        "Poster_view_count":139.0,
        "Solution_body":"<p>are you still facing this issue?<\/p>\n<p>I am in eu-west-2 using a SageMaker Studio notebook and the TensorFlow 2.6 Python 3.8 CPU Optimized image (running app is tensorflow-2.6-cpu-py38-ubuntu20.04-v1).<\/p>\n<p>When I run the below commands, I get the right outputs.<\/p>\n<pre><code>!python3 -V\n<\/code><\/pre>\n<p>returns Python 3.8.2<\/p>\n<pre><code>import sys\nsys.version \n<\/code><\/pre>\n<p>returns\n3.8.2 (default, Dec  9 2021, 06:26:16) \\n[GCC 9.3.0]'<\/p>\n<pre><code>import tensorflow as tf\nprint(tf.__version__)\n<\/code><\/pre>\n<p>returns 2.6.2<\/p>\n<p>It seems this has now been fixed<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.5,
        "Solution_reading_time":7.54,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":73.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"**Describe the bug**\r\nAfter running a model evaluation suite and exprorint to wandb using \"to_wandb\" function, the confusion matrix appears in the w&b page without the values\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n\r\n**Expected behavior**\r\nThe confusion matrix in w&b should appear like the confusion matrix in the notebook which has it values shown\r\n![1654716717893](https:\/\/user-images.githubusercontent.com\/21197955\/172704682-e1097eaa-5371-48b6-96d7-f0df1006c043.jpeg)\r\n\r\n\r\n\r\n**Environment (please complete the following information):**\r\n - OS: linux\r\n - Python Version:3.7.1\r\n - Deepchecks Version:0.7.2\r\n\r\n",
        "Challenge_closed_time":1657703.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654717842000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/deepchecks\/deepchecks\/issues\/1592",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":13.0,
        "Challenge_reading_time":8.52,
        "Challenge_repo_contributor_count":35.0,
        "Challenge_repo_fork_count":159.0,
        "Challenge_repo_issue_count":2171.0,
        "Challenge_repo_star_count":2280.0,
        "Challenge_repo_watch_count":13.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Weird behavior with \"to_wandb\" and confusion matrix",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":75,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey @DL1992,\r\n\r\nFor me the export works fine, can you provide us with some more info about what you did?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/9868530\/173320868-74292589-5da2-4a15-9583-0855f592a602.png)\r\n hmmm, literally just result = suite.run follow by result.to_wandb.\r\nmy wandb version is 0.12.9\r\n what is the wandb and plotly version on the wandb server? This issue is stale and we couldn't reproduce it.\r\n@DL1992 feel free to reach out to us if this problem persists and we will try to help personally. Closing for now",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":5.8,
        "Solution_reading_time":6.64,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":76.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to make a GPT-2 model with deepspeed on an azure VM. I found ~2 bugs which I was able to patch, but I have stumbled upon a really tough one. You see, it says I need pytorch. No surprise. I install pytorch. It still says I don't have it. I used both pip and pip3 many times. I install pytorch from github and run setup.py. It says I need python 3. When I get python 3 it says the same. When I try google colab it gives me the following error:  <br \/>\n<code>Traceback (most recent call last):   <\/code>  File &quot;pretrain_gpt2.py&quot;, line 709, in &lt;module&gt;  <br \/>\n<code>   main()  <\/code>  File &quot;pretrain_gpt2.py&quot;, line 654, in main  <br \/>\n<code>   args.eod_token = get_train_val_test_data(args)  <\/code>  File &quot;pretrain_gpt2.py&quot;, line 600, in get_train_val_test_data  <br \/>\n<code>   args)  <\/code>  File &quot;\/content\/DeepSpeedExamples\/Megatron-LM\/configure_data.py&quot;, line 34, in apply  <br \/>\n<code>   return make_loaders(args)  <\/code>  File &quot;\/content\/DeepSpeedExamples\/Megatron-LM\/configure_data.py&quot;, line 170, in make_loaders  <br \/>\n<code>   train, tokenizer = data_utils.make_dataset(**data_set_args)  <\/code>  File &quot;\/content\/DeepSpeedExamples\/Megatron-LM\/data_utils\/<strong>init<\/strong>.py&quot;, line 109, in make_dataset  <br \/>\n<code>   ds = split_ds(ds, split)  <\/code>  File &quot;\/content\/DeepSpeedExamples\/Megatron-LM\/data_utils\/datasets.py&quot;, line 194, in split_ds  <br \/>\n<code>   rtn_ds[i] = SplitDataset(ds, split_inds)   <\/code> File &quot;\/content\/DeepSpeedExamples\/Megatron-LM\/data_utils\/datasets.py&quot;, line 134, in <strong>init<\/strong>  <br \/>\n <code>  self.lens = itemgetter(*self.split_inds)(list(self.wrapped_data.lens))  <\/code>TypeError: itemgetter expected 1 arguments, got 0  <\/p>\n<p>How do I fix both the google colab and the azure VM errors?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1610037769210,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/222550\/deepspeed-gpt-2-megatron-lm-problems",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.3,
        "Challenge_reading_time":23.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"Deepspeed gpt-2 megatron-LM problems",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":213,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=b4907c12-a726-468e-8576-0c4a9c876196\">@sammyboy123  <\/a>     <br \/>\nI would start by installing PyTorch via pip. Instructions can be found <a href=\"https:\/\/pytorch.org\/get-started\/locally\/\">here<\/a>. There is also a verification section which will test if you have PyTorch installed correctly. You also might find the DeepSpeed <a href=\"https:\/\/www.deepspeed.ai\/getting-started\/\">Getting Started page<\/a> helpful. There are specific tutorials for Azure and also a docker image available.    <\/p>\n<p>Let me know if this doesn't work for you or you are still facing issues.    <\/p>\n<p>-------------------------------    <\/p>\n<p>Please don\u2019t forget to <strong>&quot;Accept the answer&quot;<\/strong> and \u201cup-vote\u201d wherever the information provided helps you, this can be beneficial to other community members.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.3,
        "Solution_reading_time":10.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":99.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have subscribed to Standard_NC6 compute instance. has 56 GB RAM but only 10GB is allocated for the GPU. my model and data is huge which need at least 40GB Ram for gpu. how can I allocate more memory for the GPU ? \nI use Azure machine learning environment + notebooks \nalso I use pytorch for building my model <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1681156161463,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1215210\/limited-gpu-ram",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":3.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"limited gpu ram",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello Turki,<\/p>\n<p>The Standard_NC6 only has 12 GiB of RAM (GPU memory) as seen in:<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/bb1d7ed8-9421-421e-a25c-dfb2b026dd24?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/virtual-machines\/nc-series\">https:\/\/learn.microsoft.com\/en-us\/azure\/virtual-machines\/nc-series<\/a><\/p>\n<p>If your model requires 40 GiB of RAM you will have to upgrade to Standard_NC24 for at least 48 GiB of GPU memory (RAM).<\/p>\n<hr \/>\n<p>If this is helpful please accept answer.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.5,
        "Solution_reading_time":7.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1374169767267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":548.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":23.3159841667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run this on Amazon Sagemaker but I am getting this error while when I try to run it on my local machine, it works very fine.<\/p>\n<p>this is my code:<\/p>\n<pre><code>import tensorflow as tf\n\nimport IPython.display as display\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12,12)\nmpl.rcParams['axes.grid'] = False\n\nimport numpy as np\nimport PIL.Image\nimport time\nimport functools\n    \ndef tensor_to_image(tensor):\n  tensor = tensor*255\n  tensor = np.array(tensor, dtype=np.uint8)\n  if np.ndim(tensor)&gt;3:\n    assert tensor.shape[0] == 1\n    tensor = tensor[0]\n  return PIL.Image.fromarray(tensor)\n\ncontent_path = tf.keras.utils.get_file('YellowLabradorLooking_nw4.jpg', 'https:\/\/example.com\/IMG_20200216_163015.jpg')\n\n\nstyle_path = tf.keras.utils.get_file('kandinsky3.jpg','https:\/\/example.com\/download+(2).png')\n\n\ndef load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n\n\ndef imshow(image, title=None):\n  if len(image.shape) &gt; 3:\n    image = tf.squeeze(image, axis=0)\n\n  plt.imshow(image)\n  if title:\n    plt.title(title)\n\n\ncontent_image = load_img(content_path)\nstyle_image = load_img(style_path)\n\nplt.subplot(1, 2, 1)\nimshow(content_image, 'Content Image')\n\nplt.subplot(1, 2, 2)\nimshow(style_image, 'Style Image')\n\nimport tensorflow_hub as hub\nhub_module = hub.load('https:\/\/tfhub.dev\/google\/magenta\/arbitrary-image-stylization-v1-256\/1')\nstylized_image = hub_module(tf.constant(content_image), tf.constant(style_image))[0]\ntensor_to_image(stylized_image)\n\n\nfile_name = 'stylized-image5.png'\ntensor_to_image(stylized_image).save(file_name)\n<\/code><\/pre>\n<p>This is the exact error I get:<\/p>\n<pre><code>---------------------------------------------------------------------------\n<\/code><\/pre>\n<p>TypeError                                 Traceback (most recent call last)<\/p>\n<pre><code>&lt;ipython-input-24-c47a4db4880c&gt; in &lt;module&gt;()\n     53 \n     54 \n---&gt; 55 content_image = load_img(content_path)\n     56 style_image = load_img(style_path)\n     57 \n<\/code><\/pre>\n<p> in load_img(path_to_img)<\/p>\n<pre><code>     34 \n     35     shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n---&gt; 36     long_dim = max(shape)\n     37     scale = max_dim \/ long_dim\n     38 \n<\/code><\/pre>\n<p>~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/tensorflow\/python\/framework\/ops.py in <strong>iter<\/strong>(self)<\/p>\n<pre><code>    475     if not context.executing_eagerly():\n    476       raise TypeError(\n--&gt; 477           &quot;Tensor objects are only iterable when eager execution is &quot;\n    478           &quot;enabled. To iterate over this tensor use tf.map_fn.&quot;)\n    479     shape = self._shape_tuple()\n<\/code><\/pre>\n<p>TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1594076057097,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62765658",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":12.0,
        "Challenge_reading_time":41.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":null,
        "Challenge_title":"Tensorflow error. TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1530.0,
        "Challenge_word_count":281,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565307837780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":570.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>Your error is being raised in this function <code>load_img<\/code>:<\/p>\n<pre><code>def load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n<\/code><\/pre>\n<p>Specifically, this line:<\/p>\n<pre><code>    long_dim = max(shape)\n<\/code><\/pre>\n<p>You are passing a tensor to the <a href=\"https:\/\/docs.python.org\/3\/library\/functions.html#max\" rel=\"nofollow noreferrer\">built-in Python max function<\/a> in graph execution mode. You can only iterate through tensors in eager-execution mode. You probably want to use <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/math\/reduce_max\" rel=\"nofollow noreferrer\">tf.reduce_max<\/a> instead:<\/p>\n<pre><code>    long_dim = tf.reduce_max(shape)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1594159994640,
        "Solution_link_count":2.0,
        "Solution_readability":11.7,
        "Solution_reading_time":13.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1441651557140,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Parker, CO, USA",
        "Answerer_reputation_count":851.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":0.9014630556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to follow AWS Sagemaker tutorial to train a machine learning model with a Jupyter notebook environment. <\/p>\n\n<p>According to the tutorial, I'm supposed to copy the following code and run it to import required libraries and set environment variables. <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                \nimport pandas as pd                               \nimport matplotlib.pyplot as plt                   \nfrom IPython.display import Image                 \nfrom IPython.display import display               \nfrom time import gmtime, strftime                 \nfrom sagemaker.predictor import csv_serializer   \n\n# Define IAM role\nrole = get_execution_role()\nprefix = 'sagemaker\/DEMO-xgboost-dm'\ncontainers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com\/xgboost:latest',\n              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest',\n              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest',\n              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'} # each region has its XGBoost container\nmy_region = boto3.session.Session().region_name # set the region of the instance\nprint(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")\n<\/code><\/pre>\n\n<p>And the expected outcome is below.  <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wLJ2j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wLJ2j.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>However, I am getting this error.<\/p>\n\n<blockquote>\n  <p>KeyError                                  Traceback (most recent call last)\n   in ()\n       18               'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'} # each region has its XGBoost container\n       19 my_region = boto3.session.Session().region_name # set the region of the instance\n  ---> 20 print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")<\/p>\n  \n  <p>KeyError: 'ap-northeast-2'<\/p>\n<\/blockquote>\n\n<p>I assume that this is happening because my region is <strong>\"ap-northeast-2\"<\/strong>. \nI have a feeling that I need to change the containers for my region.  <\/p>\n\n<p><strong>If my guess is correct, how can I find containers for my region?<\/strong><br>\n<strong>Also, am I overlooking anything else?<\/strong> <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1576462664100,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59349805",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":32.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":null,
        "Challenge_title":"How to find XGBoost containers for different regions in AWS Sagemaker",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":2197.0,
        "Challenge_word_count":264,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1539556112483,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seoul, South Korea",
        "Poster_reputation_count":2954.0,
        "Poster_view_count":1143.0,
        "Solution_body":"<p>I expect your rational is correct. There isn't an entry for your region in the code. I don't know if there's a list of these containers per region. That being said, you find them in ECR (Elastic Container Registry). <\/p>\n\n<p>Keep in mind, that you can probably fix this quickly by switching to one of the supported regions. Otherwise:<\/p>\n\n<p>If AWS doesn't have a publicly listed container in your region, you can register the container yourself in AWS with ECR. You'll need to login to ECR using the AWS CLI and docker login.<\/p>\n\n<p>You can use the command <code>aws ecr get-login --region ap-northeast-2<\/code> in order to get the token you'll need for docker login.<\/p>\n\n<p>Then, clone this repo: <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-xgboost-container<\/a><\/p>\n\n<p>You can build this image locally and push it up to ECR. After that, login to the AWS console (or use the AWS CLI) and find the ARN of the image. It should match the format of the others in your code. <\/p>\n\n<p>After that, just add another key\/value entry into the code for your <code>containers<\/code> variable and use <code>'ap-northeast-2': '&lt;ARN of the docker image&gt;'<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1576465909367,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":15.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":187.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1369156710532,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Japan",
        "Answerer_reputation_count":189.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I execute code without parallel computation, <code>n_trials<\/code> in the <code>optimize<\/code> function means how many trials the program runs. When executed via parallel computation (following the tutorial <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/10_key_features\/004_distributed.html\" rel=\"nofollow noreferrer\">here<\/a> via launching it again in another console), it does <code>n_trials<\/code> for each process, not for all the sum of processes like I would like.<\/p>\n<p>Is there a way to make sure that the sum of all parallel processes' trials are equal to a fixed number, regardless of how many process I launch?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629886489473,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68920952",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":9.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to set n_trials for multiple processes when using parallelization?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":330.0,
        "Challenge_word_count":92,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529092998780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":788.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>Yes, <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.study.MaxTrialsCallback.html#optuna.study.MaxTrialsCallback\" rel=\"nofollow noreferrer\"><code>MaxTrialsCallback<\/code><\/a> is the exact feature for such a situation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":38.9,
        "Solution_reading_time":3.53,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to figure out about standard connectors between SAP ERP product and Azure ML especially for NLP scenarios. Can you please suggest on this.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664541861543,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1030800\/azure-ml-for-sap-erp",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":2.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML for SAP ERP",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":30,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=dfa9d536-725c-462d-87c8-47fbafb1a2bc\">@D-0887  <\/a> Thanks for the question. Here is the blog that could help and <a href=\"https:\/\/github.com\/microsoft\/nlp-recipes\">nlp recipes<\/a>.    <br \/>\n<a href=\"https:\/\/blogs.sap.com\/2022\/08\/03\/azure-machine-learning-triggering-calculations-ml-in-sap-data-warehouse-cloud\/\">https:\/\/blogs.sap.com\/2022\/08\/03\/azure-machine-learning-triggering-calculations-ml-in-sap-data-warehouse-cloud\/<\/a>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":26.2,
        "Solution_reading_time":6.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I keep getting this error in sagemaker when iterating through pytorch dataloader batch cycles:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;main.py&quot;, line 371, in &lt;module&gt;\n    g_scaler=g_scaler, d_scaler=d_scaler, runtime_log_folder=runtime_log_folder, runtime_log_file_name=runtime_log_file_name)\n  File &quot;main.py&quot;, line 78, in train_fn\n    for idx, (x, y) in enumerate(loop):\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/tqdm\/std.py&quot;, line 1171, in __iter__\n    for obj in iterable:\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 525, in __next__\n    (data, worker_id) = self._next_data()\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 1252, in _next_data\n    return (self._process_data(data), w_id)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 1299, in _process_data\n    data.reraise()\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/_utils.py&quot;, line 429, in reraise\n    raise self.exc_type(msg)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/botocore\/exceptions.py&quot;, line 84, in __init__\n    super(HTTPClientError, self).__init__(**kwargs)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/botocore\/exceptions.py&quot;, line 40, in __init__\n    msg = self.fmt.format(**kwargs)\nKeyError: 'error'\n\n---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n&lt;ipython-input-1-81655136a841&gt; in &lt;module&gt;\n     58                             py_version='py3')\n     59 \n---&gt; 60 pytorch_estimator.fit({'train': Runtime.dataset_path}, job_name=Runtime.job_name)\n     61 \n     62 #print(pytorch_estimator.latest_job_tensorboard_artifacts_path())\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n    955         self.jobs.append(self.latest_training_job)\n    956         if wait:\n--&gt; 957             self.latest_training_job.wait(logs=logs)\n    958 \n    959     def _compilation_job_name(self):\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   1954         # If logs are requested, call logs_for_jobs.\n   1955         if logs != &quot;None&quot;:\n-&gt; 1956             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   1957         else:\n   1958             self.sagemaker_session.wait_for_job(self.job_name)\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in logs_for_job(self, job_name, wait, poll, log_type)\n   3751 \n   3752         if wait:\n-&gt; 3753             self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n   3754             if dot:\n   3755                 print()\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3304                 ),\n   3305                 allowed_statuses=[&quot;Completed&quot;, &quot;Stopped&quot;],\n-&gt; 3306                 actual_status=status,\n   3307             )\n   3308 \n\nUnexpectedStatusException: Error for Training job 2022-06-03-05-16-49-pix2pix-U12239-2022-05-09-14-39-18-training: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/opt\/conda\/bin\/python3.6 main.py --runtime_var dataset_name=U12239-2022-05-09-14-39-18,job_name=2022-06-03-05-16-49-pix2pix-U12239-2022-05-09-14-39-18-training,model_name=pix2pix&quot;\n\n  0%|          | 0\/248 [00:00&lt;?, ?it\/s]\n  0%|          | 1\/248 [00:30&lt;2:07:28, 30.97s\/it]\n  0%|          | 1\/248 [00:30&lt;2:07:28, 30.97s\/it]\nTraceback (most recent call last):\n  File &quot;main.py&quot;, line 371, in &lt;module&gt;\n    g_scaler=g_scaler, d_scaler=d_scaler, runtime_log_folder=runtime_log_folder, runtime_log_file_name=runtime_log_file_name)\n  File &quot;main.py&quot;, line 78, in train_fn\n    for idx, (x, y) in enumerate(loop):\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/tqdm\/std.py&quot;, line 1171, in __iter__\n    for obj in iterable:\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 525, in __next__\n    (data, worker_id) = self._next_data()\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 1252, in _next_data\n    return (self\n<\/code><\/pre>\n<p>Here is the code which results in the error:<\/p>\n<pre><code>def train_fn(disc, gen, loader, opt_disc, opt_gen, l1, bce, g_scaler, d_scaler,runtime_log_folder,runtime_log_file_name):\n\n    total_output=''\n    \n    loop = tqdm(loader, leave=True)\n    device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n\n    print(&quot;Loop&quot;)\n    print(loop)\n    print(&quot;Length loop&quot;)\n    print(len(loop))\n    for idx, (x, y) in enumerate(loop): #&lt;--error happens here\n        print(&quot;Loop index&quot;)\n        print(idx)\n        print(&quot;Loop item&quot;)\n        print(x,y)\n        x = x.to(device)\n        y = y.to(device)\n        \n        # train discriminator\n        with torch.cuda.amp.autocast():\n            y_fake = gen(x)\n\n            D_real = disc(x, y)\n            D_fake = disc(x, y_fake.detach())\n            # use detach so as to avoid breaking computational graph when do optimizer.step on discriminator\n            # can use detach, or when do loss.backward put loss.backward(retain_graph = True)\n\n            D_real_loss = bce(D_real, torch.ones_like(D_real))\n            D_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n\n            D_loss = (D_real_loss + D_fake_loss) \/ 2\n            \n            # log tensorboard\n            \n        disc.zero_grad()\n        d_scaler.scale(D_loss).backward()\n        d_scaler.step(opt_disc)\n        d_scaler.update()\n        \n        # train generator\n        with torch.cuda.amp.autocast():\n            \n            D_fake = disc(x, y_fake)\n\n            # compute fake loss\n            # trick discriminator to believe these are real, hence send in torch.oneslikedfake\n            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n\n            # compute L1 loss\n            L1 = l1(y_fake, y) * args.l1_lambda\n\n            G_loss = G_fake_loss + L1\n            \n            # log tensorboard\n           \n        opt_gen.zero_grad()\n        g_scaler.scale(G_loss).backward()\n        g_scaler.step(opt_gen)\n        g_scaler.update()\n        \n        # print epoch, generator loss, discriminator loss\n        print(f'[Epoch {epoch}\/{args.num_epochs} (b: {idx})] [D loss: {D_loss}, D real loss: {D_real_loss}, D fake loss: {D_fake_loss}] [G loss: ##{G_loss}, G fake loss: {G_fake_loss}, L1 loss: {L1}]')\n        output = f'[Epoch {epoch}\/{args.num_epochs} (b: {idx})] [D loss: {D_loss}, D real loss: {D_real_loss}, D fake loss: {D_fake_loss}] [G loss: ##{G_loss}, G fake loss: {G_fake_loss}, L1 loss: {L1}]\\n'\n        total_output+=output\n\n\n\n    runtime_log = get_json_file_from_s3(runtime_log_folder, runtime_log_file_name)\n    runtime_log += total_output\n    upload_json_file_to_s3(runtime_log_folder,runtime_log_file_name,json.dumps(runtime_log))\n\n\n\ndef __getitem__(self, index):\n    print(&quot;Index &quot;,index)\n    pair_key = self.list_files[index]\n    print(&quot;Pair key &quot;,pair_key)\n    pair = Boto.s3_client.list_objects(Bucket=Boto.bucket_name, Prefix=pair_key, Delimiter='\/')\n\n    input_image_key = pair.get('Contents')[1].get('Key')\n    input_image_path = f's3:\/\/{Boto.bucket_name}\/{input_image_key}'\n    print(&quot;Input image path &quot;,input_image_path)\n    input_image_s3_source = get_file_from_filepath(input_image_path)\n    input_image = np.array(Image.open(input_image_s3_source))\n\n    target_image_key = pair.get('Contents')[0].get('Key')\n    target_image_path = f's3:\/\/{Boto.bucket_name}\/{target_image_key}'\n    print(&quot;Target image path &quot;,target_image_path)\n    target_image_s3_source = get_file_from_filepath(target_image_path)\n    target_image = np.array(Image.open(target_image_s3_source))\n\n    augmentations = config.both_transform(image=input_image, image0=target_image)\n\n    # get input image and target image by doing augmentations of images\n    input_image, target_image = augmentations['image'], augmentations['image0']\n\n    input_image = config.transform_only_input(image=input_image)['image']\n    target_image = config.transform_only_mask(image=target_image)['image']\n    \n    print(&quot;Input image size &quot;,input_image.size())\n    print(&quot;Target image size &quot;,target_image.size())\n    \n    return input_image, target_image\n\n<\/code><\/pre>\n<p>I did multiple runs and here are the traces of the failure points<\/p>\n<pre><code>i) 2022-06-03-05-00-04-pix2pix-U12239-2022-05-09-14-39-18-training\nNo index shown\n[Epoch 0\/100 (b: 0)]\n\nii) 2022-06-03-05-16-49-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\n[Epoch 0\/100 (b: 0)]\n\niii) 2022-06-03-05-44-46-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\n[Epoch 0\/100 (b: 0)]\n\niv) 2022-06-03-06-08-33-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\n[Epoch 1\/100 (b: 0)]\n\nv) 2022-06-15-02-49-20-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\nPair key  datasets\/training-data\/testing\/2022-05-09-14-39-18\/match-raws-finals\/U12239\/P423712\/Pair_71\/\n[Epoch 0\/100 (b: 0)\n\nvi) 2022-06-15-02-59-43-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  64\nPair key  datasets\/training-data\/testing\/2022-05-09-14-39-18\/match-raws-finals\/U12239\/P425642\/Pair_27\/\n[Epoch 0\/100 (b: 247)]\n\nvii) 2022-06-15-04-49-33-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  64\nPair key  datasets\/training-data\/testing\/2022-05-09-14-39-18\/match-raws-finals\/U12239\/P415414\/Pair_124\/\nNo specific epoch \n<\/code><\/pre>\n<p>My batch size is 248, so as you can see it seems to fail either at the start of the batch (0) or at the end (247). Also there are some common Indexes in the get item which seems to cause it to fail, namely Index 64 and Index 160. However there doesn't seem to be a common data point in the dataset that causes it to fail, as can be seen from the pair key all 3 data points in the datasets are different.<\/p>\n<p>Does anyone have any idea why this error happens please?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655303532370,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72633246",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":124.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":74,
        "Challenge_solved_time":null,
        "Challenge_title":"Error in pytorch data loader batch cycles",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":55.0,
        "Challenge_word_count":745,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1643118380396,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>Try to run the same training script outside of a SageMaker training job and see what happens.<br \/>\nIf the error doesn't happen on a standalone script, try to run it as a <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"nofollow noreferrer\">Local SageMaker training job<\/a>, so you can reproduce it in seconds instead of minutes, and potentially use a debugger to figure out what is the problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":6.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":1.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"I have an ML workload that involves providing predictions for large datasets on demand. The model is a PyTorch text classifier and the workload involves pricing predictions for 10's of thousands of records. The jobs arrive randomly but are very infrequent.\n\nI've created a standard Sagemaker endpoint and performance is acceptable when I do client batching, i.e. \n\n\n```\noutputs = []\nfor batch in create_minibatch(inputs, batch_size=128):\n    predictions = predictor.predict(batch)\n    outputs.extend(predictions)\n```\n\n\nThis takes around a minute for 25k records using a single instance.\n\nI've considered using the AWS Batch mode, but it takes around 4-6 minutes to create the job, so any benefit from the ability to scale up to multiple instances seems to be lost due to the start-up cost. Is it possible to use Sagemaker Batch processing with a persistent endpoint? \n\nThe alternative is to use client batching (using the code above) - but if I create multiple instances can I be sure that each batch is returned in the order I request? In the example above I need to `zip` the inputs and outputs.\n\nIs there a better way of serving this workload - I feel it falls somewhere in between the API and Batch paradigm?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677060099170,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1677406584411,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU41qCk_B9QIm5pwf28LsCLw\/sagemaker-batch-jobs",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":14.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker batch jobs",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":41.0,
        "Challenge_word_count":194,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\n\nPlease find my answers below.\n\n**Question:** Is it possible to use Sagemaker Batch processing with a persistent endpoint?\n\n**Answer:** Currently SageMaker does not have such option.\n\n**Question:** The alternative is to use client batching (using the code above) - but if I create multiple instances can I be sure that each batch is returned in the order I request? In the example above I need to zip the inputs and outputs.\n\n**Answer**: When you send each batch to your endpoint, the request is synchronous, and your application will be waiting to get a response right away. The response will be in the order you sent. So maintaining the order is a matter of how you manage your requests. Regardless of how many instances you use in the endpoint.\n\n\n**Question:** Is there a better way of serving this workload - I feel it falls somewhere in between the API and Batch paradigm?\n\n**Answer:**\n\nThere is two main issues at play here:\n1. Cost\n2. Time\n\nDepending on your business need and which of the above is more important for you, or if you need to find a balance between the two.\n\nSageMaker has the following inference options that might be useful for your case:\n1. [Real-time inference](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/realtime-endpoints.html): here you provision resources for the endpoint and it stays up and running, so when you need to do predictions you use it straight away with no wait. Cost is based time the endpoint was `InService` and number and type of instance (see [pricing page](https:\/\/aws.amazon.com\/sagemaker\/pricing\/)).\n2. [Batch Inference](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html): Here you send large number of records that you want to get predictions for. It requires resources to be provisioned first, which can take few minutes before actual prediction happens. Cost will be calculated based on the time it took to do predictions.\n\nTheoretically speaking, time took to do predictions in 1 or 2 should be very similar. However in 1, you provision the endpoint in advance, so when you invoke it, it feels faster, because your instance been there and ready. And comes with extra cost.\n\nNow if this option is good for you, you can always provision the endpoint in advance before you start predictions, and tear it down after. This way you get the benefit from both worlds. \n\nHowever, there is currently no option where the resource are persistent and you can just start predicting when you want. Just because of cost and operational considerations are in play here.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1677119127968,
        "Solution_link_count":3.0,
        "Solution_readability":8.4,
        "Solution_reading_time":31.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":25.0,
        "Solution_word_count":405.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1436771091480,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Brno, \u010cesko",
        "Answerer_reputation_count":51.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using AutoML called via Custom Python Script module in AzureML designer.\nFor that, I need to install automl packages:<\/p>\n\n<pre><code>os.system(f\"pip install azureml-sdk[automl]==1.0.85 --upgrade\")\n<\/code><\/pre>\n\n<p>It worked correctly, but now when I call automl training I received this error:<\/p>\n\n<pre><code>pkg_resources.ContextualVersionConflict: (azureml-dataprep 1.3.2 (\/azureml-envs\/azureml_8d08fe76aaa5abe0ec642fd2de335a04\/lib\/python3.6\/site-packages), Requirement.parse('azureml-dataprep&lt;1.2.0a,&gt;=1.1.37a'), {'azureml-automl-core'})\n<\/code><\/pre>\n\n<p>Looks like there was an update in azureml-dataprep to version 1.3.2 which is not compatible with azureml-sdk[automl]==1.0.85.<\/p>\n\n<ol>\n<li>Would it be possible to add AutoML packages as default package in AzureML designer?<\/li>\n<li>Would it be possible to update azureml-sdk version in AzureML designer?<\/li>\n<li>Is there any workaround right now?<\/li>\n<\/ol>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1584439428327,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1584442215112,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60720060",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":13.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"ContextualVersionConflict issue with azureml-automl-core in AzureML designer",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":210.0,
        "Challenge_word_count":103,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436771091480,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brno, \u010cesko",
        "Poster_reputation_count":51.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Fixed after release new version of AzureML SDK in designer and update script to:<\/p>\n\n<pre><code>os.system(f\"pip install azureml-sdk[automl]==1.4.0 --upgrade\")\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":2.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1319019150600,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3073.0,
        "Answerer_view_count":341.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run XGBoost on AWS Sagemaker and trying to call the container for XGBoost.<\/p>\n<pre><code>\ncontainers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com\/xgboost:latest',\n              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest',\n              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest',\n              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'}\n\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;binary:logistic&quot;,\n        &quot;num_round&quot;:50\n        }\n\nestimator = sagemaker.estimator.Estimator(image_name=containers['us-east-1'], \n                                          hyperparameters=hyperparameters,\n                                          role=sagemaker.get_execution_role(),\n                                          train_instance_count=1, \n                                          train_instance_type='ml.m5.2xlarge', \n                                          train_volume_size=5, # 5 GB \n                                          output_path=output_path,\n                                          train_use_spot_instances=True,\n                                          train_max_run=300,\n                                          train_max_wait=600)\n\n\n<\/code><\/pre>\n<p>However, running the following throws an error:<\/p>\n<pre><code>estimator.fit({'train': s3_input_train,'validation': s3_input_test})\n<\/code><\/pre>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Invalid DNS suffix 'amazonaws.com' for region 'us-east-1' in training image. Please provide the valid &lt;region&gt;.&lt;dns-suffix&gt;: 'ap-south-1.amazonaws.com'\n<\/code><\/pre>\n<p>Can someone help on how to fix this error? Thank you.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":5,
        "Challenge_created_time":1603828567090,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64561968",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":21.1,
        "Challenge_reading_time":21.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Building XGBoost on SageMaker",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":357.0,
        "Challenge_word_count":101,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1319019150600,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3073.0,
        "Poster_view_count":341.0,
        "Solution_body":"<p>The notebook instance was created in ap-south-1 and the S3 bucket was in us-east-1. Creating another notebook instance from the same region as the S3 bucket resolved the issue.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.9,
        "Solution_reading_time":2.28,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1393284798160,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Jose, CA",
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We are using SageMaker Batch Transform job and to fit as many records in a mini-batch as can fit within the <code>MaxPayloadInMB<\/code> limit, we are setting <code>BatchStrategy<\/code> to <code>MultiRecord<\/code> and <code>SplitType<\/code> to <code>Line<\/code>.<\/p>\n<p>Input to the SageMaker batch transform job is:<\/p>\n<pre><code>{&quot;requestBody&quot;: {&quot;data&quot;: {&quot;Age&quot;: 90, &quot;Experience&quot;: 26, &quot;Income&quot;: 30, &quot;Family&quot;: 3, &quot;CCAvg&quot;: 1}}, &quot;mName&quot;: &quot;loanprediction&quot;, &quot;mVersion&quot;: &quot;1&quot;, &quot;testFlag&quot;: &quot;false&quot;, &quot;environment&quot;: &quot;DEV&quot;, &quot;transactionId&quot;: &quot;5-687sdf87-0bc7e3cb3454dbf261ed1353&quot;, &quot;timestamp&quot;: &quot;2022-01-15T01:45:32.955Z&quot;}\n{&quot;requestBody&quot;: {&quot;data&quot;: {&quot;Age&quot;: 55, &quot;Experience&quot;: 26, &quot;Income&quot;: 450, &quot;Family&quot;: 3, &quot;CCAvg&quot;: 1}}, &quot;mName&quot;: &quot;loanprediction&quot;, &quot;mVersion&quot;: &quot;1&quot;, &quot;testFlag&quot;: &quot;false&quot;, &quot;environment&quot;: &quot;DEV&quot;, &quot;transactionId&quot;: &quot;5-69e22778-594916685f4ceca66c08bfbc&quot;, &quot;timestamp&quot;: &quot;2022-01-15T01:46:32.386Z&quot;}\n<\/code><\/pre>\n<p>This is the SageMaker batch transform job config:<\/p>\n<pre><code>apiVersion: sagemaker.aws.amazon.com\/v1\nkind: BatchTransformJob\nmetadata:\n        generateName: '...-batchtransform'\nspec:\n        batchStrategy: MultiRecord\n        dataProcessing:\n                JoinSource: Input\n                OutputFilter: $\n                inputFilter: $.requestBody\n        modelClientConfig:\n                invocationsMaxRetries: 0\n                invocationsTimeoutInSeconds: 3\n        mName: '..'\n        region: us-west-2\n        transformInput:\n                contentType: application\/json\n                dataSource:\n                        s3DataSource:\n                                s3DataType: S3Prefix\n                                s3Uri: s3:\/\/......\/part-\n                splitType: Line\n        transformOutput:\n                accept: application\/json\n                assembleWith: Line\n                kmsKeyId: '....'\n                s3OutputPath: s3:\/\/....\/batch_output\n        transformResources:\n                instanceCount: ..\n                instanceType: '..'\n<\/code><\/pre>\n<p>The SageMaker batch transform job fails with:<\/p>\n<p>Error in batch transform data-log -<\/p>\n<blockquote>\n<p>2022-01-27T00:55:39.781:[sagemaker logs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:<\/p>\n\n<p>2022-01-27T00:55:39.781:[sagemaker logs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:<\/p>\n400 Bad Request 2022-01-27T00:55:39.781:[sagemaker\nlogs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:\n<p>Failed to decode JSON object: Extra data: line 2 column 1 (char\n163)<\/p>\n<\/blockquote>\n<p><strong>Observation:<\/strong>\nThis issue occurs when we provide <code>batchStrategy: MultiRecord<\/code> in the manifest along with these data processing configs:<\/p>\n<pre><code>dataProcessing:\n        JoinSource: Input\n        OutputFilter: $\n        inputFilter: $.requestBody\n<\/code><\/pre>\n<p><strong>NOTE:<\/strong> If we put <code>batchStrategy: SingleRecord<\/code> along with the aforementioned data processing configs, it just works fine (job succeeds)!<\/p>\n<p><strong>Question:<\/strong> How can we achieve successful run with <code>batchStrategy: MultiRecord<\/code> along with the aforementioned data processing config?<\/p>\n<p>A successful output with <code>batchStrategy: SingleRecord<\/code> looks like this:<\/p>\n<blockquote>\n<p>{&quot;SageMakerOutput&quot;:{&quot;prediction&quot;:0},&quot;environment&quot;:&quot;DEV&quot;,&quot;transactionId&quot;:&quot;5-687sdf87-0bc7e3cb3454dbf261ed1353&quot;,&quot;mName&quot;:&quot;loanprediction&quot;,&quot;mVersion&quot;:&quot;1&quot;,&quot;requestBody&quot;:{&quot;data&quot;:{&quot;Age&quot;:90,&quot;CCAvg&quot;:1,&quot;Experience&quot;:26,&quot;Family&quot;:3,&quot;Income&quot;:30}},&quot;testFlag&quot;:&quot;false&quot;,&quot;timestamp&quot;:&quot;2022-01-15T01:45:32.955Z&quot;}\n{&quot;SageMakerOutput&quot;:{&quot;prediction&quot;:0},&quot;environment&quot;:&quot;DEV&quot;,&quot;transactionId&quot;:&quot;5-69e22778-594916685f4ceca66c08bfbc&quot;,&quot;mName&quot;:&quot;loanprediction&quot;,&quot;mVersion&quot;:&quot;1&quot;,&quot;requestBody&quot;:{&quot;data&quot;:{&quot;Age&quot;:55,&quot;CCAvg&quot;:1,&quot;Experience&quot;:26,&quot;Family&quot;:3,&quot;Income&quot;:450}},&quot;testFlag&quot;:&quot;false&quot;,&quot;timestamp&quot;:&quot;2022-01-15T01:46:32.386Z&quot;}\nRegion name \u2013 optional: Relevant resource ARN \u2013 optional:\narn:aws:sagemaker:us-west-2:435945521637:transform-job\/my-pipeline-9v28r-bat-e548fbfb125946528957e0f123456789<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1643344870580,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1643607224392,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70888883",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":22.6,
        "Challenge_reading_time":66.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker batch transform job failure for 'batchStrategy: MultiRecord' along with data processing",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":659.0,
        "Challenge_word_count":278,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1316155655123,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cupertino, CA, USA",
        "Poster_reputation_count":8141.0,
        "Poster_view_count":1066.0,
        "Solution_body":"<p>When your input data is in JSON line format and you choose a SingleRecord BatchStrategy, your container will receive a single JSON payload body like below<\/p>\n<pre><code>{ &lt;some JSON data&gt; }\n<\/code><\/pre>\n<p>However, if you use MultiRecord, Batch transform will split your JSON line input (which might contain 100 lines for example) into multiple records (say 10 records) all sent at once to your container as shown below:<\/p>\n<pre><code>{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n.\n.\n.\n{ &lt;some JSON data&gt; }\n<\/code><\/pre>\n<p>Therefore your container should be able to handle such input for it to work. However, from the error message, I can see it is complaining about invalid JSON format as it reads the second row of the request.<\/p>\n<p>I also noticed that you have supplied <code>ContentType<\/code> and <code>AcceptType<\/code> as <code>application\/json<\/code> but instead should be <code>application\/jsonlines<\/code><\/p>\n<p>Could you please test your container to see if it can handle multiple JSON line records per single invocation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":13.97,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":158.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":27.9829091667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to perform hyperparameter search using AzureML. My models are small (around 1GB) thus I would like to run multiple models on the same GPU\/node to save costs but I do not know how to achieve this.<\/p>\n<p>The way I currently submit jobs is the following (resulting in one training run per GPU\/node):<\/p>\n<pre><code>experiment = Experiment(workspace, experiment_name)\nconfig = ScriptRunConfig(source_directory=&quot;.\/src&quot;,\n                         script=&quot;train.py&quot;,\n                         compute_target=&quot;gpu_cluster&quot;,\n                         environment=&quot;env_name&quot;,\n                         arguments=[&quot;--args args&quot;])\nrun = experiment.submit(config)\n<\/code><\/pre>\n<p><code>ScriptRunConfig<\/code> can be provided with a <code>distributed_job_config<\/code>. I tried to use <code>MpiConfiguration<\/code> there but if this is done the run fails due to an MPI error that reads as if the cluster is configured to only allow one run per node:<\/p>\n<blockquote>\n<pre><code>Open RTE detected a bad parameter in hostfile: [...]\nThe max_slots parameter is less than the slots parameter:\nslots = 3\nmax_slots = 1\n[...] ORTE_ERROR_LOG: Bad Parameter in file util\/hostfile\/hostfile.c at line 407\n<\/code><\/pre>\n<\/blockquote>\n<p>Using <code>HyperDriveConfig<\/code> also defaults to submitting one run to one GPU and additionally providing a <code>MpiConfiguration<\/code> leads to the same error as shown above.<\/p>\n<p>I guess I could always rewrite my train script to train multiple models in parallel, s.t. each <code>run<\/code> wraps multiple trainings. I would like to avoid this option though, because then logging and checkpoint writes become increasingly messy and it would require a large refactor of the train pipeline. Also this functionality seems so basic that I hope there is a way to do this gracefully. Any ideas?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635412142523,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69751254",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.4,
        "Challenge_reading_time":23.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Submitting multiple runs to the same node on AzureML",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":364.0,
        "Challenge_word_count":241,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1396607378876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":107.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>Use Run.create_children method which will start child runs that are \u201clocal\u201d to the parent run, and don\u2019t need authentication.<\/p>\n<p>For AMLcompute max_concurrent_runs map to maximum number of nodes that will be used to run  a hyperparameter tuning run.\nSo there would be 1 execution per node.<\/p>\n<p>single service deployed but you can load multiple model versions in the init then the score function, depending on the request\u2019s param, uses particular model version to score.\nor with the new ML Endpoints (Preview).\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-endpoints\" rel=\"nofollow noreferrer\">What are endpoints (preview) - Azure Machine Learning | Microsoft Docs<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1635512880996,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":9.0,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":93.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nThere are some errors: https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3402182291\/jobs\/5657762171#step:3:1022\r\n\r\n```\r\n=========================== short test summary info ============================\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\n======================== 48 warnings, 3 errors in 3.79s ========================\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nINFO:submit_groupwise_azureml_pytest.py:Test execution completed!\r\n\r\n```\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":1668591.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668089448000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1841",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":32.96,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Error in some of the AzureML tests",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":152,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@pradnyeshjoshi any thoughts for this error?",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":0.57,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":6.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1492048364132,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cambridge, MA, USA",
        "Answerer_reputation_count":4436.0,
        "Answerer_view_count":713.0,
        "Challenge_adjusted_solved_time":351.9080963889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a large number of JSON requests for a model split across multiple files in an S3 bucket. I would like to use Sagemaker's Batch Transform feature to process all of these requests (I have done a couple of test runs using small amounts of data and the transform job succeeds). My main issue is here (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors<\/a>), specifically:<\/p>\n<blockquote>\n<p>If a batch transform job fails to process an input file because of a problem with the dataset, SageMaker marks the job as failed. If an input file contains a bad record, the transform job doesn't create an output file for that input file because doing so prevents it from maintaining the same order in the transformed data as in the input file. When your dataset has multiple input files, a transform job continues to process input files even if it fails to process one. The processed files still generate useable results.<\/p>\n<\/blockquote>\n<p>This is not preferable mainly because if 1 request fails (whether its a transient error, a malformmated request, or something wrong with the model container) in a file with a large number of requests, all of those requests will get discarded (even if all of them succeeded and the last one failed). I would ideally prefer Sagemaker to just write the output of the failed response to the file and keep going, rather than discarding the entire file.<\/p>\n<p>My question is, are there any suggestions to mitigating this issue? I was thinking about storing 1 request per file in S3, but this seems somewhat ridiculous? Even if I did this, is there a good way of seeing which requests specifically failed after the transform job finishes?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643261540133,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70873792",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":24.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"How to handle Sagemaker Batch Transform discarding a file with a failed model request",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":357.0,
        "Challenge_word_count":295,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597858315076,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":84.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>You've got the right idea: the fewer datapoints are in each file, the less likely a given file is to fail. The issue is that while you can pass a prefix with many files to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">CreateTransformJob<\/a>, partitioning one datapoint per file at least requires an S3 read per datapoint, plus a model invocation per datapoint, which is probably not great. Be aware also that <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=1000415&amp;tstart=0\" rel=\"nofollow noreferrer\">apparently there are hidden rate limits<\/a>.<\/p>\n<p>Here are a couple options:<\/p>\n<ol>\n<li><p>Partition into small-ish files, and plan on failures being rare. Hopefully, not many of your datapoints would actually fail. If you partition your dataset into e.g. 100 files, then a single failure only requires reprocessing 1% of your data. Note that Sagemaker has built-in retries, too, so most of the time failures should be caused by your data\/logic, not randomness on Sagemaker's side.<\/p>\n<\/li>\n<li><p>Deal with failures directly in your model. The same doc you quoted in your question also says:<\/p>\n<\/li>\n<\/ol>\n<blockquote>\n<p>If you are using your own algorithms, you can use placeholder text, such as ERROR, when the algorithm finds a bad record in an input file. For example, if the last record in a dataset is bad, the algorithm places the placeholder text for that record in the output file.<\/p>\n<\/blockquote>\n<p>Note that the reason Batch Transform does this whole-file failure is to maintain a 1-1 mapping between rows in the input and the output. If you can substitute the output for failed datapoints with an error message from inside your model, without actually causing the model itself to fail processing, Batch Transform will be happy.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1644528409280,
        "Solution_link_count":2.0,
        "Solution_readability":11.5,
        "Solution_reading_time":23.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":272.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"Seems that the Neptune_catalyst.ipynb is failing. \r\nPerhaps there is some type as it seems to be missing the `run` object. \r\nhttps:\/\/github.com\/neptune-ai\/examples\/runs\/2932574924?check_suite_focus=true",
        "Challenge_closed_time":1625030.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624893279000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/neptune-ai\/examples\/issues\/42",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.0,
        "Challenge_reading_time":3.03,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":160.0,
        "Challenge_repo_star_count":28.0,
        "Challenge_repo_watch_count":11.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Neptune_catalyst.ipynb fails",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":22,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"on it. #43 fixing here fixed in #43 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":0.5,
        "Solution_reading_time":0.41,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":8.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":1345114008840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Lyon, France",
        "Answerer_reputation_count":4233.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I uploaded a PlainText file in a JSON format to the new Azure Machine Learning Studio (studio.azureml.net), but I cannot connect the PlainText object with any module. I get all the time the error message \"Cannot connect PlainText to Dataset...\". <\/p>\n\n<p>At the documentation (<a href=\"http:\/\/help.azureml.net\/Content\/html\/e8219c57-e8dd-4989-9559-bbd73ba5bcea.htm\" rel=\"nofollow\">here<\/a>) is written that \"Plain text can be read and then split up into columns with the help of downstream preprocessing modules.\", but I can't find any downstream preprocessing modules.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1418490465200,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1446192454607,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/27461432",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":8.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Cannot connect PlainText (JSON) to Dataset at Azure Machine Learning",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":2157.0,
        "Challenge_word_count":85,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408374893790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>Actually Azure ML can't process JSON data. It will probably be added in a future update, but the easiest way (in my opinion) to consume that data is to convert it into CSV format. This can be done quickly with Power Query. Then you upload the CSV file as a new dataset.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":3.31,
        "Solution_score_count":6.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there any plan? Any date we can expect?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661977194510,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/989398\/is-azure-supporting-distributed-gpu",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":2.6,
        "Challenge_reading_time":1.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Is Azure supporting distributed GPU?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":13,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=7f2ff54e-2fc4-4d74-b946-fc6ec46d4863\">@nam  <\/a>     <\/p>\n<p>I hope yo are doing well. We have multiple options for Distributed GPU for Azure Machine Learnig for SDK v1 as below -     <br \/>\n<strong>Message Passing Interface (MPI)<\/strong>    <br \/>\nHorovod    <br \/>\nDeepSpeed    <br \/>\nEnvironment variables from Open MPI    <br \/>\n<strong>PyTorch<\/strong>    <br \/>\nProcess group initialization    <br \/>\nLaunch options    <br \/>\nDistributedDataParallel (per-process-launch)    <br \/>\nUsing torch.distributed.launch (per-node-launch)    <br \/>\nPyTorch Lightning    <br \/>\nHugging Face Transformers    <br \/>\n<strong>TensorFlow<\/strong>    <br \/>\nEnvironment variables for TensorFlow (TF_CONFIG)    <br \/>\n<strong>Accelerate GPU training with InfiniBand<\/strong>    <\/p>\n<p>For V2 there should be big change. Please feel free to let us know any problems. Thanks.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":11.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Here is a high-level picture of what I am trying to achieve: I want to train a LightGBM model with spark as a compute <a href=\"https:\/\/microsoft.github.io\/SynapseML\/docs\/features\/lightgbm\/about\/\" rel=\"nofollow noreferrer\">backend<\/a>, all in SageMaker using their Training Job api.\nTo clarify:<\/p>\n<ol>\n<li>I have to use LightGBM in general, there is no option here.<\/li>\n<li>The reason I need to use spark compute backend is because the training with the current dataset does not fit in memory anymore.<\/li>\n<li>I want to use SageMaker Training job setting so I could use SM Hyperparameter optimisation job to find the best hyperparameters for LightGBM. While LightGBM spark interface itself does offer some hyperparameter tuning capabilities, it does not offer Bayesian HP tuning.<\/li>\n<\/ol>\n<p>Now, I know the general approach to running custom training in SM: build a container in a certain way, and then just pull it from ECR and kick-off a training job\/hyperparameter tuning job through <code>sagemaker.Estimator<\/code> <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html\" rel=\"nofollow noreferrer\">API<\/a>. Now, in this case SM would handle resource provisioning for you, would create an instance and so on. What I am confused about is that essentially, to use spark compute backend, I would need to have an EMR cluster running, so the SDK would have to handle that as well. However, I do not see how this is possible with the API above.<\/p>\n<p>Now, there is also that thing called <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">Sagemaker Pyspark SDK<\/a>. However, the provided <code>SageMakerEstimator<\/code> API from that package does not support on-the-fly cluster configuration either.<\/p>\n<p>Does anyone know a way how to run a Sagemaker training job that would use an EMR cluster so that later the same job could be used for hyperparameter tuning activities?<\/p>\n<p>One way I see is to run an EMR cluster in the background, and then just create a regular SM estimator job that would connect to the EMR cluster and do the training, essentially running a spark driver program in SM Estimator job.<\/p>\n<p>Has anyone done anything similar in the past?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643032642193,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70835006",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":29.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"How to integrate spark.ml pipeline fitting and hyperparameter optimisation in AWS Sagemaker?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":196.0,
        "Challenge_word_count":340,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1357233199987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2171.0,
        "Poster_view_count":126.0,
        "Solution_body":"<p>Thanks for your questions. Here are answers:<\/p>\n<ul>\n<li><p><strong>SageMaker PySpark SDK<\/strong> <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/<\/a> does the opposite of what you want: being able to call a non-spark (or spark) SageMaker job from a Spark environment. Not sure that's what you need here.<\/p>\n<\/li>\n<li><p><strong>Running Spark in SageMaker jobs<\/strong>. While you can use SageMaker Notebooks to connect to a remote EMR cluster for interactive coding, you do not need EMR to run Spark in SageMaker jobs (Training and Processing). You have 2 options:<\/p>\n<ul>\n<li><p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_processing.html#pysparkprocessor\" rel=\"nofollow noreferrer\">SageMaker Processing has a built-in Spark Container<\/a>, which is easy to use but unfortunately not connected to SageMaker Model Tuning (that works with Training only). If you use this, you will have to find and use a third-party, external parameter search library ; for example <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-distributed-hyperparameter-and-neural-architecture-tuning-jobs-with-syne-tune\/\" rel=\"nofollow noreferrer\">Syne Tune<\/a> from AWS itself (that supports bayesian optimization)<\/p>\n<\/li>\n<li><p>SageMaker Training can run custom docker-based jobs, on one or multiple machines. If you can fit your Spark code within SageMaker Training spec, then you will be able to use SageMaker Model Tuning to tune your Spark code. However there is no framework container for Spark on SageMaker Training, so you would have to build your own, and I am not aware of any examples. Maybe you could get inspiration from the <a href=\"https:\/\/github.com\/aws\/sagemaker-spark-container\" rel=\"nofollow noreferrer\">Processing container code here<\/a> to build a custom Training container<\/p>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>Your idea of using the Training job as a client to launch an EMR cluster is good and should work (if SM has the right permissions), and will indeed allow you to use SM Model Tuning. I'd recommend:<\/p>\n<ul>\n<li>each SM job to create a new transient cluster (auto-terminate after step) to keep costs low and avoid tuning results to be polluted by inter-job contention that could arise if running everything on the same cluster.<\/li>\n<li>use the cheapest possible instance type for the SM estimator, because it will need to stay up during all duration of your EMR experiment to collect and print your final metric (accuracy, duration, cost...)<\/li>\n<\/ul>\n<p>In the same spirit, I once used SageMaker Training myself to launch Batch Transform jobs for the sole purpose of leveraging the bayesian search API to find an inference configuration that minimizes cost.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":12.6,
        "Solution_reading_time":35.55,
        "Solution_score_count":2.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":376.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1320746685067,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hamburg",
        "Answerer_reputation_count":7552.0,
        "Answerer_view_count":456.0,
        "Challenge_adjusted_solved_time":87.8971722222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run inference on a Tensorflow model deployed on SageMaker from a Python Spark job.\nI am running a (Databricks) notebook which has the following cell:<\/p>\n\n<pre><code>def call_predict():\n        batch_size = 1\n        data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n        tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n        prediction = predictor.predict(tensor_proto)\n        print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n<\/code><\/pre>\n\n<p>If I just call call_predict() it works fine:<\/p>\n\n<pre><code>call_predict()\n<\/code><\/pre>\n\n<p>and I get the output:<\/p>\n\n<pre><code>Process time: 65.261396\nOut[61]: {'model_spec': {'name': u'generic_model',\n  'signature_name': u'serving_default',\n  'version': {'value': 1578909324L}},\n 'outputs': {u'ages': {'dtype': 1,\n   'float_val': [5.680944442749023],\n   'tensor_shape': {'dim': [{'size': 1L}]}}}}\n<\/code><\/pre>\n\n<p>but when I try to call from a Spark context (in a UDF) I get a serialization error.\nThe code I'm trying to run is:<\/p>\n\n<pre><code>dataRange = range(1, 10001)\nrangeRDD = sc.parallelize(dataRange, 8)\nnew_data = rangeRDD.map(lambda x : call_predict())\nnew_data.count()\n<\/code><\/pre>\n\n<p>and the error I get is:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nPicklingError                             Traceback (most recent call last)\n&lt;command-2282434&gt; in &lt;module&gt;()\n      2 rangeRDD = sc.parallelize(dataRange, 8)\n      3 new_data = rangeRDD.map(lambda x : call_predict())\n----&gt; 4 new_data.count()\n      5 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in count(self)\n   1094         3\n   1095         \"\"\"\n-&gt; 1096         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n   1097 \n   1098     def stats(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in sum(self)\n   1085         6.0\n   1086         \"\"\"\n-&gt; 1087         return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n   1088 \n   1089     def count(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in fold(self, zeroValue, op)\n    956         # zeroValue provided to each partition is unique from the one provided\n    957         # to the final reduce call\n--&gt; 958         vals = self.mapPartitions(func).collect()\n    959         return reduce(op, vals, zeroValue)\n    960 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in collect(self)\n    829         # Default path used in OSS Spark \/ for non-credential passthrough clusters:\n    830         with SCCallSiteSync(self.context) as css:\n--&gt; 831             sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n    832         return list(_load_from_socket(sock_info, self._jrdd_deserializer))\n    833 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _jrdd(self)\n   2573 \n   2574         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n-&gt; 2575                                       self._jrdd_deserializer, profiler)\n   2576         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n   2577                                              self.preservesPartitioning, self.is_barrier)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _wrap_function(sc, func, deserializer, serializer, profiler)\n   2475     assert serializer, \"serializer should not be empty\"\n   2476     command = (func, profiler, deserializer, serializer)\n-&gt; 2477     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n   2478     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n   2479                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _prepare_for_python_RDD(sc, command)\n   2461     # the serialized command will be compressed by broadcast\n   2462     ser = CloudPickleSerializer()\n-&gt; 2463     pickled_command = ser.dumps(command)\n   2464     if len(pickled_command) &gt; sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  # Default 1M\n   2465         # The broadcast will have same life cycle as created PythonRDD\n\n\/databricks\/spark\/python\/pyspark\/serializers.pyc in dumps(self, obj)\n    709                 msg = \"Could not serialize object: %s: %s\" % (e.__class__.__name__, emsg)\n    710             cloudpickle.print_exec(sys.stderr)\n--&gt; 711             raise pickle.PicklingError(msg)\n    712 \n    713 \n\nPicklingError: Could not serialize object: TypeError: can't pickle _ssl._SSLSocket objects\n<\/code><\/pre>\n\n<p>Not sure what is this serialization error - does is complain about failing to deserialize the Predictor<\/p>\n\n<p>My notebook has a cell which was called prior to the above cells with the following imports:<\/p>\n\n<pre><code>import sagemaker\nimport boto3\nfrom sagemaker.tensorflow.model import TensorFlowPredictor\nimport tensorflow as tf\nimport numpy as np\nimport time\n<\/code><\/pre>\n\n<p>The Predictor was created with the following code:<\/p>\n\n<pre><code>sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY,\n                                aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\nsagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY,\n                                        aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\nboto_session = boto3.Session(region_name='us-east-1')\nsagemaker_session = sagemaker.Session(boto_session, sagemaker_client=sagemaker_client, sagemaker_runtime_client=sagemaker_runtime_client)\n\npredictor = TensorFlowPredictor('endpoint-poc', sagemaker_session)\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579190415880,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59773503",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":67.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":51,
        "Challenge_solved_time":null,
        "Challenge_title":"Using Sagemaker predictor in a Spark UDF function",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":322.0,
        "Challenge_word_count":474,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>The udf function will be executed by multiple spark tasks in parallel. Those tasks run in completely isolated python processes and they are scheduled to physically different machines. Hence each data, those functions reference, must be on the same node. This is the case for everything created within the udf.<\/p>\n\n<p>Whenever you reference any object outside of the udf from the function, this data structure needs to be serialised (pickled) to each executor. Some object state, like open connections to a socket, cannot be pickled.<\/p>\n\n<p>You need to make sure, that connections are lazily opened each executor. It must happen only on the first function call on that executor. The <a href=\"https:\/\/spark.apache.org\/docs\/latest\/streaming-programming-guide.html#design-patterns-for-using-foreachrdd\" rel=\"nofollow noreferrer\">connection pooling topic<\/a> is covered in the docs, however only in the spark streaming guide (though it also applies for normal batch jobs).<\/p>\n\n<p>Normally one can use the Singleton Pattern for this. But in python people use the Borgh pattern.<\/p>\n\n<pre><code>class Env:\n    _shared_state = {\n        \"sagemaker_client\": None\n        \"sagemaker_runtime_client\": None\n        \"boto_session\": None\n        \"sagemaker_session\": None\n        \"predictor\": None\n    }\n    def __init__(self):\n        self.__dict__ = self._shared_state\n        if not self.predictor:\n            self.sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n            self.sagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\n            self.boto_session = boto3.Session(region_name='us-east-1')\n            self.sagemaker_session = sagemaker.Session(self.boto_session, sagemaker_client=self.sagemaker_client, sagemaker_runtime_client=self.sagemaker_runtime_client)\n\n            self.predictor = TensorFlowPredictor('endpoint-poc', self.sagemaker_session)\n\n\n#....\ndef call_predict():\n   env = Env()\n   batch_size = 1\n   data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n   tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n   prediction = env.predictor.predict(tensor_proto)\n\n   print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n\nnew_data = rangeRDD.map(lambda x : call_predict())\n<\/code><\/pre>\n\n<p>The Env class is defined on the master node. Its <code>_shared_state<\/code> has empty entries. When then Env object is instantiated first time, it shares the state with all further instances of Env on any subsequent call to the udf. On each separate parallel running process this will happen exactly one time. This way the sessions are shared and do not need to pickled. <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1579506845700,
        "Solution_link_count":1.0,
        "Solution_readability":12.0,
        "Solution_reading_time":35.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":288.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1592412555903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":51.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am running into the following error when I try to run Automated ML through the studio on a GPU compute cluster:<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/uLyxr.png\" alt=\"Azure ML error message\" \/><\/p>\n<blockquote>\n<p>Error: AzureMLCompute job failed. JobConfigurationMaxSizeExceeded: The\nspecified job configuration exceeds the max allowed size of 32768\ncharacters. Please reduce the size of the job's command line arguments\nand environment settings<\/p>\n<\/blockquote>\n<p>The attempted run is on a registered tabulated dataset in filestore and is a simple regression case. Strangely, it works just fine with the CPU compute instance I use for my other pipelines. I have been able to run it a few times using that and wanted to upgrade to a cluster only to be hit by this error. I found online that it could be a case of having the following setting: AZUREML_COMPUTE_USE_COMMON_RUNTIME:false; but I am not sure where to put this in when just running from the web studio.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638986274657,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1641204588636,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70279636",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.3,
        "Challenge_reading_time":13.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Auto ML JobConfigurationMaxSizeExceeded error when using a cluster",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":171.0,
        "Challenge_word_count":160,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1592412555903,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":51.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>It looks like the bug was fixed. I just ran it on a cluster without changing any of the parameters. Thank you Yutong for the help!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.1,
        "Solution_reading_time":1.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>I make the following sweep (yaml) file:<\/p>\n<pre><code class=\"lang-auto\">program: train_mnist.py\nmethod: grid\nparameters:\n  lr_schedule:\n    values: [ step, cyclic ]\n  epoch_total:\n    values: [ 2, 4 ]\nmetric:\n  goal: maximize\n  name: test-result\/accuracy\nproject: my-mnist-test-project\nname: MNIST-Sweep-Test\ndescription: test sweep demo\n<\/code><\/pre>\n<p>and I use <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/advanced-sweeps\/local-controller#running-the-local-controller-from-the-command-line\">local controller<\/a> to perform sweep locally. However, it seems block here:<\/p>\n<pre><code class=\"lang-auto\">(pytorch) geyao@geyaodeMacBook-Air wandb_test % wandb sweep --controller sweep_config.yaml\nwandb: Creating sweep from: sweep_config.yaml\nwandb: Created sweep with ID: o2mzl569\nwandb: View sweep at: https:\/\/wandb.ai\/geyao\/my-mnist-test-project\/sweeps\/o2mzl569\nwandb: Run sweep agent with: wandb agent geyao\/my-mnist-test-project\/o2mzl569\nwandb: Starting wandb controller...\nSweep: o2mzl569 (grid) | Runs: 0\n\n# ------blocked here!------\n<\/code><\/pre>\n<p>When I turn off the network, it will be:<\/p>\n<pre data-code-wrap=\"shell\"><code class=\"lang-nohighlight\">(pytorch) geyao@geyaodeMacBook-Air wandb_test % wandb sweep --controller sweep_config.yaml\nwandb: Creating sweep from: sweep_config.yaml\nwandb: Network error (ConnectionError), entering retry loop.\n<\/code><\/pre>\n<p>Why local controller tries to connect the network? How can I perform local sweep with\/without network in the right way?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660803433186,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/local-controller-seems-block\/2955",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":12.1,
        "Challenge_reading_time":19.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Local controller seems block",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":346.0,
        "Challenge_word_count":148,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/geyao\">@geyao<\/a> , the local controller doesn\u2019t have the full functionality of W&amp;B cloud, and is not intended for actual hyperparameter optimization workloads. It\u2019s intended for development and debugging of new algorithms for the Sweeps tool. You don\u2019t need to connect to W&amp;B cloud service to use the controller.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":4.54,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":50.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2159.1657988889,
        "Challenge_answer_count":2,
        "Challenge_body":"Is there a link that shows how much GPU memory is available on the following GPU instances on AWS?\n\n1. g4-series instances (NVidia T4)\n2. g5-series instances (NVidia A10)\n3. p3d-series instances (NVidia V100)\n4. p4d-series instances (NVidia A100)\n\nUpdate: the information is available for the [p3d series](https:\/\/aws.amazon.com\/ec2\/instance-types\/p3\/) and [g5 series](https:\/\/aws.amazon.com\/ec2\/instance-types\/g5\/), though not for the [g4 series](https:\/\/aws.amazon.com\/ec2\/instance-types\/g4\/) or the [p4 series](https:\/\/aws.amazon.com\/ec2\/instance-types\/p4\/) instances. Is it possible to retrieve the information for the latter two instances anywhere (without having to launch the instances)?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643029354176,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668613197792,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUvPdBv2rwTEiYKKHDPLUTWA\/how-much-gpu-memory-are-available-on-the-g4-g5-p3d-and-p4d-series-instances",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":9.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"How much GPU memory are available on the g4, g5, p3d, and p4d series instances?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":2195.0,
        "Challenge_word_count":95,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This link has a table that compares instances' GPU memory\nhttps:\/\/docs.amazonaws.cn\/en_us\/AmazonECS\/latest\/developerguide\/ecs-gpu.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1676386194668,
        "Solution_link_count":1.0,
        "Solution_readability":24.1,
        "Solution_reading_time":1.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1497960178323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":12088.0,
        "Answerer_view_count":3630.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running pyspark from an Azure Machine Learning notebook. I am trying to move a file using the dbutil module.<\/p>\n\n<pre><code>from pyspark.sql import SparkSession\n    spark = SparkSession.builder.getOrCreate()\n    def get_dbutils(spark):\n        try:\n            from pyspark.dbutils import DBUtils\n            dbutils = DBUtils(spark)\n        except ImportError:\n            import IPython\n            dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n        return dbutils\n\n    dbutils = get_dbutils(spark)\n    dbutils.fs.cp(\"file:source\", \"dbfs:destination\")\n<\/code><\/pre>\n\n<p>I got this error: \nModuleNotFoundError: No module named 'pyspark.dbutils'\nIs there a workaround for this? <\/p>\n\n<p>Here is the error in another Azure Machine Learning notebook:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-1-183f003402ff&gt; in get_dbutils(spark)\n      4         try:\n----&gt; 5             from pyspark.dbutils import DBUtils\n      6             dbutils = DBUtils(spark)\n\nModuleNotFoundError: No module named 'pyspark.dbutils'\n\nDuring handling of the above exception, another exception occurred:\n\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-1-183f003402ff&gt; in &lt;module&gt;\n     10         return dbutils\n     11 \n---&gt; 12 dbutils = get_dbutils(spark)\n\n&lt;ipython-input-1-183f003402ff&gt; in get_dbutils(spark)\n      7         except ImportError:\n      8             import IPython\n----&gt; 9             dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n     10         return dbutils\n     11 \n\nKeyError: 'dbutils'\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":5,
        "Challenge_created_time":1588351032100,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1591892764407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61546680",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":12.2,
        "Challenge_reading_time":19.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"ModuleNotFoundError: No module named 'pyspark.dbutils'",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":7629.0,
        "Challenge_word_count":151,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330373362200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kennett Square, PA",
        "Poster_reputation_count":445.0,
        "Poster_view_count":104.0,
        "Solution_body":"<p>This is a known issue with Databricks Utilities - DButils.<\/p>\n\n<p>Most of DButils aren't supported for Databricks Connect. The only parts that do work are <strong>fs<\/strong> and <strong>secrets<\/strong>. <\/p>\n\n<p><strong>Reference:<\/strong> <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/dev-tools\/databricks-connect#limitations\" rel=\"nofollow noreferrer\">Databricks Connect - Limitations<\/a> and <a href=\"https:\/\/datathirst.net\/blog\/2019\/3\/7\/databricks-connect-limitations\" rel=\"nofollow noreferrer\">Known issues<\/a>.<\/p>\n\n<p><strong>Note:<\/strong> Currently fs and secrets work (locally). Widgets (!!!), libraries etc do not work. This shouldn\u2019t be a major issue. If you execute on Databricks using the Python Task dbutils will fail with the error:<\/p>\n\n<pre><code>ImportError: No module named 'pyspark.dbutils'\n<\/code><\/pre>\n\n<p>I'm able to execute the query successfully by running as a notebook.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RFVm8.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RFVm8.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.0,
        "Solution_reading_time":14.37,
        "Solution_score_count":3.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi all,     <\/p>\n<p>I am following the steps on this tutorial:     <br \/>\nTutorial: Score machine learning models with PREDICT in serverless Apache Spark pools <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\">https:\/\/learn.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool<\/a>      <\/p>\n<p>I don't know what is the AML_MODEL_URI. I thought it was the REST endpoint or the Swagger URI from the endpoint.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/150362-image.png?platform=QnA\" alt=\"150362-image.png\" \/>    <\/p>\n<p>But it is not working. I am getting this error on Synapse: &quot;RuntimeError: Load model failed    <br \/>\nTraceback (most recent call last):&quot;    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/150308-image.png?platform=QnA\" alt=\"150308-image.png\" \/>    <\/p>\n<p>I appreciate you help.    <\/p>\n<p>Kind regards,     <br \/>\nAnaid     <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1637180271803,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/631200\/what-is-aml-model-uri-predict-in-serverless-apache",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":14.8,
        "Challenge_reading_time":14.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"What is AML_MODEL_URI - PREDICT in serverless Apache Spark pools (Synapse & Azure Machine learning AML)",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":103,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=4bb27b25-616e-491c-b986-136b5bf96f77\">@Anaid  <\/a>,    <\/p>\n<p>Thanks for the question and using MS Q&amp;A platform.    <\/p>\n<blockquote>\n<p>AML_MODEL_URL is the same name of the model in the ML workspace with (follow the format of <code>aml:\/\/<\/code> + Name of the Model).    <\/p>\n<\/blockquote>\n<p>Example: <code>aml:\/\/sklearn_regression_model:1<\/code> (follow the format of <code>aml:\/\/<\/code> + Name of the Model).    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/153599-image.png?platform=QnA\" alt=\"153599-image.png\" \/>    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":13.2,
        "Solution_reading_time":22.33,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":163.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":23.80566,
        "Challenge_answer_count":1,
        "Challenge_body":"1. We trained custom model with manually annotated data set of documents but the accuracy is low, we want to annotate and train again. When I create a new version will it learn from the current set of inputs and also preserve the old training ? Do I need to give all the data for every incremental training ? \n\n2. Since there is some low accuracy issue, I want to add a2i . How to do it in console UI for batch processing ?\nWhen the people make additional annotation in a2i, will the comprehend learn incrementally ?\nOr do we need to make run the training job again ?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676615139900,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1676961564280,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUuwusX1xNQO6J-M-AkCFlig\/comprehend-incremental-training",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":7.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Comprehend incremental training",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":53.0,
        "Challenge_word_count":106,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You will need to provide all of the data for each incremental training. \n\nComprehend is not integrated with A2I at this time, so you would need to re-submit a new training job with all the annotations.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1677047264656,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":2.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1421401313787,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":326.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm stuck with the MLFlow model registry. Does anyone know how to load a model using the object &quot;mlflow.tracking.client.MlflowClient&quot;?<\/p>\n<p>I would like to do a predict after with that. I'm sure I'm wrong somewhere because I've already done that in the past. I'm not able to find it in the doc, in the web.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609255468723,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1611139575950,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65494496",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.9,
        "Challenge_reading_time":4.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to load a model using the object \"mlflow.tracking.client.MlflowClient\"?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":608.0,
        "Challenge_word_count":62,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423640080283,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lyon, France",
        "Poster_reputation_count":457.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>You'll have to make use of <code>mlflow.&lt;model_flavor&gt;.load_model()<\/code> to load a given model from the Model Registry. For example:<\/p>\n<pre><code>import mlflow.pyfunc\n\nmodel = mlflow.pyfunc.load_model(\n          model_uri=&quot;models:\/&lt;model_name&gt;\/&lt;model_version&gt;&quot;\n          )\n\nmodel.predict(...)\n<\/code><\/pre>\n<p>With <code>mlflow.tracking.client.MlflowClient<\/code> you can retrieve metadata about a model from the model registry, but for retrieving the actual model you will need to use <code>mlflow.&lt;model_flavor&gt;.load_model<\/code>. For example, you could use the MlflowClient to get the download URI for a given model, and then use <code>mlflow.&lt;flavor&gt;.load_model<\/code> to retrieve that model.<\/p>\n<pre><code>model_uri = client.get_model_version_download_uri(&quot;&lt;model_name&gt;&quot;, &lt;version&gt;)\nmodel = mlflow.pyfunc.load_model(model_uri)\n\nmodel.predict(...)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.4,
        "Solution_reading_time":12.16,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":81.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":25,
        "Challenge_body":"**Describe the bug**\r\nStarting in version 2.0.9 the neptune_ml widget is having an issue where the json values being passed in are getting the following error \r\n```\r\n{'error': JSONDecodeError('Expecting value: line 1 column 1 (char 0)',)}\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run through the 01-Introduction-to-Node-Classification-Gremlin notebook\r\n2. When you get to the export step the error occurs\r\n\r\n**Additional context**\r\nThis is not a problem in version 2.0.7",
        "Challenge_closed_time":1620330.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615509404000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/81",
        "Challenge_link_count":0,
        "Challenge_participation_count":25,
        "Challenge_readability":10.2,
        "Challenge_reading_time":6.48,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Neptune_ML widget error in 2.0.9",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":74,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This appears to be an issue with the versions of `ipython` that SageMaker is using.  If you update the Lifecycle start script by putting the following code at the bottom (just before EOF) and stopping and starting the notebook.\r\n```\r\nsource activate JupyterSystemEnv\r\npip install --upgrade ipython==7.16.1\r\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\r\n``` Hi, i have updated the Lifecycle scripts as suggested and that works - but then it fails on the training:\r\n\r\n`\"status\": \"Failed\",\r\n    \"failureReason\": \"ClientError: Failed to download data`\r\n\r\n...\r\npreloading-2021-04-05-17-33-3910000\/preloading-output\/graph.bin has an illegal char sub-sequence '\/\/' in it\"`\r\n\r\ni just used the movie lens database and steps in the notebook. it adds an extra '\\' in the \"outputLocation\"...?\r\n\r\ncan you help?  \r\n Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?  > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n\r\n\r\n<img width=\"1103\" alt=\"error_train_screen\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705359-4123d580-96d5-11eb-9b65-59e38f3e5140.png\">\r\n > > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n> \r\n> <img alt=\"error_train_screen\" width=\"1103\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705359-4123d580-96d5-11eb-9b65-59e38f3e5140.png\">\r\n\r\n<img width=\"1117\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705546-73cdce00-96d5-11eb-81fa-633c14942847.png\">\r\n > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n\r\nhi @austinkline  - thanks for helping me out. So as you can see from the screenshots, it fails to download the data and seems to be adding an extra slash...\r\n\r\nso I changed the script: `--s3-processed-uri {str(s3_bucket_uri)}preloading \"\"\"` \r\nand it then ran fine.... perhaps you want to correct that in the notebook?\r\n\r\nbut when making the prediction I am getting:\r\n\r\n<img width=\"1120\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113713442-42f29680-96df-11eb-8dc8-131e8377fa4c.png\">\r\n\r\n\r\nso Toy Story comes up as 'Thriller\" and not 'Comedy' as  per the notebook\r\n\r\n\r\nhow can I see which actual model the classification is using? Is it a graph convolutional network, I recall seeing that in the notebooks in the repository. It would be good to see the actual DGL model & code. \r\n\r\nThanks!!\r\n Thanks for the info. I'll spend some time reproducing and get back to you I was not able to reproduce this issue after running a fresh notebook created via cloud-formation found in our public docs\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/8711160\/113755017-afac6780-96c4-11eb-86ee-42798d595609.png)\r\n\r\n@Kristof-Neys I wonder if the state of the notebook got mixed up somehow? I would suggest creating a fresh notebook instance and trying again. The bug which needed the workaround lifecycle configuration has been resolved and released to pypi so that is not needed anymore > I was not able to reproduce this issue after running a fresh notebook created via cloud-formation found in our public docs\r\n> \r\n> ![image](https:\/\/user-images.githubusercontent.com\/8711160\/113755017-afac6780-96c4-11eb-86ee-42798d595609.png)\r\n> \r\n> @Kristof-Neys I wonder if the state of the notebook got mixed up somehow? I would suggest creating a fresh notebook instance and trying again. The bug which needed the workaround lifecycle configuration has been resolved and released to pypi so that is not needed anymore\r\n\r\nthank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?  > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n\r\nChecked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console. \r\n > > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n> \r\n> Checked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console.\r\n\r\nyeah thanks - just found it in the S3, says rgcn which presumably stands for the relational graph convolutional network > > > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n> > \r\n> > \r\n> > Checked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console.\r\n> \r\n> yeah thanks - just found it in the S3, says rgcn which presumably stands for the relational graph convolutional network\r\n\r\nyes. that's correct. Hi @Kristof-Neys and updates? Did recreating work for you? Hi @austinkline - thanks for reaching out. I have been caught up in another project but was just about to look at it. I'll update you guys probably tomorrow.  hi @austinkline & Team, i am finally getting around to this. I started everything new but now I cannot export the configuration any more, I get the following error:\r\n`{'error': ConnectionError(MaxRetryError(\"HTTPSConnectionPool(host='none', port=443): Max retries exceeded with url: \/neptune-export (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f28f5e81748>: Failed to establish a new connection: [Errno -2] Name or service not known',))\",),)}`\r\n\r\nUPdate: when I re-started everything and used the notebook of last week... i get\r\n\r\n`403 \"Missing Authentication Token\" `\r\n\r\n\r\n\r\nany ideas? Thanks!!\r\n     Let's start by gathering what version you're running again and what your configuration looks like. What we want to figure out is whether the exporter or Neptune is throwing the exception provided. That is to say, was the exporter unable to be called due to a missing auth token, or did the exporter start and then it was unable to communicate with Neptune. You also could take a look at cloudwatch logs for your api gateway on the corresponding exporter resource and see if it has any additional info you can point to. I'll go ahead and provision a fresh stack and see if I get the same issue once we've confirmed your auth setting.\r\n\r\nCan you provide your notebook version and configuration by running the following:\r\n\r\n1. What cell did you execute that gave you the above mentioned error?\r\n\r\n2. What version of `graph-notebook` are you running?\r\n```\r\n%graph_notebook_version\r\n```\r\n\r\n3. What is your configuration? Really we just care about the authentication setting\r\n**NOTE: PLEASE ERASE OR BLOCK OUT YOUR HOST ENDPOINT FROM YOUR CONFIGURATION WHEN PROVIDING THIS INFO**\r\n\r\n```\r\n%graph_notebook_config\r\n```\r\n > Let's start by gathering what version you're running again and what your configuration looks like. What we want to figure out is whether the exporter or Neptune is throwing the exception provided. That is to say, was the exporter unable to be called due to a missing auth token, or did the exporter start and then it was unable to communicate with Neptune. You also could take a look at cloudwatch logs for your api gateway on the corresponding exporter resource and see if it has any additional info you can point to. I'll go ahead and provision a fresh stack and see if I get the same issue once we've confirmed your auth setting.\r\n> \r\n> Can you provide your notebook version and configuration by running the following:\r\n> \r\n>     1. What cell did you execute that gave you the above mentioned error?\r\n> \r\n>     2. What version of `graph-notebook` are you running?\r\n> \r\n> \r\n> ```\r\n> %graph_notebook_version\r\n> ```\r\n> \r\n>     1. What is your configuration? Really we just care about the authentication setting\r\n>        **NOTE: PLEASE ERASE OR BLOCK OUT YOUR HOST ENDPOINT FROM YOUR CONFIGURATION WHEN PROVIDING THIS INFO**\r\n> \r\n> \r\n> ```\r\n> %graph_notebook_config\r\n> ```\r\n\r\n@austinkline thank you! Very much appreciate taking time & effort. Ok, so these are the detail:\r\n\r\ncell that I am running:\r\n`%%neptune_ml export start --export-url {neptune_ml.get_export_service_host()} --export-iam --wait --store-to export_results\r\n${export_params}`\r\n=> this gives me error: \r\n`{\r\n  \"message\": \"Missing Authentication Token\"\r\n}`\r\n\r\n\r\nVersion graph-notebook: 2.1.0\r\n\r\n%graph_notebook_config:\r\n`{\r\n  \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"DEFAULT\",\r\n  \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n  \"ssl\": true,\r\n  \"aws_region\": \"us-east-1\",\r\n  \"sparql\": {\r\n    \"path\": \"sparql\"\r\n  }\r\n}`\r\n\r\nThe strange thing is that all worked well two weeks ago, altho I did get wrong predictions, but at least the export worked and I could train model and get predictions etc. Now I cannot get beyond the export.... \r\n\r\nthank you again\r\n\r\n @Kristof-Neys I believe I found the bug we're dealing with. Can you flip IAM auth on in your config and see if the exporter\/other components work?\r\n\r\n```\r\n%%graph_notebook_config\r\n{\r\n  \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"IAM\",\r\n  \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n  \"ssl\": true,\r\n  \"aws_region\": \"us-east-1\",\r\n  \"sparql\": {\r\n    \"path\": \"sparql\"\r\n  }\r\n}\r\n```\r\n\r\nNote that we're changing the auth mode to IAM > @Kristof-Neys I believe I found the bug we're dealing with. Can you flip IAM auth on in your config and see if the exporter\/other components work?\r\n> \r\n> ```\r\n> %%graph_notebook_config\r\n> {\r\n>   \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n>   \"port\": 8182,\r\n>   \"auth_mode\": \"IAM\",\r\n>   \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n>   \"ssl\": true,\r\n>   \"aws_region\": \"us-east-1\",\r\n>   \"sparql\": {\r\n>     \"path\": \"sparql\"\r\n>   }\r\n> }\r\n> ```\r\n> \r\n> Note that we're changing the auth mode to IAM\r\nhey @austinkline  - that worked!, export and training went fine....but still predicting the wrong genre.... - how can this be??\r\n\r\n<img width=\"904\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/115867858-82d1b180-a433-11eb-809c-a1aa733e5d90.png\">\r\n\r\n\r\n @Kristof-Neys The issue you are seeing is actually one where the text in the notebook is incorrect.  Drama is what is coming back from the model that is generated .  I have created an issue to track this https:\/\/github.com\/aws\/graph-notebook\/issues\/116 and will address this with the additional feedback on those notebooks in the near future.  > @Kristof-Neys The issue you are seeing is actually one where the text in the notebook is incorrect. Drama is what is coming back from the model that is generated . I have created an issue to track this #116 and will address this with the additional feedback on those notebooks in the near future.\r\n\r\nok understood - thank you\r\n Closing this out since we're tracking the reported issue of notebooks being out of date in #116. Please cut us a new ticket if you run into any further issues! Hi guys, I'm facing a similar issue, I applied your fix(setting \"auth_mode\": \"IAM\") but did not work, any suggestions? Hi @llealgt , is this referring to the same issue mentioned at https:\/\/github.com\/aws\/graph-notebook\/issues\/445#issuecomment-1426192856? Hi @michaelnchin, nope, it's not the same, this happens when running notebook \r\nNeptune-ML-01-Introduction-to-Node-Classification-Gremlin\r\nThe other errors happen in notebook \r\nNeptune-ML-00-Getting-Started-with-Neptune-ML-Gremlin\r\nI guess it is related but they are different errors in different notebooks.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":9.0,
        "Solution_readability":9.2,
        "Solution_reading_time":145.23,
        "Solution_score_count":null,
        "Solution_sentence_count":102.0,
        "Solution_word_count":1555.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I was wondering if anybody from the W&amp;B team can confirm that there is an outage at the moment.<\/p>\n<p>I\u2019ve been having issues starting runs and it seems like other folks are having issues syncing runs with a network time out error (<a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/4424\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">[CLI]: canno't sync my runs \u00b7 Issue #4424 \u00b7 wandb\/wandb \u00b7 GitHub<\/a>). It\u2019s been ongoing for about 2 hours now.<\/p>\n<p>The status page is saying everything is fine - <a href=\"https:\/\/status.wandb.com\" rel=\"noopener nofollow ugc\">https:\/\/status.wandb.com<\/a><\/p>\n<p>All the best,<br>\nAlexey<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667333754242,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/w-b-outage-11-1-2022\/3360",
        "Challenge_link_count":3,
        "Challenge_participation_count":7,
        "Challenge_readability":8.0,
        "Challenge_reading_time":8.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"W&B Outage? 11\/1\/2022",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":110.0,
        "Challenge_word_count":84,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thank you for your patience! Our engineers were able to push a fix for this. There\u2019s still currently an issue regarding batch moving runs, but for the most part this issue has been resolved.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":2.41,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1574939238203,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":350.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Pretty self-explanatory question. When should I use Azure ML Notebooks VS Azure Databricks? I feel there\u2019s a great overlap between the two products and one is definitely better marketed than the other.. <\/p>\n\n<p>I\u2019m mainly looking for information concerning datasets sizes and typical workflow. Why should I use Databricks over AzureML if I don\u2019t have a Spark oriented workflow ?<\/p>\n\n<p>Thanks !<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585769120130,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1625712808263,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60978808",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.3,
        "Challenge_reading_time":6.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"When should I use Azure ML Notebooks VS Azure Databricks? Both are competitor products in my opinion",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":3755.0,
        "Challenge_word_count":78,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1447320137140,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":313.0,
        "Poster_view_count":40.0,
        "Solution_body":"<p>@Nethim, from my pov these are the main difference:<br><\/p>\n\n<ol>\n<li><p>Data Distribution:<\/p>\n\n<ul>\n<li>Azure ML Notebooks are good when you are training with a limited data on single machine. Though Azure ML provides training clusters, the data distribution among the nodes is to be handled in the code.<\/li>\n<li>Azure Databricks with its RDDs are designed to handle data distributed on multiple nodes.This is advantageous when your data size is huge.When your data size is small and can fit in a scaled up single machine\/ you are using a pandas dataframe, then use of Azure databricks is a overkill<\/li>\n<\/ul><\/li>\n<li><p>Data Cleaning:\nDatabricks can support a lot of file formats natively and querying and cleaning huge datasets are easy where as this has to be handled custom in AzureML notebooks. This can be done with a aml notebooks but cleaning and writing to stores has to be handled.<\/p><\/li>\n<li>Training\nBoth has the capabilities if distributing the training, Databricks provides inbuilt ML algorithms that can act on chunk of data on that node and coordinate with other nodes. Though this can be done on both AzureMachineLearning and Databricks with tf,horovod etc.,<\/li>\n<\/ol>\n\n<p>In general(just my opinion), if the dataset is small, aml notebooks is good.If the data size is huge, then Azure databricks is easy for datacleanup and format conversions.Then the training can happen on AML or databricks.Though databricks has a learning curve whereas Azure ML can be easy with the python and pandas.<\/p>\n\n<p>Thanks.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":18.96,
        "Solution_score_count":6.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":243.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1493314794172,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":844.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am training a Machine learning model in google colab, to be more specific I am training a GAN with PyTorch-lightning. The problem occurs is when I get disconnected from my current runtime due to inactivity. When I try to reconnect my Browser(tried on firefox and chrome) becomes first laggy and than freezes, my pc starts to lag so that I am not able to close my browser and it doesn't go away. I am forced to press the power button of my PC in order to restart the PC.\nI have no clue why this happens.\nI tried various batch sizes(also the size 1) but it still happens. It can't be that my dataset is too big either(since i tried it on a dataset with 10images for testing puposes).\nI hope someone can help me.<\/p>\n\n<p>Here is my code (For using the code you will need comet.nl and enter the comet.ml api key):<\/p>\n\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision  \nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\n\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport pytorch_lightning as pl\nfrom pytorch_lightning import loggers\n\nimport numpy as np\nfrom numpy.random import choice\n\nfrom PIL import Image\n\nimport os\nfrom pathlib import Path\nimport shutil\n\nfrom collections import OrderedDict\n\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# randomly flip some labels\ndef noisy_labels(y, p_flip=0.05):  # # flip labels with 5% probability\n    # determine the number of labels to flip\n    n_select = int(p_flip * y.shape[0])\n    # choose labels to flip\n    flip_ix = choice([i for i in range(y.shape[0])], size=n_select)\n    # invert the labels in place\n    y[flip_ix] = 1 - y[flip_ix]\n    return y\n\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0.0, std=0.1):\n        self.std = std\n        self.mean = mean\n\n    def __call__(self, tensor):\n        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\ndef get_valid_labels(img):\n  return (0.8 - 1.1) * torch.rand(img.shape[0], 1, 1, 1) + 1.1  # soft labels\n\ndef get_unvalid_labels(img):\n  return noisy_labels((0.0 - 0.3) * torch.rand(img.shape[0], 1, 1, 1) + 0.3)  # soft labels\n\nclass Generator(nn.Module):\n    def __init__(self, ngf, nc, latent_dim):\n        super(Generator, self).__init__()\n        self.ngf = ngf\n        self.latent_dim = latent_dim\n        self.nc = nc\n\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d(latent_dim, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n             nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass Discriminator(nn.Module):\n    def __init__(self, ndf, nc):\n        super(Discriminator, self).__init__()\n        self.nc = nc\n        self.ndf = ndf\n\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass DCGAN(pl.LightningModule):\n\n    def __init__(self, hparams, logger, checkpoint_folder, experiment_name):\n        super().__init__()\n        self.hparams = hparams\n        self.logger = logger  # only compatible with comet_logger at the moment\n        self.checkpoint_folder = checkpoint_folder\n        self.experiment_name = experiment_name\n\n        # networks\n        self.generator = Generator(ngf=hparams.ngf, nc=hparams.nc, latent_dim=hparams.latent_dim)\n        self.discriminator = Discriminator(ndf=hparams.ndf, nc=hparams.nc)\n        self.generator.apply(weights_init)\n        self.discriminator.apply(weights_init)\n\n        # cache for generated images\n        self.generated_imgs = None\n        self.last_imgs = None\n\n        # For experience replay\n        self.exp_replay_dis = torch.tensor([])\n\n        # creating checkpoint folder\n        dirpath = Path(self.checkpoint_folder)\n        if not dirpath.exists():\n          os.makedirs(dirpath, 0o755)\n\n    def forward(self, z):\n        return self.generator(z)\n\n    def adversarial_loss(self, y_hat, y):\n        return F.binary_cross_entropy(y_hat, y)\n\n    def training_step(self, batch, batch_nb, optimizer_idx):\n        # For adding Instance noise for more visit: https:\/\/www.inference.vc\/instance-noise-a-trick-for-stabilising-gan-training\/\n        std_gaussian = max(0, self.hparams.level_of_noise - ((self.hparams.level_of_noise * 1.5) * (self.current_epoch \/ self.hparams.epochs)))\n        AddGaussianNoiseInst = AddGaussianNoise(std=std_gaussian) # the noise decays over time\n\n        imgs, _ = batch\n        imgs = AddGaussianNoiseInst(imgs) # Adding instance noise to real images\n        self.last_imgs = imgs\n\n        # train generator\n        if optimizer_idx == 0:\n            # sample noise\n            z = torch.randn(imgs.shape[0], self.hparams.latent_dim, 1, 1)\n\n            # generate images\n            self.generated_imgs = self(z)\n            self.generated_imgs = AddGaussianNoiseInst(self.generated_imgs) # Adding instance noise to fake images\n\n            # Experience replay\n            # for discriminator\n            perm = torch.randperm(self.generated_imgs.size(0))  # Shuffeling\n            r_idx = perm[:max(1, self.hparams.experience_save_per_batch)]  # Getting the index\n            self.exp_replay_dis = torch.cat((self.exp_replay_dis, self.generated_imgs[r_idx]), 0).detach()  # Add our new example to the replay buffer\n\n            # ground truth result (ie: all fake)\n            g_loss = self.adversarial_loss(self.discriminator(self.generated_imgs), get_valid_labels(self.generated_imgs)) # adversarial loss is binary cross-entropy\n\n            tqdm_dict = {'g_loss': g_loss}\n            log = {'g_loss': g_loss, \"std_gaussian\": std_gaussian}\n            output = OrderedDict({\n                'loss': g_loss,\n                'progress_bar': tqdm_dict,\n                'log': log\n            })\n            return output\n\n        # train discriminator\n        if optimizer_idx == 1:\n            # Measure discriminator's ability to classify real from generated samples\n            # how well can it label as real?\n            real_loss = self.adversarial_loss(self.discriminator(imgs), get_valid_labels(imgs))\n\n            # Experience replay\n            if self.exp_replay_dis.size(0) &gt;= self.hparams.experience_batch_size:\n              fake_loss = self.adversarial_loss(self.discriminator(self.exp_replay_dis.detach()), get_unvalid_labels(self.exp_replay_dis))  # train on already seen images\n\n              self.exp_replay_dis = torch.tensor([]) # Reset experience replay\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"d_exp_loss\": fake_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n            else:\n              fake_loss = self.adversarial_loss(self.discriminator(self.generated_imgs.detach()), get_unvalid_labels(self.generated_imgs))  # how well can it label as fake?\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n    def configure_optimizers(self):\n        lr = self.hparams.lr\n        b1 = self.hparams.b1\n        b2 = self.hparams.b2\n\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n        return [opt_g, opt_d], []\n\n    def train_dataloader(self):\n        transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize([0.5], [0.5])])\n        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n        return DataLoader(dataset, batch_size=self.hparams.batch_size)\n        # transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n        #                                 transforms.ToTensor(),\n        #                                 transforms.Normalize([0.5], [0.5])\n        #                                 ])\n\n        # train_dataset = torchvision.datasets.ImageFolder(\n        #     root=\".\/drive\/My Drive\/datasets\/ghibli_dataset_small_overfit\/\",\n        #     transform=transform\n        # )\n        # return DataLoader(train_dataset, num_workers=self.hparams.num_workers, shuffle=True, batch_size=self.hparams.batch_size)\n\n    def on_epoch_end(self):\n        z = torch.randn(4, self.hparams.latent_dim, 1, 1)\n        # match gpu device (or keep as cpu)\n        if self.on_gpu:\n            z = z.cuda(self.last_imgs.device.index)\n\n        # log sampled images\n        sample_imgs = self.generator(z)\n        sample_imgs = sample_imgs.view(-1, self.hparams.nc, self.hparams.image_size, self.hparams.image_size)\n        grid = torchvision.utils.make_grid(sample_imgs, nrow=2)\n        self.logger.experiment.log_image(grid.permute(1, 2, 0), f'generated_images_epoch{self.current_epoch}', step=self.current_epoch)\n\n        # save model\n        if self.current_epoch % self.hparams.save_model_every_epoch == 0:\n          trainer.save_checkpoint(self.checkpoint_folder + \"\/\" + self.experiment_name + \"_epoch_\" + str(self.current_epoch) + \".ckpt\")\n          comet_logger.experiment.log_asset_folder(self.checkpoint_folder, step=self.current_epoch)\n\n          # Deleting the folder where we saved the model so that we dont upload a thing twice\n          dirpath = Path(self.checkpoint_folder)\n          if dirpath.exists() and dirpath.is_dir():\n                shutil.rmtree(dirpath)\n\n          # creating checkpoint folder\n          access_rights = 0o755\n          os.makedirs(dirpath, access_rights)\n\nfrom argparse import Namespace\n\nargs = {\n    'batch_size': 48,\n    'lr': 0.0002,\n    'b1': 0.5,\n    'b2': 0.999,\n    'latent_dim': 128, # tested value which worked(in V4_1): 100\n    'nc': 1,\n    'ndf': 32,\n    'ngf': 32,\n    'epochs': 10,\n    'save_model_every_epoch': 5,\n    'image_size': 64,\n    'num_workers': 2,\n    'level_of_noise': 0.15,\n    'experience_save_per_batch': 1, # this value should be very low; tested value which works: 1\n    'experience_batch_size': 50 # this value shouldnt be too high; tested value which works: 50\n}\nhparams = Namespace(**args)\n\n# Parameters\nexperiment_name = \"DCGAN_V4_2_MNIST\"\ndataset_name = \"MNIST\"\ncheckpoint_folder = \"DCGAN\/\"\ntags = [\"DCGAN\", \"MNIST\", \"OVERFIT\", \"64x64\"]\ndirpath = Path(checkpoint_folder)\n\n# init logger\ncomet_logger = loggers.CometLogger(\n    api_key=\"\",\n    rest_api_key=\"\",\n    project_name=\"gan\",\n    experiment_name=experiment_name,\n    #experiment_key=\"f23d00c0fe3448ee884bfbe3fc3923fd\"  # used for resuming trained id can be found in comet.ml\n)\n\n#defining net\nnet = DCGAN(hparams, comet_logger, checkpoint_folder, experiment_name)\n\n#logging\ncomet_logger.experiment.set_model_graph(str(net))\ncomet_logger.experiment.add_tags(tags=tags)\ncomet_logger.experiment.log_dataset_info(dataset_name)\n\ntrainer = pl.Trainer(#resume_from_checkpoint=\"GHIBLI_DCGAN_OVERFIT_64px_epoch_6000.ckpt\",\n                     logger=comet_logger,\n                     max_epochs=args[\"epochs\"]\n                     )\ntrainer.fit(net)\ncomet_logger.experiment.end()\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586987562523,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1587130797008,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61239274",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":149.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":145,
        "Challenge_solved_time":null,
        "Challenge_title":"Google Colab freezes my browser and pc when trying to reconnect to a notebook",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1013.0,
        "Challenge_word_count":1119,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1493314794172,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":844.0,
        "Poster_view_count":170.0,
        "Solution_body":"<p>I fixed it with importing this:<\/p>\n\n<pre><code>from IPython.display import clear_output \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nThe banner message is shown on the top page of Studio Lab.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to Studio Lab Top Page\r\n2. The message is shown.\r\n\r\n**Expected behavior**\r\nWe can use the Studio Lab as usual.\r\n\r\nI confirmed the following error.\r\n\r\n* We can start runtime but when clicking \"Open Project\", `ERR_EMPTY_RESPONSE` occurs in the browser.\r\n* When we click the start runtime, \"There was a problem when loading your project. This should be resolved shortly. Please try again later.\" occurred.\r\n\r\n**Screenshots**\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/544269\/202335625-de4d1505-97a7-4748-93d5-f0d6b0f5c597.png)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Windows\r\n - Browser Chrome\r\n\r\n**Additional context**\r\n\r\nAs the message suggests, we are working to restore the service. We apologize for any inconvenience.\r\nI'll announce after the service is back. \r\n",
        "Challenge_closed_time":1668731.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668650514000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/166",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":7.0,
        "Challenge_reading_time":13.81,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Starting 16th Nov 2022 04:00 PM PST, we are experiencing elevated error starting runtimes. The SageMaker Studio Lab team is working to restore the service. We apologize for any inconvenience.",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":153,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <br \/>\nI have been using object detection from Custom Vision to train images. Classification do not suit my goal so I'm looking at alternative methods to training images. Can I get some suggestions with using Azure for images segmentation?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1610951620483,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/234232\/images-segmentation-using-azure",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":3.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Images Segmentation using Azure",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":43,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=d94e82d7-8d83-4f86-8055-f9af74b9130d\">@Nam Ly  <\/a>    <\/p>\n<p>Suggestions and refer below url for Azure for images segmentation.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/samples\/browse\/?products=azure&amp;term=vision&amp;terms=%22Custom%20Vision%22\">Custom Vision integration sample skill for cognitive search<\/a>    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/classify-images-custom-vision\/\">Classify images with the Custom Vision service<\/a>    <\/p>\n<p>Please don\u2019t forget to <code>Accept the answer<\/code> and <code>up-vote<\/code> wherever the information provided helps you, this can be beneficial to other community members.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.2,
        "Solution_reading_time":9.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    }
]