[
    {
        "Answerer_created_time":1263294862568,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":0.3139213889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>following the answers to this question <a href=\"https:\/\/stackoverflow.com\/questions\/48264656\/load-s3-data-into-aws-sagemaker-notebook\">Load S3 Data into AWS SageMaker Notebook<\/a> I tried to load data from S3 bucket to SageMaker Jupyter Notebook.<\/p>\n<p>I used this code:<\/p>\n<pre><code>import pandas as pd\n\nbucket='my-bucket'\ndata_key = 'train.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n\npd.read_csv(data_location)\n<\/code><\/pre>\n<p>I replaced <code>'my-bucket'<\/code> by the ARN (Amazon Ressource name) of my S3 bucket (e.g. &quot;<code>arn:aws:s3:::name-of-bucket<\/code>&quot;) and replaced <code>'train.csv'<\/code> by the csv-filename which is stored in the S3 bucket. Regarding the rest I did not change anything at all. What I got was this <code>ValueError<\/code>:<\/p>\n<pre><code>ValueError: Failed to head path 'arn:aws:s3:::name-of-bucket\/name_of_file_V1.csv': Parameter validation failed:\nInvalid bucket name &quot;arn:aws:s3:::name-of-bucket&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.\\-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).*:s3:[a-z\\-0-9]+:[0-9]{12}:accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$&quot;\n<\/code><\/pre>\n<p>What did I do wrong? Do I have to modify the name of my S3 bucket?<\/p>",
        "Challenge_closed_time":1613558457267,
        "Challenge_comment_count":1,
        "Challenge_created_time":1613557327150,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66239966",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":19.3,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":0.3139213889,
        "Challenge_title":"Loading data from S3 bucket to SageMaker Jupyter Notebook - ValueError - Invalid bucket name",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":345,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559131080072,
        "Poster_location":null,
        "Poster_reputation_count":1166.0,
        "Poster_view_count":248.0,
        "Solution_body":"<p>The path should be:<\/p>\n<pre><code>data_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n<\/code><\/pre>\n<p>where <code>bucket<\/code> is <code>&lt;bucket-name&gt;<\/code> <strong>not ARN<\/strong>. For example <code>bucket=my-bucket-333222<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.1,
        "Solution_reading_time":3.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1572449042430,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":2082.0,
        "Answerer_view_count":238.0,
        "Challenge_adjusted_solved_time":2.8597922222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The entire error message after executing <code>terraform apply<\/code> within the terraform-folder of <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\" rel=\"nofollow noreferrer\">this source code in my GitHub-repo<\/a> (inspired by <a href=\"https:\/\/www.linkedin.com\/pulse\/terraform-sagemaker-part-2a-creating-custom-notebook-instance-david\" rel=\"nofollow noreferrer\">this tutorial<\/a> and <a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\" rel=\"nofollow noreferrer\">its related GitHub-repo<\/a>):<\/p>\n<pre><code>aws_sagemaker_notebook_instance.notebook_instance: Creating...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [10s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [20s elapsed]\n...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [15m21s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [15m31s elapsed]\n\u2577\n\u2502 Error: error waiting for sagemaker notebook instance (aws-sm-notebook-instance) to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(&lt;nil&gt;)\n\u2502\n\u2502   with aws_sagemaker_notebook_instance.notebook_instance,\n\u2502   on notebook_instance.tf line 2, in resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot;:\n\u2502    2: resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n\u2502\n<\/code><\/pre>\n<p>Internet research seemed to provide the solution in <a href=\"https:\/\/yuyasugano.medium.com\/machine-learning-infrastructure-terraforming-sagemaker-part-2-f2460a9a4663\" rel=\"nofollow noreferrer\">this article<\/a>, which inspired be to increase the allowed <code>IDLE_TIME<\/code> in the <code>on-start.sh<\/code> - script to <code>IDLE_TIME=1800<\/code> (in seconds, which equals 30 minutes). This should've been sufficient for the deployment time of around 15 minutes; yet, it threw the same error again.<\/p>\n<p>Next, I found <a href=\"https:\/\/stackoverflow.com\/questions\/65884743\/resolving-broken-deleted-state-in-terraform\">this post on StackOverFlow<\/a> suggesting to<\/p>\n<blockquote>\n<p>run <code>terraform refresh<\/code>, which will cause Terraform to refresh its state\nfile against what actually exists with the cloud provider.<\/p>\n<\/blockquote>\n<p>Unfortunately, running <code>terraform apply<\/code> right after refreshing didn't resolve the issue either.\nI'm wondering why the aforementioned <code>IDLE_TIME=1800<\/code> - setting does not have any effect. This should be more than sufficient for a 15-minute apply-time.<\/p>\n<hr \/>\n<p><strong>EDIT: adding code specifics for enhanced understanding<\/strong><\/p>\n<p><strong>1. Creating the SageMaker notebook instance<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n  name                    = &quot;aws-sm-notebook-instance&quot;\n  role_arn                = aws_iam_role.notebook_iam_role.arn\n  instance_type           = &quot;ml.t2.medium&quot;\n  lifecycle_config_name   = aws_sagemaker_notebook_instance_lifecycle_configuration.notebook_config.name\n  default_code_repository = aws_sagemaker_code_repository.git_repo.code_repository_name\n}\n<\/code><\/pre>\n<p><strong>2. Defining the SageMaker notebook lifecycle configuration<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance_lifecycle_configuration&quot; &quot;notebook_config&quot; {\n  name      = &quot;dev-platform-al-sm-lifecycle-config&quot;\n  on_create = filebase64(&quot;..\/scripts\/on-create.sh&quot;)\n  on_start  = filebase64(&quot;..\/scripts\/on-start.sh&quot;)\n}\n<\/code><\/pre>\n<p><strong>3. Defining the Git repo to instantiate on the SageMaker notebook instance<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_code_repository&quot; &quot;git_repo&quot; {\n  code_repository_name = &quot;aws-sm-notebook-instance-repo&quot;\n\n  git_config {\n    repository_url = &quot;https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance.git&quot;\n  }\n}\n<\/code><\/pre>\n<p><strong>Contents of <code>on-start.sh<\/code> (including IDLE_TIME - parameter)<\/strong>\nNote that this script will be invoked by the <code>scripts\/autostop.py<\/code> - script, which you can find <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\/blob\/main\/scripts\/autostop.py\" rel=\"nofollow noreferrer\">here<\/a> in the associated <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\" rel=\"nofollow noreferrer\">public repo containing the source code<\/a>.<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\n\n## IDLE AUTOSTOP STEPS\n## ----------------------------------------------------------------\n\n## Setting the timeout (in seconds) for how long the SageMaker notebook can run idly before being auto-stopped\n# -&gt; e.g. 1800 s = 30 min since first deployment can take between 15 and 20 minutes which could then fail like so:\n# &quot;Error: error waiting for sagemaker notebook instance (aws-sm-notebook-instance) to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(&lt;nil&gt;)&quot;\n# Hint for solution under following link: https:\/\/yuyasugano.medium.com\/machine-learning-infrastructure-terraforming-sagemaker-part-2-f2460a9a4663\nIDLE_TIME=1800\n\n# Getting the autostop.py script from GitHub\necho &quot;Fetching the autostop script...&quot;\nwget https:\/\/raw.githubusercontent.com\/andreasluckert\/aws-sm-notebook-instance\/main\/scripts\/autostop.py\n\n# Using crontab to autostop the notebook when idle time is breached\necho &quot;Starting the SageMaker autostop script in cron.&quot;\n(crontab -l 2&gt;\/dev\/null; echo &quot;*\/5 * * * * \/usr\/bin\/python $PWD\/autostop.py --time $IDLE_TIME --ignore-connections&quot;) | crontab -\n\n\n\n## CUSTOM CONDA KERNEL USAGE STEPS\n## ----------------------------------------------------------------\n\n# Setting the proper user credentials\nsudo -u ec2-user -i &lt;&lt;'EOF'\nunset SUDO_UID\n\n# Setting the source for the custom conda kernel\nWORKING_DIR=\/home\/ec2-user\/SageMaker\/custom-miniconda\nsource &quot;$WORKING_DIR\/miniconda\/bin\/activate&quot;\n\n# Loading all the custom kernels\nfor env in $WORKING_DIR\/miniconda\/envs\/*; do\n    BASENAME=$(basename &quot;$env&quot;)\n    source activate &quot;$BASENAME&quot;\n    python -m ipykernel install --user --name &quot;$BASENAME&quot; --display-name &quot;Custom ($BASENAME)&quot;\ndone\n<\/code><\/pre>",
        "Challenge_closed_time":1631187921612,
        "Challenge_comment_count":6,
        "Challenge_created_time":1631108848430,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1631177626360,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69104302",
        "Challenge_link_count":10,
        "Challenge_participation_count":7,
        "Challenge_readability":16.9,
        "Challenge_reading_time":85.0,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":21.9647727778,
        "Challenge_title":"Terraform Error: error waiting for sagemaker notebook instance to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(<nil>)",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":768,
        "Challenge_word_count":500,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572449042430,
        "Poster_location":"Germany",
        "Poster_reputation_count":2082.0,
        "Poster_view_count":238.0,
        "Solution_body":"<p>The solution to the problem was to check the CloudWatch Log events under <code>CloudWatch -&gt; Log groups -&gt; \/aws\/sagemaker\/NotebookInstances -&gt; aws-sm-notebook-instance\/LifecycleConfigOnCreate<\/code> to find the following error-message:<\/p>\n<pre><code>\/bin\/bash: \/tmp\/OnCreate_2021-09-08-12-24rw5al34g: \/bin\/bash^M: bad interpreter: No such file or directory\n<\/code><\/pre>\n<p>A bit of internet research brought me to <a href=\"https:\/\/askubuntu.com\/questions\/304999\/not-able-to-execute-a-sh-file-bin-bashm-bad-interpreter\/305001#305001\">this solution related to newline characters in shell-scripts<\/a>, which depend on whether you are on <code>Windows<\/code> or a <code>UNIX<\/code>-system.\nAs I'm working on Windows, the shell-scripts created in VS-Code comprised dos-specific <code>CRLF<\/code> newline-handling, which could be resolved via the button on the bottom-right in <code>VS-Code<\/code> to switch the <em>carriage return<\/em> (CRLF) character to the <em>line feed<\/em> (LF) character used by UNIX.<\/p>\n<p>As the compute instance employed by AWS Sagemaker is a Linux-system, it cannot handle the dos-style CRLF newline-characters in the shell-scripts and this &quot;adds&quot; a <code>^M<\/code> after <code>\/bin\/bash<\/code> which obviously leads to an error as such an interpreter does not exist.<\/p>\n<p>So, finally <code>terraform apply<\/code> worked out well:<\/p>\n<pre><code>$ terraform apply\n...\n...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [7m30s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [7m40s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Creation complete after 7m43s [id=aws-sm-notebook-instance]\n\nApply complete! Resources: 1 added, 1 changed, 1 destroyed.\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.3,
        "Solution_reading_time":23.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":184.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":1.7733930556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm playing around with Azure ML Studio. Now I would like to add a new column in my dataset to calculate and in a further step to cluster my data. What's the best way to do it? I tried to add a column with sql (alter table) but it didn't work.<\/p>\n\n<p>btw. the \"add columns\" function only adds columns from another dataset...<\/p>\n\n<p>Thanks in advance!<\/p>",
        "Challenge_closed_time":1525265262092,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525258877877,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50133056",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.8,
        "Challenge_reading_time":4.82,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.7733930556,
        "Challenge_title":"Add a column in Microsoft Azure ML Studio",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":517,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1463242510132,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>The \"Apply SQL Transformation\" module should be able to do it. For example, I have a dataset with an <em>age<\/em> column and here's the SQL to create another column called <em>double_age<\/em>:<\/p>\n\n<pre><code>select age, age * 2 as double_age from t1;\n<\/code><\/pre>\n\n<p>Which produces a dataset with just the <em>age<\/em> and <em>double_age<\/em> columns:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/uZblo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uZblo.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.3,
        "Solution_reading_time":6.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1343828614448,
        "Answerer_location":"Seville, Spain",
        "Answerer_reputation_count":359.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":85.6601075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an inference pipeline with some PythonScriptStep with a ParallelRunStep in the middle. Everything works fine except for the fact that all mini batches are run on one node during the ParallelRunStep, no matter how many nodes I put in the <code>node_count<\/code> config argument.<\/p>\n<p>All the nodes seem to be up and running in the cluster, and according to the logs the <code>init()<\/code> function has been run on them multiple times. Diving into the logs I can see in <strong>sys\/error\/10.0.0.*<\/strong> that all the workers except the one that is working are saying:<\/p>\n<p><code>FileNotFoundError: [Errno 2] No such file or directory: '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/virtualstage\/azureml\/c36eb050-adc9-4c34-8a33-5f6d42dcb19c\/wd\/tmp8_txakpm\/bg.png'<\/code><\/p>\n<p><strong>bg.png<\/strong> happens to be a side argument created in a previous PythonScriptStep that I'm passing to the ParallelRunStep:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>bg_file = PipelineData('bg',  datastore=data_store)\nbg_file_ds = bg_file.as_dataset()\nbg_file_named = bg_file_ds.as_named_input(&quot;bg&quot;)\nbg_file_dw = bg_file_named.as_download()\n\n...\n\nparallelrun_step = ParallelRunStep(\n    name='batch-inference',\n    parallel_run_config=parallel_run_config,\n    inputs=[frames_data_named.as_download()],\n    arguments=[&quot;--bg_folder&quot;, bg_file_dw],\n    side_inputs=[bg_file_dw],\n    output=inference_frames_ds,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>What's happening here? Why the side argument seems to be available only in one worker while it fails in the others?<\/p>\n<p>BTW I found <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/957\" rel=\"nofollow noreferrer\">this<\/a> similar but unresolved question.<\/p>\n<p>Any help is much appreciated, thanks!<\/p>",
        "Challenge_closed_time":1619694485790,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619386109403,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67258465",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":23.72,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":85.6601075,
        "Challenge_title":"AzureML ParallelRunStep runs only on one node",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":244,
        "Challenge_word_count":186,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343828614448,
        "Poster_location":"Seville, Spain",
        "Poster_reputation_count":359.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>Apparently you need to specify a local mount path to use side_inputs in more than one node:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>bg_file_named = bg_file_ds.as_named_input(f&quot;bg&quot;)\nbg_file_mnt = bg_file_named.as_mount(f&quot;\/tmp\/{str(uuid.uuid4())}&quot;)\n\n...\n\nparallelrun_step = ParallelRunStep(\n    name='batch-inference',\n    parallel_run_config=parallel_run_config,\n    inputs=[frames_data_named.as_download()],\n    arguments=[&quot;--bg_folder&quot;, bg_file_mnt],\n    side_inputs=[bg_file_mnt],\n    output=inference_frames_ds,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>Sources:<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-parallel-run-step\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-parallel-run-step<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/18355\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/18355<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":31.7,
        "Solution_reading_time":13.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1522870754323,
        "Answerer_location":"Redmond, WA, USA",
        "Answerer_reputation_count":366.0,
        "Answerer_view_count":173.0,
        "Challenge_adjusted_solved_time":405.5357452778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using azureml sdk in Azure Databricks.<\/p>\n<p>When I write the script for inference model (%%writefile script.py) in a databricks cell,\nI try to load a .bin file that I loaded in Azure Machine Learning Datasets.<\/p>\n<p>I would like to do this in the script.py:<\/p>\n<pre><code>fasttext.load_model(azuremldatasetpath)\n<\/code><\/pre>\n<p>How can I do to give good dataset path of my .bin file in azuremldatasetpath variable ? (Without calling workspace in the script).<\/p>\n<p>Something like:<\/p>\n<pre><code>dataset_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'file.bin')\n<\/code><\/pre>",
        "Challenge_closed_time":1645723579036,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644263650353,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71024584",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":8.48,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":405.5357452778,
        "Challenge_title":"How give azure machine learning dataset path in an inference script?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":172,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1638189721320,
        "Poster_location":null,
        "Poster_reputation_count":151.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>You can use your model name with the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#azureml-core-model-model-get-model-path\" rel=\"nofollow noreferrer\">Model.get_model_path()<\/a> method to retrieve the path of the model file or files on the local file system. If you register a folder or a collection of files, this API returns the path of the directory that contains those files.<\/p>\n<p>More info you may want to refer: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#azureml_model_dir\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#azureml_model_dir<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":17.9,
        "Solution_reading_time":10.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1444758849803,
        "Answerer_location":null,
        "Answerer_reputation_count":11962.0,
        "Answerer_view_count":960.0,
        "Challenge_adjusted_solved_time":3.0605805556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there a way to view the schema of a graph in a Neptune cluster using Jupyter Notebook? <\/p>\n\n<p>Like you would do a \"select * from tablename limit 10\" in an RDS using SQL, similarly is there a way to get a sense of the graph data through Jupyter Notebook?<\/p>",
        "Challenge_closed_time":1584902412783,
        "Challenge_comment_count":1,
        "Challenge_created_time":1584891394693,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":1637708419888,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60801292",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.0,
        "Challenge_reading_time":3.78,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.0605805556,
        "Challenge_title":"View Neptune Graph Schema using Jupyter notebook",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":831,
        "Challenge_word_count":56,
        "Platform":"Stack Overflow",
        "Poster_created_time":1584891066787,
        "Poster_location":null,
        "Poster_reputation_count":99.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>It depends on how large your graph is as to how well this will perform but you can get a sense of the type of nodes and edges you have using something like the example below. From the tags you used I assume you are using Gremlin:<\/p>\n\n<pre><code>g.V().groupCount().by(label)\ng.E().groupCount().by(label)\n<\/code><\/pre>\n\n<p>If you have a very large graph try putting something like <code>limit(100000)<\/code> before the <code>groupCount<\/code> step.<\/p>\n\n<p>If you are using a programming language like Python (with gremlin python installed) then you will need to add a <code>next()<\/code> terminal step to the queries as in:<\/p>\n\n<pre><code>g.V().groupCount().by(label).next()\ng.E().groupCount().by(label).next()\n<\/code><\/pre>\n\n<p>Having found the labels and distribution of the labels you could use one of them to explore some properties. Let's imagine there is a label called \"person\".<\/p>\n\n<pre><code>g.V().hasLabel('person').limit(10).valueMap().toList()\n<\/code><\/pre>\n\n<p>Remember with Gremlin property graphs vertices with the same label may not necessarily have all the same properties so it's good to look at more than one vertex to get a sense for that as well.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":14.81,
        "Solution_score_count":4.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":162.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":30.0886975,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Im trying to learn Azure, with little luck (yet). All the tutorials show using PipelineData just as a file, when configured in &quot;upload&quot; mode. However, im getting &quot;FileNotFoundError: [Errno 2] No such file or directory: ''&quot; error. I would love to ask a more specific question, but i just can't see what im doing wrong.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore,Dataset,Environment\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.pipeline.steps import PythonScriptStep\nfrom azureml.pipeline.core import Pipeline, PipelineData\nimport os\n\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n\ncompute_name = &quot;cpucluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=compute_name)\naml_run_config = RunConfiguration()\naml_run_config.target = compute_target\naml_run_config.environment.python.user_managed_dependencies = False\naml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['pandas','scikit-learn'], \n    pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]'], \n    pin_sdk_version=False)\n\noutput1 = PipelineData(&quot;processed_data1&quot;,datastore=datastore, output_mode=&quot;upload&quot;)\nprep_step = PythonScriptStep(\n    name=&quot;dataprep&quot;,\n    script_name=&quot;dataprep.py&quot;,\n    source_directory=os.path.join(os.getcwd(),'dataprep'),\n    arguments=[&quot;--output&quot;, output1],\n    outputs = [output1],\n    compute_target=compute_target,\n    runconfig=aml_run_config,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>In the dataprep.py i hve the following:<\/p>\n<pre><code>import numpy, argparse, pandas\nfrom azureml.core import Run\nrun = Run.get_context()\nparser = argparse.ArgumentParser()\nparser.add_argument('--output', dest='output', required=True)\nargs = parser.parse_args()\ndf = pandas.DataFrame(numpy.random.rand(100,3))\ndf.iloc[:, 2] = df.iloc[:,0] + df.iloc[:,1]\nprint(df.iloc[:5,:])\ndf.to_csv(args.output)\n\n<\/code><\/pre>\n<p>So, yeah. pd is supposed to write to the output, but my compute cluster says the following:<\/p>\n<pre><code>&quot;User program failed with FileNotFoundError: [Errno 2] No such file or directory: ''\\&quot;.\n<\/code><\/pre>\n<p>When i dont include the to_csv() function, the cluster does not complain<\/p>",
        "Challenge_closed_time":1626667519372,
        "Challenge_comment_count":2,
        "Challenge_created_time":1626541888290,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1626559748576,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68422680",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":15.6,
        "Challenge_reading_time":32.96,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":34.8975227778,
        "Challenge_title":"how to write to Azure PipelineData properly?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":404,
        "Challenge_word_count":207,
        "Platform":"Stack Overflow",
        "Poster_created_time":1588424911652,
        "Poster_location":null,
        "Poster_reputation_count":59.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Here is an <a href=\"https:\/\/github.com\/james-tn\/highperformance_python_in_azure\/blob\/master\/parallel_python_processing\/pipeline_definition.ipynb\" rel=\"nofollow noreferrer\">example<\/a> for PRS.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipelinedata?view=azure-ml-py\" rel=\"nofollow noreferrer\">PipelineData<\/a> was intended to represent &quot;transient&quot; data from one step to the next one, while OutputDatasetConfig was intended for capturing the final state of a dataset (and hence why you see features like lineage, ADLS support, etc). PipelineData always outputs data in a folder structure like {run_id}{output_name}. OutputDatasetConfig allows to decouple the data from the run and hence it allows you to control where to land the data (although by default it will produce similar folder structure). The OutputDatasetConfig allows even to register the output as a Dataset, where getting rid of such folder structure makes sense. From the docs itself: &quot;Represent how to copy the output of a run and be promoted as a FileDataset. The OutputFileDatasetConfig allows you to specify how you want a particular local path on the compute target to be uploaded to the specified destination&quot;.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-batch-scoring-classification#create-dataset-objects\" rel=\"nofollow noreferrer\">OutFileDatasetConfig<\/a> is a control plane concept to pass data between pipeline steps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1626668067887,
        "Solution_link_count":3.0,
        "Solution_readability":14.5,
        "Solution_reading_time":19.88,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":167.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":498.5483013889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Azure ML provides client libraries (e.g. azureml for Python) for dataset management and model deploying. From what I understand, the custom algorithm would be serialized as a Pickle file, but I'm not sure what happens after that. If I have a custom model with a deep NN architecture and set up a web service for training and another for scoring, do I still need the machine that the model was developed on for the web services to run? I found this on the azureml documentation that was helpful:<\/p>\n<blockquote>\n<p>If a function has no source file associated with it (for example, you're developing inside of a REPL environment) then the functions byte code is serialized. If the function refers to any global variables those will also be serialized using Pickle. In this mode all of the state which you're referring to needs to be already defined (e.g. your published function should come after any other functions you are calling).<\/p>\n<p>If a function is saved on disk then the entire module the function is defined in will be serialized and re-executed on the server to get the function back. In this mode the entire contents of the file is serialized and the order of the function definitions don't matter.<\/p>\n<\/blockquote>\n<p>What if the function uses a library like TensorFlow or Keras? Can someone explain what happens after the Pickle model is created?<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1532614432912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530819659027,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51198775",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":17.6,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":498.5483013889,
        "Challenge_title":"How does Azure web service deployment work locally?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":126,
        "Challenge_word_count":239,
        "Platform":"Stack Overflow",
        "Poster_created_time":1338127253383,
        "Poster_location":"Minneapolis, MN, United States",
        "Poster_reputation_count":115.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>You need to take the model.pkl file, zip it, and upload it into Azure Machine Learning Studio as a new dataset. Then add the python module and connect it to your newly generated zip.<\/p>\n\n<p>You can now use it inside the AML Studio experiment. To use the model add the following code in your python module:<\/p>\n\n<pre><code>import pandas as pd\nimport sys\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    sys.path.insert(0,\".\\Script Bundle\")\n    model = pickle.load(open(\".\\Script Bundle\\model.pkl\", 'rb'))\n    pred = model.predict(dataframe1)\n    return pd.DataFrame([pred[0]]),\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/blogs.technet.microsoft.com\/uktechnet\/2018\/04\/25\/deploying-externally-generated-pythonr-models-as-web-services-using-azure-machine-learning-studio\/\" rel=\"nofollow noreferrer\">You may find this post useful<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.5,
        "Solution_reading_time":10.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":89.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1577353307072,
        "Answerer_location":null,
        "Answerer_reputation_count":491.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":1.6874425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I\u2019m having some issues trying to access a FileDataset created from two http URIs in an Azure ML Pipeline PythonScriptStep.<\/p>\n<p>In the step, I\u2019m only getting a single file named <code>['https%3A\u2019]<\/code> when doing an <code>os.listdir()<\/code> on my mount point. I would have expected two files, with their actual names instead. This happens both when sending the dataset <code>as_upload<\/code> and <code>as_mount<\/code>. Even happens when I send the dataset reference to the pipeline step and mount it directly from the step.<\/p>\n<p>The dataset is registered in a notebook, the same notebook that creates and invokes the pipeline, as seen below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>tempFileData = Dataset.File.from_files(\n        ['https:\/\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg',\n        'https:\/\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg'])\ntempFileData.register(ws, name='FileData', create_new_version=True)\n\n#...\n\nread_datasets_step = PythonScriptStep(\n    name='The Dataset Reader',\n    script_name='read-datasets.py',\n    inputs=[fileData.as_named_input('Files'), fileData.as_named_input('Files_mount').as_mount(), fileData.as_named_input('Files_download').as_download()],\n    compute_target=compute_target,\n    source_directory='.\/dataset-reader',\n    allow_reuse=False,\n)\n\n<\/code><\/pre>\n<p>The <code>FileDataset<\/code> seems to be registered properly, if I examine it within the notebook I get the following result:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;source&quot;: [\n    &quot;https:\/\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg&quot;,\n    &quot;https:\/\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg&quot;\n  ],\n  &quot;definition&quot;: [\n    &quot;GetFiles&quot;\n  ],\n  &quot;registration&quot;: {\n    &quot;id&quot;: &quot;...&quot;,\n    &quot;name&quot;: &quot;FileData&quot;,\n    &quot;version&quot;: 4,\n    &quot;workspace&quot;: &quot;Workspace.create(...)&quot;\n  }\n}\n<\/code><\/pre>\n<p>For reference, the machine running the notebook is using AML SDK v1.24, whereas the node running the pipeline steps is running v1.25.<\/p>\n<p>Has anybody encountered anything like this? Is there a way to make it work?<\/p>\n<p>Note that I'm specifically looking at file datasets created from web uris, and not necessarily interested in getting a <code>FileDataset<\/code> to work with blob storage or similar.<\/p>",
        "Challenge_closed_time":1618855169223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618832324140,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1618849094430,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67161293",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":32.29,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":6.3458563889,
        "Challenge_title":"Issues accessing a FileDataset created from HTTP URIs in a PythonScriptStep",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":91,
        "Challenge_word_count":230,
        "Platform":"Stack Overflow",
        "Poster_created_time":1250158552416,
        "Poster_location":"Romania",
        "Poster_reputation_count":7916.0,
        "Poster_view_count":801.0,
        "Solution_body":"<p>The files should've been mounted at path &quot;https%3A\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg&quot; and &quot;https%3A\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg&quot;.<\/p>\n<p>We retain the directory structure following the url structure to avoid potential conflicts.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.3,
        "Solution_reading_time":4.39,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":2.79163,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am following a guide to get a Vertex AI pipeline working:<\/p>\n<p><a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5\" rel=\"nofollow noreferrer\">https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5<\/a><\/p>\n<p>I have implemented the following custom component:<\/p>\n<pre><code>from google.cloud import aiplatform as aip\nfrom google.oauth2 import service_account\n\nproject = &quot;project-id&quot;\nregion = &quot;us-central1&quot;\ndisplay_name = &quot;lookalike_model_pipeline_1646929843&quot;\n\nmodel_name = f&quot;projects\/{project}\/locations\/{region}\/models\/{display_name}&quot;\napi_endpoint = &quot;us-central1-aiplatform.googleapis.com&quot; #europe-west2\nmodel_resource_path = model_name\nclient_options = {&quot;api_endpoint&quot;: api_endpoint}\n\n# Initialize client that will be used to create and send requests.\nclient = aip.gapic.ModelServiceClient(credentials=service_account.Credentials.from_service_account_file('..\\\\service_accounts\\\\aiplatform_sa.json'), \nclient_options=client_options)\n#get model evaluation\nresponse = client.list_model_evaluations(parent=model_name)\n<\/code><\/pre>\n<p>And I get following error:<\/p>\n<pre><code>(&lt;class 'google.api_core.exceptions.PermissionDenied'&gt;, PermissionDenied(&quot;Permission 'aiplatform.modelEvaluations.list' denied on resource '\/\/aiplatform.googleapis.com\/projects\/project-id\/locations\/us-central1\/models\/lookalike_model_pipeline_1646929843' (or it may not exist).&quot;), &lt;traceback object at 0x000002414D06B9C0&gt;)\n<\/code><\/pre>\n<p>The model definitely exists and has finished training. I have given myself admin rights in the aiplatform service account. In the guide, they do not use a service account, but uses only client_options instead. The client_option has the wrong type since it is a dict(str, str) when it should be: Optional['ClientOptions']. But this doesn't cause an error.<\/p>\n<p>My main question is: how do I get around this permission issue?<\/p>\n<p>My subquestions are:<\/p>\n<ol>\n<li>How can I use my model_name variable in a URL to get to the model?<\/li>\n<li>How can I create an Optional['ClientOptions'] object to pass as client_option<\/li>\n<li>Is there another way I can list_model_evaluations from a model that is in VertexAI, trained using automl?<\/li>\n<\/ol>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1646971559036,
        "Challenge_comment_count":7,
        "Challenge_created_time":1646943487337,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1646987974512,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71430286",
        "Challenge_link_count":2,
        "Challenge_participation_count":9,
        "Challenge_readability":16.7,
        "Challenge_reading_time":31.25,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":7.7976941667,
        "Challenge_title":"Permission Denied using Google AiPlatform ModelServiceClient",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":936,
        "Challenge_word_count":215,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562706291280,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>I tried using your code and it did not also work for me and got a different error. As @DazWilkin mentioned it is recommended to use the Cloud Client.<\/p>\n<p>I used <code>aiplatform_v1<\/code> and it worked fine. One thing I noticed is that you should always define a value for <code>client_options<\/code> so it will point to the correct endpoint. Checking the code for <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/google\/cloud\/aiplatform_v1\/services\/model_service\/client.py#L122\" rel=\"nofollow noreferrer\">ModelServiceClient<\/a>, if I'm not mistaken the endpoint defaults to <strong>&quot;aiplatform.googleapis.com&quot;<\/strong> which don't have a location prepended. AFAIK the endpoint should prepend a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/locations\" rel=\"nofollow noreferrer\">location<\/a>.<\/p>\n<p>See code below. I used AutoML models and it returns their model evaluations.<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\nfrom typing import Optional\n\ndef get_model_eval(\n        project_id: str,\n        model_id: str,\n        client_options: dict,\n        location: str = 'us-central1',\n        ):\n\n    client_model = aiplatform.services.model_service.ModelServiceClient(client_options=client_options)\n\n    model_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\n    list_eval_request = aiplatform.types.ListModelEvaluationsRequest(parent=model_name)\n    list_eval = client_model.list_model_evaluations(request=list_eval_request)\n    print(list_eval)\n\n\n\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint} # api_endpoint is required for client_options\nproject_id = 'project-id'\nlocation = 'us-central1'\nmodel_id = '99999999999' # aiplatform_v1 uses the model_id\n\nget_model_eval(\n        client_options = client_options,\n        project_id = project_id,\n        location = location,\n        model_id = model_id,\n        )\n<\/code><\/pre>\n<p>This is an output snippet from my AutoML Text Classification:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/RXrxh.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RXrxh.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1646998024380,
        "Solution_link_count":4.0,
        "Solution_readability":16.5,
        "Solution_reading_time":28.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":184.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1604093818187,
        "Answerer_location":"Krakow, Poland",
        "Answerer_reputation_count":1200.0,
        "Answerer_view_count":263.0,
        "Challenge_adjusted_solved_time":4.8079869444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm struggling to correctly set Vertex AI pipeline which does the following:<\/p>\n<ol>\n<li>read data from API and store to GCS and as as input for batch prediction.<\/li>\n<li>get an existing model (Video classification on Vertex AI)<\/li>\n<li>create Batch prediction job with input from point 1.<br \/>\nAs it will be seen, I don't have much experience with Vertex Pipelines\/Kubeflow thus I'm asking for help\/advice, hope it's just some beginner mistake.\nthis is the gist of the code I'm using as pipeline<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import dsl\n\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Output,\n    Artifact,\n    Model,\n)\n\nPROJECT_ID = 'my-gcp-project'\nBUCKET_NAME = &quot;mybucket&quot;\nPIPELINE_ROOT = &quot;{}\/pipeline_root&quot;.format(BUCKET_NAME)\n\n\n@component\ndef get_input_data() -&gt; str:\n    # getting data from API, save to Cloud Storage\n    # return GS URI\n    gcs_batch_input_path = 'gs:\/\/somebucket\/file'\n    return gcs_batch_input_path\n\n\n@component(\n    base_image=&quot;python:3.9&quot;,\n    packages_to_install=['google-cloud-aiplatform==1.8.0']\n)\ndef load_ml_model(project_id: str, model: Output[Artifact]):\n    &quot;&quot;&quot;Load existing Vertex model&quot;&quot;&quot;\n    import google.cloud.aiplatform as aip\n\n    model_id = '1234'\n    model = aip.Model(model_name=model_id, project=project_id, location='us-central1')\n\n\n\n@dsl.pipeline(\n    name=&quot;batch-pipeline&quot;, pipeline_root=PIPELINE_ROOT,\n)\ndef pipeline(gcp_project: str):\n    input_data = get_input_data()\n    ml_model = load_ml_model(gcp_project)\n\n    gcc_aip.ModelBatchPredictOp(\n        project=PROJECT_ID,\n        job_display_name=f'test-prediction',\n        model=ml_model.output,\n        gcs_source_uris=[input_data.output],  # this doesn't work\n        # gcs_source_uris=['gs:\/\/mybucket\/output\/'],  # hardcoded gs uri works\n        gcs_destination_output_uri_prefix=f'gs:\/\/{PIPELINE_ROOT}\/prediction_output\/'\n    )\n\n\nif __name__ == '__main__':\n    from kfp.v2 import compiler\n    import google.cloud.aiplatform as aip\n    pipeline_export_filepath = 'test-pipeline.json'\n    compiler.Compiler().compile(pipeline_func=pipeline,\n                                package_path=pipeline_export_filepath)\n    # pipeline_params = {\n    #     'gcp_project': PROJECT_ID,\n    # }\n    # job = aip.PipelineJob(\n    #     display_name='test-pipeline',\n    #     template_path=pipeline_export_filepath,\n    #     pipeline_root=f'gs:\/\/{PIPELINE_ROOT}',\n    #     project=PROJECT_ID,\n    #     parameter_values=pipeline_params,\n    # )\n\n    # job.run()\n<\/code><\/pre>\n<p>When running the pipeline it throws this exception when running Batch prediction:<br \/>\n<code>details = &quot;List of found errors: 1.Field: batch_prediction_job.model; Message: Invalid Model resource name. <\/code>\nso I'm not sure what could be wrong. I tried to load model in the notebook (outside of component) and it correctly returns.<\/p>\n<p>Second issue I'm having is referencing GCS URI as output from component to batch job input.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>   input_data = get_input_data2()\n   gcc_aip.ModelBatchPredictOp(\n        project=PROJECT_ID,\n        job_display_name=f'test-prediction',\n        model=ml_model.output,\n        gcs_source_uris=[input_data.output],  # this doesn't work\n        # gcs_source_uris=['gs:\/\/mybucket\/output\/'],  # hardcoded gs uri works\n        gcs_destination_output_uri_prefix=f'gs:\/\/{PIPELINE_ROOT}\/prediction_output\/'\n    )\n<\/code><\/pre>\n<p>During compilation, I get following exception <code>TypeError: Object of type PipelineParam is not JSON serializable<\/code>, though I think this could be issue of ModelBatchPredictOp component.<\/p>\n<p>Again any help\/advice appreciated, I'm dealing with this from yesterday, so maybe I missed something obvious.<\/p>\n<p>libraries I'm using:<\/p>\n<pre><code>google-cloud-aiplatform==1.8.0  \ngoogle-cloud-pipeline-components==0.2.0  \nkfp==1.8.10  \nkfp-pipeline-spec==0.1.13  \nkfp-server-api==1.7.1\n<\/code><\/pre>\n<p><strong>UPDATE<\/strong>\nAfter comments, some research and tuning, for referencing model this works:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component\ndef load_ml_model(project_id: str, model: Output[Artifact]):\n    region = 'us-central1'\n    model_id = '1234'\n    model_uid = f'projects\/{project_id}\/locations\/{region}\/models\/{model_id}'\n    model.uri = model_uid\n    model.metadata['resourceName'] = model_uid\n<\/code><\/pre>\n<p>and then I can use it as intended:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>batch_predict_op = gcc_aip.ModelBatchPredictOp(\n        project=gcp_project,\n        job_display_name=f'batch-prediction-test',\n        model=ml_model.outputs['model'],\n        gcs_source_uris=[input_batch_gcs_path],\ngcs_destination_output_uri_prefix=f'gs:\/\/{BUCKET_NAME}\/prediction_output\/test'\n    )\n<\/code><\/pre>\n<p><strong>UPDATE 2<\/strong>\nregarding GCS path, a workaround is to define path outside of the component and pass it as an input parameter, for example (abbreviated):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@dsl.pipeline(\n    name=&quot;my-pipeline&quot;,\n    pipeline_root=PIPELINE_ROOT,\n)\ndef pipeline(\n        gcp_project: str,\n        region: str,\n        bucket: str\n):\n    ts = datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)\n    \n    gcs_prediction_input_path = f'gs:\/\/{BUCKET_NAME}\/prediction_input\/video_batch_prediction_input_{ts}.jsonl'\n    batch_input_data_op = get_input_data(gcs_prediction_input_path)  # this loads input data to GCS path\n\n    batch_predict_op = gcc_aip.ModelBatchPredictOp(\n        project=gcp_project,\n        model=training_job_run_op.outputs[&quot;model&quot;],\n        job_display_name='batch-prediction',\n        # gcs_source_uris=[batch_input_data_op.output],\n        gcs_source_uris=[gcs_prediction_input_path],\n        gcs_destination_output_uri_prefix=f'gs:\/\/{BUCKET_NAME}\/prediction_output\/',\n    ).after(batch_input_data_op)  # we need to add 'after' so it runs after input data is prepared since get_input_data doesn't returns anything\n\n<\/code><\/pre>\n<p>still not sure, why it doesn't work\/compile when I return GCS path from <code>get_input_data<\/code> component<\/p>",
        "Challenge_closed_time":1640097300176,
        "Challenge_comment_count":7,
        "Challenge_created_time":1639525350357,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1640079991423,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70356856",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":17.4,
        "Challenge_reading_time":78.55,
        "Challenge_score_count":5,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":158.8749497222,
        "Challenge_title":"Vertex AI Model Batch prediction, issue with referencing existing model and input file on Cloud Storage",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1202,
        "Challenge_word_count":495,
        "Platform":"Stack Overflow",
        "Poster_created_time":1372196724860,
        "Poster_location":"Prague, Czech Republic",
        "Poster_reputation_count":276.0,
        "Poster_view_count":47.0,
        "Solution_body":"<p>I'm glad you solved most of your main issues and found a workaround for model declaration.<\/p>\n<p>For your <code>input.output<\/code> observation on <code>gcs_source_uris<\/code>, the reason behind it is because the way the function\/class returns the value. If you dig inside the class\/methods of <code>google_cloud_pipeline_components<\/code>  you will find that it implements a structure that will allow you to use <code>.outputs<\/code> from the returned value of the function called.<\/p>\n<p>If you go to the implementation of one of the components of the pipeline you will find that it returns an output array from <code>convert_method_to_component<\/code> function. So, in order to have that implemented in your custom class\/function your function should return a value which can be called as an attribute. Below is a basic implementation of it.<\/p>\n<pre><code>class CustomClass():\n     def __init__(self):\n       self.return_val = {'path':'custompath','desc':'a desc'}\n      \n     @property\n     def output(self):\n       return self.return_val \n\nhello = CustomClass()\nprint(hello.output['path'])\n<\/code><\/pre>\n<p>If you want to dig more about it you can go to the following pages:<\/p>\n<ul>\n<li><p><a href=\"https:\/\/github.com\/bharathdsce\/kubeflow\/blob\/fcd627714664956b2c280b0109b64633bc99fa05\/components\/google-cloud\/google_cloud_pipeline_components\/aiplatform\/utils.py#L383\" rel=\"nofollow noreferrer\">convert_method_to_component<\/a>, which is the implementation of <code>convert_method_to_component<\/code><\/p>\n<\/li>\n<li><p><a href=\"https:\/\/www.programiz.com\/python-programming\/property\" rel=\"nofollow noreferrer\">Properties<\/a>, basics of property in python.<\/p>\n<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.6,
        "Solution_reading_time":21.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":177.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1430233500800,
        "Answerer_location":null,
        "Answerer_reputation_count":212.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":21511.1452602778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I suspect this has to more to do with IAM roles than Sagemaker.<\/p>\n\n<p>I'm following the example <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"noreferrer\">here<\/a><\/p>\n\n<p>Specifically, when it makes this call<\/p>\n\n<pre><code>tf_estimator.fit('s3:\/\/bucket\/path\/to\/training\/data')\n<\/code><\/pre>\n\n<p>I get this error<\/p>\n\n<pre><code>ClientError: An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:sts::013772784144:assumed-role\/AmazonSageMaker-ExecutionRole-20181022T195630\/SageMaker is not authorized to perform: iam:GetRole on resource: role SageMakerRole\n<\/code><\/pre>\n\n<p>My notebook instance has an IAM role attached to it.\nThat role has the <code>AmazonSageMakerFullAccess<\/code> policy. It also has a custom policy that looks like this<\/p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:GetObject\",\n            \"s3:PutObject\",\n            \"s3:DeleteObject\",\n            \"s3:ListBucket\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::*\"\n        ]\n    }\n]\n<\/code><\/pre>\n\n<p>}<\/p>\n\n<p>My input files and .py script is in an s3 bucket with the phrase <code>sagemaker<\/code> in it.<\/p>\n\n<p>What else am I missing?<\/p>",
        "Challenge_closed_time":1543010904776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1542853625410,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53423061",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.6,
        "Challenge_reading_time":16.26,
        "Challenge_score_count":6,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":43.6887127778,
        "Challenge_title":"How do I make this IAM role error in aws sagemaker go away?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":8160,
        "Challenge_word_count":130,
        "Platform":"Stack Overflow",
        "Poster_created_time":1319234288808,
        "Poster_location":null,
        "Poster_reputation_count":4966.0,
        "Poster_view_count":304.0,
        "Solution_body":"<p>If you're running the example code on a SageMaker notebook instance, you can use the execution_role which has the <code>AmazonSageMakerFullAccess<\/code> attached.<\/p>\n<pre><code>from sagemaker import get_execution_role\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n<\/code><\/pre>\n<p>And you can pass this role when initializing <code>tf_estimator<\/code>.\nYou can check out the example <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html\" rel=\"nofollow noreferrer\">here<\/a> for using <code>execution_role<\/code> with S3 on notebook instance.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1620293748347,
        "Solution_link_count":1.0,
        "Solution_readability":18.4,
        "Solution_reading_time":8.09,
        "Solution_score_count":8.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":57.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1456411465888,
        "Answerer_location":null,
        "Answerer_reputation_count":105.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":22032.4912397222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have built my own Docker container that provides inference code to be deployed as endpoint on Amazon Sagemaker. However, this container needs to have access to some files from s3. The used IAM role has access to all s3 buckets that I am trying to reach.<\/p>\n\n<p>Code to download files using a boto3 client:<\/p>\n\n<pre><code>import boto3\n\nmodel_bucket = 'my-bucket'\n\ndef download_file_from_s3(s3_path, local_path):\n    client = boto3.client('s3')\n    client.download_file(model_bucket, s3_path, local_path)\n<\/code><\/pre>\n\n<p>The IAM role's policies:<\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::my-bucket\/*\"\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>Starting the docker container locally allows me to download files from s3 just like expected. <\/p>\n\n<p>Deploying as an endpoint on Sagemaker, however, the request times out:<\/p>\n\n<pre><code>botocore.vendored.requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='my-bucket.s3.eu-central-1.amazonaws.com', port=443): Max retries exceeded with url: \/path\/to\/my-file (Caused by ConnectTimeoutError(&lt;botocore.awsrequest.AWSHTTPSConnection object at 0x7f66244e69b0&gt;, 'Connection to my-bucket.s3.eu-central-1.amazonaws.com timed out. (connect timeout=60)'))\n<\/code><\/pre>\n\n<p>Any help is appreciated!<\/p>",
        "Challenge_closed_time":1562142120396,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561982365937,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56835306",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.3,
        "Challenge_reading_time":18.79,
        "Challenge_score_count":4,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":44.3762386111,
        "Challenge_title":"Download file using boto3 within Docker container deployed on Sagemaker Endpoint",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1171,
        "Challenge_word_count":149,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456411465888,
        "Poster_location":null,
        "Poster_reputation_count":105.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>For anyone coming across this question, when creating a model, the 'Enable Network Isolation' property defaults to True.\nFrom AWS docs:<\/p>\n<blockquote>\n<p>If you enable network isolation, the containers are not able to make any outbound network calls, even to other AWS services such as Amazon S3. Additionally, no AWS credentials are made available to the container runtime environment.<\/p>\n<\/blockquote>\n<p>So this property needs to be set to False in order to connect to any other AWS service.<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/5qERm.jpg\" alt=\"AWS Sagemaker UI Network Isolation set to False\" \/><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1641299334400,
        "Solution_link_count":1.0,
        "Solution_readability":9.1,
        "Solution_reading_time":7.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442430064503,
        "Answerer_location":null,
        "Answerer_reputation_count":6147.0,
        "Answerer_view_count":1230.0,
        "Challenge_adjusted_solved_time":970.3788427778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I create a model in Azure ML studio. \nI deployed the web service.<\/p>\n\n<p>Now, I know how to check one record at a time, but how can I load a csv file and made the algorithm go through all records ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/1tHuM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1tHuM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If I click on Batch Execution - it will ask me to create an account for Azure storage. <\/p>\n\n<p>Is any way to execute multiple records from csv file without creating any other accounts?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/90zP7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/90zP7.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1518941429767,
        "Challenge_comment_count":0,
        "Challenge_created_time":1515448065933,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48158545",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":10.25,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":970.3788427778,
        "Challenge_title":"How to execute multiple rows in web service Azure Machine Learning Studio",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":204,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457596845392,
        "Poster_location":"San Diego, CA, United States",
        "Poster_reputation_count":4046.0,
        "Poster_view_count":825.0,
        "Solution_body":"<p>Yes, there is a way and it is simple. What you need is an excel add-in. You need not create any other account.<\/p>\n\n<p>You can either read <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/excel-add-in-for-web-services\" rel=\"nofollow noreferrer\">Excel Add-in for Azure Machine Learning web services doc<\/a> or you can watch <a href=\"https:\/\/www.youtube.com\/watch?v=ju1CzDjiOMQ\" rel=\"nofollow noreferrer\">Azure ML Excel Add-in video<\/a>. <\/p>\n\n<p>If you search for <a href=\"https:\/\/www.google.co.in\/search?q=excel%20add%20in%20for%20azure%20ml&amp;client=firefox-b-ab&amp;dcr=0&amp;source=lnms&amp;tbm=vid&amp;sa=X&amp;ved=0ahUKEwinqP3a_67ZAhXBr48KHdiYAXUQ_AUICigB&amp;biw=1280&amp;bih=616\" rel=\"nofollow noreferrer\">videos on excel add in for azure ml<\/a>, you get other useful videos too. <\/p>\n\n<p>I hope this is the solution you are looking for.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":11.6,
        "Solution_reading_time":11.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":84.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1280505139752,
        "Answerer_location":"Bangalore, India",
        "Answerer_reputation_count":4265.0,
        "Answerer_view_count":403.0,
        "Challenge_adjusted_solved_time":24.7636894444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was trying Azure Machine Learning Services following this tutorial (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/quickstart-installation\" rel=\"nofollow noreferrer\">Link<\/a>). After successfully creating the Azure Machine Learning services accounts, I successfully installed the Workbench on my Windows 10 Laptop (Behind Proxy; Proxy has been configured at the WorkBench). Next, I was trying to create project following this section (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/quickstart-installation#create-a-project-in-workbench\" rel=\"nofollow noreferrer\">Link<\/a>). Once I click on the Create button, it goes to \"Creating\" state and stays there for ever. The errors displayed at Errors.log is the following. Any suggestion will be appreciated. <\/p>\n\n<pre><code>[2018-07-09 09:47:08.437] [ERROR] HttpService - {\"event\":\"HttpService\",\"task\":\"Failed\",\"data\":{\"url\":\"http:\/\/localhost:54240\/projects\/v1.0\/create\/template\",\"status\":500,\"statusText\":\"INKApi Error\",\"jsonError\":null,\"requestId\":null,\"sessionType\":\"Workbench\"},\"sid\":\"365395c0-832b-11e8-b4ce-e5d7046c6143\"}\n\n[2018-07-09 09:47:08.960] [ERROR] CreateProjectForm - {\"event\":\"CreateProject\",\"task\":\"Error\",\"data\":{\"_body\":null,\"status\":500,\"ok\":false,\"statusText\":\"INKApi Error\",\"headers\":{\"Date\":[\"Mon\",\" 09 Jul 2018 04:17:06 GMT\"],\"Via\":[\"1.1 localhost.localdomain\"],\"Proxy-Connection\":[\"close\"],\"Content-Length\":[\"0\"],\"Content-Type\":[\"text\/html\"]},\"type\":2,\"url\":\"http:\/\/localhost:54240\/projects\/v1.0\/create\/template\"},\"sid\":\"365395c0-832b-11e8-b4ce-e5d7046c6143\"}\n\n[2018-07-09 09:47:08.963] [FATAL] ExceptionLogger - {\"event\":\"exception\",\"task\":\"\",\"data\":{\"message\":\"Cannot read property 'error' of null\",\"name\":\"TypeError\",\"stack\":\"TypeError: Cannot read property 'error' of null\\n    at SafeSubscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:61476:58)\\n    at SafeSubscriber.__tryOrUnsub (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212279:20)\\n    at SafeSubscriber.error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212241:30)\\n    at Subscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212172:30)\\n    at Subscriber.error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212146:22)\\n    at MergeMapSubscriber.OuterSubscriber.notifyError (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:210968:30)\\n    at InnerSubscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:211072:25)\\n    at InnerSubscriber.Subscriber.error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212146:22)\\n    at DeferSubscriber.OuterSubscriber.notifyError (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:210968:30)\\n    at InnerSubscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:211072:25)\"},\"sid\":\"365395c0-832b-11e8-b4ce-e5d7046c6143\"}\n<\/code><\/pre>",
        "Challenge_closed_time":1531201276972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531112127690,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51238413",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":29.4,
        "Challenge_reading_time":46.14,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":24.7636894444,
        "Challenge_title":"Azure Machine Learning Workbench hangs while creating new project",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":67,
        "Challenge_word_count":160,
        "Platform":"Stack Overflow",
        "Poster_created_time":1280505139752,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":4265.0,
        "Poster_view_count":403.0,
        "Solution_body":"<p>It was happening because of the Proxy (although I have configured the Proxy on the Workbench). When I am connected to internet directly, everything works fine (Able to create project, train, compare models etc). However the Workbench should return meaningful error instead of hanging or simply waiting while creating the project.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":4.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1479159384132,
        "Answerer_location":"Illinois, United States",
        "Answerer_reputation_count":513.0,
        "Answerer_view_count":113.0,
        "Challenge_adjusted_solved_time":42.2916322222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I want to run a pipeline for different files, but some of them don't need all of the defined nodes. How can I pass them?<\/p>",
        "Challenge_closed_time":1573135218943,
        "Challenge_comment_count":2,
        "Challenge_created_time":1572974635783,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1572982969067,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58716474",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":3.0,
        "Challenge_reading_time":2.06,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":44.6064333334,
        "Challenge_title":"How to run a pipeline except for a few nodes?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":859,
        "Challenge_word_count":34,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572973807452,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>To filter out a few lines of a pipeline you can simply filter the pipeline list from inside of python, my favorite way is to use a list comprehension.<\/p>\n\n<p><strong>by name<\/strong><\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>nodes_to_run = [node for node in pipeline.nodes if 'dont_run_me' not in node.name]\nrun(nodes_to_run, io)\n<\/code><\/pre>\n\n<p><strong>by tag<\/strong><\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>nodes_to_run = [node for node in pipeline.nodes if 'dont_run_tag' not in node.tags]\nrun(nodes_to_run, io)\n<\/code><\/pre>\n\n<p>It's possible to filter by any attribute tied to the pipeline node, (name, inputs, outputs, short_name, tags)<\/p>\n\n<p>If you need to run your pipeline this way in production or from the command line, you can either tag your pipeline to run with tags, or add a custom <code>click.option<\/code> to your <code>run<\/code> function inside of <code>kedro_cli.py<\/code> then run this filter when the flag is <code>True<\/code>.<\/p>\n\n<p><strong>Note<\/strong>\nThis assumes that you have your pipeline loaded into memory as <code>pipeline<\/code> and catalog loaded in as <code>io<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":14.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":148.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1464811778510,
        "Answerer_location":null,
        "Answerer_reputation_count":196.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":11.9435663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have successfully initialized a ModelQualityMonitor object.\nThen I created a monitoring schedule using the CreateMonitoringSchedule API! In the background sagemaker runs two processing jobs which merges the ground truth data with the collected endpoint data and then analyzes and creates the predefined regression metrics:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html<\/a><\/p>\n<p>Unfortunately, I am missing the MAPE (Mean Absolute Percentage Error) in the metrics, and would like to create this with in the future (also in CloudWatch).<\/p>\n<p>Sagemaker provides the following functionalities:<\/p>\n<ul>\n<li>Preprocessing and Postprocessing:\nIn addition to using the built-in mechanisms, you can extend the code with the preprocessing and postprocessing scripts.<\/li>\n<li>Bring Your Own Containers:\nAmazon SageMaker Model Monitor provides a prebuilt container with ability to analyze the data captured from endpoints for tabular datasets. If you would like to bring your own container, Model Monitor provides extension points which you can leverage.<\/li>\n<li>CloudWatch Metrics for Bring Your Own Containers<\/li>\n<\/ul>\n<p>Those points are documented on this site: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-custom-monitoring-schedules.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-custom-monitoring-schedules.html<\/a><\/p>\n<p>How exactly can I achieve my target of including MAPE with the above points?<\/p>\n<p>Here is a code snippet of my current implementation:<\/p>\n<pre><code>from sagemaker.model_monitor.model_monitoring import ModelQualityMonitor\nfrom sagemaker.model_monitor import EndpointInput\nfrom sagemaker.model_monitor.dataset_format import DatasetFormat\n\n# Create the model quality monitoring object\nMQM = ModelQualityMonitor(\n    role=role,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    volume_size_in_gb=20,\n    max_runtime_in_seconds=1800,\n    sagemaker_session=sagemaker_session,\n)\n\n# suggest a baseline\njob = MQM.suggest_baseline(\n    job_name=baseline_job_name,\n    baseline_dataset=&quot;.\/baseline.csv&quot;,\n    dataset_format=DatasetFormat.csv(header=True),\n    output_s3_uri=baseline_results_uri,\n    problem_type=&quot;Regression&quot;,\n    inference_attribute=&quot;predicted_price&quot;,\n    ground_truth_attribute=&quot;price&quot;,\n)\njob.wait(logs=False)\nbaseline_job = MQM.latest_baselining_job\n\n# create a monitoring schedule\nendpointInput = EndpointInput(\n    endpoint_name=&quot;dev-TestEndpoint&quot;,\n    destination=&quot;\/opt\/ml\/processing\/input_data&quot;,\n    inference_attribute=&quot;$.data.predicted_price&quot;\n)\nMQM.create_monitoring_schedule(\n    monitor_schedule_name=&quot;DS-Schedule&quot;,\n    endpoint_input=endpointInput,\n    output_s3_uri=baseline_results_uri,\n    constraints=baseline_job.suggested_constraints(),\n    problem_type=&quot;Regression&quot;,\n    ground_truth_input=ground_truth_upload_path,\n    schedule_cron_expression=&quot;cron(0 * ? * * *)&quot;, # hourly\n    enable_cloudwatch_metrics=True\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1651171215176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651128218337,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72039147",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":21.7,
        "Challenge_reading_time":43.53,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":11.9435663889,
        "Challenge_title":"Is there a way to include custom Regression Metrics in ModelQualityMonitor in AWS sagemaker?",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":165,
        "Challenge_word_count":261,
        "Platform":"Stack Overflow",
        "Poster_created_time":1613661928947,
        "Poster_location":null,
        "Poster_reputation_count":160.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>Amazon SageMaker model monitor only supports metrics that are defined <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html\" rel=\"nofollow noreferrer\">here<\/a> out of the box.\nIf you need to include another metric such as MAPE (Mean Absolute Percentage Error) in your case, you will have to rely on BYOC approach, note that with this approach you cannot &quot;add&quot; a metric to the available list, unfortunately you will have to implement the entire suite of metrics yourself. I understand this is not ideal for customers, I'd encourage you to reach out to your AWS account manager to create a request to add MAPE (Mean Absolute Percentage Error) as a supported metric in the long run. I've made a note of it as well and will rely it back to the team.<\/p>\n<p>In the meantime, you can find examples on how to BYOC <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/detect-nlp-data-drift-using-custom-amazon-sagemaker-model-monitor\/\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>I work for AWS but my opinions are my own.<\/p>\n<p>Thanks,\nRaghu<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.2,
        "Solution_reading_time":13.93,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":150.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":123.1885911111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am really confused about organizing google Vertex Ai dataset and train the autoML model in GCP. Could any one please help me to understand?<\/p>\n<p>Let me explain scenarios in which I have confusion.<\/p>\n<p>Let\u2019s suppose if I have <em>Text entity extraction<\/em> dataset in vertex Ai \u201c<strong>contract_delivery_02<\/strong>\u201d with 25 files. I have 3 labels created (<em>DelIncoTerms, DelLocation and DelWindow<\/em>) and I have trained model. This is working great.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KvWom.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KvWom.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Now, I have 10 more files to upload, where I have introduced 2 additional labels (<em>DelPrice &amp; DelDelivery<\/em>).<\/p>\n<p>My questions<\/p>\n<ol>\n<li>Do I require to do upload all the files (25 + 10) again ?<\/li>\n<li>Do I require to retrain my whole autoML model again ? or is there any other approach for this scenario?<\/li>\n<\/ol>",
        "Challenge_closed_time":1657060259888,
        "Challenge_comment_count":2,
        "Challenge_created_time":1656616780960,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72821008",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":7.4,
        "Challenge_reading_time":12.93,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":123.1885911111,
        "Challenge_title":"Vertex AI updating dataset and train model",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":149,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462469556836,
        "Poster_location":null,
        "Poster_reputation_count":509.0,
        "Poster_view_count":65.0,
        "Solution_body":"<p>For question #1, you don't have to upload all files again. In your <strong>Dataset<\/strong>, you just have to add your <strong>2 new labels<\/strong> and then upload your additional 10 files.\n<a href=\"https:\/\/i.stack.imgur.com\/rLep2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rLep2.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Once uploaded, you may now proceed to put labels on your newly added files (in your example, total of 10 files) and then assign the new labels on <strong>ALL files<\/strong> (25 + 10). You can do this by double-clicking the newly added text from the UI and then assign necessary labels.\n<a href=\"https:\/\/i.stack.imgur.com\/jqIaj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jqIaj.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For question #2, since there are newly added labels and training texts, it is necessary for you to retrain the whole autoML for more accurate Model and better quality of results.<\/p>\n<p>You may refer to this <a href=\"https:\/\/cloud.google.com\/natural-language\/automl\/docs\/prepare#expandable-2\" rel=\"nofollow noreferrer\">Text Entity Extraction preparation of data<\/a> and <a href=\"https:\/\/cloud.google.com\/natural-language\/automl\/docs\/models\" rel=\"nofollow noreferrer\">Training Models<\/a> documentation for more details.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":12.9,
        "Solution_reading_time":17.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":155.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1506516283190,
        "Answerer_location":"Torino, TO, Italia",
        "Answerer_reputation_count":875.0,
        "Answerer_view_count":52.0,
        "Challenge_adjusted_solved_time":433.2236119444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello Stackoverflowers,<\/p>\n\n<p>I'm using azureml and I'm wondering if it is possible to log a confusion matrix of the xgboost model I'm training, together with the other metrics I'm already logging. Here's a sample of the code I'm using:<\/p>\n\n<pre><code>from azureml.core.model import Model\nfrom azureml.core import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nimport json\n\nwith open('.\/azureml.config', 'r') as f:\n    config = json.load(f)\n\nsvc_pr = ServicePrincipalAuthentication(\n   tenant_id=config['tenant_id'],\n   service_principal_id=config['svc_pr_id'],\n   service_principal_password=config['svc_pr_password'])\n\n\nws = Workspace(workspace_name=config['workspace_name'],\n                        subscription_id=config['subscription_id'],\n                        resource_group=config['resource_group'],\n                        auth=svc_pr)\n\ny_pred = model.predict(dtest)\n\nacc = metrics.accuracy_score(y_test, (y_pred&gt;.5).astype(int))\nrun.log(\"accuracy\",  acc)\nf1 = metrics.f1_score(y_test, (y_pred&gt;.5).astype(int), average='binary')\nrun.log(\"f1 score\",  f1)\n\n\ncmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\nrun.log_confusion_matrix('Confusion matrix', cmtx)\n<\/code><\/pre>\n\n<p>The above code raises this kind of error:<\/p>\n\n<pre><code>TypeError: Object of type ndarray is not JSON serializable\n<\/code><\/pre>\n\n<p>I already tried to transform the matrix in a simpler one, but another error occurred as before I logged a \"manual\" version of it (<code>cmtx = [[30000, 50],[40, 2000]]<\/code>).<\/p>\n\n<pre><code>run.log_confusion_matrix('Confusion matrix', [list([int(y) for y in x]) for x in cmtx])\n\nAzureMLException: AzureMLException:\n    Message: UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-    c5103b205379\/Confusion matrix already exists.\n    InnerException None\n    ErrorResponse \n{\n    \"error\": {\n        \"message\": \"UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-c5103b205379\/Confusion matrix already exists.\"\n    }\n}\n<\/code><\/pre>\n\n<p>This makes me think that I'm not properly handling the command <code>run.log_confusion_matrix()<\/code>. So, again, which is the best way I can log a confusion matrix to my azureml experiments?<\/p>",
        "Challenge_closed_time":1593519812260,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591960207257,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62343056",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":29.81,
        "Challenge_score_count":3,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":433.2236119444,
        "Challenge_title":"How to log a confusion matrix to azureml platform using python",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1418,
        "Challenge_word_count":216,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506516283190,
        "Poster_location":"Torino, TO, Italia",
        "Poster_reputation_count":875.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>I eventually found a solution thanks to colleague of mine. I'm hence answering myself, in order to close the question and, maybe, help somebody else.<\/p>\n<p>You can find the proper function in this link: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----<\/a>.<\/p>\n<p>Anyway, you also have to consider that, apparently, Azure doesn't work with the standard confusion matrix format returned by sklearn. It accepts indeed ONLY list of list, instead of numpy array, populated with numpy.int64 elements. So you also have to transform the matrix in a simpler format (for the sake of simplicity I used the nested list comprehension in the command below:<\/p>\n<pre><code>cmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\ncmtx = {\n\n&quot;schema_type&quot;: &quot;confusion_matrix&quot;,\n&quot;parameters&quot;: params,\n &quot;data&quot;: {&quot;class_labels&quot;: [&quot;0&quot;, &quot;1&quot;],\n          &quot;matrix&quot;: [[int(y) for y in x] for x in cmtx]}\n}\nrun.log_confusion_matrix('Confusion matrix - error rate', cmtx)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.4,
        "Solution_reading_time":17.29,
        "Solution_score_count":6.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":126.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":4.4489933333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I started a batch prediction job in AutoML (now VertexAI) for a small csv in one of my buckets, using a classification model, then I noticed the csv had an error but was unable to find a way to cancel the job using the web GUI, it just says &quot;running&quot; but I see no &quot;stop&quot; or &quot;cancel&quot; button.<\/p>\n<p>Fortunately, it was done after 20 minutes, but I need to know how to stop a job since I will require predictions for way bigger files and can't risk having to wait until the job ends by itself. It was kind of desperating being able to watch the log throwing error after error and not being able to stop the job. I tried to delete the job but it said it can't be deleted while its running.<\/p>\n<p>I found a related question, but it was not answered, the job just finished itself after a couple of days. I can't risk that.\n<a href=\"https:\/\/stackoverflow.com\/questions\/68077606\/how-do-i-force-batch-prediction-to-stop\">How do I force &quot;batch prediction&quot; to stop?<\/a><\/p>\n<p>I will greatly appreciate any help.<\/p>",
        "Challenge_closed_time":1625628028808,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625608160277,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1625612012432,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68277691",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":14.01,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":5.5190363889,
        "Challenge_title":"How do I stop a Google Cloud's AutoML (now VertexAI) batch prediction job using the web GUI?",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":619,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Poster_created_time":1431573886067,
        "Poster_location":null,
        "Poster_reputation_count":38.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>Unfortunately the cancel\/stop feature is not yet available in the Vertex AI UI. As per <a href=\"https:\/\/stackoverflow.com\/questions\/68077606\/how-do-i-force-batch-prediction-to-stop\">How do I force &quot;batch prediction&quot; to stop?<\/a>, the OP sent a feedback. You can ask if there was a public issue tracker created for this so you can monitor the progress of the feature request there.<\/p>\n<p>But there is a workaround for this, just send a request <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.batchPredictionJobs\/cancel\" rel=\"nofollow noreferrer\">projects.locations.batchPredictionJobs.cancel<\/a> via REST.<\/p>\n<p>To do this you can send a request via curl. In this example the model and endpoint are located in <code>us-central1<\/code> thus the location defined in the request.<\/p>\n<p>Just supply your <code>project-id<\/code> and the <code>batch-prediction-id<\/code> on the request. To get the <code>batch-prediction-id<\/code> you can get it via UI:<\/p>\n<p>Get <code>batch-prediction-id<\/code> via UI:<\/p>\n<ul>\n<li>Open &quot;Batch Predictions&quot; tab in the Vertex AI UI<\/li>\n<li>Click on the job you want to cancel<\/li>\n<li>Job information will be displayed and the 1st entry will contain the Job ID<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/lKJdT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lKJdT.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To cancel the job send a cancel request via curl. If requests is successful, the response body is empty.<\/p>\n<pre><code>curl -X POST -H &quot;Content-Type: application\/json&quot; \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\nhttps:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/your-project-id\/locations\/us-central1\/batchPredictionJobs\/batch-prediction-job-id:cancel\n<\/code><\/pre>\n<p>Check in Vertex AI UI if the job was canceled.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/atSqt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/atSqt.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":13.5,
        "Solution_reading_time":27.51,
        "Solution_score_count":2.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":219.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":24.7979425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When reading the examples from Microsoft on azure ML CLI v2, they use the symbols:\n&quot;|&quot;, &quot;&gt;&quot;, etc., in their yml files.<\/p>\n<p>What do they mean, and where can I find explanations of possible syntax for the Azure CLI v2 engine?<\/p>",
        "Challenge_closed_time":1649231701843,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649142429250,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71747545",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":3.6,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":24.7979425,
        "Challenge_title":"Commands in the Azure ML yml files",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":100,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Poster_created_time":1620049475608,
        "Poster_location":"Denmark",
        "Poster_reputation_count":3.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>| - This pipe symbol in YAML document is used for <em><strong>&quot;Multiple line statements&quot;<\/strong><\/em><\/p>\n<pre><code>description: |\n  # Azure Machine Learning &quot;hello world&quot; job\n\n  This is a &quot;hello world&quot; job running in the cloud via Azure Machine Learning!\n\n  ## Description\n\n  Markdown is supported in the studio for job descriptions! You can edit the description there or via CLI.\n<\/code><\/pre>\n<p>in the above example, we need to write some multiple line description. So, we need to use &quot;|&quot; symbol<\/p>\n<p>&quot;&gt;&quot; - This symbol is used to save some content directly to a specific location document.<\/p>\n<pre><code>command: echo &quot;hello world&quot; &gt; .\/outputs\/helloworld.txt\n<\/code><\/pre>\n<p>In this above command, we need to post <strong>&quot;hello world&quot;<\/strong> to <em><strong>&quot;helloworld.txt&quot;<\/strong><\/em><\/p>\n<p>Check the below link for complete documentation regarding YAML files.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-yaml-job-command\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-yaml-job-command<\/a><\/p>\n<p>All these symbols are the YAML job commands which are used to accomplish a specific task through CLI.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.6,
        "Solution_reading_time":16.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":139.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1428951492492,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":1261.0,
        "Answerer_view_count":111.0,
        "Challenge_adjusted_solved_time":20.6618036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Despite prominent how-to posts on how to add datasets to Azure Machine Learning that say Excel is supported, when I actually go to add a dataset and select a local Excel file, there's no option for \"Excel\" in the required datatype property dropdown. I'm surprised that Azure wouldn't support Excel (right?) - am I missing something?<\/p>",
        "Challenge_closed_time":1476307260447,
        "Challenge_comment_count":0,
        "Challenge_created_time":1476303145067,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40007515",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":5.11,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.1431611111,
        "Challenge_title":"Azure Machine Learning Studio: how to add a dataset from a local Excel file?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1250,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1357592818807,
        "Poster_location":"Ann Arbor, MI",
        "Poster_reputation_count":99.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>The dropdown list indicates the \"Destination\" datatype for the new DATASET file you are creating, not the source type.<\/p>\n\n<p>I just uploaded a <code>.xlsx<\/code> file successfully into a <code>.CSV<\/code> file in AML.<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1476377527560,
        "Solution_link_count":0.0,
        "Solution_readability":5.2,
        "Solution_reading_time":2.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1327302732867,
        "Answerer_location":"USA",
        "Answerer_reputation_count":19711.0,
        "Answerer_view_count":1030.0,
        "Challenge_adjusted_solved_time":0.1674261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Running SageMaker within a local Jupyter notebook (using VS Code) works without issue, except that attempting to train an XGBoost model using the AWS hosted container results in errors (container name: <code>246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1-cpu-py3<\/code>).<\/p>\n<h2>Jupyter Notebook<\/h2>\n<pre class=\"lang-py prettyprint-override\"><code>import sagemaker\n\nsession = sagemaker.LocalSession()\n\n# Load and prepare the training and validation data\n...\n\n# Upload the training and validation data to S3\ntest_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\nval_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\ntrain_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n\nregion = session.boto_region_name\ninstance_type = 'ml.m4.xlarge'\ncontainer = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1', 'py3', instance_type=instance_type)\n\nrole = 'arn:aws:iam::&lt;USER ID #&gt;:role\/service-role\/AmazonSageMaker-ExecutionRole-&lt;ROLE ID #&gt;'\n\nxgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n\nxgb_estimator.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6,\n                                  subsample=0.8, objective='reg:squarederror', early_stopping_rounds=10,\n                                  num_round=200)\n\ns3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\ns3_input_validation = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='csv')\n\nxgb_estimator.fit({'train': s3_input_train, 'validation': s3_input_validation})\n<\/code><\/pre>\n<h2>Docker Container KeyError<\/h2>\n<pre><code>algo-1-tfcvc_1  | ERROR:sagemaker-containers:Reporting training FAILURE\nalgo-1-tfcvc_1  | ERROR:sagemaker-containers:framework error: \nalgo-1-tfcvc_1  | Traceback (most recent call last):\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_containers\/_trainer.py&quot;, line 84, in train\nalgo-1-tfcvc_1  |     entrypoint()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 94, in main\nalgo-1-tfcvc_1  |     train(framework.training_env())\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 90, in train\nalgo-1-tfcvc_1  |     run_algorithm_mode()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 68, in run_algorithm_mode\nalgo-1-tfcvc_1  |     checkpoint_config=checkpoint_config\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/train.py&quot;, line 115, in sagemaker_train\nalgo-1-tfcvc_1  |     validated_data_config = channels.validate(data_config)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 106, in validate\nalgo-1-tfcvc_1  |     channel_obj.validate(value)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 52, in validate\nalgo-1-tfcvc_1  |     if (value[CONTENT_TYPE], value[TRAINING_INPUT_MODE], value[S3_DIST_TYPE]) not in self.supported:\nalgo-1-tfcvc_1  | KeyError: 'S3DistributionType'\n\n<\/code><\/pre>\n<h2>Local PC Runtime Error<\/h2>\n<pre><code>RuntimeError: Failed to run: ['docker-compose', '-f', '\/tmp\/tmp71tx0fop\/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1\n<\/code><\/pre>\n<p>If the Jupyter notebook is run using the Amazon cloud SageMaker environment (rather than on the local PC), there are no errors. Note that when running on the cloud notebook, the session is initialized as:<\/p>\n<pre><code>session = sagemaker.Session()\n<\/code><\/pre>\n<p>It appears that there is an issue with how the <code>LocalSession()<\/code> works with the hosted docker container.<\/p>",
        "Challenge_closed_time":1597366468132,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597366468133,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63405080",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":23.3,
        "Challenge_reading_time":56.88,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker in local Jupyter notebook: cannot use AWS hosted XGBoost container (\"KeyError: 'S3DistributionType'\" and \"Failed to run: ['docker-compose'\")",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1174,
        "Challenge_word_count":293,
        "Platform":"Stack Overflow",
        "Poster_created_time":1327302732867,
        "Poster_location":"USA",
        "Poster_reputation_count":19711.0,
        "Poster_view_count":1030.0,
        "Solution_body":"<p>When running SageMaker in a local Jupyter notebook, it expects the Docker container to be running on the local machine as well.<\/p>\n<p>The key to ensuring that SageMaker (running in a local notebook) uses the AWS hosted docker container, is to omit the <code>LocalSession<\/code> object when initializing the <code>Estimator<\/code>.<\/p>\n<h2>Wrong<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n<\/code><\/pre>\n<h2>Correct<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output')\n<\/code><\/pre>\n<p>\u00a0\u00a0<\/p>\n<h2>Additional info<\/h2>\n<p>The SageMaker Python SDK source code provides the following helpful hints:<\/p>\n<h1>File: <em>sagemaker\/local\/local_session.py<\/em><\/h1>\n<pre><code>class LocalSagemakerClient(object):\n    &quot;&quot;&quot;A SageMakerClient that implements the API calls locally.\n\n    Used for doing local training and hosting local endpoints. It still needs access to\n    a boto client to interact with S3 but it won't perform any SageMaker call.\n    ...\n<\/code><\/pre>\n<h1>File: <em>sagemaker\/estimator.py<\/em><\/h1>\n<pre><code>class EstimatorBase(with_metaclass(ABCMeta, object)):\n    &quot;&quot;&quot;Handle end-to-end Amazon SageMaker training and deployment tasks.\n\n    For introduction to model training and deployment, see\n    http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\n\n    Subclasses must define a way to determine what image to use for training,\n    what hyperparameters to use, and how to create an appropriate predictor instance.\n    &quot;&quot;&quot;\n\n    def __init__(self, role, train_instance_count, train_instance_type,\n                 train_volume_size=30, train_max_run=24 * 60 * 60, input_mode='File',\n                 output_path=None, output_kms_key=None, base_job_name=None, sagemaker_session=None, tags=None):\n        &quot;&quot;&quot;Initialize an ``EstimatorBase`` instance.\n\n        Args:\n            role (str): An AWS IAM role (either name or full ARN). ...\n            \n        ...\n\n            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with\n                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one\n                using the default AWS configuration chain.\n        &quot;&quot;&quot;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1597367070867,
        "Solution_link_count":1.0,
        "Solution_readability":16.7,
        "Solution_reading_time":32.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":235.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1334649856036,
        "Answerer_location":null,
        "Answerer_reputation_count":2091.0,
        "Answerer_view_count":501.0,
        "Challenge_adjusted_solved_time":315.0776647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Azure ML, I want to enter data to a model through a published Web Service. \nThe way to tell this to the Web Service, as far as I can tell, it to have an 'Enter Data' box coming into the same input as the Web service. <\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/m1x5x.png\" alt=\"enter image description here\"><\/p>\n\n<p>You can then set you data format in the 'Enter Data' properties:<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/VQJ9V.png\" alt=\"enter image description here\"><\/p>\n\n<p>I want that list to be an arbitrary-length array of samples. This works if your input is:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"samples\"\n      ],\n      \"Values\": [\n        [\n          1\n        ],\n        [\n          2\n        ],\n        [\n          3\n        ],\n        [\n          4\n        ],\n        [\n          5\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>This is ok, but ideally it would be easier, and (more importantly) more network-efficient, if I could send them as:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"samples\"\n      ],\n      \"Values\": [\n        [\n          1,2,3,4,5\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>Is there a correct syntax to implement this? <\/p>",
        "Challenge_closed_time":1438871325836,
        "Challenge_comment_count":2,
        "Challenge_created_time":1437737046243,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/31609319",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":13.74,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":315.0776647222,
        "Challenge_title":"'Enter Data' as list instead of list of lists in Azure ML Web Service",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":174,
        "Challenge_word_count":153,
        "Platform":"Stack Overflow",
        "Poster_created_time":1266595927520,
        "Poster_location":null,
        "Poster_reputation_count":7681.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>I have worked internally to request a confirmation of your concern - <\/p>\n\n<blockquote>\n  <p>'Enter Data' as list instead of list of lists in Azure ML Web Service<\/p>\n<\/blockquote>\n\n<p>but you expected feature is not available today in Azure ML Studio (The reason behind is Azure ML has to be able to read the input data as a tabular format, rows and columns). Such being the case, I would like to suggest you to submit a new feature request via below option:<\/p>\n\n<p>On Azure ML Studio -> the upper right corner, there is a smiley face, please click that and send the feedback.<\/p>\n\n<p>Should you have any further concerns, please feel free to let me know.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.8,
        "Solution_reading_time":7.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":114.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1431258605807,
        "Answerer_location":"Melbourne, Victoria, Australia",
        "Answerer_reputation_count":331.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":617.4599341667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a dataset in azure machine learning (.csv), on the same dataset I have multiple models build, I want to subset data for each of the model based on a different column<\/p>\n\n<p>Input:<\/p>\n\n<pre><code>ID col1 col2 col3\n1  0    13   0\n2  5    45   0\n3  10   0    34\n4  12   1    3\n<\/code><\/pre>\n\n<p>For the 1st model I want to retain all records where col1 not equal to None<\/p>\n\n<pre><code>ID col1 col2 col3\n2  5    45   0\n3  10   0    34\n4  12   1    3\n<\/code><\/pre>\n\n<p>Similarly for model 2<\/p>\n\n<pre><code>ID col1 col2 col3\n1  0    13   0\n2  5    45   0\n4  12   1    3\n<\/code><\/pre>\n\n<p>Hope it was clear<\/p>\n\n<p>The equivalent in R would be <\/p>\n\n<pre><code>df[!df$col1 == \"None\",] \n<\/code><\/pre>",
        "Challenge_closed_time":1461479422230,
        "Challenge_comment_count":0,
        "Challenge_created_time":1459161991533,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1459256566467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36260727",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.4,
        "Challenge_reading_time":8.24,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":643.7307491667,
        "Challenge_title":"Equivalent of Subset in Azure machine learning studio",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":243,
        "Challenge_word_count":130,
        "Platform":"Stack Overflow",
        "Poster_created_time":1406266059940,
        "Poster_location":"Link\u00f6ping, Sweden",
        "Poster_reputation_count":1677.0,
        "Poster_view_count":221.0,
        "Solution_body":"<p>You can use the \"Execute R Script\" module and just plug in your R code there.<\/p>\n\n<pre><code>df &lt;- maml.mapInputPort(1)\ndf &lt;- df[!df$col1 == \"None\",] \nmaml.mapOutputPort(\"df\");\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":2.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1327570314367,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation_count":2854.0,
        "Answerer_view_count":324.0,
        "Challenge_adjusted_solved_time":30.6574666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are deploying a data consortium between more than 10 companies. Wi will deploy several machine learning models (in general advanced analytics models) for all the companies and we will administrate all the models. We are looking for a solution that administrates several servers, clusters and data science pipelines. I love kedro, but not sure what is the best option to administrate all while using kedro.<\/p>\n<p>In summary, we are looking for the best solution to administrate several models, tasks and pipelines in different servers and possibly Spark clusters. Our current options are:<\/p>\n<ul>\n<li><p>AWS as our data warehouse and Databricks for administrating servers, clusters and tasks. I don't feel that the notebooks of databricks are a good solution for building pipelines and to work collaboratively, so I would like to connect kedro to databricks (is it good? is it easy to schedule the run of the kedro pipelines using databricks?)<\/p>\n<\/li>\n<li><p>Using GCP for data warehouse and use kubeflow (iin GCP) for deploying models and the administration and the schedule of the pipelines and the needed resources<\/p>\n<\/li>\n<li><p>Setting up servers from ASW or GCP, install kedro and schedule the pipelines with airflow (I see a big problem administrating 20 servers and 40 pipelines)<\/p>\n<\/li>\n<\/ul>\n<p>I would like to know if someone knows what is the best option between these alternatives, their  downsides and advantages, or if there are more possibilities.<\/p>",
        "Challenge_closed_time":1605891844630,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605830422657,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1605836750283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64921833",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":19.1,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":17.0616591667,
        "Challenge_title":"DataBricks + Kedro Vs GCP + Kubeflow Vs Server + Kedro + Airflow",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":967,
        "Challenge_word_count":241,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605828724552,
        "Poster_location":null,
        "Poster_reputation_count":59.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I'll try and summarise what I know, but be aware that I've not been part of a KubeFlow project.<\/p>\n<h2>Kedro on Databricks<\/h2>\n<p>Our approach was to build our project with CI and then execute the pipeline from a notebook. We <em>did not<\/em> use the <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/11_tools_integration\/03_databricks.html\" rel=\"nofollow noreferrer\">kedro recommended approach<\/a> of using databricks-connect due to the <a href=\"https:\/\/databricks.com\/product\/aws-pricing\" rel=\"nofollow noreferrer\">large price difference<\/a> between Jobs and Interactive Clusters (which are needed for DB-connect). If you're working on several TB's of data, this quickly becomes relevant.<\/p>\n<p>As a DS, this approach may feel natural, as a SWE though it does not. Running pipelines in notebooks feels hacky. It works but it feels non-industrialised. Databricks performs well in automatically spinning up and down clusters &amp; taking care of the runtime for you. So their value add is abstracting IaaS away from you (more on that later).<\/p>\n<h2>GCP &amp; &quot;Cloud Native&quot;<\/h2>\n<p><strong>Pro<\/strong>: GCP's main selling point is BigQuery. It is an incredibly powerful platform, simply because you can be productive from day 0. I've seen people build entire web API's on top of it. KubeFlow isn't tied to GCP so you could port this somewhere else later on. Kubernetes will also allow you to run anything else you wish on the cluster, API's, streaming, web services, websites, you name it.<\/p>\n<p><strong>Con<\/strong>: Kubernetes is complex. If you have 10+ engineers to run this project long-term, you should be OK. But don't underestimate the complexity of Kubernetes. It is to the cloud what Linux is to the OS world. Think log management, noisy neighbours (one cluster for web APIs + batch spark jobs), multi-cluster management (one cluster per department\/project), security, resource access etc.<\/p>\n<h2>IaaS server approach<\/h2>\n<p>Your last alternative, the manual installation of servers is one I would recommend only if you have a large team, extremely large data and are building a long-term product who's revenue can sustain the large maintenance costs.<\/p>\n<h2>The people behind it<\/h2>\n<p>How does the talent market look like in your region? If you can hire experienced engineers with GCP knowledge, I'd go for the 2nd solution. GCP is a mature, &quot;native&quot; platform in the sense that it abstracts a lot away for customers. If your market has mainly AWS engineers, that may be a better road to take. If you have a number of kedro engineers, that also has relevance. Note that kedro is agnostic enough to run anywhere. It's really just python code.<\/p>\n<p><strong>Subjective advise<\/strong>:<\/p>\n<p>Having worked mostly on AWS projects and a few GCP projects, I'd go for GCP. I'd use the platform's components (BigQuery, Cloud Run, PubSub, Functions, K8S) as a toolbox to choose from and build an organisation around that. Kedro can run in any of these contexts, as a triggered job by the Scheduler, as a container on Kubernetes or as a ETL pipeline bringing data into (or out of) BigQuery.<\/p>\n<p>While Databricks is &quot;less management&quot; than raw AWS, it's still servers to think about and VPC networking charges to worry over. BigQuery is simply GB queried. Functions are simply invocation count. These high level components will allow you to quickly show value to customers and you only need to go deeper (RaaS -&gt; PaaS -&gt; IaaS) as you scale.<\/p>\n<p>AWS also has these higher level abstractions over IaaS but in general, it appears (to me) that Google's offering is the most mature. Mainly because they have published tools they've been using internally for almost a decade whereas AWS has built new tools for the market. AWS is the king of IaaS though.<\/p>\n<p>Finally, a bit of content, <a href=\"https:\/\/youtu.be\/kjhXMTOLtac?t=618\" rel=\"nofollow noreferrer\">two former colleagues have discussed ML industrialisation frameworks earlier this fall<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1605947117163,
        "Solution_link_count":3.0,
        "Solution_readability":7.9,
        "Solution_reading_time":49.9,
        "Solution_score_count":4.0,
        "Solution_sentence_count":41.0,
        "Solution_word_count":606.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1614873430827,
        "Answerer_location":null,
        "Answerer_reputation_count":169.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":14.7412641667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to setup DVC with Google Drive storage as shown <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#url-format\" rel=\"nofollow noreferrer\">here<\/a>. So far, I've been unsuccessful in pushing data to the remote. I tried both with and without the Google App setup.<\/p>\n<p>After running a <code>dvc push -v<\/code>, the following exception is shown:<\/p>\n<pre><code>  File &quot;(...)\/anaconda3\/lib\/python3.8\/site-packages\/googleapiclient\/discovery.py&quot;, line 387, in _retrieve_discovery_doc\n    raise UnknownApiNameOrVersion(&quot;name: %s  version: %s&quot; % (serviceName, version))\ngoogleapiclient.errors.UnknownApiNameOrVersion: name: drive  version: v2\n<\/code><\/pre>\n<p>DVC was installed via <code>pip install dvc[gdrive]<\/code>. The <code>pip freeze<\/code> of the concerning packages is:<\/p>\n<pre><code>oauth2client==4.1.3\ngoogle-api-python-client==2.0.1\ndvc==2.0.1\n<\/code><\/pre>\n<p>Any help is thoroughly appreciated.<\/p>",
        "Challenge_closed_time":1614873489452,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614869465907,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1614874338736,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66477468",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":14.21,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.1176513889,
        "Challenge_title":"Data Version Control with Google Drive Remote: \"googleapiclient.errors.UnknownApiNameOrVersion: name: drive version: v2\"",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":979,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1593006899208,
        "Poster_location":null,
        "Poster_reputation_count":704.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>Can you try to install <code>google-api-python-client==1.12.8<\/code> and test in that way?<\/p>\n<p>Edit:<\/p>\n<p>It appears to be that, this was a bug in the 2.0.0-2.0.1 of google-api-client and resolved in 2.0.2. So this should also work <code>google-api-python-client&gt;=2.0.2<\/code><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1614927407287,
        "Solution_link_count":0.0,
        "Solution_readability":3.7,
        "Solution_reading_time":3.76,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1550902509267,
        "Answerer_location":null,
        "Answerer_reputation_count":2669.0,
        "Answerer_view_count":3292.0,
        "Challenge_adjusted_solved_time":9.0819222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>Issue<\/strong> : Unable to get best model from AutoML run.<\/p>\n<p><strong>Code:<\/strong><\/p>\n<pre><code>best_run, fitted_model = automl_run.get_output()\nprint(best_run.properties[&quot;run_algorithm&quot;])\n<\/code><\/pre>\n<p><strong>Error Message :<\/strong><\/p>\n<pre><code>ErrorResponse \n[stderr]{\n[stderr]    &quot;error&quot;: {\n[stderr]        &quot;code&quot;: &quot;UserError&quot;,\n[stderr]        &quot;message&quot;: &quot;The model you attempted to retrieve requires 'xgboost' to be installed at '==1.3.3'. You have 'xgboost==1.3.3', please reinstall 'xgboost==1.3.3' (e.g. `pip install xgboost==1.3.3`) and rerun the previous command.&quot;,\n[stderr]        &quot;target&quot;: &quot;get_output&quot;,\n[stderr]        &quot;inner_error&quot;: {\n[stderr]            &quot;code&quot;: &quot;NotSupported&quot;,\n[stderr]            &quot;inner_error&quot;: {\n[stderr]                &quot;code&quot;: &quot;IncompatibleOrMissingDependency&quot;\n[stderr]            }\n[stderr]        },\n[stderr]        &quot;reference_code&quot;: &quot;910310e6-2433-40cd-b597-9ec2950bc1d8&quot;\n[stderr]    }\n<\/code><\/pre>\n<p><strong>Conda Dependency<\/strong><\/p>\n<pre><code># Conda environment specification. The dependencies defined in this file will\n# be automatically provisioned for runs with userManagedDependencies=False.\n\n# Details about the Conda environment file format:\n# https:\/\/conda.io\/docs\/user-guide\/tasks\/manage-environments.html#create-env-file-manually\n\nname: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.12\n\n- pip:\n  - azureml-train-automl-runtime==1.38.0\n  - azureml-train-automl-client==1.38.0\n  - inference-schema\n  - azureml-interpret==1.38.0\n  - azureml-defaults==1.38.0\n- numpy&gt;=1.16.0,&lt;1.19.0\n- pandas==0.25.1\n- scikit-learn==0.22.1\n- py-xgboost&lt;=1.3.3\n- fbprophet==0.5\n- holidays==0.9.11\n- psutil&gt;=5.2.2,&lt;6.0.0\n- matplotlib=3.3.2\n- seaborn=0.9.0\n- joblib=0.13.2\n- joblib\nchannels:\n- anaconda\n- conda-forge\n<\/code><\/pre>\n<p><strong>Question:<\/strong><\/p>\n<ul>\n<li>What should be in my conda dependency that can fix this error<\/li>\n<li>I've tried making <code>py-xgboost==1.3.3<\/code> , but it didn't work.<\/li>\n<li>Any luck - how to fix this ?<\/li>\n<\/ul>",
        "Challenge_closed_time":1648186385707,
        "Challenge_comment_count":2,
        "Challenge_created_time":1648153690787,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71609028",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.9,
        "Challenge_reading_time":29.38,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":9.0819222222,
        "Challenge_title":"Azure AutoML dependency failure",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":142,
        "Challenge_word_count":173,
        "Platform":"Stack Overflow",
        "Poster_created_time":1642621384680,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":11.0,
        "Solution_body":"<blockquote>\n<p>Error - &quot;The model you attempted to retrieve requires 'xgboost' to be\ninstalled at '==1.3.3'. You have 'xgboost==1.3.3', please reinstall\n'xgboost==1.3.3' (e.g. <code>pip install xgboost==1.3.3<\/code>) and rerun the\nprevious command.&quot;<\/p>\n<\/blockquote>\n<p>As given in above error message, it should be <code>pip install xgboost==1.3.3<\/code> not <code>py-xgboost&lt;=1.3.3<\/code><\/p>\n<p>If it does not work, try downgraded version of <code>xgboost<\/code><\/p>\n<pre><code>pip install xgboost==0.90\n<\/code><\/pre>\n<p>Refer this github <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1421\" rel=\"nofollow noreferrer\">link<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.6,
        "Solution_reading_time":8.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426093220648,
        "Answerer_location":"Bengaluru, India",
        "Answerer_reputation_count":1861.0,
        "Answerer_view_count":294.0,
        "Challenge_adjusted_solved_time":11.1307344444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any way to access the kedro pipeline environment name? Actually below is my problem.<\/p>\n<p>I am loading the config paths as below<\/p>\n<pre><code>conf_paths = [&quot;conf\/base&quot;, &quot;conf\/local&quot;]  \nconf_loader = ConfigLoader(conf_paths)\nparameters = conf_loader.get(&quot;parameters*&quot;, &quot;parameters*\/**&quot;)\ncatalog = conf_loader.get(&quot;catalog*&quot;)\n\n<\/code><\/pre>\n<p>But  I have few environments like  <code>&quot;conf\/server&quot; <\/code>, <code>&quot;conf\/test&quot;<\/code> etc, So if I have env name available I can add it to conf_paths as <code>&quot;conf\/&lt;env_name&gt;&quot;<\/code>  so that kedro will read the files from the respective env folder.\nBut now if the env path is not added to conf_paths, the files are not being read by kedro even if i specify the env name while I  run kedro like    <code>kedro run --env=server <\/code>\nI searched for all the docs but was not able to find any solution.<\/p>\n<p>EDIT:\nElaborating more on the problem.\nI am using the above-given parameters and catalog dicts in the nodes. I only have keys that are common for all runs in <code>conf\/base\/parameters.yml<\/code> and the environment specific keys in <code>conf\/server\/parameters.yml<\/code> but when i do <code>kedro run --env=server<\/code> I am getting <code>keyerror<\/code> which means the keys in <code>conf\/server\/parameters.yml<\/code> is not available in the parameters dict. If I add  <code>conf\/server<\/code> to config_paths kedro is running well without keyerror.<\/p>",
        "Challenge_closed_time":1639600021140,
        "Challenge_comment_count":8,
        "Challenge_created_time":1639518159733,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1639559950496,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70355869",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":11.3,
        "Challenge_reading_time":19.8,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":22.7392797222,
        "Challenge_title":"How to access environment name in kedro pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":712,
        "Challenge_word_count":201,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495105930728,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>You don't need to define config paths, config loader etc unless you are trying to override something.<\/p>\n<p>If you are using kedro 0.17.x, the hooks.py will look something like this.<\/p>\n<p>Kedro will pass, base, local and the env you specified during runtime in <code>conf_paths<\/code> into <code>ConfigLoader<\/code>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class ProjectHooks:\n    @hook_impl\n    def register_config_loader(\n        self, conf_paths: Iterable[str], env: str, extra_params: Dict[str, Any]\n    ) -&gt; ConfigLoader:\n        return ConfigLoader(conf_paths)\n\n    @hook_impl\n    def register_catalog(\n        self,\n        catalog: Optional[Dict[str, Dict[str, Any]]],\n        credentials: Dict[str, Dict[str, Any]],\n        load_versions: Dict[str, str],\n        save_version: str,\n        journal: Journal,\n    ) -&gt; DataCatalog:\n        return DataCatalog.from_config(\n            catalog, credentials, load_versions, save_version, journal\n        )\n<\/code><\/pre>\n<p>In question, I can see you have defined <code>conf_paths<\/code> and <code>conf_loader<\/code> and the env path is not present. So kedro will ignore the env passed during runtime.<\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.7,
        "Solution_reading_time":13.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":121.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":6.7810055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am submitting the training through a script file. Following is the content of the <code>train.py<\/code> script. Azure ML is treating all these as one run (instead of run per alpha value as coded below) as <code>Run.get_context()<\/code> is returning the same Run id.<\/p>\n<p><strong>train.py<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.opendatasets import Diabetes\nfrom azureml.core import Run\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.externals import joblib\n\nimport math\nimport os\nimport logging\n\n# Load dataset\ndataset = Diabetes.get_tabular_dataset()\nprint(dataset.take(1))\n\ndf = dataset.to_pandas_dataframe()\ndf.describe()\n\n# Split X (independent variables) &amp; Y (target variable)\nx_df = df.dropna()      # Remove rows that have missing values\ny_df = x_df.pop(&quot;Y&quot;)    # Y is the label\/target variable\n\nx_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=66)\nprint('Original dataset size:', df.size)\nprint(&quot;Size after dropping 'na':&quot;, x_df.size)\nprint(&quot;Training split size: &quot;, x_train.size)\nprint(&quot;Test split size: &quot;, x_test.size)\n\n# Training\nalphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters\n\n# Create and log interactive runs\n\noutput_dir = os.path.join(os.getcwd(), 'outputs')\n\nfor hyperparam_alpha in alphas:\n    # Get the experiment run context\n    run = Run.get_context()\n    print(&quot;Started run: &quot;, run.id)\n    run.log(&quot;train_split_size&quot;, x_train.size)\n    run.log(&quot;test_split_size&quot;, x_train.size)\n    run.log(&quot;alpha_value&quot;, hyperparam_alpha)\n\n    # Train\n    print(&quot;Train ...&quot;)\n    model = Ridge(hyperparam_alpha)\n    model.fit(X = x_train, y = y_train)\n    \n    # Predict\n    print(&quot;Predict ...&quot;)\n    y_pred = model.predict(X = x_test)\n\n    # Calculate &amp; log error\n    rmse = math.sqrt(mean_squared_error(y_true = y_test, y_pred = y_pred))\n    run.log(&quot;rmse&quot;, rmse)\n    print(&quot;rmse&quot;, rmse)\n\n    # Serialize the model to local directory\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir, exist_ok=True) \n\n    print(&quot;Save model ...&quot;)\n    model_name = &quot;model_alpha_&quot; + str(hyperparam_alpha) + &quot;.pkl&quot; # Pickle file\n    file_path = os.path.join(output_dir, model_name)\n    joblib.dump(value = model, filename = file_path)\n\n    # Upload the model\n    run.upload_file(name = model_name, path_or_stream = file_path)\n\n    # Complete the run\n    run.complete()\n<\/code><\/pre>\n<p><strong>Experiments view<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/E6xAG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E6xAG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Authoring code (i.e. control plane)<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom azureml.core import Workspace, Experiment, RunConfiguration, ScriptRunConfig, VERSION, Run\n\nws = Workspace.from_config()\nexp = Experiment(workspace = ws, name = &quot;diabetes-local-script-file&quot;)\n\n# Create new run config obj\nrun_local_config = RunConfiguration()\n\n# This means that when we run locally, all dependencies are already provided.\nrun_local_config.environment.python.user_managed_dependencies = True\n\n# Create new script config\nscript_run_cfg = ScriptRunConfig(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    script = 'train.py',\n    run_config = run_local_config) \n\nrun = exp.submit(script_run_cfg)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>",
        "Challenge_closed_time":1599436122230,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599411710610,
        "Challenge_favorite_count":3,
        "Challenge_last_edit_time":1599438192360,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63766714",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":46.74,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":6.7810055556,
        "Challenge_title":"Run.get_context() gives the same run id",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2523,
        "Challenge_word_count":324,
        "Platform":"Stack Overflow",
        "Poster_created_time":1245726715288,
        "Poster_location":"Cumming, GA",
        "Poster_reputation_count":77230.0,
        "Poster_view_count":6359.0,
        "Solution_body":"<h2>Short Answer<\/h2>\n<h3>Option 1: create child runs within run<\/h3>\n<p><code>run = Run.get_context()<\/code> assigns the run object of the run that you're currently in to <code>run<\/code>. So in every iteration of the hyperparameter search, you're logging to the same run. To solve this, you need to create child (or sub-) runs for each hyperparameter value. You can do this with <code>run.child_run()<\/code>. Below is the template for making this happen.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>run = Run.get_context()\n\nfor hyperparam_alpha in alphas:\n    # Get the experiment run context\n    run_child = run.child_run()\n    print(&quot;Started run: &quot;, run_child.id)\n    run_child.log(&quot;train_split_size&quot;, x_train.size)\n<\/code><\/pre>\n<p>On the <code>diabetes-local-script-file<\/code> Experiment page, you can see that Run <code>9<\/code> was the parent run and Runs <code>10-19<\/code> were the child runs if you click &quot;Include child runs&quot; page. There is also a &quot;Child runs&quot; tab on Run 9 details page.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5IMcz.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5IMcz.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Long answer<\/h2>\n<p>I highly recommend abstracting the hyperparameter search away from the data plane (i.e. <code>train.py<\/code>) and into the control plane (i.e. &quot;authoring code&quot;). This becomes especially valuable as training time increases and you can arbitrarily parallelize and also choose Hyperparameters more intelligently by using Azure ML's <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters\" rel=\"noreferrer\"><code>Hyperdrive<\/code><\/a>.<\/p>\n<h3>Option 2 Create runs from control plane<\/h3>\n<p>Remove the loop from your code, add the code like below (<a href=\"https:\/\/gist.github.com\/swanderz\/f5c0dc1aefc796d37f6bc3600f2ae3cd\" rel=\"noreferrer\">full data and control here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import argparse\nfrom pprint import pprint\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--alpha', type=float, default=0.5)\nargs = parser.parse_args()\nprint(&quot;all args:&quot;)\npprint(vars(args))\n\n# use the variable like this\nmodel = Ridge(args.alpha)\n<\/code><\/pre>\n<p>below is how to submit a single run using a script argument. To submit multiple runs, just use a loop in the control plane.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters\n\nlist_rcs = [ScriptRunConfig(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    script = 'train.py',\n    arguments=['--alpha',a],\n    run_config = run_local_config) for a in alphas]\n\nlist_runs = [exp.submit(rc) for rc in list_rcs]\n\n<\/code><\/pre>\n<h3>Option 3 Hyperdrive (IMHO the recommended approach)<\/h3>\n<p>In this way you outsource the hyperparameter source to <code>Hyperdrive<\/code>. The UI will also report results exactly how you want them, and via the API you can easily download the best model.  Note you can't use this locally anymore and must use <code>AMLCompute<\/code>, but to me it is a worthwhile trade-off.<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters#configure-experiment\" rel=\"noreferrer\">This is a great overview<\/a>. Excerpt below (<a href=\"https:\/\/gist.github.com\/swanderz\/f5c0dc1aefc796d37f6bc3600f2ae3cd#file-hyperdrive-ipynb\" rel=\"noreferrer\">full code here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>param_sampling = GridParameterSampling( {\n        &quot;alpha&quot;: choice(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)\n    }\n)\n\nestimator = Estimator(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    entry_script = 'train.py',\n    compute_target=cpu_cluster,\n    environment_definition=Environment.get(workspace=ws, name=&quot;AzureML-Tutorial&quot;)\n)\n\nhyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n                          hyperparameter_sampling=param_sampling, \n                          policy=None,\n                          primary_metric_name=&quot;rmse&quot;, \n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                          max_total_runs=10,\n                          max_concurrent_runs=4)\n\nrun = exp.submit(hyperdrive_run_config)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q1AhJ.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q1AhJ.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":14.3,
        "Solution_reading_time":57.41,
        "Solution_score_count":7.0,
        "Solution_sentence_count":41.0,
        "Solution_word_count":417.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":3.9519652778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Amazon SageMaker has <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">inference pipelines<\/a> that process requests for inferences on data. It sounds as though inferences are similar (or perhaps identical) to predictions. Are there any differences between inferences and predictions? If so, what? If not, why not just call it a prediction pipeline?<\/p>",
        "Challenge_closed_time":1561569885852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561555658777,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56773989",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":6.54,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.9519652778,
        "Challenge_title":"In Amazon SageMaker, what (if any) is the difference between an inference and prediction?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":42,
        "Challenge_word_count":61,
        "Platform":"Stack Overflow",
        "Poster_created_time":1336973807643,
        "Poster_location":"Minneapolis, MN, United States",
        "Poster_reputation_count":1907.0,
        "Poster_view_count":174.0,
        "Solution_body":"<p>Inference usually refers to applying a learned transformation to input data. That learned transformation could be something else than a prediction (eg dim reduction, clustering, entity extraction etc). So calling that process a prediction would be a bit too restrictive in my opinion<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":3.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1621410539876,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":3579.0,
        "Answerer_view_count":1775.0,
        "Challenge_adjusted_solved_time":4.1121516667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>got a folder called data-asset which contains a yaml file with the following<\/p>\n<pre><code>type: uri_folder\nname: &lt;name_of_data&gt;\ndescription: &lt;description goes here&gt;\npath: &lt;path&gt;\n<\/code><\/pre>\n<p>In a pipeline am referencing this using azure cli inline script using the following command az ml data create -f .yml but getting error<\/p>\n<p>full error-D:\\a\\1\\s\\ETL\\data-asset&gt;az ml data create -f data-asset.yml\nERROR: 'ml' is misspelled or not recognized by the system.<\/p>\n<p>Examples from AI knowledge base:\naz extension add --name anextension\nAdd extension by name<\/p>\n<p>trying to implement this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-data-assets?tabs=CLI\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-data-assets?tabs=CLI<\/a><\/p>\n<p>how can a resolve this?<\/p>",
        "Challenge_closed_time":1659361184080,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659348144177,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73192053",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":12.93,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.6221952778,
        "Challenge_title":"azure cli not recognizing the following command az ml data create -f <file-name>.yml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":112,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606756004663,
        "Poster_location":null,
        "Poster_reputation_count":49.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>One of the workaround you can follow to resolve the above issue;<\/p>\n<p>Based on this <a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/21390#issuecomment-1161782243\" rel=\"nofollow noreferrer\"><em><strong>GitHub issue<\/strong><\/em><\/a> as suggested by @<em>adba-msft<\/em> .<\/p>\n<blockquote>\n<p><strong>Please make sure that you have upgraded your azure cli to latest and<\/strong>\n<strong>Azure CLI ML extension v2 is being used.<\/strong><\/p>\n<\/blockquote>\n<p>To check and upgrade the cli we can use the below <code>cmdlts<\/code>:<\/p>\n<pre><code>az version\n\naz upgrade\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Uopde.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Uopde.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For more information please refer this similar <a href=\"https:\/\/stackoverflow.com\/questions\/73110661\/create-is-misspelled-or-not-recognized-by-the-system-on-az-ml-dataset-create\"><em><strong>SO THREAD|'create' is misspelled or not recognized by the system on az ml dataset create<\/strong><\/em><\/a> .<\/p>\n<p>I did observe the same issue after trying the aforementioned suggestion by @<em>Dor Lugasi-Gal<\/em> it works for me with (in my case <code>az ml -h<\/code>) after installed the extension with  <code>az extension add -n ml -y<\/code> can able to get the result of <code>az ml -h<\/code> without any error.<\/p>\n<p><em><strong>SCREENSHOT FOR REFERENCE:-<\/strong><\/em><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/39LHa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/39LHa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1659362947923,
        "Solution_link_count":6.0,
        "Solution_readability":12.5,
        "Solution_reading_time":21.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":161.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1518707555920,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":1664.0,
        "Answerer_view_count":560.0,
        "Challenge_adjusted_solved_time":9.0124052778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using nested parameters in my <code>parameters.yml<\/code> and would like to override these using runtime parameters for the <code>kedro run<\/code> CLI command:<\/p>\n<pre><code>train:\n    batch_size: 32\n    train_ratio: 0.9\n    epochs: 5\n<\/code><\/pre>\n<p>The following doesn't seem to work:<\/p>\n<pre><code>kedro run --params  train.batch_size:64,train.epochs:50 \n<\/code><\/pre>\n<p>the values for epoch and batch_size are those from the <code>parameters.yml<\/code>. How can I override these parameters with the cli command?<\/p>",
        "Challenge_closed_time":1596531353283,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596500312837,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63238607",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.2,
        "Challenge_reading_time":7.39,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":8.6223461111,
        "Challenge_title":"Override nested parameters using kedro run CLI command",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":549,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1362514672823,
        "Poster_location":"Vorarlberg, Austria",
        "Poster_reputation_count":1570.0,
        "Poster_view_count":159.0,
        "Solution_body":"<p>The additional parameters get passed into the <code>KedroContext<\/code> object via <code>load_context(Path.cwd(), env=env, extra_params=params)<\/code> in <code>kedro_cli.py<\/code>. Here you can see that there's a callback (protected) function called <code>_split_params<\/code> which splits the key-value pairs on <code>:<\/code>.<\/p>\n<p>This <code>_split_params<\/code> first splits string on commas (to get multiple params) and then on colons. Actually adding a print\/logging statement of what gets passed into <code>extra_params<\/code> will show you something like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>{'train.batch_size': 64, 'train.epochs': 50}\n<\/code><\/pre>\n<p>I think you have a couple options:<\/p>\n<ol>\n<li>Un-nesting the params. That way you will override them correctly.<\/li>\n<li>Adding custom logic to <code>_split_params<\/code> in <code>kedro_cli.py<\/code> to create a nested dictionary on <code>.<\/code> characters which gets passed into the func mentioned above. I think you can reuse a lot of the existing logic.<\/li>\n<\/ol>\n<p>NB: This was tested on <code>kedro==0.16.2<\/code>.<\/p>\n<p>NB2: The way <code>kedro<\/code> splits out nested params is using the <code>_get_feed_dict<\/code> and <code>_add_param_to_feed_dict<\/code> functions in <code>context.py<\/code>. Specifically, <code>_add_param_to_feed_dict<\/code> is a recursive function that unpacks a dictionary and formats as <code>&quot;{}.{}&quot;.format(key, value)<\/code>. IMO you can use the logic from here.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1596532757496,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":19.54,
        "Solution_score_count":3.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":171.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1473257955532,
        "Answerer_location":null,
        "Answerer_reputation_count":145.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":26.3318861111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have read similar questions regarding azure app service, but I still can't find an answer. I was trying to deploy a model in azure kubernetes service, but I came across an error when importing cv2 (which is essential to me).<\/p>\n<p>Opencv-python is included in my environment .yaml file:<\/p>\n<pre><code>name: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n  # You must list azureml-defaults as a pip dependency\n  - azureml-defaults&gt;=1.0.45\n  - Cython\n  - matplotlib&gt;=3.2.2\n  - numpy&gt;=1.18.5\n  - opencv-python&gt;=4.1.2\n  - pillow\n  - PyYAML&gt;=5.3\n  - scipy&gt;=1.4.1\n  - torch&gt;=1.6.0\n  - torchvision&gt;=0.7.0\n  - tqdm&gt;=4.41.0\nchannels:\n- conda-forge\n<\/code><\/pre>\n<p>I am deploying as follows:<\/p>\n<pre><code>aks_service = Model.deploy(ws,\n                       models=[model],\n                       inference_config=inference_config,\n                       deployment_config=gpu_aks_config,\n                       deployment_target=aks_target,\n                       name=aks_service_name)\n<\/code><\/pre>\n<p>And I get this error:<\/p>\n<pre><code>    Traceback (most recent call last):\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 129, in init_process\n    self.load_wsgi()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 138, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 52, in load\n    return self.load_wsgiapp()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 41, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/util.py&quot;, line 350, in import_app\n    __import__(module)\n  File &quot;\/var\/azureml-server\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import create_app\n  File &quot;\/var\/azureml-server\/create_app.py&quot;, line 3, in &lt;module&gt;\n    from app import main\n  File &quot;\/var\/azureml-server\/app.py&quot;, line 32, in &lt;module&gt;\n    from aml_blueprint import AMLBlueprint\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 23, in &lt;module&gt;\n    main_module_spec.loader.exec_module(main)\n  File &quot;\/var\/azureml-app\/score.py&quot;, line 8, in &lt;module&gt;\n    import cv2\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/cv2\/__init__.py&quot;, line 5, in &lt;module&gt;\n    from .cv2 import *\nImportError: libGL.so.1: cannot open shared object file: No such file or directory\nWorker exiting (pid: 41)\n<\/code><\/pre>",
        "Challenge_closed_time":1603897093470,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603802298680,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64554615",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.1,
        "Challenge_reading_time":41.93,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":26.3318861111,
        "Challenge_title":"Import cv2 error when deploying in Azure Kubernetes Service - python",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":484,
        "Challenge_word_count":243,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473257955532,
        "Poster_location":null,
        "Poster_reputation_count":145.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>This might go wrong at some point, but my workaround was installing opencv-python-headless instead of opencv.<\/p>\n<p>In the environment .yaml file, just replace:<\/p>\n<pre><code>- opencv-python&gt;=4.1.2\n<\/code><\/pre>\n<p>with:<\/p>\n<pre><code>- opencv-python-headless\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.0,
        "Solution_reading_time":3.72,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1327234712912,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":53015.0,
        "Answerer_view_count":3262.0,
        "Challenge_adjusted_solved_time":0.0405816667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Basically I'm receiving an output like this from my azure ws output:<\/p>\n\n<pre><code>{\n    'Results': {\n        'WSOutput': {\n            'type': 'table',\n            'value': {\n                'ColumnNames': ['ID', 'Start', 'Ask', 'Not', 'Passed', 'Suggest'],\n                'ColumnTypes': ['Int32', 'Int32', 'Int32', 'Double', 'Int64', 'Int32'],\n                'Values': [['13256025', '25000', '19000', '0.35', '1', '25000']]\n            }\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>The string, as you can see, has the info to create a datatable object. Now, I can't seem to find an easy way to cast it to an actual datatable POCO. I'm able to manually code a parser with Newtonsoft.Json.Linq but there has to be an easier way. <\/p>\n\n<p>Does anybody know how? I can't seem to find anything on the net.<\/p>",
        "Challenge_closed_time":1519929928907,
        "Challenge_comment_count":7,
        "Challenge_created_time":1519929782813,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1519930038023,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49056593",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":6.5,
        "Challenge_reading_time":9.56,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.0405816667,
        "Challenge_title":"Convert a datatable string from Azure ML WS to an actual Datatable C# Object?",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":83,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324654920387,
        "Poster_location":"Waterloo, ON, Canada",
        "Poster_reputation_count":5211.0,
        "Poster_view_count":449.0,
        "Solution_body":"<p>Yes, there is a open source online gernator on the net (<a href=\"http:\/\/jsonutils.com\/\" rel=\"nofollow noreferrer\">http:\/\/jsonutils.com\/<\/a>). Copy paste your result will give you that:<\/p>\n\n<pre><code> public class Value\n    {\n        public IList&lt;string&gt; ColumnNames { get; set; }\n        public IList&lt;string&gt; ColumnTypes { get; set; }\n        public IList&lt;IList&lt;string&gt;&gt; Values { get; set; }\n    }\n\n    public class WSOutput\n    {\n        public string type { get; set; }\n        public Value value { get; set; }\n    }\n\n    public class Results\n    {\n        public WSOutput WSOutput { get; set; }\n    }\n\n    public class Example\n    {\n        public Results Results { get; set; }\n    }\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":7.82,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1520413126203,
        "Answerer_location":null,
        "Answerer_reputation_count":37123.0,
        "Answerer_view_count":4058.0,
        "Challenge_adjusted_solved_time":47.4339141667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I trained a model in AZURE ML. Now i want to use that model in my ios app to predict the output\u00a0.<\/p>\n\n<p>How to download the model from AZURE and use it my swift code.<\/p>",
        "Challenge_closed_time":1525849618928,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525678856837,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1558224843256,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50209284",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.0,
        "Challenge_reading_time":2.66,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":47.4339141667,
        "Challenge_title":"How to use the trained model developed in AZURE ML",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":516,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1510206999776,
        "Poster_location":null,
        "Poster_reputation_count":81.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>As far as I know, the model could run in <strong>Azure Machine Learning Studio<\/strong>.It seems that you are unable to download it, the model could do nothing outside of Azure ML. <\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/41236871\/how-to-download-the-trained-models-from-azure-machine-studio\">Here<\/a> is a similar post for you to refer, I have also tried @Ahmet's \nmethod, but result is like @mrjrdnthms says.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1525850503192,
        "Solution_link_count":1.0,
        "Solution_readability":8.6,
        "Solution_reading_time":5.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":16.7675258334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently in the process of setting up a custom sagemaker container to run training jobs on Sagemaker and have succeeded in doing so. However, I am a bit confused over this question which is currently bugging me and is definitely something that I need to consider in the future<\/p>\n<ol>\n<li>Is it possible to run custom scripts on a custom container when declaring a sagemaker training job?<\/li>\n<\/ol>\n<p>My current understanding when it comes to creating a custom sagemaker image is that I create a train file that gets executed when running a training job, but I could never find documentation on whether is it possible to overwrite this and run a training script (but using the same custom container), like how we run training jobs using in-built algorithms. Is it the case that for custom algorithms we are restricted by this limitation?<\/p>",
        "Challenge_closed_time":1656147044190,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656086681097,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72746701",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.5,
        "Challenge_reading_time":11.47,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":16.7675258334,
        "Challenge_title":"Running custom scripts in a custom container while running a sagemaker training job",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":26,
        "Challenge_word_count":157,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644981356940,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I don't fully understand your question (can you make an example please?), specially when you say<\/p>\n<blockquote>\n<p>like how we run training jobs using in-built algorithms<\/p>\n<\/blockquote>\n<p>But basically you can do whatever you want in your container, as probably you already did, you have a proper <code>train<\/code> file in your container, which is the one that sagemaker calls as the entrypoint. In that file you can call external script (which are in your container too) and also pass parameters to your container (see how for example hyperparameters are passed). There is a quite clear documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/docker-containers-create.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":9.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":100.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":5.2579222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been exploring Azure ML Pipeline. I am referring to <a href=\"https:\/\/github.com\/MicrosoftLearning\/DP100\/blob\/master\/06A%20-%20Creating%20a%20Pipeline.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a> for the below code:<\/p>\n<p>Here is a small snippet from a MS Repo:<\/p>\n<pre><code>train_step = PythonScriptStep(name = &quot;Prepare Data&quot;,\nsource_directory = experiment_folder,\nscript_name = &quot;prep_diabetes.py&quot;,\narguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n'--prepped-data', prepped_data_folder],\noutputs=[prepped_data_folder],\ncompute_target = pipeline_cluster,\nrunconfig = pipeline_run_config,\nallow_reuse = True)\n<\/code><\/pre>\n<p>This suggests that while defining a pipeline, we must provide it a compute resource(pipeline_cluster). This obviously makes sense, since specific compute might be required for a specific step.<\/p>\n<p>But do we need to have this compute resource up and running always, so that whenever a pipeline is triggered, it can find the compute resource?<\/p>\n<p>Also, i figured we can probably keep a cluster with Zero minimum nodes, in which cases cluster is resized whenever pipeline is triggered. But i think there is a minimal cost incurrent in probably container registry regularly in such a setup. Is this the recommended way to deploy ML pipelines or some more efficient approach is possible?<\/p>",
        "Challenge_closed_time":1620823042460,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620804113940,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67498965",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":18.8,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":5.2579222222,
        "Challenge_title":"Pre-existing Compute Resource necessary for running a scheduled Azure ML pipeline?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":29,
        "Challenge_word_count":171,
        "Platform":"Stack Overflow",
        "Poster_created_time":1315165259620,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":339.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>Yep you're right -- create a <code>ComputeTarget<\/code> with a minimum of zero nodes. The <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/container-registry\/#pricing\" rel=\"nofollow noreferrer\">container registry costs<\/a> are ~$0.16 USD\/day and, IIRC, that cost is bundled in with Azure Machine learning.<\/p>\n<p>This is what our team does for our published pipelines in production.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.2,
        "Solution_reading_time":5.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1635045129020,
        "Answerer_location":null,
        "Answerer_reputation_count":53.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":250.6002630556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm dabbling with ML and was able to take a tutorial and get it to work for my needs.  It's a simple recommender system using TfidfVectorizer and linear_kernel.  I run into a problem with how I go about deploying it through Sagemaker with an end point.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel \nimport json\nimport csv\n\nwith open('data\/big_data.json') as json_file:\n    data = json.load(json_file)\n\nds = pd.DataFrame(data)\n\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(ds['content'])\ncosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n\nresults = {}\n\nfor idx, row in ds.iterrows():\n    similar_indices = cosine_similarities[idx].argsort()[:-100:-1]\n    similar_items = [(cosine_similarities[idx][i], ds['id'][i]) for i in similar_indices]\n\n    results[row['id']] = similar_items[1:]\n\ndef item(id):\n    return ds.loc[ds['id'] == id]['id'].tolist()[0]\n\ndef recommend(item_id, num):\n    print(&quot;Recommending &quot; + str(num) + &quot; products similar to &quot; + item(item_id) + &quot;...&quot;)\n    print(&quot;-------&quot;)\n    recs = results[item_id][:num]\n    for rec in recs:\n        print(&quot;Recommended: &quot; + item(rec[1]) + &quot; (score:&quot; + str(rec[0]) + &quot;)&quot;)\n\nrecommend(item_id='129035', num=5)\n<\/code><\/pre>\n<p>As a starting point I'm not sure if the output from <code>tf.fit_transform(ds['content'])<\/code> is considered the model or the output from <code>linear_kernel(tfidf_matrix, tfidf_matrix)<\/code>.<\/p>",
        "Challenge_closed_time":1636075476147,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635046349497,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1635173315200,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69693666",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.2,
        "Challenge_reading_time":21.77,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":285.8685138889,
        "Challenge_title":"How to Deploy ML Recommender System on AWS",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":63,
        "Challenge_word_count":164,
        "Platform":"Stack Overflow",
        "Poster_created_time":1635045129020,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>I came to the conclusion that I didn't need to deploy this through SageMaker.  Since the final linear_kernel output was a Dictionary I could do quick ID lookups to find correlations.<\/p>\n<p>I have it working on AWS with API Gateway\/Lambda, DynamoDB and an EC2 server to collect, process and plug the data into DynamoDB for fast lookups.  No expensive SageMaker endpoint needed.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":4.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1505653015243,
        "Answerer_location":null,
        "Answerer_reputation_count":1128.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":0.1140905556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to export a machine learning model I created in Azure Machine Learning studio. One of the required input is \"Path to blob beginning with container\"<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/xvSDd.png\" alt=\"Here is the screenshoot from azure export\"><\/p>\n\n<p>How do I find this path? I have already created a blob storage but I have no idea how to find the path to the blob storage. <\/p>",
        "Challenge_closed_time":1538035308943,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538034898217,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1538038508827,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52532078",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":5.24,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1140905556,
        "Challenge_title":"How to find the path to blob?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":15096,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>you should be able to find this from the Azure portal. Open the storage account, drill down into blobs, then your container. Use properties for the context menu, the URL should be the path ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":4.7,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1379362192207,
        "Answerer_location":"Embrach, Schweiz",
        "Answerer_reputation_count":181.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":0.6535952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have to do a sales prediction and I'm evaluation using a ML.NET solution hosted in a virtual machine(in Azure) vs using Azure ML Studio. The data may change once or twice per month. Which solutions should I choose? Also, for my use case, pricing might be a factor.\nThank you. <\/p>",
        "Challenge_closed_time":1575557093136,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575554740193,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1575622440107,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59196919",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.6,
        "Challenge_reading_time":3.76,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.6535952778,
        "Challenge_title":"ML.NET vs Azure ML Studio",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":510,
        "Challenge_word_count":56,
        "Platform":"Stack Overflow",
        "Poster_created_time":1406013299956,
        "Poster_location":null,
        "Poster_reputation_count":450.0,
        "Poster_view_count":105.0,
        "Solution_body":"<p>In short:\nIf you are building a .NET application and want to integrate ML, use ML.NET.\nIf you don't do .NET, use Azure ML.\nDocs are helpful here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/data-guide\/technology-choices\/data-science-and-machine-learning\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/data-guide\/technology-choices\/data-science-and-machine-learning<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.2,
        "Solution_reading_time":5.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1651331397896,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":43.4867702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I need to setup a shared cache in minikube in such a way that different services can use that cache to pull and update DVC models and data needed for training Machine Learning models. The structure of the project is to use 1 pod to periodically update the cache with new models and outputs. Then, multiple pods can read the cache to recreate the updated models and data. So I need to be able to update the local cache directory and pull from it using DVC commands, so that all the services have consistent view on the latest models and data created by a service.<\/p>\n<p>More specifically, I have a docker image called <code>inference-service<\/code> that should only <code>dvc pull<\/code> or some how use the info in the shared dvc cache to get the latest model and data locally in <code>models<\/code> and <code>data<\/code> folders (see dockerfile) in minikube. I have another image called <code>test-service<\/code> that\nruns the ML pipeline using <code>dvc repro<\/code> which creates the models and data that DVC needs (dvc.yaml) to track and store in the shared cache. So <code>test-service<\/code> should push created outputs from the ML pipeline into the shared cache so that <code>inference-service<\/code> can pull it and use it instead of running dvc repro by itself. <code>test-service<\/code> should only re-train and write the updated models and data into the shared cache while <code>inference-service<\/code> should only read and recreate the updated\/latest models and data from the shared cache.<\/p>\n<p><em><strong>Problem: the cache does get mounted on the minikube VM, but the inference service does not pull (using <code>dvc pull -f<\/code>) the data and models after the test service is done with <code>dvc repro<\/code> and results the following warnings and failures:<\/strong><\/em><\/p>\n<p><em>relevant kubernetes pod log of inference-service<\/em><\/p>\n<pre><code>WARNING: Output 'data\/processed\/train_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/train_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/validation_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/validation_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/test_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/test_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/interim\/train_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/train_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/validation_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/validation_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/test_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/test_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'models\/mlb.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/tfidf_vectorizer.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/model.pkl'(stage: 'train') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'reports\/scores.json'(stage: 'evaluate') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: No file hash info found for '\/root\/models\/model.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/reports\/scores.json'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/train_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/validation_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/test_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/train_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/validation_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/test_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/mlb.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/tfidf_vectorizer.pkl'. It won't be created.\n10 files failed\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\n\/root\/models\/model.pkl\n\/root\/reports\/scores.json\n\/root\/data\/processed\/train_preprocessed.pkl\n\/root\/data\/processed\/validation_preprocessed.pkl\n\/root\/data\/processed\/test_preprocessed.pkl\n\/root\/data\/interim\/train_featurized.pkl\n\/root\/data\/interim\/validation_featurized.pkl\n\/root\/data\/interim\/test_featurized.pkl\n\/root\/models\/mlb.pkl\n\/root\/models\/tfidf_vectorizer.pkl\nIs your cache up to date?\n<\/code><\/pre>\n<p><em>relevant kubernetes pod log of test-service<\/em><\/p>\n<pre><code>Stage 'preprocess' is cached - skipping run, checking out outputs\nGenerating lock file 'dvc.lock'\nUpdating lock file 'dvc.lock'\nStage 'featurize' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'train' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'evaluate' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nUse `dvc push` to send your updates to remote storage.\n<\/code><\/pre>\n<p><strong>Project Tree<\/strong><\/p>\n<pre><code>\u251c\u2500 .dvc\n\u2502  \u251c\u2500 .gitignore\n\u2502  \u251c\u2500 config\n\u2502  \u2514\u2500 tmp\n\u251c\u2500 deployment\n\u2502  \u251c\u2500 docker-compose\n\u2502  \u2502  \u251c\u2500 docker-compose.yml\n\u2502  \u251c\u2500 minikube-dep\n\u2502  \u2502  \u251c\u2500 inference-test-services_dep.yaml\n\u2502  \u251c\u2500 startup_minikube_with_mount.sh.sh\n\u251c\u2500 Dockerfile # for inference service\n\u251c\u2500 dvc-cache # services should push and pull from this cache folder and see this as the DVC repo\n\u251c- dvc.yaml\n\u251c- params.yaml\n\u251c\u2500 src\n\u2502  \u251c\u2500 build_features.py\n|  \u251c\u2500 preprocess_data.py\n|  \u251c\u2500 serve_model.py\n|  \u251c\u2500 startup.sh  \n|  \u251c\u2500 requirements.txt\n\u251c\u2500 test_dep\n\u2502  \u251c\u2500 .dvc # same as .dvc in the root folder\n|  |  \u251c\u2500...\n\u2502  \u251c\u2500 Dockerfile # for test service\n\u2502  \u251c\u2500 dvc.yaml\n|  \u251c\u2500 params.yaml\n\u2502  \u2514\u2500 src\n\u2502     \u251c\u2500 build_features.py # same as root src folder\n|     \u251c\u2500 preprocess_data.py # same as root src folder\n|     \u251c\u2500 serve_model.py # same as root src folder\n|     \u251c\u2500 startup_test.sh  \n|     \u251c\u2500 requirements.txt  # same as root src folder\n<\/code><\/pre>\n<p><strong>dvc.yaml<\/strong><\/p>\n<pre><code>stages:\n  preprocess:\n    cmd: python ${preprocess.script}\n    params:\n      - preprocess\n    deps:\n      - ${preprocess.script}\n      - ${preprocess.input_train}\n      - ${preprocess.input_val}\n      - ${preprocess.input_test}\n    outs:\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n  featurize:\n    cmd: python ${featurize.script}\n    params:\n      - preprocess\n      - featurize\n    deps:\n      - ${featurize.script}\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n    outs:\n      - ${featurize.output_train}\n      - ${featurize.output_val}\n      - ${featurize.output_test}\n      - ${featurize.mlb_out}\n      - ${featurize.tfidf_vectorizer_out}\n  train:\n    cmd: python ${train.script}\n    params:\n      - featurize\n      - train\n    deps:\n      - ${train.script}\n      - ${featurize.output_train}\n    outs:\n      - ${train.model_out}\n  evaluate:\n    cmd: python ${evaluate.script}\n    params:\n      - featurize\n      - train\n      - evaluate\n    deps:\n      - ${evaluate.script}\n      - ${train.model_out}\n      - ${featurize.output_val}\n    metrics:\n      - ${evaluate.scores_path}\n<\/code><\/pre>\n<p><strong>params.yaml<\/strong><\/p>\n<pre><code>preprocess:\n  script: src\/preprocess\/preprocess_data.py\n  input_train: data\/raw\/train.tsv\n  input_val: data\/raw\/validation.tsv\n  input_test: data\/raw\/test.tsv\n  output_train: data\/processed\/train_preprocessed.pkl\n  output_val: data\/processed\/validation_preprocessed.pkl\n  output_test: data\/processed\/test_preprocessed.pkl\n\nfeaturize:\n  script: src\/features\/build_features.py\n  output_train: data\/interim\/train_featurized.pkl\n  output_val: data\/interim\/validation_featurized.pkl\n  output_test: data\/interim\/test_featurized.pkl\n  mlb_out: models\/mlb.pkl\n  tfidf_vectorizer_out: models\/tfidf_vectorizer.pkl\n\ntrain:\n  script: src\/models\/train_model.py\n  model_out: models\/model.pkl\n\nevaluate:\n  script: src\/models\/evaluate_model.py\n  scores_path: reports\/scores.json\n  roc_json: reports\/roc_plot.json\n  prc_json: reports\/prc_plot.json\n<\/code><\/pre>",
        "Challenge_closed_time":1654875420463,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654718868090,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1655157056467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72551630",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":119.44,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":135,
        "Challenge_solved_time":43.4867702778,
        "Challenge_title":"How to setup a DVC shared cache without git repository between different services in minikube?",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":196,
        "Challenge_word_count":1051,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562403039336,
        "Poster_location":null,
        "Poster_reputation_count":298.0,
        "Poster_view_count":47.0,
        "Solution_body":"<p>After running <code>dvc repro<\/code> in <code>test-service<\/code>, a new <code>dvc.lock<\/code> will be created, containing the file hashes relative to your pipeline (i.e. the hash for <code>models\/model.pkl<\/code> etc).<\/p>\n<p>If you're running a shared cache, <code>inference-service<\/code> should have access to the updated <code>dvc.lock<\/code>. If that is present, it will be sufficient to run <code>dvc checkout<\/code> to populate the workspace with the files corresponding to the hashes in the shared cache.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":6.67,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":67.0,
        "Tool":"DVC"
    }
]