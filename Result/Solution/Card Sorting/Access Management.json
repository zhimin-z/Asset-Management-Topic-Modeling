[
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":13.3927341667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am getting the following error when I try to convert a datetime variable to date.<\/p>\n\n<p><strong>My Code<\/strong><\/p>\n\n<pre><code>import datetime as dt \n\ndf['TXN_DATE_2'] = df['TXN_DATE'].dt.date\n<\/code><\/pre>\n\n<p><strong>Error<\/strong><\/p>\n\n<blockquote>\n  <p>raise NotImplementedError('Python Bridge conversion table not\n  implemented for type [{0}]'.format(value.getType()))\n  NotImplementedError: Python Bridge conversion table not implemented\n  for type [] Process returned with non-zero exit\n  code 1<\/p>\n<\/blockquote>\n\n<p>Can anyone please tell me what is going on.<\/p>",
        "Challenge_closed_time":1499329512083,
        "Challenge_comment_count":0,
        "Challenge_created_time":1499274119590,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to convert a datetime variable to date using Python script in Azure ML. The error message states that the Python Bridge conversion table is not implemented for the given type.",
        "Challenge_last_edit_time":1499281298240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44932098",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":15.3868036111,
        "Challenge_title":"Azure ML- Execute Python Script -Datatime.date not working",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":449.0,
        "Challenge_word_count":70,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1499273894443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Please try to use the code below to convert as you want.<\/p>\n\n<pre><code>import pandas as pd\nimport datetime as dt\ndf['TXN_DATE_2'] = pd.to_datetime(df['TXN_DATE']).dt.date\n<\/code><\/pre>\n\n<p>Hope it helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":2.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.2444444444,
        "Challenge_answer_count":0,
        "Challenge_body":"## Expected Behavior\r\nIt seems new version on AzureML extension to VS Code doesn't have this option in settings. I needed to downgrade to 0.6x.\r\n\r\n## Actual Behavior\r\nCurrent version 0.10.0 doesn't have the option. Cannot locally debug or documentation doesn't provide info about that.\r\n\r\n## Specifications\r\n\r\n  - Version: 0.10.0\r\n  - Platform: VS Code, Windows\r\n",
        "Challenge_closed_time":1654701310000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1654678830000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a reoccurring error message when opening a remote connection to Azure Machine Learning Compute Instance. The error message is related to a failed request to a specific URL and is causing annoyance to the user. The error type is REQUEST_SEND_ERROR and the user is running Visual Studio Code version 1.66.1 on a Linux OS.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1589",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":5.42,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":100.0,
        "Challenge_repo_issue_count":2059.0,
        "Challenge_repo_star_count":290.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":6.2444444444,
        "Challenge_title":"Run and debug experiments locally - azureML.CLI Compatibility Mode for CLI v1 - cannot find",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":63,
        "Discussion_body":"@michalmar We have completely deprecated the v1 CLI Compatibility mode settings from v0.8.0 onwards and v2 mode will be the way going forward :).",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":31.4527527778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi, I'm wondering if I can register azure cosmos db as a datastore in azure machine learning?     <br \/>\nFrom your documentation, it seems not <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore%28class%29?view=azure-ml-py\">https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore%28class%29?view=azure-ml-py<\/a>    <\/p>\n<ul>\n<li> Do you have a plan to implement the feature in near future?     <\/li>\n<li> Any recommended alternative solutions for now?     <\/li>\n<\/ul>\n<p>Thanks.    <\/p>",
        "Challenge_closed_time":1597444998180,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597331768270,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring if Azure Cosmos DB can be registered as a datastore in Azure Machine Learning, but it seems that it is not currently supported based on the documentation. The user is asking if there are any plans to implement this feature in the future and if there are any recommended alternative solutions for now.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/66297\/azure-cosmos-db-as-a-datastore-in-ml",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.7,
        "Challenge_reading_time":7.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":31.4527527778,
        "Challenge_title":"azure cosmos db as a datastore in ml",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. Currently, Cosmos DB isn't a supported datasource when using <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#supported-data-storage-service-types\">Azure ML datastores<\/a>. However, the product team are aware of this request and will provide updates accordingly. An alternative for now will be to use <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/import-from-cosmos-db\">Azure ML Studio (Classic)<\/a> which supports Cosmos DB as data source. You can also try a heuristic approach via <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/execute-python-script\">Execute Python Script module<\/a> in Designer to import data using <a href=\"https:\/\/stackoverflow.com\/questions\/44249604\/how-to-read-data-from-azures-cosmosdb-in-python\">python<\/a>. Hope this helps.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":16.1,
        "Solution_reading_time":12.3,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":77.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":83.6952572222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am getting this error on a Linux box (Gentoo w\/ .NET via Mono properly installed)<\/p>\n<p><strong>&quot;Unable to retrieve .NET dependencies. Please make sure you are connected to the Internet and have a stable network connection.&quot;<\/strong><\/p>\n<p>The error is triggered when creating a dataset from a directory using<\/p>\n<p>&quot;dataset = Dataset.File.from_files(path=(datastore, path_to_dataset_in_datastore))&quot;<\/p>\n<p>Some system info:  <br \/>\nPython: 3.8.8.  <br \/>\nazureml-automl-core 1.26.0  <br \/>\nazureml-core 1.26.0  <br \/>\nazureml-dataprep 2.13.2  <br \/>\nazureml-dataprep-native 32.0.0  <br \/>\nazureml-dataprep-rslex 1.11.2  <br \/>\nazureml-dataset-runtime 1.26.0  <br \/>\nazureml-pipeline 1.26.0  <br \/>\nazureml-pipeline-core 1.26.0  <br \/>\nazureml-pipeline-steps 1.26.0  <br \/>\nazureml-sdk 1.26.0  <br \/>\nazureml-telemetry 1.26.0  <br \/>\nazureml-train 1.26.0  <br \/>\nazureml-train-automl-client 1.26.0  <br \/>\nazureml-train-core 1.26.0  <br \/>\nazureml-train-restclients-hyperdrive 1.26.0<\/p>\n<p>.NET Info:  <br \/>\nMono JIT compiler version 6.6.0.161 (tarball Sat Apr 10 16:41:12 PDT 2021)  <br \/>\nCopyright (C) 2002-2014 Novell, Inc, Xamarin Inc and Contributors. <a href=\"https:\/\/www.mono-project.com\">www.mono-project.com<\/a>  <br \/>\nTLS: __thread  <br \/>\nSIGSEGV: altstack  <br \/>\nNotifications: epoll  <br \/>\nArchitecture: amd64  <br \/>\nDisabled: none  <br \/>\nMisc: softdebug  <br \/>\nInterpreter: yes  <br \/>\nLLVM: supported, not enabled.  <br \/>\nSuspend: hybrid  <br \/>\nGC: sgen (concurrent by default)<\/p>",
        "Challenge_closed_time":1619068615363,
        "Challenge_comment_count":2,
        "Challenge_created_time":1618767312437,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error on a Linux box while creating a dataset from a directory using AzureML. The error message states that it is unable to retrieve .NET dependencies and suggests checking the internet connection and network stability. The system information and .NET info are also provided.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/361522\/azureml-error-on-linux-unable-to-retrieve-net-depe",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":7.2,
        "Challenge_reading_time":20.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":83.6952572222,
        "Challenge_title":"AzureML Error on Linux: \"Unable to retrieve .NET dependencies. Please make sure you are connected ...\"",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":179,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=48487b4e-cd4d-4b5e-aae5-dcf8dfaf50c4\">@Victor Fragoso  <\/a>  Thanks for the details. Gentoo is not a 'natively' supported distribution of linux for Datasets. The Exception message doesn't link to a .NET docs page with instructions on installing the system dependencies required for .NET to work. Though it seems a different one is being thrown related to not being able to connect to out blob storage which has pre-prepared dependency sets for some linux distros (not gentoo).    <\/p>\n<p>This page <a href=\"https:\/\/learn.microsoft.com\/en-us\/dotnet\/core\/install\/linux\">Install .NET on Linux Distributions | Microsoft Learn<\/a> does not detail support for .NET on gentoo.    <br \/>\nYou can get the names of the missing dependencies themselves by running:    <\/p>\n<pre><code>from dotnetcore2 import runtime  \nruntime._enable_debug_logging()  \nruntime.ensure_dependencies()  \n<\/code><\/pre>\n<p>This code snippet should print the libraies missing required by .NET core 2.1.    <br \/>\nIf the above does not print anything, other than the Exception, then instead this should:    <\/p>\n<pre><code>from dotnetcore2 import runtime  \nprint(runtime._gather_dependencies(runtime._get_bin_folder()))  \n<\/code><\/pre>\n",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":15.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":148.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":93.5734752778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Azure Machine learning Studio, I have imported a dataset from a locally stored spreadsheet. In the designer, I drag the dataset into the workspace, right click, and select 'Visualize. I get the following error:   <\/p>\n<p>&quot;Unable to visualize this dataset. This might be because your data is stored behind a virtual network or your data does not support profile&quot;. I've searched for hours for a remedy, but find nothing.   <\/p>\n<p>What do I do to fix this error?<\/p>",
        "Challenge_closed_time":1603128837008,
        "Challenge_comment_count":5,
        "Challenge_created_time":1602791972497,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to visualize a dataset in Microsoft Azure Machine Learning Studio. The error message states that the dataset cannot be visualized, possibly due to the data being stored behind a virtual network or not supporting profile. The user is seeking a solution to fix this error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/127980\/error-when-visualizing-dataset-in-microsoft-azure",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":93.5734752778,
        "Challenge_title":"Error when Visualizing Dataset in Microsoft Azure Machine Learning Studio",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":88,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=ab00ff52-eb99-4909-97c6-13620f09e957\">@Dana Shields  <\/a> I have tried this scenario with my workspace and i was able to replicate the message you have seen. It looks like you are using the Dataset type as File while creating the dataset which is causing the issue. Please register the dataset as Tabular type and then use the dataset in designer. This should show you the preview of the data. Here is a screen shot from my workspace of the designer.    <\/p>\n<p><img src=\"\/answers\/storage\/temp\/33346-image.png\" alt=\"33346-image.png\" \/>    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":7.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":107.1936713889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<br>\nsometimes I meet with this kind of error, when I run on a remote device:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/3\/3210bee6d41f470899393ebd804a54d0b8eacb85.png\" data-download-href=\"\/uploads\/short-url\/78TKyZQzgGPLZDlHXk14ZJlglYV.png?dl=1\" title=\"Screenshot from 2023-04-23 08-40-26\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/3\/3210bee6d41f470899393ebd804a54d0b8eacb85_2_690x93.png\" alt=\"Screenshot from 2023-04-23 08-40-26\" data-base62-sha1=\"78TKyZQzgGPLZDlHXk14ZJlglYV\" width=\"690\" height=\"93\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/3\/3210bee6d41f470899393ebd804a54d0b8eacb85_2_690x93.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/3\/3210bee6d41f470899393ebd804a54d0b8eacb85_2_1035x139.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/3\/3210bee6d41f470899393ebd804a54d0b8eacb85_2_1380x186.png 2x\" data-dominant-color=\"2A2319\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2023-04-23 08-40-26<\/span><span class=\"informations\">1482\u00d7200 40.2 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\n<code>Error: Run xxx errored: AssertionError()<\/code><br>\nDubugging tool in vscode cannot locate it. It just pops up and tells you the run is failed. I can find which line is the reason of this error, but have no idea what happens.<br>\nAfter checking for much time, I can do nothing but disconnect with remote device and connect again. And the problem is gone\u2026 Is there a more specific reason?<br>\nThanks a lot!<br>\nJialei Li<\/p>",
        "Challenge_closed_time":1682618207071,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682232309854,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an assertion error while running on a remote device, which causes the run to fail. The debugging tool in vscode cannot locate the error, and the user is unsure of the reason behind it. The issue is resolved by disconnecting and reconnecting to the remote device, but the user is seeking a more specific reason for the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-assertion-errror\/4260",
        "Challenge_link_count":5,
        "Challenge_participation_count":4,
        "Challenge_readability":16.1,
        "Challenge_reading_time":26.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":107.1936713889,
        "Challenge_title":"Wandb assertion errror",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":55.0,
        "Challenge_word_count":139,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,<br>\nI found the reason now. It is relative with other part of my project\u2019s program. It has nothing to do with wandb actually\u2026 Still thanks a lot for your willingness to check error for me <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.8,
        "Solution_reading_time":4.77,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":44.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0900975,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I created a batch endpoint, deployment and now creating deployment job in Machine Learning Workspace. I am providing the input data via datastore to read data from an external data storage. I followed the following tutorial <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data-batch-endpoints-jobs?view=azureml-api-2&amp;tabs=sdk\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data-batch-endpoints-jobs?view=azureml-api-2&amp;tabs=sdk<\/a> and under the title <strong>Security considerations when reading data<\/strong> within the table, in the second row, identity of the job should be enough. But I am still getting authorization error when I try to read the data. Is it because I want to read from an external storage account, meaning that compute cluster should be authenticated by the external storage account as well? <\/p>\n<p>Thank you for your time and help.<\/p>",
        "Challenge_closed_time":1684230604864,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684230280513,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an authorization error while trying to read data from an external storage account using a batch endpoint in Machine Learning Workspace. They followed a tutorial and provided input data via datastore, but are still facing issues. They are questioning if the compute cluster needs to be authenticated by the external storage account as well.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1285828\/ml-batch-endpoint-using-datastore-to-access-an-ext",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":13.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.0900975,
        "Challenge_title":"ML Batch Endpoint, using datastore to access an external data storage",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":120,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Yes, you are correct. In order to read data from an external storage account, the compute cluster must be authenticated by the external storage account as well. This is because the compute cluster needs to have permission to access the data in the external storage account.<\/p>\n<p>There are a few ways to authenticate the compute cluster with the external storage account. One way is to use a service principal. A service principal is an identity that can be used to access Azure resources. To create a service principal, you can use the Azure portal or the Azure CLI.<\/p>\n<p>Once you have created a service principal, you need to grant it access to the external storage account. You can do this by assigning the service principal a role in the external storage account. The role that you assign will determine what level of access the service principal has to the external storage account.<\/p>\n<p>Once you have granted the service principal access to the external storage account, you need to configure the compute cluster to use the service principal. You can do this by setting the <code>AZURE_STORAGE_ACCOUNT_CONNECTION_STRING<\/code> environment variable on the compute cluster. The value of this environment variable should be the connection string for the external storage account.<\/p>\n<p>Once you have configured the compute cluster to use the service principal, you should be able to read data from the external storage account. If you are still getting an authorization error, you can try the following:<\/p>\n<ul>\n<li> Make sure that the service principal has the correct permissions to access the external storage account.<\/li>\n<li> Make sure that the <code>AZURE_STORAGE_ACCOUNT_CONNECTION_STRING<\/code> environment variable is set correctly on the compute cluster.<\/li>\n<li> Restart the compute cluster.<\/li>\n<\/ul>\n<p>If you are still having trouble, you can contact Azure support for help.<\/p>\n<p>Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/><\/p>\n<p>and upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/><\/p>\n<p>button if you find this helpful.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.4,
        "Solution_reading_time":28.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":318.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.7363491667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>I am trying to figure out the folder structure of Azure ML workspace in my storage account.  <br \/>\nI want to be able to delete old pipeline runs and experiments that have piled up in my workspace directly from Azure Storage Explorer without breaking the system.  <br \/>\nMy datastores and folder structure are as follows:  <\/p>\n<p>Datastore: workspaceartifactstore  <br \/>\nBlob container: azureml  <br \/>\nFolder structure:  <br \/>\n\u251c\u2500 ComputeRecord  <br \/>\n\u251c\u2500 Dataset  <br \/>\n\u251c\u2500 ExperimentRun  <br \/>\n\u251c\u2500 LocalUpload  <\/p>\n<p>Datastore: workspaceblobstore (Default)  <br \/>\nBlob container: azureml-blobstore-<em>(a series of numbers)<\/em>  <br \/>\nFolder structure:  <br \/>\n\u251c\u2500 azureml  <br \/>\n\u2502   \u251c\u2500\u2500 <em>(a series of numbers)<\/em>-setup  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 _tracer.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 azureml_globals.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 context_managers.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 job_prep.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 log_history_status.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 request_utilities.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 run_token_provider.py  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 utility_context_managers.py  <br \/>\n\u2502   \u251c\u2500\u2500 <em>(another series of numbers)<\/em>-setup  <br \/>\n\u2502    \u2502     \u251c\u2500\u2500 <em>sames files as above<\/em>  <\/p>\n<p>It would help if I understood what does each of these containers actually store.  <br \/>\nI already tried to delete all blobs stored in 'workspaceblobstore', but it didn't remove any pipeline or experiment from ML Studio.  <br \/>\nI have a few datasets registered in my workspace, and I don't want to delete them (nor unregister them).  <\/p>\n<p>Can I set a data retention policy on both containers in order to delete old blobs?  <br \/>\nCan I safely delete the blobs (folders) stored in 'workspaceartifactstore' too? Will they be recreated automatically when I run a new experiment?  <br \/>\nWhy are there two separate 'azureml' and 'azureml-blobstore-<em>(a series of numbers)<\/em>' containers? Is it possible to merge them?  <\/p>\n<p>Thanks.  <\/p>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1647528776467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647500925610,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to understand the folder structure of Azure ML workspace in their storage account and wants to delete old pipeline runs and experiments without breaking the system. They have two datastores with different blob containers and folder structures and are unsure of what each container stores. The user has tried deleting all blobs in one container but it did not remove any pipeline or experiment from ML Studio. They are also unsure if they can safely delete the blobs stored in 'workspaceartifactstore' and if they can set a data retention policy on both containers. The user is also curious about the purpose of the two separate 'azureml' and 'azureml-blobstore-(a series of numbers)' containers and if they can be merged.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/775834\/azure-ml-workspace-blob-structure-can-i-safely-del",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":23.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":7.7363491667,
        "Challenge_title":"Azure ML workspace blob structure \/ Can I safely delete these blobs?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":253,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. I've worked on a <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/60501\">similar inquiry<\/a> and the advise is to not delete data stored in default datastore to avoid weird errors. The option to easily delete experiment runs is on the roadmap. Here's a similar <a href=\"https:\/\/stackoverflow.com\/questions\/57497332\/how-to-delete-an-experiment-from-an-azure-machine-learning-workspace\">thread<\/a>. Feel free to raise and track feature request on <a href=\"https:\/\/feedback.azure.com\/d365community\/forum\/b9a0c624-ad25-ec11-b6e6-000d3a4f09d0\">ideas portal<\/a>.    <\/p>\n<blockquote>\n<p>According to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#prerequisites\">documentation<\/a>, when you create a workspace, an Azure blob container and an Azure file share are automatically registered as datastores to the workspace. They're named workspaceblobstore and workspacefilestore, respectively. The workspaceblobstore is used to store workspace artifacts and your machine learning experiment logs. It's also set as the default datastore and can't be deleted from the workspace. The workspacefilestore is used to store notebooks and R scripts authorized via compute instance.    <\/p>\n<\/blockquote>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.2,
        "Solution_reading_time":16.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.7161111111,
        "Challenge_answer_count":0,
        "Challenge_body":"`Should we install dvc[https:\/\/dvc.org\/] (`pip install dvc <3`) for you right now?`\r\nhttps:\/\/github.com\/DAGsHub\/fds\/blob\/6e93c2b3259a7601f392c09604a60fc0ff360ad8\/fds\/run.py#L27",
        "Challenge_closed_time":1621785253000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621771875000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the load_dataset function from Hugging Face as it is unable to access the DVC tracked data directory due to a data loading bug. The error message displayed is \"OSError: [Errno 30] Read-only file system: '\/data'\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/13",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":8.4,
        "Challenge_reading_time":3.13,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":145.0,
        "Challenge_repo_star_count":369.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.7161111111,
        "Challenge_title":"Markdown in dvc install prompt isn't rendered as markdown",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":21,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1367453125248,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":2284.4218233333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>During a training script executed on a compute target, we're trying to download a registered Dataset from an ADLS2 Datastore. The problem is that it takes <strong>hours<\/strong> to download ~1.5Gb (splitted into ~8500 files) to the compute target with the following method : <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Datastore, Dataset, Run, Workspace\n\n# Retrieve the run context to get Workspace\nRUN = Run.get_context(allow_offline=True)\n\n# Retrieve the workspace\nws = RUN.experiment.workspace\n\n# Creating the Dataset object based on a registered Dataset\ndataset = Dataset.get_by_name(ws, name='my_dataset_registered')\n\n# Download the Dataset locally\ndataset.download(target_path='\/tmp\/data', overwrite=False)\n<\/code><\/pre>\n\n<p><strong>Important note :<\/strong> the Dataset is registered to a path in the Datalake that contains a lot of subfolders (as well subsubfolders, ..) containing small files of around 170Kb.<\/p>\n\n<p><strong>Note:<\/strong> I'm able to download the complete dataset to local computer within a few minutes using <code>az copy<\/code> or the Storage Explorer. Also, the Dataset is defined at a folder stage with the ** wildcard for scanning subfolders : <code>datalake\/relative\/path\/to\/folder\/**<\/code><\/p>\n\n<p>Is that a known issue ? How can I improve transfer speed ?<\/p>\n\n<p>Thanks !<\/p>",
        "Challenge_closed_time":1583895127880,
        "Challenge_comment_count":1,
        "Challenge_created_time":1583493276803,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge while downloading a registered dataset from an ADLS2 Datastore to a compute target during a training script execution. The download process takes hours to complete, even though the dataset is only 1.5GB in size and is split into 8500 files. The dataset is registered to a path in the Datalake that contains many subfolders, each containing small files of around 170KB. The user is seeking advice on how to improve the transfer speed.",
        "Challenge_last_edit_time":1583526482776,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60562966",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.7,
        "Challenge_reading_time":18.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":111.6252991667,
        "Challenge_title":"Transfer from ADLS2 to Compute Target very slow Azure Machine Learning",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":593.0,
        "Challenge_word_count":175,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1447320137140,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":313.0,
        "Poster_view_count":40.0,
        "Solution_body":"<p><em>Edited to be more answer-like:<\/em><\/p>\n\n<p>It'd be helpful to include: what versions of azureml-core and azureml-dataprep SDK you are using, what type of VM you are running as the compute instance, and what types of files (e.g. jpg? txt?) your dataset is using. Also, what are you trying to achieve by downloading the complete dataset to your compute?<\/p>\n\n<p>Currently, compute instance image comes with azureml-core 1.0.83 and azureml-dataprep 1.1.35 pre-installed, which are 1-2 months old. You might be using even older versions. You can try upgrading by running in your notebook:<\/p>\n\n<pre><code>%pip install -U azureml-sdk\n<\/code><\/pre>\n\n<p>If you don't see any improvements to your scenario, you can file an issue on the official docs page to get someone to help debug your issue, such as the ref page for <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.file_dataset.filedataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">FileDataset<\/a>.<\/p>\n\n<p><em>(edited on June 9, 2020 to remove mention of experimental release because that is not happening anymore)<\/em><\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1591750401340,
        "Solution_link_count":1.0,
        "Solution_readability":9.1,
        "Solution_reading_time":14.06,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":150.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1475181309096,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Brazil",
        "Answerer_reputation_count":4242.0,
        "Answerer_view_count":421.0,
        "Challenge_adjusted_solved_time":19.7379844444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am creating my own Docker image so that I can use my own models in AWS SageMaker. I sucessfully created a Docker image using command line inside the Jupyter Notebook in SageMaker ml.t2.medium instance using a customized Dockerfile:<\/p>\n\n<pre><code>REPOSITORY            TAG                 IMAGE ID            CREATED             SIZE\nsklearn               latest              01234212345        6 minutes ago       1.23GB\n<\/code><\/pre>\n\n<p>But when I run in Jupyter:<\/p>\n\n<pre><code>! aws ecr create-repository --repository-name sklearn\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>An error occurred (AccessDeniedException) when calling the CreateRepository operation: User: arn:aws:sts::1234567:assumed-role\/AmazonSageMaker-ExecutionRole-12345\/SageMaker is not authorized to perform: ecr:CreateRepository on resource: *\n<\/code><\/pre>\n\n<p>I already set up SageMaker, EC2, EC2ContainerService permissions and the following policy for EC2Container but I still get the same error.<\/p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"sagemaker:*\",\n        \"ec2:*\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n<\/code><\/pre>\n\n<p>Any idea on how I can solve this issue?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1528126261856,
        "Challenge_comment_count":3,
        "Challenge_created_time":1528052776997,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an \"AccessDeniedException\" error when trying to create a repository using AWS ECR in SageMaker. The user has already set up permissions for SageMaker, EC2, and EC2ContainerService, but is still unable to perform the action. The user is seeking advice on how to resolve the issue.",
        "Challenge_last_edit_time":1528055205112,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50669991",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":13.9,
        "Challenge_reading_time":15.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":20.4124608333,
        "Challenge_title":"AWS SageMaker is not authorized to perform: ecr:CreateRepository on resource: *",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":4509.0,
        "Challenge_word_count":145,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1475181309096,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brazil",
        "Poster_reputation_count":4242.0,
        "Poster_view_count":421.0,
        "Solution_body":"<p>I solved the problem. We must set a permission at SageMaker Execution Role as following:<\/p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"ecr:*\"            ],\n        \"Resource\": \"*\"\n    }\n]}\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":2.88,
        "Solution_score_count":5.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.3700394444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Having read Erland's article     <\/p>\n<p><a href=\"https:\/\/www.sqlservergeeks.com\/a-tip-about-using-python-for-regular-expressions-in-t-sql-by-erland-sommarskog\/\">https:\/\/www.sqlservergeeks.com\/a-tip-about-using-python-for-regular-expressions-in-t-sql-by-erland-sommarskog\/<\/a>    <\/p>\n<p>on <em>regular expressions<\/em> for SQL Server and the advantage of enabling sp_execute_external_script.  This works with the version of Anaconda3 that installs with SQL Server 2019.   There is an issue on the laptop used here because group policy (via the government policy) demands the use of FIPS.  For this reason, installing R or Java will fail (I suspect it doesn't sit well with managed copies of encryption).  Anaconda requires many over-rides during installation via <em>Privileged Management Administrator<\/em> and is installed locally through a painstaking process.       <\/p>\n<p>My bigger fear is that if I remove Anaconda3 to install using SQL, it will either fail similarly as did R and Java, or worse, I'll have trouble re-installing the version of Anaconda currently on the machine.    <\/p>\n<p>So again, the question is whether or not it is possible to enable sp_execute_external_script on SQL without installing R, Python, or Java.   I tried Java and R and both fail to install.   The Java and Python are already installed.<\/p>",
        "Challenge_closed_time":1670970968172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670958836030,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in enabling sp_execute_external_script on SQL Server due to the government policy that demands the use of FIPS, which causes the installation of R or Java to fail. The user has installed Anaconda3 locally through a painstaking process and is concerned about removing it to install using SQL, as it may fail similarly as did R and Java, or have trouble re-installing the current version of Anaconda. The user is seeking a solution to enable sp_execute_external_script on SQL without installing R, Python, or Java.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1127660\/will-sql-server-enable-sp-execute-external-script",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":18.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":3.3700394444,
        "Challenge_title":"Will SQL Server enable  sp_execute_external_script to work with a previously installed Anaconda3",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":181,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I am not sure that I understand the question. To use Python from SQL Server, you need to do one of:<\/p>\n<p>1) Use the Python that comes with SQL 2017 or SQL 2019.  <br \/>\n2) Install any version of Python you like as described on <a href=\"https:\/\/learn.microsoft.com\/en-us\/sql\/machine-learning\/install\/sql-machine-learning-services-windows-install-sql-2022\">https:\/\/learn.microsoft.com\/en-us\/sql\/machine-learning\/install\/sql-machine-learning-services-windows-install-sql-2022<\/a>. (That page is for SQL 2022, but it should be good for SQL 2019 as well.)<\/p>\n<p>If you don't have any external languages installed, you can still enable sp_execute_external_script, but you don't have much use for it, obviously.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.9,
        "Solution_reading_time":9.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":23.5220988889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I get the following error inside the child runs in ML studio while doing an Automated ML experiment.  <\/p>\n<p>&quot;Identity does not have permissions for Microsoft.MachineLearningServices\/workspaces\/metadata\/artifacts\/write actions.&quot;  <\/p>\n<p>I am the owner of the resource group so I am not sure what the issue is.  <\/p>\n<p>Thanks  <\/p>",
        "Challenge_closed_time":1621971310163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621886630607,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a permission error while finishing an Automated ML experiment in ML studio. The error message stated that the user did not have permissions for Microsoft.MachineLearningServices\/workspaces\/metadata\/artifacts\/write actions, despite being the owner of the resource group.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/407580\/permission-error-while-finishing-auto-ml-run",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.7,
        "Challenge_reading_time":4.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":23.5220988889,
        "Challenge_title":"Permission error while finishing auto ml run",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":54,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello everyone, <a href=\"\/users\/na\/?userid=1321ce4c-8332-49c7-b902-2bcd4256debc\">@Shubham Miglani  <\/a> <a href=\"\/users\/na\/?userid=8886df29-ba7f-42f0-a932-a7883bbe54ea\">@Nick Schafer  <\/a>     <\/p>\n<p>We have identified the issue and a hot fix is rolling out.  It will be fixed in all regions by end of today. Sorry for the experience.     <\/p>\n<p>Regards,    <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":4.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.1574388889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We have an active subscription. For few of the users we activated owner role for the machine learning resource group.    <br \/>\nBut when they login to the portal\/ML environment and try to switch directory and subscription, they don't see our production subscription and hence the workspace, although directory is correct. User had created a trial subscription on its own before and he only has visibility to that.    <\/p>\n<p>I checked with a test account and after login to ML studio I see this, which I believe is the same reason user does not see the subscription.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/255195-screenshot-2022-10-28-085206.png?platform=QnA\" alt=\"255195-screenshot-2022-10-28-085206.png\" \/>    <\/p>\n<p>How can I safely give user subscription access only for ML resource group    <\/p>",
        "Challenge_closed_time":1666977116507,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666972949727,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges with providing access to an Azure subscription for certain users. Although the owner role has been activated for the machine learning resource group, some users are unable to see the production subscription and workspace when they log in to the portal\/ML environment. This is because they had created a trial subscription on their own before and only have visibility to that. The user is seeking advice on how to safely give subscription access only for the ML resource group.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1067157\/access-to-azure-subscription-for-users",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":10.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.1574388889,
        "Challenge_title":"Access to Azure subscription for users",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":120,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=8cb8b6e9-f7ac-47d9-bcd3-6c4b5af3201d\">@RT-7199  <\/a>     <\/p>\n<p>Thank you for asking this question on the **Microsoft Q&amp;A Platform. **    <\/p>\n<p>The role will depend on the activity that the user performs, for example, to create a new workspace you will require the role owner or contributor at the Resource group-level. (If you receive a failure when trying to create a workspace for the first time, make sure that your role allows Microsoft.MachineLearningServices\/register\/action. This action allows you to register the Azure Machine Learning resource provider with your Azure subscription.)    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/255119-image.png?platform=QnA\" alt=\"255119-image.png\" \/>    <\/p>\n<p>You can get more information about RBAC for Azure Machine Learning workspace <strong><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-assign-roles?tabs=labeler#common-scenarios\">here<\/a><\/strong>    <\/p>\n<p>Hope this helps!    <\/p>\n<p>----------    <\/p>\n<p><strong><a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/support\/accepted-answers\">Accept Answer<\/a><\/strong> and Upvote, if any of the above helped, this thread can help others in the community looking for remediation for similar issues.    <br \/>\n<em>NOTE: To answer you as quickly as possible, please mention me in your reply.<\/em>    <\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.2,
        "Solution_reading_time":17.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":152.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1640731722280,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":0.0418091667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I Can not run <code>apt to install git-lfs<\/code> on sagemaker notebook instance. I want to run git commands in my notebook.<\/p>",
        "Challenge_closed_time":1640732355180,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640732204667,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is unable to install \"git-lfs\" on their AWS Sagemaker notebook instance and is seeking a solution to run git commands in their notebook.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70513398",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":2.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0418091667,
        "Challenge_title":"I can not install \"git-lfs\" on aws sagemaker notebook instance",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":489.0,
        "Challenge_word_count":30,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1596727881047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>use the following commands to install git-lfs<\/p>\n<pre><code>!curl -s https:\/\/packagecloud.io\/install\/repositories\/github\/git-lfs\/script.rpm.sh | sudo bash\n\n!sudo yum install git-lfs -y\n\n!git lfs install\n<\/code><\/pre>\n<p>that should make it work<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.9,
        "Solution_reading_time":3.29,
        "Solution_score_count":6.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1527065173880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":25218.0,
        "Answerer_view_count":2553.0,
        "Challenge_adjusted_solved_time":62.1080711111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I tried to run this code to create an ml workspace from the azure-ml-cli, referencing an existing azure container registry from another subscription:<\/p>\n<pre><code>az ml workspace create --workspace-name &quot;test-mlws&quot; --keyvault &quot;&lt;key-vault-service-id&gt;&quot; --container-registry &quot;&lt;container-registry-zervice-id&gt;&quot; --location westeurope\n<\/code><\/pre>\n<p>The deploy failed with this error code:<\/p>\n<pre><code>{'code': 'InternalServerError', 'message': 'Received 403 from a service request'}\n<\/code><\/pre>\n<p>I can't find any documentation about it, and I guess it's due to the container registry I used which belongs to another subscription. Anyone who knows if it's mandatory for the registry to be in the same subscription?<\/p>",
        "Challenge_closed_time":1618194887616,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617971298560,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while trying to create an ml workspace from the azure-ml-cli, referencing an existing azure container registry from another subscription. The error code received was 'InternalServerError' with a message stating 'Received 403 from a service request'. The user suspects that the error is due to the container registry belonging to another subscription and is unsure if it is mandatory for the registry to be in the same subscription.",
        "Challenge_last_edit_time":1618194925572,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67021176",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":18.2,
        "Challenge_reading_time":10.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":62.1080711111,
        "Challenge_title":"Azure Machine Learning - Use personal container registry",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":773.0,
        "Challenge_word_count":97,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519297984923,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cagliari, CA, Italia",
        "Poster_reputation_count":127.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>From <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace?tabs=python#limitations\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace?tabs=python#limitations<\/a><\/p>\n<blockquote>\n<p>If you want to use <strong>existing services from a different Azure\nsubscription<\/strong> than the workspace, you must register the Azure Machine\nLearning namespace in the subscription that contains those services.<\/p>\n<\/blockquote>\n<p>So, in order to use the ACR in that different subscription, you need to register resource provider <code>Microsoft.MachineLearningServices<\/code> in that subscription contains ACR. For information on how to see if it is registered and how to register it, see the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/azure-resource-manager\/management\/resource-providers-and-types\" rel=\"nofollow noreferrer\">Azure resource providers and types<\/a> article.<\/p>\n<p>To register a resource provider, use:<\/p>\n<pre><code>Register-AzResourceProvider -ProviderNamespace Microsoft.MachineLearningServices\n<\/code><\/pre>\n<p>To see information for a particular resource provider, use:<\/p>\n<pre><code>Get-AzResourceProvider -ProviderNamespace Microsoft.MachineLearningServices\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":17.4,
        "Solution_reading_time":17.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":723.7180555556,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer is using SageMaker Studio in VpcOnly mode (VPC, protected subnets **without** internet access, **NO** NAT gateways). \nThe all functionality is fine. However, when I try create a SageMaker projects - as described [here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-projects-create.html), SageMaker Studio is unable to list the project templates (timeout and unspecified error) resulting in empty list of the available project templates.\n\nProjects are enabled for the users - as described [here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-projects-studio-updates.html). The problem is with project creation.\n\nIs internet access (e.g. via NAT gateways) is needed for SageMaker projects?",
        "Challenge_closed_time":1618085440000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615480055000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while creating SageMaker projects in VpcOnly mode without internet access. SageMaker Studio is unable to list the project templates resulting in an empty list of available project templates. The user has confirmed that projects are enabled for the users, but the problem is with project creation. The user is unsure if internet access is needed for SageMaker projects.",
        "Challenge_last_edit_time":1668618206192,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUcyhpq1pxRTmtjkDRAh_MDA\/sagemaker-studio-projects-in-vpconly-mode-without-internet-access",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":10.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":723.7180555556,
        "Challenge_title":"SageMaker Studio projects in VpcOnly mode without internet access",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":678.0,
        "Challenge_word_count":91,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Figured it out. SageMaker Studio projects need Service Catalog access and VPCE for `com.amazonaws.${AWS::Region}.servicecatalog`",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925560363,
        "Solution_link_count":0.0,
        "Solution_readability":15.5,
        "Solution_reading_time":1.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":14.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":511.6538888889,
        "Challenge_answer_count":0,
        "Challenge_body":"Error in pipeline in GithubActions\r\n`14:25:43.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDOUT: Starting Mlflow UI on port 5000\r\n14:25:46.430 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\r\n14:25:46.453 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\r\n14:25:46.468 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 ERROR mlflow.cli: Error initializing backend store\r\n14:25:46.480 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: ]\r\n14:25:46.483 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.484 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: )\r\n14:25:46.485 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tFOREIGN KEY(experiment_id) REFERENCES experiments (experiment_id)\r\n14:25:46.487 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT runs_lifecycle_stage CHECK (lifecycle_stage IN ('active', 'deleted')), \r\n14:25:46.489 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT status CHECK (status IN ('SCHEDULED', 'FAILED', 'FINISHED', 'RUNNING')), \r\n14:25:46.491 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT source_type CHECK (source_type IN ('NOTEBOOK', 'JOB', 'LOCAL', 'UNKNOWN', 'PROJECT')), \r\n14:25:46.493 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT run_pk PRIMARY KEY (run_uuid), \r\n14:25:46.495 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \texperiment_id INTEGER, \r\n14:25:46.496 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tartifact_uri VARCHAR(200), \r\n14:25:46.497 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tlifecycle_stage VARCHAR(20), \r\n14:25:46.500 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_version VARCHAR(50), \r\n14:25:46.505 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tend_time BIGINT, \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstart_time BIGINT, \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstatus VARCHAR(20), \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tuser_id VARCHAR(256), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tentry_point_name VARCHAR(50), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_name VARCHAR(500), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_type VARCHAR(20), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tname VARCHAR(250), \r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \trun_uuid VARCHAR(32) NOT NULL, \r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: (Background on this error at: http:\/\/sqlalche.me\/e\/gkpj)\r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.516 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: The above exception was the direct cause of the following exception:\r\n14:25:46.516 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\r\n14:25:46.517 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1618, in _run_visitor\r\n14:25:46.517 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     conn._run_visitor(visitorcallable, element, **kwargs)\r\n14:25:46.518 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     visitorcallable(self.dialect, self, **kwargs).traverse_single(element)\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: (Background on this error at: http:\/\/sqlalche.me\/e\/gkpj)\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: ]\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: )\r\n14:25:46.542 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tFOREIGN KEY(experiment_id) REFERENCES experiments (experiment_id)\r\n14:25:46.542 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT runs_lifecycle_stage CHECK (lifecycle_stage IN ('active', 'deleted')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Updating database tables\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Will assume transactional DDL.\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT status CHECK (status IN ('SCHEDULED', 'FAILED', 'FINISHED', 'RUNNING')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT source_type CHECK (source_type IN ('NOTEBOOK', 'JOB', 'LOCAL', 'UNKNOWN', 'PROJECT')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT run_pk PRIMARY KEY (run_uuid), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \texperiment_id INTEGER, \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tartifact_uri VARCHAR(200), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tlifecycle_stage VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_version VARCHAR(50), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tend_time BIGINT, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstart_time BIGINT, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstatus VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tuser_id VARCHAR(256), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tentry_point_name VARCHAR(50), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_name VARCHAR(500), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_type VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tname VARCHAR(250), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \trun_uuid VARCHAR(32) NOT NULL, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: CREATE TABLE runs (\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: [SQL: \r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     cursor.execute(statement, parameters)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/default.py\", line 588, in do_execute\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.dialect.do_execute(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1245, in _execute_context\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     raise value.with_traceback(tb)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/util\/compat.py\", line 152, in reraise\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     reraise(type(exception), exception, tb=exc_tb, cause=cause)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/util\/compat.py\", line 398, in raise_from_cause\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     util.raise_from_cause(sqlalchemy_exception, exc_info)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1476, in _handle_dbapi_exception\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self._handle_dbapi_exception(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1249, in _execute_context\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     ret = self._execute_context(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1039, in _execute_ddl\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return connection._execute_ddl(self, multiparams, params)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 72, in _execute_on_connection\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(self, multiparams, params)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 982, in execute\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.connection.execute(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 821, in visit_table\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(obj, **kw)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/visitors.py\", line 138, in traverse_single\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.traverse_single(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 777, in visit_metadata\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(obj, **kw)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/visitors.py\", line 138, in traverse_single\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 2049, in _run_visitor\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     bind._run_visitor(\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/schema.py\", line 4315, in create_all\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     InitialBase.metadata.create_all(engine)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/db\/utils.py\", line 30, in _initialize_tables\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     mlflow.store.db.utils._initialize_tables(self.engine)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/sqlalchemy_store.py\", line 99, in __init__\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return SqlAlchemyStore(store_uri, artifact_uri)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 64, in _get_sqlalchemy_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return builder(store_uri=store_uri, artifact_uri=artifact_uri)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 37, in get_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 91, in _get_tracking_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     _get_tracking_store(backend_store_uri, default_artifact_root)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 105, in initialize_backend_stores\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     initialize_backend_stores(backend_store_uri, default_artifact_root)\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/cli.py\", line 291, in server\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: Traceback (most recent call last):\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     cursor.execute(statement, parameters)\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/default.py\", line 588, in do_execute\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.dialect.do_execute(\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1245, in _execute_context\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: Traceback (most recent call last):\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: CREATE TABLE runs (\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: [SQL: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 ERROR mlflow.cli: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"`\r\nwhich causes test to not pass",
        "Challenge_closed_time":1610230183000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1608388229000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an issue with MLFlow where the name of the bucket is hardcoded, making it impossible to use MLFlow with AWS S3. This poses a challenge for those using Minio in Gateway mode with MLFlow on AWS as S3 buckets are globally unique.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/prinz-nussknacker\/prinz\/issues\/78",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":23.7,
        "Challenge_reading_time":251.52,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":210.0,
        "Challenge_repo_star_count":8.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":290,
        "Challenge_solved_time":511.6538888889,
        "Challenge_title":"Error when starting new experiment in mlflow",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":1131,
        "Discussion_body":"Already fixed in #79 by introducing delay between mlflow server start and starting experiments in mlflow",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1565289301123,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":79.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":3320.1937519445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What are the different data sources we can import data into Azure Machine Learning Services storage or notebook. I mean from Salesforce or any ERP or any website? As of now I have seen importing data using URL or getting it from data location in storage where notebook will also be stored.<\/p>\n\n<p>I have not got anything to try on. I googled for different methods, but couldn't find relevant link. So I didn't try much there.<\/p>",
        "Challenge_closed_time":1566330717407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554359325097,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on the various data sources that can be imported into Azure Machine Learning Services storage or notebook, including from Salesforce, ERP, or websites. They have attempted to find information online but have been unsuccessful.",
        "Challenge_last_edit_time":1554378019900,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55509207",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":5.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3325.3867527778,
        "Challenge_title":"Data sources in Azure Machine Learning Services",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1140.0,
        "Challenge_word_count":82,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545289999703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Thanks for your question. You can import data from Azure Blob, Azure File, ADLS Gen1, ADLS Gen2, Azure SQL, Azure PostgreSQL. \nFor more information: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data<\/a><\/p>\n\n<p>You can create an Azure ML Dataset for your training scenarios. Dataset can be created either from the data store mentioned above or from public urls.\nFor more information: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-register-datasets\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-register-datasets<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.7,
        "Solution_reading_time":10.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1324988509368,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Moscow, Russia",
        "Answerer_reputation_count":1593.0,
        "Answerer_view_count":93.0,
        "Challenge_adjusted_solved_time":0.0671325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed an Endpoint on Amazon SageMaker.\nNow I am trying to Invoke it.<\/p>\n\n<p>If I run this code in Sagemaker's Jupyter Notebook: <\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime')\nendpoint_name = 'DEMO-XGBoostEndpoint'\nbody = ','.join(['1.0'] * 6)\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=body)\nresponse['Body'].read()\n<\/code><\/pre>\n\n<p>it works properly.<\/p>\n\n<p>But if I run the same code, with added credentials for boto3 client, from my machine:<\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime', \n                       aws_access_key_id=ACCESS_ID,\n                       aws_secret_access_key= ACCESS_KEY)\nendpoint_name = 'DEMO-XGBoostEndpoint'\nbody = ','.join(['1.0'] * 6)\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=body)\nresponse['Body'].read()\n<\/code><\/pre>\n\n<p>I get this error:<\/p>\n\n<blockquote>\n  <p>ClientError: An error occurred (AccessDeniedException) when calling the InvokeEndpoint operation: User: arn:aws:iam::249707424405:user\/yury.logachev is not authorized to perform: sagemaker:InvokeEndpoint on resource: arn:aws:sagemaker:us-east-1:249707424405:endpoint\/demo-xgboostendpoint-2018-12-12-22-07-28 with an explicit deny<\/p>\n<\/blockquote>\n\n<p>If I run the latter piece of code (with added credentials as a parameters of client) on Sagemaker's Jupyter Notebook, I also get the same error.<\/p>\n\n<p>I understand that the solution should be linked with roles, policies etc, but could not find out it.<\/p>",
        "Challenge_closed_time":1547664263500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547411152043,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an AccessDeniedException error when trying to invoke an Endpoint on Amazon SageMaker using boto3 client with added credentials. The error message indicates that the user is not authorized to perform the sagemaker:InvokeEndpoint operation on the specified resource. The user suspects that the issue is related to roles and policies but is unsure of the solution.",
        "Challenge_last_edit_time":1547664021823,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54172907",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":20.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":70.3087380556,
        "Challenge_title":"Amazon Sagemaker. AccessDeniedException when calling the InvokeEndpoint operation",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":2435.0,
        "Challenge_word_count":155,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324988509368,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Moscow, Russia",
        "Poster_reputation_count":1593.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>The problem was with the MFA autharization. \nWhen I invoked the model from inside the model, the MFA was passed. \nBut when I tried to invoke the model from my machine, the MFA was not passed, so the access was denied.<\/p>\n\n<p>I created special user without MFA to debug the model, and that solved my problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":3.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1298484007147,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, United States",
        "Answerer_reputation_count":9271.0,
        "Answerer_view_count":1819.0,
        "Challenge_adjusted_solved_time":119.2915472222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to add an IAM user for using sagemaker. I used the <code>AmazonSageMakerFullAccess<\/code> policy. But when I log in as this user I can see all of the s3 buckets of the root account and download files from them.<\/p>\n<p>The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\" rel=\"nofollow noreferrer\">sagemaker documentation<\/a> states<\/p>\n<blockquote>\n<p>When attaching the AmazonSageMakerFullAccess policy to a role, you must do one of the following to allow Amazon SageMaker to access your S3 bucket:<\/p>\n<p>Include the string &quot;SageMaker&quot; or &quot;sagemaker&quot; in the name of the bucket where you store training data, or the model artifacts resulting from model training, or both.<\/p>\n<p>Include the string &quot;SageMaker&quot; or &quot;sagemaker&quot; in the object name of the training data object(s).<\/p>\n<p>Tag the S3 object with &quot;sagemaker=true&quot;. The key and value are case sensitive. For more information, see Object Tagging in the Amazon Simple Storage Service Developer Guide.<\/p>\n<p>Add a bucket policy that allows access for the execution role. For more information, see Using Bucket Policies and User Policies in the Amazon Simple Storage Service Developer Guide.<\/p>\n<\/blockquote>\n<p>This seems to be inaccurate the user can access s3 buckets lacking <code>sagemaker<\/code> in the name. How do I limit the access?<\/p>\n<p>the full policy is below<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;sagemaker:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;ecr:GetAuthorizationToken&quot;,\n                &quot;ecr:GetDownloadUrlForLayer&quot;,\n                &quot;ecr:BatchGetImage&quot;,\n                &quot;ecr:BatchCheckLayerAvailability&quot;,\n                &quot;cloudwatch:PutMetricData&quot;,\n                &quot;cloudwatch:PutMetricAlarm&quot;,\n                &quot;cloudwatch:DescribeAlarms&quot;,\n                &quot;cloudwatch:DeleteAlarms&quot;,\n                &quot;ec2:CreateNetworkInterface&quot;,\n                &quot;ec2:CreateNetworkInterfacePermission&quot;,\n                &quot;ec2:DeleteNetworkInterface&quot;,\n                &quot;ec2:DeleteNetworkInterfacePermission&quot;,\n                &quot;ec2:DescribeNetworkInterfaces&quot;,\n                &quot;ec2:DescribeVpcs&quot;,\n                &quot;ec2:DescribeDhcpOptions&quot;,\n                &quot;ec2:DescribeSubnets&quot;,\n                &quot;ec2:DescribeSecurityGroups&quot;,\n                &quot;application-autoscaling:DeleteScalingPolicy&quot;,\n                &quot;application-autoscaling:DeleteScheduledAction&quot;,\n                &quot;application-autoscaling:DeregisterScalableTarget&quot;,\n                &quot;application-autoscaling:DescribeScalableTargets&quot;,\n                &quot;application-autoscaling:DescribeScalingActivities&quot;,\n                &quot;application-autoscaling:DescribeScalingPolicies&quot;,\n                &quot;application-autoscaling:DescribeScheduledActions&quot;,\n                &quot;application-autoscaling:PutScalingPolicy&quot;,\n                &quot;application-autoscaling:PutScheduledAction&quot;,\n                &quot;application-autoscaling:RegisterScalableTarget&quot;,\n                &quot;logs:CreateLogGroup&quot;,\n                &quot;logs:CreateLogStream&quot;,\n                &quot;logs:DescribeLogStreams&quot;,\n                &quot;logs:GetLogEvents&quot;,\n                &quot;logs:PutLogEvents&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;,\n                &quot;s3:PutObject&quot;,\n                &quot;s3:DeleteObject&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::*SageMaker*&quot;,\n                &quot;arn:aws:s3:::*Sagemaker*&quot;,\n                &quot;arn:aws:s3:::*sagemaker*&quot;\n            ]\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:CreateBucket&quot;,\n                &quot;s3:GetBucketLocation&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:ListAllMyBuckets&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEqualsIgnoreCase&quot;: {\n                    &quot;s3:ExistingObjectTag\/SageMaker&quot;: &quot;true&quot;\n                }\n            }\n        },\n        {\n            &quot;Action&quot;: &quot;iam:CreateServiceLinkedRole&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:iam::*:role\/aws-service-role\/sagemaker.application-autoscaling.amazonaws.com\/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringLike&quot;: {\n                    &quot;iam:AWSServiceName&quot;: &quot;sagemaker.application-autoscaling.amazonaws.com&quot;\n                }\n            }\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEquals&quot;: {\n                    &quot;iam:PassedToService&quot;: &quot;sagemaker.amazonaws.com&quot;\n                }\n            }\n        }\n    ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1525129267447,
        "Challenge_comment_count":1,
        "Challenge_created_time":1524699817877,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue where an IAM user with the AmazonSageMakerFullAccess policy can access all S3 buckets of the root account and download files from them, even if the buckets do not have \"SageMaker\" in their name. The user is seeking a way to limit this access.",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50032795",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":27.9,
        "Challenge_reading_time":64.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":119.2915472222,
        "Challenge_title":"prevent access to s3 buckets for sagemaker users",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1703.0,
        "Challenge_word_count":302,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1298484007147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, NY, United States",
        "Poster_reputation_count":9271.0,
        "Poster_view_count":1819.0,
        "Solution_body":"<p>looks like the sagemaker notebook wizard has you create a role that has limited s3 access. If I add this and the default <code>AmazonSageMakerFullAccess<\/code> the user is properly restricted. <a href=\"https:\/\/i.stack.imgur.com\/9BjRD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9BjRD.png\" alt=\"Amazon make sagemaker role\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/IM7WW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IM7WW.png\" alt=\"choose iam roles\"><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.6,
        "Solution_reading_time":6.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":216.3194444444,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nWhen using the Neptune ML widget to export data like the command below from the 01- Node Classification notebook:\r\n```\r\n%%neptune_ml export start --export-url {neptune_ml.get_export_service_host()} --export-iam --wait --store-to export_results\r\n${export_params}\r\n```\r\nThe following error is thrown\r\n```\r\n{\r\n  \"message\": \"Credential should be scoped to correct service: 'execute-api'. \"\r\n}  \r\n```\r\n\r\n**Expected behavior**\r\nThe export should run to completion\r\n\r\n\r\n",
        "Challenge_closed_time":1628716798000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1627938048000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while connecting a local notebook to a remote Neptune with SSL enabled. The user has set up an SSH tunnel via bastion to the Neptune cluster and started the graph-notebook. However, when running a command, the user is getting an SSL error. The user expects to be able to connect to the remote Neptune with SSL enabled.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/167",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":25.0,
        "Challenge_repo_fork_count":129.0,
        "Challenge_repo_issue_count":493.0,
        "Challenge_repo_star_count":546.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":216.3194444444,
        "Challenge_title":"[BUG] Neptune ML Export widget throwing error",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Discussion_body":"This issue occurs on a cluster created using the default CFN script with IAM disabled\r\n",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":1.0578722222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>According to Kedro's <a href=\"https:\/\/kedro.readthedocs.io\/en\/0.15.7\/kedro.extras.datasets.html\" rel=\"nofollow noreferrer\">documentation<\/a>, Azure Blob Storage is one of the available data sources. Does this extend to ADLS Gen2 ?<\/p>\n<p>Haven't tried Kedro yet, but before I invest some time on it, I wanted to make sure I could connect to ADLS Gen2.<\/p>\n<p>Thank you in advance !<\/p>",
        "Challenge_closed_time":1636712829167,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636709020827,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring if Azure Data Lake Storage Gen2 (ADLS Gen2) can be used as a data source for Kedro pipeline, as Azure Blob Storage is listed as an available data source in Kedro's documentation. The user wants to confirm this before investing time in Kedro.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69940562",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":5.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.0578722222,
        "Challenge_title":"Azure Data Lake Storage Gen2 (ADLS Gen2) as a data source for Kedro pipeline",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":350.0,
        "Challenge_word_count":65,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586517832390,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":127.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>Yes this works with Kedro. You're actually pointing a really old version of the docs, nowadays all filesystem based datasets in Kedro use <a href=\"https:\/\/github.com\/fsspec\/filesystem_spec\" rel=\"nofollow noreferrer\">fsspec<\/a> under the hood which means they work with S3, HDFS, local and many more filesystems seamlessly.<\/p>\n<p>The ADLS Gen2 is supported by <code>ffspec<\/code> via the underlying <code>adlfs<\/code> library which is <a href=\"https:\/\/github.com\/fsspec\/adlfs\" rel=\"nofollow noreferrer\">documented here<\/a>.<\/p>\n<p>From a Kedro point of view all you need to do is declare your catalog entry like so:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code> motorbikes:\n     type: pandas.CSVDataSet\n     filepath: abfs:\/\/your_bucket\/data\/02_intermediate\/company\/motorbikes.csv\n     credentials: dev_az\n<\/code><\/pre>\n<p>We also have more examples <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/01_data_catalog.html\" rel=\"nofollow noreferrer\">here<\/a>, particularly example 15.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.5,
        "Solution_reading_time":13.03,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":103.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1401427814950,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":749.0,
        "Answerer_view_count":50.0,
        "Challenge_adjusted_solved_time":19.2463625,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have two computers: Ubuntu1 and Ubuntu2. \nUbuntu1 runs MongoDB with database Sacred3. \nI want to connect from U2 to U1 via ssh and store there my experiment results.<\/p>\n\n<p>What I tried and failed:\n1. I installed mongo DB, created sacred3, I have ssh key to it. \nI edited <code>\/etc\/mongod.conf<\/code> adding:<\/p>\n\n<p><code># network interfaces\nnet:\n  port: 27017\n  bindIp: 0.0.0.0<\/code><\/p>\n\n<p>Then I enabled port forwarding with<\/p>\n\n<p><code>ssh -fN  -i ~\/.ssh\/sacred_key-pair.pem -L 6666:localhost:27017 ubuntu@106.969.696.969<\/code> \/\/ (with proper ip)<\/p>\n\n<p>so, as I undertstand, if I connect to my localhost:6666 it will be forwarded to 106.969.696.969:27017 <\/p>\n\n<p>So after that, I'm runnig an experiment with <a href=\"https:\/\/sacred.readthedocs.io\/en\/stable\/observers.html\" rel=\"nofollow noreferrer\">Sacred framework<\/a>:<\/p>\n\n<p>python exp1.py -m localhost:6666:sacred3<\/p>\n\n<p>and this should write experiment to remote DB, HOWEVER i I get:<\/p>\n\n<p><code>pymongo.errors.ServerSelectionTimeoutError: localhost:27017: [Errno 111] Connection refused<\/code><\/p>\n\n<p>which is driving me mad. please help!<\/p>\n\n#\n\n<p>below contents of exp1.py:<\/p>\n\n<pre><code>from sacred import Experiment\nfrom sacred.observers import MongoObserver\n\nex = Experiment()\nex.observers.append(MongoObserver.create())\n\ndef compute():\n    summ = layer1 - layer2\n    return summ\n\n\n@ex.config\ndef my_config():\n\n    hp_list = [{\"neurons\" : [32,32] , \"dropout\": 1.0},\n            {\"neurons\" : [32,32] , \"dropout\": 0.7},\n            {\"neurons\" : [32,16] , \"dropout\": 0.9},\n            {\"neurons\" : [24,16] , \"dropout\": 0.9},\n            {\"neurons\" : [24,8] , \"dropout\":  0.9},\n            {\"neurons\" : [16,8] , \"dropout\":  0.9},\n            {\"neurons\" : [64,64] , \"dropout\": 0.9},\n            {\"neurons\" : [64,64] , \"dropout\": 0.7},\n            {\"neurons\" : [64,32] , \"dropout\": 0.9},\n            {\"neurons\" : [64,32] , \"dropout\": 0.7},\n            {\"neurons\" : [48,32] , \"dropout\": 0.9},\n            {\"neurons\" : [48,32] , \"dropout\": 0.7},\n            {\"neurons\" : [48,16] , \"dropout\": 0.9},\n            {\"neurons\" : [48,16] , \"dropout\": 0.7},]\n\n    n_epochs = 2 \n\n\n@ex.capture\ndef training_loop(hp_list, n_epochs):\n    for j in hp_list:\n        print(\"Epoch: \", n_epochs)\n#       layer1 = random.randint(18,68)\n#       layer2 = random.randint(18,68)\n#       layer3 = random.randint(18,68)\n        layer1 = j[\"neurons\"][0]\n        layer2 = j[\"neurons\"][1]\n        dropout_ratio = j[\"dropout\"]\n\n\n        print(\"WHATS UUUUUP\",j, layer1, layer2, dropout_ratio, sep=\"_\")\n        # vae_training_loop_NN_DO(i, layer1, layer2, dropout_ratio )\n\n\n@ex.automain\ndef my_main():\n    training_loop()\n\n<\/code><\/pre>",
        "Challenge_closed_time":1571149528216,
        "Challenge_comment_count":4,
        "Challenge_created_time":1571145034667,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to save experiment results to a remote MongoDB database on Ubuntu1 from Ubuntu2 via an SSH tunnel. The user has installed MongoDB, created a database, and edited the mongod.conf file to enable port forwarding. However, when running an experiment with the Sacred framework, the user encounters a ServerSelectionTimeoutError, indicating that the connection to the remote database is being refused.",
        "Challenge_last_edit_time":1571149003303,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58395547",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":8.3,
        "Challenge_reading_time":31.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":1.2482080556,
        "Challenge_title":"How to save data to remote mongoDB via ssh tunnel? (connection refused)",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":329.0,
        "Challenge_word_count":266,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501710879087,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Warszawa, Polska",
        "Poster_reputation_count":300.0,
        "Poster_view_count":42.0,
        "Solution_body":"<p>According to the documentation <a href=\"https:\/\/sacred.readthedocs.io\/en\/stable\/observers.html\" rel=\"nofollow noreferrer\">supplied<\/a>, it looks like you're creating two observers, or overriding the connection argument you passed with <code>-m<\/code>, with the <code>MongoObserver.create()<\/code>specified in the code which uses the default mongo host and port <code>localhost:27017<\/code>. You either supply the observer connection via the <code>-m<\/code> argument or in code, not both.<\/p>\n\n<p>Try removing the <code>MongoObserver.create()<\/code> line altogether, or hardcoding the connection arguments: <code>MongoObserver(url='localhost:6666', db_name='sacred3')<\/code> <\/p>\n\n<p>Also, it looks like your mongo host is <a href=\"https:\/\/serverfault.com\/questions\/489192\/ssh-tunnel-refusing-connections-with-channel-2-open-failed\">not liking the binding to localhost<\/a> so you should also replace <code>localhost<\/code> in your ssh command with <code>127.0.0.1<\/code> or <code>[::1]<\/code>, e.g <code>ssh -fN -i ~\/.ssh\/sacred_key-pair.pem -L 6666:127.0.0.1:27017 ubuntu@106.969.696.969<\/code> or <code>ssh -fN -i ~\/.ssh\/sacred_key-pair.pem -L 6666:[::1]:27017 ubuntu@106.969.696.969<\/code><\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1571218290208,
        "Solution_link_count":2.0,
        "Solution_readability":12.9,
        "Solution_reading_time":15.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":113.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.6412836111,
        "Challenge_answer_count":1,
        "Challenge_body":"Can Sagemaker Git Repositories use ssh secrets (no name and password)?",
        "Challenge_closed_time":1649828878883,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649790570262,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring if Sagemaker Git Repositories can utilize ssh secrets instead of a username and password.",
        "Challenge_last_edit_time":1668627587288,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU-P1Hlk4OR6K6kAug-wHT_g\/can-sagemaker-git-repositories-use-ssh-secrets-no-name-and-password",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":1.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":10.6412836111,
        "Challenge_title":"Can Sagemaker Git Repositories use ssh secrets (no name and password)?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1708.0,
        "Challenge_word_count":21,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes, Sagemaker can use SSH for private repos. There are multiple options on how to connect to a repo in Sagemaker.\n\n** Option 1**: Using SSH to work with a private repo\nYou can follow the same steps you do in your local machine to connect to a private repo through SSH, steps to follow:\n    \n1. Open `Terminal` and type `ssh-keygen` to create an SSH key in your Amazon Sagemaker instance. \n2. Add the public key to your Git account (Github or Gitlab)\n3. Get the SSH url of your repo and git clone\n\n**Option 2**: Using AWS Secret Manager \nYou can follow the steps in AWS official documentation [here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-resource.html). \n\n\n**Option 3**: Using GitHub with Personal Access Tokens **Recommended**\n\nLet\u2019s assume you have already generated an Access Tokens through the GitHub\u2019s Settings \/ Developer Settings \/ Personal Access Tokens page.\n\nYou can just simply go ahead and clone the repository using Studio UI. When it asks your username and password, you can provide your GitHub username and the Personal Access Token. If you want to cache your credentials avoiding to type it every time when you\u2019re interacting with the GitHub server, you can cache or store it on your home folder with the following command issued in the Terminal:\n\n``` $ git config --global git credential.helper [cache|store] ```\n\nIf you choose to store your credentials, it will be written to the `~\/.git-credentials` file located in your home folder. The \u201ccache\u201d helper stores the credential in-memory only and never lands on disk. It also accepts the --timeout <seconds> option, which changes the amount of time its daemon is kept running (the default is \u201c900\u201d, or 15 minutes)\n\nBefore you make your first commit, you still need to configure the git client to use your identity when we\u2019re checking in some new code into the repository. You need to run the following two commands from the terminal:\n```\n$ git config --global user.email \u201cuser@email.com\u201d\n$ git config --global user.name \u201cUser Name\u201d\n```\n\nSagemaker Studio is fully integrated with git and you can do it through the UI.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1650645924384,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":25.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":335.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":55.8666666667,
        "Challenge_answer_count":1,
        "Challenge_body":"When locally testing my custom-job through \"gcloud ai custom-jobs local-run\" command, I would like to have access to a bucket mounted though gcsFuse as it happens when I launch the same containerized job from GCloud console. Is there the option to have the same access locally?\n\nThank you for helping",
        "Challenge_closed_time":1664527380000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664326260000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to locally test a custom-job using \"gcloud ai custom-jobs local-run\" command and wants to access a bucket mounted through gcsFuse, similar to when launching the same job from GCloud console. The user is seeking options to have the same access locally.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Mount-gcsfuse-in-gcloud-ai-custom-jobs-local-run\/m-p\/471834#M609",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":4.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":55.8666666667,
        "Challenge_title":"Mount gcsfuse in gcloud ai custom-jobs local-run",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":243.0,
        "Challenge_word_count":56,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"What you could do is use cloud storage as a file system within ai training, since while using fuse your training jobs on both of the platforms can access your data that is stored on Cloud Storage as files on your local file system, also the documentation I shared provides you useful information as the problems you might encounter, permissions, a brief description of cloud storage fuse, performance related information, the restrictions this method has and also how you can make use of the logs.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.7,
        "Solution_reading_time":6.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":90.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":94.8744444444,
        "Challenge_answer_count":0,
        "Challenge_body":"load_dataset function from hugging face can't access the dvc tracked data directory \r\n--> OSError: [Errno 30] Read-only file system: '\/data'",
        "Challenge_closed_time":1642070875000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1641729327000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the \"dvc-cc init\" command as it only takes the first three letters of the repository name for the DVC folder name. The user is also prompted to enter the remote DVC folder and username for accessing the DVC storage server.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/johannespischinger\/senti_anal\/issues\/11",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":2.07,
        "Challenge_repo_contributor_count":3.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":95.0,
        "Challenge_repo_star_count":2.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":94.8744444444,
        "Challenge_title":"data loading bug with dvc",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":23,
        "Discussion_body":"What command are you using? Note `\/data` is not same as `.\/data`",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1342713532568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seoul, South Korea",
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":4.4646297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have one Linux machine and one Windows machine for developments. For data sharing, we have set up a shared Windows directory in another Windows machine, which both my Linux and Windows can access.<\/p>\n<p>I am now using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> for version control of the shared data. To make it easy, I mount the shared Windows folder both in Windows and in Linux development machine. In Windows, it looks like<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>In Linux, it looks like:<\/p>\n<pre><code>[core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>As you can see, Windows and Linux have different mounting points. So my question is: is there a way to make that both Windows and Linux have the same <code>\u00f9rl<\/code> in the DVC configuration file?<\/p>\n<p>If this is impossible, is there another alternative solution for DVC keeps data in remote shared Windows folder? Thanks.<\/p>",
        "Challenge_closed_time":1641179335767,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641163263100,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has set up a shared Windows directory between a Linux and a Windows machine for data sharing. They are using DVC for version control of the shared data and have mounted the shared Windows folder in both Windows and Linux development machines. However, Windows and Linux have different mounting points, and the user is looking for a way to make both Windows and Linux have the same URL in the DVC configuration file. The user is seeking an alternative solution for DVC to keep data in remote shared Windows folder if this is impossible.",
        "Challenge_last_edit_time":1641199464667,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70560288",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":14.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":4.4646297222,
        "Challenge_title":"DVC Shared Windows Directory Setup",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":128.0,
        "Challenge_word_count":156,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1331553057367,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":10643.0,
        "Poster_view_count":504.0,
        "Solution_body":"<p>If you are using a local remote this way, you won't be able to have to the same <code>url<\/code> on both platforms since the mount points are different (as you already realized).<\/p>\n<p>The simplest way to configure this would be to pick one (Linux or Windows) <code>url<\/code> to use as your default case that gets git-committed into <code>.dvc\/config<\/code>. On the other platform you (or your users) can override that <code>url<\/code> in the local configuration file: <code>.dvc\/config.local<\/code>.<\/p>\n<p>(Note that <code>.dvc\/config.local<\/code> is a git-ignored file and will not be included in any commits)<\/p>\n<p>So if you wanted Windows to be the default case, in <code>.dvc\/config<\/code> you would have:<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>and on your Linux machine you would add the file <code>.dvc\/config.local<\/code> containing:<\/p>\n<pre><code>['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>See the DVC docs for <code>dvc config --local<\/code> and <code>dvc remote modify --local<\/code> for more details:<\/p>\n<ul>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/config#description<\/a><\/li>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.4,
        "Solution_reading_time":20.54,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":160.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1507661294190,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":98.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":28.2785163889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We would like to enforce specific security groups to be set on the SageMaker training jobs (XGBoost in script mode).\nHowever, distributed training, in this case, won\u2019t work out of the box, since the containers need to communicate with each other. What are the minimum inbound\/outbound rules (ports) that we need to specify for training jobs so that they can communicate?<\/p>",
        "Challenge_closed_time":1662835019252,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662733216593,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to add specific security groups to their Amazon SageMaker training jobs, but is facing challenges with distributed training as the containers need to communicate with each other. They are seeking information on the minimum inbound\/outbound rules (ports) required for communication during training.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73663585",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":5.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":28.2785163889,
        "Challenge_title":"Add Security groups in Amazon SageMaker for distributed training jobs",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":19.0,
        "Challenge_word_count":70,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1662621266503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":48.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>setting up training in VPC including specifying security groups is documented here:\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-vpc.html#train-vpc-groups\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-vpc.html#train-vpc-groups<\/a><\/p>\n<p>Normally you would allow all communication between the training nodes. To do this you specify the security group source and destination to the name of the security group itself, and allow all IPv4 traffic. If you want to figure out what ports are used, you could: 1\/ define the permissive security group. 2\/ Turn on VPC flow logs 3\/ run training. 4\/ examine VPC Flow logs 5\/ update the security group only to the required ports.<\/p>\n<p>I must say restricting communication between the training nodes might be an extreme, so I would challenge the customer why it's really needed, as all nodes carry the same job, have the same IAM role, and are transiate by nature.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.1,
        "Solution_reading_time":12.21,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":132.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":5.8410222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>Background<\/strong>: In my projects I'm using GIT and <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to keep track of versions:<\/p>\n<ul>\n<li>GIT - only for source codes<\/li>\n<li>DVC - for dataset, model objects and outputs<\/li>\n<\/ul>\n<p>I'm testing different approaches in separate branches, i.e:<\/p>\n<ul>\n<li>random_forest<\/li>\n<li>neural_network_1<\/li>\n<li>...<\/li>\n<\/ul>\n<p>Typically as an output I'm keeping predictions in csv file with standarised name (i.e.: pred_test.csv). As a consequence in different branches I've different pred_test.csv files. The structure of the file is very simple, it contains two columns:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction<\/li>\n<\/ul>\n<p><strong>Question<\/strong>: What is the best way to merge those prediction files into single big file?<\/p>\n<p>I would like to obtain a file with structure:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction_random_forest<\/li>\n<li>Prediction_neural_network_1<\/li>\n<li>Prediction_...<\/li>\n<\/ul>\n<p>My main issue is how to access files with predictions which are in different branches?<\/p>",
        "Challenge_closed_time":1645113091963,
        "Challenge_comment_count":5,
        "Challenge_created_time":1645092064283,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is using Git and DVC to keep track of versions of their project's source codes, datasets, model objects, and outputs in separate branches. They have different prediction CSV files in each branch and want to merge them into a single file with a standardized structure. The user is seeking advice on the best way to access files with predictions that are in different branches.",
        "Challenge_last_edit_time":1645132430276,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71155959",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":9.4,
        "Challenge_reading_time":14.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":5.8410222222,
        "Challenge_title":"How to merge data (CSV) files from multiple branches (Git and DVC)?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":274.0,
        "Challenge_word_count":141,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1265742671200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2735.0,
        "Poster_view_count":552.0,
        "Solution_body":"<p>I would try to use <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\" rel=\"nofollow noreferrer\"><code>dvc get<\/code><\/a> in this case:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc get -o random_forest_pred.csv --rev random_forest . pred_test.csv\n<\/code><\/pre>\n<p>It should bring the <code>pred_test.csv<\/code> from the <code>random_forest<\/code> branch.<\/p>\n<blockquote>\n<p>Mind the <code>.<\/code> before the <code>pred_test.csv<\/code> please, it's needed and it means that &quot;use the current repo&quot;, since <code>dvc get<\/code> could also be used on other repos (e.g. GitHub URL)<\/p>\n<\/blockquote>\n<p>Then I think you could use some CLI or write a script to join the files:<\/p>\n<p><a href=\"https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file\">https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.7,
        "Solution_reading_time":12.55,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":82.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1435524174732,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bursa, Turkey",
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":19.1911183333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an aws sagemaker end-point which need to be called from .Net core client, I have used the AWS SDK that deals with SageMaker and provided the required credentials however, always it keeps saying : <\/p>\n\n<p>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.<\/p>\n\n<p>var requestBody = \"{'url':'\"+\"<a href=\"https:\/\/cdn.pixabay.com\/photo\/2018\/05\/28\/22\/11\/message-in-a-bottle-3437294_960_720.jpg\" rel=\"nofollow noreferrer\">https:\/\/cdn.pixabay.com\/photo\/2018\/05\/28\/22\/11\/message-in-a-bottle-3437294_960_720.jpg<\/a>\" + \"'}\";<\/p>\n\n<pre><code>        var request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest()\n        {\n            EndpointName = \"CG-model-v1-endpoint\",\n            ContentType = \"application\/json;utf-8\",\n            Body = new MemoryStream(Encoding.ASCII.GetBytes(JsonConvert.SerializeObject(requestBody)))\n\n        };\n\n\n        var awsClient = new AmazonSageMakerRuntimeClient(awsAccessKeyId: \"XXXX\", awsSecretAccessKey: \"XXX\", region: RegionEndpoint.EUCentral1);\n\n        try\n        {\n            var resposnse = await awsClient.InvokeEndpointAsync(request);\n\n        }\n        catch (Exception ex)\n        {\n\n            return ApiResponse&lt;bool&gt;.Create(false);\n        }\n<\/code><\/pre>",
        "Challenge_closed_time":1563872105436,
        "Challenge_comment_count":3,
        "Challenge_created_time":1563803017410,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while calling an AWS SageMaker end-point from a .Net core client using the AWS SDK. The error message \"The request signature we calculated does not match the signature you provided\" is displayed despite providing the required credentials. The user has shared the code snippet used for the request.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57147396",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":16.4,
        "Challenge_reading_time":16.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":19.1911183333,
        "Challenge_title":"Amazon Sage Maker: How to authenticate AWS SageMaker End-Point Request",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":222.0,
        "Challenge_word_count":111,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1435524174732,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bursa, Turkey",
        "Poster_reputation_count":11.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>I found the error , it was simply because of the request content-type,it had to be application\/json instead of application\/json;utf-8<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.4,
        "Solution_reading_time":1.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.64844,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I came across <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-datasets#mount-vs-download\">this page<\/a> which describes how to work with AML datasets. I'm specifically interested in mounting. It states that &quot;Mounting is supported for Linux-based computes&quot;. Is there no way to do this on Windows?    <\/p>\n<p>Thanks,    <br \/>\nYordan    <\/p>",
        "Challenge_closed_time":1653382797127,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653326462743,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to mount an AML dataset on Windows but the instructions they found state that mounting is only supported for Linux-based computers. They are seeking clarification on whether there is a way to mount the dataset on Windows.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/860774\/mount-aml-dataset-on-windows",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":5.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":15.64844,
        "Challenge_title":"Mount AML dataset on Windows",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":46,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@YordanZaykov-7763 Yes, currently this is only supported for linux based computes for Azure ML. Windows only supports download option.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.3,
        "Solution_reading_time":6.76,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1370074627432,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Munich, Germany",
        "Answerer_reputation_count":51580.0,
        "Answerer_view_count":11462.0,
        "Challenge_adjusted_solved_time":1.0600122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using AWS SageMaker. I already used it before and I had no problems reading data from an S3 bucket.\nSo, I set up a new notebook instance and id this:<\/p>\n<pre><code>from sagemaker import get_execution_role\nrole = get_execution_role()\n\nbucket='my-bucket'\n\ndata_key = 'myfile.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n\ndf = pd.read_csv(data_location)\n<\/code><\/pre>\n<p>What I got is this:<\/p>\n<pre><code>PermissionError: Access Denied\n<\/code><\/pre>\n<p>Note: I checked the IAM Roles and also the policies and it seems to me that I have all the necessary rights to access the S3 bucket (AmazonS3FullAccess etc. are granted). What is different from the situation before is that my data is encrypted. Is there something I have to set up besides the roles?<\/p>\n<p>Edit:<\/p>\n<p>The role I use consist of three policies. These are<\/p>\n<ul>\n<li>AmazonS3FullAccess<\/li>\n<li>AmazonSageMakerFullAccess<\/li>\n<\/ul>\n<p>and an Execution Role where I added kms:encrypt and kms:decrypt. It looks like this one:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;xyz&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:PutObject&quot;,\n                &quot;s3:GetObject&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:DeleteObject&quot;,\n                &quot;kms:Encrypt&quot;,\n                &quot;kms:Decrypt&quot;\n            ],\n            &quot;Resource&quot;: &quot;arn:aws:s3:::*&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>Is there something missing?<\/p>",
        "Challenge_closed_time":1616081046907,
        "Challenge_comment_count":7,
        "Challenge_created_time":1616075760093,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a PermissionError while trying to read data from an encrypted S3 bucket using AWS SageMaker. The user has checked the IAM roles and policies and has confirmed that they have all the necessary rights to access the S3 bucket. The user's role consists of three policies, including AmazonS3FullAccess and AmazonSageMakerFullAccess, and an Execution Role where they added kms:encrypt and kms:decrypt. The user is seeking advice on whether there is anything else they need to set up besides the roles.",
        "Challenge_last_edit_time":1616077230863,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66692579",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":12.4,
        "Challenge_reading_time":19.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1.4685594445,
        "Challenge_title":"AWS SageMaker: PermissionError: Access Denied - Reading data from S3 bucket",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1327.0,
        "Challenge_word_count":172,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559131080072,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1166.0,
        "Poster_view_count":248.0,
        "Solution_body":"<p>You need to add (or modify) an IAM policy to grant access to the key the bucket uses for its encryption:<\/p>\n<pre><code>{\n  &quot;Sid&quot;: &quot;KMSAccess&quot;,\n  &quot;Action&quot;: [\n    &quot;kms:Decrypt&quot;\n  ],\n  &quot;Effect&quot;: &quot;Allow&quot;,\n  &quot;Resource&quot;: &quot;arn:aws:kms:example-region-1:123456789098:key\/111aa2bb-333c-4d44-5555-a111bb2c33dd&quot;\n}\n<\/code><\/pre>\n<p>Alternatively you can change the key policy of the KMS key directly to grant the sagemaker role access directly. <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-bucket-access-default-encryption\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-bucket-access-default-encryption\/<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":28.0,
        "Solution_reading_time":9.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":2.4924897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using a self-deployed ClearML server with the clearml-data CLI, I would like to manage (or view) my datasets in the WebUI as shown on the ClearML webpage (<a href=\"https:\/\/clear.ml\/mlops\/clearml-feature-store\/\" rel=\"nofollow noreferrer\">https:\/\/clear.ml\/mlops\/clearml-feature-store\/<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/2T8IC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2T8IC.png\" alt=\"ClearML feature store\" \/><\/a><\/p>\n<p>However, this feature does not show up in my Web UI. According to the pricing page, the feature store is not a premium feature. Do I need to configure my server in a special way to use this feature?<\/p>",
        "Challenge_closed_time":1615831186403,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615821875663,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in managing or viewing their datasets in the ClearML Web UI, despite using a self-deployed ClearML server with the clearml-data CLI. The feature store, which is supposed to enable this functionality, is not showing up in the user's Web UI. The user is unsure if any special configuration is required to use this feature.",
        "Challenge_last_edit_time":1615822213440,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66640850",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":9.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":2.5863166667,
        "Challenge_title":"How to manage datasets in ClearML Web UI?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":310.0,
        "Challenge_word_count":86,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1551960780550,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":354.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>Disclaimer: I'm part of the ClearML (formerly Trains) Team<\/p>\n<p>I think this screenshot is taken from the premium version...\nThe feature itself exists in the open-source version, but I &quot;think&quot; some of the dataset visualization capabilities are not available in the open-source self hosted version.<\/p>\n<p>Nonetheless, you have a fully featured feature-store, with the ability to add your own metrics \/ samples for every dataset\/feature version. The open-source version also includes the advanced versioning &amp; delta based storage for datasets\/features (i.e. only the change set from the parent version is stored)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":7.99,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":90.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":29.6355555556,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nThere are several areas in the code where we have an explicit check for the `neptune.amazonaws.com` DNS suffix; this is used to determine if we need to use Neptune-specific configuration options and request URI elements. \r\n\r\nHowever, these checks misidentify endpoints of Neptune clusters in AWS CN regions, which use the `neptune.<region>.amazonaws.com.cn` DNS suffix instead, as non-AWS endpoints. As a result, required config options such as `auth_mode` and `region` are not set correctly.\r\n\r\nAll of the following checks need to be changed to \"amazonaws.com\":\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/magics\/graph_magic.py#L160\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/neptune\/client.py#L129\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/configuration\/generate_config.py#L54\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/68e888def530be70e08b5250c8146292fb49cfa1\/src\/graph_notebook\/configuration\/get_config.py#L14",
        "Challenge_closed_time":1635986654000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1635879966000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a bug where there is missing documentation on how to connect to Neptune from a MacOS device. The user suggests adding the missing details to the documentation on connecting to Neptune via ssh-tunnel. One important missing detail is the need to create a host alias to ensure proper functionality.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/222",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":16.11,
        "Challenge_repo_contributor_count":25.0,
        "Challenge_repo_fork_count":129.0,
        "Challenge_repo_issue_count":493.0,
        "Challenge_repo_star_count":546.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":29.6355555556,
        "Challenge_title":"Configuration options not being set correctly when using CN region Neptune endpoint as host",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":103,
        "Discussion_body":"Resolved as of release 3.0.8",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":1423640080283,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Lyon, France",
        "Answerer_reputation_count":457.0,
        "Answerer_view_count":125.0,
        "Challenge_adjusted_solved_time":0.8004613889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a compute instance:<\/p>\n<p>Virtual machine size\nSTANDARD_DS3_V2 (4 Cores, 14 GB RAM, 28 GB Disk)<\/p>\n<p>Processing Unit\nCPU - General purpose<\/p>\n<p>But, I'm not able to access it when trying to set it for data drift monitoring.\nThe dropdown list is empty. I can't understand why. Can you help me please?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Pf10h.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Pf10h.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1601278985528,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601276103867,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a compute instance in Azure ML studio but is unable to access it when trying to set it for data drift monitoring as the \"compute target\" field is still blank. The dropdown list is empty and the user is seeking help to understand why.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64097278",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":7.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.8004613889,
        "Challenge_title":"Why is the field \"compute target\" for data drift monitoring in Azure ML studio still blank whereas I have a compute instance?",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":88.0,
        "Challenge_word_count":84,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423640080283,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lyon, France",
        "Poster_reputation_count":457.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>I found the answer. You must give a <strong>cluster<\/strong> compute instance to do data drift in Azure Machine Learning Studio. As it is not clear, I'm planning to add something in the documentation of Azure.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":2.67,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":35.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1522870754323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Redmond, WA, USA",
        "Answerer_reputation_count":366.0,
        "Answerer_view_count":173.0,
        "Challenge_adjusted_solved_time":405.5357452778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using azureml sdk in Azure Databricks.<\/p>\n<p>When I write the script for inference model (%%writefile script.py) in a databricks cell,\nI try to load a .bin file that I loaded in Azure Machine Learning Datasets.<\/p>\n<p>I would like to do this in the script.py:<\/p>\n<pre><code>fasttext.load_model(azuremldatasetpath)\n<\/code><\/pre>\n<p>How can I do to give good dataset path of my .bin file in azuremldatasetpath variable ? (Without calling workspace in the script).<\/p>\n<p>Something like:<\/p>\n<pre><code>dataset_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'file.bin')\n<\/code><\/pre>",
        "Challenge_closed_time":1645723579036,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644263650353,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in providing the correct dataset path for a .bin file that was loaded in Azure Machine Learning Datasets while writing an inference model script using AzureML SDK in Azure Databricks. They are seeking a solution to provide the correct dataset path without calling the workspace in the script.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71024584",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":8.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":405.5357452778,
        "Challenge_title":"How give azure machine learning dataset path in an inference script?",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":172.0,
        "Challenge_word_count":86,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1638189721320,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":151.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>You can use your model name with the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#azureml-core-model-model-get-model-path\" rel=\"nofollow noreferrer\">Model.get_model_path()<\/a> method to retrieve the path of the model file or files on the local file system. If you register a folder or a collection of files, this API returns the path of the directory that contains those files.<\/p>\n<p>More info you may want to refer: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#azureml_model_dir\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#azureml_model_dir<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":17.9,
        "Solution_reading_time":10.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":66.3502777778,
        "Challenge_answer_count":1,
        "Challenge_body":"My customer's 220 Gb of training data took 54 minutes for Sagemaker to download. This is a rate of only 70 MB\/s, which is unexpectedly slow. He is accessing the data in S3 from his p3.8xlarge instance through a private VPC endpoint, so the theoretical maximum bandwidth is 25 Gbps. Is there anything that can be done to speed up the download? \n\nHe started the Sagemaker training with the following function:\n\nestimator = Estimator(image_name, role=role, output_path=output_location,\n                      train_instance_count=1, train_instance_type='ml.p3.8xlarge',\n                     train_volume_size=300, train_max_run = 5*24*60*60 ,\n                     security_group_ids='sg-00f1529adc4076841')\n\nThe output was:\n2018-10-18 23:27:15 Starting - Starting the training job...\nLaunching requested ML instances......\nPreparing the instances for training...\n2018-10-18 23:29:15 Downloading - Downloading input data............\n....................................................................\n....................................................................\n....................................................................\n2018-10-19 00:23:50 Training - Downloading the training image..\n \nDataset download took ~54mins",
        "Challenge_closed_time":1540622900000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1540384039000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue where Sagemaker is taking an unexpectedly long time to download 220 GB of training data, with a rate of only 70 MB\/s. The user is accessing the data in S3 from a p3.8xlarge instance through a private VPC endpoint, with a theoretical maximum bandwidth of 25 Gbps. The user is looking for ways to speed up the download.",
        "Challenge_last_edit_time":1668610211872,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUPpqUS0ckRXCHW0BXgxV5wQ\/sagemaker-taking-an-unexpectedly-long-time-to-download-training-data",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":16.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":66.3502777778,
        "Challenge_title":"Sagemaker taking an unexpectedly long time to download training data",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1128.0,
        "Challenge_word_count":126,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"How are they connect to S3? are they using a VPC endpoint \/ NAT?\nIf they are using a VPC endpoint, My recommendation will be the open a support ticket, it's possible that support will be able to look at the network logs.\n\nAnother option for the customer is to use [pipe input](https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/), pipe mode is recommended for large datasets, and it'll shorter their startup time because the data is being streamed instead of being downloaded to your training instances.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925589024,
        "Solution_link_count":1.0,
        "Solution_readability":10.6,
        "Solution_reading_time":6.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":1.5416555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have around 10000 images in my S3 bucket. I need to cut each of these images to 12 smaller images and save them in another folder in the S3 bucket. I want to do this through the AWS Sagemaker. I am not able to read the image from the S3 bucket from my Sagemaker Jupter notebook. I have the code for cutting the images. <\/p>\n\n<p>Need help in reading images and storing them back into S3 from Sagemaker.Is it possible to do this, and also efficiently?<\/p>",
        "Challenge_closed_time":1559476844132,
        "Challenge_comment_count":2,
        "Challenge_created_time":1559410433010,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user needs to cut 10000 images in their S3 bucket into 12 smaller images and save them in another folder in the same bucket using AWS Sagemaker. However, they are facing challenges in reading the images from S3 in their Sagemaker Jupyter notebook and require assistance in efficiently storing them back into S3.",
        "Challenge_last_edit_time":1559471294172,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56408976",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.6,
        "Challenge_reading_time":6.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":18.4475338889,
        "Challenge_title":"How to read AWS S3 images from Sagemaker for processing",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":641.0,
        "Challenge_word_count":96,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495175078600,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Coimbatore, Tamil Nadu, India",
        "Poster_reputation_count":126.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>You can bring images to a local repo of your SageMaker instance (eg \/home\/ec2-user\/SageMaker\/Pics\/ with the following command:<\/p>\n\n<pre><code>aws s3 sync s3:\/\/pic_folder_in_s3 \/home\/ec2-user\/SageMaker\/Pics\n<\/code><\/pre>\n\n<p>or in python:<\/p>\n\n<pre><code>import subprocess as sb\n\nsb.call('aws s3 sync s3:\/\/pic_folder_in_s3 \/home\/ec2-user\/SageMaker\/Pics'.split())\n<\/code><\/pre>\n\n<p>Note that in order for the transfer to happen, the role carried by your SageMaker instance must have the right to read from this S3 location<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":6.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.9089091667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a compute instance:    <\/p>\n<p><strong>Virtual machine size<\/strong>    <br \/>\nSTANDARD_DS3_V2 (4 Cores, 14 GB RAM, 28 GB Disk)    <\/p>\n<p><strong>Processing Unit<\/strong>    <br \/>\nCPU - General purpose    <\/p>\n<p>But, I'm not able to access it when trying to set it for data drift monitoring.    <br \/>\nThe dropdown list is empty. I can't understand why. Can you help me please?    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/28475-datadrift.png?platform=QnA\" alt=\"28475-datadrift.png\" \/>    <\/p>",
        "Challenge_closed_time":1601279018463,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601275746390,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has created a compute instance in ML studio but is unable to access it when trying to set it for data drift monitoring as the \"compute target\" field is still blank and the dropdown list is empty. The user is seeking help to understand why this is happening.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/109397\/why-is-the-field-compute-target-for-data-drift-mon",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":8.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.9089091667,
        "Challenge_title":"Why is the field \"compute target\" for data drift monitoring in ML studio still blank whereas I have a compute instance?",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":84,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I found the answer. You must give a <strong>cluster<\/strong> compute instance to do data drift in Azure Machine Learning Studio.  <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":1.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1342713532568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seoul, South Korea",
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":1.6038955556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I've been working on the automation of processes and it includes fetching data from an external source through DVC(data version control) for which I am using SSH client to pull and push changes. For automation, I'm using <strong>Jenkins<\/strong> and the problem I'm facing is that for ssh we need to give a password on runtime, and in automation that's not an option. I've tried multiple ways to specify passwords for ssh like sshpass and ssh config but it turns out Jenkins when building creates some file name <strong>script.sh<\/strong> in a directory <em>repoName@tmp<\/em> in var\/lib\/jenkins\/.... and therefore it is giving permission denied error. no matter what I try. If anyone could give any suggestions to this problem it would be appreciated.<\/p>",
        "Challenge_closed_time":1611228029327,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611222255303,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with SSH automation in Jenkins while fetching data from an external source through DVC. The problem is that SSH requires a password at runtime, which is not possible in automation. The user has tried using sshpass and ssh config, but Jenkins creates a file named script.sh in a directory repoName@tmp in var\/lib\/jenkins\/..., resulting in a permission denied error. The user is seeking suggestions to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65824766",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":9.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.6038955556,
        "Challenge_title":"SSH automation in jenkins",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":124,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1493101921288,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pakistan",
        "Poster_reputation_count":133.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>You could use key-based auth for SSH instead instead of password auth so that your Jenkins user can access your SSH DVC remote without needing to specify a password.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":2.12,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":29.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1385461358920,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Spain",
        "Answerer_reputation_count":3017.0,
        "Answerer_view_count":373.0,
        "Challenge_adjusted_solved_time":2.8375658334,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>GCP has finally released managed Jupyter notebooks.  I would like to be able to interact with the notebook locally by connecting to it.  Ie. i use PyCharm to connect to the externaly configured jupyter notebbok server by passing its URL &amp; token param.<\/p>\n\n<p>Question also applies to AWS Sagemaker notebooks.<\/p>",
        "Challenge_closed_time":1555057761750,
        "Challenge_comment_count":0,
        "Challenge_created_time":1555047546513,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to remotely connect to GCP ML Engine and AWS Sagemaker managed notebooks using PyCharm by passing the notebook server's URL and token parameters.",
        "Challenge_last_edit_time":1561940059343,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55645119",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":8.0,
        "Challenge_reading_time":4.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.8375658334,
        "Challenge_title":"How to remotely connect to GCP ML Engine\/AWS Sagemaker managed notebooks?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":3701.0,
        "Challenge_word_count":60,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1298837928100,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Los Angeles, CA",
        "Poster_reputation_count":1367.0,
        "Poster_view_count":243.0,
        "Solution_body":"<p>On AWS, you can use AWS Glue to create a <a href=\"https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/dev-endpoint.html\" rel=\"nofollow noreferrer\">developer endpoint<\/a>, and then you create the Sagemaker notebook from there. A developer endpoint gives you access to connect to your python or Scala spark REPL via ssh, and it also allows you to tunnel the connection and access from any other tool, including PyCharm.<\/p>\n\n<p>For PyCharm professional we have even <a href=\"https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/dev-endpoint-tutorial-pycharm.html\" rel=\"nofollow noreferrer\">tighter integration<\/a>, allowing you to SFTP files and debug remotely.<\/p>\n\n<p>And if you need to install any dependencies on the notebook, apart from doing it directly on the notebook, you can always choose <code>new&gt;terminal<\/code> and you will have a connection to that machine directly from your jupyter environment where you can install <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">anything you want<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":15.0,
        "Solution_reading_time":13.57,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":125.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1589738451347,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":179.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":0.1002166667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am having trouble contacting an AMLS web service hosted on AKS in a vnet. I am able to successfully provision AKS and deploy the models, but I am not able to access the web service using the Python requests module:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>headers = {'Content-Type':'application\/json',\n           'Authorization': 'Bearer ' + &lt;AKS_KEY&gt;}\nresp = requests.post(&lt;AKS_URI&gt;, json={&quot;data&quot;:{&quot;x&quot;: &quot;1&quot;}}, headers=headers)\nprint(resp.text)\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<blockquote>\n<p>Error: HTTPConnectionPool(host='', port=80): Max retries exceeded with url: &lt;AKS_URL&gt; (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f33f6035a10&gt;: Failed to establish a new connection: [Errno 110] Connection timed out'))<\/p>\n<\/blockquote>\n<p>However, I am able to successfully connect to the web service using Postman:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>curl --location --request POST &lt;AKS_URI&gt; \\\n--header 'Authorization: Bearer &lt;AKS_KEY&gt;' \\\n--header 'Content-Type: application\/json' \\\n--data-raw '{&quot;data&quot;: {&quot;x&quot;: &quot;1&quot;}}'\n<\/code><\/pre>\n<p>If I load the AKS service in my AMLS workspace <code>aks_service.run()<\/code> also gives me the same error message. I don't have these problems when I deploy without vnet integration.<\/p>\n<p>What could be causing this?<\/p>",
        "Challenge_closed_time":1600397953580,
        "Challenge_comment_count":4,
        "Challenge_created_time":1600383486567,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble connecting to an AMLS web service hosted on AKS in a vnet using Python requests module. They are able to provision AKS and deploy the models, but not able to access the web service. The error message received is \"Max retries exceeded with url\" and \"Failed to establish a new connection\". However, they are able to connect to the web service using Postman. The same error message is received when loading the AKS service in AMLS workspace. The issue is not encountered when deploying without vnet integration.",
        "Challenge_last_edit_time":1600397592800,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63947132",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":13.9,
        "Challenge_reading_time":19.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":4.0186147222,
        "Challenge_title":"Trouble connecting to AMLS web service on AKS using Python requests",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":76.0,
        "Challenge_word_count":164,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589738451347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":179.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>I fixed this by adding an inbound security rule enabled for the scoring endpoint in the NSG group that controls the virtual network.<\/p>\n<p>This should be done so that the scoring endpoint can be called from outside the virtual network (see <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-inferencing-vnet\" rel=\"nofollow noreferrer\">documentation<\/a>), but apparently Postman can figure out how to access the endpoint without this security rule!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":6.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":72.28,
        "Challenge_answer_count":0,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nI have a problem saving xgboost run in mlflow server. The run has a status of UNFINISHED, no metrics or artifacts are created. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/101572186\/183577670-53398204-debf-428b-8b0c-3c7ca83f4785.png)\r\n\r\nWhen I use `mlflow ui` everything is fine, but when I run mlflow server with SQLite as backend store the problem occurs.\r\nCommand used to run mlflow server- `mlflow server --host 0.0.0.0 --port 5000 --default-artifact-root \/mlflow\/artifacts\/ --backend-store-uri sqlite:\/\/\/\/\/mlflow\/experiments\/mlflow.db`\n\n### Reproducible Example\n\n```python\nimport mlflow\r\nfrom pycaret.classification import *\r\nimport pandas as pd\r\n\r\nmlflow.set_tracking_uri('http:\/\/localhost:5000')\r\n\r\ndata = pd.DataFrame({'V1': [-1.34419, -1.89211, 1.69421, 0.263328, 0.107918, 0.154241, 0.33468, 1.447778, -0.918269, 0.86319, -1.630049, 1.643798, 1.274341, -1.296742, -0.193585, 1.627422, -0.66805, -1.664491, -1.86911, 0.892885],\r\n                     'V2': [0.85556, -1.70503, -0.02896, 1.746258, -0.084151, 1.673185, 1.113326, -0.23231, 1.054817, -1.407584, 0.474997, 0.150687, -0.738246, -0.045513, 1.58637, 0.984249, 0.624333, 0.298866, 0.662204, 0.967942],\r\n                     'V3': [1.768638, -0.503169, -0.25622, -0.937752, -0.062189, -0.820652, -1.786942, -1.770495, 1.808681, -0.280286, -1.389736, 0.182212, -0.602959, -0.354683, -1.065631, 1.649264, 0.389538, -1.674815, 0.281824, -1.683662],\r\n                     'V4': [1.512828, 1.177697, -1.156862, -1.877876, 1.526013, 1.644001, -1.282481, -0.720543, 0.323963, -1.931616, 1.632839, 1.706752, 1.895627, 1.860705, -1.559702, 1.517466, 1.254323, 1.84415, -1.175013, -1.600652],\r\n                     'V5': [0.820483, -1.20923, -0.012221, 1.682836, 0.104248, 1.258085, 0.404062, 0.18019, 1.352545, -0.497071, 0.771277, 1.614052, -0.693854, 0.002655, 0.277743, -0.977744, -0.97259, -1.501586, -0.731194, -0.551264],\r\n                     'V6': [1.079115, -0.734152, -1.630816, -1.877664, 1.577477, -1.902078, 1.012828, -1.107726, 1.742781, -1.338595, 1.788969, -0.851507, 1.061596, -0.635559, -1.171469, -1.001642, 1.493507, 0.732088, 1.565327, -1.845441],\r\n                     'V7': [1.165929, 1.804607, 0.886589, -0.027458, -1.444197, -0.415643, 0.863924, -1.177661, 1.684514, 1.023797, -1.234116, -0.989024, 0.815575, -0.668453, 0.591911, -0.798925, 1.024032, -1.983963, 1.900752, 1.201001],\r\n                     'V8': [-0.536923, 0.641581, -0.585228, 1.061145, -0.303192, -0.652068, 0.858556, 0.11012, 1.839738, -1.51798, -0.942028, -0.736386, -0.098261, 0.699127, 0.173854, -1.16775, -0.417662, 0.021639, 1.745042, -1.119667],\r\n                     'V9': [0.643498, -1.090347, 0.120182, -0.819219, -1.296763, 0.530723, -1.367664, -0.708116, -1.304274, 1.486166, 1.656498, 1.645308, -0.257558, 0.400849, 1.356781, 1.693433, 0.42606, 0.370683, -0.239278, -0.541334],\r\n                     'V10': [-0.744989, 0.506658, 1.15586, 1.461127, 1.928769, -0.330472, 1.514159, -1.209056, -0.741453, -1.479674, 1.92057, -1.148481, 0.949433, 0.674107, -1.410627, 1.497083, -1.262624, -0.856706, -1.708155, 0.93153],\r\n                     'V11': [0.967242, 1.968385, -1.362337, -0.46194, 0.809224, 0.226177, 1.782128, -0.114595, 0.698243, -0.141743, -0.117251, 1.762656, -0.068839, 0.648945, -1.497037, -1.455443, -0.291242, 1.806048, -1.945438, 0.251282],\r\n                     'V12': [0.010432, -0.101522, -1.764095, 1.326967, -1.299122, -0.549148, 0.807092, -0.75387, 0.955056, 0.640369, -0.917832, 0.250338, 0.624729, 1.566922, 0.118619, 1.907585, -0.919995, 0.868393, -1.103909, 0.347108],\r\n                     'V13': [0.122315, -1.140017, -0.876424, -1.075771, 0.668814, 1.916654, -0.864906, 0.132892, 0.740058, 0.94469, -0.260381, 0.92833, -1.186423, -0.18321, 1.99266, -0.779091, -1.649025, -1.688821, 1.075145, -1.988603],\r\n                     'V14': [-1.494, 0.679776, 0.813194, 1.8687, -0.20273, -0.363265, 1.98902, 0.100025, 1.462866, 0.561017, 0.418922, 1.981837, -1.834009, -1.657952, 0.585069, -0.898764, 0.683234, 0.743215, -0.050289, -0.668302], \r\n                     'V15': [0.199787, 0.81829, 1.200156, -1.684249, 0.847466, 1.326102, 0.323103, -1.010648, -1.868355, -1.204467, 1.777393, 0.375692, -1.654002, 0.50357, -1.372448, -0.522425, 0.360716, 1.007605, 1.009369, -0.353638],\r\n                     'V16': [1.535552, -0.082278, -0.083154, 0.069432, 1.356735, -0.042527, -0.462543, 1.813852, -1.664882, 0.408013, -1.802172, -1.920202, 1.987332, -1.126771, 1.485496, 1.972345, -0.33345, 1.414685, -0.06674, 1.383197],\r\n                     'V17': [-0.249929, 1.668129, 0.860046, 0.013955, 0.085628, 1.285539, -0.754444, -0.306815, -1.244118, -0.61328, 0.711952, 1.384674, 1.710264, 1.337836, -0.029678, -1.382343, -1.963618, 0.088497, -0.110544, 0.954066],\r\n                     'V18': [0.665032, -1.214589, 0.486172, 1.184611, 1.152936, -0.192168, -1.096281, -0.762198, -0.338583, 0.170551, -0.045797, -0.897271, 0.433204, -0.986375, 0.430157, 1.846751, -0.905146, -1.398763, 1.790667, -1.580808],\r\n                     'V19': [1.347637, -0.356925, 0.414118, 0.277104, 0.41587, -1.237646, 0.580625, 1.468221, -0.254781, 0.245683, -1.25356, 0.241325, 1.15677, -1.74525, 1.970698, -0.038675, -0.314979, 0.114507, 1.378524, -0.139709],\r\n                     'V20': [-1.291686, -1.714475, 0.012188, 1.002238, -1.587334, 1.408967, 1.055095, -1.356865, 1.307388, 0.697003, -0.112676, 1.762375, 0.82697, 1.084934, 1.656421, 0.786079, -1.580991, 1.753751, -0.242525, 1.854008],\r\n                     'Class': [1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1]})\r\n\r\nsetup(data = data,\r\ntarget = 'Class', \r\nexperiment_name = 'xgb_test', \r\nfix_imbalance = True,\r\nlog_experiment = True, \r\nsilent=True, \r\nuse_gpu=True,\r\nfold=5,\r\npreprocess=False)\r\n\r\nmodels = ['xgboost','knn','rf']\r\ntop_models = compare_models(include = model)\r\ndd = pull()\n```\n\n\n### Expected Behavior\n\nArtifacts and metrics should be crated. \n\n### Actual Results\n\n```python-traceback\nError from logs.log:\r\n\r\n2022-08-09 06:11:05,384:ERROR:dashboard_logger.log_model() for XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\r\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\r\n              early_stopping_rounds=None, enable_categorical=False,\r\n              eval_metric=None, feature_types=None, gamma=0, gpu_id=0,\r\n              grow_policy='depthwise', importance_type=None,\r\n              interaction_constraints='', learning_rate=0.300000012,\r\n              max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=6,\r\n              max_leaves=0, min_child_weight=1, missing=nan,\r\n              monotone_constraints='()', n_estimators=100, n_jobs=-1,\r\n              num_parallel_tree=1, objective='binary:logistic',\r\n              predictor='auto', random_state=989, ...) raised an exception:\r\n2022-08-09 06:11:05,385:ERROR:Traceback (most recent call last):\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/internal\/tabular.py\", line 2362, in compare_models\r\n    dashboard_logger.log_model(\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/loggers\/__init__.py\", line 93, in log_model\r\n    logger.log_params(params, model_name=full_name)\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/loggers\/mlflow_logger.py\", line 46, in log_params\r\n    mlflow.log_params(params)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/fluent.py\", line 675, in log_params\r\n    MlflowClient().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[])\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/client.py\", line 918, in log_batch\r\n    self._tracking_client.log_batch(run_id, metrics, params, tags)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 315, in log_batch\r\n    self.store.log_batch(\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 309, in log_batch\r\n    self._call_endpoint(LogBatch, req_body)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/utils\/rest_utils.py\", line 256, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/utils\/rest_utils.py\", line 185, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Invalid value [{'key': 'objective', 'value': 'binary:logistic'}, {'key': 'use_label_encoder', 'value': 'None'}, {'key': 'base_score', 'value': '0.5'}, {'key': 'booster', 'value': 'gbtree'}, {'key': 'callbacks', 'value': 'None'}, {'key': 'colsample_bylevel', 'value': '1'}, {'key': 'colsample_bynode', 'value': '1'}, {'key': 'colsample_bytree', 'value': '1'}, {'key': 'early_stopping_rounds', 'value': 'None'}, {'key': 'enable_categorical', 'value': 'False'}, {'key': 'eval_metric', 'value': 'None'}, {'key': 'feature_types', 'value': 'None'}, {'key': 'gamma', 'value': '0'}, {'key': 'gpu_id', 'value': '0'}, {'key': 'grow_policy', 'value': 'depthwise'}, {'key': 'importance_type', 'value': 'None'}, {'key': 'interaction_constraints', 'value': ''}, {'key': 'learning_rate', 'value': '0.300000012'}, {'key': 'max_bin', 'value': '256'}, {'key': 'max_cat_to_onehot', 'value': '4'}, {'key': 'max_delta_step', 'value': '0'}, {'key': 'max_depth', 'value': '6'}, {'key': 'max_leaves', 'value': '0'}, {'key': 'min_child_weight', 'value': '1'}, {'key': 'missing', 'value': 'nan'}, {'key': 'monotone_constraints', 'value': '()'}, {'key': 'n_estimators', 'value': '100'}, {'key': 'n_jobs', 'value': '-1'}, {'key': 'num_parallel_tree', 'value': '1'}, {'key': 'predictor', 'value': 'auto'}, {'key': 'random_state', 'value': '989'}, {'key': 'reg_alpha', 'value': '0'}, {'key': 'reg_lambda', 'value': '1'}, {'key': 'sampling_method', 'value': 'uniform'}, {'key': 'scale_pos_weight', 'value': '1'}, {'key': 'subsample', 'value': '1'}, {'key': 'tree_method', 'value': 'gpu_hist'}, {'key': 'validate_parameters', 'value': '1'}, {'key': 'verbosity', 'value': '0'}] for parameter 'params' supplied. Hint: Value was of type 'list'. See the API docs for more information about request parameters.\n```\n\n\n### Installed Versions\n\n<details>\r\npycaret- Version: 2.3.10 <\/br>\r\nmlflow- Version: 1.27.0 <\/br>\r\nxgboost-  Version: 2.0.0.dev0 <\/br>\r\n<\/details>\r\n",
        "Challenge_closed_time":1660286399000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1660026191000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while running \"mlflow ui\" which results in a \"FileNotFoundError\". The expected behavior is for it to run without any issues. The version being used is 2.3.10.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2838",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":137.91,
        "Challenge_repo_contributor_count":105.0,
        "Challenge_repo_fork_count":1603.0,
        "Challenge_repo_issue_count":2975.0,
        "Challenge_repo_star_count":7363.0,
        "Challenge_repo_watch_count":128.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":66,
        "Challenge_solved_time":72.28,
        "Challenge_title":"[BUG]: MLflow server integration",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":925,
        "Discussion_body":"With new mlflow release-1.28.0- and **[Tracking \/ Model Registry] Fix an mlflow server bug that rejected parameters and tags with empty string values (https:\/\/github.com\/mlflow\/mlflow\/pull\/6179, @dbczumar)** bug fixed, the problem no longer occurs and artifacts are saved correctly",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.0041666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I see in this page of documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-interface-endpoint.html that:\n>\"*You can connect to your notebook instance from your VPC through an interface endpoint in your Virtual Private Cloud (VPC) instead of connecting over the internet. When you use a VPC interface endpoint, communication between your VPC and the notebook instance is conducted entirely and securely within the AWS network.*\"\n\nHow would customer interact on their laptop with the UI of a notebook instance sitting in a VPC?",
        "Challenge_closed_time":1541516577000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541494962000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to connect to a SageMaker Notebook instance through a VPC interface endpoint instead of over the internet. They are specifically asking how they can interact with the UI of the notebook instance on their laptop while it is located in a VPC.",
        "Challenge_last_edit_time":1668438538512,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU5z-7bQ9zQOi_NrVlHy_5oA\/which-connection-method-when-using-sagemaker-notebook-through-vpc-interface-endpoint",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":8.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":6.0041666667,
        "Challenge_title":"Which connection method when using SageMaker Notebook through VPC Interface Endpoint?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":466.0,
        "Challenge_word_count":88,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"If you are trying to access from within VPC, you'll have a direct connection. Otherwise, you'll need a configuration in place, such as Amazon VPN or AWS Direct Connect, to connect to your notebooks. Here is the blog post where we tried to explain how to set up AWS PrivateLink for Amazon SageMaker notebooks: https:\/\/aws.amazon.com\/blogs\/machine-learning\/direct-access-to-amazon-sagemaker-notebooks-from-amazon-vpc-by-using-an-aws-privatelink-endpoint\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925550451,
        "Solution_link_count":1.0,
        "Solution_readability":14.0,
        "Solution_reading_time":5.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1550779047856,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":363.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":4538.5667388889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am getting the below error.\nDoes anyone have any idea how to solve it?<\/p>\n<pre><code>Failed to create pipeline job. Error: Vertex AI Service Agent \n'XXXXX@gcp-sa-aiplatform-cc.iam.gserviceaccount.com' should be granted\n access to the image gcr.io\/gcp-project-id\/application:latest\n<\/code><\/pre>",
        "Challenge_closed_time":1659424830192,
        "Challenge_comment_count":3,
        "Challenge_created_time":1643025892210,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to create a pipeline job in GCP Vertex AI Service Agent. The error message indicates that the service agent needs to be granted access to the GCR image 'gcr.io\/gcp-project-id\/application:latest'.",
        "Challenge_last_edit_time":1643085989932,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70833594",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":8.5,
        "Challenge_reading_time":4.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":4555.2605505556,
        "Challenge_title":"GCP Vertex AI Service Agent access to GCR image Error",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":707.0,
        "Challenge_word_count":44,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501131989640,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":329.0,
        "Poster_view_count":88.0,
        "Solution_body":"<p><code>{PROJECT_NUMBER}@gcp-sa-aiplatform-cc.iam.gserviceaccount.com<\/code> is google's <a href=\"https:\/\/cloud.google.com\/iam\/docs\/service-agents\" rel=\"nofollow noreferrer\">AI Platform service agent<\/a>.\nThis Service agent requires access to read\/pull the docker image from your project's gcr to create container for pipeline run.<\/p>\n<p>If You have permission to edit <a href=\"https:\/\/cloud.google.com\/iam\/docs\/understanding-roles\" rel=\"nofollow noreferrer\">IAM roles<\/a>, You can try adding <a href=\"https:\/\/cloud.google.com\/artifact-registry\/docs\/access-control#roles\" rel=\"nofollow noreferrer\">Artifact Registry roles<\/a> to the above service agent.\nYou can start with adding <code>roles\/artifactregistry.reader<\/code>.<\/p>\n<p>Hope this helps :)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.8,
        "Solution_reading_time":10.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":66.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":58.1621369444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a CDM folder with data coming from Dynamics 365 Business Central.  <br \/>\nI need to do some data cleaning\/preprocessing and then apply my models on that data, but I didn't find a proper way to read CDM folders.  <br \/>\nI found some code on the Microsoft github repository, but is marked as obsolete.  <\/p>\n<p><a href=\"https:\/\/github.com\/Azure-Samples\/cdm-azure-data-services-integration\">Azure-Samples\/cdm-azure-data-services-integration<\/a>  <\/p>\n<p>I'm searching for something like the <strong>Apache Spark CDM connector<\/strong> but to use within Azure Machine Learning service.  <\/p>\n<p>ps: I know that is possible to copy\/transform files with <em>Azure Data Factory<\/em> and that is supports CDM folders too, but is not what I want. I want to read CDM folder from python, do my stuff (data cleaning, preprocessing, applying models, ecc) then save the results.  <\/p>\n<p>Is there any way?   <br \/>\nAny advice is welcome.  <\/p>\n<p>Thanks.  <\/p>",
        "Challenge_closed_time":1620687553120,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620478169427,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in reading data from a CDM folder that contains data from Dynamics 365 Business Central. They need to perform data cleaning and preprocessing before applying their models, but they have not found a proper way to read CDM folders. The user is searching for a solution similar to the Apache Spark CDM connector to use within Azure Machine Learning service. They do not want to use Azure Data Factory to copy\/transform files. The user is seeking advice and any possible solutions.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/387697\/read-data-from-cdm-folder",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":12.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":58.1621369444,
        "Challenge_title":"Read data from CDM folder",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":142,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. The data source you specified isn't a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#supported-data-storage-service-types\">supported storage type<\/a> in AML. If you're using unsupported storage, we recommend that you <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/quickstart-create-data-factory-copy-data-tool\">move<\/a> your data to supported Azure storage solutions. Currently, we don't have a python connector for connecting to CRMs. A workaround would be to load your data to a database and connect to the database using python. Hope this helps, sorry for any inconvenience.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.6,
        "Solution_reading_time":8.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1500629225150,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5939.0,
        "Answerer_view_count":886.0,
        "Challenge_adjusted_solved_time":0.9110955556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is the terraform shown in the docs:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.example.id\n  }\n}\n<\/code><\/pre>\n<p>I created a service catalog product with id: &quot;prod-xxxxxxxxxxxxx&quot;.\nWhen I substitute the service catalog product id into the above template,\nto get the following:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.prod-xxxxxxxxxxxxx\n  }\n}\n<\/code><\/pre>\n<p>I run terraform plan, but the following error occurs:<\/p>\n<pre><code>A managed resource &quot;aws_servicecatalog_product&quot; &quot;prod-xxxxxxxxxxxxx&quot; has not been declared in the root module.\n\n<\/code><\/pre>\n<p>What do I need to do to fix this error?<\/p>",
        "Challenge_closed_time":1654269294487,
        "Challenge_comment_count":2,
        "Challenge_created_time":1654264489180,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create an AWS Sagemaker project using Terraform, but encounters an error stating that a managed resource for the service catalog product has not been declared in the root module. The user is seeking guidance on how to fix this error.",
        "Challenge_last_edit_time":1654266014543,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72490682",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":16.3,
        "Challenge_reading_time":13.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.3348075,
        "Challenge_title":"How to create an aws sagemaker project using terraform?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":298.0,
        "Challenge_word_count":92,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1653511725307,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>Since the documentation is lacking a bit of clarity, in order to have this work as in the example, you would first have to create the Service Catalog product in Terraform as well, e.g.:<\/p>\n<pre><code>resource &quot;aws_servicecatalog_product&quot; &quot;example&quot; {\n  name  = &quot;example&quot;\n  owner = [aws_security_group.example.id] # &lt;---- This would need to be created first\n  type  = aws_subnet.main.id # &lt;---- This would need to be created first\n\n  provisioning_artifact_parameters {\n    template_url = &quot;https:\/\/s3.amazonaws.com\/cf-templates-ozkq9d3hgiq2-us-east-1\/temp1.json&quot;\n  }\n\n  tags = {\n    foo = &quot;bar&quot;\n  }\n}\n<\/code><\/pre>\n<p>You can reference it then in the SageMaker project the same way as in the example:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.example.id\n  }\n}\n<\/code><\/pre>\n<p>Each of the resources that gets created has a set of attributes that can be accessed as needed by other resources, data sources or outputs. In order to understand how this works, I strongly suggest reading the documentation about referencing values [1]. Since you already created the Service Catalog product, the only thing you need to do is provide the string value for the product ID:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = &quot;prod-xxxxxxxxxxxxx&quot;\n  }\n}\n<\/code><\/pre>\n<p>When I can't understand what value is expected by an argument (e.g., <code>product_id<\/code> in this case), I usually read the docs and look for examples like in [2]. Note: That example is CloudFormation, but it can help you understand what type of a value is expected (e.g., string, number, bool).<\/p>\n<p>You could also import the created Service Catalog product into Terraform so you can manage it with IaC [3]. You should understand all the implications of <code>terraform import<\/code> though before trying it [4].<\/p>\n<hr \/>\n<p>[1] <a href=\"https:\/\/www.terraform.io\/language\/expressions\/references\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/expressions\/references<\/a><\/p>\n<p>[2] <a href=\"https:\/\/docs.amazonaws.cn\/en_us\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-project.html#aws-resource-sagemaker-project--examples--SageMaker_Project_Example\" rel=\"nofollow noreferrer\">https:\/\/docs.amazonaws.cn\/en_us\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-project.html#aws-resource-sagemaker-project--examples--SageMaker_Project_Example<\/a><\/p>\n<p>[3] <a href=\"https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/servicecatalog_product#import\" rel=\"nofollow noreferrer\">https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/servicecatalog_product#import<\/a><\/p>\n<p>[4] <a href=\"https:\/\/www.terraform.io\/cli\/commands\/import\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/cli\/commands\/import<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":9.0,
        "Solution_readability":17.8,
        "Solution_reading_time":40.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":270.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":137.3216258333,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Can you please help with content for hf-fastai2<\/p>",
        "Challenge_closed_time":1644114659104,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643620301251,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking help with content for hf-fastai2, possibly indicating difficulty accessing a study group or resources related to it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/access-to-study-group\/1850",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":4.3,
        "Challenge_reading_time":0.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":137.3216258333,
        "Challenge_title":"Access to study group",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":163.0,
        "Challenge_word_count":11,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Slides can be found in fastai <a href=\"https:\/\/discord.com\/channels\/689892369998676007\/859175939368026162\/937472311836176425\" rel=\"noopener nofollow ugc\">discord.<\/a><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.5,
        "Solution_reading_time":2.39,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1454844135036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"T\u00fcrkiye",
        "Answerer_reputation_count":462.0,
        "Answerer_view_count":83.0,
        "Challenge_adjusted_solved_time":151.3788311111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying out <strong>Amazon Sagemaker<\/strong>, I haven't figured out how we can have Continuous training.\n<br>\nFor example if i have a CSV file in s3 and I want to train each time the CSV file is updated.<\/p>\n\n<p>I know we can go again to the notebook and re-run the whole notebook to make this happen.\n<br>\nBut i am looking for an automated way, with some python scripts or using a lambda function with s3 events etc<\/p>",
        "Challenge_closed_time":1530199631932,
        "Challenge_comment_count":3,
        "Challenge_created_time":1529654311630,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in implementing continuous training in Amazon Sagemaker. They are looking for an automated way to train the model each time a CSV file in S3 is updated, instead of manually re-running the notebook. They are considering using Python scripts or a Lambda function with S3 events.",
        "Challenge_last_edit_time":1529654668140,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50983316",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":7.3,
        "Challenge_reading_time":5.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":151.4778616667,
        "Challenge_title":"Continuous Training in Sagemaker",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1191.0,
        "Challenge_word_count":82,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1440734188430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":1491.0,
        "Poster_view_count":112.0,
        "Solution_body":"<p>You can use boto3 sdk for python to start training on lambda then you need to trigger the lambda when csv is update.<\/p>\n\n<blockquote>\n  <p><a href=\"http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/sagemaker.html\" rel=\"nofollow noreferrer\">http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/sagemaker.html<\/a><\/p>\n<\/blockquote>\n\n<p>Example python code<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a><\/p>\n<\/blockquote>\n\n<p>Addition: You dont need to use lambda you just start\/cronjob the python script any kind of instance which has python and aws sdk in it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.0,
        "Solution_reading_time":10.3,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.7209733333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello, My project\u2019s project need some data from user feedback. There will be some missing value, but when I try it I find the result is not reliable. What is the fundamental idea? What is the best setting? I just want to make the result constantly.<\/p>",
        "Challenge_closed_time":1675101477624,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675098882120,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on the fundamental idea of missing value cleanse and the best setting to ensure reliable results when dealing with missing data in user feedback.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1165512\/what-is-the-fundamental-idea-of-missing-value-clea",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":3.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.7209733333,
        "Challenge_title":"What is the fundamental idea of missing value cleanse?",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":54,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=3d56ad4b-236f-4005-8b59-e187ae456696\">Haans<\/a> <\/p>\n<p>Thanks for reaching out to us. For Clean Missing Value component, please refer to this document - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/component-reference\/clean-missing-data\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/component-reference\/clean-missing-data<\/a><\/p>\n<p>Use this component to <strong>remove, replace, or infer missing values.<\/strong><\/p>\n<p>Data scientists often check data for missing values and then perform various operations to fix the data or insert new values. The goal of such cleaning operations is to prevent problems caused by missing data that can arise when training a model.<\/p>\n<p>This component supports multiple types of operations for &quot;cleaning&quot; missing values, including:<\/p>\n<ul>\n<li> Replacing missing values with a placeholder, mean, or other value<\/li>\n<li> Completely removing rows and columns that have missing values<\/li>\n<li> Inferring values based on statistical methods<\/li>\n<\/ul>\n<p>Using this component does not change your source dataset. Instead, it creates a new dataset in your workspace that you can use in the subsequent workflow. You can also save the new, cleaned dataset for reuse.<\/p>\n<p>This component also outputs a definition of the transformation used to clean the missing values. You can re-use this transformation on other datasets that have the same schema, by using the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/component-reference\/apply-transformation\">Apply Transformation<\/a> component.<\/p>\n<p>The component returns two outputs:<\/p>\n<ul>\n<li> <strong>Cleaned dataset<\/strong>: A dataset comprised of the selected columns, with missing values handled as specified, along with an indicator column, if you selected that option.\n   Columns not selected for cleaning are also &quot;passed through&quot;.<\/li>\n<li> <strong>Cleaning transformation<\/strong>: A data transformation used for cleaning, that can be saved in your workspace and applied to new data later.<\/li>\n<\/ul>\n<p>If you don't want to the missing data to effect the result a lot, you may try mean as an option.<\/p>\n<p>I hope this helps!<\/p>\n<p>Regards,<\/p>\n<p>Yutong<\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.0,
        "Solution_reading_time":30.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":294.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":24.0520611111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Can we connect Azure ML Notebooks directly to Snowflake using Private end-points, my ML Workspace is inside a VNet.<\/p>",
        "Challenge_closed_time":1653034845860,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652948258440,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to know if it is possible to connect Azure ML Notebooks to Snowflake using Private endpoints, as their ML Workspace is located inside a VNet.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/855820\/connect-azure-ml-with-snowflake-using-private-endp",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.2,
        "Challenge_reading_time":2.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":24.0520611111,
        "Challenge_title":"Connect Azure ML with Snowflake using Private endpoint?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=03343194-9922-4c28-abc7-1d7c46b6d2d6\">@Varun  <\/a>     <\/p>\n<p>Thanks for reaching out to us, currently there is no internal way in Azure Machine Learning Studio to connect to Snowflake. I am sorry for all inconveniences.     <\/p>\n<p>But you can run a  Python 3 code to use the Snowflake python connector - <a href=\"https:\/\/docs.snowflake.com\/en\/user-guide\/python-connector.html\">https:\/\/docs.snowflake.com\/en\/user-guide\/python-connector.html<\/a>    <\/p>\n<p>With Azure ML Studio, there's no built-in support for SnowFlake, I will forward your feedback to product group to see if there any plan in the future.     <\/p>\n<p>Hope this helps!    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p><em>-Please kindly accept the answer if you feel helpful, thanks a lot for supporting the community.<\/em>     <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.0,
        "Solution_reading_time":10.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":103.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":1.0024183334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I Want to use <strong>Sagemaker KMeans BuilIn Algorithm<\/strong> in one of my applications. I have a large CSV file in S3 (raw data) that I split into several parts to be easy to clean. Before I had cleaned, I tried to use it as the input of Kmeans to perform the training job but It doesn't work.<\/p>\n\n<p>My manifest file:<\/p>\n\n<pre><code>[\n    {\"prefix\": \"s3:\/\/&lt;BUCKET_NAME&gt;\/kmeans_data\/KMeans-2019-28-07-13-40-00-001\/\"}, \n    \"file1.csv\", \n    \"file2.csv\"\n]\n<\/code><\/pre>\n\n<p>The error I've got:<\/p>\n\n<pre><code>Failure reason: ClientError: Unable to read data channel 'train'. Requested content-type is 'application\/x-recordio-protobuf'. Please verify the data matches the requested content-type. (caused by MXNetError) Caused by: [16:47:31] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.1620.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100: (Input Error) The header of the MXNet RecordIO record at position 0 in the dataset does not start with a valid magic number. Stack trace returned 10 entries: [bt] (0) \/opt\/amazon\/lib\/libaialgs.so(+0xb1f0) [0x7fb5674c31f0] [bt] (1) \/opt\/amazon\/lib\/libaialgs.so(+0xb54a) [0x7fb5674c354a] [bt] (2) \/opt\/amazon\/lib\/libaialgs.so(aialgs::iterator_base::Next()+0x4a6) [0x7fb5674cc436] [bt] (3) \/opt\/amazon\/lib\/libmxnet.so(MXDataIterNext+0x21) [0x7fb54ecbcdb1] [bt] (4) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call_unix64+0x4c) [0x7fb567a1e858] [bt] (5) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call+0x15f) [0x7fb567a1d95f\n<\/code><\/pre>\n\n<p>My question is: It's possible to use multiple CSV files as input in Sagemaker KMeans BuilIn Algorithm only in GUI? If it's possible, How should I format my manifest?<\/p>",
        "Challenge_closed_time":1564340123623,
        "Challenge_comment_count":0,
        "Challenge_created_time":1564336514917,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to use Sagemaker KMeans Built-In Algorithm in their application and has a large CSV file in S3 that they split into several parts. However, when they tried to use it as input for KMeans, they encountered an error related to the content-type. The user is asking if it's possible to use multiple CSV files as input in Sagemaker KMeans Built-In Algorithm and how to format the manifest file.",
        "Challenge_last_edit_time":1564342604716,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57243583",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":23.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1.0024183334,
        "Challenge_title":"Sagemaker KMeans Built-In - List of files csv as input",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":522.0,
        "Challenge_word_count":188,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1464391892936,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Poster_reputation_count":2243.0,
        "Poster_view_count":148.0,
        "Solution_body":"<p>the manifest looks fine, but based on the error message, it looks like you haven't set the right data format for you S3 data. It's expecting protobuf, which is the default format :)<\/p>\n\n<p>You have to set the CSV data format explicitly. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.s3_input\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.s3_input<\/a>. <\/p>\n\n<p>It should look something like this:<\/p>\n\n<pre><code>s3_input_train = sagemaker.s3_input(\n  s3_data='s3:\/\/{}\/{}\/train\/manifest_file'.format(bucket, prefix),    \n  s3_data_type='ManifestFile',\n  content_type='csv')\n\n...\n\nkmeans_estimator = sagemaker.estimator.Estimator(kmeans_image, ...)\nkmeans_estimator.set_hyperparameters(...)\n\ns3_data = {'train': s3_input_train}\nkmeans_estimator.fit(s3_data)\n<\/code><\/pre>\n\n<p>Please note the KMeans estimator in the SDK only supports protobuf, see <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/kmeans.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/kmeans.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":22.3,
        "Solution_reading_time":14.62,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":84.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1388584380832,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":396.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":157.3425175,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm researching the use of MLflow as part of our data science initiatives and I wish to set up a minimum working example of remote execution on databricks from windows.<\/p>\n\n<p>However, when I perform the remote execution a path is created locally on windows in the MLflow package which is sent to databricks. This path specifies the upload location of the '.tar.gz' file corresponding to the Github repo containing the MLflow Project. In cmd this has a combination of '\\' and '\/', but on databricks there are no separators at all in this path, which raises the 'rsync: No such file or directory (2)' error.<\/p>\n\n<p>To be more general, I reproduced the error using an MLflow standard example and following this <a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/projects.html\" rel=\"nofollow noreferrer\">guide<\/a> from databricks. The MLflow example is the <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/sklearn_elasticnet_wine\" rel=\"nofollow noreferrer\">sklearn_elasticnet_wine<\/a>, but I had to add a default value to a parameter so I forked it and the MLproject which can be executed remotely can be found at (<a href=\"https:\/\/github.com\/aestene\/mlflow\/tree\/master\/examples\/sklearn_elasticnet_wine\" rel=\"nofollow noreferrer\">forked repo<\/a>).<\/p>\n\n<p>The Project can be executed remotely by the following command (assuming a databricks instance has been set up)<\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/aestene\/mlflow#examples\/sklearn_elasticnet_wine -b databricks -c db-clusterconfig.json --experiment-id &lt;insert-id-here&gt;\n<\/code><\/pre>\n\n<p>where \"db-clusterconfig.json\" correspond to the cluster to set up in databricks and is in this example set to<\/p>\n\n<pre><code>{\n    \"autoscale\": {\n        \"min_workers\": 1,\n        \"max_workers\": 2\n    },\n    \"spark_version\": \"5.5.x-scala2.11\",\n    \"node_type_id\": \"Standard_DS3_v2\",\n    \"driver_node_type_id\": \"Standard_DS3_v2\",\n    \"ssh_public_keys\": [],\n    \"custom_tags\": {},\n    \"spark_env_vars\": {\n        \"PYSPARK_PYTHON\": \"\/databricks\/python3\/bin\/python3\"\n    }\n}\n<\/code><\/pre>\n\n<p>When running the project remotely, this is the output in cmd:<\/p>\n\n<pre><code>2019\/10\/04 10:09:50 INFO mlflow.projects: === Fetching project from https:\/\/github.com\/aestene\/mlflow#examples\/sklearn_elasticnet_wine into C:\\Users\\ARNTS\\AppData\\Local\\Temp\\tmp2qzdyq9_ ===\n2019\/10\/04 10:10:04 INFO mlflow.projects.databricks: === Uploading project to DBFS path \/dbfs\\mlflow-experiments\\3947403843428882\\projects-code\\aa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz ===\n2019\/10\/04 10:10:05 INFO mlflow.projects.databricks: === Finished uploading project to \/dbfs\\mlflow-experiments\\3947403843428882\\projects-code\\aa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz ===\n2019\/10\/04 10:10:05 INFO mlflow.projects.databricks: === Running entry point main of project https:\/\/github.com\/aestene\/mlflow#examples\/sklearn_elasticnet_wine on Databricks ===\n2019\/10\/04 10:10:06 INFO mlflow.projects.databricks: === Launched MLflow run as Databricks job run with ID 8. Getting run status page URL... ===\n2019\/10\/04 10:10:18 INFO mlflow.projects.databricks: === Check the run's status at https:\/\/&lt;region&gt;.azuredatabricks.net\/?o=&lt;databricks-id&gt;#job\/8\/run\/1 ===\n<\/code><\/pre>\n\n<p>Where the DBFS path has a leading '\/' before the remaining are '\\'. <\/p>\n\n<p>The command spins up a cluster in databricks and is ready to execute the job, but ends up with the following error message on the databricks side:<\/p>\n\n<pre><code>rsync: link_stat \"\/dbfsmlflow-experiments3947403843428882projects-codeaa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz\" failed: No such file or directory (2)\nrsync error: some files\/attrs were not transferred (see previous errors) (code 23) at main.c(1183) [sender=3.1.1]\n<\/code><\/pre>\n\n<p>Where we can see the same path but without the '\\' inserted. I narrowed down the creation of this path to this <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/projects\/databricks.py\" rel=\"nofollow noreferrer\">file<\/a> in the MLflow Github repo, where the following code creates the path (line 133):<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>dbfs_path = os.path.join(DBFS_EXPERIMENT_DIR_BASE, str(experiment_id),\n                                     \"projects-code\", \"%s.tar.gz\" % tarfile_hash)\ndbfs_fuse_uri = os.path.join(\"\/dbfs\", dbfs_path)\n<\/code><\/pre>\n\n<p>My current hypothesis is that <code>os.path.join()<\/code> in the first line joins the string together in a \"windows fashion\" such that they have backslashes. Then the following call to <code>os.path.join()<\/code> adds a '\/'. The databricks file system is then unable to handle this path and something causes the 'tar.gz' file to not be properly uploaded or to be accessed at the wrong path. <\/p>\n\n<p>It should also be mentioned that the project runs fine locally.<\/p>\n\n<p>I'm running the following versions:<\/p>\n\n<p>Windows 10<\/p>\n\n<p>Python 3.6.8<\/p>\n\n<p>MLflow 1.3.0 (also replicated the fault with 1.2.0)<\/p>\n\n<p>Any feedback or suggestions are greatly appreciated!<\/p>",
        "Challenge_closed_time":1570752025483,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570185592420,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error when performing remote execution of MLflow on Databricks from Windows. The error is caused by an invalid DBFS path created locally on Windows in the MLflow package, which is sent to Databricks. The path has a combination of '\\' and '\/' in cmd, but on Databricks, there are no separators at all, causing the 'rsync: No such file or directory (2)' error. The user suspects that the issue is caused by the 'os.path.join()' function in the MLflow Github repo, which joins the string together in a \"Windows fashion\" with backslashes, and then adds a '\/' causing the Databricks file system to be unable to handle the path. The project runs",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58234777",
        "Challenge_link_count":8,
        "Challenge_participation_count":2,
        "Challenge_readability":10.1,
        "Challenge_reading_time":66.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":157.3425175,
        "Challenge_title":"MLflow remote execution on databricks from windows creates an invalid dbfs path",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":350.0,
        "Challenge_word_count":537,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1570173110492,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Thanks for the catch, you're right that using <code>os.path.join<\/code> when working with DBFS paths is incorrect, resulting in a malformed path that breaks project execution. I've filed to <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1926\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1926<\/a> track this, if you're interested in making a bugfix PR (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst\" rel=\"nofollow noreferrer\">see the MLflow contributor guide for info on how to do this<\/a>) to replace <code>os.path.join<\/code> here with <code>os.posixpath.join<\/code> I'd be happy to review :)<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":7.8,
        "Solution_reading_time":8.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":69.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1649289661367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":200.3805083333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We have a connection from SageMaker to Snowflake, and need to see <strong>views<\/strong> (as opposed to tables) listed when using Data Wrangler. Is there a reason that views are not shown in the data listing of Data Wrangler?<\/p>\n<p>We have checked security settings and access. These are not materialized views.<\/p>",
        "Challenge_closed_time":1649289755440,
        "Challenge_comment_count":1,
        "Challenge_created_time":1648568385610,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to see views in the data listing of Data Wrangler in SageMaker UI despite having a connection from SageMaker to Snowflake and checking security settings and access.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71665007",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":200.3805083333,
        "Challenge_title":"Unable to see VIEWS in SageMaker UI",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":57.0,
        "Challenge_word_count":56,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1346443720088,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11650.0,
        "Poster_view_count":977.0,
        "Solution_body":"<p>Currently Data Wrangler does not support browsing Views in the UI but you can still query them with SELECT.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":1.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1516367794196,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":1055.1677944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>System information\nOS Platform and Distribution: Windows 10\nMLflow installed: using pip\nMLflow version: version 1.24.0\n**Python version: Python 3.9.7 **<\/p>\n<p>Describe the problem\nI have created a docker-compose system with a backend\/artifact storages, mlflow server and nginx to add an authentication layer.<\/p>\n<pre><code>...\nmlflow:\n        restart: always\n        build: .\n        environment:\n            - AWS_ACCESS_KEY_ID=${MINIO_USR}\n            - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       \n        expose:\n            - '5000'\n        networks:\n            - frontend\n            - backend\n        depends_on:\n            - storage                       \n        image: 'mlflow:Dockerfile'\n        container_name: mlflow_server_nginx\n\n    nginx:\n        restart: always\n        build: .\/nginx\n        container_name: mlflow_nginx\n        ports:\n            - 5043:443\n        links:\n            - mlflow:mlflow\n        volumes:\n            - 'path\/to\/nginx\/auth:\/etc\/nginx\/conf.d'\n            - 'path\/to\/nginx\/nginx.conf:\/etc\/nginx\/nginx.conf:ro'\n        networks:\n            - frontend\n        depends_on:\n            - mlflow\n<\/code><\/pre>\n<p>I have created an user\/password via htpasswd and a custom SSL CA (.pem\/.key) using openssl and my-mlflow.com server-name.<\/p>\n<p>When the docker-compose system is built i can access to mlflow UI via my browser. But when i try to create a new experiment using python trying diferent approaches, i get next errors:\nExecuted code 1:<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\n#os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n#os.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate (_ssl.c:1108)')))\n<\/code><\/pre>\n<p>After read some notes in the documentation and realated issues I tryed next<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\n#os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\nos.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLError(9, '[SSL] PEM lib (_ssl.c:4012)')))\n<\/code><\/pre>\n<p>Finally<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\nos.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n#os.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLCertVerificationError(&quot;hostname 'localhost' doesn't match '*.my-mlflow.com'&quot;)))\n<\/code><\/pre>\n<p>Can you give me some hints about how to solve it?<\/p>\n<p>Thank you very much!\nFernando....<\/p>",
        "Challenge_closed_time":1652276299263,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648650339347,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to connect mlflow server via nginx ssl authentication. They have created a docker-compose system with a backend\/artifact storages, mlflow server, and nginx to add an authentication layer. The user has created a user\/password via htpasswd and a custom SSL CA (.pem\/.key) using openssl and my-mlflow.com server-name. They are able to access the mlflow UI via their browser, but when they try to create a new experiment using python, they get errors related to SSL certificate verification. The user has tried different approaches to solve the issue but has not been successful.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71679081",
        "Challenge_link_count":12,
        "Challenge_participation_count":1,
        "Challenge_readability":18.4,
        "Challenge_reading_time":68.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":1007.2110877778,
        "Challenge_title":"How can I connect mlflow server via nginx ssl authentication?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":625.0,
        "Challenge_word_count":377,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1580841805372,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seville, Spain",
        "Poster_reputation_count":33.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>You can set:<\/p>\n<pre><code>os.environ['MLFLOW_TRACKING_INSECURE_TLS'] = 'true'\n<\/code><\/pre>\n<p>And then try to get your cert-chain straight from there for production use.<\/p>\n<p>Also see Documentation: <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#id19\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tracking.html#id19<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1652448943407,
        "Solution_link_count":2.0,
        "Solution_readability":14.1,
        "Solution_reading_time":4.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":26.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1523192621643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":1229.0,
        "Answerer_view_count":175.0,
        "Challenge_adjusted_solved_time":4.4849386111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to create a dataset on my azure ML workspace from a GitHub action<\/p>\n<p>I've created a datastore and uploaded data to that datastore\nwhen I try to create a dataset using the cli, I get this error:<\/p>\n<p><code>'create' is misspelled or not recognized by the system.<\/code><\/p>\n<p>this is the command i use:<\/p>\n<pre><code>&gt; az ml dataset create \n          -n insurance_dataset \n          --resource-group rg-name \n          --workspace-name ml-ws-name \n          -p 'file:azureml\/datastore\/$(az ml datastore show-default -w ml-ws-name -g rg-name --query name -o tsv)\/insurance\/insurance.csv'\n<\/code><\/pre>\n<p>any idea what am I doing wrong?<\/p>",
        "Challenge_closed_time":1658774288592,
        "Challenge_comment_count":3,
        "Challenge_created_time":1658758142813,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to create a dataset on their Azure ML workspace from a GitHub action. The error message states that 'create' is misspelled or not recognized by the system when using the CLI command 'az ml dataset create'. The user has already created a datastore and uploaded data to it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73110661",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":13.0,
        "Challenge_reading_time":8.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.4849386111,
        "Challenge_title":"'create' is misspelled or not recognized by the system on az ml dataset create",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":99,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1523192621643,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":1229.0,
        "Poster_view_count":175.0,
        "Solution_body":"<p>in my case, the issue was solved by upgrading the ml extension to <code>azure-cli-ml v2<\/code><\/p>\n<p>Remove any existing installation of the of <code>ml<\/code> extension and also the CLI v1 <code>azure-cli-ml<\/code> extension:<\/p>\n<pre><code>az extension remove -n azure-cli-ml\naz extension remove -n ml\n<\/code><\/pre>\n<p>Now, install the ml extension:<\/p>\n<pre><code>az extension add -n ml -y\n<\/code><\/pre>\n<p>which still doesn't explain why the <code>create<\/code> command wasn't recognized, but the v2 behavior works fine for me.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":32.6,
        "Solution_reading_time":6.87,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":70.4262202778,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>I am inspecting and analysing my best runs. I expected that <code>group<\/code> and <code>job_type<\/code> would be populated with the resumed run\u2019s values after running the code below.<\/p>\n<pre><code class=\"lang-python\">run_id = input(\"id=\")\nwith wandb.init(entity=wandb_entity, project=wandb_project, id=run_id, resume=\"must\") as wandb_r:\n    config = wandb_r.config\n    group = wandb_r.group\n    job_type = wandb_r.job_type\n<\/code><\/pre>\n<p>Even though <code>config<\/code> is successfully recovered, <code>group<\/code> and <code>job_type<\/code> are just empty strings. How do I retrieve group and job_type values from WandB? Thanks.<\/p>",
        "Challenge_closed_time":1662128318611,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661874784218,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to retrieve the `group` and `job_type` values of a resumed run using the code provided, but even though the `config` value is successfully recovered, the `group` and `job_type` values are empty strings. The user is seeking help on how to retrieve these values from WandB.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-retrieve-the-group-and-job-type-of-a-resumed-run\/3031",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":10.2,
        "Challenge_reading_time":8.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":70.4262202778,
        "Challenge_title":"How to retrieve the `group` and `job_type` of a resumed run?",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":115.0,
        "Challenge_word_count":78,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/avm21\">@avm21<\/a>, it looks like we don\u2019t download these on resumed runs but rather we don\u2019t update them unless you explicitly change them on a resumed run. If you need to get group\/job_type you can use the public API like this to access anything you may need:<\/p>\n<pre><code class=\"lang-auto\">import wandb\nfrom wandb import Api\n\napi = Api()\n\nwith wandb.init(entity=wandb_entity, project=wandb_project, id=run_id, resume=\"must\") as wandb_r:\n    config = wandb_r.config\n\n    # A resumed run will still have the path attribute which can be used to access the run via the API\n    api_run = api.run(wandb_r.path)\n\n    # This will correctly print the group of the run\n    print(api_run.group)\n<\/code><\/pre>\n<p>Let me know if you have any questions around this.<\/p>\n<p>Thank you,<br>\nNate<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":9.97,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":113.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1373651649052,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"M\u00fcnchen, Deutschland",
        "Answerer_reputation_count":1066.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":1490.0125933333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create a dataset in Azure ML where the data source are multiple files (eg images) in a Blob Storage. How do you do that correctly?<\/p>\n<h3>Here is the error I get following the documented approach in the UI<\/h3>\n<p>When I create the dataset in the UI and select the blob storage and directory with either just <code>dirname<\/code> or <code>dirname\/**<\/code> then the files can not be found in the explorer tab with the error <code>ScriptExecution.StreamAccess.NotFound: The provided path is not valid or the files could not be accessed.<\/code> When I try to download the data with the code snippet in the consume tab then I get the error:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace, Dataset\n\n# set variables \n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndataset = Dataset.get_by_name(workspace, name='teststar')\ndataset.download(target_path='.', overwrite=False)\n<\/code><\/pre>\n<pre><code>Error Message: ScriptExecutionException was caused by StreamAccessException.\n  StreamAccessException was caused by NotFoundException.\n    Found no resources for the input provided: 'https:\/\/mystoragename.blob.core.windows.net\/data\/testdata\/**'\n\n<\/code><\/pre>\n<p>When I just select one of the files instead of <code>dirname<\/code> or <code>dirname\/**<\/code> then everything works. Does AzureML actually support Datasets consisting of multiple files?<\/p>\n<h3>Here is my setup:<\/h3>\n<p>I have a Data Storage with one container <code>data<\/code>. In there is a directory <code>testdata<\/code> containing <code>testfile1.txt<\/code> and <code>testfile2.txt<\/code>.<\/p>\n<p>In AzureML I created a datastore <code>testdatastore<\/code> and there I select the <code>data<\/code> container in my data storage.<\/p>\n<p>Then in Azure ML I create a Dataset from datastore, select file dataset and the datastore above. Then I can browse the files, select a folder and select that files in subdirectories should be included. This then creates the path <code>testdata\/**<\/code> which does not work as described above.<\/p>\n<p>I got the same issue when creating the dataset and datastore in python:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import azureml.core\nfrom azureml.core import Workspace, Datastore, Dataset\n\nws = Workspace.from_config()\n\ndatastore = Datastore(ws, &quot;mydatastore&quot;)\n\ndatastore_paths = [(datastore, 'testdata')]\ntest_ds = Dataset.File.from_files(path=datastore_paths)\ntest_ds.register(ws, &quot;testpython&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1618851944876,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613487159093,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a dataset in Azure ML using multiple files from a Blob Storage, but encounters an error message stating that the provided path is not valid or the files could not be accessed. The user has tried selecting the directory with either just dirname or dirname\/**, but the files cannot be found. The user is wondering if Azure ML supports datasets consisting of multiple files. The user has also encountered the same issue when creating the dataset and datastore in Python.",
        "Challenge_last_edit_time":1613487899540,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66226685",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":33.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":1490.2182730556,
        "Challenge_title":"AzureML create dataset from datastore with multiple files - path not valid",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":2337.0,
        "Challenge_word_count":311,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373651649052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"M\u00fcnchen, Deutschland",
        "Poster_reputation_count":1066.0,
        "Poster_view_count":60.0,
        "Solution_body":"<p>I uploaded and registered the files with this script and everything works as expected.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Datastore, Dataset, Workspace\n\nimport logging\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=&quot;%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s&quot;,\n    datefmt=&quot;%Y-%m-%d %H:%M:%S&quot;,\n)\n\ndatastore_name = &quot;mydatastore&quot;\ndataset_path_on_disk = &quot;.\/data\/images_greyscale&quot;\ndataset_path_in_datastore = &quot;images_greyscale&quot;\n\nazure_dataset_name = &quot;images_grayscale&quot;\nazure_dataset_description = &quot;dataset transformed into the coco format and into grayscale images&quot;\n\n\nworkspace = Workspace.from_config()\ndatastore = Datastore.get(workspace, datastore_name=datastore_name)\n\nlogger.info(&quot;Uploading data...&quot;)\ndatastore.upload(\n    src_dir=dataset_path_on_disk, target_path=dataset_path_in_datastore, overwrite=False\n)\nlogger.info(&quot;Uploading data done.&quot;)\n\nlogger.info(&quot;Registering dataset...&quot;)\ndatastore_path = [(datastore, dataset_path_in_datastore)]\ndataset = Dataset.File.from_files(path=datastore_path)\ndataset.register(\n    workspace=workspace,\n    name=azure_dataset_name,\n    description=azure_dataset_description,\n    create_new_version=True,\n)\nlogger.info(&quot;Registering dataset done.&quot;)\n\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":23.7,
        "Solution_reading_time":19.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":84.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":92.8743952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,     <\/p>\n<p>I am following the instruction here to install my data gateway for Azure Machine Learning Studio    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/classic\/use-data-from-an-on-premises-sql-server\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/classic\/use-data-from-an-on-premises-sql-server<\/a>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/92587-image.png?platform=QnA\" alt=\"92587-image.png\" \/>    <\/p>\n<p>I install on my personal and another one on the server    <br \/>\nmy personal one works well.    <br \/>\nBut on the data gateway from the server always show below message:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/92588-image.png?platform=QnA\" alt=\"92588-image.png\" \/>    <\/p>",
        "Challenge_closed_time":1620043441103,
        "Challenge_comment_count":1,
        "Challenge_created_time":1619709093280,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the data gateway server as it cannot be reached while trying to install it for Azure Machine Learning Studio. The personal installation works fine, but the server installation always shows an error message.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/377424\/data-gateway-issue-server-cannot-be-reached",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":18.0,
        "Challenge_reading_time":10.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":92.8743952778,
        "Challenge_title":"Data gateway issue server cannot be reached",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":67,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=e35fd686-d5ff-42db-9a7e-95deb29fb95e\">@Austin Kuo  <\/a> Thanks, Please follow the guidance given here: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/import-from-on-premises-sql-server-database#manually-set-properties-in-the-import-data-module\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/import-from-on-premises-sql-server-database#manually-set-properties-in-the-import-data-module<\/a> and <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/v1\/data-factory-troubleshoot-gateway-issues#problem\">https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/v1\/data-factory-troubleshoot-gateway-issues#problem<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":73.7,
        "Solution_reading_time":10.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":12.1803666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What is the proper way to connect DVC to Min.IO that is connected to some buckets on S3.<\/p>\n<pre><code>AWS-S3(My_Bucket) &gt; Min.io(MY_Bucket aliased as S3)\n<\/code><\/pre>\n<p>Right now i am accessing my bucket by using mc for example <code>mc cp s3\/my_bucket\/datasets datasets<\/code> to copy stuff from there. But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC so i can use for example <code>&quot;DVC mc-S3 pull&quot;<\/code> and <code>&quot;DVC AWS-S3 pull&quot;<\/code>.<\/p>\n<p>How do i got for it because while googling i couldn't find anything that i could easily follow.<\/p>",
        "Challenge_closed_time":1627513406643,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627469557323,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to connect DVC to Min.IO, which is connected to some buckets on S3. They are currently accessing their bucket using mc, but need to set up DVC to work with Min.IO as a hub between AWS.S3 and DVC. The user is seeking guidance on how to achieve this as they have been unable to find any helpful resources online.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68559059",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":4.9,
        "Challenge_reading_time":8.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":12.1803666667,
        "Challenge_title":"DVC connect to Min.IO to access S3",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":375.0,
        "Challenge_word_count":101,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578574709920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Poland",
        "Poster_reputation_count":85.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>It looks like you are looking for a combination of things.<\/p>\n<p>First, Jorge mentioned you can set <code>endpointurl<\/code> to access Minio the same way as you would access regular S3:<\/p>\n<pre><code>dvc remote add -d minio-remote s3:\/\/mybucket\/path\ndvc remote modify minio-remote endpointurl https:\/\/minio.example.com                          \n<\/code><\/pre>\n<p>Second, it seems you can create <em>two<\/em> remotes - one for S3, one for Minio and use <code>-r<\/code> option that is available for many data management related commands:<\/p>\n<pre><code>dvc pull -r minio-remote\ndvc pull -r s3-remote\ndvc push -r minio-remote\n...\n<\/code><\/pre>\n<p>This way you could <code>push<\/code>\/<code>pull<\/code> data to\/from a specific storage.<\/p>\n<blockquote>\n<p>But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC<\/p>\n<\/blockquote>\n<p>There are other possible ways, I think to organize this. It indeed depends on what semantics you expect from <code>DVC mc-S3 pull<\/code>. Please let us know if <code>-r<\/code> is not enough and clarify the question- that would help us here.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.7,
        "Solution_reading_time":13.51,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":153.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1427753565560,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":106.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":101.7854555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are in the process of setting up Azure Machine Learning within our Azure instance. Our SQL Server sits on a virtual machine and access is restricted using ACL's  <\/p>\n\n<p>We have looked extensively for a virtual IP or an IP within Machine Learning to add to the ACL but we cannot find it. <\/p>\n\n<p>We have tested access by entering 0.0.0.0\/0 to our ACL which allows access to ML  but obviously this isnt secure and not something that we wish to continue with. <\/p>\n\n<p>Thanks in advance. <\/p>",
        "Challenge_closed_time":1427753688880,
        "Challenge_comment_count":1,
        "Challenge_created_time":1427387261240,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in granting Azure Machine Learning access to their SQL Server on a virtual machine due to restricted access using ACLs. They have been unable to find a virtual IP or an IP within Machine Learning to add to the ACL and have resorted to using an insecure method of allowing access.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/29283841",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.5,
        "Challenge_reading_time":6.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":101.7854555556,
        "Challenge_title":"Grant Azure Machine Learning access to SQL Server on Virtual Machine with ACL",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":229.0,
        "Challenge_word_count":101,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1375366005447,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":151.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>Azure public IP address are published and refreshed at regular intervals it can be found here: <a href=\"http:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=41653\" rel=\"nofollow\">http:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=41653<\/a><\/p>\n\n<p>You can use these to specify the restricted IP range for access<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.2,
        "Solution_reading_time":4.33,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1491421190663,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Salt Lake City, UT, USA",
        "Answerer_reputation_count":426.0,
        "Answerer_view_count":89.0,
        "Challenge_adjusted_solved_time":18.6974344444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am having problems accessing tables in an Oracle database over a SQLAlchemy connection. Specifically, I am using Kedro <code>catalog.load('table_name')<\/code> and getting the error message <code>Table table_name not found<\/code>. So I decided to test my connection using the method listed in this answer: <a href=\"https:\/\/stackoverflow.com\/questions\/41887344\/how-to-verify-sqlalchemy-engine-object\">How to verify SqlAlchemy engine object<\/a>.<\/p>\n<pre><code>from sqlalchemy import create_engine\nengine = create_engine('oracle+cx_oracle:\/\/USER:PASSWORD@HOST:PORT\/?service_name=SERVICE_NAME')\nengine.connect()\n<\/code><\/pre>\n<p>Error: <code>InvalidRequestError: could not retrieve isolation level<\/code><\/p>\n<p>I have tried explicitly adding an isolation level as explained in the <a href=\"https:\/\/docs.sqlalchemy.org\/en\/14\/core\/connections.html#setting-transaction-isolation-levels-including-dbapi-autocommit\" rel=\"nofollow noreferrer\">documentation<\/a> like this:<\/p>\n<pre><code>engine = create_engine('oracle+cx_oracle:\/\/USER:PASSWORD@HOST:PORT\/?service_name=SERVICE_NAME', execution_options={'isolation_level': 'AUTOCOMMIT'})\n<\/code><\/pre>\n<p>and this:<\/p>\n<pre><code>engine.connect().execution_options(isolation_level='AUTOCOMMIT')\n<\/code><\/pre>\n<p>and this:<\/p>\n<pre><code>connection = engine.connect()\nconnection = connection.execution_options(\n    isolation_level=&quot;AUTOCOMMIT&quot;\n)\n<\/code><\/pre>\n<p>but I get the same error in all cases.<\/p>",
        "Challenge_closed_time":1611332594540,
        "Challenge_comment_count":3,
        "Challenge_created_time":1611242774307,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing issues while accessing tables in an Oracle database over a SQLAlchemy connection. They are receiving an error message \"Table table_name not found\" while using Kedro catalog.load('table_name'). The user tried to test the connection using a method listed in an answer on Stack Overflow, but they received an error message \"InvalidRequestError: could not retrieve isolation level\". The user has tried different methods to add an isolation level, but they are still facing the same error.",
        "Challenge_last_edit_time":1611265283776,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65830524",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":20.9,
        "Challenge_reading_time":20.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":24.9500647222,
        "Challenge_title":"SQLAlchemy Oracle - InvalidRequestError: could not retrieve isolation level",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":196.0,
        "Challenge_word_count":116,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1491421190663,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Salt Lake City, UT, USA",
        "Poster_reputation_count":426.0,
        "Poster_view_count":89.0,
        "Solution_body":"<p>Upgrading from SqlAlchemy 1.3.21 to 1.3.22 solved the problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":0.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":9.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3294444444,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm trying to figure out a minimally permissive yet operational network configuration for Amazon SageMaker training to train on data from Amazon FSx for Lustre. My understanding is that both the file system and the SageMaker instance can have their own security groups and that FSx uses TCP on ports 988 and 1021-1023. Therefore, I think a good network configuration for using SageMaker with FSx is the following:\n* SageMaker EC2 equipped with the security group SM-SG that allows Inbound only with TCP on 988 and 1021-1023 from FSX-SG only.\n* Amazon FSx equipped with the security group FSX-SG that allows outbound only with TCP on 988 and 1021-1023 towards SM-SG only.\nIs this configuration enough for the training to work? Do FSx and SageMaker need other ports and sources to be opened to operate normally?",
        "Challenge_closed_time":1605281179000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605279993000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to configure a secure network for Amazon SageMaker training to use data from Amazon FSx for Lustre. They propose a network configuration that involves setting up security groups for both SageMaker and FSx, allowing inbound and outbound traffic on specific TCP ports. The user is unsure if this configuration is sufficient for the training to work and if any additional ports or sources need to be opened.",
        "Challenge_last_edit_time":1668217529291,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUrTkxH_kIT-a_LJSGYS5SXA\/how-do-i-achieve-the-least-access-secure-networking-for-sagemaker-training-on-amazon-fsx-for-lustre",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":11.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3294444444,
        "Challenge_title":"How do I achieve the least-access secure networking for SageMaker Training on Amazon FSx for Lustre?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":195.0,
        "Challenge_word_count":149,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"For the security group for Amazon FSx (Example: FSx-SG), you need to add the following additional rules:\n\n1. FSx-SG needs inbound access from the security group for SageMaker (Example: SM-SG). The SageMaker instance needs to initiate a connection to the Amazon FSx file system, which is an inbound TCP packet to FSx.\n2. FSx-SG needs inbound and outbound access to itself. This is because, Amazon FSx for Lustre is a clustered file system, where each file system is typically powered by multiple file servers, and the file servers need to  communicate with one another.\n\nFor more information on the minimum set of rules required for FSx-SG, see [File system access control with Amazon VPC][1].\n[1]: https:\/\/docs.aws.amazon.com\/fsx\/latest\/LustreGuide\/limit-access-security-groups.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925559451,
        "Solution_link_count":1.0,
        "Solution_readability":12.0,
        "Solution_reading_time":9.75,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":116.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1344510903550,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hannover, Germany",
        "Answerer_reputation_count":33554.0,
        "Answerer_view_count":2182.0,
        "Challenge_adjusted_solved_time":3240.8772311111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The experiment software <a href=\"http:\/\/sacred.readthedocs.io\/en\/latest\/quickstart.html\" rel=\"nofollow noreferrer\">sacred<\/a> was run without MongoDB in the background with a configured <a href=\"http:\/\/sacred.readthedocs.io\/en\/latest\/observers.html#mongo-observer\" rel=\"nofollow noreferrer\">mongo-observer<\/a>. When it tried to write the settings to MongoDB, this failed, creating the file <code>\/tmp\/sacred_mongo_fail__eErwU.pickle<\/code>, with the message<\/p>\n\n<pre><code>Warning: saving to MongoDB failed! Stored experiment entry in \/tmp\/sacred_mongo_fail__eErwU.pickle\nTraceback (most recent calls WITHOUT Sacred internals):\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sacred\/observers\/mongo.py\", line 127, in started_event\n    self.run_entry[experiment][sources] = self.save_sources(ex_info)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sacred\/observers\/mongo.py\", line 239, in save_sources\n    file = self.fs.find_one({filename: abs_path, md5: md5})\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/gridfs\/__init__.py\", line 261, in find_one\n    for f in self.find(filter, *args, **kwargs):\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/gridfs\/grid_file.py\", line 658, in next\n    next_file = super(GridOutCursor, self).next()\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 1114, in next\n    if len(self.__data) or self._refresh():\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 1036, in _refresh\n    self.__collation))\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 873, in __send_message\n    **kwargs)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/mongo_client.py\", line 888, in _send_message_with_response\n    server = topology.select_server(selector)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/topology.py\", line 214, in select_server\n    address))\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/topology.py\", line 189, in select_servers\n    self._error_message(selector))\nServerSelectionTimeoutError: localhost:27017: [Errno 111] Connection refused\n<\/code><\/pre>\n\n<p>How can this pickle file be imported into MongoDB manually?<\/p>",
        "Challenge_closed_time":1510651584203,
        "Challenge_comment_count":0,
        "Challenge_created_time":1498984762217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an issue while running the experiment software 'sacred' without MongoDB in the background. As a result, the software failed to write the settings to MongoDB, creating a pickle file instead. The user is seeking guidance on how to import this pickle file into MongoDB manually.",
        "Challenge_last_edit_time":1498985131100,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44868932",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.5,
        "Challenge_reading_time":29.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":3240.783885,
        "Challenge_title":"How to import the pickle file if sacred failed to connect to MongoDB",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":537.0,
        "Challenge_word_count":171,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1344510903550,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hannover, Germany",
        "Poster_reputation_count":33554.0,
        "Poster_view_count":2182.0,
        "Solution_body":"<ol>\n<li>Load the pickle file, <\/li>\n<li>set the <code>_id<\/code>,<\/li>\n<li>insert <\/li>\n<\/ol>\n\n<p><\/p>\n\n<pre><code>db = pymongo.MongoClient().sacred\nentry = pickle.load(open('\/tmp\/sacred_mongo_fail__eErwU.pickle'))\nentry['_id'] = list(db.runs.find({}, {\"_id\": 1}))[-1]['_id']\ndb.runs.insert_one(entry)\n<\/code><\/pre>\n\n<p>This is quick and dirty, depends on the <code>find<\/code> to list objects in order, and could use <a href=\"https:\/\/stackoverflow.com\/questions\/2138873\/cleanest-way-to-get-last-item-from-python-iterator\">Cleanest way to get last item from Python iterator<\/a> instead of <code>list(...)[-1]<\/code>, but it should work.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1510652289132,
        "Solution_link_count":1.0,
        "Solution_readability":10.0,
        "Solution_reading_time":8.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":57.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":1645519217332,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":336.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":166.7237425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How can I store additional information in an <code>optuna trial<\/code> when using it via the Hydra sweep plugin?<\/p>\n<p>My use case is as follows:\nI want to optimize a bunch of hyperparameters. I am storing all reproducibility information of all experiments (i.e., trials) in a separate database.\nI know I can get the best values via <code>optuna.load_study().best_params<\/code> or even <code>best_trial<\/code>. However, that only allows me to replicate the experiment - potentially this takes quite some time. To overcome this issue, I need to somehow link it to my own database. I would like to store the ID of my own database somewhere in the <code>trial<\/code> object.<\/p>\n<p>Without using Hydra, I suppose I'd set <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/003_attributes.html#sphx-glr-tutorial-20-recipes-003-attributes-py\" rel=\"nofollow noreferrer\">User Attributes<\/a>. However, with Hydra <a href=\"https:\/\/github.com\/facebookresearch\/hydra\/blob\/535dc7aacfe607e25848b2c4b8068317095a730b\/plugins\/hydra_optuna_sweeper\/hydra_plugins\/hydra_optuna_sweeper\/_impl.py#L183\" rel=\"nofollow noreferrer\">abstracting all that away<\/a>, there seems no option to do so.<\/p>\n<p>I know that I can just query my own database for the exact combination of best params that optuna found, but that just seems like a difficult solution to a simple problem.<\/p>\n<p>Some minimal code:<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>from dataclasses import dataclass\n\nimport hydra\nfrom hydra.core.config_store import ConfigStore\nfrom omegaconf import MISSING\n\n\n@dataclass\nclass TrainConfig:\n    x: float | int = MISSING\n    y: int = MISSING\n    z: int | None = None\n\n\nConfigStore.instance().store(name=&quot;config&quot;, node=TrainConfig)\n\n\n@hydra.main(version_base=None, config_path=&quot;conf&quot;, config_name=&quot;sweep&quot;)\ndef sphere(cfg: TrainConfig) -&gt; float:\n    x: float = cfg.x\n    y: float = cfg.y\n    return x**2 + y**2\n\n\nif __name__ == &quot;__main__&quot;:\n    sphere()\n<\/code><\/pre>\n<pre class=\"lang-yaml prettyprint-override\"><code>defaults:\n  - override hydra\/sweeper: optuna\n  - override hydra\/sweeper\/sampler: tpe\n\nhydra:\n  sweeper:\n    sampler:\n      seed: 123\n    direction: minimize\n    study_name: sphere\n    storage: sqlite:\/\/\/trials.db\n    n_trials: 20\n    n_jobs: 1\n    params:\n      x: range(-5.5, 5.5, step=0.5)\n      y: choice(-5 ,0 ,5)\n      z: choice(0, 3, 5)\n\nx: 1\ny: 1\nz: 1\n<\/code><\/pre>",
        "Challenge_closed_time":1657793209687,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657194655610,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to store additional information in an Optuna trial when using it via the Hydra sweep plugin. They are optimizing a bunch of hyperparameters and storing all reproducibility information of all experiments in a separate database. However, they need to link it to their own database and would like to store the ID of their own database somewhere in the trial object. Without using Hydra, they would set User Attributes, but with Hydra abstracting all that away, there seems to be no option to do so.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72897321",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":31.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":166.2650213889,
        "Challenge_title":"Store user attributes in Optuna Sweeper plugin for Hydra",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":83.0,
        "Challenge_word_count":275,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1645519217332,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":336.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>A hacky solution via the <a href=\"https:\/\/hydra.cc\/docs\/plugins\/optuna_sweeper\/#experimental--custom-search-space-optimization\" rel=\"nofollow noreferrer\"><code>custom_search_space<\/code><\/a>.<\/p>\n<pre><code>hydra:\n  sweeper:\n    sampler:\n      seed: 123\n    direction: minimize\n    study_name: sphere\n    storage: sqlite:\/\/\/trials.db\n    n_trials: 20\n    n_jobs: 1\n    params:\n      x: range(-5.5, 5.5, step=0.5)\n      y: choice(-5 ,0 ,5)\n      z: choice([0, 1], [2, 3], [2, 5])\n    custom_search_space: package.run.configure\n<\/code><\/pre>\n<pre><code>def configure(_, trial: Trial) -&gt; None:\n    trial.set_user_attr(&quot;experiment_db_id&quot;, 123456)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1657794861083,
        "Solution_link_count":1.0,
        "Solution_readability":15.6,
        "Solution_reading_time":8.21,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":52.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1286692213960,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":328.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":1559.6549622222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a MongoDB database (the Bitnami one) hosted on Azure. I want to import the data there to use it in my Azure Machine Learning experiment.<\/p>\n\n<p>Currently, I am exporting the data to <strong>.csv<\/strong> using <strong>mongoexport<\/strong> and then copy\/pasting it to the <strong>\"Enter Manually Data\"<\/strong> module. This is fine for small amounts of data but I would prefer to have a more robust technique for larger databases.<\/p>\n\n<p>I also thought about using the <strong>\"Import Data\"<\/strong> module from http url along with the <strong>http port (28017) of my mongodb<\/strong> instance but read this was not the recommended use of the http mongodb feature.<\/p>\n\n<p>Finally, I have installed <strong>cosmosDB<\/strong> instead of my bitnami MongoDB and it worked fine but this thing <strong>costs an arm<\/strong> when used with sitecore (it reaches around 100\u20ac per day) and we can't afford it so I switched back to by Mongo.<\/p>\n\n<p>So is there a better way to export data from Mongo to Azure ML ?<\/p>",
        "Challenge_closed_time":1504686538047,
        "Challenge_comment_count":0,
        "Challenge_created_time":1499071780183,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in importing data from their MongoDB database hosted on Azure to use it in their Azure Machine Learning experiment. They are currently exporting the data to .csv using mongoexport and manually entering it into the module, which is not feasible for larger databases. They have also considered using the \"Import Data\" module from http url along with the http port of their mongodb instance, but it is not recommended. They have tried using cosmosDB, but it is expensive, so they switched back to MongoDB. The user is seeking a better way to export data from Mongo to Azure ML.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44881303",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":13.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1559.6549622222,
        "Challenge_title":"Best way to import MongoDB data in Azure Machine Learning",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":724.0,
        "Challenge_word_count":169,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1441267698016,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":781.0,
        "Poster_view_count":97.0,
        "Solution_body":"<p>one way is to use a Python code block in AzureML, something like this:<\/p>\n\n<pre><code>import pandas as p\nimport pymongo as m\n\ndef azureml_main():\n    c = m.MongoClient(host='host_IP')\n    a = p.DataFrame(c.database_names())\n    return a\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":3.06,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1365101584443,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Munich, Germany",
        "Answerer_reputation_count":7203.0,
        "Answerer_view_count":445.0,
        "Challenge_adjusted_solved_time":0.7841066667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a step function statemachine which creates SageMaker batch transform job, the definition is written in Terraform, I wanted to add the stepfunction execution id to the batch transform job names:<\/p>\n<p>in stepfunction terraform file:<\/p>\n<pre><code>  definition = templatefile(&quot;stepfuntion.json&quot;,\n    {\n      xxxx\n)\n<\/code><\/pre>\n<p>in the &quot;stepfuntion.json&quot;:<\/p>\n<pre><code>{...\n          &quot;TransformJobName&quot;: &quot;jobname-$$.Execution.Id&quot;,\n  \n          }\n      },\n        &quot;End&quot;: true\n      }\n    }\n  }\n<\/code><\/pre>\n<p>But after terraform apply, it didn't generate the actual id, it gave me <code>jobname-$$.Execution.Id<\/code>, can anyone help with this please?<\/p>\n<p>Resources: <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/input-output-contextobject.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/input-output-contextobject.html<\/a>\n&quot;To access the context object, first specify the parameter name by appending .$ to the end, as you do when selecting state input with a path. Then, to access context object data instead of the input, prepend the path with $$.. This tells AWS Step Functions to use the path to select a node in the context object.&quot;<\/p>\n<p>Can someone tell me what I'm missing please?<\/p>",
        "Challenge_closed_time":1610017101707,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610012100150,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to add the step function execution id to the SageMaker batch transform job names using Terraform, but after applying the changes, it did not generate the actual id and instead gave the user \"jobname-$$.Execution.Id\". The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":1610014278923,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65609804",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":17.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1.3893213889,
        "Challenge_title":"How to append stepfunction execution id to SageMaker job names?",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":488.0,
        "Challenge_word_count":148,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>The var you are trying to use terraform doesn't know about it<\/p>\n<blockquote>\n<p>jobname-$$.Execution.Id.<\/p>\n<\/blockquote>\n<p>That's something specific to the Step function and available within state machine not available for terraform.<\/p>",
        "Solution_comment_count":17.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":3.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.1427777778,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer is trying to setup Sagemaker studio. He is following our published instructions to set up using IAM: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/onboard-iam.html\n\nBut is getting an error: User:  arn:aws:iam:xxxx:user\/user1 is not authorized to perform: sagemaker:CreateDomain on resource: arn:aws:sagemaker: us-east-2:xxxx:domain\/yyyy\n\nHe has admin priviledges on the account and AmazonSageMakerFullAccess. We noticed that the AmazonSageMakerFullAccess policy actually has a limitation. You can perform all sagemaker actions, but not on a resource with arn \u201carn:aws:sagemaker:*:*:domain\/*\u201d. \nWe confirmed there are no other domains in that region with the CLI as you are only allowed one \u2013 so that isn\u2019t blocking.\nAnd aws sagemaker list-user-profiles returns no user profiles. \n\nHas anyone seen that error before or know the workaround? Should he create a custom policy to enable creating domains or would there be any implications of that? Are there specific permissions he should have so as to onboard using IAM?",
        "Challenge_closed_time":1586807470000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586796156000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while setting up Sagemaker Studio using IAM, specifically while trying to create a domain. Despite having admin privileges and AmazonSageMakerFullAccess, the policy has a limitation that prevents actions on resources with the arn \"arn:aws:sagemaker:*:*:domain\/*\". There are no other domains in the region, and the CLI returns no user profiles. The user is seeking a workaround and wondering if creating a custom policy would have any implications.",
        "Challenge_last_edit_time":1668609159168,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUyWQfPusnSHG6Ujfzx27o1w\/sagemaker-studio-create-domain-error",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":13.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":3.1427777778,
        "Challenge_title":"Sagemaker Studio - create domain error",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1619.0,
        "Challenge_word_count":146,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"A user with admin privileges would have access to `\"iam:CreateServiceLinkedRole\"` and `\"sagemaker:CreateDomain\"` actions, unless SCPs or permissions boundaries are involved. However, for the purpose of onboarding Amazon SageMaker Studio with limited permissions, I would grant the user [least privilege](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security_iam_id-based-policy-examples.html#security_iam_service-with-iam-policy-best-practices) by reviewing [Control Access to the Amazon SageMaker API by Using Identity-based Policies](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security_iam_id-based-policy-examples.html#api-access-policy) and [Actions, Resources, and Condition Keys for Amazon SageMaker](https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/list_amazonsagemaker.html) documentation:\n\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": \"sagemaker:CreateDomain\",\n        \"Resource\": \"arn:aws:sagemaker:<REGION>:<ACCOUNT-ID>:domain\/*\"\n    }\n\nNOTE: An AWS account is limited to one Domain, per region, see [CreateDomain](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateDomain.html).\n\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": \"iam:CreateServiceLinkedRole\",\n        \"Resource\": \"*\",\n        \"Condition\": {\n            \"StringEquals\": {\n                \"iam:AWSServiceName\": \"sagemaker.amazonaws.com\"\n            }\n        }\n    }\n\nCheers!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925577936,
        "Solution_link_count":4.0,
        "Solution_readability":28.1,
        "Solution_reading_time":17.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1593579580856,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":14.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":0.8406536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I am trying iris to get acquainted with was sagemaker I am following simple tutorials from <a href=\"https:\/\/towardsdatascience.com\/training-and-deploying-custom-tensorflow-models-with-aws-sagemaker-72027722ad76\" rel=\"nofollow noreferrer\">link<\/a>. I have created a bucket named &quot;tf-practise-iris-data&quot; and gave the IAM role of Sagemaker access to the s3 bucket as mentioned in the tutorial. I also tried creating a new bucket with a different name thinking there might be some problem with a bucket but still it is having the same issue, this is the snippet of my code <a href=\"https:\/\/i.stack.imgur.com\/FgjVK.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FgjVK.png\" alt=\"enter image description here\" \/><\/a>. And I have turned off Block all public access from the bucket but still nothing.<\/p>",
        "Challenge_closed_time":1642446326470,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642443300117,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to upload data using sagemaker_session.upload_data in the S3 bucket that they created. The data is getting stored in the default S3 bucket instead. The user has given the IAM role of Sagemaker access to the S3 bucket and turned off Block all public access, but the issue persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70745798",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":12.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.8406536111,
        "Challenge_title":"Unable to upload data using sagemaker_session.upload_data in s3 bucket that I created, it is getting stored in default s3 bucket",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":242.0,
        "Challenge_word_count":124,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1593579580856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":14.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Solved it!!!!!!!!!!!!<\/p>\n<pre><code>prefix = &quot;checking-with-new-bucket&quot;\ntraining_input_path = sagemaker_session.upload_data('train.csv', bucket = 'checking-with-new-bucket',key_prefix = prefix + &quot;\/training&quot;)\ntraining_input_path\n<\/code><\/pre>\n<p>Which gave output as<\/p>\n<pre><code>'s3:\/\/checking-with-new-bucket\/checking-with-new-bucket\/training\/train.csv'\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":31.6,
        "Solution_reading_time":5.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1491898605956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sydney NSW, Australia",
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":545.9161877778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We have applications for multiple tenants on our AWS account and would like to distinguish between them in different IAM roles. In most places this is already possible by limiting resource access based on naming patterns.<\/p>\n<p>For CloudWatch log groups of SageMaker training jobs however I have not seen a working solution yet. The tenants can choose the job name arbitrarily, and hence the only part of the LogGroup name that is available for pattern matching would be the prefix before the job name. This prefix however seems to be fixed to <code>\/aws\/sagemaker\/TrainingJobs<\/code>.<\/p>\n<p>Is there a way to change or extend this prefix in order to make such limiting possible? Say, for example <code>\/aws\/sagemaker\/TrainingJobs\/&lt;product&gt;-&lt;stage&gt;-&lt;component&gt;\/&lt;training-job-name&gt;-...<\/code> so that a resource limitation like <code>\/aws\/sagemaker\/TrainingJobs\/&lt;product&gt;-*<\/code> becomes possible?<\/p>",
        "Challenge_closed_time":1651281451296,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649316153020,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to distinguish between multiple tenants in different IAM roles on their AWS account. They are facing challenges in limiting resource access based on naming patterns for CloudWatch log groups of SageMaker training jobs. The prefix before the job name seems to be fixed to \"\/aws\/sagemaker\/TrainingJobs\" and the user wants to know if there is a way to change or extend this prefix to make resource limitation possible.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71777914",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":12.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":545.9161877778,
        "Challenge_title":"Change AWS SageMaker LogGroup Prefix?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":127,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1257535237563,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":1904.0,
        "Poster_view_count":321.0,
        "Solution_body":"<p>I think it is not possible to change the log streams names for any of the SageMaker services.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5504083334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm working for a campany located in Germany. We want to use Azure Machine Learning (and other stuff like that).   <br \/>\nWe are only allowed to use Azure in the Region &quot;Germany&quot;, because the data of our customers cannot left germany.  <\/p>\n<p>Now I saw, that a lot of stuff in Azure Machine Learning is not available in Germany?  <\/p>\n<p>Questions:  <\/p>\n<ol>\n<li> Is that true?  <\/li>\n<li> Does some one now, at what time Microsoft plans to make the stuff available in Germany?  <\/li>\n<\/ol>\n<p>Thank you for a answer!  <\/p>\n<p>Patrick  <\/p>",
        "Challenge_closed_time":1612862196647,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612860215177,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in using Azure Machine Learning and other related services in the \"Germany\" region due to restrictions on customer data leaving the country. The user is seeking information on whether these services are available in Germany and if not, when they will be made available.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/265151\/azure-machine-learning-(and-cognitive-services)-is",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.5,
        "Challenge_reading_time":7.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.5504083334,
        "Challenge_title":"Azure Machine Learning (and cognitive services) is not supported in Region \"Germany\"?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":106,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=1d5d0740-76fa-4574-b01a-5fcee1ddf5b1\">@Patrick Huber  <\/a>     <br \/>\nYes, Azure Machine Learning is not available in Germany region.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/65687-image.png?platform=QnA\" alt=\"65687-image.png\" \/>    <\/p>\n<p><a href=\"https:\/\/feedback.azure.com\/forums\/34192--general-feedback\">Please check in Azure feedback<\/a>    <\/p>\n<p>If the Answer is helpful, please click <code>Accept Answer<\/code> and <strong>up-vote<\/strong>, this can be beneficial to other community members.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.1,
        "Solution_reading_time":7.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1657058369727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":2.7470963889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am calling <code>Sagemaker API<\/code> from python script inside <code>EC2<\/code> instance to create online feature store. I gave required permission and its creating feature group.\nHowever I observed that key I'm passing in below program (<code>online_store_kms_key_id = 'arn:aws:kms:us-east-1:1234:key\/1111'<\/code>) is not being used to write objects to s3 bucket instead it's using default bucket key.\nI'm not sure what is causing this to happen? Why its not using key given in create feature group config? Any idea?<\/p>\n<p>code snippet:<\/p>\n<pre><code>customer_data = pd.read_csv(&quot;data.csv&quot;,dtype={'customer_id': int,'city_code': int, 'state_code': int, 'country_code': int, 'eventtime': float })\n\n    customers_feature_group_name = &quot;customers-fg-01&quot;\n    customers_feature_group = FeatureGroup(name=customers_feature_group_name, sagemaker_session=sagemaker_session\n                                           )\n\n    current_time_sec = int(round(time.time()))\n\n    record_identifier_feature_name = &quot;customer_id&quot;\n\n    customers_feature_group.load_feature_definitions(data_frame=customer_data)\n\n    customers_feature_group.create(\n        s3_uri=&quot;s3:\/\/xxxx\/sagemaker-featurestore\/&quot;,\n        record_identifier_name=record_identifier_feature_name,\n        event_time_feature_name=&quot;eventtime&quot;,\n        role_arn='arn:aws:iam::1234:role\/role-1234',\n        enable_online_store=True,\n        online_store_kms_key_id = 'arn:aws:kms:us-east-1:1234:key\/1111'\n    )\n<\/code><\/pre>",
        "Challenge_closed_time":1659050391287,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659040058783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while creating an online feature store using Sagemaker API from a Python script inside an EC2 instance. The key provided in the create feature group configuration is not being used to write objects to the S3 bucket, instead, the default bucket key is being used. The user is seeking help to understand why this is happening.",
        "Challenge_last_edit_time":1659040501740,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73158818",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":18.4,
        "Challenge_reading_time":19.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":2.87014,
        "Challenge_title":"Sagemaker API online feature store creation not using given kms key",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":47.0,
        "Challenge_word_count":116,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1365570541220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":519.0,
        "Poster_view_count":576.0,
        "Solution_body":"<p>For encryption of data stored in s3 ( offline store ) you need to add a field\n'offline_store_kms_key_id ' to the create() method call, please refer the document below<\/p>\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/prep_data\/feature_store.html#sagemaker.feature_store.feature_group.FeatureGroup.create\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/prep_data\/feature_store.html#sagemaker.feature_store.feature_group.FeatureGroup.create<\/a><\/p>\n<p>Also please go through the below document to check the policies and also to verify if you have a symmetric customer managed keys or asymmetric customer managed keys as feature store only supports symmetric keys.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/feature-store-security.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/feature-store-security.html<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":22.8,
        "Solution_reading_time":12.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":69.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1270568377790,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Philadelphia",
        "Answerer_reputation_count":13056.0,
        "Answerer_view_count":354.0,
        "Challenge_adjusted_solved_time":1.5477955556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using AWS sagemaker, I have some secret keys and access keys to access some APIs that I don't want to expose directly in code.<\/p>\n<p>What are the ways like environment variables etc., that can be used to hide these keys and I can use them securely, and how to set them.<\/p>",
        "Challenge_closed_time":1625064328872,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625052591003,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking ways to securely hide secret keys and access keys for APIs while using AWS sagemaker. They are looking for methods such as environment variables to set and use these keys securely.",
        "Challenge_last_edit_time":1625058756808,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68193944",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.4,
        "Challenge_reading_time":3.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":3.2605191667,
        "Challenge_title":"Set custom environment variables in AWS",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":277.0,
        "Challenge_word_count":58,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1567880532003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lahore, Pakistan",
        "Poster_reputation_count":137.0,
        "Poster_view_count":100.0,
        "Solution_body":"<p>AWS System Manager (SSM) is designed to store keys and tokens securely.<\/p>\n<p>Depending on how your notebook is defined, you could <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">use the 'env' property<\/a> directly or in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-environment-variables\" rel=\"nofollow noreferrer\">training data<\/a>, or you could access SSM directly from sagemaker. For example this Snowflake KB article explains how to fetch auth info from ssm: <a href=\"https:\/\/community.snowflake.com\/s\/article\/Connecting-a-Jupyter-Notebook-Part-3\" rel=\"nofollow noreferrer\">https:\/\/community.snowflake.com\/s\/article\/Connecting-a-Jupyter-Notebook-Part-3<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":23.8,
        "Solution_reading_time":11.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1276294622427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":4334.0,
        "Answerer_view_count":496.0,
        "Challenge_adjusted_solved_time":135.0612036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have <a href=\"https:\/\/docs.aws.amazon.com\/code-samples\/latest\/catalog\/python-comprehend-TopicModeling.py.html\" rel=\"nofollow noreferrer\">Amazon sample code<\/a> for running <code>comprehend.start_topics_detection_job<\/code>. Here is the code with the variables filled in for my job:<\/p>\n\n<pre><code>import re\nimport csv\nimport pytz\nimport boto3\nimport json\n\n# https:\/\/docs.aws.amazon.com\/code-samples\/latest\/catalog\/python-comprehend-TopicModeling.py.html\n# https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/API_InputDataConfig.html\n\n# Set these values before running the program\ninput_s3_url = \"s3:\/\/comprehend-topic-modelling-bucket\/input_800_cleaned_articles\/\"\ninput_doc_format = \"ONE_DOC_PER_LINE\"\noutput_s3_url = \"s3:\/\/comprehend-topic-modelling-bucket\/output\"\ndata_access_role_arn = \"arn:aws:iam::372656143103:role\/access-aws-services-from-sagemaker\"\nnumber_of_topics = 30\n\n# Set up job configuration\ninput_data_config = {\"S3Uri\": input_s3_url, \"InputFormat\": input_doc_format}\noutput_data_config = {\"S3Uri\": output_s3_url}\n\n# Begin a job to detect the topics in the document collection\ncomprehend = boto3.client('comprehend')\nstart_result = comprehend.start_topics_detection_job(\n    NumberOfTopics=number_of_topics,\n    InputDataConfig=input_data_config,\n    OutputDataConfig=output_data_config,\n    DataAccessRoleArn=data_access_role_arn)\n\n# Output the results\nprint('Start Topic Detection Job: ' + json.dumps(start_result))\njob_id = start_result['JobId']\nprint(f'job_id: {job_id}')\n\n# Retrieve and output information about the job\ndescribe_result = comprehend.describe_topics_detection_job(JobId=job_id)\nprint('Describe Job: ' + json.dumps(describe_result)) . #&lt;===LINE 36\n\n# List and output information about current jobs\nlist_result = comprehend.list_topics_detection_jobs()\nprint('list_topics_detection_jobs_result: ' + json.dumps(list_result))\n<\/code><\/pre>\n\n<p>It's failing with the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-8-840a7ee043d4&gt; in &lt;module&gt;()\n     34 # Retrieve and output information about the job\n     35 describe_result = comprehend.describe_topics_detection_job(JobId=job_id)\n---&gt; 36 print('Describe Job: ' + json.dumps(describe_result))\n     37 \n     38 # List and output information about current jobs\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\n    229         cls is None and indent is None and separators is None and\n    230         default is None and not sort_keys and not kw):\n--&gt; 231         return _default_encoder.encode(obj)\n    232     if cls is None:\n    233         cls = JSONEncoder\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in encode(self, o)\n    197         # exceptions aren't as detailed.  The list call should be roughly\n    198         # equivalent to the PySequence_Fast that ''.join() would do.\n--&gt; 199         chunks = self.iterencode(o, _one_shot=True)\n    200         if not isinstance(chunks, (list, tuple)):\n    201             chunks = list(chunks)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in iterencode(self, o, _one_shot)\n    255                 self.key_separator, self.item_separator, self.sort_keys,\n    256                 self.skipkeys, _one_shot)\n--&gt; 257         return _iterencode(o, 0)\n    258 \n    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in default(self, o)\n    178         \"\"\"\n    179         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n--&gt; 180                         o.__class__.__name__)\n    181 \n    182     def encode(self, o):\n\nTypeError: Object of type 'datetime' is not JSON serializable\n<\/code><\/pre>\n\n<p>It fails instantly, the second I pus \"run\". It seems to me that the call to <code>comprehend.start_topics_detection_job<\/code> may be failing, leading to an error line 36, <code>print('Describe Job: ' + json.dumps(describe_result))<\/code>.<\/p>\n\n<p>What am I missing?<\/p>\n\n<p><strong>UPDATE<\/strong><\/p>\n\n<p>The same IAM role is being used for the notebook, as well as in the above code. Here are the permissions currently assigned to that IAM role:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6ihIr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6ihIr.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1556698045340,
        "Challenge_comment_count":3,
        "Challenge_created_time":1556211825007,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while running Amazon sample code for running comprehend.start_topics_detection_job. The error is related to the object of type 'datetime' not being JSON serializable. The error occurs instantly, and the user suspects that the call to comprehend.start_topics_detection_job may be failing. The same IAM role is being used for the notebook and the code, and the user has provided the permissions assigned to that IAM role.",
        "Challenge_last_edit_time":1586235824608,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55854377",
        "Challenge_link_count":5,
        "Challenge_participation_count":4,
        "Challenge_readability":14.9,
        "Challenge_reading_time":57.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":135.0612036111,
        "Challenge_title":"comprehend.start_topics_detection_job Fails with Silent Error?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":232.0,
        "Challenge_word_count":379,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1276294622427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4334.0,
        "Poster_view_count":496.0,
        "Solution_body":"<p>It turns out that there was nothing wrong with the call to <code>comprehend.describe_topics_detection_job<\/code> -- it was just returning, in <code>describe_result<\/code>, something that could not be json serialized, so <code>json.dumps(describe_result))<\/code> was throwing an error. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.8,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1450057717008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":21.4211194444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using AzureMLBatchExecution activity in Azure Data Factory, is it secure to pass the DB query as a global parameter to the AzureML web service? <\/p>",
        "Challenge_closed_time":1476431199480,
        "Challenge_comment_count":0,
        "Challenge_created_time":1476354083450,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning the security of passing a database query as a global parameter to the AzureML web service when using the AzureMLBatchExecution activity in Azure Data Factory.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40018320",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":21.4211194444,
        "Challenge_title":"Is it secure to pass the DB query to AzureML as a global parameter?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":39,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1452608563363,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":105.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>When you talk about \"secure\", are you worried about secure transmission between AML and ADF, or secure storage of your DB query information? For the former, all communication between these two services will be done with HTTPS. For the latter, our production storage has its strict access control. Besides, we only log the count of the global parameters and never the values. I believe it's secure to pass your DB query as a global parameter to the AzureML web service.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":5.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":371.2785480556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What can I do when I forget my password in the local wandb?<br>\nIt seems that deleting or uninstalling  doesn\u2019t work.<\/p>",
        "Challenge_closed_time":1646784628352,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645448025579,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has forgotten their password in the local wandb and is seeking advice on what to do. They have tried deleting or uninstalling but it hasn't worked.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/forgot-password-in-local\/1959",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.2,
        "Challenge_reading_time":1.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":371.2785480556,
        "Challenge_title":"Forgot password in local",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":286.0,
        "Challenge_word_count":24,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/nightmare4214\">@nightmare4214<\/a> ,<\/p>\n<p>Could you try the following steps?<\/p>\n<ul>\n<li>Log into the docker container using <code>docker exec -it wandb-local bash<\/code>\n<\/li>\n<li>Type <code>\/usr\/local\/bin\/local password EMAIL@ADDRESS.com<\/code> (where <code>EMAIL@ADDRESS.com<\/code> is your email)<\/li>\n<\/ul>\n<p>This should let you manually reset your password for the local instance and you should be able to log in through this. Please let me know if this does not work for you.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":7.04,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":69.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4111.2283333333,
        "Challenge_answer_count":0,
        "Challenge_body":"I'm not exactly sure on how to interpret this and what to refine. The error I'm getting is in the azureml.PipelineStep automl step. Here is the [link](https:\/\/mlworkspace.azure.ai\/portal\/subscriptions\/ff2e23ae-7d7c-4cbd-99b8-116bb94dca6e\/resourceGroups\/RG-ITSMLTeam-Dev\/providers\/Microsoft.MachineLearningServices\/workspaces\/avadevitsmlsvc\/experiments\/deal-deal-nema\/runs\/7abe9617-ac79-413b-8843-7fd3878313f0).\r\n\r\nWhen my dataset has more than ~1200 features, I consistently get this error, but when there are fewer features it works fine. Is there some limitation here?",
        "Challenge_closed_time":1581034133000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1566233711000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to register a model using Jupyter Notebook and is receiving an error message stating \"HttpOperationError: Operation returned an invalid status code 'Service invocation failed!Request: GET https:\/\/cert-westeurope.experiments.azureml.net\/rp\/workspaces'\". The code being used to register the model is provided in the post.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/534",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.08,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":4111.2283333333,
        "Challenge_title":"Azureml Automl \"Error: Null\" Vague Error",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":59,
        "Discussion_body":"Hi Nema. Unfortunately I don't have access to your workspace. Would you provide more details on the failed experiment such as experiment\/pipeline id so that I can take a look at the logs of it? Hi Sonny, here is the run id: `eb6f111d-1251-40d2-b745-e3c4fbb31fcf` Thank you for the runid. I found automl setup has been timed out after some time. I will work with automl team for more details.  It seems to have been a one-off random occurrence. Considering solved. Somehow I lost track on this. You can let me know if you have any further issues.  ",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1440323757107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":424.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":1.0847583334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Does anybody install DVC on MinIO storage?<\/p>\n<p>I have read <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type\" rel=\"nofollow noreferrer\">docs<\/a> but not all clear for me.<\/p>\n<p>Which command should I use for setup MinIO storage with this entrance parameters:<\/p>\n<p>storage url: <a href=\"https:\/\/minio.mysite.com\/minio\/bucket-name\/\" rel=\"nofollow noreferrer\">https:\/\/minio.mysite.com\/minio\/bucket-name\/<\/a>\nlogin: my_login\npassword: my_password<\/p>",
        "Challenge_closed_time":1621599285543,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621595380413,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to install DVC on MinIO storage and is looking for the appropriate command to set up the storage with specific login and password parameters.",
        "Challenge_last_edit_time":1621622008696,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67635688",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":7.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.0847583334,
        "Challenge_title":"Installation DVC on MinIO storage",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1547.0,
        "Challenge_word_count":47,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526481416047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":85.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p><strong>Install<\/strong><\/p>\n<p>I usually use it as a Python package, int this case you need to install:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install &quot;dvc[s3]&quot;\n<\/code><\/pre>\n<p><strong>Setup remote<\/strong><\/p>\n<p>By default DVC supports AWS S3 storages and they work fine.<br \/>\nAlso they support &quot;S3-compatible storage&quot;, but setup for this type of remotes is nod described properly. In particular case of MinIO you have <strong>bucket<\/strong> - directory on MinIO server where actual data stores (it is similar to AWS bucket), but DVC uses AWS CLI to authenticate. In case of MinIO you need to pass them explicitly.<\/p>\n<p>Then follow commands to setup your DVC remote:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code># setup default remote (change &quot;bucket-name&quot; to your minio backet name)\ndvc remote add -d minio s3:\/\/bucket-name -f\n\n# add information about storage url (where &quot;https:\/\/minio.mysite.com&quot; your url)\ndvc remote modify minio endpointurl https:\/\/minio.mysite.com\n\n#  add info about login and password\ndvc remote modify minio access_key_id my_login\ndvc remote modify minio secret_access_key my_password\n<\/code><\/pre>\n<p><strong>If you move from old remote<\/strong>, use follow command to move your data:<\/p>\n<p>Before setup (download all old remote cache to local machine):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc pull -r &lt;old_remote_name&gt; --all-commits --all-tags --all-branches\n<\/code><\/pre>\n<p>After setup (upload all cache to a new remote):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc push -r &lt;new_remote_name&gt; --all-commits --all-tags --all-branches\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.0,
        "Solution_reading_time":21.73,
        "Solution_score_count":6.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":204.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":111.9549286111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am exploring AWS sagemaker for ML. I have created a bucket:<\/p>\n\n<pre><code>bucket_name = 'test-bucket' \ns3 = boto3.resource('s3')\ntry:\n   if  my_region == 'us-east-1':\n      s3.create_bucket(Bucket=bucket_name)\n   else: \n      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\nprint('S3 bucket created successfully')\nexcept Exception as e:\n    print('S3 error: ',e)\n<\/code><\/pre>\n\n<p>I have a csv in my local and I want to load that into the bucket I created.<\/p>\n\n<p>All the links I have referred  have directions to load it from a link and unzip it. Is there a way to load the data into the bucket from the local.<\/p>",
        "Challenge_closed_time":1545428602016,
        "Challenge_comment_count":0,
        "Challenge_created_time":1545025564273,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is exploring AWS Sagemaker for ML and has created an S3 bucket. They want to load a CSV file from their local machine into the bucket, but are unable to find any resources that provide directions for doing so.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53809556",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":111.9549286111,
        "Challenge_title":"Load csv into S3 from local",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1703.0,
        "Challenge_word_count":92,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455496483356,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco, CA, USA",
        "Poster_reputation_count":3912.0,
        "Poster_view_count":311.0,
        "Solution_body":"<p>If you are using Amazon SageMaker you can use the SageMaker python library that is implementing the most useful commands for data scientists, including the upload of files to S3. It is already installed on your SageMaker notebook instance by default. <\/p>\n\n<pre><code>import sagemaker\nsess = sagemaker.Session()\n\n# Uploading the local file to S3\nsess.upload_data(path='local-file.txt', bucket=bucket_name, key_prefix='input')    \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.2,
        "Solution_reading_time":5.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1443426419048,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Challenge_adjusted_solved_time":7.5235611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Since connecting to Azure SQL database from \u201cExecute R Script\u201d module in \u201cAzure Machine Learning Studio\u201d is not possible, and using Import Data modules (a.k.a Readers) is the only recommended approach, my question is that what can I do when I need more than 2 datasets as input for \"Execute R Script module\"?<\/p>\n\n<pre><code>\/\/ I'm already doing the following to get first 2 datasets,\ndataset1 &lt;- maml.mapInputPort(1)\ndataset2 &lt;- maml.mapInputPort(2)\n<\/code><\/pre>\n\n<p>How can I \"import\" a dataset3?<\/p>",
        "Challenge_closed_time":1491492855983,
        "Challenge_comment_count":1,
        "Challenge_created_time":1491465771163,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge while using the \"Execute R Script\" module in \"Azure Machine Learning Studio\" as they are unable to connect to Azure SQL database from the module. They are using Import Data modules to get the datasets, but they need more than 2 datasets as input for the module and are unsure how to import a third dataset.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/43249220",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.0,
        "Challenge_reading_time":7.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":7.5235611111,
        "Challenge_title":"Need more than 2 datasets for \u201cExecute R Script\u201d module in \u201cAzure Machine Learning Studio\u201d",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":337.0,
        "Challenge_word_count":91,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1487901477287,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>One thing you can do is combining two data-sets together and selecting the appropriate fields using the R script. That would be an easy workaround.   <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":1.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.9460580556,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I am trying to connect to my Azure ML workspace using SDK for python, using Virtual Studio Code to do so. After pip installing the needed SDK packages:    <br \/>\n    pip install azureml-sdk  <br \/>\n    pip install azureml-sdk[notebooks,automl,explain]  <\/p>\n<p>I downloaded the .json configuration file for my workspace, made sure it was in the correct location for the file path and tried the following code (with my subscription id, resource group and workspace name in place of the fillers in this chunk of code):    <\/p>\n<pre><code>{  \n    &quot;subscription_id&quot;: &quot;1234567-abcde-890-fgh...&quot;,  \n    &quot;resource_group&quot;: &quot;aml-resources&quot;,  \n    &quot;workspace_name&quot;: &quot;aml-workspace&quot;  \n}  \n<\/code><\/pre>\n<p>Upon executing this in my ipy kernel in Virutal Studio Code I got a UserErrorException (see image below, I have blocked out subscription id's and other sensitive information):    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/41683-subscriptionerror.png?platform=QnA\" alt=\"41683-subscriptionerror.png\" \/>    <\/p>\n<p>I then tried this alternative way to connect to my workspace using the following code (again with my info filled in instead of the fillers in the code):    <br \/>\n    from azureml.core import Workspace  <\/p>\n<pre><code>from azureml.core import Workspace  \n  \nws = Workspace.get(name='aml-workspace',  \n                   subscription_id='1234567-abcde-890-fgh...',  \n                   resource_group='aml-resources')  \n  \nws = Workspace.from_config()  \n<\/code><\/pre>\n<p>This produced the same error upon execution. I have tried using different subscriptions with different workspace names and resource groups and it gives me the same error every time. It appears to be telling me I do not have access to the subscription that I am logged in to? I am unsure how to fix this. I am trying to do this as part of the lessons in the Microsoft Azure Data Scientist certification if anyone is familiar with that or has run into the same problem while trying to complete the modules for that certification provided through Microsoft.     <\/p>",
        "Challenge_closed_time":1606097262372,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606083056563,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing issues while connecting to their Azure ML workspace using SDK for Python in Virtual Studio Code. They have installed the required SDK packages and downloaded the .json configuration file for their workspace. However, upon executing the code, they are getting a UserErrorException, which suggests that they do not have access to the subscription they are logged in to. They have tried using different subscriptions with different workspace names and resource groups, but the error persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/171465\/problems-connecting-to-workspace-using-azure-machi",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":11.1,
        "Challenge_reading_time":26.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":3.9460580556,
        "Challenge_title":"Problems connecting to workspace using Azure Machine Learning SDK for Python",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":279,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>can you try using InteractiveLoginAuthentication?  <\/p>\n<p>below code might help you  <\/p>\n<pre><code>from azureml.core.authentication import InteractiveLoginAuthentication\nia = InteractiveLoginAuthentication(tenant_id='YourTenant id')\n# You can find tenant id under azure active directory-&gt;properties\nws = Workspace.get(name='aml-workspace',\n                    subscription_id='1234567-abcde-890-fgh...',\n                    resource_group='aml-resources',auth=ia)\n<\/code><\/pre>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":21.7,
        "Solution_reading_time":6.01,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1565289301123,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":79.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":1547.3938463889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking for a working example how to access data on a <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data#access-datastores-during-training\" rel=\"nofollow noreferrer\">Azure Machine Learning managed data store<\/a> from within a train.py script. I followed the instructions in the link and my script is able to resolve the datastore.<\/p>\n\n<p>However, whatever I tried (<code>as_download(), as_mount()<\/code>) the only thing I always got was a <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.data_reference.datareference?view=azure-ml-py\" rel=\"nofollow noreferrer\">DataReference<\/a> object. Or maybe I just don't understand how actually read data from a file with that.<\/p>\n\n<pre><code>run = Run.get_context()\nexp = run.experiment\nws = run.experiment.workspace\n\nds = Datastore.get(ws, datastore_name='mydatastore')\ndata_folder_mount = ds.path('mnist').as_mount()\n\n# So far this all works. But how to go from here?\n<\/code><\/pre>",
        "Challenge_closed_time":1565289458360,
        "Challenge_comment_count":4,
        "Challenge_created_time":1559718840513,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking a working example of how to access data on an Azure Machine Learning managed data store from within a train.py script. They have followed the instructions in the link provided and their script is able to resolve the datastore, but they are unable to read data from a file and are only getting a DataReference object.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56455761",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":12.2,
        "Challenge_reading_time":13.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1547.3938463889,
        "Challenge_title":"Access data on AML datastore from training script",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":866.0,
        "Challenge_word_count":107,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1342685175156,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":12103.0,
        "Poster_view_count":1451.0,
        "Solution_body":"<p>You can pass in the DataReference object you created as the input to your training product (scriptrun\/estimator\/hyperdrive\/pipeline). Then in your training script, you can access the mounted path via argument.\nfull tutorial: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.0,
        "Solution_reading_time":6.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0855555556,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nI am currently defining some machines configuration using machine-env1.yaml, machine-env2.yaml which basically contains node selectors and CPU, GPU, and TPU requests configuration, and then running:\n\npolyaxon run -f polyaxonfile.yaml -f machine-env1.yaml\n\nI have two problems with this approach:\n\nI need to copy the env files to all our git repos, which means if I make a change I need to perform several pull requests\nI need to tell the data-scientits to pull the last commit, sometimes that's not possible because they can not merge\/rebase the changes.\n\nBased on those two issues, in the end we tell data-scientists to just use:\n\nenvironment:\n  nodeSelector:\n    nodes: large-pool\n...\nrun:\n  ...\n  container:\n      resources:\n        limits:\n          cpu: 3000m\n          memory: 6000Mi\n        requests:\n          cpu: 2000m\n          memory: 4000Mi\n\nWhich is error prone and confusing for them, and make the files bigger and difficult to change.\n\nAny elegant way to abstract this type of configuration from the data-scientists?",
        "Challenge_closed_time":1649337274000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649336966000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in configuring Polyaxon for their data scientists. They are currently using machine-env files to define node selectors and CPU, GPU, and TPU requests configuration, but this approach requires copying the files to all git repos and telling data scientists to pull the last commit. As a result, they are looking for a more elegant way to abstract this type of configuration from the data scientists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1484",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.6,
        "Challenge_reading_time":13.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.0855555556,
        "Challenge_title":"I would like to configure Polyaxon in a way to avoid asking data-scientists to configure pre-emptible node-pools or request TPUs on their own",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":170,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"We have already shared a resource on how to configure the environments in this guide\n\nif you are using multiple git repos and you do not want to replicate the yaml files in all repos you can register those files as presets:\n\nUsers will be able to use --presets machine1 or --presets=env1\n\nNote that in the example in that link, it shows that it defines a queue but you do not have to define a queue, a preset is just any YAML file that can be used with the override operator -f main.yaml -f override1.yaml -f override2.yaml in this case override1.yaml and override2.yaml it can be saved as organization presets using the UI.\n\nMore info from the intro section about presets and the UI section\n\nAlso, when you define presets you can use them directly on the operation or component\n\npresets: [preset1, preset2]\n\nThis is similar to the CLI command\n\npolyaxon run -f polyaxon.yaml --presets preset1,preset2",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":10.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":156.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":33.9291283333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We have migrated 4 on-premise DBs to Azure in an Azure Managed Instance.<\/p>\n<p>We are trying to connect to these DBs from Azure Machine Learning Studio.<\/p>\n<p>Some googling and a 2 year old post suggests ML does not support Managed Instances, only SQL Databases.  Our testing kinda confirmed that :-(<\/p>\n<p>Is it correct that ML can only connect to a SQL Databases and not a MI?<\/p>\n<p>We are using MI so that we can use Azure Managed Instance Link once we have site to site vpn configured.<\/p>\n<p>Thanks,<\/p>\n<p>Ross<\/p>",
        "Challenge_closed_time":1675311477688,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675189332826,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in connecting to Azure Managed Instance from Azure Machine Learning Studio. They have migrated 4 on-premise DBs to Azure Managed Instance and are trying to connect to these DBs from Azure Machine Learning Studio. However, they have found that ML only supports SQL Databases and not Managed Instances. They are using Managed Instance to utilize Azure Managed Instance Link once they have site to site VPN configured.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1165919\/how-to-connect-to-managed-instance-from-machine-le",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":7.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":33.9291283333,
        "Challenge_title":"How To Connect To Managed Instance from Machine Learning Studio",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":98,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@<a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=24186a71-0c44-4fba-8380-5a960c9e0867\">RossW<\/a> Yes, Azure Machine Learning studio currently does not support connections to Azure Managed Instances. It only supports connections to Azure SQL Databases. You can use the SQL Database as a data source for your machine learning models in Azure Machine Learning studio.<\/p>\n<p>To connect to an Azure SQL Database from Azure Machine Learning studio, you need to follow these steps:<\/p>\n<ol>\n<li> Create an Azure SQL Database and make sure that it is accessible from your Azure Machine Learning workspace.<\/li>\n<li> In Azure Machine Learning studio, go to the Data tab and click on the +New button.<\/li>\n<li> Select the SQL Database option and provide the necessary details, such as the server name, database name, and authentication method.<\/li>\n<li> Click on the Connect button to establish a connection to the Azure SQL Database.<\/li>\n<li> Once the connection is established, you can use the SQL Database as a data source for your machine learning models in Azure Machine Learning studio.<\/li>\n<\/ol>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.0,
        "Solution_reading_time":13.94,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":162.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":32.7005666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/GU4v0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/GU4v0.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>How is the above distance calculated and can I set my own threshold?<\/p>",
        "Challenge_closed_time":1662747458550,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662629736510,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on how the Baseline drift distance is calculated in data quality monitoring in Sagemaker and whether they can set their own threshold.",
        "Challenge_last_edit_time":1663220877288,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73646830",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":4.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":32.7005666667,
        "Challenge_title":"What inference can be made out of Baseline drift distance in data quality monitoring sagemaker?",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":55.0,
        "Challenge_word_count":37,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1660119311500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>You can find information on how the distributions are compared here (see <code>distribution_constraints<\/code> in the table):<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-byoc-constraints.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-byoc-constraints.html<\/a><\/p>\n<p>You can change the threshold in the constraint file to what you would like.<\/p>\n<p>The baseline computes baseline schema constraints and statistics for each feature using Deequ. If youwould like more details you can take a look at the implementation here:<\/p>\n<p><a href=\"https:\/\/github.com\/awslabs\/deequ\/blob\/master\/src\/main\/scala\/com\/amazon\/deequ\/analyzers\/Distance.scala\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/deequ\/blob\/master\/src\/main\/scala\/com\/amazon\/deequ\/analyzers\/Distance.scala<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":22.8,
        "Solution_reading_time":11.71,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1435766573232,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":10645.0,
        "Answerer_view_count":1173.0,
        "Challenge_adjusted_solved_time":37311.0782480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am sort of new to Python, so I probably don't understand fully how to exactly import the libraries correctly into Azure ML.<\/p>\n\n<p>I have a bunch of data stored in Table storage which I have local Python code to successfully join all of them as a preparation for the ML experiment. I learned that AzureML environment does not have the Azure-Storage libraries installed, and therefore procceded the steps according <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn955437.aspx\" rel=\"nofollow noreferrer\">this<\/a> to upload a ZIP file containing the Azure-storage libraries that I found under anaconda3\\lib\\site-packages. I took all of the azure directories and shoved them under one single zip file and followed the bottom of the document in the link to upload the zip file as a DataSet and attach the dataset to an Execute Python script node in ML.<\/p>\n\n<p>I am getting errors like this when I try to run the node:<\/p>\n\n<pre><code>requestId = 825883c7ccb74f7e869e68e60d3cd919 errorComponent=Module. taskStatusCode=400. e \"C:\\pyhome\\lib\\ssl.py\", line 856, in send return self._sslobj.write(data) File \"C:\\pyhome\\lib\\ssl.py\", line 581, in write return self._sslobj.write(data)socket.timeout: The write operation timed outDuring handling of the above exception, another exception occurred:Traceback (most recent call last): File \"C:\\pyhome\\lib\\site-packages\\requests\\adapters.py\", line 376, in send timeout=timeout File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 609, in urlopen _stacktrace=sys.exc_info()[2]) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\util\\retry.py\", line 247, in increment raise six.reraise(type(error), error, _stacktrace) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\packages\\six.py\", line 309, in reraise raise value.with_traceback(tb) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 559, in urlopen body=body, headers=headers) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 353, in _make_request conn.request(method, url, **httplib_request_kw) File \"C:\\pyhome\\lib\\http\\client.py\", line 1083, in request self._send_request(method, url, body, headers) File \"C:\\pyhome\\lib\\http\\client.py\", line 1128, in _send_request self.endheaders(body) File \"C:\\pyhome\\lib\\http\\client.py\", line 1079, in endheaders self._send_output(message_body) File \"C:\\pyhome\\lib\\http\\client.py\", line 911, in _send_output self.send(msg) File \"C:\\pyhome\\lib\\http\\client.py\", line 885, in send self.sock.sendall(data) File \"C:\\pyhome\\lib\\ssl.py\", line 886, in sendall v = self.send(data[count:]) File \"C:\\pyhome\\lib\\ssl.py\", line 856, in send return self._sslobj.write(data) File \"C:\\pyhome\\lib\\ssl.py\", line 581, in write return self._sslobj.write(data)requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', timeout('The write operation timed out',))During handling of the above exception, another exception occurred:Traceback (most recent call last): File \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 221, in _perform_request response = self._httpclient.perform_request(request) File \"c:\\temp\\script bundle\\azure\\storage\\_http\\httpclient.py\", line 114, in perform_request proxies=self.proxies) File \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 468, in request resp = self.send(prep, **send_kwargs) File \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 576, in send r = adapter.send(request, **kwargs) File \"C:\\pyhome\\lib\\site-packages\\requests\\adapters.py\", line 426, in send raise ConnectionError(err, request=request)requests.exceptions.ConnectionError: ('Connection aborted.', timeout('The write operation timed out',))During handling of the above exception, another exception occurred:Traceback (most recent call last): File \"C:\\server\\invokepy.py\", line 199, in batch odfs = mod.azureml_main(*idfs) File \"C:\\temp\\fa22884a19884f658d411dc0bdf05715.py\", line 33, in azureml_main data = table_service.query_entities(table_name) File \"c:\\temp\\script bundle\\azure\\storage\\table\\tableservice.py\", line 728, in query_entities resp = self._query_entities(*args, **kwargs) File \"c:\\temp\\script bundle\\azure\\storage\\table\\tableservice.py\", line 795, in _query_entities operation_context=_context) File \"c:\\temp\\script bundle\\azure\\storage\\table\\tableservice.py\", line 1093, in _perform_request return super(TableService, self)._perform_request(request, parser, parser_args, operation_context) File \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 279, in _perform_request raise ex File \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 251, in _perform_request raise AzureException(ex.args[0])azure.common.AzureException: ('Connection aborted.', timeout('The write operation timed out',))Process returned with non-zero exit code \n<\/code><\/pre>\n\n<p>I am not sure what I am doing wrong<\/p>",
        "Challenge_closed_time":1511661620620,
        "Challenge_comment_count":0,
        "Challenge_created_time":1511629802230,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble importing Azure Storage libraries into AzureML. They have uploaded a ZIP file containing the libraries as a DataSet and attached it to an Execute Python script node, but are encountering errors related to connection timeouts when trying to run the node. The user is unsure of what they are doing wrong.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47488544",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":65.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":62,
        "Challenge_solved_time":8.8384416667,
        "Challenge_title":"Using Azure Storage libraries in AzureML - Custom python library",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":150.0,
        "Challenge_word_count":471,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340380852680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":73.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>In Azure ML Studio, Python scripts (and R scripts for that matter) run in a sandbox so cannot access resources over the network. See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts?#limitations\" rel=\"nofollow noreferrer\">limitations<\/a>:<\/p>\n<blockquote>\n<p>The Execute Python Script currently has the following limitations:<\/p>\n<ol>\n<li>Sandboxed execution. The Python runtime is currently sandboxed and, as\na result, does not allow access to the network...<\/li>\n<\/ol>\n<\/blockquote>\n<p>So if you want to read from a blob use the separate Import Data module and if you want to write to a blob use the separate Export Data module.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1645949683923,
        "Solution_link_count":1.0,
        "Solution_readability":11.5,
        "Solution_reading_time":8.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":23.5547222222,
        "Challenge_answer_count":0,
        "Challenge_body":"When I don't have the optional MLFlow dependency installed I get the following exception the first time I try to import the `numbertracker`.  The second time I run the import, everything works just fine.\r\n\r\n```python\r\nfrom whylogs.core.statistics import numbertracker\r\n\r\n\r\n\r\nFailed to import MLFLow\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n<ipython-input-1-3964e19b3cb4> in <module>\r\n----> 1 from whylogs.core.statistics import numbertracker\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/__init__.py in <module>\r\n      4 from .app.session import get_or_create_session\r\n      5 from .app.session import reset_default_session\r\n----> 6 from .mlflow import enable_mlflow\r\n      7 \r\n      8 __all__ = [\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/mlflow\/__init__.py in <module>\r\n----> 1 from .patcher import enable_mlflow\r\n      2 \r\n      3 __all__ = [\"enable_mlflow\"]\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/mlflow\/patcher.py in <module>\r\n    145 \r\n    146 _active_whylogs = []\r\n--> 147 _original_end_run = mlflow.tracking.fluent.end_run\r\n    148 \r\n    149 \r\n\r\nNameError: name 'mlflow' is not defined\r\n```",
        "Challenge_closed_time":1603222865000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603138068000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge where any user can access any MLflow project, and they want to restrict access to only project members.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/72",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.9,
        "Challenge_reading_time":14.1,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":95.0,
        "Challenge_repo_issue_count":1268.0,
        "Challenge_repo_star_count":2244.0,
        "Challenge_repo_watch_count":30.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":23.5547222222,
        "Challenge_title":"MLFlow NameError",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":108,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1574678086832,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Amstelveen, Netherlands",
        "Answerer_reputation_count":3917.0,
        "Answerer_view_count":640.0,
        "Challenge_adjusted_solved_time":6.4474319444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have installed mlflow on GCP VM instance, \nnow I want to access mlflow UI with external IP.\nI tried setting up a firewall rule and opening the default port for mlflow, but not able to access it.\nCan someone give step by step process for just running mlflow on VM instance?<\/p>",
        "Challenge_closed_time":1583767598368,
        "Challenge_comment_count":3,
        "Challenge_created_time":1583744387613,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has installed MLFlow on a GCP VM instance and is trying to access the MLFlow UI with an external IP. Despite setting up a firewall rule and opening the default port for MLFlow, the user is unable to access it and is seeking a step-by-step process for running MLFlow on a VM instance.",
        "Challenge_last_edit_time":1583832420660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60597319",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.2,
        "Challenge_reading_time":3.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":6.4474319444,
        "Challenge_title":"Running MLFlow on GCP VM",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1537.0,
        "Challenge_word_count":56,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1451124057623,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":736.0,
        "Poster_view_count":234.0,
        "Solution_body":"<p>I've decided to check on my test VM and run mlflow server on GCE VM. Have a look at my steps below:<\/p>\n\n<ol>\n<li>create VM instance based on Ubuntu Linux 18.04 LTS<\/li>\n<li><p><a href=\"https:\/\/www.mlflow.org\/docs\/latest\/quickstart.html\" rel=\"noreferrer\">install MLflow<\/a>:<\/p>\n\n<pre><code>$ sudo apt update\n$ sudo apt upgrade\n$ cd ~\n$ git clone https:\/\/github.com\/mlflow\/mlflow\n$ cd mlflow\n$ sudo apt install python3-pip\n$ pip3 install mlflow\n$ python3 setup.py build\n$ sudo python3 setup.py install\n$ mlflow --version\nmlflow, version 1.7.1.dev0\n<\/code><\/pre><\/li>\n<li><p>run mlflow server on internal IP of VM instance (default 127.0.0.1):<\/p>\n\n<pre><code>$ ifconfig \nens4: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1460\ninet 10.XXX.15.XXX  netmask 255.255.255.255  broadcast 0.0.0.0\n...\n\n$ mlflow server --host 10.XXX.15.XXX\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Starting gunicorn 20.0.4\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Listening at: http:\/\/10.128.15.211:5000 (8631)\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Using worker: sync\n[2020-03-09 15:05:50 +0000] [8634] [INFO] Booting worker with pid: 8634\n[2020-03-09 15:05:51 +0000] [8635] [INFO] Booting worker with pid: 8635\n[2020-03-09 15:05:51 +0000] [8636] [INFO] Booting worker with pid: 8636\n[2020-03-09 15:05:51 +0000] [8638] [INFO] Booting worker with pid: 8638\n<\/code><\/pre><\/li>\n<li><p>check from VM instance (from second connection):<\/p>\n\n<pre><code>$ curl -I http:\/\/10.XXX.15.XXX:5000\nHTTP\/1.1 200 OK\nServer: gunicorn\/20.0.4\nDate: Mon, 09 Mar 2020 15:06:08 GMT\nConnection: close\nContent-Length: 853\nContent-Type: text\/html; charset=utf-8\nLast-Modified: Mon, 09 Mar 2020 14:57:11 GMT\nCache-Control: public, max-age=43200\nExpires: Tue, 10 Mar 2020 03:06:08 GMT\nETag: \"1583765831.3202355-853-3764264575\"\n<\/code><\/pre><\/li>\n<li><p><a href=\"https:\/\/cloud.google.com\/vpc\/docs\/add-remove-network-tags\" rel=\"noreferrer\">set network tag<\/a> <code>mlflow-server<\/code> <\/p><\/li>\n<li><p><a href=\"https:\/\/cloud.google.com\/vpc\/docs\/using-firewalls#creating_firewall_rules\" rel=\"noreferrer\">create firewall rule<\/a> to allow access on port 5000<\/p>\n\n<pre><code>$ gcloud compute --project=test-prj firewall-rules create mlflow-server --direction=INGRESS --priority=999 --network=default --action=ALLOW --rules=tcp:5000 --source-ranges=0.0.0.0\/0 --target-tags=mlflow-server\n<\/code><\/pre><\/li>\n<li><p>check from on-premises Linux machine <code>nmap -Pn 35.225.XXX.XXX<\/code><\/p>\n\n<pre><code>Starting Nmap 7.80 ( https:\/\/nmap.org ) at 2020-03-09 16:20 CET\nNmap scan report for 74.123.225.35.bc.googleusercontent.com (35.225.XXX.XXX)\nHost is up (0.20s latency).\nNot shown: 993 filtered ports\nPORT     STATE  SERVICE\n...\n5000\/tcp open   upnp\n...\n<\/code><\/pre><\/li>\n<li><p>go to web browser <a href=\"http:\/\/35.225.XXX.XXX:5000\/\" rel=\"noreferrer\">http:\/\/35.225.XXX.XXX:5000\/<\/a><\/p><\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/u2aFt.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/u2aFt.png\" alt=\"mlflow\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":11.0,
        "Solution_readability":8.4,
        "Solution_reading_time":39.18,
        "Solution_score_count":5.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":297.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1417158887760,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Gurgaon, India",
        "Answerer_reputation_count":872.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":0.7564702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created AWS S3 bucket and tried sample kmeans example on Jupyter notebook.\nBeing account owner I have read\/write permissions but I am unable to write logs with following error, <\/p>\n\n<pre><code> ClientError: An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n<\/code><\/pre>\n\n<p>here's the kmeans sample code, <\/p>\n\n<pre><code> from sagemaker import get_execution_role\n role = get_execution_role()\n bucket='testingshk' \n\n import pickle, gzip, numpy, urllib.request, json\nurllib.request.urlretrieve(\"http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl.gz\", \"mnist.pkl.gz\")\n with gzip.open('mnist.pkl.gz', 'rb') as f:\n train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n\n from sagemaker import KMeans\n data_location = 's3:\/\/{}\/kmeans_highlevel_example\/data'.format(bucket)\n output_location = 's3:\/\/{}\/kmeans_example\/output'.format(bucket)\n\n print('training data will be uploaded to: {}'.format(data_location))\n print('training artifacts will be uploaded to: {}'.format(output_location))\n\n kmeans = KMeans(role=role,\n            train_instance_count=2,\n            train_instance_type='ml.c4.8xlarge',\n            output_path=output_location,\n            k=10,\n            data_location=data_location)\n kmeans.fit(kmeans.record_set(train_set[0]))\n<\/code><\/pre>",
        "Challenge_closed_time":1523661700916,
        "Challenge_comment_count":0,
        "Challenge_created_time":1523658977623,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an Access Denied error while trying to write logs to an AWS S3 bucket using a kmeans sample code on Jupyter notebook. The user has read\/write permissions as an account owner but is still unable to write logs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49826004",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":15.2,
        "Challenge_reading_time":16.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.7564702778,
        "Challenge_title":"AWS S3 bucket write error",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":915.0,
        "Challenge_word_count":112,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400307037672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Austria",
        "Poster_reputation_count":83.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>Even if you have all the access to the bucket, you need to provide access key and secret in order to put some object in bucket if it is private. Or if you make bucket access public to all then you can push object to bucket without any problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":2.97,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1429262032907,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Manchester, United Kingdom",
        "Answerer_reputation_count":11490.0,
        "Answerer_view_count":2150.0,
        "Challenge_adjusted_solved_time":0.3111347222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Are these roles to be deleted?<\/p>\n<ol>\n<li>AmazonSageMakerServiceCatalogProductsLaunchRole<\/li>\n<li>AmazonSageMakerServiceCatalogProductsUseRole<\/li>\n<li>AWSServiceRoleForAmazonSageMakerNotebooks<\/li>\n<\/ol>\n<p>Are these roles to be deleted?<\/p>\n<ol>\n<li>AmazonSageMakerServiceCatalogProductsUseRole<\/li>\n<li>Plus some execution policies<\/li>\n<\/ol>\n<p>Is Jupyter server within sagemaker studio also be stopped for not being charged?<\/p>",
        "Challenge_closed_time":1633709785048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633708664963,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking advice on which IAM roles and policies to delete in order to avoid being charged by AWS. They have listed some roles and policies and are asking if they should be deleted. They also inquire about stopping the Jupyter server within SageMaker Studio to avoid charges.",
        "Challenge_last_edit_time":1634116759467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69498670",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":6.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.3111347222,
        "Challenge_title":"Which IAM roles and policies should I delete to not being charged by AWS?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":105.0,
        "Challenge_word_count":50,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1623330365063,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":75.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p><strong>AWS IAM is a free service<\/strong> - you do not get charged for roles, policies or any other aspect of IAM.<\/p>\n<p>From <a href=\"https:\/\/aws.amazon.com\/iam\/#:%7E:text=IAM%20is%20a%20feature%20of,AWS%20services%20by%20your%20users.\" rel=\"nofollow noreferrer\">the documentation<\/a>:<\/p>\n<blockquote>\n<p>IAM is a feature of your AWS account offered at no additional charge. You will be charged only for use of other AWS services by your users.<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.0,
        "Solution_reading_time":6.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1653511725307,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":26.0,
        "Challenge_adjusted_solved_time":42.7102333333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to pull a prebuilt xgboost image from the public aws xgboost registry specified here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title<\/a>, however whenever I run the sagemaker pipeline I get the error:<\/p>\n<pre><code>ClientError: Failed to invoke sagemaker:CreateModelPackage. \nError Details: Access denied for registry ID: 246618743249, repository name: sagemaker-xgboost. \nPlease check if your ECR image exists and has proper pull permissions for SageMaker.\n<\/code><\/pre>\n<p>Here is the attached role boundary I am using to run the pipeline:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Action&quot;: [\n                &quot;codebuild:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;codepipeline:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;events:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;logs:CreateLogGroup&quot;,\n                &quot;logs:CreateLogStream&quot;,\n                &quot;logs:DescribeLogStreams&quot;,\n                &quot;logs:DescribeLogGroups&quot;,\n                &quot;logs:PutLogEvents&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:iam::xxxxxxxxxxxx:role\/ml-*&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;ecr:*&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:ecr:us-west-2:246618743249:repository\/246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;ecr:GetAuthorizationToken&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;*&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>and below is the attached policies for the role:<\/p>\n<pre><code>{\n    &quot;Statement&quot;: [\n        {\n            &quot;Action&quot;: &quot;ecr:*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Sid&quot;: &quot;&quot;\n        }\n    ],\n    &quot;Version&quot;: &quot;2012-10-17&quot;\n}\n<\/code><\/pre>\n<p>plus the AWSCodePipelineFullAccess, AWSCodeBuildAdminAccess, and AmazonSageMakerFullAccess managed policies.<\/p>\n<p>Why can't I access the image\/why am I getting this error? As you can see I gave my role full permissions for the ecr registry in the boundary, and full permissions for ecr in the attached policy.<\/p>",
        "Challenge_closed_time":1658508649572,
        "Challenge_comment_count":2,
        "Challenge_created_time":1658354562373,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an \"Access denied\" error while trying to pull a prebuilt xgboost image from the public AWS xgboost registry. The error message suggests that the user does not have proper pull permissions for SageMaker. The user has provided the attached role boundary and policies, which include full permissions for the ECR registry, AWSCodePipelineFullAccess, AWSCodeBuildAdminAccess, and AmazonSageMakerFullAccess managed policies. The user is seeking help to understand why they are unable to access the image despite having full permissions.",
        "Challenge_last_edit_time":1658354892732,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73058582",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":21.7,
        "Challenge_reading_time":36.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":42.8019997222,
        "Challenge_title":"Access denied for aws public sagemaker xgboost registry",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":201,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1653511725307,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>I had to change the boundary to be this: <code> arn:aws:ecr:us-west-2:246618743249:repository\/sagemaker-xgboost<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":1.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":0.1979908334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently trying to connect to a DocumentDb (MongoDb) using Azure Machine Learning Studio.<\/p>\n\n<p>I am currently following <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/import-from-cosmos-db\" rel=\"nofollow noreferrer\">this<\/a> guide, however it seems out of date already. The assumptions I have taken have lead me to get an <code>Error 1000: ... DocumentDb client threw an exception<\/code> <code>The underlying connection was closed. The connection was closed unexpectedly.<\/code><\/p>\n\n<p>The guide, and Azure Machine Learning Studio, outline the following parameters to make a connection.<\/p>\n\n<p>Endpoint URL, Database ID, DocumentDb Key, Collection ID. It also tells you to look under the <code>Keys<\/code> blade to find these, which does not exist anymore.<\/p>\n\n<p>These are the assumptions I have taken;<\/p>\n\n<ul>\n<li>Endpoint URL = host + port under the Connection String blade. <code>https:\/\/host.com:port\/<\/code><\/li>\n<li>Database ID = the database name listed under the Data Explorer blade.<\/li>\n<li>DocumentDb Key = Primary Password under the Connection String blade.<\/li>\n<li>Collection ID = the name of a collection in the db from the Data Explorer blade.<\/li>\n<\/ul>\n\n<p>I have, for now, also opened all connections to the database just to make sure I wasn't closing the network to outside requests which, I guess, means that at least the DocumentDb key is a poor assumption.<\/p>\n\n<hr>\n\n<p>After some input from Jon, below, here is the current state of things<\/p>\n\n<ul>\n<li>Endpoint URL = the Uri from the Overview blade.<\/li>\n<li>Database ID = the database name listed under the Data Explorer blade.<\/li>\n<li>DocumentDb Key = the Primary Password under the Connection String blade.<\/li>\n<li>Collection ID = the name of a collection in the db from the Data Explorer blade.<\/li>\n<li>Sql query = <code>select top 10 * from CollectionID<\/code><\/li>\n<li>Sql parameters = {}<\/li>\n<\/ul>",
        "Challenge_closed_time":1530573817947,
        "Challenge_comment_count":12,
        "Challenge_created_time":1530540068210,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while trying to connect to a DocumentDb (MongoDb) using Azure Machine Learning Studio. The guide they are following seems to be outdated, and they are getting an error message. The guide outlines parameters to make a connection, but some of them are not available anymore. The user has made some assumptions, but they are not working. They have received some input from someone and have updated the parameters, but it is unclear if the issue has been resolved.",
        "Challenge_last_edit_time":1530573105180,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51137973",
        "Challenge_link_count":2,
        "Challenge_participation_count":13,
        "Challenge_readability":9.7,
        "Challenge_reading_time":25.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":9.3749269445,
        "Challenge_title":"Azure Machine Learning Studio - Import from Cosmos Db",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":801.0,
        "Challenge_word_count":268,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1350771597060,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1758.0,
        "Poster_view_count":191.0,
        "Solution_body":"<p>Through discussion in the comments, it may be that the \"Endpoint URL\" just needed to be updated, but I'll go over all of the inputs in case anyone else needs a reference to it.<\/p>\n\n<ul>\n<li>Endpoint URL - Can use the URI in the CosmosDB \"Overview\" pane in the Azure Portal<\/li>\n<li>Database ID - The name of the database to connect to<\/li>\n<li>DocumentDB Key - The primary password from the \"Connection Strings\" pane in the Azure Portal<\/li>\n<li>Collection ID - The name of the collection to read data from<\/li>\n<\/ul>\n\n<p>And, for reference, here's what my data explorer looks like in CosmosDB (database ID then collection ID):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/7Z4Q7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7Z4Q7.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And the settings in Azure ML Studio to import the data:\n<a href=\"https:\/\/i.stack.imgur.com\/LheXz.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LheXz.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.1,
        "Solution_reading_time":13.02,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":132.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1280505139752,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bangalore, India",
        "Answerer_reputation_count":4265.0,
        "Answerer_view_count":403.0,
        "Challenge_adjusted_solved_time":957.2375636111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For the Python API for tabular dataset of AzureML (<code>azureml.data.TabularDataset<\/code>), there are two experimental methods which have been introduced:<\/p>\n<ol>\n<li><code>download(stream_column, target_path=None, overwrite=False, ignore_not_found=True)<\/code><\/li>\n<li><code>mount(stream_column, mount_point=None)<\/code><\/li>\n<\/ol>\n<p>Parameter <code>stream_column<\/code> has been defined as The stream column to mount or download.<\/p>\n<p>What is the actual meaning of <code>stream_column<\/code>? I don't see any example any where?<\/p>\n<p>Any pointer will be helpful.<\/p>\n<p>The stack trace:<\/p>\n<pre><code>Method download: This is an experimental method, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n\/tmp\/ipykernel_11561\/3904436543.py in &lt;module&gt;\n----&gt; 1 tab_dataset.download(target_path=&quot;..\/data\/tabular&quot;)\n\n\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/_base_sdk_common\/_docstring_wrapper.py in wrapped(*args, **kwargs)\n     50     def wrapped(*args, **kwargs):\n     51         module_logger.warning(&quot;Method {0}: {1} {2}&quot;.format(func.__name__, _method_msg, _experimental_link_msg))\n---&gt; 52         return func(*args, **kwargs)\n     53     return wrapped\n     54 \n\n\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/data\/_loggerfactory.py in wrapper(*args, **kwargs)\n    130             with _LoggerFactory.track_activity(logger, func.__name__, activity_type, custom_dimensions) as al:\n    131                 try:\n--&gt; 132                     return func(*args, **kwargs)\n    133                 except Exception as e:\n    134                     if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\nTypeError: download() missing 1 required positional argument: 'stream_column'\n<\/code><\/pre>",
        "Challenge_closed_time":1646484376340,
        "Challenge_comment_count":1,
        "Challenge_created_time":1644217302490,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with the Python API for tabular dataset of AzureML, specifically with the experimental methods 'download' and 'mount'. The error message indicates that the 'download' method is missing a required positional argument 'stream_column', and the user is seeking clarification on the meaning of this parameter.",
        "Challenge_last_edit_time":1645197572643,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71014584",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.4,
        "Challenge_reading_time":25.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":629.7427361111,
        "Challenge_title":"Azure ML Tabular Dataset : missing 1 required positional argument: 'stream_column'",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":356.0,
        "Challenge_word_count":166,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1280505139752,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":4265.0,
        "Poster_view_count":403.0,
        "Solution_body":"<p><strong>Update on 5th March, 2022<\/strong><\/p>\n<p>I posted this as a support ticket with Azure. Following is the answer I have received:<\/p>\n<blockquote>\n<p>As you can see from our documentation of <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">TabularDataset Class<\/a>,\nthe \u201cstream_column\u201d parameter is required. So, that error is occurring\nbecause you are not passing any parameters when you are calling the\ndownload method.    The \u201cstream_column\u201d parameter should have the\nstream column to download\/mount. So, you need to pass the column name\nthat contains the paths from which the data will be streamed.<br \/>\nPlease find an example <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-labeled-dataset#explore-labeled-datasets-via-pandas-dataframe\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1648643627872,
        "Solution_link_count":2.0,
        "Solution_readability":11.5,
        "Solution_reading_time":12.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":97.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":1.5172702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to connect to remote tracking server (<a href=\"http:\/\/123.456.78.90\" rel=\"nofollow noreferrer\">http:\/\/123.456.78.90<\/a>) that requires authentication<\/p>\n<p>When I do this:<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>import mlflow\nmlflow.set_tracking_uri(\"http:\/\/123.456.78.90\")\nmlflow.set_experiment(\"my-experiment\")<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p>I get an error<\/p>\n<p><em>MlflowException: API request to endpoint \/api\/2.0\/mlflow\/experiments\/list failed with error code 401 != 200.\nResponse body: 401 Authorization Required<\/em><\/p>\n<p>I understand that I need to log in first but I have no idea how to do it<\/p>",
        "Challenge_closed_time":1637773273483,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637767811310,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to connect to a remote MLFlow tracking server that requires authentication. However, when attempting to connect, they receive an error message indicating that they need to log in first, but they are unsure how to do so.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70098779",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":11.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.5172702778,
        "Challenge_title":"How to connect to MLFlow tracking server that has auth?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":2102.0,
        "Challenge_word_count":85,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1637766437852,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p><a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#logging-to-a-tracking-server\" rel=\"nofollow noreferrer\">MLflow documentation<\/a> says:<\/p>\n<blockquote>\n<p><code>MLFLOW_TRACKING_USERNAME<\/code> and <code>MLFLOW_TRACKING_PASSWORD<\/code> - username and password to use with HTTP Basic authentication. To use Basic authentication, you must set both environment variables.<\/p>\n<\/blockquote>\n<p>So you just need to set these variables in your code using <code>os.environ<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>os.environ['MLFLOW_TRACKING_USERNAME'] = 'name'\nos.environ['MLFLOW_TRACKING_PASSWORD'] = 'pass'\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.1,
        "Solution_reading_time":8.68,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":50.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1600124498003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":110.7464591667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've set up a ClearML server in GCP using the sub-domain approach. I can access all three domains (<code>https:\/\/app.clearml.mydomain.com<\/code>, <code>https:\/\/api.clearml.mydomain.com<\/code> and <code>https:\/\/files.clearml.mydomain.com<\/code>) in a browser and see what I think is the correct response, but when connecting with the python SDK via <code>clearml-init<\/code> I get the following error:<\/p>\n<pre><code>clearml.backend_api.session.session.LoginError: Failed getting token (error 400 from https:\/\/api.clearml.mydomain.com): Bad Request\n<\/code><\/pre>\n<p>Are there any likely causes of this error?<\/p>",
        "Challenge_closed_time":1640162037740,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639763350487,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has set up a ClearML server in GCP using the sub-domain approach and can access all three domains in a browser. However, when connecting with the python SDK via clearml-init, the user is encountering a LoginError with an error 400 message. The user is seeking possible causes for this error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70397010",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":8.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":110.7464591667,
        "Challenge_title":"What would stop credentials from validation on a ClearML server?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":194.0,
        "Challenge_word_count":76,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600124498003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":46.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Following the discussion <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/517\" rel=\"nofollow noreferrer\">here<\/a>, it seemed that the load balancer being used was blocking <code>GET<\/code> requests with a payload which are used by ClearML. A <a href=\"https:\/\/github.com\/allegroai\/clearml\/pull\/521\" rel=\"nofollow noreferrer\">fix<\/a> is being worked on to allow the method to be changed to a <code>POST<\/code> request via an environment variable.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":5.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":51.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":6580.6306711111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Ok I've been dealing with this issue in Sagemaker for almost a week and I'm ready to pull my hair out. I've got a custom training script paired with a data processing script in a BYO algorithm Docker deployment type scenario. It's a Pytorch model built with Python 3.x, and the BYO Docker file was originally built for Python 2, but I can't see an issue with the problem that I am having.....which is that after a successful training run Sagemaker doesn't save the model to the target S3 bucket.<\/p>\n<p>I've searched far and wide and can't seem to find an applicable answer anywhere. This is all done inside a Notebook instance. Note: I am using this as a contractor and don't have full permissions to the rest of AWS, including downloading the Docker image.<\/p>\n<p>Dockerfile:<\/p>\n<pre><code>FROM ubuntu:18.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python-pip \\\n         python3-pip3\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python3 get-pip.py &amp;&amp; \\\n    pip3 install future numpy torch scipy scikit-learn pandas flask gevent gunicorn &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=&quot;\/opt\/program:${PATH}&quot;\n\nCOPY decision_trees \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n<p>Docker Image Build:<\/p>\n<pre><code>%%sh\n\nalgorithm_name=&quot;name-this-algo&quot;\n\ncd container\n\nchmod +x decision_trees\/train\nchmod +x decision_trees\/serve\n\naccount=$(aws sts get-caller-identity --query Account --output text)\n\nregion=$(aws configure get region)\nregion=${region:-us-east-2}\n\nfullname=&quot;${account}.dkr.ecr.${region}.amazonaws.com\/${algorithm_name}:latest&quot;\n\naws ecr describe-repositories --repository-names &quot;${algorithm_name}&quot; &gt; \/dev\/null 2&gt;&amp;1\n\nif [ $? -ne 0 ]\nthen\n    aws ecr create-repository --repository-name &quot;${algorithm_name}&quot; &gt; \/dev\/null\nfi\n\n# Get the login command from ECR and execute it directly\n$(aws ecr get-login --region ${region} --no-include-email)\n\n# Build the docker image locally with the image name and then push it to ECR\n# with the full name.\n\ndocker build  -t ${algorithm_name} .\ndocker tag ${algorithm_name} ${fullname}\n\ndocker push ${fullname}\n<\/code><\/pre>\n<p>Env setup and session start:<\/p>\n<pre><code>common_prefix = &quot;pytorch-lstm&quot;\ntraining_input_prefix = common_prefix + &quot;\/training-input-data&quot;\nbatch_inference_input_prefix = common_prefix + &quot;\/batch-inference-input-data&quot;\n\nimport os\nfrom sagemaker import get_execution_role\nimport sagemaker as sage\n\nsess = sage.Session()\n\nrole = get_execution_role()\nprint(role)\n<\/code><\/pre>\n<p>Training Directory, Image, and Estimator Setup, then a <code>fit<\/code> call:<\/p>\n<pre><code>TRAINING_WORKDIR = &quot;a\/local\/directory&quot;\n\ntraining_input = sess.upload_data(TRAINING_WORKDIR, key_prefix=training_input_prefix)\nprint (&quot;Training Data Location &quot; + training_input)\n\naccount = sess.boto_session.client('sts').get_caller_identity()['Account']\nregion = sess.boto_session.region_name\nimage = '{}.dkr.ecr.{}.amazonaws.com\/image-that-works:working'.format(account, region)\n\ntree = sage.estimator.Estimator(image,\n                       role, 1, 'ml.p2.xlarge',\n                       output_path=&quot;s3:\/\/sagemaker-directory-that-definitely\/exists&quot;,\n                       sagemaker_session=sess)\n\ntree.fit(training_input)\n<\/code><\/pre>\n<p>The above script is working, for sure. I have print statements in my script and they are printing the expected results to the console. This runs as it's supposed to, finishes up, and says that it's deploying model artifacts when IT DEFINITELY DOES NOT.<\/p>\n<p>Model Deployment:<\/p>\n<pre><code>model = tree.create_model()\npredictor = tree.deploy(1, 'ml.m4.xlarge')\n<\/code><\/pre>\n<p>This throws an error that the model can't be found. A call to <code>aws sagemaker describe-training-job<\/code> shows that the training was completed but I found that the time it took to upload the model was super fast, so obviously there's an error somewhere and it's not telling me. Thankfully it's not just uploading it to the aether.<\/p>\n<pre><code>{\n            &quot;Status&quot;: &quot;Uploading&quot;,\n            &quot;StartTime&quot;: 1595982984.068,\n            &quot;EndTime&quot;: 1595982989.994,\n            &quot;StatusMessage&quot;: &quot;Uploading generated training model&quot;\n        },\n<\/code><\/pre>\n<p>Here's what I've tried so far:<\/p>\n<ol>\n<li>I've tried uploading it to a different bucket. I figured my permissions were the problem so I pointed it to one that I new allowed me to upload as I had done it before to that bucket. No dice.<\/li>\n<li>I tried backporting the script to Python 2.x, but that caused more problems than it probably would have solved, and I don't really see how that would be the problem anyways.<\/li>\n<li>I made sure the Notebook's IAM role has sufficient permissions, and it does have a SagemakerFullAccess policy<\/li>\n<\/ol>\n<p>What bothers me is that there's no error log I can see. If I could be directed to that I would be happy too, but if there's some hidden Sagemaker kungfu that I don't know about I would be forever grateful.<\/p>\n<hr \/>\n<p>EDIT<\/p>\n<p>The training job runs and prints to both the Jupyter cell and CloudWatch as expected. I've since lost the cell output in the notebook but below is the last few lines in CloudWatch. The first number is the epoch and the rest are various custom model metrics.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/y3I9L.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/y3I9L.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1596039270563,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595988324557,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Sagemaker where the model is not being saved to the target S3 bucket after a successful training run. The user has tried uploading it to a different bucket, backporting the script to Python 2.x, and ensuring that the Notebook's IAM role has sufficient permissions. The user is also unable to find any error logs related to the issue.",
        "Challenge_last_edit_time":1596039837756,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63145277",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":11.3,
        "Challenge_reading_time":72.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":14.1516683334,
        "Challenge_title":"Sagemaker Training Job Not Uploading\/Saving Training Model to S3 Output Path",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":2438.0,
        "Challenge_word_count":680,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360536048187,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Miami, FL, USA",
        "Poster_reputation_count":45.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>Can you verify from the training job logs that your training script is running? It doesn't look like your Docker image would respond to the command <code>train<\/code>, which is what SageMaker requires, and so I suspect that your model isn't actually getting trained\/saved to <code>\/opt\/ml\/model<\/code>.<\/p>\n<p>AWS documentation about how SageMaker runs the Docker container: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html<\/a><\/p>\n<p>edit: summarizing from the comments below - the training script must also save the model to <code>\/opt\/ml\/model<\/code> (the model isn't saved automatically).<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1619730108172,
        "Solution_link_count":2.0,
        "Solution_readability":16.5,
        "Solution_reading_time":10.21,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1077063889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How to use safe and protected communication?<\/p>",
        "Challenge_closed_time":1656964172656,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656963784913,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in establishing a secure and protected communication.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/913567\/problems-secure-connection",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":1.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.1077063889,
        "Challenge_title":"problems secure connection",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":9,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Your question is too vague for anyone to answer. Can you please elaborate?    <\/p>\n<p>If you are talking about securing Azure Machine Learning you will need to use Virtual Networks (VNet) to protect.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-network-security-overview\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-network-security-overview<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":5.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1432829415467,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":501.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":674.1436052778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an experiment in AzureML which has a R module at its core. Additionally, I have some .RData files stored in Azure blob storage. The blob container is set as private (no anonymous access).<\/p>\n\n<p>Now, I am trying to make a https call from inside the R script to the azure blob storage container in order to download some files. I am using the <code>httr<\/code> package's <code>GET()<\/code> function and properly set up the url, authentication etc...The code works in R on my local machine but the same code gives me the following error when called from inside the R module in the experiment<\/p>\n\n<pre><code>error:1411809D:SSL routines:SSL_CHECK_SERVERHELLO_TLSEXT:tls invalid ecpointformat list\n<\/code><\/pre>\n\n<p>Apparently this is an error from the underlying OpenSSL library (which got fixed a while ago). Some suggested workarounds I found <a href=\"https:\/\/stackoverflow.com\/questions\/20046176\/rcurl-errors-when-fetching-ssl-endpoint\">here<\/a> were to set <code>sslversion = 3<\/code> and <code>ssl_verifypeer = 1<\/code>, or turn off verification <code>ssl_verifypeer = 0<\/code>. Both of these approaches returned the same error.<\/p>\n\n<p>I am guessing that this has something to do with the internal Azure certificate \/ validation...? Or maybe I am missing or overseeing something?<\/p>\n\n<p>Any help or ideas would be greatly appreciated. Thanks in advance.<\/p>\n\n<p>Regards<\/p>",
        "Challenge_closed_time":1450358402136,
        "Challenge_comment_count":1,
        "Challenge_created_time":1447931485157,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error when trying to make an https call from inside an R module in AzureML to download files from a private Azure blob storage container. The error is related to SSL routines and is caused by an issue with the underlying OpenSSL library. The user has tried suggested workarounds but none have worked. The user suspects that the issue may be related to the internal Azure certificate\/validation.",
        "Challenge_last_edit_time":1495540337367,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/33802274",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":18.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":674.1436052778,
        "Challenge_title":"Error:1411809D:SSL routines - When trying to make https call from inside R module in AzureML",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":358.0,
        "Challenge_word_count":203,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432829415467,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":501.0,
        "Poster_view_count":76.0,
        "Solution_body":"<p>After a while, an answer came back from the support team, so I am going to post the relevant part as an answer here for anyone who lands here with the same problem. <\/p>\n\n<p>\"This is a known issue. The container (a sandbox technology known as \"drawbridge\" running on top of Azure PaaS VM) executing the Execute R module doesn't support outbound HTTPS traffic. Please try to switch to HTTP and that should work.\"<\/p>\n\n<p>As well as that a solution is on the way :<\/p>\n\n<p>\"We are actively looking at how to fix this bug. \"<\/p>\n\n<p>Here is the original <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/5866e16c-a145-481e-8764-f7c7823742b0\/https-call-from-inside-r-module-possible-?forum=MachineLearning\" rel=\"nofollow\">link<\/a> as a reference.\nhth<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":9.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":107.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1313736279736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":207794.0,
        "Answerer_view_count":16864.0,
        "Challenge_adjusted_solved_time":4.9399880556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Trying to figure out whether this behaviour on IPython (v7.12.0, on Amazon SageMaker) is a bug or I'm missing some proper way \/ documented constraint...<\/p>\n<p>Say I have some Python variables like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>NODE_VER = &quot;v16.14.2&quot;\nNODE_DISTRO = &quot;linux-x64&quot;\n<\/code><\/pre>\n<p>These commands both work fine in a notebook:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>!echo $PATH\n# Shows **contents of system path**\n!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:\n# Shows \/usr\/local\/lib\/nodejs\/node-v16.14.2-linux-x64\/bin\n<\/code><\/pre>\n<p>...But this does not:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$PATH\n# Shows:\n# \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:**contents of system path**\n<\/code><\/pre>\n<p>I've tried a couple of combinations of e.g. using <code>$NODE_VER<\/code> syntax instead (which produces <code>node--\/<\/code> instead of <code>node-{NODE_VER}-{NODE_DISTRO}\/<\/code>, but seems like any combination using both shell variables (PATH) and Python variables (NODE_VER\/NODE_DISTRO) fails.<\/p>\n<p>Can anybody help me understand why and how to work around it?<\/p>\n<p>My end goal, as you might have guessed already, is to actually add this folder to the PATH rather than just echoing it - something like:<\/p>\n<pre><code>!export PATH=\/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$PATH\n<\/code><\/pre>",
        "Challenge_closed_time":1648184137556,
        "Challenge_comment_count":1,
        "Challenge_created_time":1648174324543,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to mix shell variables and Python variables in IPython's '!command'. While the commands using only shell variables or only Python variables work fine, the combination of both fails. The user is seeking help to understand why this is happening and how to work around it to add a folder to the PATH.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71611419",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.2,
        "Challenge_reading_time":20.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":2.7258369444,
        "Challenge_title":"Mixing shell variables and python variables in IPython '!command'",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":160.0,
        "Challenge_word_count":155,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1587281590603,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":473.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p><a href=\"https:\/\/stackoverflow.com\/questions\/69194172\/how-to-reference-both-a-python-and-environment-variable-in-jupyter-bash-magic\">How to reference both a python and environment variable in jupyter bash magic?<\/a><\/p>\n<p>Try<\/p>\n<pre><code>!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$$PATH\n<\/code><\/pre>\n<p><code>$$PATH<\/code> forces it to use the system variable rather than try to find a Python\/local one.<\/p>\n<p>Various examples:<\/p>\n<pre><code>In [130]: foo = 'foo*.txt'\nIn [131]: HOME = 'myvar'\nIn [132]: !echo $foo\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt\nIn [133]: !echo $foo $HOME\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt myvar\nIn [134]: !echo $foo $$HOME\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\nIn [135]: !echo $foo $PWD\n\/home\/paul\/mypy\nIn [136]: !echo $foo $$PWD\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\/mypy\nIn [137]: !echo {foo} $PWD\n{foo} \/home\/paul\/mypy\nIn [138]: !echo {foo} $$PWD\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\/mypy\n<\/code><\/pre>\n<p>Any variable not locally defined forces the behavior you see:<\/p>\n<pre><code>In [139]: !echo $abc\n\nIn [140]: !echo {foo} $abc\n{foo}\n<\/code><\/pre>\n<p>It may put the substitution in a <code>try\/except<\/code> block, and &quot;give up&quot; if there's any <code>NameError<\/code>.<\/p>\n<p>This substitution can occur in most of the magics, not just <code>!<\/code>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1648192108500,
        "Solution_link_count":1.0,
        "Solution_readability":7.6,
        "Solution_reading_time":18.76,
        "Solution_score_count":3.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":160.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.7385794444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently pursuing 'Microsoft Azure AI Fundamentals' course and I am currently stuck on unit 6 of 8 (titled-Explore automated machine learning in azure ML) as I am unable to find the administrative access into azure ML to create the resource and I am getting redirected to payment page for subscription. Please help me to get into Azure ML using administrative access. <\/p>",
        "Challenge_closed_time":1685005009496,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684995150610,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is currently stuck on unit 6 of the Microsoft Azure AI Fundamentals course as they are unable to find administrative access to Azure ML to create a resource. They are being redirected to a payment page for subscription and need help accessing Azure ML with administrative access.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1291402\/stuck-on-azure-ai-fundamentals-course",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":5.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2.7385794444,
        "Challenge_title":"Stuck on Azure AI Fundamentals course",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":69,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=eb5a735f-18f5-4565-8bc6-44e2094e0255\">@Soumyadeep Podder  <\/a>Thanks for the question. If you\u2019re being redirected to the payment page for subscription, it could mean that you don\u2019t have an active Azure subscription. You can try signing up for a free trial of Azure.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":3.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1512520584492,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bloomington, IN, USA",
        "Answerer_reputation_count":868.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":7347.4097222222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to run a machine learning experiment in azureml.<\/p>\n<p>I can't figure out how to get the workspace context from the control script.  Examples like <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-bring-data#control-script\" rel=\"nofollow noreferrer\">this one<\/a> in the microsoft docs use Workspace.from_config().  When I use this in the control script I get the following error:<\/p>\n<blockquote>\n<p>&quot;message&quot;: &quot;We could not find config.json in: [path] or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.&quot;<\/p>\n<\/blockquote>\n<p>I've also tried including my subscription id and the resource specs like so:<\/p>\n<pre><code>subscription_id = 'id'\nresource_group = 'name'\nworkspace_name = 'name'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n<\/code><\/pre>\n<p>In this case I have to monitor the log and authenticate on each run as I would locally.<\/p>\n<p>How do you get the local workspace from a control script for azureml?<\/p>",
        "Challenge_closed_time":1641958092267,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615507417267,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble getting the workspace context from the control script while running a machine learning experiment in AzureML. They have tried using Workspace.from_config() and including subscription id and resource specs, but both methods have not worked. The user is seeking help to get the local workspace from a control script for AzureML.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66592313",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":9.5,
        "Challenge_reading_time":14.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":7347.4097222222,
        "Challenge_title":"Get local workspace in azureml",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":333.0,
        "Challenge_word_count":143,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1512520584492,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bloomington, IN, USA",
        "Poster_reputation_count":868.0,
        "Poster_view_count":51.0,
        "Solution_body":"<p>This had no answers for 10 months, and now they are coming in :).  I figuerd this out quite a while ago but haven't gotten around to posting the answer.  Here it is.<\/p>\n<p>From the training script, you can get the workspace from the run context as follows:<\/p>\n<pre><code>from azureml.core import Run\nRun.get_context()\nws = run.experiment.workspace\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.5,
        "Solution_reading_time":4.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1534058291092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":33.0,
        "Challenge_adjusted_solved_time":55.4162013889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create sagemaker studio project using aws cdk following below steps:<\/p>\n<p>create domain (<a href=\"https:\/\/github.com\/aws-samples\/aws-cdk-sagemaker-studio.git\" rel=\"nofollow noreferrer\">using this example<\/a>)\ncreate user (<a href=\"https:\/\/github.com\/aws-samples\/aws-cdk-sagemaker-studio.git\" rel=\"nofollow noreferrer\">using this example<\/a>)\ncreate jupyter app\ncreate project<\/p>\n<p>Code for creating jupyter app:<\/p>\n<pre><code>\ndef __init__(self, scope: Construct,\n             construct_id: str, *,\n             app_name: str,\n             app_type: str,\n             domain_id: str,\n             user_profile_name: str,\n             depends_on=None, **kwargs) -&gt; None:\n    super().__init__(scope, construct_id)\n\n    sagemaker_jupyter_app = sg.CfnApp(self, construct_id,\n                                      app_name=app_name,\n                                      app_type=app_type,\n                                      domain_id=domain_id,\n                                      user_profile_name=user_profile_name\n                                      )\n    sagemaker_jupyter_app.add_depends_on(depends_on_user_creation)\n<\/code><\/pre>\n<p>Code for creating project:<\/p>\n<pre><code>\ndef __init__(self, scope: Construct,\n             construct_id: str, *,\n             project_name: str,\n             project_description: str,\n             product_id: str,\n             depends_on=None,\n             **kwargs) -&gt; None:\n    super().__init__(scope, construct_id)\n\n    sagemaker_studio_project = sg.CfnProject(self, construct_id,\n                                             project_name=project_name,\n                                             service_catalog_provisioning_details={\n                                                 &quot;ProductId&quot;: &quot;prod-7tjedn5dz4jrw&quot;\n                                             },\n                                             project_description=project_description\n                                             )\n<\/code><\/pre>\n<p>Domain, user, jupyter app all gets created successfully. The problem comes in with project.\nBelow is the error :<\/p>\n<blockquote>\n<p>Resource handler returned message: &quot;Product prod-7tjedn5dz4jrw does\nnot exist or access was denied (Service: SageMaker, Status Code: 400,\nRequest ID: 768116aa-e77b-4691-a972-38b83093fdc4)&quot; (RequestToken:\n45ca2a0c-3f03-e3e0-f29d-d9443ff4dfc1, HandlerErrorCode:\nGeneralServiceException)<\/p>\n<\/blockquote>\n<p>I am running this code from an ec2 instance that has SagemakerFullAccess\nI also tried attaching SagemakerFullAccess execution role with project...but got the same error.\nI have also attached below policy to my domain:<\/p>\n<ul>\n<li>AmazonSageMakerAdmin-ServiceCatalogProductsServiceRolePolicy<\/li>\n<\/ul>",
        "Challenge_closed_time":1650740870208,
        "Challenge_comment_count":1,
        "Challenge_created_time":1650541371883,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a Sagemaker studio project using AWS CDK, but is encountering an error message stating that the product does not exist or access was denied. The user has created the domain, user, and Jupyter app successfully, but is unable to create the project. The user has tried running the code from an EC2 instance with SagemakerFullAccess and attaching the SagemakerFullAccess execution role to the project, but the error persists. The user has also attached the AmazonSageMakerAdmin-ServiceCatalogProductsServiceRolePolicy to the domain.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71953876",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":18.5,
        "Challenge_reading_time":29.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":55.4162013889,
        "Challenge_title":"How to create Sagemaker studio project using aws cdk",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":491.0,
        "Challenge_word_count":193,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1534058291092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":116.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>Basically this was an issue related to IAM.\nRunning cdk program requires bootstrapping it using the command <code>cdk bootstrap<\/code>\nAfter running this command cdk was creating a bunch of roles out of which one role will be related to cloudformation's execution role. Something like<\/p>\n<blockquote>\n<p>cdk-serialnumber-cfn-exec-role-Id-region<\/p>\n<\/blockquote>\n<p>Now this role was used by cloudformation to run the stack.<\/p>\n<p>Using sagemaker from console automatically adds the role associated with domain\/user at<\/p>\n<blockquote>\n<p>ServiceCatalog -&gt; Portfolios -&gt; Imported -&gt; Amazon SageMaker Solutions and ML Ops products -&gt; Groups, roles, and users<\/p>\n<\/blockquote>\n<p>Thats was the reason why product id was accessible from console.<\/p>\n<p>After adding the role created by cdk bootsrap to the above path I was able to run my stack.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":10.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":120.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1355002392776,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bolzano, Italia",
        "Answerer_reputation_count":530.0,
        "Answerer_view_count":91.0,
        "Challenge_adjusted_solved_time":1320.8344933333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After running <code>dvc push data.csv<\/code> (to ssh-remote), when i try to dvc-pull the same file on another machine from the same remote, it won't get pulled. Below are the logs and the error:<\/p>\n<pre><code>2021-01-21 22:17:26,643 DEBUG: checking if 'data.csv'('HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)') has changed.\n2021-01-21 22:17:26,643 DEBUG: 'data.csv' doesn't exist.\n2021-01-21 22:17:26,644 WARNING: Cache 'HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)' not found. File 'data.csv' won't be created.\n2021-01-21 22:17:26,644 DEBUG: cache '\/usr\/src\/bohr\/.dvc\/cache\/27\/9936268f488e1e613f81a537f29055' expected 'HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)' actual 'None'\n...\n2021-01-21 22:17:26,660 ERROR: failed to pull data from the cloud - Checkout failed for following targets:\ndata.csv\n<\/code><\/pre>\n<p>However, the file is present on the remote:<\/p>\n<pre><code>$ ls -la ~\/.dvcstorage\/bohr\/27\/9936268f488e1e613f81a537f29055\n-rw-rw-r-- 1 hbabii hbabii 1458311 Jan 22 00:19 \/home\/hbabii\/.dvcstorage\/bohr\/27\/9936268f488e1e613f81a537f29055\n<\/code><\/pre>\n<p>I double-checked that I am pulling from and pushing to the same remote. I am using DVC v1.11.11.<\/p>\n<p>Could you please give me any hints on what could be wrong?<\/p>\n<p>Cheers, Hlib<\/p>",
        "Challenge_closed_time":1616082756536,
        "Challenge_comment_count":4,
        "Challenge_created_time":1611327752360,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue where they are unable to pull an existing file from an SSH DVC remote, even though the file is present on the remote. The logs show that the cache is not found and the pull fails with an error message. The user has confirmed that they are pushing and pulling from the same remote and is using DVC v1.11.11.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65847574",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":5.5,
        "Challenge_reading_time":19.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1320.8344933333,
        "Challenge_title":"Failed to pull existing files from SSH DVC Remote",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1715.0,
        "Challenge_word_count":159,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1355002392776,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bolzano, Italia",
        "Poster_reputation_count":530.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p>In the end, the problem was that I indeed was pulling from the wrong remote (I had multiple remotes, their configuration was tricky, and local configurations differed on different machines).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.2,
        "Solution_reading_time":2.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":30.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1442422586352,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Atlanta, United States",
        "Answerer_reputation_count":20328.0,
        "Answerer_view_count":2380.0,
        "Challenge_adjusted_solved_time":2.0320580556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to use terraform to create a model on SageMaker by following <a href=\"https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/sagemaker_model\" rel=\"nofollow noreferrer\">this page<\/a>\nI can't assign a full access policy to the sagemaker role due to permission constrains, so I created a role and attached a policy with part of the permissions<\/p>\n<p>When I tested <code>Terraform plan<\/code>, it gave me this:<\/p>\n<pre><code>Error: Invalid template interpolation value\n...\n<\/code><\/pre>\n<pre><code>..........................\n 141:                 &quot;ecr:GetRepositoryPolicy&quot;\n 142:             ],\n 143:             &quot;Resource&quot;: [\n 144:                 &quot;arn:aws:s3:::${aws_s3_bucket.xx_xxxxxxxxxx_xxx_bucket}&quot;,\n 145:                 &quot;arn:aws:s3:::${local.binaries_bucket_name}&quot;,\n 146:                 &quot;arn:aws:s3:::${aws_s3_bucket.xx_xxxxxxxxxx_xxx_bucket}\/*&quot;,\n 147:                 &quot;arn:aws:s3:::${local.binaries_bucket_name}\/*&quot;,\n 148:                 &quot;arn:aws:ecr:us-east-1:*:repository\/*&quot;,\n 149.....................\n 157:         }\n 158:     ]\n 159: }\n 160: POLICY\n    |----------------\n    | aws_s3_bucket.xx_xxxxxxxxxx_xxx_bucket is object with 25 attributes\n\nCannot include the given value in a string template: string required.\n<\/code><\/pre>\n<p>I'm new to this, just wondering if this is complaining the bucket name is too long or something else? What should I do to fix this, I'm a bit confused. Many thanks.<\/p>\n<p>(PS: Terraform version <code>v0.13.4<\/code> + provider registry.terraform.io\/hashicorp\/aws <code>v3.20.0<\/code>)<\/p>",
        "Challenge_closed_time":1607629351412,
        "Challenge_comment_count":3,
        "Challenge_created_time":1607622036003,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while using Terraform to create a model on SageMaker due to permission constraints. They created a role and attached a policy with part of the permissions, but when they tested Terraform plan, they received an error message indicating that they cannot include the given value in a string template, and a string is required. The user is unsure if the error is due to the bucket name being too long or something else and is seeking guidance on how to fix the issue.",
        "Challenge_last_edit_time":1607700252620,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65239565",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":12.3,
        "Challenge_reading_time":20.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":2.0320580556,
        "Challenge_title":"Terraform - Cannot include the given value in a string template: string required",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":8882.0,
        "Challenge_word_count":152,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>It appears what you want here is the ARN of the S3 bucket, which is provided by <a href=\"https:\/\/www.terraform.io\/docs\/configuration\/blocks\/resources\/behavior.html#accessing-resource-attributes\" rel=\"noreferrer\">exported resource attributes<\/a>. Specifically, you probably want the <code>arn<\/code> resource attribute.<\/p>\n<p>Updating your policy like:<\/p>\n<pre><code> 144:             &quot;${aws_s3_bucket.xx_xxxxxxxxxx_xxx_bucket.arn}&quot;,\n 146:             &quot;${aws_s3_bucket.xx_xxxxxxxxxx_xxx_bucket.arn}\/*&quot;,\n<\/code><\/pre>\n<p>will provide you with the String that you need by accessing the <code>arn<\/code> attribute. The currently written policy is accessing <code>aws_s3_bucket.xx_xxxxxxxxxx_xxx_bucket<\/code>, which is a Map (possibly Object) of every argument and attribute for that resource, and will not interpolate within the string of your policy.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.1,
        "Solution_reading_time":11.28,
        "Solution_score_count":5.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":85.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":110.2481841667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I\u2019m having an issue importing the data into my Machine Learning Studio. It shows me a Red Cross with the error 0030 - which means that there\u2019s an issue in downloading the data. For background, I\u2019m importing data from the Web URL via HTTP option. I looked up the issue on the troubleshooting page, followed the advice, which shows I\u2019ve done everything correctly. My data link works perfectly fine in my browser. When I enter the http link into my browser, it immediately downloads the csv file. However, my studio is not downloading the data. Importing the data is the first step in my experiment, and I can\u2019t move forward without it. Immediate help would be greatly appreciated! I\u2019ve attached pictures for reference. [1]: \/api\/attachments\/72499-0ebb78a4-4805-46e8-a7f1-fbf99682af5f.png?platform=QnA <\/p>",
        "Challenge_closed_time":1614759574360,
        "Challenge_comment_count":2,
        "Challenge_created_time":1614362680897,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while importing data into Azure ML Studio experiment. The error 0030 is displayed, indicating a problem in downloading the data. The user is importing data from a Web URL via HTTP option, and the data link works fine in the browser but not in the studio. This is the first step in the experiment, and the user needs immediate help to move forward.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/291213\/importing-data-in-azure-ml-studio-experiment",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":10.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":110.2481841667,
        "Challenge_title":"Importing Data in Azure ML Studio Experiment",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":131,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,  <\/p>\n<p>This exception in Azure Machine Learning occurs when it is not possible to download a file. You will receive this exception when an attempted read from an HTTP source has failed after three (3) retry attempts.  <\/p>\n<p>Resolution: Verify that the URI to the HTTP source is correct and that the site is currently accessible via the Internet.  <\/p>\n<p>Is this file on any place need authentication?   <\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":5.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":0.3501991667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to get AWS SageMaker to call AWS Comprehend. I'm getting this message in SageMaker:<\/p>\n\n<blockquote>\n  <p>ClientError: An error occurred (AccessDeniedException) when calling\n  the StartTopicsDetectionJob operation: User:\n  arn:aws:sts::545176143103:assumed-role\/access-aws-services-from-sagemaker\/SageMaker\n  is not authorized to perform: iam:PassRole on resource:\n  arn:aws:iam::545176143103:role\/access-aws-services-from-sagemaker<\/p>\n<\/blockquote>\n\n<p>When creating the Jupyter notebook, I used this role:<\/p>\n\n<blockquote>\n  <p>arn:aws:sagemaker:us-east-2:545176143103:notebook-instance\/access-comprehend-from-sagemaker<\/p>\n<\/blockquote>\n\n<p>...with the following policies attached:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/t6feB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t6feB.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I'm using the same IAM role in SageMaker:<\/p>\n\n<pre><code> data_access_role_arn = \"arn:aws:iam::545176143103:role\/access-aws-services-from-sagemaker\"\n<\/code><\/pre>\n\n<p>It looks like I'm giving the role all the access it needs. How can I correct this error?<\/p>",
        "Challenge_closed_time":1556151395660,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556150134943,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an AccessDeniedException error when trying to get AWS SageMaker to call AWS Comprehend. The error message indicates that the user's IAM role does not have the necessary permissions to perform the iam:PassRole action on the specified resource. The user has attached the required policies to the IAM role but is still facing the error.",
        "Challenge_last_edit_time":1586235828680,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55840023",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":15.8,
        "Challenge_reading_time":15.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3501991667,
        "Challenge_title":"IAM Roles for Sagemaker?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":10244.0,
        "Challenge_word_count":98,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1276294622427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4334.0,
        "Poster_view_count":496.0,
        "Solution_body":"<p>Based on your error, it looks like there's a permissions issue with the SageMaker notebook trying to change IAM settings from within a notebook that does not explicitly have permission to do so.<\/p>\n\n<hr>\n\n<p>You have a few options here to remedy this:<\/p>\n\n<p><strong>Option 1: Granting the SageMaker notebook permissions to define IAM role within the notebook during runtime.<\/strong><\/p>\n\n<p>From the console, click on <code>Hosted Notebooks<\/code> along the left navbar, then under <code>Permissions<\/code>, click the attached IAM role. Here, you can add policies such as <code>IAMFullAccess<\/code> or <code>IAMReadOnlyAccess<\/code>. This should solve for the permissions error when you try to attach an IAM role from within the notebook.<\/p>\n\n<p><strong>Option 2: Explicitly define the permissions you want SageMaker to have in the console.<\/strong><\/p>\n\n<p>From the console, click on <code>Hosted Notebooks<\/code> along the left navbar, then under <code>Permissions<\/code>, click the attached IAM role. Here, you can directly add policies for resource permissions (such as Comprehend). Without attaching explicit IAM access policies to this role, you wouldn't be able to change permissions during runtime.<\/p>\n\n<p><strong>Option 3: Both<\/strong><\/p>\n\n<p>If you'd like to pre-define access for some resources, but also potentially add other resource permissions during experimentation, you can do both steps 1 and 2 (Add IAM + other resource permissions to the hosted notebook in console, with the ability to change your SageMaker IAM role inline during experimentation).<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1556152415310,
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":19.88,
        "Solution_score_count":5.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":222.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.3446652778,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi Friends,\n\nI have deleted everything in Sagemaker - but support is asking me to delete the experiments that are still in my account : they sent me a link to follow\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/experiments-cleanup.html\n\nbut I have no idea how to complete this task -- does anyone know in terms that someone who has no idea what this means - can follow and achieve this task\n\nyou have no idea how much it would mean to me for any assistance",
        "Challenge_closed_time":1668070837516,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668037196721,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has deleted everything in SageMaker but support is asking them to delete the experiments that are still in their account. The user is seeking help to complete this task as they have no idea how to do it. They have shared a link provided by support but are looking for assistance in simpler terms.",
        "Challenge_last_edit_time":1668612841943,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUFMzl26gfQna8sAZcCDJw_Q\/sagemaker-experiments-deletion-help-needed",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.4,
        "Challenge_reading_time":6.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":9.3446652778,
        "Challenge_title":"SageMaker Experiments Deletion Help Needed",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":79,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You need to use a computer with Python, the SageMaker SDK installed, and AWS credentials with enough permissions for that account configured. If you are already using SageMaker Studio, that should work.\n\nUse the [second method](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/experiments-cleanup.html#experiments-cleanup-boto3). Create a file (Menu File -> New -> Python File). Rename it as  `cleanup_experiments.py`(right click on the file on top and select Rename Python File), then paste the code in the documentation (those three sections, one after another). Save the file and open a terminal (Menu File -> New -> Terminal). Navigate to the directory where you saved the file and execute the command `python cleanup_experiments.py`",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1668129579400,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":9.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":98.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":3.4555330556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to connect a VM I have in AzureML studio.  I keep getting the following:  Connection attempt timed out for ''. Verify that server is accessible and SSH service is accepting connections.<\/p>",
        "Challenge_closed_time":1644985009903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644973069297,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a connection timeout issue while trying to connect a VM in AzureML studio and is receiving an error message stating that the connection attempt has timed out. The user is advised to verify the accessibility of the server and ensure that the SSH service is accepting connections.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71135228",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":2.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":3.316835,
        "Challenge_title":"Connection timeout in AzureML studio",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":208.0,
        "Challenge_word_count":36,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1555914279856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Go to your VM config and test your connection through the 'connect' tab.  Is your test successful?  If not, check if port 22 is blocked.  Watch for automated blocking rules applied to your VM.<\/p>\n<p>we have DSVM attach in preview - might be interesting for you: <a href=\"https:\/\/github.com\/Azure\/azureml-previews\/tree\/main\/previews\/dsvm-attach\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azureml-previews\/tree\/main\/previews\/dsvm-attach<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1644985509216,
        "Solution_link_count":2.0,
        "Solution_readability":10.0,
        "Solution_reading_time":5.93,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":49.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":790.3911111111,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\nshould highlight `instance type` field\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/843303\/182809305-2d25c565-18f8-4da0-ad9e-847c28cc62b0.png)\r\n\r\nthe field `AutoStopIdleTimeInMinutes` also is required.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1662449543000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1659604135000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a bug in the Hong Kong region where after manually stopping a Sagemaker workspace, the web console shows an \"UNKNOWN\" status.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/70",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":10.42,
        "Challenge_repo_contributor_count":42.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":148.0,
        "Challenge_repo_star_count":11.0,
        "Challenge_repo_watch_count":14.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":790.3911111111,
        "Challenge_title":"[Bug] highlight incorrect field in screenshot of importing Sagemaker workspace",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":98,
        "Discussion_body":"Already fixed",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":142.7236111111,
        "Challenge_answer_count":0,
        "Challenge_body":"### System Info\n\n```Shell\npytorch: 1.10.2\r\npython:3.8\n```\n\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] One of the scripts in the examples\/ folder of Accelerate or an officially supported `no_trainer` script in the `examples` folder of the `transformers` repo (such as `run_no_trainer_glue.py`)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nSagemaker Multi-GPU distributed data training, while \"model.generate\" it always returns empty tensors.\n\n### Expected behavior\n\n```Shell\nI'm trying to run a distributed training in a Sagemaker training job, the inference is not working properly, I found it as a future work on huggingface documentation so I'm wondering If that's why it's not working yet on sagemaker Multi-GPU.\r\n\r\nThanks\n```\n",
        "Challenge_closed_time":1664255368000,
        "Challenge_comment_count":7,
        "Challenge_created_time":1663741563000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an issue with the example DAG for Sagemaker, which currently only uses access key and secret key instead of a temporary access token. The user expects the DAG to use a temporary access token. No specific details about the error or expected behavior are provided.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/accelerate\/issues\/706",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":13.6,
        "Challenge_reading_time":11.09,
        "Challenge_repo_contributor_count":137.0,
        "Challenge_repo_fork_count":491.0,
        "Challenge_repo_issue_count":1528.0,
        "Challenge_repo_star_count":4764.0,
        "Challenge_repo_watch_count":83.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":142.7236111111,
        "Challenge_title":"Have accelerate for  Distributed Training: Data Parallelism feature working on AWS Sagemaker yet?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":122,
        "Discussion_body":"Hello @HebaGamalElDin, please provide minimal reproducible example for us to deep dive and help you.  Hello @pacman100, I'm fine tuning a transformer model from the hub of huggingface.. below is the training function that utilizes the accelerator on sagemaker training jobs.\r\n\r\n```\r\ndef train(context: Context, num_epochs):\r\n    model = context.model\r\n    model = accelerator.prepare(model)\r\n    optimizer = AdamW(model.parameters(), lr=1e-3)\r\n    \r\n    num_training_steps = num_epochs * len(context.train_dataloader)\r\n    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)\r\n    optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(optimizer, context.train_dataloader, context.val_dataloader, lr_scheduler)\r\n    \r\n    \r\n    losses = []\r\n    min_cer = 1.0\r\n    min_train_loss = 1.0\r\n    for epoch in range(num_epochs):\r\n        model.train()\r\n        for j, batch in enumerate(train_dataloader):\r\n            inputs: torch.Tensor = batch[\"input\"]#.to(accelerator.device)\r\n            labels: torch.Tensor = batch[\"label_tensor\"]#.to(accelerator.device)\r\n\r\n            outputs = model(pixel_values=inputs, labels=labels)\r\n            #print(outputs)\r\n            loss = outputs.loss\r\n            accelerator.backward(loss)\r\n            #loss.backward()\r\n\r\n            optimizer.step()\r\n            lr_scheduler.step()\r\n            optimizer.zero_grad()\r\n            losses.append(loss)\r\n            accelerator.print(f\"Epoch {epoch}-------Batch---{j}-----Loss---{loss}\")\r\n            \r\n        model.eval()\r\n        for i, batch in enumerate(eval_dataloader):\r\n            inputs: torch.Tensor = batch[\"input\"]#.to(accelerator.device)\r\n            with torch.no_grad():\r\n                predictions = accelerator.unwrap_model(model).generate(inputs)\r\n\r\n                generated_ids = accelerator.gather(predictions).cpu().numpy()\r\n                print(f\"Generated IDs: {generated_ids}\")\r\n                labels = accelerator.gather(batch[\"label_tensor\"]).cpu().numpy()\r\n                \r\n                generated_text = context.processor.batch_decode(generated_ids, skip_special_tokens=True)\r\n                labels_text = context.processor.batch_decode(labels, skip_special_tokens=True)\r\n                \r\n                predictions, labels = postprocess_text(generated_text, labels_text)\r\n                \r\n                cer_metric.add_batch(predictions=predictions, references=labels)\r\n                wer_metric.add_batch(predictions=predictions, references=labels)\r\n                print(f\"Predictions: {predictions}-----------Labels: {labels}\")\r\n        cer = cer_metric.compute()\r\n        wer = wer_metric.compute()\r\n\r\n        accelerator.print(f\"Average CER: {cer}------ Average WER: {wer}\")\r\n```\r\n\r\nthe python estimator is as follows:\r\n\r\n```\r\nfrom sagemaker.pytorch import PyTorch\r\nimport sagemaker\r\nrole = sagemaker.get_execution_role()\r\npt_estimator = PyTorch(\r\n    base_job_name=\"transformer-ocr-training\",\r\n    source_dir=\"source\",\r\n    entry_point=\"Train.py\",\r\n    role=role,    \r\n    py_version=\"py38\",\r\n    \r\n    image_uri =\"763104351884.dkr.ecr.us-east-1.amazonaws.com\/huggingface-pytorch-training:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\",\r\n\r\n    #framework_version=\"1.12.0\",\r\n\r\n    instance_count=1,\r\n    instance_type=\"ml.p3.16xlarge\"\r\n    #distribution={'smdistributed':{'dataparallel':{ 'enabled': True }}}\r\n)\r\n\r\npt_estimator.fit(\"s3:\/\/handwritten-ocr-training\")\r\n```\r\n\r\nExactly when I'm generate in for the evaluation set it always retrieves empty tensors. What am I missing here? However the number of processes is 8 GPUs so the accelerate has access to all of them however it's not generating in the validation all decoded strings are empty, appreciate your help! Hello @HebaGamalElDin, you are not using the \ud83e\udd17 Accelerate integration of AWS SageMaker correctly. To help you and others going forwards, I have spent time creating this repo https:\/\/github.com\/pacman100\/accelerate-aws-sagemaker which details on how to correctly use AWS SageMaker with \ud83e\udd17 Accelerate. it works correctly with generation `model.generate`. Please go through the README and files in the above repo and let us know if you still have issues.  Hello @pacman100 .. Thank you for the warm help.\r\nI have one question please, what I didn't get is how to configure accelerate inside the training job?\r\nmeaning where to run the command `accelerate config --config_file accelerate_config.yaml`? Have the [accelerate_config.yaml](https:\/\/github.com\/pacman100\/accelerate-aws-sagemaker\/blob\/af5caadcea0fa8186c11a784b6f86591c8fa5b3f\/src\/seq2seq\/accelerate_config.yaml) file should been replaced the python SDK estimator *PyTorch* in my case? Hello, you don't have to use any SageMaker estimator (PyTorch estimator in your case) as Accelerate internally uses Hugging Face SageMaker Estimator https:\/\/github.com\/huggingface\/accelerate\/blob\/main\/src\/accelerate\/commands\/launch.py#L776 along with all the necessary env variables to handle SageMaker DDP.\r\n\r\nJust create the accelerate config with command `accelerate config` on any virtual machine\/local machine\/sagemaker notebooks on which you have aws cli installed with aws credentials setup. After that when you run `accelerate launch` it will internally use HF estimator to create the training job on AWS SageMaker. I am running `accelerate config` and `accelerate launch` on a local machine with aws credentials setup.\r\n\r\n\r\n\r\n @pacman100 Okay I got that thank you.\r\nOne more question please, I'm encountering an issue when I'm testing, most of validation batches entirely are empty while some others are okay, this problem doesn't happen while training is on 1 GPU, What could be the problem here please?!\r\n**HINT: I'm logging the length of the text predictions coming by model.generate() for each batch, the majority is zero as shown in the below screenshot.**\r\n![image](https:\/\/user-images.githubusercontent.com\/36745656\/192077881-b5a598d0-1762-4335-9f2b-f07daa627318.png)\r\n",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1394547235287,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Warsaw, Poland",
        "Answerer_reputation_count":352.0,
        "Answerer_view_count":22.0,
        "Challenge_adjusted_solved_time":2.3974027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>dvc documentation for <code>dvc gc<\/code> command states, the <code>-r<\/code> option indicates \"Remote storage to collect garbage in\" but I'm not sure if I understand it correctly. For example I execute this command:<\/p>\n\n<pre><code>dvc gc -r myremote\n<\/code><\/pre>\n\n<p>What exactly happens if I execute this command? I have 2 possible answers:<\/p>\n\n<ol>\n<li>dvc checks which files should be deleted, then moves these files to \"myremote\" and then deletes all these files in local cache but not in remote.<\/li>\n<li>dvc checks which files should be deleted and deletes these files both in local cache and \"myremote\"<\/li>\n<\/ol>\n\n<p>Which one of them is correct?<\/p>",
        "Challenge_closed_time":1569833846627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569828438393,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the functionality of the \"dvc gc\" command with the \"-r\" option, which is used to indicate remote storage for garbage collection. The user is unsure whether executing the command will delete files only from the local cache or from both local and remote storage.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58163305",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":8.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.5022872222,
        "Challenge_title":"dvc gc and files in remote cache",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1617.0,
        "Challenge_word_count":107,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522254698710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Russia",
        "Poster_reputation_count":784.0,
        "Poster_view_count":77.0,
        "Solution_body":"<p>one of DVC maintainers here.<\/p>\n\n<p>Short answer: 2. is correct.<\/p>\n\n<p>A bit of additional information:\nPlease be careful when using <code>dvc gc<\/code>. It will clear your cache from all dependencies that are not mentioned in the current HEAD of your git repository. \nWe are working on making <code>dvc gc<\/code> preserving whole history by default. <\/p>\n\n<p>So if you don't want to delete files from your history commits, it would be better to wait for completion of <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2325\" rel=\"nofollow noreferrer\">this<\/a> task.<\/p>\n\n<p>[EDIT]\nPlease see comment below.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1569837069043,
        "Solution_link_count":1.0,
        "Solution_readability":7.5,
        "Solution_reading_time":7.71,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":86.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1589293508567,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>On Azure ML Workspace Notebook, I'm trying to get my workspace instance, as seen at<\/p>\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-auto-train-models#configure-workspace.<\/a><\/p>\n<p>I have a config file and I am running the notebook in an Azure compute instance.<\/p>\n<p>I tried to execute Workspace.from_config().<\/p>\n<p>As a result, I'm getting the 'MSIAuthentication' object has no attribute 'get_token' error.<\/p>\n<p>I tried to submit both <code>MsiAuthentication<\/code> and <code>InteractiveLoginAuthentication<\/code>, as suggested in<\/p>\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb.<\/a><\/p>",
        "Challenge_closed_time":1625753450150,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625753450150,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to execute Workspace.from_config() on Azure ML Workspace Notebook. The error message states that 'MSIAuthentication' object has no attribute 'get_token'. The user has tried to submit both MsiAuthentication and InteractiveLoginAuthentication but the issue persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68303285",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":28.1,
        "Challenge_reading_time":16.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.0,
        "Challenge_title":"'MSIAuthentication' object has no attribute 'get_token'",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1670.0,
        "Challenge_word_count":73,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589293508567,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":833.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p><strong>There are 2 solutions I've found:<\/strong><\/p>\n<p>1.- Use the kernel &quot;Python 3.6 - AzureML&quot;<\/p>\n<p>2.- <code>pip install azureml-core --upgrade<\/code><\/p>\n<p>This will <strong>upgrade<\/strong><\/p>\n<blockquote>\n<p>azureml-core to 1.32.0<\/p>\n<\/blockquote>\n<p>But will <strong>downgrade<\/strong>:<\/p>\n<blockquote>\n<p>azure-mgmt-resource to 13.0.0 (was 18.0.0)<\/p>\n<\/blockquote>\n<blockquote>\n<p>azure-mgmt-storage down to 11.2.0 (was 18.0.0)<\/p>\n<\/blockquote>\n<blockquote>\n<p>urllib3 to 1.26.5 (was 1.26.6)<\/p>\n<\/blockquote>\n<p>This upgrade \/ downgrade allows the same package versions as in the python 3.6 anaconda install<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.4,
        "Solution_reading_time":8.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1508517418056,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":156.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":3.9598897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to access the Workspace object in my <code>train.py<\/code> script, when running in an Estimator.  <\/p>\n\n<p>I currently can access the Run object, using the following code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>run = Run.get_context()\n<\/code><\/pre>\n\n<p>But I cannot seem to get my hands on the Workspace object in my training script.  I would use this mostly to get access to the Datastores and Datasets (as I would hope to keep all data set references inside the training script, instead of passing them as input datasets)<\/p>\n\n<p>Any idea if\/how this is possible ?<\/p>",
        "Challenge_closed_time":1591197955080,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591183699477,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to access the Workspace object in their training script in AzureML, specifically to get access to Datastores and Datasets. They are currently able to access the Run object but are unsure how to access the Workspace object.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62171724",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":8.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":3.9598897222,
        "Challenge_title":"How can I access the Workspace object from a training script in AzureML?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1148.0,
        "Challenge_word_count":102,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>Sure, try this:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\nws = run.experiment.workspace\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.8,
        "Solution_reading_time":1.78,
        "Solution_score_count":12.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":12.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":19.6539680556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi, I am new to Azure ML, and I have been trying to replicate the same structure presented in the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-train-models-with-aml\">MNIST tutorial<\/a>, but I don't understand how to adapt it to my case.     <\/p>\n<p>I am running a python file from the experiment, but I don't understand how I can access data that is currently in a folder in the cloud file system from the script running in the experiment.     <br \/>\nI have found many examples about accessing one single .csv file, but my data is made of many images.    <\/p>\n<p>From my understanding I should first load the folder to a datastore, then use Dataset.File.upload_directory to create a dataset containing my folder, and here is how I tried to do it:     <\/p>\n<pre><code># Create dataset from data directory  \ndatastore = Datastore.get(ws, 'workspaceblobstore')  \ndataset = Dataset.File.upload_directory(path_data, target, pattern=None, overwrite=False, show_progress=True)  \n  \nfile_dataset = dataset.register(workspace=ws, name='reduced_classification_dataset',  \n                                                 description='reduced_classification_dataset',  \n                                                 create_new_version=True)  \n<\/code><\/pre>\n<p>But then I don't understand if and how I can access this data like a normal file system from my python script, or I need further steps to be able to do that.     <\/p>",
        "Challenge_closed_time":1614833313088,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614762558803,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is new to Azure ML and is trying to access data from a folder in the cloud file system for a python script running in an experiment. They have tried to create a dataset using Dataset.File.upload_directory, but are unsure how to access the data like a normal file system from their script.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/296661\/azureml-notebooks-how-to-access-data-from-an-exper",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":17.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":19.6539680556,
        "Challenge_title":"AzureML Notebooks: how to access data from an experiment",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":187,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=bf39b622-b8af-42d8-809c-296225cdbb39\">@Matzof  <\/a> Thanks for the question. Please follow the below code for writing.    <\/p>\n<pre><code>   datastore = ## get your defined in Workspace as Datastore   \ndatastore.upload(src_dir='.\/files\/to\/copy\/...',  \n                 target_path='target\/directory',  \n                 overwrite=True)  \n<\/code><\/pre>\n<p>Datastore.upload only support blob and fileshare. For adlsgen2 upload, you can try our new dataset upload API:    <\/p>\n<pre><code>from azureml.core import Dataset, Datastore  \ndatastore = Datastore.get(workspace, 'mayadlsgen2')  \nDataset.File.upload_directory(src_dir='.\/data', target=(datastore,'data'))  \n<\/code><\/pre>\n<p>Pandas is integrated with fsspec which provides Pythonic implementation for filesystems including s3, gcs, and Azure. You can check the source for Azure here: <a href=\"https:\/\/github.com\/dask\/adlfs\">dask\/adlfs: fsspec-compatible Azure Datake and Azure Blob Storage access (github.com)<\/a>. With this you can use normal filesystem operations like ls, glob, info, etc.     <\/p>\n<p>You can find an example (for reading data) here: <a href=\"https:\/\/github.com\/Azure\/azureml-examples\/blob\/main\/tutorials\/using-dask\/1.intro-to-dask.ipynb\">azureml-examples\/1.intro-to-dask.ipynb at main \u00b7 Azure\/azureml-examples (github.com)<\/a>     <\/p>\n<p>Writing is essentially the same as reading, you need to switch the protocol to abfs (or az), slightly modify how you're accessing the data, and provide credentials unless your blob has public write access.     <\/p>\n<p>You can use the Azure ML Datastore to retrieve credentials like this (taken from example):     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/74112-2.png?platform=QnA\" alt=\"74112-2.png\" \/>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.5,
        "Solution_reading_time":22.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":177.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":69.30036,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>User is not authorized to query provided resources due to s2s call not providing any active baggage to verify role-based access.  <\/p>",
        "Challenge_closed_time":1614577233696,
        "Challenge_comment_count":2,
        "Challenge_created_time":1614327752400,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue while trying to automate a machine learning model on Azure portal. The problem was caused by the lack of authorization to query resources due to the absence of active baggage to verify role-based access.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/290347\/automate-machine-learning-model-on-azure-portal-fa",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.1,
        "Challenge_reading_time":2.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":69.30036,
        "Challenge_title":"Automate machine learning model on Azure portal failed",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":29,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>No update yet.  <\/p>\n<p>I have been receiving emails from Microsoft that they will disable my account for going against their policy.   <\/p>\n<p>Reason was because I am frequently using the Azure platform.  <\/p>\n<p>I decided to take a break from the platform to avoid them deleting my account.  <\/p>\n<p>Don't know what to do \ud83d\ude2d next<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.3,
        "Solution_reading_time":4.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":56.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":21.5217911111,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have a project \u201cfoo\u201d on my personal wandb account (entity \u201cuser\u201d). However, I am also a member of a team (\u201cteam\u201d). When I try to sync an offline run using <code>wandb sync path\/to\/foo\/run<\/code>, I want it to be saved in project \u201cfoo\u201d on my personal account. However, wandb creates a new project \u201cfoo\u201d that is owned by \u201cteam\u201d.<\/p>\n<p>Is there any way I can fix this? Do I need to change the way I\u2019m logged in to my wandb account? wandb says that I am logged in as <code>user (team)<\/code>, but I\u2019m not sure how to change that.<\/p>",
        "Challenge_closed_time":1683034536819,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682957058371,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has a personal project named \"foo\" on their Wandb account, but when they try to sync an offline run using \"wandb sync path\/to\/foo\/run\", it creates a new project named \"foo\" owned by their team instead of saving it in their personal account. The user is looking for a solution to fix this issue and is unsure if they need to change their login credentials.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-sync-confusing-personal-project-for-team-project\/4315",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":4.9,
        "Challenge_reading_time":7.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":21.5217911111,
        "Challenge_title":"Wandb sync confusing personal project for team project",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":103.0,
        "Challenge_word_count":106,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/adamoyoung\">@adamoyoung<\/a> thanks for reporting this issue. Could you please provide the <code>--entity<\/code> and <code>--project<\/code> arguments as follows:<br>\n<code>wandb sync -e personal -p foo path\/to\/foo\/run<\/code><\/p>\n<p>Would this work for you? There\u2019s a <code>Project Defaults<\/code> section in your <a href=\"https:\/\/wandb.ai\/settings\">personal settings page<\/a> where this in your case seems to be configured for your team entity. You may change that if you wanted the default to be your personal account.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.9,
        "Solution_reading_time":7.15,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":70.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1427953176670,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":145.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":14.9208969444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm having problems pushing files with DVC to DAGsHub.<\/p>\n<p>Workflow:<\/p>\n<ul>\n<li>I used my email to signup to DAGsHub.<\/li>\n<li>I created a repo and clone it to my computer.<\/li>\n<li>I added files to the repo and track them using DVC and Git to track the pointer files.<\/li>\n<li>Running DVC push -r origin, it asks me for my password. When I enter the password and hit enter - nothing happens.<\/li>\n<\/ul>\n<p>It sits and waits, barring me from even canceling the operation with Ctrl+C.\nI'm forced to manually close the terminal, open a new one, ending the &quot;Python&quot; process in task manager and delete the lock file in .dvc\/tmp\/lock.<\/p>",
        "Challenge_closed_time":1620591578072,
        "Challenge_comment_count":2,
        "Challenge_created_time":1620537862843,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing issues while pushing files with DVC to DAGsHub. The Git bash command prompt hangs when the user runs DVC push -r origin, and it asks for a password. However, when the user enters the password and hits enter, nothing happens, and the operation cannot be canceled with Ctrl+C. The user is forced to manually close the terminal, end the \"Python\" process in task manager, and delete the lock file in .dvc\/tmp\/lock.",
        "Challenge_last_edit_time":1620625906412,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67454531",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.0,
        "Challenge_reading_time":8.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":14.9208969444,
        "Challenge_title":"Git bash command prompt hanging when running dvc push to DAGsHub",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":254.0,
        "Challenge_word_count":117,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1620537484800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p><strong>Short answer<\/strong><\/p>\n<p>Do not use <code>ask_password<\/code>.\nInstead, save your token in the local config by running once:<\/p>\n<pre><code>dvc remote modify origin --local --unset ask_password\ndvc remote modify origin --local password &lt;--access token--&gt;\n<\/code><\/pre>\n<p><code>dvc push -r origin<\/code> should work then.<\/p>\n<p><strong>Long answer<\/strong><\/p>\n<p><a href=\"https:\/\/www.atlassian.com\/git\/tutorials\/git-bash#:%7E:text=What%20is%20Git%20Bash%3F,operating%20system%20through%20written%20commands.\" rel=\"nofollow noreferrer\">Git Bash<\/a> is not running the regular Windows command prompt but an emulated Unix-style bash prompt. From the information in your question, I cannot know for sure, but this is probably causing the <code>msvcrt<\/code> package used by DVC to prompt the password on windows machines to fail\/hang.<\/p>\n<p>There are potentially 3 ways to deal with the issue:<\/p>\n<ol>\n<li>Run <code>dvc pull<\/code> from the regular Windows cmd prompt.<\/li>\n<li>Find a way to make Git Bash wrap Python calls with <code>winpty<\/code> - I am not 100% positive about how to do this, but not using <code>winpty<\/code> seems to be the reason <code>msvcrt<\/code> fails at prompting for your password.<\/li>\n<li>The simplest solution - Do not use <code>ask_password<\/code>.\nInstead, save your token in the local config by running once:\n<pre><code>dvc remote modify origin --local --unset ask_password\ndvc remote modify origin --local password &lt;--access token--&gt;\n<\/code><\/pre>\nYou can get your access token by clicking on the question mark beside the DVC\nremote of your DAGsHub repository, then click on &quot;Reveal my token&quot;.<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":21.52,
        "Solution_score_count":4.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":211.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1366363157040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"South Africa",
        "Answerer_reputation_count":1761.0,
        "Answerer_view_count":328.0,
        "Challenge_adjusted_solved_time":4148.7025030556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use code that I got from github: <a href=\"https:\/\/github.com\/liamca\/azure-search-machine-learning-text-analytics\" rel=\"nofollow noreferrer\">https:\/\/github.com\/liamca\/azure-search-machine-learning-text-analytics<\/a> and the creating of an index works perfect, but the Keyphrase portion is giving me a 403 - Forbidden: Access is denied error. This happens in the TextExtractionHelper class on the following line of code:<\/p>\n\n<pre><code>if (!response.IsSuccessStatusCode)\n            {\n                throw new Exception(\"Call to get key phrases failed with HTTP status code: \" +\n                                    response.StatusCode + \" and contents: \" + content);\n            }\n<\/code><\/pre>\n\n<p>Based on the information in the comments, I created an account at this link: <a href=\"https:\/\/datamarket.azure.com\/account\/keys\" rel=\"nofollow noreferrer\">https:\/\/datamarket.azure.com\/account\/keys<\/a> and used the key that it provided, but I am getting the above error.<\/p>\n\n<p>Here is the code in case you do not want to download from github:<\/p>\n\n<pre><code> class Program\n    {\n        static string searchServiceName = \"&lt;removed&gt;\";     \/\/ Learn more here: https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/search-what-is-azure-search\/\n        static string searchServiceAPIKey = \"&lt;removed&gt;\";\n        static string azureMLTextAnalyticsKey = \"&lt;removed&gt;\";     \/\/ Learn more here: https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-apps-text-analytics\/\n\n        static string indexName = \"textanalytics\";\n        static SearchServiceClient serviceClient = new SearchServiceClient(searchServiceName, new SearchCredentials(searchServiceAPIKey));\n        static SearchIndexClient indexClient = serviceClient.Indexes.GetClient(indexName);\n\n        static void Main(string[] args)\n        {\n            string filetext = \"Build great search experiences for your web and mobile apps. \" +\n                \"Many applications use search as the primary interaction pattern for their users. When it comes to search, user expectations are high. They expect great relevance, suggestions, near-instantaneous responses, multiple languages, faceting, and more. Azure Search makes it easy to add powerful and sophisticated search capabilities to your website or application. The integrated Microsoft natural language stack, also used in Bing and Office, has been improved over 16 years of development. Quickly and easily tune search results, and construct rich, fine-tuned ranking models to tie search results to business goals. Reliable throughput and storage provide fast search indexing and querying to support time-sensitive search scenarios. \" +\n                \"Reduce complexity with a fully managed service. \" +\n                \"Azure Search removes the complexity of setting up and managing your own search index. This fully managed service helps you avoid the hassle of dealing with index corruption, service availability, scaling, and service updates. Create multiple indexes with no incremental cost per index. Easily scale up or down as the traffic and data volume of your application changes.\";\n\n            \/\/ Note, this will create a new Azure Search Index for the text and the key phrases\n            Console.WriteLine(\"Creating Azure Search index...\");\n            AzureSearch.CreateIndex(serviceClient, indexName);\n\n            \/\/ Apply the Machine Learning Text Extraction to retrieve only the key phrases\n            Console.WriteLine(\"Extracting key phrases from processed text... \\r\\n\");\n            KeyPhraseResult keyPhraseResult = TextExtraction.ProcessText(azureMLTextAnalyticsKey, filetext);\n\n            Console.WriteLine(\"Found the following phrases... \\r\\n\");\n            foreach (var phrase in keyPhraseResult.KeyPhrases)\n                Console.WriteLine(phrase);\n\n            \/\/ Take the resulting key phrases to a new Azure Search Index\n            \/\/ It is highly recommended that you upload documents in batches rather \n            \/\/ individually like is done here\n            Console.WriteLine(\"Uploading extracted text to Azure Search...\\r\\n\");\n            AzureSearch.UploadDocuments(indexClient, \"1\", keyPhraseResult);\n            Console.WriteLine(\"Wait 5 seconds for content to become searchable...\\r\\n\");\n            Thread.Sleep(5000);\n\n            \/\/ Execute a test search \n            Console.WriteLine(\"Execute Search...\");\n            AzureSearch.SearchDocuments(indexClient, \"Azure Search\");\n\n            Console.WriteLine(\"All done.  Press any key to continue.\");\n            Console.ReadLine();\n\n        }\n    }\n<\/code><\/pre>\n\n<p>The below is in the TextExtractionHelper class:<\/p>\n\n<pre><code>\/\/\/ &lt;summary&gt;\n\/\/\/ This is a sample program that shows how to use the Azure ML Text Analytics app (https:\/\/datamarket.azure.com\/dataset\/amla\/text-analytics)\n\/\/\/ &lt;\/summary&gt;\npublic class TextExtraction\n{\n    private const string ServiceBaseUri = \"https:\/\/api.datamarket.azure.com\/\";\n    public static KeyPhraseResult ProcessText(string accountKey, string inputText)\n    {\n        KeyPhraseResult keyPhraseResult = new KeyPhraseResult();\n        using (var httpClient = new HttpClient())\n        {\n            string inputTextEncoded = HttpUtility.UrlEncode(inputText);\n            httpClient.BaseAddress = new Uri(ServiceBaseUri);\n            string creds = \"AccountKey:\" + accountKey;\n            string authorizationHeader = \"Basic \" + Convert.ToBase64String(Encoding.ASCII.GetBytes(creds));\n            httpClient.DefaultRequestHeaders.Add(\"Authorization\", authorizationHeader);\n            httpClient.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(\"application\/json\"));\n            \/\/ get key phrases\n            string keyPhrasesRequest = \"data.ashx\/amla\/text-analytics\/v1\/GetKeyPhrases?Text=\" + inputTextEncoded;\n            Task&lt;HttpResponseMessage&gt; responseTask = httpClient.GetAsync(keyPhrasesRequest);\n            responseTask.Wait();\n            HttpResponseMessage response = responseTask.Result;\n            Task&lt;string&gt; contentTask = response.Content.ReadAsStringAsync();\n            contentTask.Wait();\n            string content = contentTask.Result;\n            if (!response.IsSuccessStatusCode)\n            {\n                throw new Exception(\"Call to get key phrases failed with HTTP status code: \" +\n                                    response.StatusCode + \" and contents: \" + content);\n            }\n            keyPhraseResult = JsonConvert.DeserializeObject&lt;KeyPhraseResult&gt;(content);\n        }\n        return keyPhraseResult;\n    }\n}\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold result of Key Phrases call\n\/\/\/ &lt;\/summary&gt;\npublic class KeyPhraseResult\n{\n    public List&lt;string&gt; KeyPhrases { get; set; }\n}\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold result of Sentiment call\n\/\/\/ &lt;\/summary&gt;\npublic class SentimentResult\n{\n    public double Score { get; set; }\n}\n\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold result of Language detection call\n\/\/\/ &lt;\/summary&gt;\npublic class LanguageResult\n{\n    public bool UnknownLanguage { get; set; }\n    public IList&lt;DetectedLanguage&gt; DetectedLanguages { get; set; }\n}\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold information about a single detected language\n\/\/\/ &lt;\/summary&gt;\npublic class DetectedLanguage\n{\n    public string Name { get; set; }\n\n    \/\/\/ &lt;summary&gt;\n    \/\/\/ This is the short ISO 639-1 standard form of representing\n    \/\/\/ all languages. The short form is a 2 letter representation of the language.\n    \/\/\/ en = English, fr = French for example\n    \/\/\/ &lt;\/summary&gt;\n    public string Iso6391Name { get; set; }\n    public double Score { get; set; }\n}\n<\/code><\/pre>\n\n<p><strong>UPDATE<\/strong><\/p>\n\n<p>After many hours of taking different sample code and trying to put them together, I finally got something \"kind of\" working. Here is all my code:<\/p>\n\n<pre><code>class Program\n{\n    static string searchServiceName = \"&lt;removed&gt;\";     \/\/ Learn more here: https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/search-what-is-azure-search\/\n    static string searchServiceAPIKey = \"&lt;removed&gt;\";\n    \/\/static string azureMLTextAnalyticsKey = \"&lt;removed&gt;\";     \/\/ Learn more here: https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-apps-text-analytics\/\n\n    static string indexName = \"textanalytics\";\n    static SearchServiceClient serviceClient = new SearchServiceClient(searchServiceName, new SearchCredentials(searchServiceAPIKey));\n    static SearchIndexClient indexClient = serviceClient.Indexes.GetClient(indexName);\n\n    static void Main()\n    {\n        MakeRequests();\n        Console.WriteLine(\"Hit ENTER to exit...\");\n        Console.ReadLine();\n    }\n\n    static async void MakeRequests()\n    {\n        \/\/ Note, this will create a new Azure Search Index for the text and the key phrases\n        Console.WriteLine(\"Creating Azure Search index...\");\n        AzureSearch.CreateIndex(serviceClient, indexName);\n\n        \/\/ Apply the Machine Learning Text Extraction to retrieve only the key phrases\n        Console.WriteLine(\"Extracting key phrases from processed text... \\r\\n\");\n        KeyPhraseResult keyPhraseResult = await TextExtraction.ProcessText();\n\n        Console.WriteLine(\"Found the following phrases... \\r\\n\");\n        foreach (var phrase in keyPhraseResult.KeyPhrases)\n            Console.WriteLine(phrase);\n\n        \/\/ Take the resulting key phrases to a new Azure Search Index\n        \/\/ It is highly recommended that you upload documents in batches rather \n        \/\/ individually like is done here\n        Console.WriteLine(\"Uploading extracted text to Azure Search...\\r\\n\");\n        AzureSearch.UploadDocuments(indexClient, \"1\", keyPhraseResult);\n        Console.WriteLine(\"Wait 5 seconds for content to become searchable...\\r\\n\");\n        Thread.Sleep(5000);\n\n        \/\/ Execute a test search \n        Console.WriteLine(\"Execute Search...\");\n        AzureSearch.SearchDocuments(indexClient, \"Azure Search\");\n\n        Console.WriteLine(\"All done.  Press any key to continue.\");\n        Console.ReadLine();\n    }\n}\n<\/code><\/pre>\n\n<p>Here is my TextExtractionHelper class:<\/p>\n\n<pre><code>public class TextExtraction\n{\n    static string azureMLTextAnalyticsKey = \"&lt;Removed&gt;\";     \/\/ Learn more here: https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-apps-text-analytics\/\n    private const string ServiceBaseUri = \"https:\/\/westus.api.cognitive.microsoft.com\/\";\n    public static async Task&lt;KeyPhraseResult&gt; ProcessText()\n    {\n        string filetext = \"Build great search experiences for your web and mobile apps. \" +\n            \"Many applications use search as the primary interaction pattern for their users. When it comes to search, user expectations are high. They expect great relevance, suggestions, near-instantaneous responses, multiple languages, faceting, and more. Azure Search makes it easy to add powerful and sophisticated search capabilities to your website or application. The integrated Microsoft natural language stack, also used in Bing and Office, has been improved over 16 years of development. Quickly and easily tune search results, and construct rich, fine-tuned ranking models to tie search results to business goals. Reliable throughput and storage provide fast search indexing and querying to support time-sensitive search scenarios. \" +\n            \"Reduce complexity with a fully managed service. \" +\n            \"Azure Search removes the complexity of setting up and managing your own search index. This fully managed service helps you avoid the hassle of dealing with index corruption, service availability, scaling, and service updates. Create multiple indexes with no incremental cost per index. Easily scale up or down as the traffic and data volume of your application changes.\";\n\n\n        KeyPhraseResult keyPhraseResult = new KeyPhraseResult();\n        using (var httpClient = new HttpClient())\n        {\n\n            httpClient.BaseAddress = new Uri(ServiceBaseUri);\n\n            \/\/ Request headers.\n            httpClient.DefaultRequestHeaders.Add(\"Ocp-Apim-Subscription-Key\", azureMLTextAnalyticsKey);\n            httpClient.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(\"application\/json\"));\n\n            byte[] byteData = Encoding.UTF8.GetBytes(\"{\\\"documents\\\":[\" +\n               \"{\\\"id\\\":\\\"1\\\",\\\"text\\\":\\\"\" + filetext + \"\\\"},]}\");\n\n            \/\/byte[] byteData = Encoding.UTF8.GetBytes(\"{\\\"documents\\\":[\" +\n            \/\/   \"{\\\"id\\\":\\\"1\\\",\\\"text\\\":\\\"Build great search experiences for your web and mobile apps.\" +\n            \/\/   \"Many applications use search as the primary interaction pattern for their users. When it comes to search, user expectations are high. They expect great relevance, suggestions, near-instantaneous responses, multiple languages, faceting, and more. Azure Search makes it easy to add powerful and sophisticated search capabilities to your website or application. The integrated Microsoft natural language stack, also used in Bing and Office, has been improved over 16 years of development. Quickly and easily tune search results, and construct rich, fine-tuned ranking models to tie search results to business goals. Reliable throughput and storage provide fast search indexing and querying to support time-sensitive search scenarios.\" +\n            \/\/   \"Reduce complexity with a fully managed service. \" +\n            \/\/   \"Azure Search removes the complexity of setting up and managing your own search index. This fully managed service helps you avoid the hassle of dealing with index corruption, service availability, scaling, and service updates. Create multiple indexes with no incremental cost per index. Easily scale up or down as the traffic and data volume of your application changes.\\\"},\" +\n            \/\/   \"]}\");\n\n            \/\/ Detect key phrases:\n            var keyPhrasesRequest = \"text\/analytics\/v2.0\/keyPhrases\";\n            \/\/var response = await CallEndpoint(httpClient, uri, byteData);\n\n            \/\/ get key phrases\n            using (var getcontent = new ByteArrayContent(byteData))\n            {\n                getcontent.Headers.ContentType = new MediaTypeHeaderValue(\"application\/json\");\n                var response = await httpClient.PostAsync(keyPhrasesRequest, getcontent);\n\n                Task&lt;string&gt; contentTask = response.Content.ReadAsStringAsync();\n\n                string content = contentTask.Result;\n\n                if (!response.IsSuccessStatusCode)\n                {\n                    throw new Exception(\"Call to get key phrases failed with HTTP status code: \" +\n                                        response.StatusCode + \" and contents: \" + content);\n                }\n                keyPhraseResult = JsonConvert.DeserializeObject&lt;KeyPhraseResult&gt;(content);\n                \/\/return await response.Content.ReadAsStringAsync();\n            }\n        }\n        return keyPhraseResult;\n    }\n}\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold result of Key Phrases call\n\/\/\/ &lt;\/summary&gt;\npublic class KeyPhraseResult\n{\n    public List&lt;string&gt; KeyPhrases { get; set; }\n}\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold result of Sentiment call\n\/\/\/ &lt;\/summary&gt;\npublic class SentimentResult\n{\n    public double Score { get; set; }\n}\n\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold result of Language detection call\n\/\/\/ &lt;\/summary&gt;\npublic class LanguageResult\n{\n    public bool UnknownLanguage { get; set; }\n    public IList&lt;DetectedLanguage&gt; DetectedLanguages { get; set; }\n}\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold information about a single detected language\n\/\/\/ &lt;\/summary&gt;\npublic class DetectedLanguage\n{\n    public string Name { get; set; }\n\n    \/\/\/ &lt;summary&gt;\n    \/\/\/ This is the short ISO 639-1 standard form of representing\n    \/\/\/ all languages. The short form is a 2 letter representation of the language.\n    \/\/\/ en = English, fr = French for example\n    \/\/\/ &lt;\/summary&gt;\n    public string Iso6391Name { get; set; }\n    public double Score { get; set; }\n}\n<\/code><\/pre>\n\n<p>So I am now able to pull the KeyPhrases from the text! But, now I am sitting with a problem where it doesnt seem like the JSON string is being deserialized and my keyPhraseResult is now getting a null value.<\/p>\n\n<p>What am I missing ?<\/p>\n\n<p>If anyone is able to help, I would greatly appreciate it.<\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1480685355832,
        "Challenge_comment_count":1,
        "Challenge_created_time":1480585773797,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a 403 - Forbidden: Access is denied error while using Azure Text Analytics with C#. The error occurs in the TextExtractionHelper class while trying to get key phrases. The user has created an account and used the provided key, but the error persists. The user has tried different sample codes and has managed to get the key phrases, but the JSON string is not being deserialized, resulting in a null value for keyPhraseResult. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1480606926256,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40907303",
        "Challenge_link_count":12,
        "Challenge_participation_count":2,
        "Challenge_readability":14.1,
        "Challenge_reading_time":189.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":121,
        "Challenge_solved_time":27.6616763889,
        "Challenge_title":"Azure Text Analytics using C# giving errors",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1204.0,
        "Challenge_word_count":1536,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1366363157040,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"South Africa",
        "Poster_reputation_count":1761.0,
        "Poster_view_count":328.0,
        "Solution_body":"<p>So I got it working! With the help from this link: <a href=\"https:\/\/stackoverflow.com\/questions\/40932669\/deserializing-json-using-c-sharp-to-return-items\">Deserializing JSON using C# to return items<\/a>, which i posted to simplify where my issue was occurring now.<\/p>\n\n<p>So what this code is doing is the following:<\/p>\n\n<ol>\n<li>Creating an Index in Azure called textanalytics.<\/li>\n<li>Creating a JSON string of the text provided.<\/li>\n<li>Retrieving the KeyPhrases and adding these to the Index created in point 1 above.<\/li>\n<\/ol>\n\n<p>Below is my entire code, in case it helps someone else:<\/p>\n\n<p>(Please ensure that you add the relevant references from Nuget packages: Microsoft.Azure.Search and Newtonsoft.Json)<\/p>\n\n<p>Program.cs(This is a console application):<\/p>\n\n<pre><code>using Microsoft.Azure.Search;\nusing System;\nusing System.Configuration;\nusing System.IO;\nusing System.Threading;\n\nnamespace AzureSearchTextAnalytics\n{\nclass Program\n{\n    static string searchServiceName = \"&lt;removed&gt;\";     \/\/ This is the Azure Search service name that you create in Azure\n    static string searchServiceAPIKey = \"&lt;removed&gt;\";   \/\/ This is the Primary key that is provided after creating a Azure Search Service\n\n    static string indexName = \"textanalytics\";\n    static SearchServiceClient serviceClient = new SearchServiceClient(searchServiceName, new SearchCredentials(searchServiceAPIKey));\n    static SearchIndexClient indexClient = serviceClient.Indexes.GetClient(indexName);\n\n    static void Main()\n    {\n        MakeRequests();\n        Console.WriteLine(\"Hit ENTER to exit...\");\n        Console.ReadLine();\n    }\n\n    static async void MakeRequests()\n    {\n        \/\/ Note, this will create a new Azure Search Index for the text and the key phrases\n        Console.WriteLine(\"Creating Azure Search index...\");\n        AzureSearch.CreateIndex(serviceClient, indexName);\n\n        \/\/ Apply the Machine Learning Text Extraction to retrieve only the key phrases\n        Console.WriteLine(\"Extracting key phrases from processed text... \\r\\n\");\n        KeyPhraseResult keyPhraseResult = await TextExtraction.ProcessText();\n\n        Console.WriteLine(\"Found the following phrases... \\r\\n\");\n        foreach (var phrase in keyPhraseResult.KeyPhrases)\n            Console.WriteLine(phrase);\n\n        \/\/ Take the resulting key phrases to a new Azure Search Index\n        \/\/ It is highly recommended that you upload documents in batches rather \n        \/\/ individually like is done here\n        Console.WriteLine(\"Uploading extracted text to Azure Search...\\r\\n\");\n        AzureSearch.UploadDocuments(indexClient, \"1\", keyPhraseResult);\n        Console.WriteLine(\"Wait 5 seconds for content to become searchable...\\r\\n\");\n        Thread.Sleep(5000);\n\n        \/\/ Execute a test search \n        Console.WriteLine(\"Execute Search...\");\n        AzureSearch.SearchDocuments(indexClient, \"Azure Search\");\n\n        Console.WriteLine(\"All done.  Press any key to continue.\");\n        Console.ReadLine();\n    }\n}\n}\n<\/code><\/pre>\n\n<p>My TextExtractionHelper.cs:<\/p>\n\n<pre><code>using System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Text;\nusing System.Threading.Tasks;\nusing System.Web;\nusing Newtonsoft.Json;\nusing System.Configuration; \/\/ get it from http:\/\/www.newtonsoft.com\/json\nusing Newtonsoft.Json.Linq;\n\nnamespace AzureSearchTextAnalytics\n{\n\/\/\/ &lt;\/summary&gt;\npublic class TextExtraction\n{\n    static string azureMLTextAnalyticsKey = \"&lt;removed&gt;\";     \/\/ This key you will get when you have added TextAnalytics in Azure.\n    private const string ServiceBaseUri = \"https:\/\/westus.api.cognitive.microsoft.com\/\"; \/\/This you will get when you have added TextAnalytics in Azure\n    public static async Task&lt;KeyPhraseResult&gt; ProcessText()\n    {\n        string filetext = \"Build great search experiences for your web and mobile apps. \" +\n            \"Many applications use search as the primary interaction pattern for their users. When it comes to search, user expectations are high. They expect great relevance, suggestions, near-instantaneous responses, multiple languages, faceting, and more. Azure Search makes it easy to add powerful and sophisticated search capabilities to your website or application. The integrated Microsoft natural language stack, also used in Bing and Office, has been improved over 16 years of development. Quickly and easily tune search results, and construct rich, fine-tuned ranking models to tie search results to business goals. Reliable throughput and storage provide fast search indexing and querying to support time-sensitive search scenarios. \" +\n            \"Reduce complexity with a fully managed service. \" +\n            \"Azure Search removes the complexity of setting up and managing your own search index. This fully managed service helps you avoid the hassle of dealing with index corruption, service availability, scaling, and service updates. Create multiple indexes with no incremental cost per index. Easily scale up or down as the traffic and data volume of your application changes.\";\n\n\n        KeyPhraseResult keyPhraseResult = new KeyPhraseResult();\n        using (var httpClient = new HttpClient())\n        {\n\n            httpClient.BaseAddress = new Uri(ServiceBaseUri);\n\n            \/\/ Request headers.\n            httpClient.DefaultRequestHeaders.Add(\"Ocp-Apim-Subscription-Key\", azureMLTextAnalyticsKey);\n            httpClient.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(\"application\/json\"));\n\n            byte[] byteData = Encoding.UTF8.GetBytes(\"{\\\"documents\\\":[\" +\n               \"{\\\"id\\\":\\\"1\\\",\\\"text\\\":\\\"\" + filetext + \"\\\"},]}\");\n\n            \/\/ Detect key phrases:\n            var keyPhrasesRequest = \"text\/analytics\/v2.0\/keyPhrases\";\n\n            \/\/ get key phrases\n            using (var getcontent = new ByteArrayContent(byteData))\n            {\n                getcontent.Headers.ContentType = new MediaTypeHeaderValue(\"application\/json\");\n                var response = await httpClient.PostAsync(keyPhrasesRequest, getcontent);\n\n                Task&lt;string&gt; contentTask = response.Content.ReadAsStringAsync();\n\n                string content = contentTask.Result;\n\n                if (!response.IsSuccessStatusCode)\n                {\n                    throw new Exception(\"Call to get key phrases failed with HTTP status code: \" +\n                                        response.StatusCode + \" and contents: \" + content);\n                }\n\n\n                var result = JsonConvert.DeserializeObject&lt;RootObject&gt;(content);\n\n                keyPhraseResult.KeyPhrases = result.documents[0].keyPhrases;\n            }\n\n        }\n        return keyPhraseResult;\n    }\n}\n\npublic class Documents\n{\n    public List&lt;string&gt; keyPhrases { get; set; }\n    public string id { get; set; }\n}\n\npublic class RootObject\n{\n    public List&lt;Documents&gt; documents { get; set; }\n    public List&lt;object&gt; errors { get; set; }\n}\n\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold result of Key Phrases call\n\/\/\/ &lt;\/summary&gt;\npublic class KeyPhraseResult\n{\n    public List&lt;string&gt; KeyPhrases { get; set; }\n}\n}\n<\/code><\/pre>\n\n<p>AzureSearch.cs:<\/p>\n\n<pre><code>using Microsoft.Azure.Search;\nusing Microsoft.Azure.Search.Models;\nusing System;\nusing System.Collections.Generic;\nusing System.Configuration;\nusing System.Linq;\nusing System.Text;\nusing System.Threading.Tasks;\n\nnamespace AzureSearchTextAnalytics\n{\npublic class AzureSearch\n{\n    public static void CreateIndex(SearchServiceClient serviceClient, string indexName)\n    {\n\n        if (serviceClient.Indexes.Exists(indexName))\n        {\n            serviceClient.Indexes.Delete(indexName);\n        }\n\n        var definition = new Index()\n        {\n            Name = indexName,\n            Fields = new[]\n            {\n                new Field(\"fileId\", DataType.String)                       { IsKey = true },\n                new Field(\"fileText\", DataType.String)                      { IsSearchable = true, IsFilterable = false, IsSortable = false, IsFacetable = false },\n                new Field(\"keyPhrases\", DataType.Collection(DataType.String)) { IsSearchable = true, IsFilterable = true,  IsFacetable = true }\n            }\n        };\n\n        serviceClient.Indexes.Create(definition);\n    }\n\n    public static void UploadDocuments(SearchIndexClient indexClient, string fileId, KeyPhraseResult keyPhraseResult)\n    {\n        List&lt;IndexAction&gt; indexOperations = new List&lt;IndexAction&gt;();\n        var doc = new Document();\n        doc.Add(\"fileId\", fileId);\n        doc.Add(\"keyPhrases\", keyPhraseResult.KeyPhrases.ToList());\n        indexOperations.Add(IndexAction.Upload(doc));\n\n        try\n        {\n            indexClient.Documents.Index(new IndexBatch(indexOperations));\n        }\n        catch (IndexBatchException e)\n        {\n            \/\/ Sometimes when your Search service is under load, indexing will fail for some of the documents in\n            \/\/ the batch. Depending on your application, you can take compensating actions like delaying and\n            \/\/ retrying. For this simple demo, we just log the failed document keys and continue.\n            Console.WriteLine(\n            \"Failed to index some of the documents: {0}\",\n                   String.Join(\", \", e.IndexingResults.Where(r =&gt; !r.Succeeded).Select(r =&gt; r.Key)));\n        }\n\n    }\n\n\n    public static void SearchDocuments(SearchIndexClient indexClient, string searchText)\n    {\n        \/\/ Search using the supplied searchText and output documents that match \n        try\n        {\n            var sp = new SearchParameters();\n\n            DocumentSearchResult&lt;OCRTextIndex&gt; response = indexClient.Documents.Search&lt;OCRTextIndex&gt;(searchText, sp);\n            foreach (SearchResult&lt;OCRTextIndex&gt; result in response.Results)\n            {\n                Console.WriteLine(\"File ID: {0}\", result.Document.fileId);\n                Console.WriteLine(\"Key Phrases: {0}\", string.Join(\",\", result.Document.keyPhrases));\n\n            }\n        }\n        catch (Exception e)\n        {\n            Console.WriteLine(\"Failed search: {0}\", e.Message.ToString());\n        }\n\n    }\n\n}\n}\n<\/code><\/pre>\n\n<p>DataModel.cs<\/p>\n\n<pre><code>using Microsoft.Azure.Search.Models;\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\nusing System.Threading.Tasks;\n\nnamespace AzureSearchTextAnalytics\n{\n[SerializePropertyNamesAsCamelCase]\npublic class OCRTextIndex\n{\n    public string fileId { get; set; }\n\n    public string[] keyPhrases { get; set; }\n\n}\n}\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1495542255267,
        "Solution_link_count":3.0,
        "Solution_readability":13.8,
        "Solution_reading_time":120.9,
        "Solution_score_count":6.0,
        "Solution_sentence_count":101.0,
        "Solution_word_count":900.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1038888889,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nMy job is stack with a warning status, I configured a private bitbucket connection and the cloning fails.",
        "Challenge_closed_time":1649328708000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649328334000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing issues with their job status due to a failed cloning process while configuring a private bitbucket connection in their init git container. They are seeking guidance on how to debug the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1472",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":7.2,
        "Challenge_reading_time":1.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.1038888889,
        "Challenge_title":"How to debug my init git container",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Before updating the connections or changing anything about your current deployment, please perform the following debugging steps:\n\nEnable logs from all containers:\n\n\n\nYou can also inspect the operations from the statuses page to get more information (for distributed runs you can select the correct pod)\n\n\nYou can suspend the init container using :\n\n  - connection: my-connection\n    git: {...}\n    container:\n      command: [\"\/bin\/bash\", \"-c\"]\n      args: [\"sleep 3600\"]\nUse shell to get inside the container (for distributed runs you can select the correct pod and container):",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.1,
        "Solution_reading_time":6.86,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":80.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1395431574432,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Islamabad Capital Territory, Pakistan",
        "Answerer_reputation_count":1520.0,
        "Answerer_view_count":214.0,
        "Challenge_adjusted_solved_time":2.3479152778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I started to work with AWS SageMaker. I have an AWS Starter Account. I have been trying to deploy a built-in algorithm for 2 days but I always get <code>AccessDeniedException<\/code> despite the fact that I created IAM role according to <a href=\"https:\/\/aws.amazon.com\/tr\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/tr\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a><\/p>\n\n<blockquote>\n  <p>ClientError: An error occurred (AccessDeniedException) when calling the CreateTrainingJob operation: User: arn:aws:sts::161745376217:assumed-role\/AmazonSageMaker-ExecutionRole-20200203T194557\/SageMaker is not authorized to perform: sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:us-east-1:161745376217:training-job\/blazingtext-2020-02-03-18-12-14-017 with an explicit deny<\/p>\n<\/blockquote>\n\n<p>Could you help me to solve this problem ?\nThank you so much<\/p>",
        "Challenge_closed_time":1580764036812,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580755584317,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an \"AccessDeniedException\" error while trying to deploy a built-in algorithm on AWS SageMaker despite creating an IAM role as per the tutorial. The error message suggests that the user is not authorized to perform the \"CreateTrainingJob\" action on the resource. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1580790326547,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60045326",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.9,
        "Challenge_reading_time":13.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2.3479152778,
        "Challenge_title":"AWS SageMaker Access Denied",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":3755.0,
        "Challenge_word_count":86,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1481983208856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>You have created a role for SageMaker to access S3 bucket, but it seems your IAM user doesn't have access to SageMaker service. Please make sure your IAM user has permission to SageMaker.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":2.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1614873430827,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":169.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":14.7412641667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to setup DVC with Google Drive storage as shown <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#url-format\" rel=\"nofollow noreferrer\">here<\/a>. So far, I've been unsuccessful in pushing data to the remote. I tried both with and without the Google App setup.<\/p>\n<p>After running a <code>dvc push -v<\/code>, the following exception is shown:<\/p>\n<pre><code>  File &quot;(...)\/anaconda3\/lib\/python3.8\/site-packages\/googleapiclient\/discovery.py&quot;, line 387, in _retrieve_discovery_doc\n    raise UnknownApiNameOrVersion(&quot;name: %s  version: %s&quot; % (serviceName, version))\ngoogleapiclient.errors.UnknownApiNameOrVersion: name: drive  version: v2\n<\/code><\/pre>\n<p>DVC was installed via <code>pip install dvc[gdrive]<\/code>. The <code>pip freeze<\/code> of the concerning packages is:<\/p>\n<pre><code>oauth2client==4.1.3\ngoogle-api-python-client==2.0.1\ndvc==2.0.1\n<\/code><\/pre>\n<p>Any help is thoroughly appreciated.<\/p>",
        "Challenge_closed_time":1614873489452,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614869465907,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is attempting to set up Data Version Control (DVC) with Google Drive storage but is unable to push data to the remote. They have tried both with and without the Google App setup. After running a \"dvc push -v\" command, an exception is shown indicating an \"UnknownApiNameOrVersion\" error with the Google Drive API version. The user has installed DVC via pip and the concerning packages are oauth2client, google-api-python-client, and dvc.",
        "Challenge_last_edit_time":1614874338736,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66477468",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":14.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.1176513889,
        "Challenge_title":"Data Version Control with Google Drive Remote: \"googleapiclient.errors.UnknownApiNameOrVersion: name: drive version: v2\"",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":979.0,
        "Challenge_word_count":102,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1593006899208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":704.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>Can you try to install <code>google-api-python-client==1.12.8<\/code> and test in that way?<\/p>\n<p>Edit:<\/p>\n<p>It appears to be that, this was a bug in the 2.0.0-2.0.1 of google-api-client and resolved in 2.0.2. So this should also work <code>google-api-python-client&gt;=2.0.2<\/code><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1614927407287,
        "Solution_link_count":0.0,
        "Solution_readability":3.7,
        "Solution_reading_time":3.76,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1481732617127,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dallas, TX, United States",
        "Answerer_reputation_count":70265.0,
        "Answerer_view_count":9379.0,
        "Challenge_adjusted_solved_time":1.3378227778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have these two datasets defined:<\/p>\n<pre><code>flp_test_query:\n  type: pandas.SQLQueryDataSet\n  credentials: dw_dev_credentials\n  sql: select numero from dwdb.dwschema.flp_tst\n  load_args:\n    index_col: [numero]\n\nflp_test:\n  type: pandas.SQLTableDataSet\n  credentials: dw_dev_credentials\n  table_name: flp_tst\n  load_args:\n    index_col: ['numero']\n    columns: ['numero']\n  save_args:\n    if_exists: 'append'\n<\/code><\/pre>\n<p>However, I only manged to get <code>flp_test_query<\/code> working, as when I try to access <code>flp_tst<\/code> I get this error:<\/p>\n<blockquote>\n<p>ValueError: Table flp_tst not found<\/p>\n<\/blockquote>\n<p>I did try to define table name as <code>table_name: dwschema.flp_tst<\/code> and <code>table_name: dwdb.dwschema.flp_tst<\/code> but all trew the same error. What am I missing?<\/p>",
        "Challenge_closed_time":1626124577252,
        "Challenge_comment_count":2,
        "Challenge_created_time":1626119761090,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with Kedro where they are unable to access a SQL Server table named \"flp_tst\" using the defined dataset \"flp_test\". They have tried defining the table name in different ways but are still receiving a \"Table flp_tst not found\" error. They are able to access another dataset \"flp_test_query\" successfully.",
        "Challenge_last_edit_time":1644169948123,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68353241",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.1,
        "Challenge_reading_time":10.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.3378227778,
        "Challenge_title":"Kedro can not find SQL Server table",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":117.0,
        "Challenge_word_count":90,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1271930452580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":5469.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>From the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.extras.datasets.pandas.SQLTableDataSet.html\" rel=\"nofollow noreferrer\">docs<\/a> it looks like you can specify the schema in <code>load_args<\/code>, eg<\/p>\n<pre><code>  load_args:\n    index_col: ['numero']\n    columns: ['numero']\n    schema: 'dwschema'\n<\/code><\/pre>\n<p>or<\/p>\n<pre><code>load_args = {&quot;schema&quot;,&quot;dwschema&quot;}\ndata_set = SQLTableDataSet(table_name=table_name,\n                           credentials=credentials,\n                            load_args=load_args)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.4,
        "Solution_reading_time":6.87,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1518533097007,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Australia",
        "Answerer_reputation_count":896.0,
        "Answerer_view_count":177.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was using the code <code>pd.read_json('s3:\/\/example2020\/kaggle.json')<\/code> to access S3 bucket data, but it threw the error of <code>FileNotFoundError: example2020\/kaggle.json<\/code>. <\/p>\n\n<p>The methods I tried:<\/p>\n\n<p><strong>[Region]<\/strong>\nThe s3 bucket is in Ohio region while the SageMaker notebook instance is in Singapore. Not sure if this matters. I tried to recreate a s3 bucket in Singapore region but I still cannot access it and got the same file not found error. <\/p>\n\n<p><strong>[IAM Role]<\/strong>\nI checked the permission of IAM-SageMaker Execution role\n<a href=\"https:\/\/i.stack.imgur.com\/st4AR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/st4AR.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1581359496750,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581359496750,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to access S3 bucket data using SageMaker. They are getting a \"FileNotFoundError\" error while using the code \"pd.read_json('s3:\/\/example2020\/kaggle.json')\". The user has tried recreating the S3 bucket in a different region and checked the permission of the IAM-SageMaker Execution role, but the issue persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60156370",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":10.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.0,
        "Challenge_title":"how SageMaker to access s3 bucket data",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1386.0,
        "Challenge_word_count":95,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518533097007,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Australia",
        "Poster_reputation_count":896.0,
        "Poster_view_count":177.0,
        "Solution_body":"<p>The problem is still IAM permission. <\/p>\n\n<p>I created a new notebook instance and a new IAM role. You would be asked how to access s3 bucket. I chose <code>all s3 bucket<\/code>. Then the problem solved. \n<a href=\"https:\/\/i.stack.imgur.com\/B0qOO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B0qOO.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><br>\n<br>\n<strong>[Solution]<\/strong>\nIn Resource tab, check whether bucket name is general.\n <a href=\"https:\/\/i.stack.imgur.com\/LL6Fw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LL6Fw.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If you changed old IAM and it is not working, you can create a new IAM role. And attach this role to the notebook.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":9.2,
        "Solution_reading_time":9.71,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":90.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":1.5463091667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Trying to use clearml-server on own Ubuntu 18.04.5 with SSH Port Forwarding and not beeing able to see my debug samples.<\/p>\n<p>My setup:<\/p>\n<ul>\n<li>ClearML server on hostA<\/li>\n<li>SSH Tunnel connections to access Web App from working machine via localhost:18080<\/li>\n<li>Web App: <code>ssh -N -L 18081:127.0.0.1:8081 user@hostA<\/code><\/li>\n<li>Fileserver: <code>ssh -N -L 18081:127.0.0.1:8081 user@hostA<\/code><\/li>\n<\/ul>\n<p>In Web App under Task-&gt;Results-&gt;Debug Samples the Images are still refrenced by localhost:8081<\/p>\n<p>Where can I set the fileserver URL to be localhost:18081 in Web App?\nI tried ~\/clearml.conf, but this did not work ( I think it is for my python script ).<\/p>",
        "Challenge_closed_time":1610390345336,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610384778623,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while using ClearML server on their Ubuntu 18.04.5 with SSH Port Forwarding. They are unable to see their debug samples in the Web App under Task->Results->Debug Samples as the images are still referenced by localhost:8081. The user is looking for a way to set the fileserver URL to be localhost:18081 in the Web App.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65671395",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":9.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.5463091667,
        "Challenge_title":"ClearML SSH port forwarding fileserver not available in WEB Ui",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":106,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604391794420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":89.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<p>In ClearML, debug images' URL is registered once they are uploaded to the fileserver. The WebApp doesn't actually decide on the URL for each debug image, but rather obtains it for each debug image from the server. This allows you to potentially upload debug images to a variety of storage targets, ClearML File Server simply being the most convenient, built-in option.<\/p>\n<p>So, the WebApp will always look for <code>localhost:8008<\/code> for debug images that have already been uploaded to the fileserver and contain <code>localhost:8080<\/code> in their URL.\nA possible solution is to simply add another tunnel in the form of <code>ssh -N -L 8081:127.0.0.1:8081 user@hostA<\/code>.<\/p>\n<p>For future experiments, you can choose to keep using <code>8081<\/code> (and keep using this new tunnel), or to change the default fileserver URL in <code>clearml.conf<\/code> to point to port <code>localhost:18081<\/code>, assuming you're running your experiments from the same machine where the tunnel to <code>18081<\/code> exists.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.1,
        "Solution_reading_time":13.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":159.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1361156607787,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bergen, Norway",
        "Answerer_reputation_count":6168.0,
        "Answerer_view_count":334.0,
        "Challenge_adjusted_solved_time":0.3461716667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an AWS SageMaker domain in my account created via Terraform. The resource was modified outside of Terraform. The modification was the equivalent of the following:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>aws sagemaker update-domain --domain-id d-domainid123 --default-user-settings '{&quot;KernelGatewayAppSettings&quot;: { &quot;CustomImages&quot;: [ { ... } ] } }'\n<\/code><\/pre>\n<p>Ever since, all <code>terraform plan<\/code> operations want to replace the AWS SageMaker domain:<\/p>\n<pre><code>  # module.main.aws_sagemaker_domain.default must be replaced\n-\/+ resource &quot;aws_sagemaker_domain&quot; &quot;default&quot; {\n      ~ arn                                            = &quot;arn:aws:sagemaker:eu-central-1:000111222333:domain\/d-domainid123&quot; -&gt; (known after apply)\n      ...\n        # (6 unchanged attributes hidden)\n      ~ default_user_settings {\n            # (2 unchanged attributes hidden)\n          - kernel_gateway_app_settings { # forces replacement\n               - custom_images = [ ... ]\n            }\n        }\n    }\n<\/code><\/pre>\n<p>My goal is to reconcile the situation without Terraform or me needing to create a new domain. I can't modify the Terraform sources to match the state of the SageMaker domain because that would force the recreation of domains in other accounts provisioned from the same Terraform source code.<\/p>\n<p><strong>I want to issue an <code>aws<\/code> CLI command that updates the domain and removes the <code>&quot;KernelGatewayAppSettings&quot;: { ... }<\/code> key completely from the <code>&quot;DefaultUserSettings&quot;<\/code> of the SageMaker domain. Is there a way to do this?<\/strong><\/p>\n<p>I tried the following, but the empty object is still there, so they did not work.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>aws sagemaker update-domain --domain-id d-domainid123 --default-user-settings '{&quot;KernelGatewayAppSettings&quot;: {} }'\naws sagemaker update-domain --domain-id d-domainid123 --default-user-settings '{&quot;KernelGatewayAppSettings&quot;: null }'\n\n# Still:\naws sagemaker describe-domain --domain-id d-domainid123\n{\n    &quot;DomainArn&quot;: ...,\n    &quot;DomainId&quot;: ...,\n    ...\n    &quot;DefaultUserSettings&quot;: {\n        &quot;ExecutionRole&quot;: &quot;arn:aws:iam::0001112233444:role\/SageMakerStudioExecutionRole&quot;,\n        &quot;SecurityGroups&quot;: [\n            &quot;...&quot;\n        ],\n        &quot;KernelGatewayAppSettings&quot;: {\n            &quot;CustomImages&quot;: []\n        }\n    },\n    ...\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1662394449848,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662393203630,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has an AWS SageMaker domain created via Terraform, which was modified outside of Terraform using the AWS Update API. As a result, all Terraform plan operations want to replace the AWS SageMaker domain. The user wants to issue an AWS CLI command that updates the domain and removes the \"KernelGatewayAppSettings\" key completely from the \"DefaultUserSettings\" of the SageMaker domain without needing to create a new domain or modify the Terraform sources. The user has tried updating the domain with an empty object or null value, but the empty object is still present.",
        "Challenge_last_edit_time":1662455073636,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73611956",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.8,
        "Challenge_reading_time":31.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":0.3461716667,
        "Challenge_title":"Remove JSON object via AWS Update* API to prevent Terraform from recreating the resource",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":32.0,
        "Challenge_word_count":223,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1219760368600,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Miskolc, Hungary",
        "Poster_reputation_count":3743.0,
        "Poster_view_count":178.0,
        "Solution_body":"<p>One option you have is to use the <a href=\"https:\/\/www.terraform.io\/language\/meta-arguments\/lifecycle\" rel=\"nofollow noreferrer\">lifecycle meta argument<\/a> to ignore out-of-band changes to the resource.<\/p>\n<pre><code>  lifecycle {\n    ignore_changes = [\n      default_user_settings\n    ]\n  }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.8,
        "Solution_reading_time":3.86,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1288196087392,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Oslo, Norge",
        "Answerer_reputation_count":1010.0,
        "Answerer_view_count":119.0,
        "Challenge_adjusted_solved_time":1.8016833333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Azure Data Lake Gen2 with public endpoint and a standard Azure ML instance.\nI have created both components with my user and I am listed as Contributor.<\/p>\n<p>I want to use data from this data lake in Azure ML.<\/p>\n<p>I have added the data lake as a Datastore using Service Principal authentication.<\/p>\n<p>I then try to create a Tabular Dataset using the Azure ML GUI I get the following error:<\/p>\n<p>Access denied\nYou do not have permission to the specified path or file.<\/p>\n<pre><code>{\n  &quot;message&quot;: &quot;ScriptExecutionException was caused by StreamAccessException.\\n  StreamAccessException was caused by AuthenticationException.\\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for '[REDACTED]' on storage failed with status code 'Forbidden' (This request is not authorized to perform this operation using this permission.), client request ID '1f9e329b-2c2c-49d6-a627-91828def284e', request ID '5ad0e715-a01f-0040-24cb-b887da000000'. Error message: [REDACTED]\\n&quot;\n}\n<\/code><\/pre>\n<p>I have tried having our Azure Portal Admin, with Admin access to both Azure ML and Data Lake try the same and she gets the same error.<\/p>\n<p>I tried creating the Dataset using Python sdk and get a similar error:<\/p>\n<pre><code>ExecutionError: \nError Code: ScriptExecution.StreamAccess.Authentication\nFailed Step: 667ddfcb-c7b1-47cf-b24a-6e090dab8947\nError Message: ScriptExecutionException was caused by StreamAccessException.\n  StreamAccessException was caused by AuthenticationException.\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for 'https:\/\/mydatalake.dfs.core.windows.net\/mycontainer?directory=mydirectory\/csv&amp;recursive=true&amp;resource=filesystem' on storage failed with status code 'Forbidden' (This request is not authorized to perform this operation using this permission.), client request ID 'a231f3e9-b32b-4173-b631-b9ed043fdfff', request ID 'c6a6f5fe-e01f-0008-3c86-b9b547000000'. Error message: {&quot;error&quot;:{&quot;code&quot;:&quot;AuthorizationPermissionMismatch&quot;,&quot;message&quot;:&quot;This request is not authorized to perform this operation using this permission.\\nRequestId:c6a6f5fe-e01f-0008-3c86-b9b547000000\\nTime:2020-11-13T06:34:01.4743177Z&quot;}}\n| session_id=75ed3c11-36de-48bf-8f7b-a0cd7dac4d58\n<\/code><\/pre>\n<p>I have created Datastore and Datasets of both a normal blob storage and a managed sql database with no issues and I have only contributor access to those so I cannot understand why I should not be Authorized to add data lake. The fact that our admin gets the same error leads me to believe there are some other issue.<\/p>\n<p>I hope you can help me identify what it is or give me some clue of what more to test.<\/p>\n<p>Edit:\nI see I might have duplicated this post: <a href=\"https:\/\/stackoverflow.com\/questions\/63891547\/how-to-connect-amls-to-adls-gen-2\">How to connect AMLS to ADLS Gen 2?<\/a>\nI will test that solution and close this post if it works<\/p>",
        "Challenge_closed_time":1605260557303,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605250166337,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an AuthenticationException error when trying to create a Tabular Dataset in Azure ML using data from an Azure Data Lake Gen2 Datastore. The user has added the data lake as a Datastore using Service Principal authentication and has tried creating the Dataset using both the Azure ML GUI and Python SDK, but still gets the same error. The user has contributor access to other Datastores but cannot understand why they are not authorized to add data from the data lake. The user is seeking help to identify the issue or get some clues on what to test.",
        "Challenge_last_edit_time":1605254071243,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64816630",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":39.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":2.8863794445,
        "Challenge_title":"AuthenticationException when creating Azure ML Dataset from Azure Data Lake Gen2 Datastore",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":3179.0,
        "Challenge_word_count":360,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1288196087392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Oslo, Norge",
        "Poster_reputation_count":1010.0,
        "Poster_view_count":119.0,
        "Solution_body":"<p>This was actually a duplicate of <a href=\"https:\/\/stackoverflow.com\/questions\/63891547\/how-to-connect-amls-to-adls-gen-2\">How to connect AMLS to ADLS Gen 2?<\/a>.<\/p>\n<p>The solution is to give the service principal that Azure ML uses to access the data lake the Storage Blob Data Reader access. And note you have to wait at least some minutes for this to have effect.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.5,
        "Solution_reading_time":4.73,
        "Solution_score_count":4.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":53.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1411212343683,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, BC, Canada",
        "Answerer_reputation_count":3592.0,
        "Answerer_view_count":268.0,
        "Challenge_adjusted_solved_time":1.5132372222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've uploaded my own Jupyter notebook to Sagemaker, and am trying to create an iterator for my training \/ validation data which is in S3, as follow:<\/p>\n\n<pre><code>train = mx.io.ImageRecordIter(\n        path_imgrec         = \u2018s3:\/\/bucket-name\/train.rec\u2019 \u2026\u2026 )\n<\/code><\/pre>\n\n<p>I receive the following exception: <\/p>\n\n<pre><code>MXNetError: [04:33:32] src\/io\/s3_filesys.cc:899: Need to set enviroment variable AWS_SECRET_ACCESS_KEY to use S3\n<\/code><\/pre>\n\n<p>I've checked that the IAM role attached with this notebook instance has S3 access. Any clues on what might be needed to fix this?<\/p>",
        "Challenge_closed_time":1522305475727,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522300028073,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is trying to create an iterator for their training\/validation data in S3 using AWS Sagemaker, but is encountering an exception that requires setting the AWS_SECRET_ACCESS_KEY environment variable to use S3. The user has checked that the IAM role attached to the notebook instance has S3 access and is seeking help to fix the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49548422",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":7.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.5132372222,
        "Challenge_title":"Training data in S3 in AWS Sagemaker",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1353.0,
        "Challenge_word_count":80,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449031669083,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":777.0,
        "Poster_view_count":103.0,
        "Solution_body":"<p>If your IAM roles are setup correctly, then you need to download the file to the Sagemaker instance first and then work on it. Here's how:<\/p>\n\n<pre><code># Import roles\nimport sagemaker\nrole = sagemaker.get_execution_role()\n\n# Download file locally\ns3 = boto3.resource('s3')\ns3.Bucket(bucket).download_file('your_training_s3_file.rec', 'training.rec')\n\n#Access locally\ntrain = mx.io.ImageRecordIter(path_imgrec=\u2018training.rec\u2019 \u2026\u2026 )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.6,
        "Solution_reading_time":5.8,
        "Solution_score_count":4.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1351080779276,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Leuven, Belgium",
        "Answerer_reputation_count":3126.0,
        "Answerer_view_count":262.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>My training data looks like <\/p>\n\n<pre><code>df = pd.DataFrame({'A' : [2, 5], 'B' : [1, 7]})\n<\/code><\/pre>\n\n<p>I have trained a model in AWS Sagemaker and I deployed the model behind an endpoint.\nThe endpoint accepts the payload as \"text\/csv\".<\/p>\n\n<p>to invoke the endpoint using boto3 you can do:<\/p>\n\n<pre><code>import boto3\nclient = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(\n    EndpointName=\"my-sagemaker-endpoint-name\",\n    Body= my_payload_as_csv,\n    ContentType = 'text\/csv')\n<\/code><\/pre>\n\n<p>How do i construct the payload \"my_payload_as_csv\" from my Dataframe in order to invoke the Sagemaker Endpoint correctly?<\/p>",
        "Challenge_closed_time":1590329636843,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590329636843,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to construct a \"text\/csv\" payload from a pandas DataFrame in order to invoke a Sagemaker endpoint correctly using boto3. The user has trained a model in AWS Sagemaker and deployed it behind an endpoint that accepts the payload as \"text\/csv\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61987233",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":9.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.0,
        "Challenge_title":"How to construct a \"text\/csv\" payload when invoking a sagemaker endpoint",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":2627.0,
        "Challenge_word_count":86,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351080779276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Leuven, Belgium",
        "Poster_reputation_count":3126.0,
        "Poster_view_count":262.0,
        "Solution_body":"<p>if you start from the dataframe example<\/p>\n\n<pre><code>df = pd.DataFrame({'A' : [2, 5], 'B' : [1, 7]})\n<\/code><\/pre>\n\n<p>you take a row<\/p>\n\n<pre><code>df_1_record = df[:1]\n<\/code><\/pre>\n\n<p>and convert <code>df_1_record<\/code> to a csv like this:<\/p>\n\n<pre><code>import io\nfrom io import StringIO\ncsv_file = io.StringIO()\n# by default sagemaker expects comma seperated\ndf_1_record.to_csv(csv_file, sep=\",\", header=False, index=False)\nmy_payload_as_csv = csv_file.getvalue()\n<\/code><\/pre>\n\n<p><code>my_payload_as_csv<\/code> looks like<\/p>\n\n<pre><code>'2,1\\n'\n<\/code><\/pre>\n\n<p>then you can invoke the sagemaker endpoint<\/p>\n\n<pre><code>import boto3\nclient = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(\n    EndpointName=\"my-sagemaker-endpoint-name\",\n    Body= my_payload_as_csv,\n    ContentType = 'text\/csv')\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.2,
        "Solution_reading_time":10.94,
        "Solution_score_count":7.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4325,
        "Challenge_answer_count":0,
        "Challenge_body":"",
        "Challenge_closed_time":1638706309000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638704752000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to use \"dvc pull\" in the result branch due to an error message that says \"rmdir: data: Das Ger\u00e4t oder die Ressource ist belegt\" when trying to view the results after successfully completing a job. The issue is related to the sshfs connection that is mounted to use external data. The user has to unmount the \"data\" folder using fusermount -u data before viewing the results and then mount it again to start a new job.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/se4ai2122-cs-uniba\/CT-COVID\/issues\/30",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":5.2,
        "Challenge_reading_time":0.66,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":66.0,
        "Challenge_repo_star_count":1.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":0.4325,
        "Challenge_title":"Missing params field for evaluate stage in dvc.yaml",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":8,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2032822222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi,   <br \/>\nWe have resource group(RG1) created in Germany region. Resource created are   <\/p>\n<ol>\n<li> Vnet  <\/li>\n<li> VPN gateway  <\/li>\n<li> Disks - HDD  <\/li>\n<li> VM (database installed)  <\/li>\n<\/ol>\n<p>We wanted to use  Azure machine learning service to read data from database(present in resource group RG1 in German region). We later found that azure machine learning service is not available in Germany region and in order to create end points for Azure ML service both the networks(vnets of the DB and Azure ML ) should be in the same region.   <\/p>\n<p>So we tried migrating the resource and resource group (RG1) from German region to west Europe by using wizard on the portal. But we got prompted that disk , vnet and vpn gateway cannot be moved to different region ) .   <\/p>\n<p>Is there anyway we could move them ? or any alternate solutions ? .Else, we would end up recreating every thing in westeurope which i would like to avoid.   <\/p>\n<p>Regards,   <br \/>\nSuman <\/p>",
        "Challenge_closed_time":1606984258243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606983526427,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has a resource group in the Germany region with various resources including a VM with a database. They want to use Azure machine learning service to read data from the database but found out that the service is not available in Germany. They tried to migrate the resource group to the West Europe region but were unable to move the disks, Vnet, and VPN gateway. The user is seeking an alternative solution to avoid recreating everything in West Europe.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/184565\/moving-resources-across-the-regions",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.5,
        "Challenge_reading_time":12.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.2032822222,
        "Challenge_title":"Moving resources across the regions",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":168,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Maybe this is helpful:    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/site-recovery\/azure-to-azure-tutorial-migrate\">https:\/\/learn.microsoft.com\/en-us\/azure\/site-recovery\/azure-to-azure-tutorial-migrate<\/a>    <\/p>\n<p>----------    <\/p>\n<p>(If the reply was helpful please don't forget to <strong>upvote<\/strong> and\/or <strong>accept as answer<\/strong>, thank you)    <\/p>\n<p>Regards    <br \/>\n Andreas Baumgarten    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.6,
        "Solution_reading_time":5.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1631803441500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"mexico",
        "Answerer_reputation_count":1258.0,
        "Answerer_view_count":685.0,
        "Challenge_adjusted_solved_time":28.3038852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been using <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/08-model-monitoring.ipynb\" rel=\"nofollow noreferrer\">this<\/a> example of creating a Vertex AI monitoring job. It sends an email.<\/p>\n<pre><code>alerting_config = vertex_ai_beta.ModelMonitoringAlertConfig(\nemail_alert_config=vertex_ai_beta.ModelMonitoringAlertConfig.EmailAlertConfig(\n    user_emails=NOTIFY_EMAILS\n)\n<\/code><\/pre>\n<p>Is there any way to instead send a Pubsub message?<\/p>",
        "Challenge_closed_time":1660246628340,
        "Challenge_comment_count":1,
        "Challenge_created_time":1660144734353,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using an example of creating a Vertex AI monitoring job that sends an email. They are looking for a way to send a Pubsub message instead of an email.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73308825",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":18.1,
        "Challenge_reading_time":7.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":28.3038852778,
        "Challenge_title":"Can Vertex AI model monitoring job send a pubsub message instead of email?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":86.0,
        "Challenge_word_count":47,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>You can configure the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/using-model-monitoring#set-up-alerts\" rel=\"nofollow noreferrer\">alert to be sent to Cloud Logging<\/a>. To enable Cloud Logging alerts you have to set the <code>enableLogging<\/code> field on your <code>ModelMonitoringAlertConfig<\/code> configuration to <code>TRUE<\/code>. Then you can forward the logs to any service that Cloud Logging supports, Pub\/Sub is one of these.<\/p>\n<p>For this you\u2019ll need one of the following permissions:<\/p>\n<ul>\n<li>Owner (roles\/owner)<\/li>\n<li>Logging Admin (roles\/logging.admin)<\/li>\n<li>Logs Configuration Writer (roles\/logging.configWriter)<\/li>\n<\/ul>\n<p>Then you need to <a href=\"https:\/\/cloud.google.com\/logging\/docs\/export\/configure_export_v2#creating_sink\" rel=\"nofollow noreferrer\">create a sink<\/a>.<\/p>\n<p>After that you have created the sink you\u2019ll need to set the <a href=\"https:\/\/cloud.google.com\/logging\/docs\/export\/configure_export_v2#dest-auth\" rel=\"nofollow noreferrer\">destination permissions<\/a>.<\/p>\n<p>While Cloud Logging provides you with the ability to exclude logs from being ingested, you might want to consider keeping logs that help with supportability. Using these logs can help you quickly troubleshoot and identify issues with your applications.<\/p>\n<p>Logs routed to Pub\/Sub are generally available within seconds, with 99% of logs available in less than 60 seconds.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.7,
        "Solution_reading_time":18.63,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":155.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1517578984080,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2090.0,
        "Answerer_view_count":163.0,
        "Challenge_adjusted_solved_time":7.2704063889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have a .pkl which I would like to put into production. I would like to do a daily query of my SQL server and do a prediction on about 1000 rows. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">documentation<\/a> implies I have to load the daily data into s3. Is there a way around this? It should be able to fit in memory no problem. <\/p>\n\n<p>The answer to \" <a href=\"https:\/\/stackoverflow.com\/questions\/48319893\/is-there-some-kind-of-persistent-local-storage-in-aws-sagemaker-model-training\">is there some kind of persistent local storage in aws sagemaker model training?<\/a> \" says that \"<em>The notebook instance is coming with a local EBS (5GB) that you can use to copy some data into it and run the fast development iterations without copying the data every time from S3.<\/em>\" The 5GB could be enough but I am not sure you can actually run from a notebook in this manner. If I set up a VPN could I just query using pyodbc?<\/p>\n\n<p>Is there sagemaker integration with AWS Lambda? That in combination with a docker container would suit my needs.<\/p>",
        "Challenge_closed_time":1531919252070,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531871216253,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a .pkl file that they want to put into production for daily predictions on about 1000 rows from their SQL server. The documentation implies that the daily data needs to be loaded into s3, but the user wants to know if there is a way around this since the data can fit in memory. They are considering using a local EBS or setting up a VPN to query using pyodbc. They are also wondering if there is sagemaker integration with AWS Lambda and a docker container that would suit their needs.",
        "Challenge_last_edit_time":1531893078607,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51391639",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":14.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":13.3432825,
        "Challenge_title":"Is it possible to predict in sagemaker without using s3",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1576.0,
        "Challenge_word_count":176,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400315847156,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4342.0,
        "Poster_view_count":315.0,
        "Solution_body":"<p>While you need to to specify a s3 \"folder\" as input, this folder can contain only a dummy file. \nAlso if you bring your own docker container for training like in <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">this example<\/a>, you can do pretty much everthing in it. So you could do your daily query inside your docker container, because they have access to the internet. <\/p>\n\n<p>Also inside this container you have access to all the other aws services. Your access is defined by the role you're passing to your training job.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":7.96,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":91.9480555556,
        "Challenge_answer_count":0,
        "Challenge_body":"'m going through this notebook: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb\r\n\r\nI need to start the training using the docker image from my local registry. I provided all required data in the environment I created:\r\n\r\nconda_env.docker.enabled = True\r\nconda_env.docker.base_image = \"tf_od_api:latest\"\r\nconda_env.docker.base_image_registry.address = \"mylocalacr.azurecr.io\"\r\nconda_env.docker.base_image_registry.username = \"MyToken\"\r\nconda_env.docker.base_image_registry.password = \"MyPassword\"\r\n\r\nconda_env.python.user_managed_dependencies = True\r\n\r\nsrc = ScriptRunConfig(source_directory='azureml-examples\/workflows\/train\/fastai\/pets\/src',\r\n                      script='aml_wrapper.py',\r\n                      compute_target=attached_dsvm_compute,\r\n                      environment=conda_env)\r\nrun = exp.submit(config=src)\r\nrun.wait_for_completion(show_output=True)\r\n\r\nAnd when I start the pipeline I got: \"FailedPullingImage: Unable to pull docker image\\n\\timageName: Run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"\r\n\r\nIf I set conda_env.python.user_managed_dependencies = False\r\n\r\nthen the pipeline can pull my image from my local registry, build a new image with all required python dependencies on top of my base image and push the new image to my local registry. But on the second step of the pipeline, when it tries to pull the image for running it, that was just created and pushed, it again crashes with the same error: \"Run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"",
        "Challenge_closed_time":1614604742000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1614273729000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"the user encountered a challenge when attempting to upload data to a datastore, resulting in an \"authorization permission mismatch\" error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1371",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.6,
        "Challenge_reading_time":21.99,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":91.9480555556,
        "Challenge_title":"Azure ML Run docker command to pull public image failed ",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":179,
        "Discussion_body":"please try this \r\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1247#issuecomment-738887772 Seems like it solved the issue. Thanks!",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1308292969430,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":651.0,
        "Answerer_view_count":99.0,
        "Challenge_adjusted_solved_time":2443.6834486111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We're currently working with 3 employees in the same notebook-instance, however, since this is a shared workspace this makes version management extra difficult. Is it possible to link aws credentials to your git account from within SageMaker? Or are there any other ways recommended for version management? <\/p>\n\n<p>Right now we're using a single git account for committing the code from within jupyter terminal. <\/p>",
        "Challenge_closed_time":1543573965692,
        "Challenge_comment_count":0,
        "Challenge_created_time":1534776705277,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with version management in SageMaker while working with three employees in the same notebook-instance. They are looking for ways to link AWS credentials to their git account or any other recommended methods for version management. Currently, they are using a single git account for committing the code from within the Jupyter terminal.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51933366",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":5.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2443.6834486111,
        "Challenge_title":"Version management in SageMaker",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":836.0,
        "Challenge_word_count":68,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1534775846323,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The situation has changed : Git is now <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility\/\" rel=\"nofollow noreferrer\">available<\/a> in SageMaker<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":40.0,
        "Solution_reading_time":3.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416648155470,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":14749.0,
        "Answerer_view_count":968.0,
        "Challenge_adjusted_solved_time":76.2321219444,
        "Challenge_answer_count":1,
        "Challenge_body":"<h1>Question<\/h1>\n<p>Please advise how to trouble shoot the problem.<\/p>\n<h1>Problem<\/h1>\n<p>Cannot access the RedShift cluster endpoint from the SageMaker studio instance.<\/p>\n<pre><code>import socket\nsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nresult = sock.connect_ex(('dsoaws.cw7xniw3gvef.us-east-2.redshift.amazonaws.com',5439))\nif result == 0:\n   print(&quot;Port is open&quot;)\nelse:\n   print(&quot;Port is not open&quot;)\nsock.close()\n---\n\nPort is not open\n<\/code><\/pre>\n<h1>RedShift Cluster<\/h1>\n<p>Endpoint is <code>dsoaws.cw7xniw3gvef.us-east-2.redshift.amazonaws.com:5439\/dsoaws<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/t4w92.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t4w92.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The network setting shows the VPC is vpc-5b123432 allowing access from sg-56cb133e.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/nu5kM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/nu5kM.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<hr \/>\n<h1>SageMaker Studio<\/h1>\n<p>The SageMaker Studio instance is in the save VPC vpc-5b123432. However, not sure if sg-56cb133e is actually attached to the SageMaker studio instance. Please advise how to confirm if the sg-56cb133e is attached to the instance.<\/p>\n<pre><code>import json\nimport boto3\nfrom botocore.exceptions import ClientError\nfrom botocore.config import Config\n\nconfig = Config(\n   retries = {\n      'max_attempts': 10,\n      'mode': 'adaptive'\n   }\n)\n\n\niam = boto3.client('iam', config=config)\nsts = boto3.client('sts')\nredshift = boto3.client('redshift')\nsm = boto3.client('sagemaker')\nec2 = boto3.client('ec2')\n\ntry:\n    domain_id = sm.list_domains()['Domains'][0]['DomainId'] #['NotebookInstances'][0]['NotebookInstanceName']\n    describe_domain_response = sm.describe_domain(DomainId=domain_id)\n    vpc_id = describe_domain_response['VpcId']\n    print(vpc_id)\n    security_groups = ec2.describe_security_groups()['SecurityGroups']\n    for security_group in security_groups:\n        if vpc_id == security_group['VpcId']:\n            security_group_id = security_group['GroupId']\n    print(security_group_id)\nexcept:\n    pass\n-----\nvpc-5b123432\nsg-56cb133e\n<\/code><\/pre>\n<hr \/>\n<h1>Security Group<\/h1>\n<p><a href=\"https:\/\/i.stack.imgur.com\/JgCgi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JgCgi.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gjmLh.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gjmLh.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h1>IAM<\/h1>\n<p>The IAM role <code>SageMaker<\/code> is attached to the SageMaker Studio.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oY9ix.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oY9ix.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1628118669360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1627882823273,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to access the RedShift cluster endpoint from the SageMaker Studio instance. The network settings show that the VPC is allowing access from the security group, but it is unclear if the security group is attached to the SageMaker Studio instance. The IAM role \"SageMaker\" is attached to the SageMaker Studio.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68616817",
        "Challenge_link_count":10,
        "Challenge_participation_count":1,
        "Challenge_readability":15.4,
        "Challenge_reading_time":38.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":65.5128019444,
        "Challenge_title":"AWS - Cannot access RedShift endpoint from the SageMaker Studio",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":434.0,
        "Challenge_word_count":229,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416648155470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":14749.0,
        "Poster_view_count":968.0,
        "Solution_body":"<h1>Cause<\/h1>\n<p>Did not use VPC Only sagemaker deployment as having used the Quick Start onboard.<\/p>\n<h1>Fix<\/h1>\n<ol>\n<li>Deleted the SageMaker Studio. R<\/li>\n<li>Recreated by using the Standard Setup + VPC only\n<a href=\"https:\/\/i.stack.imgur.com\/zZ7be.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zZ7be.png\" alt=\"enter image description here\" \/><\/a><\/li>\n<li>Added the NAT and configured the routing tables.<\/li>\n<\/ol>\n<h1>References<\/h1>\n<ul>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/securing-amazon-sagemaker-studio-connectivity-using-a-private-vpc\/\" rel=\"nofollow noreferrer\">Securing Amazon SageMaker Studio connectivity using a private VPC<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-studio-vpc-networkfirewall\" rel=\"nofollow noreferrer\">Amazon SageMaker Studio in a private VPC with NAT Gateway and Network Firewall<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1628157258912,
        "Solution_link_count":4.0,
        "Solution_readability":17.8,
        "Solution_reading_time":12.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1587438027383,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":7334.0,
        "Answerer_view_count":674.0,
        "Challenge_adjusted_solved_time":1.7229975,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to register a dataset from ADLS Gen2 in my Azure Machine Learning workspace (<code>azureml-core==1.12.0<\/code>). Given that service principal information is not required in the Python SDK <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py#register-azure-data-lake-gen2-workspace--datastore-name--filesystem--account-name--tenant-id-none--client-id-none--client-secret-none--resource-url-none--authority-url-none--protocol-none--endpoint-none--overwrite-false-\" rel=\"noreferrer\">documentation<\/a> for <code>.register_azure_data_lake_gen2()<\/code>, I successfully used the following code to register ADLS gen2 as a datastore:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Datastore\n\nadlsgen2_datastore_name = os.environ['adlsgen2_datastore_name']\naccount_name=os.environ['account_name'] # ADLS Gen2 account name\nfile_system=os.environ['filesystem']\n\nadlsgen2_datastore = Datastore.register_azure_data_lake_gen2(\n    workspace=ws,\n    datastore_name=adlsgen2_datastore_name,\n    account_name=account_name, \n    filesystem=file_system\n)\n<\/code><\/pre>\n<p>However, when I try to register a dataset, using<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Dataset\nadls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)\ndata = Dataset.Tabular.from_delimited_files((adls_ds, 'folder\/data.csv'))\n<\/code><\/pre>\n<p>I get an error<\/p>\n<blockquote>\n<p>Cannot load any data from the specified path. Make sure the path is accessible and contains data.\n<code>ScriptExecutionException<\/code> was caused by <code>StreamAccessException<\/code>.\nStreamAccessException was caused by AuthenticationException.\n<code>'AdlsGen2-ReadHeaders'<\/code> for '[REDACTED]' on storage failed with status code 'Forbidden' (This request is not authorized to perform this operation using this permission.), client request ID &lt;CLIENT_REQUEST_ID&gt;, request ID &lt;REQUEST_ID&gt;. Error message: [REDACTED]\n| session_id=&lt;SESSION_ID&gt;<\/p>\n<\/blockquote>\n<p>Do I need the to enable the service principal to get this to work? Using the ML Studio UI, it appears that the service principal is required even to register the datastore.<\/p>\n<p>Another issue I noticed is that AMLS is trying to access the dataset here:\n<code>https:\/\/adls_gen2_account_name.**dfs**.core.windows.net\/container\/folder\/data.csv<\/code> whereas the actual URI in ADLS Gen2 is: <code>https:\/\/adls_gen2_account_name.**blob**.core.windows.net\/container\/folder\/data.csv<\/code><\/p>",
        "Challenge_closed_time":1600155716360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600115991930,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to register a dataset from ADLS Gen2 in their Azure Machine Learning workspace using Python SDK. They successfully registered ADLS Gen2 as a datastore but encountered an error when trying to register a dataset. The error message suggests an authentication issue, and the user is unsure if they need to enable the service principal. Additionally, the user noticed that AMLS is trying to access the dataset using the wrong URI.",
        "Challenge_last_edit_time":1600160631356,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63891547",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":18.0,
        "Challenge_reading_time":34.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":11.0345638889,
        "Challenge_title":"How to connect AMLS to ADLS Gen 2?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":3331.0,
        "Challenge_word_count":219,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589738451347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":179.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>According to this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-data-lake-storage-generation-2\" rel=\"noreferrer\">documentation<\/a>,you need to enable the service principal.<\/p>\n<p>1.you need to register your application and grant the service principal with <strong>Storage Blob Data Reader access<\/strong>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/FZl8O.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FZl8O.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>2.try this code:<\/p>\n<pre><code>adlsgen2_datastore = Datastore.register_azure_data_lake_gen2(workspace=ws,\n                                                             datastore_name=adlsgen2_datastore_name,\n                                                             account_name=account_name,\n                                                             filesystem=file_system,\n                                                             tenant_id=tenant_id,\n                                                             client_id=client_id,\n                                                             client_secret=client_secret\n                                                             )\n\nadls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)\ndataset = Dataset.Tabular.from_delimited_files((adls_ds,'sample.csv'))\nprint(dataset.to_pandas_dataframe())\n<\/code><\/pre>\n<p><strong>Result:<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/50mit.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/50mit.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1600166834147,
        "Solution_link_count":5.0,
        "Solution_readability":25.5,
        "Solution_reading_time":16.17,
        "Solution_score_count":9.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":20.56922,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Bunch of secrets being automatically created in key vault that is integrated into Azure ML workspace(s). These secrets seem to be generated by the ML resource\/service itself and continues to generate new secrets.<\/p>\n<p>\u00a0Can you please help with the document for these secrets?<\/p>",
        "Challenge_closed_time":1682931416008,
        "Challenge_comment_count":1,
        "Challenge_created_time":1682857366816,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where a large number of secrets are being automatically created in their key vault, which is integrated into Azure ML workspaces. These secrets are being generated by the ML resource\/service itself and continue to be generated. The user is seeking help with documentation for these secrets.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1270782\/secrets-automatically-created-in-key-vault",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.4,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":20.56922,
        "Challenge_title":"Secrets automatically created in key vault",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":49,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@<a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=dfa9d536-725c-462d-87c8-47fbafb1a2bc\">D-0887<\/a> Thanks, When you perform operations in Azure ML that require secret values to be stored like creating connections, datastores, or workspace management operations, the key vault instance associated to the workspace is used to store those secrets.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":22.0,
        "Solution_reading_time":4.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":51.3389041667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to transfer a generated csv file <code>test_df.csv<\/code> from my Azure ML notebook folder which has a path <code>\/Users\/Ankit19.Gupta\/test_df.csv<\/code> to a datastore which has a web path <code>https:\/\/abc.blob.core.windows.net\/azureml\/LocalUpload\/f3db18b6<\/code>. I have written the python code as<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n    \ndatastore.upload_files('\/Users\/Ankit19.Gupta\/test_df.csv',\n                  target_path='https:\/\/abc.blob.core.windows.net\/azureml\/LocalUpload\/f3db18b6',\n                  overwrite=True)\n<\/code><\/pre>\n<p>But it is showing the following error message:<\/p>\n<pre><code>UserErrorException: UserErrorException:\n    Message: '\/' does not point to a file. Please upload the file to cloud first if running in a cloud notebook.\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;'\/' does not point to a file. Please upload the file to cloud first if running in a cloud notebook.&quot;\n    }\n}\n<\/code><\/pre>\n<p>I have tried <a href=\"https:\/\/stackoverflow.com\/questions\/67897947\/how-to-transfer-data-from-azure-ml-notebooks-to-a-storage-container\">this<\/a> but it is not working for me. Can anyone please help me to resolve this issue. Any help would be appreciated.<\/p>",
        "Challenge_closed_time":1663658453172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663473633117,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to transfer a generated csv file from their Azure ML notebook folder to a datastore using Python code. However, they are encountering an error message stating that '\/' does not point to a file and to upload the file to the cloud first if running in a cloud notebook. The user has tried a solution from Stack Overflow but it did not work. They are seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73760033",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":18.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":51.3389041667,
        "Challenge_title":"How to transfer a csv file from notebook folder to a datastore",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":43.0,
        "Challenge_word_count":143,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540154634483,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":237.0,
        "Poster_view_count":173.0,
        "Solution_body":"<p>The way the path was mentioned is not accurate. The datastore path will be different manner.\nReplace the below code for the small change in the calling path.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4o1WU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4o1WU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n    \ndatastore.upload_files('.\/Users\/foldername\/filename.csv',\n                  target_path=\u2019your targetfolder',\n                  overwrite=True)\n<\/code><\/pre>\n<p>We need to call all the parent folders before the folder.  <strong><code>\u201c.\/\u201d<\/code><\/strong> is the way we can call the dataset from datastore.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.9,
        "Solution_reading_time":9.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1550902509267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2669.0,
        "Answerer_view_count":3292.0,
        "Challenge_adjusted_solved_time":17.0441908334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I had configured my databricks workspace in local using,<\/p>\n<p><code>databricks configure --profile &lt;profile_name&gt; --token<\/code><\/p>\n<p>by which I am able to list the clusters and create secret scope.<\/p>\n<p>But I am unable to create mlflow experiments. I had set the tracking uri to &quot;databricks&quot; and also tested with &quot;databricks\/&lt;profile_name&quot; and tested but i am unable to create or track any experiments on my databricks workspace.<\/p>\n<p>I get this following error;<\/p>\n<p><code>from mlflow.tracking import MlflowClient client = MlflowClient() mlflow.set_tracking_uri(&quot;databricks&quot;) experiment =  client.get_experiment_by_name('\/Shared\/test')<\/code><\/p>\n<p>MlflowException: API request to endpoint was successful but the response body was not in a valid JSON format. Response body: '&lt;!doctype html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;\/&gt;&lt;meta http-equiv=&quot;Content-Language&quot; content=&quot;en&quot;\/&gt;&lt;title&gt;Databricks - Sign In&lt;\/title&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=960&quot;\/&gt;&lt;link rel=&quot;icon&quot; type=&quot;image\/png&quot; href=&quot;\/favicon.ico&quot;\/&gt;&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text\/html; charset=UTF8&quot;\/&gt;&lt;link rel=&quot;icon&quot; href=&quot;\/favicon.ico&quot;&gt;&lt;script defer=&quot;defer&quot; src=&quot;\/login\/login.0ceb14c0.js&quot;&gt;&lt;\/script&gt;&lt;\/head&gt;&lt;body class=&quot;light-mode&quot;&gt;&lt;uses-legacy-bootstrap&gt;&lt;div id=&quot;login-page&quot;&gt;&lt;\/div&gt;&lt;\/uses-legacy-bootstrap&gt;&lt;\/body&gt;&lt;\/html&gt;'<\/p>\n<p>Could someone help me on what I am missing here?<\/p>\n<p>I am expecting to create\/track mlflow experiements in databricks workspace via dev-tools(vscode).<\/p>",
        "Challenge_closed_time":1649225844710,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649164485623,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has configured their Databricks workspace in local using a token and is able to list clusters and create secret scope, but they are unable to create MLflow experiments. They have set the tracking URI to \"databricks\" and tested with \"databricks\/<profile_name>\" but are unable to create or track any experiments on their Databricks workspace. They are getting an error message that the response body was not in a valid JSON format. The user is seeking help to create\/track MLflow experiments in Databricks workspace via dev-tools (VSCode).",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71752458",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.7,
        "Challenge_reading_time":25.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":17.0441908334,
        "Challenge_title":"MLflow Experiments Tracking : local (dev tools - vscode) to databricks workspace",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":220.0,
        "Challenge_word_count":150,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1515569029903,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":5.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>I had the same problem while trying to load a model from model registry with mismatching versions (client 1.22.0).<\/p>\n<p>I had to downgrade the client version to make it work.<\/p>\n<p>Downgraded first the client to 1.21 and then server to 1.20<\/p>\n<p>Refer - <a href=\"https:\/\/docs.databricks.com\/dev-tools\/api\/latest\/mlflow.html#operation\/transition-model-version-stage\" rel=\"nofollow noreferrer\">https:\/\/docs.databricks.com\/dev-tools\/api\/latest\/mlflow.html#operation\/transition-model-version-stage<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.4,
        "Solution_reading_time":6.8,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":46.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1522634496792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Rio de Janeiro, RJ, Brasil",
        "Answerer_reputation_count":264.0,
        "Answerer_view_count":23.0,
        "Challenge_adjusted_solved_time":26.5148555556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have many experiment, like:<\/p>\n<p><img src=\"https:\/\/user-images.githubusercontent.com\/40580910\/95883598-82a07d00-0d51-11eb-847d-872452f6caa4.png\" alt=\"image\" \/><\/p>\n<p>and now, i want load an experiment<\/p>\n<pre><code>#%% sumonando os pacotes e verificando azureml.core\nimport azureml.core\nimport pandas as pd\nimport numpy as np\nimport logging\n\nprint(&quot;AzureML SDK Version: &quot;, azureml.core.VERSION)\n\n#%% Conectando ao azure e crinado o exparimento\n\nfrom azureml.core import Workspace, Experiment\n\nws = Workspace.from_config() \nprint(Experiment.list(ws))\n#%%\nExperiment = Experiment.from_directory('teste2-Monitor-Runs') `\n<\/code><\/pre>\n<p>but<\/p>\n<pre><code>&quot;error&quot;: {\n    &quot;message&quot;: &quot;No cache found for current project, try providing resource group and workspace \narguments&quot;\n}`\n<\/code><\/pre>\n<hr \/>\n<p>Content: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.experiment(class)?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.core.Experiment class - Azure Machine Learning Python<\/a><\/p>",
        "Challenge_closed_time":1602700300720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602604847240,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to load an experiment in AzureML using Python code, but is encountering an error message stating that no cache was found for the current project and suggesting to provide resource group and workspace arguments. The user has shared the Python code used for connecting to Azure and creating the experiment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64338898",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.8,
        "Challenge_reading_time":14.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":26.5148555556,
        "Challenge_title":"How to load an experiment in azureml?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":254.0,
        "Challenge_word_count":92,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522634496792,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rio de Janeiro, RJ, Brasil",
        "Poster_reputation_count":264.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>I believe it is that way.<\/p>\n<pre><code>from azureml.core import Experiment, Workspace\nExperiment = ws.experiments[&quot;teste2-Monitor-Runs&quot;]\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.7,
        "Solution_reading_time":2.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":14.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2942933334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Unit 4 of 7  <br \/>\nExercise - Back up an Azure virtual machine  <br \/>\nCreate a backup for Azure virtual machines  <\/p>\n<p>I am unable to run the following command in cloud shell to set up the environment:  <br \/>\nRGROUP=$(az group create --name vmbackups --location westus2 --output tsv --query name)  <\/p>\n<p>Following error pop up:  <br \/>\nERROR: (AuthorizationFailed) The client 'live.com#...... does not have authorization to perform action 'Microsoft.Resources\/subscriptions\/resourcegroups\/write' over scope '\/subscriptions\/.......\/resourcegroups\/vmbackups' or the scope is invalid. If access was recently granted, please refresh your credentials.  <br \/>\nCode: AuthorizationFailed  <br \/>\nMessage: The client 'live.com#l...... with object id '.......' does not have authorization to perform action 'Microsoft.Resources\/subscriptions\/resourcegroups\/write' over scope '\/subscriptions\/.....\/resourcegroups\/vmbackups' or the scope is invalid. If access was recently granted, please refresh your credentials.  <\/p>\n<p>When I do refresh, sign out or sign in do not helps. Anybody has any idea what to do?  <br \/>\nThank you  <\/p>",
        "Challenge_closed_time":1651918064643,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651917005187,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an \"AuthorizationFailed\" error while trying to run a command to set up the environment for creating a backup for Azure virtual machines. The error message suggests that the user's credentials do not have the necessary authorization to perform the required action. The user has tried refreshing their credentials, signing out and signing in, but the issue persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/840311\/code-authorizationfailed",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":14.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.2942933334,
        "Challenge_title":"Code: AuthorizationFailed",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":146,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=854a8d3c-feda-48d5-ba7c-744d587335c9\">@Krisztian  <\/a>     <\/p>\n<p>Welcome to Microsoft Q&amp;A community.     <\/p>\n<p>Have you tried to do this first?    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/199884-image.png?platform=QnA\" alt=\"199884-image.png\" \/>    <\/p>\n<p>Cheers,    <\/p>\n<p>Please &quot;Accept the answer&quot; if the information helped you. This will help us and others in the community as well.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":6.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":31.5278255556,
        "Challenge_answer_count":1,
        "Challenge_body":"When having made a labeling job on Ground Truth, an outgoing mail should be sent to team member, but in my case, mail not be sent with no error message.\n\n1. in case no private team created (the first job creation) : mail can be sent. (set up a team during job creation)\n2. in case a private team already set up: mail cannot be sent. (select a existing team during job creation)\n\nI think policies of the job role might not be enough, for example, cognito policy. How can I make sure the cause of the error?",
        "Challenge_closed_time":1662130403760,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662016903588,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with outgoing mail for a labeling job on Ground Truth. The mail is not being sent to team members without any error message. The user has observed that the mail can be sent if no private team is created during job creation, but if a private team is already set up, the mail cannot be sent. The user suspects that the job role policies, such as the cognito policy, might not be sufficient and is seeking guidance on how to identify the cause of the error.",
        "Challenge_last_edit_time":1667926589664,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUqbxtiU_kSe-GoPcj6g0pzg\/outgoing-mail-for-sagemaker-labeling-job",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":6.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":31.5278255556,
        "Challenge_title":"Outgoing mail for sagemaker labeling job",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":70.0,
        "Challenge_word_count":102,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"To Successfully create a SageMaker Labeling Job you will need the following Permission Policies applied within your account:\n\n1. The IAM entity you have used to create the job will need permissions outlined in the \"Permissions Required to Use the Amazon SageMaker Ground Truth Console\" [1]\n2. Your Labelling Job Role will need SageMakerFullAccess [2]\n\nWith these permissions in place your job should create successfully.\n\nLinks to documentation provided by AWS:\n\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security_iam_id-based-policy-examples.html#console-permissions\n\n[2] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonSageMakerFullAccess",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1662130403760,
        "Solution_link_count":2.0,
        "Solution_readability":17.4,
        "Solution_reading_time":9.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":75.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1436432728608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Colleferro, Italy",
        "Answerer_reputation_count":809.0,
        "Answerer_view_count":361.0,
        "Challenge_adjusted_solved_time":142.5591269445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an experiment in which a module R script uses functions defined in a zip source (Data Exploration). <a href=\"https:\/\/blogs.msdn.microsoft.com\/benjguin\/2014\/09\/24\/how-to-upload-an-r-package-to-azure-machine-learning\/\" rel=\"nofollow noreferrer\">Here<\/a> it's described how to do about the packages not already existing in the Azure environment.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/jKLlP.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jKLlP.png\" alt=\"enter image description here\"><\/a> <\/p>\n\n<p>The DataExploration module has been imported from a file Azure.zip containing all the packages and functions I need (as shown in the next picture).<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WlrVE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WlrVE.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I run the experiment nothing goes wrong. At the contrary, watching the log it seems clear that Azure is able to manage the source.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/AuJLD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AuJLD.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The problem is that, when I deploy the web service (classic), if I run the experiment I get the following error:<\/p>\n\n<blockquote>\n  <p>FailedToEvaluateRScript: The following error occurred during\n  evaluation of R script: R_tryEval: return error: Error in\n  .zip.unpack(pkg, tmpDir) : zip file 'src\/scales_0.4.0.zip' not found ,\n  Error code: LibraryExecutionError, Http status code: 400, Timestamp:\n  Thu, 21 Jul 2016 09:05:25 GMT<\/p>\n<\/blockquote>\n\n<p>It's like he cannot see the scales_0.4.0.zip into the 'src' folder.<\/p>\n\n<p>The strange fact is that all used to work until some days ago. Then I have copied the experiment on a second workspace and it gives me the above error. <\/p>\n\n<p>I have also tried to upload again the DataExploration module on the new workspace, but it's the same.<\/p>",
        "Challenge_closed_time":1469606718630,
        "Challenge_comment_count":1,
        "Challenge_created_time":1469093505773,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while deploying a web service on Azure. The experiment runs without any errors, but when the user tries to run the experiment after deploying the web service, an error occurs stating that the zip file 'src\/scales_0.4.0.zip' is not found. The user has tried uploading the DataExploration module again, but the issue persists. The problem seems to be that Azure cannot see the scales_0.4.0.zip file in the 'src' folder.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38500359",
        "Challenge_link_count":7,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":25.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":142.5591269445,
        "Challenge_title":"Azure: importing not already existing packages in 'src'",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":144.0,
        "Challenge_word_count":241,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436432728608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation_count":809.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>I have \"solved\" thanks to the help of the AzureML support: it is a bug they are trying to solve right now.<\/p>\n\n<p>The bug shows up when you have <strong>more R script modules<\/strong>, and the <strong>first has no a zip<\/strong> input module while the following have. <\/p>\n\n<p><em>Workaround<\/em>: connect the zip input module to the first R script module too.\n<a href=\"https:\/\/i.stack.imgur.com\/C4fV9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/C4fV9.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.5,
        "Solution_reading_time":6.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.9927311111,
        "Challenge_answer_count":1,
        "Challenge_body":"does cloudformation support sagemaker batch transform? if yes, can the jobs be triggered\/run automatically once the stack is created?",
        "Challenge_closed_time":1649089461868,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649085888036,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of automating Sagemaker batch transform using CloudFormation and triggering the jobs automatically once the stack is created.",
        "Challenge_last_edit_time":1668627644320,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUyENAstk3Q_--wYwScAIq-A\/how-to-automate-sagemaker-batch-transform",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":2.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.9927311111,
        "Challenge_title":"How to automate sagemaker batch transform?",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1167.0,
        "Challenge_word_count":24,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"While CloudFormation doesn't currently offer a resource for a SageMaker Batch Transform ([resource list here in the docs](https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/AWS_SageMaker.html)), there are plenty of other integration points to automate running these jobs.\n\n### CloudFormation\n\nI'd actually argue that CloudFormation is probably not a great fit for this anyway because CloudFormation defines **resources** which can be created, updated, and deleted. I could maybe see a correspondence between \"Create\" = \"Run a job\", maybe \"Delete\" = \"Delete job outputs\", and possibly \"Update\" = \"Re-run the job\"? But these are opinionated choices that might not make sense in every case.\n\nIf you really wanted, you could create a [Custom CloudFormation resource](https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/template-custom-resources.html) backed by an AWS Lambda function using the [CreateTransformJob API](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html) (via whatever language you prefer e.g. [boto3 in Python](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_transform_job)).\n\nNote that:\n- If you wanted to use the [SageMaker Python SDK](https:\/\/sagemaker.readthedocs.io\/en\/stable\/) (`import sagemaker`, `Transformer`, etc) instead of the low-level boto3 interface in Python - you'd need to install this extra library in your Lambda function. Tools like AWS SAM and CDK can help with this.\n- The maximum Lambda timeout is 15 minutes, you may not want to keep your Lambda function running (billable) just waiting for the transform to complete anyway, and even the overall Custom Resource will have a longer max timeout within which it must stabilize after a create\/update\/delete request... So additional orchestration may be required beyond a single synchronous Lambda function call.\n\n### Other (better?) options\n\nAs mentioned above, you can create, describe and stop SageMaker Batch Transform jobs from any environment where you're able to call AWS APIs \/ use AWS SDKs... And you can even use the high-level open-source `sagemaker` SDK from anywhere you install it. Interesting options might include:\n\n- [Amazon SageMaker Pipelines](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pipelines.html): SageMaker Pipelines have native \"steps\" for a range of SageMaker processes, including transform jobs but also training, pre-processing and more. You can define a multi-step pipeline from the SageMaker Python SDK (in your notebook or elsewhere) and then start it running on-demand (with parameters) by calling the [StartPipelineExecution API](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_StartPipelineExecution.html).\n- [AWS Step Functions](https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/welcome.html): Step Functions provides general-purpose serverless orchestration so while the orchestration for SageMaker jobs in particular might be a little more complex (one step to start the job, then a polling check to check wait for completion) - the visual workflow editor and range of integrations to other services may be useful.\n- [Amazon S3 Lambda integrations](https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-s3.html) can trigger an event automatically (to start your transform job) when new data is uploaded to Amazon S3.\n- [Scheduled EventBridge Rules](https:\/\/docs.aws.amazon.com\/eventbridge\/latest\/userguide\/eb-create-rule-schedule.html) can run actions on a regular schedule (such as calling Lambda functions, kicking off these pipelines, etc) - in case you need a schedule-based execution rather than in response to some event.\n\nThe choice will depend on what the initial trigger for your workflow would be (schedule? Data upload? Some other AWS event? An API call from outside AWS?) and what other steps need to be orchestrated as well as your transform job in the overall flow.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1649089461868,
        "Solution_link_count":10.0,
        "Solution_readability":13.3,
        "Solution_reading_time":50.56,
        "Solution_score_count":1.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":479.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1597855076910,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Delft, Netherlands",
        "Answerer_reputation_count":60.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":0.3015636111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am unable to perform the simple action:<\/p>\n<pre><code>import sagemaker\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n<\/code><\/pre>\n<p>because my notebook instance is not connected to the internet. I have an STS endpoint interface in the same subnet as my notebook instance but I thought the sagemaker API is using the global endpoint. I actually get the following error message after a while:<\/p>\n<pre><code>ConnectTimeoutError: Connect timeout on endpoint URL: &quot;https:\/\/sts.us-east-1.amazonaws.com\/&quot;\n<\/code><\/pre>\n<p>How do I fix this? Or does one need to update the sagemaker module?<\/p>",
        "Challenge_closed_time":1608634938236,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608633852607,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with AWS SageMaker as they are unable to connect to STS due to the internet being disabled. They are receiving a ConnectTimeoutError message and are seeking a solution to fix the problem or update the sagemaker module.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65407274",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.3015636111,
        "Challenge_title":"AWS SageMaker (with internet disabled) unable to connect to STS",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":510.0,
        "Challenge_word_count":91,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597855076910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delft, Netherlands",
        "Poster_reputation_count":60.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>So the solution is to include a VPC endpoint for the sagemaker API (api.sagemaker...) as well as STS.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":1.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1367338176003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Diego, CA, USA",
        "Answerer_reputation_count":271801.0,
        "Answerer_view_count":20314.0,
        "Challenge_adjusted_solved_time":22.9453238889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to render a url inside a pandas dataframe output.  I followed along with some of the other examples out there, here is my implementation:<\/p>\n\n<pre><code>def create_url(product_name):\n    search = 'http:\/\/www.example.com\/search'\n    url = 'http:\/\/www.example.com\/search\/'+product_name\n    return url\n\ndef make_clickable(url):\n    return '&lt;a target=\"_blank\" href=\"{}\"&gt;{}&lt;\/a&gt;'.format(url, url)\n\n...\n\ndf['url'] = df['product_name'].apply(format_url)\ndf.style.format({'url': make_clickable})\n<\/code><\/pre>\n\n<p>This produces a correctly formatted raw text hyperlink, however its not clickable within the output.<\/p>\n\n<p>I should add that I'm doing this in an AWS sagemaker jupyterlab notebook which potentially disables hyperlinking in the output.  Not sure how I would check that though.<\/p>",
        "Challenge_closed_time":1557849422180,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557848494950,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to render a URL inside a pandas dataframe output in JupyterLab, but the hyperlink is not clickable. The user has implemented a function to create a URL and another function to make it clickable, but it is not working. The user suspects that the issue may be due to AWS Sagemaker JupyterLab notebook potentially disabling hyperlinking in the output.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56134165",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":10.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.2575638889,
        "Challenge_title":"Render hyperlink in pandas df in jupyterlab",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":241.0,
        "Challenge_word_count":96,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1376684625670,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1105.0,
        "Poster_view_count":222.0,
        "Solution_body":"<p>If this doesn't work, I'm guessing it's an AWS thing<\/p>\n\n<ol>\n<li><code>IPython.display.HTML<\/code><\/li>\n<li><code>pandas.DataFrame.to_html<\/code> with <code>escape=False<\/code><\/li>\n<li><code>pandas.set_option('display.max_colwidth', 2000)<\/code> Must be a large number to accommodate length of link tag.  I'll say that I think this is broken.  It shouldn't be necessary to set <code>'display.max_colwidth'<\/code> in order to make sure <code>to_html<\/code> outputs properly.  But it is :-\/<\/li>\n<\/ol>\n\n<hr>\n\n<pre><code>from IPython import display\n\npd.set_option('display.max_colwidth', 2000)\n\ndisplay.HTML(df.assign(url=[*map(make_clickable, df.url)]).to_html(escape=False))\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YcVj6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YcVj6.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1557931098116,
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":11.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":75.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":2.7001211111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to ask if it is possible to use DVC with several accounts on the same machine. At the moment, all commands (<code>dvc pull<\/code>, <code>dvc push<\/code>, ...) are executed under my name. But after several people joined this project too, I do not want them to execute commands under my name.<\/p>\n<p>When I was alone on this project I generated ssh key:<\/p>\n<pre><code>ssh-keygen\n<\/code><\/pre>\n<p>Connected to server where DVC remote data is stored:<\/p>\n<pre><code>ssh-copy-id username@server_IP\n<\/code><\/pre>\n<p>Created config file which lets me execute all <code>dvc<\/code> commands using ssh:<\/p>\n<pre><code>[core]\n    remote = storage_server\n['remote &quot;storage_server&quot;']\n    url = ssh:\/\/username@server_IP:\/home\/DVC_remote\/DVC_project\n<\/code><\/pre>\n<p>What I should do so that several people could execute commands on their own name?<\/p>",
        "Challenge_closed_time":1643651143316,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643641422880,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is asking if it is possible to use DVC with multiple accounts on the same machine. They have been executing all commands under their name, but with other people joining the project, they want to avoid this. The user has generated an SSH key, connected to the server where DVC remote data is stored, and created a config file to execute all DVC commands using SSH. The user is seeking advice on how to allow multiple people to execute commands under their own names.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70928144",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":11.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2.7001211111,
        "Challenge_title":"Multiple users in DVC",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":181.0,
        "Challenge_word_count":115,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457469301700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":585.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>You need to make the &quot;username&quot; part of the config personalized based on who is running the command. There are a few options to do this (based on <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type\" rel=\"nofollow noreferrer\">this document<\/a>, see the SSH part):<\/p>\n<h2>Basic options are:<\/h2>\n<ul>\n<li>User defined in the SSH config file (e.g. <code>~\/.ssh\/config<\/code>) for this host (URL);<\/li>\n<li>Current system user;<\/li>\n<\/ul>\n<p>So, the simplest even options could be just remove it from the URL and rely on the current system user?<\/p>\n<h2>Local (git-ignored or per-project DVC config) config<\/h2>\n<p>You could do is to remove the <code>username<\/code> part from the <code>url<\/code> and run something like this:<\/p>\n<pre><code>dvc remote modify --local storage_server user username\n<\/code><\/pre>\n<p><code>--local<\/code> here means that DVC will create a separate additional config that will be ignored by Git. This way if every user runs this command in every project they use they will customize the username.<\/p>\n<hr \/>\n<p>Let me know if that helps or something doesn't work. I'll try to help.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":14.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":160.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1255292583096,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Udaipur, Rajasthan, India",
        "Answerer_reputation_count":119996.0,
        "Answerer_view_count":13669.0,
        "Challenge_adjusted_solved_time":0.1888275,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I've created some <a href=\"https:\/\/studio.azureml.net\" rel=\"nofollow\">Azure Machine Learning<\/a> Workspaces and associated them with \"classic\" storage accounts; but would like to have them associated with \"not-classic\" (or whatever the term is) storage accounts.<\/p>\n\n<p>Is there a way to convert the storage accounts from \"classic\", or to change the storage account associated with a Machine Learning Workspace?<\/p>",
        "Challenge_closed_time":1463238035912,
        "Challenge_comment_count":3,
        "Challenge_created_time":1463237356133,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user has created Azure Machine Learning Workspaces and associated them with \"classic\" storage accounts, but now wants to associate them with \"not-classic\" storage accounts. They are seeking a way to convert the storage accounts from \"classic\" or change the storage account associated with a Machine Learning Workspace.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37228077",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":10.2,
        "Challenge_reading_time":5.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1888275,
        "Challenge_title":"Converting Azure \"classic\" storage accounts",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1254.0,
        "Challenge_word_count":59,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1299959670312,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United States",
        "Poster_reputation_count":41475.0,
        "Poster_view_count":1912.0,
        "Solution_body":"<p>As of today, there's no automatic way of converting a \"Classic\" storage account into \"Azure Resource Manager (ARM)\" storage account. Today, you would need to copy data from a classic storage account to a new storage account.<\/p>\n\n<p>Having said that, there's no difference in how the data is stored in both kinds of storage accounts. Both of them support connecting via account name\/key and\/or shared access signature. The difference is how these storage account themselves are managed. In ARM storage accounts, you can assign granular role-based access control (RBAC) to control what a user can do as far as managing the storage accounts (like updating, deleting, viewing\/regenerating keys).<\/p>\n\n<p>Regarding your question about using new storage accounts with ML workspace, I don't think it's possible today (I may be wrong though). Reason being, ML is still managed via old portal which doesn't have the capability to manage ARM storage accounts.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.5,
        "Solution_reading_time":11.85,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":149.0,
        "Tool":"Azure Machine Learning"
    }
]