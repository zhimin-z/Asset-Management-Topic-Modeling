[
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":49.3241666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\nWhen `MlflowMetricsDataset` has no \"prefix\" specified, the name in the catalog is used instead. However, when the run_id is specified, it is overriden by the current run id when the prefix is automatically set.\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create a mlflow run interactively: \r\n```python\r\nmlflow.start_run()\r\nmlflow.end_run()\r\n```\r\nAnd browse the ui to retrieve the run_id\r\n\r\n2. Declare a `MlflowMetricsDataset` in the `catalog.yml`: with no prefix and an existing run_id.\r\n```python\r\nmy_metrics:\r\n    type: kedro_mlflow.io.MlflowMetricsDataSet\r\n    run_id: 123456789 # existing run_id\r\n```\r\n\r\n3. Launch the pipeline which saves this catalog: `kedro run`\r\n\r\n## Expected Result\r\n\r\nA metric should be loggedin run \"1346579\".\r\n\r\n## Actual Result\r\n\r\nThe metric is logged is a new run.\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes",
        "Challenge_closed_time":1603665805000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603488238000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The issue is that when the \"get_mlflow_config\" function is called within \"load_context\", it uses the working directory instead of the given path. This can cause problems when used in interactive mode outside of the kedro project root.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/102",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":7.1,
        "Challenge_reading_time":11.05,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":49.3241666667,
        "Challenge_title":"MlflowMetricsDataSet ignores run_id when prefix is not specified",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":126,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1604093818187,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Krakow, Poland",
        "Answerer_reputation_count":1200.0,
        "Answerer_view_count":263.0,
        "Challenge_adjusted_solved_time":562.4734544444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to Google Cloud Platform and I'm trying to create a Feature Store to fill with values from a csv file from Google Cloud Storage. The aim is to do that from a local notebook in Python.\nI'm basically following the code <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/official\/feature_store\/gapic-feature-store.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, making the appropriate changes since I'm working with the credit card public dataset.\nThe error that raises when I run the code is the following:<\/p>\n<pre><code>GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set.\n<\/code><\/pre>\n<p>and it happens during the ingestion of the data from the csv file.<\/p>\n<p>Here it is the code I'm working on:<\/p>\n<pre><code>import os\nfrom datetime import datetime\nfrom google.cloud import bigquery\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform_v1.types import feature as feature_pb2\nfrom google.cloud.aiplatform_v1.types import featurestore as featurestore_pb2\nfrom google.cloud.aiplatform_v1.types import \\\n    featurestore_service as featurestore_service_pb2\nfrom google.cloud.aiplatform_v1.types import entity_type as entity_type_pb2\nfrom google.cloud.aiplatform_v1.types import FeatureSelector, IdMatcher\n\ncredential_path = r&quot;C:\\Users\\...\\.json&quot;\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n\n## Constants\nPROJECT_ID = &quot;my-project-ID&quot;\nREGION = &quot;us-central1&quot;\nAPI_ENDPOINT = &quot;us-central1-aiplatform.googleapis.com&quot;\nINPUT_CSV_FILE = &quot;my-input-file.csv&quot;\nFEATURESTORE_ID = &quot;fraud_detection&quot;\n\n## Output dataset\nDESTINATION_DATA_SET = &quot;fraud_predictions&quot;\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\nDESTINATION_DATA_SET = &quot;{prefix}_{timestamp}&quot;.format(\n    prefix=DESTINATION_DATA_SET, timestamp=TIMESTAMP\n)\n\n## Output table. Make sure that the table does NOT already exist; \n## the BatchReadFeatureValues API cannot overwrite an existing table\nDESTINATION_TABLE_NAME = &quot;training_data&quot;\n\nDESTINATION_PATTERN = &quot;bq:\/\/{project}.{dataset}.{table}&quot;\nDESTINATION_TABLE_URI = DESTINATION_PATTERN.format(\n    project=PROJECT_ID, dataset=DESTINATION_DATA_SET, \n    table=DESTINATION_TABLE_NAME\n)\n\n## Create dataset\nclient = bigquery.Client(project=PROJECT_ID)\ndataset_id = &quot;{}.{}&quot;.format(client.project, DESTINATION_DATA_SET)\ndataset = bigquery.Dataset(dataset_id)\ndataset.location = REGION\ndataset = client.create_dataset(dataset)\nprint(&quot;Created dataset {}.{}&quot;.format(client.project, dataset.dataset_id))\n\n## Create client for CRUD and data_client for reading feature values.\nclient = aiplatform.gapic.FeaturestoreServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\ndata_client = aiplatform.gapic.FeaturestoreOnlineServingServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\nBASE_RESOURCE_PATH = client.common_location_path(PROJECT_ID, REGION)\n\n## Create featurestore (only the first time)\ncreate_lro = client.create_featurestore(\n    featurestore_service_pb2.CreateFeaturestoreRequest(\n        parent=BASE_RESOURCE_PATH,\n        featurestore_id=FEATURESTORE_ID,\n        featurestore=featurestore_pb2.Featurestore(\n            online_serving_config=featurestore_pb2.Featurestore.OnlineServingConfig(\n                fixed_node_count=1\n            ),\n        ),\n    )\n)\n\n## Wait for LRO to finish and get the LRO result.\nprint(create_lro.result())\n\nclient.get_featurestore(\n    name=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID)\n)\n\n## Create credit card entity type (only the first time)\ncc_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;creditcards&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Credit card entity&quot;,\n        ),\n    )\n)\n\n## Create fraud entity type (only the first time)\nfraud_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;frauds&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Fraud entity&quot;,\n        ),\n    )\n)\n\n## Create features for credit card type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v1&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v2&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v3&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v4&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v5&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v6&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v7&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v8&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v9&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v10&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v11&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v12&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v13&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v14&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v15&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v16&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v17&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v18&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v19&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v20&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v21&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v22&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v23&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v24&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v25&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v26&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v27&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v28&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;amount&quot;,\n        ),\n    ],\n).result()\n\n## Create features for fraud type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;class&quot;,\n        ),\n    ],\n).result()\n\n## Import features values for credit cards\nimport_cc_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/cc_details_train.csv&quot;])),\n    entity_id_field=&quot;cc_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v1&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v2&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v3&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v4&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v5&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v6&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v7&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v8&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v9&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v10&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v11&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v12&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v13&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v14&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v15&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v16&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v17&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v18&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v19&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v20&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v21&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v22&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v23&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v24&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v25&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v26&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v27&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v28&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;amount&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_cc_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n\n## Import features values for frauds\nimport_fraud_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/data_fraud_train.csv&quot;])),\n    entity_id_field=&quot;fraud_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;class&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_fraud_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n<\/code><\/pre>\n<p>When I check the <code>Ingestion Jobs<\/code> from the <code>Feature<\/code> section of Google Cloud Console I see that the job has finished but no values are added to my features.<\/p>\n<p>Any advice it is really precious.<\/p>\n<p>Thank you all.<\/p>\n<p><strong>EDIT 1<\/strong>\nIn the image below there is an example of the first row of the csv file I used as input (<code>cc_details_train.csv<\/code>). All the unseen features  are similar, the feature <code>class<\/code> can assume 0 or 1 values.\nThe injection job lasts about 5 minutes to import (ideally) 3000 rows, but it ends without error and without importing any value.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" alt=\"Rows of my csv file\" \/><\/a><\/p>",
        "Challenge_closed_time":1636364178672,
        "Challenge_comment_count":7,
        "Challenge_created_time":1635242363740,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user is encountering an error while attempting to create a feature store from a local notebook in python and ingest data from a csv file on google cloud storage.",
        "Challenge_last_edit_time":1636446864247,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69721067",
        "Challenge_link_count":3,
        "Challenge_participation_count":8,
        "Challenge_readability":32.3,
        "Challenge_reading_time":202.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":90,
        "Challenge_solved_time":311.6152588889,
        "Challenge_title":"GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":357.0,
        "Challenge_word_count":714,
        "Platform":"Stack Overflow",
        "Poster_created_time":1616589293616,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Alatri, Frosinone, FR",
        "Poster_reputation_count":67.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p><strong>VERTEX AI recomendations when using CSV to ImportValues \/ using ImportFeatureValuesRequest<\/strong><\/p>\n<p>Its possible that when using this feature you might end not able to import any data at all. You must pay attention to the time field you are using as it must be in compliance with google time formats.<\/p>\n<ol>\n<li>feature_time_field, must follow the time constraint rule set by google which is RFC3339, ie: '2021-04-15T08:28:14Z'. You can check details about the field <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.featurestores.entityTypes\/importFeatureValues#request-body\" rel=\"nofollow noreferrer\">here<\/a> and details about timestamp format can be found <a href=\"https:\/\/developers.google.com\/protocol-buffers\/docs\/reference\/google.protobuf#timestamp\" rel=\"nofollow noreferrer\">here<\/a>.<\/li>\n<li>Other columns, fields must match is designed value. One exception is field entity_id_field, As it can be any value.<\/li>\n<\/ol>\n<p>Note: I my test i found that if i do not properly set up the time field as google recommended date format it will just not upload any feature value at all.<\/p>\n<p><em>test.csv<\/em><\/p>\n<pre><code>cc_id,time,v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16,v17,v18,v19,v20,v21,v22,v23,v24,v25,v26,v27,v28,amount\n100,2021-04-15T08:28:14Z,-1.359807,-0.072781,2.534897,1.872351,2.596267,0.465238,0.923123,0.347986,0.987354,1.234657,2.128645,1.958237,0.876123,-1.712984,-0.876436,1.74699,-1.645877,-0.936121,1.456327,0.087623,1.900872,2.876234,1.874123,0.923451,0.123432,0.000012,1.212121,0.010203,1000\n<\/code><\/pre>\n<p><em>output:<\/em><\/p>\n<pre><code>imported_entity_count: 1\nimported_feature_value_count: 29\n<\/code><\/pre>\n<p><strong>About optimization and working with features<\/strong><\/p>\n<p>You can check the official documentation <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-text#single-label-classification\" rel=\"nofollow noreferrer\">here<\/a> to see the min and max amount of records recommended for processing. As a piece of advice you should only use the actual working features to run and the recommended amount of values for it.<\/p>\n<p><strong>See your running ingested job<\/strong><\/p>\n<p>Either if you use VertexUI or code to generated the ingested job. You can track its run by going into the UI to this path:<\/p>\n<pre><code>VertexAI &gt; Features &gt; View Ingested Jobs \n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1638471768683,
        "Solution_link_count":3.0,
        "Solution_readability":13.0,
        "Solution_reading_time":31.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":239.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":6.0524908334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an experiment (exp) which is published as a web service (exp [Predictive Exp.])  in the azure machine learning studio, the data used by this experiment was pushed by R using AzureML package<\/p>\n\n<pre><code>library(AzureML)\n\nws &lt;- workspace(\n  id = 'xxxxxxxxx',\n  auth = 'xxxxxxxxxxx'\n)\n\nupload.dataset(data_for_azure, ws, \"data_for_azure\")\n<\/code><\/pre>\n\n<p>The above thing worked, but lets say I want to update the dataset(same schema just added more rows)<\/p>\n\n<p>I tired this but this does not work:<\/p>\n\n<pre><code>delete.datasets(ws, \"data_for_azure\")\n\nrefresh(ws, what = c(\"everything\", \"data_for_azure\", \"exp\", \"exp [Predictive Exp.]\")) \n<\/code><\/pre>\n\n<p>I get the error stating the following:<\/p>\n\n<pre><code>Error: AzureML returns error code:\nHTTP status code : 409\nUnable to delete dataset due to lingering dependants\n<\/code><\/pre>\n\n<p>I went through the documentation, and I know that a simple refresh is not possible(same name), the only alternative I see is to delete the web service and perform everything again<\/p>\n\n<p>Any solution will be greatly helped!<\/p>",
        "Challenge_closed_time":1458567162307,
        "Challenge_comment_count":0,
        "Challenge_created_time":1458545373340,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in refreshing a dataset in Azure machine learning studio. The user has tried to delete the dataset and refresh it, but it resulted in an error stating that the dataset cannot be deleted due to lingering dependants. The user is looking for a solution to update the dataset without having to delete the web service and perform everything again.",
        "Challenge_last_edit_time":1458567175056,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36125274",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":14.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":6.0524908334,
        "Challenge_title":"Refresh the dataset in Azure machine learning",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1196.0,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Poster_created_time":1406266059940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Link\u00f6ping, Sweden",
        "Poster_reputation_count":1677.0,
        "Poster_view_count":221.0,
        "Solution_body":"<p>From the R doc.<\/p>\n\n<blockquote>\n  <p>The AzureML API does not support uploads for <em>replacing<\/em> datasets with\n  new data by re-using a name. If you need to do this, first delete the\n  dataset from the AzureML Studio interface, then upload a new version.<\/p>\n<\/blockquote>\n\n<p>Now, I think this is particular for the R sdk, as the Python SDK, and the AzureML Studio UI lets you upload a new dataset. Will check in with the R team about this.<\/p>\n\n<p>I would recommend uploading it as a new dataset with a new name, and then replacing the dataset in your experiment with this new dataset. Sorry this seem's round about, but I think is the easier option.<\/p>\n\n<p>Unless you want to upload the new version using the AzureML Studio, in which case go to +NEW, Dataset, select your file and select the checkbox that says this is an existing dataset. The filename should be the same. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":10.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":154.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1296746642860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":524.0,
        "Answerer_view_count":48.0,
        "Challenge_adjusted_solved_time":1636.7548416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We want to access an onprem SQL database with an existing Gateway, is that possible in AML?  The tool only seems to allow creating new gateways.<\/p>",
        "Challenge_closed_time":1488397699940,
        "Challenge_comment_count":0,
        "Challenge_created_time":1482505382510,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know if it is possible to use an existing Gateway to access an on-premises SQL database in Azure Machine Learning, as the tool only seems to allow creating new gateways.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41303697",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":2.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1636.7548416667,
        "Challenge_title":"Use an existing Gateway with Azure Machine Learning?",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":33,
        "Platform":"Stack Overflow",
        "Poster_created_time":1296746642860,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":524.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>Confirmed that this is not possible, AML only allows use of AML-created gateways.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":1.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":13.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":0.3501991667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to get AWS SageMaker to call AWS Comprehend. I'm getting this message in SageMaker:<\/p>\n\n<blockquote>\n  <p>ClientError: An error occurred (AccessDeniedException) when calling\n  the StartTopicsDetectionJob operation: User:\n  arn:aws:sts::545176143103:assumed-role\/access-aws-services-from-sagemaker\/SageMaker\n  is not authorized to perform: iam:PassRole on resource:\n  arn:aws:iam::545176143103:role\/access-aws-services-from-sagemaker<\/p>\n<\/blockquote>\n\n<p>When creating the Jupyter notebook, I used this role:<\/p>\n\n<blockquote>\n  <p>arn:aws:sagemaker:us-east-2:545176143103:notebook-instance\/access-comprehend-from-sagemaker<\/p>\n<\/blockquote>\n\n<p>...with the following policies attached:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/t6feB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t6feB.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I'm using the same IAM role in SageMaker:<\/p>\n\n<pre><code> data_access_role_arn = \"arn:aws:iam::545176143103:role\/access-aws-services-from-sagemaker\"\n<\/code><\/pre>\n\n<p>It looks like I'm giving the role all the access it needs. How can I correct this error?<\/p>",
        "Challenge_closed_time":1556151395660,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556150134943,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an AccessDeniedException error when trying to get AWS SageMaker to call AWS Comprehend. The error message indicates that the user's IAM role does not have the necessary permissions to perform the iam:PassRole action on the specified resource. The user has attached the required policies to the IAM role but is still facing the error.",
        "Challenge_last_edit_time":1586235828680,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55840023",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":15.8,
        "Challenge_reading_time":15.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3501991667,
        "Challenge_title":"IAM Roles for Sagemaker?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":10244.0,
        "Challenge_word_count":98,
        "Platform":"Stack Overflow",
        "Poster_created_time":1276294622427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4334.0,
        "Poster_view_count":496.0,
        "Solution_body":"<p>Based on your error, it looks like there's a permissions issue with the SageMaker notebook trying to change IAM settings from within a notebook that does not explicitly have permission to do so.<\/p>\n\n<hr>\n\n<p>You have a few options here to remedy this:<\/p>\n\n<p><strong>Option 1: Granting the SageMaker notebook permissions to define IAM role within the notebook during runtime.<\/strong><\/p>\n\n<p>From the console, click on <code>Hosted Notebooks<\/code> along the left navbar, then under <code>Permissions<\/code>, click the attached IAM role. Here, you can add policies such as <code>IAMFullAccess<\/code> or <code>IAMReadOnlyAccess<\/code>. This should solve for the permissions error when you try to attach an IAM role from within the notebook.<\/p>\n\n<p><strong>Option 2: Explicitly define the permissions you want SageMaker to have in the console.<\/strong><\/p>\n\n<p>From the console, click on <code>Hosted Notebooks<\/code> along the left navbar, then under <code>Permissions<\/code>, click the attached IAM role. Here, you can directly add policies for resource permissions (such as Comprehend). Without attaching explicit IAM access policies to this role, you wouldn't be able to change permissions during runtime.<\/p>\n\n<p><strong>Option 3: Both<\/strong><\/p>\n\n<p>If you'd like to pre-define access for some resources, but also potentially add other resource permissions during experimentation, you can do both steps 1 and 2 (Add IAM + other resource permissions to the hosted notebook in console, with the ability to change your SageMaker IAM role inline during experimentation).<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1556152415310,
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":19.88,
        "Solution_score_count":5.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":222.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1394547235287,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Warsaw, Poland",
        "Answerer_reputation_count":352.0,
        "Answerer_view_count":22.0,
        "Challenge_adjusted_solved_time":2.3974027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>dvc documentation for <code>dvc gc<\/code> command states, the <code>-r<\/code> option indicates \"Remote storage to collect garbage in\" but I'm not sure if I understand it correctly. For example I execute this command:<\/p>\n\n<pre><code>dvc gc -r myremote\n<\/code><\/pre>\n\n<p>What exactly happens if I execute this command? I have 2 possible answers:<\/p>\n\n<ol>\n<li>dvc checks which files should be deleted, then moves these files to \"myremote\" and then deletes all these files in local cache but not in remote.<\/li>\n<li>dvc checks which files should be deleted and deletes these files both in local cache and \"myremote\"<\/li>\n<\/ol>\n\n<p>Which one of them is correct?<\/p>",
        "Challenge_closed_time":1569833846627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569828438393,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the functionality of the \"dvc gc\" command with the \"-r\" option, which is used to indicate remote storage for garbage collection. The user is unsure whether executing the command will delete files only from the local cache or from both local and remote storage.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58163305",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":8.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.5022872222,
        "Challenge_title":"dvc gc and files in remote cache",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1617.0,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522254698710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Russia",
        "Poster_reputation_count":784.0,
        "Poster_view_count":77.0,
        "Solution_body":"<p>one of DVC maintainers here.<\/p>\n\n<p>Short answer: 2. is correct.<\/p>\n\n<p>A bit of additional information:\nPlease be careful when using <code>dvc gc<\/code>. It will clear your cache from all dependencies that are not mentioned in the current HEAD of your git repository. \nWe are working on making <code>dvc gc<\/code> preserving whole history by default. <\/p>\n\n<p>So if you don't want to delete files from your history commits, it would be better to wait for completion of <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2325\" rel=\"nofollow noreferrer\">this<\/a> task.<\/p>\n\n<p>[EDIT]\nPlease see comment below.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1569837069043,
        "Solution_link_count":1.0,
        "Solution_readability":7.5,
        "Solution_reading_time":7.71,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":86.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":12.3300138889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Is it possible to get the public-ip of an amazon <code>sagemaker<\/code> notebook instance?<\/p>\n\n<p>I was wondering if I can ssh into it using the public ip for remote debugging purposes.<\/p>\n\n<p>I tried getting the public ip using the below curl command<\/p>\n\n<pre><code>$curl http:\/\/169.254.169.254\/latest\/meta-data\n<\/code><\/pre>\n\n<p>This just lists the local ip and not the public ip.<\/p>\n\n<p>I also tried the below command.<\/p>\n\n<pre><code>$curl ifconfig.me\n<\/code><\/pre>\n\n<p>This returns an ip address like <code>13.232.96.15<\/code>. If I try ssh into this it doesnt work.<\/p>\n\n<p>Is there any other way we can do this?<\/p>\n\n<p>Note : The ssh port 22 is open already in the security group<\/p>",
        "Challenge_closed_time":1563405774440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563361386390,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to find the public IP of an Amazon Sagemaker notebook instance to SSH into it for remote debugging purposes. They have tried using curl commands to get the public IP, but have only been able to retrieve the local IP. They are seeking alternative methods to obtain the public IP.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57074382",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":5.8,
        "Challenge_reading_time":9.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":12.3300138889,
        "Challenge_title":"How to get the public ip of amazon sagemaker's notebook instance? Is it possible?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":4461.0,
        "Challenge_word_count":114,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455097846150,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":1347.0,
        "Poster_view_count":217.0,
        "Solution_body":"<p>I don't think you can ssh to notebook instances. You can either use open them from the console, or grab the url with an API, re: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-access-ws.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-access-ws.html<\/a><\/p>\n\n<p>If you need a terminal, then you can open one from Jupyter.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.4,
        "Solution_reading_time":5.01,
        "Solution_score_count":5.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1427753565560,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":106.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":101.7854555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are in the process of setting up Azure Machine Learning within our Azure instance. Our SQL Server sits on a virtual machine and access is restricted using ACL's  <\/p>\n\n<p>We have looked extensively for a virtual IP or an IP within Machine Learning to add to the ACL but we cannot find it. <\/p>\n\n<p>We have tested access by entering 0.0.0.0\/0 to our ACL which allows access to ML  but obviously this isnt secure and not something that we wish to continue with. <\/p>\n\n<p>Thanks in advance. <\/p>",
        "Challenge_closed_time":1427753688880,
        "Challenge_comment_count":1,
        "Challenge_created_time":1427387261240,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in granting Azure Machine Learning access to their SQL Server on a virtual machine due to restricted access using ACLs. They have been unable to find a virtual IP or an IP within Machine Learning to add to the ACL and have resorted to using an insecure method of allowing access.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/29283841",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.5,
        "Challenge_reading_time":6.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":101.7854555556,
        "Challenge_title":"Grant Azure Machine Learning access to SQL Server on Virtual Machine with ACL",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":229.0,
        "Challenge_word_count":101,
        "Platform":"Stack Overflow",
        "Poster_created_time":1375366005447,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":151.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>Azure public IP address are published and refreshed at regular intervals it can be found here: <a href=\"http:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=41653\" rel=\"nofollow\">http:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=41653<\/a><\/p>\n\n<p>You can use these to specify the restricted IP range for access<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.2,
        "Solution_reading_time":4.33,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1601729162436,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":887.0,
        "Answerer_view_count":130.0,
        "Challenge_adjusted_solved_time":506.9119638889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If we have an AzureML web service endpoint that is collecting data (for Data Drift Monitoring), does overwriting the web service endpoint with a new version of the model break links with the Dataset registered for collecting data.<\/p>\n<p>The relative path to this dataset is:\n<code>&lt;Subscription-ID&gt;\/&lt;Resource-Group&gt;\/&lt;Workspace&gt;\/&lt;Webservice-Name&gt;\/&lt;model-name&gt;\/&lt;version&gt;\/inputs\/**\/inputs*.csv<\/code><\/p>\n<p>If we redeploy a new version using <code>az ml model deploy ..... --overwrite<\/code>, will we need a new reference to a new Dataset for detecting Data Drift?<\/p>\n<p>If we use <code>az ml service update ..<\/code>, will the Dataset reference be kept intact?<\/p>",
        "Challenge_closed_time":1624012850200,
        "Challenge_comment_count":2,
        "Challenge_created_time":1622187967130,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning whether overwriting an AzureML web service endpoint with a new version of the model will break links with the dataset registered for collecting data, specifically for data drift monitoring. They are asking if a new reference to a new dataset is needed for detecting data drift or if the dataset reference will be kept intact when using \"az ml service update\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67734831",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.8,
        "Challenge_reading_time":9.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":506.9119638889,
        "Challenge_title":"Does a AzureML webservice overwrite reset the Data Collection Dataset?",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":44.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>Since the Dataset Asset is a simple reference to a location in a Datastore. Assuming the model version and service name does not change, the Dataset reference also will not change. If however, with every Service Update - The model version changes then adding a Dataset with Relative Path:<\/p>\n<pre><code>&lt;Subscription-ID&gt;\/&lt;Resource-Group&gt;\/&lt;Workspace&gt;\/&lt;Webservice-Name&gt;\/&lt;model-name&gt;\/*\/inputs\/**\/inputs*.csv\n<\/code><\/pre>\n<p>Will solve the problem. Since Data Drift is another service referencing this Dataset asset, it will keep working as expected.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":7.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":70.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1417158887760,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Gurgaon, India",
        "Answerer_reputation_count":872.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":0.7564702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created AWS S3 bucket and tried sample kmeans example on Jupyter notebook.\nBeing account owner I have read\/write permissions but I am unable to write logs with following error, <\/p>\n\n<pre><code> ClientError: An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n<\/code><\/pre>\n\n<p>here's the kmeans sample code, <\/p>\n\n<pre><code> from sagemaker import get_execution_role\n role = get_execution_role()\n bucket='testingshk' \n\n import pickle, gzip, numpy, urllib.request, json\nurllib.request.urlretrieve(\"http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl.gz\", \"mnist.pkl.gz\")\n with gzip.open('mnist.pkl.gz', 'rb') as f:\n train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n\n from sagemaker import KMeans\n data_location = 's3:\/\/{}\/kmeans_highlevel_example\/data'.format(bucket)\n output_location = 's3:\/\/{}\/kmeans_example\/output'.format(bucket)\n\n print('training data will be uploaded to: {}'.format(data_location))\n print('training artifacts will be uploaded to: {}'.format(output_location))\n\n kmeans = KMeans(role=role,\n            train_instance_count=2,\n            train_instance_type='ml.c4.8xlarge',\n            output_path=output_location,\n            k=10,\n            data_location=data_location)\n kmeans.fit(kmeans.record_set(train_set[0]))\n<\/code><\/pre>",
        "Challenge_closed_time":1523661700916,
        "Challenge_comment_count":0,
        "Challenge_created_time":1523658977623,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an Access Denied error while trying to write logs to an AWS S3 bucket using a kmeans sample code on Jupyter notebook. The user has read\/write permissions as an account owner but is still unable to write logs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49826004",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":15.2,
        "Challenge_reading_time":16.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.7564702778,
        "Challenge_title":"AWS S3 bucket write error",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":915.0,
        "Challenge_word_count":112,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400307037672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Austria",
        "Poster_reputation_count":83.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>Even if you have all the access to the bucket, you need to provide access key and secret in order to put some object in bucket if it is private. Or if you make bucket access public to all then you can push object to bucket without any problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":2.97,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1614711784088,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":1978.3835261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/edge-device-fleet-create.html#edge-device-fleet-create-console\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/edge-device-fleet-create.html#edge-device-fleet-create-console<\/a> docs to create device fleet. In this console, Role ARN is optional but it throws <code>RoleARN is required<\/code>. If I provide proper RoleArn it throws <code>Failed to create\/modify RoleAlias. Check your IAM role permission<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/84UEW.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/84UEW.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wKuTV.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wKuTV.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I have no idea what is going wrong. Any hint would be appreciable.<\/p>",
        "Challenge_closed_time":1614723172687,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607600991993,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to create a device fleet using the AWS SageMaker console. The console is throwing an error message stating that the Role ARN is required, but even after providing the proper RoleArn, it throws an error message saying \"Failed to create\/modify RoleAlias. Check your IAM role permission\". The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65233943",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":12.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":1978.3835261111,
        "Challenge_title":"Unable to create Device Fleet",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":79,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426675778223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Coimbatore, Tamil Nadu, India",
        "Poster_reputation_count":10189.0,
        "Poster_view_count":1471.0,
        "Solution_body":"<p>Mohamed, this means that Sagemaker Edge Manager was unable to use the RoleAlias you provided to take the necessary actions when creating a DeviceFleet. It needs to have the AmazonSageMakerEdgeDeviceFleetPolicy attached (or have similar permissions granted) and it needs to trust both SageMaker and IoT Core.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":3.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.3909094444,
        "Challenge_answer_count":2,
        "Challenge_body":"I have an issue while getting Catboost image URI. It is a function for generating ECR image URIs for pre-built SageMaker Docker images. Here is my code catboost_container = sagemaker.image_uris.retrieve(\"catboost\", my_region, \"latest\")",
        "Challenge_closed_time":1658369922828,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658343315554,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered a FileNotFoundError while trying to retrieve the Catboost image URI using a function in SageMaker. The error message indicates that the file or directory specified in the code does not exist.",
        "Challenge_last_edit_time":1668463143111,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU3HFACW88SuKcGZ2izeOsuA\/filenotfounderror-errno-2-no-such-file-or-directory-home-ec2-user-anaconda3-envs-python3-lib-python3-8-site-packages-sagemaker-image-uri-config-catboost-json",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":14.6,
        "Challenge_reading_time":5.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":7.3909094444,
        "Challenge_title":"FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.8\/site-packages\/sagemaker\/image_uri_config\/catboost.json'",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":167.0,
        "Challenge_word_count":39,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As illustrated [here in the docs for the algorithm](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html#catboost-modes), the parameters for retrieving this URI are a bit different: It's more like using the new JumpStart models (if you're familiar with that) than the old-style pre-built algorithms.\n\n```\ntrain_model_id, train_model_version, train_scope = \"catboost-classification-model\", \"*\", \"training\"\ntraining_instance_type = \"ml.m5.xlarge\"\n\n# Retrieve the docker image\ntrain_image_uri = image_uris.retrieve(\n    region=None,\n    framework=None,\n    model_id=train_model_id,\n    model_version=train_model_version,\n    image_scope=train_scope,\n    instance_type=training_instance_type\n)\n```\n\nI tested the above snippet from the doc page on SageMaker Studio and it worked OK. If you still see errors, it's likely your SageMaker Python SDK version is outdated (which can happen if for example you don't restart SM Studio apps or SM Notebook Instances regularly). Can check with `sagemaker.__version__` and upgrade with `!pip install --upgrade sagemaker` if needed.",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1658369922830,
        "Solution_link_count":1.0,
        "Solution_readability":13.0,
        "Solution_reading_time":13.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":116.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":20.4316775,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>I understand there was a process how to connect to on-prem sql db from Azure ML studio, but with the transition to the new UI, I don't see the option to connect to the gateway. I have it successfully installed and registered in MS Azure, but from Studio it simply does not offer it as a dataset type when using the Import Data module.  <br \/>\nI can't find any documentation regarding the new UI nor any useful guides for this.  <\/p>\n<p>Would anybody know whether this function is still available in the new studio and if so how can an on-prem gateway be connected?  <\/p>\n<p>Thank you,  <br \/>\nVS<\/p>",
        "Challenge_closed_time":1638351388392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638277834353,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in connecting to an on-prem SQL database from Azure ML studio due to the absence of an option to connect to the gateway in the new UI. The user has installed and registered the gateway in MS Azure, but it is not being offered as a dataset type when using the Import Data module. The user is seeking guidance on whether this function is still available in the new studio and how to connect to an on-prem gateway.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/646058\/new-azure-ml-vs-on-prem-sql",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.3,
        "Challenge_reading_time":7.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":20.4316775,
        "Challenge_title":"NEW Azure ML vs On-Prem SQL",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":116,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=63e55afc-7396-4eb1-8eec-945a013b20aa\">@sorcrow  <\/a>     <\/p>\n<p>Thanks for reaching out to us. I just got confirmation from the pm of AML, on-prem SQL is not supported in AML yet, but it's now on our plan.     <\/p>\n<p>I will forward your feedback to product team as well.    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":11.1,
        "Solution_reading_time":18.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":152.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":13.3927341667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am getting the following error when I try to convert a datetime variable to date.<\/p>\n\n<p><strong>My Code<\/strong><\/p>\n\n<pre><code>import datetime as dt \n\ndf['TXN_DATE_2'] = df['TXN_DATE'].dt.date\n<\/code><\/pre>\n\n<p><strong>Error<\/strong><\/p>\n\n<blockquote>\n  <p>raise NotImplementedError('Python Bridge conversion table not\n  implemented for type [{0}]'.format(value.getType()))\n  NotImplementedError: Python Bridge conversion table not implemented\n  for type [] Process returned with non-zero exit\n  code 1<\/p>\n<\/blockquote>\n\n<p>Can anyone please tell me what is going on.<\/p>",
        "Challenge_closed_time":1499329512083,
        "Challenge_comment_count":0,
        "Challenge_created_time":1499274119590,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to convert a datetime variable to date using Python script in Azure ML. The error message states that the Python Bridge conversion table is not implemented for the given type.",
        "Challenge_last_edit_time":1499281298240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44932098",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":15.3868036111,
        "Challenge_title":"Azure ML- Execute Python Script -Datatime.date not working",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":449.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1499273894443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Please try to use the code below to convert as you want.<\/p>\n\n<pre><code>import pandas as pd\nimport datetime as dt\ndf['TXN_DATE_2'] = pd.to_datetime(df['TXN_DATE']).dt.date\n<\/code><\/pre>\n\n<p>Hope it helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":2.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":0.0226744444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to build a machine learning model locally using AWS SageMaker, but I got a validation error on IAM Role name. Although it's the exact role name that I created on the console.<\/p>\n<p>This is my code<\/p>\n<pre><code>    import boto3\n    import sagemaker\n    from sagemaker import get_execution_role\n    from sagemaker.amazon.amazon_estimator import image_uris\n    from sagemaker.amazon.amazon_estimator import RecordSet\n\n    sess = sagemaker.Session()\n\n\n    bucket = sagemaker.Session().default_bucket()\n    prefix = 'sagemaker\/ccard19'\n\n    role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access '\n\n    container = image_uris.retrieve('linear-learner',boto3.Session().region_name)\n    \n    # Some other code\n\n   linear = sagemaker.LinearLearner(role=role,\n                                               instance_count=1,\n                                               instance_type='ml.m4.xlarge',\n                                               predictor_type='binary_classifier')\n  \n  # Some other code\n\n  ### Fit the classifier\n  linear.fit([train_records,val_records,test_records], wait=True, logs='All')\n\n<\/code><\/pre>\n<p>And this is the error message<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: 1 validation error detected: Value 'arn:aws:iam::949010940542:role\/SageMaker-Full-Access ' at 'roleArn' failed to satisfy constraint: Member must satisfy regular expression pattern: ^arn:aws[a-z\\-]*:iam::\\d{12}:role\/?[a-zA-Z_0-9+=,.@\\-_\/]+$\n<\/code><\/pre>\n<p>Any Help please?<\/p>",
        "Challenge_closed_time":1617256809808,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617256728180,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a validation error on IAM Role name while trying to build a machine learning model locally using AWS SageMaker. The error message indicates that the value of 'roleArn' failed to satisfy the regular expression pattern. The user has provided the code snippet and the error message for reference.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66899120",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.7,
        "Challenge_reading_time":19.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.0226744444,
        "Challenge_title":"Validation error on Role name when running AWS SageMaker linear-learner locally",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":560.0,
        "Challenge_word_count":132,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378039539503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":340.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>You have <strong>space<\/strong> in the name. It should be:<\/p>\n<pre><code>role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access'\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.7,
        "Solution_reading_time":2.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1276294622427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":4334.0,
        "Answerer_view_count":496.0,
        "Challenge_adjusted_solved_time":135.0612036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have <a href=\"https:\/\/docs.aws.amazon.com\/code-samples\/latest\/catalog\/python-comprehend-TopicModeling.py.html\" rel=\"nofollow noreferrer\">Amazon sample code<\/a> for running <code>comprehend.start_topics_detection_job<\/code>. Here is the code with the variables filled in for my job:<\/p>\n\n<pre><code>import re\nimport csv\nimport pytz\nimport boto3\nimport json\n\n# https:\/\/docs.aws.amazon.com\/code-samples\/latest\/catalog\/python-comprehend-TopicModeling.py.html\n# https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/API_InputDataConfig.html\n\n# Set these values before running the program\ninput_s3_url = \"s3:\/\/comprehend-topic-modelling-bucket\/input_800_cleaned_articles\/\"\ninput_doc_format = \"ONE_DOC_PER_LINE\"\noutput_s3_url = \"s3:\/\/comprehend-topic-modelling-bucket\/output\"\ndata_access_role_arn = \"arn:aws:iam::372656143103:role\/access-aws-services-from-sagemaker\"\nnumber_of_topics = 30\n\n# Set up job configuration\ninput_data_config = {\"S3Uri\": input_s3_url, \"InputFormat\": input_doc_format}\noutput_data_config = {\"S3Uri\": output_s3_url}\n\n# Begin a job to detect the topics in the document collection\ncomprehend = boto3.client('comprehend')\nstart_result = comprehend.start_topics_detection_job(\n    NumberOfTopics=number_of_topics,\n    InputDataConfig=input_data_config,\n    OutputDataConfig=output_data_config,\n    DataAccessRoleArn=data_access_role_arn)\n\n# Output the results\nprint('Start Topic Detection Job: ' + json.dumps(start_result))\njob_id = start_result['JobId']\nprint(f'job_id: {job_id}')\n\n# Retrieve and output information about the job\ndescribe_result = comprehend.describe_topics_detection_job(JobId=job_id)\nprint('Describe Job: ' + json.dumps(describe_result)) . #&lt;===LINE 36\n\n# List and output information about current jobs\nlist_result = comprehend.list_topics_detection_jobs()\nprint('list_topics_detection_jobs_result: ' + json.dumps(list_result))\n<\/code><\/pre>\n\n<p>It's failing with the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-8-840a7ee043d4&gt; in &lt;module&gt;()\n     34 # Retrieve and output information about the job\n     35 describe_result = comprehend.describe_topics_detection_job(JobId=job_id)\n---&gt; 36 print('Describe Job: ' + json.dumps(describe_result))\n     37 \n     38 # List and output information about current jobs\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\n    229         cls is None and indent is None and separators is None and\n    230         default is None and not sort_keys and not kw):\n--&gt; 231         return _default_encoder.encode(obj)\n    232     if cls is None:\n    233         cls = JSONEncoder\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in encode(self, o)\n    197         # exceptions aren't as detailed.  The list call should be roughly\n    198         # equivalent to the PySequence_Fast that ''.join() would do.\n--&gt; 199         chunks = self.iterencode(o, _one_shot=True)\n    200         if not isinstance(chunks, (list, tuple)):\n    201             chunks = list(chunks)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in iterencode(self, o, _one_shot)\n    255                 self.key_separator, self.item_separator, self.sort_keys,\n    256                 self.skipkeys, _one_shot)\n--&gt; 257         return _iterencode(o, 0)\n    258 \n    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in default(self, o)\n    178         \"\"\"\n    179         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n--&gt; 180                         o.__class__.__name__)\n    181 \n    182     def encode(self, o):\n\nTypeError: Object of type 'datetime' is not JSON serializable\n<\/code><\/pre>\n\n<p>It fails instantly, the second I pus \"run\". It seems to me that the call to <code>comprehend.start_topics_detection_job<\/code> may be failing, leading to an error line 36, <code>print('Describe Job: ' + json.dumps(describe_result))<\/code>.<\/p>\n\n<p>What am I missing?<\/p>\n\n<p><strong>UPDATE<\/strong><\/p>\n\n<p>The same IAM role is being used for the notebook, as well as in the above code. Here are the permissions currently assigned to that IAM role:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6ihIr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6ihIr.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1556698045340,
        "Challenge_comment_count":3,
        "Challenge_created_time":1556211825007,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while running Amazon sample code for running comprehend.start_topics_detection_job. The error is related to the object of type 'datetime' not being JSON serializable. The error occurs instantly, and the user suspects that the call to comprehend.start_topics_detection_job may be failing. The same IAM role is being used for the notebook and the code, and the user has provided the permissions assigned to that IAM role.",
        "Challenge_last_edit_time":1586235824608,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55854377",
        "Challenge_link_count":5,
        "Challenge_participation_count":4,
        "Challenge_readability":14.9,
        "Challenge_reading_time":57.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":135.0612036111,
        "Challenge_title":"comprehend.start_topics_detection_job Fails with Silent Error?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":232.0,
        "Challenge_word_count":379,
        "Platform":"Stack Overflow",
        "Poster_created_time":1276294622427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4334.0,
        "Poster_view_count":496.0,
        "Solution_body":"<p>It turns out that there was nothing wrong with the call to <code>comprehend.describe_topics_detection_job<\/code> -- it was just returning, in <code>describe_result<\/code>, something that could not be json serialized, so <code>json.dumps(describe_result))<\/code> was throwing an error. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.8,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":723.7180555556,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer is using SageMaker Studio in VpcOnly mode (VPC, protected subnets **without** internet access, **NO** NAT gateways). \nThe all functionality is fine. However, when I try create a SageMaker projects - as described [here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-projects-create.html), SageMaker Studio is unable to list the project templates (timeout and unspecified error) resulting in empty list of the available project templates.\n\nProjects are enabled for the users - as described [here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-projects-studio-updates.html). The problem is with project creation.\n\nIs internet access (e.g. via NAT gateways) is needed for SageMaker projects?",
        "Challenge_closed_time":1618085440000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615480055000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while creating SageMaker projects in VpcOnly mode without internet access. SageMaker Studio is unable to list the project templates resulting in an empty list of available project templates. The user has confirmed that projects are enabled for the users, but the problem is with project creation. The user is unsure if internet access is needed for SageMaker projects.",
        "Challenge_last_edit_time":1668618206192,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUcyhpq1pxRTmtjkDRAh_MDA\/sagemaker-studio-projects-in-vpconly-mode-without-internet-access",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":10.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":723.7180555556,
        "Challenge_title":"SageMaker Studio projects in VpcOnly mode without internet access",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":678.0,
        "Challenge_word_count":91,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Figured it out. SageMaker Studio projects need Service Catalog access and VPCE for `com.amazonaws.${AWS::Region}.servicecatalog`",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925560363,
        "Solution_link_count":0.0,
        "Solution_readability":15.5,
        "Solution_reading_time":1.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":14.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1568185673007,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Kuala Lumpur, Federal Territory of Kuala Lumpur, Malaysia",
        "Answerer_reputation_count":383.0,
        "Answerer_view_count":35.0,
        "Challenge_adjusted_solved_time":18.4514063889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Was working on az ml cli v2 to deploy real-time endpoint with command <code>az ml online-deployment<\/code> through Azure pipeline. had double confirmed that the service connection used in this pipeline task had added the permissions below in Azure Portal but still showing the same error.<\/p>\n<pre><code>ERROR: Error with code: You don't have permission to alter this storage account. Ensure that you have been assigned both Storage Blob Data Reader and Storage Blob Data Contributor roles.\n<\/code><\/pre>\n<p>Using the same service connection, we are able to perform the creation of online endpoint with <code>az ml online-endpoint create<\/code> in the same and other workspaces.<\/p>",
        "Challenge_closed_time":1655429828623,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655363403560,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered a permission issue while working on Azure Machine Learning workspace's storage account. Despite adding the required permissions in Azure Portal, the error message \"You don't have permission to alter this storage account\" persisted while deploying a real-time endpoint using az ml cli v2 through Azure pipeline. However, the same service connection was able to create an online endpoint in the same and other workspaces.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72641789",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":9.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":18.4514063889,
        "Challenge_title":"Azure Machine Learning workspace's storage account permission issue",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":129.0,
        "Challenge_word_count":109,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568185673007,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kuala Lumpur, Federal Territory of Kuala Lumpur, Malaysia",
        "Poster_reputation_count":383.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>Issue was resolved. I did not change anything in the service principal and running it on second day using same yml got through the issue. I guess there might be some propagation issue, but longer than usual.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":2.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1638293279416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":0.5860544444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following the example <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/community\/matching_engine\/matching_engine_for_indexing.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a> as per GCP <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/matching-engine\/using-matching-engine#example_notebook\" rel=\"nofollow noreferrer\">docs<\/a> to test Vertex Matching Engine. I have deployed an index but while trying to query the index I am getting <code>_InactiveRpcError<\/code>. The VPC network is in <code>us-west2<\/code> with private service access enabled and the Index is deployed in <code>us-central1<\/code>. My VPC network contains the <a href=\"https:\/\/cloud.google.com\/vpc\/docs\/firewalls#more_rules_default_vpc\" rel=\"nofollow noreferrer\">pre-populated firewall rules<\/a>.<\/p>\n<p>Index<\/p>\n<pre><code>createTime: '2021-11-23T15:25:53.928606Z'\ndeployedIndexes:\n- deployedIndexId: brute_force_glove_deployed_v3\n  indexEndpoint: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexEndpoints\/XXXXXXXXXXXX\ndescription: testing python script for creating index\ndisplayName: glove_100_brute_force_20211123152551\netag: AMEw9yOVPWBOTpbAvJLllqxWMi2YurEV_sad2n13QvbIlqjOdMyiq_j20gG1ldhdZNTL\nmetadata:\n  config:\n    algorithmConfig:\n      bruteForceConfig: {}\n    dimensions: 100\n    distanceMeasureType: DOT_PRODUCT_DISTANCE\nmetadataSchemaUri: gs:\/\/google-cloud-aiplatform\/schema\/matchingengine\/metadata\/nearest_neighbor_search_1.0.0.yaml\nname: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexes\/XXXXXXXXXXXX\nupdateTime: '2021-11-23T16:04:17.993730Z'\n<\/code><\/pre>\n<p>Index-Endpoint<\/p>\n<pre><code>createTime: '2021-11-24T10:59:51.975949Z'\ndeployedIndexes:\n- automaticResources:\n    maxReplicaCount: 1\n    minReplicaCount: 1\n  createTime: '2021-11-30T15:16:12.323028Z'\n  deploymentGroup: default\n  displayName: brute_force_glove_deployed_v3\n  enableAccessLogging: true\n  id: brute_force_glove_deployed_v3\n  index: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexes\/XXXXXXXXXXXX\n  indexSyncTime: '2021-11-30T16:37:35.597200Z'\n  privateEndpoints:\n    matchGrpcAddress: 10.242.4.5\ndisplayName: index_endpoint_for_demo\netag: AMEw9yO6cuDfgpBhGVw7-NKnlS1vdFI5nnOtqVgW1ddMP-CMXM7NfGWVpqRpMRPsNCwc\nname: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexEndpoints\/XXXXXXXXXXXX\nnetwork: projects\/XXXXXXXXXXXX\/global\/networks\/XXXXXXXXXXXX\nupdateTime: '2021-11-24T10:59:53.271100Z'\n<\/code><\/pre>\n<p>Code<\/p>\n<pre><code>\nimport grpc\n\n# import the generated classes\nimport match_service_pb2\nimport match_service_pb2_grpc\n\nDEPLOYED_INDEX_SERVER_IP = '10.242.0.5'\nDEPLOYED_INDEX_ID = 'brute_force_glove_deployed_v3'\n\nquery = [-0.11333, 0.48402, 0.090771, -0.22439, 0.034206, -0.55831, 0.041849, -0.53573, 0.18809, -0.58722, 0.015313, -0.014555, 0.80842, -0.038519, 0.75348, 0.70502, -0.17863, 0.3222, 0.67575, 0.67198, 0.26044, 0.4187, -0.34122, 0.2286, -0.53529, 1.2582, -0.091543, 0.19716, -0.037454, -0.3336, 0.31399, 0.36488, 0.71263, 0.1307, -0.24654, -0.52445, -0.036091, 0.55068, 0.10017, 0.48095, 0.71104, -0.053462, 0.22325, 0.30917, -0.39926, 0.036634, -0.35431, -0.42795, 0.46444, 0.25586, 0.68257, -0.20821, 0.38433, 0.055773, -0.2539, -0.20804, 0.52522, -0.11399, -0.3253, -0.44104, 0.17528, 0.62255, 0.50237, -0.7607, -0.071786, 0.0080131, -0.13286, 0.50097, 0.18824, -0.54722, -0.42664, 0.4292, 0.14877, -0.0072514, -0.16484, -0.059798, 0.9895, -0.61738, 0.054169, 0.48424, -0.35084, -0.27053, 0.37829, 0.11503, -0.39613, 0.24266, 0.39147, -0.075256, 0.65093, -0.20822, -0.17456, 0.53571, -0.16537, 0.13582, -0.56016, 0.016964, 0.1277, 0.94071, -0.22608, -0.021106]\n\nchannel = grpc.insecure_channel(&quot;{}:10000&quot;.format(DEPLOYED_INDEX_SERVER_IP))\nstub = match_service_pb2_grpc.MatchServiceStub(channel)\n\nrequest = match_service_pb2.MatchRequest()\nrequest.deployed_index_id = DEPLOYED_INDEX_ID\nfor val in query:\n    request.float_val.append(val)\n\nresponse = stub.Match(request)\nresponse\n<\/code><\/pre>\n<p>Error<\/p>\n<pre><code>_InactiveRpcError                         Traceback (most recent call last)\n\/tmp\/ipykernel_3451\/467153318.py in &lt;module&gt;\n    108     request.float_val.append(val)\n    109 \n--&gt; 110 response = stub.Match(request)\n    111 response\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\n    944         state, call, = self._blocking(request, timeout, metadata, credentials,\n    945                                       wait_for_ready, compression)\n--&gt; 946         return _end_unary_response_blocking(state, call, False, None)\n    947 \n    948     def with_call(self,\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\n    847             return state.response\n    848     else:\n--&gt; 849         raise _InactiveRpcError(state)\n    850 \n    851 \n\n_InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\n    status = StatusCode.UNAVAILABLE\n    details = &quot;failed to connect to all addresses&quot;\n    debug_error_string = &quot;{&quot;created&quot;:&quot;@1638277076.941429628&quot;,&quot;description&quot;:&quot;Failed to pick subchannel&quot;,&quot;file&quot;:&quot;src\/core\/ext\/filters\/client_channel\/client_channel.cc&quot;,&quot;file_line&quot;:3093,&quot;referenced_errors&quot;:[{&quot;created&quot;:&quot;@1638277076.941428202&quot;,&quot;description&quot;:&quot;failed to connect to all addresses&quot;,&quot;file&quot;:&quot;src\/core\/lib\/transport\/error_utils.cc&quot;,&quot;file_line&quot;:163,&quot;grpc_status&quot;:14}]}&quot;\n&gt;\n<\/code><\/pre>",
        "Challenge_closed_time":1638293279416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638291169620,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an _InactiveRpcError while querying a Vertex AI Matching Engine Index. The VPC network is in us-west2 with private service access enabled and the Index is deployed in us-central1. The user has pre-populated firewall rules in their VPC network. The error message indicates that the RPC terminated with a status code of UNAVAILABLE and failed to connect to all addresses.",
        "Challenge_last_edit_time":1639486727367,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70173096",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":74.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":0.5860544444,
        "Challenge_title":"_InactiveRpcError while querying Vertex AI Matching Engine Index",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":350.0,
        "Challenge_word_count":372,
        "Platform":"Stack Overflow",
        "Poster_created_time":1463607987528,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lahore, Pakistan",
        "Poster_reputation_count":143.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>Currently, Matching Engine only supports Query from the same region. Can you try running the code from VM in <code>us-central1<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":1.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1516141714350,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":216.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":189.2854638889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was looking at Databricks because it integrates with AWS services like Kinesis, but it looks to me like SageMaker is a direct competitor to Databricks? We are heavily using AWS, is there any reason to add DataBricks into the stack or odes SageMaker fill the same role?<\/p>",
        "Challenge_closed_time":1553118034270,
        "Challenge_comment_count":0,
        "Challenge_created_time":1552436606600,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is confused about whether to use AWS Sagemaker or Databricks as both seem to have similar use cases. They are wondering if there is any need to add Databricks to their AWS stack or if Sagemaker can fulfill the same role.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55132599",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.2,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":9.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":189.2854638889,
        "Challenge_title":"Difference in usecases for AWS Sagemaker vs Databricks?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":11894.0,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421343783700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1387.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>SageMaker is a great tool for deployment, it simplifies a lot of processes configuring containers, you only need to write 2-3 lines to deploy the model as an endpoint and use it.  SageMaker also provides the dev platform (Jupyter Notebook) which supports Python and Scala (sparkmagic kernal) developing, and i managed installing external scala kernel in jupyter notebook. Overall, SageMaker provides end-to-end ML services. Databricks has unbeatable Notebook environment for Spark development. <\/p>\n\n<p>Conclusion<\/p>\n\n<ol>\n<li><p>Databricks is a better platform for Big data(scala, pyspark) Developing.(unbeatable notebook environment)<\/p><\/li>\n<li><p>SageMaker is better for Deployment. and if you are not working on big data, SageMaker is a perfect choice working with (Jupyter notebook + Sklearn + Mature containers + Super easy deployment). <\/p><\/li>\n<li><p>SageMaker provides \"real time inference\", very easy to build and deploy, very impressive. you can check the official SageMaker Github.\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline<\/a><\/p><\/li>\n<\/ol>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.1,
        "Solution_reading_time":16.69,
        "Solution_score_count":14.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":141.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.7368033333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We need to connect Azure Data Lake Storage Gen2 to Azure Machine Learning by means of a datastore. For security reasons we do not want to provide the credential-based authentication credentials (service principal or SAS token). Instead we want to connect with identity based access.  <\/p>\n<p>The problem we face is that we are not able to assign a managed identity to a compute instance, so we can connect from notebooks to the Data Lake. In the documentation is explained how to assign a managed identity to a cluster, but we need the same for the compute instance, as it is the only way to run commands directly from the notebook.  <\/p>\n<p>Is there a way to assign managed identity to an Azure Machine Learning Compute Instance? Otherwise, we would like to know the best approach to overcome this issue, considering that we do not want to introduce the credentials in the code.<\/p>",
        "Challenge_closed_time":1637157273152,
        "Challenge_comment_count":1,
        "Challenge_created_time":1637143820660,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in connecting Azure Data Lake Storage Gen2 to Azure Machine Learning using a datastore due to security reasons. They are unable to assign a managed identity to a compute instance, which is necessary to connect from notebooks to the Data Lake. The user is seeking a solution to assign a managed identity to an Azure Machine Learning Compute Instance or an alternative approach to overcome this issue without introducing credentials in the code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/630520\/azure-ml-managed-identity-for-compute-instance",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.9,
        "Challenge_reading_time":11.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":3.7368033333,
        "Challenge_title":"Azure ML - Managed Identity for Compute Instance",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":159,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=cbcc7cd1-be1d-4352-9648-87343bfaafff\">@Nimbeo  <\/a> Thanks for the question. Currently It\u2019s not supported yet  to assign managed identity to an Azure Machine Learning Compute Instance, you\u2019d need to use credential-based access. We have forwarded to the product team to support in the near future.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":4.22,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":111.9549286111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am exploring AWS sagemaker for ML. I have created a bucket:<\/p>\n\n<pre><code>bucket_name = 'test-bucket' \ns3 = boto3.resource('s3')\ntry:\n   if  my_region == 'us-east-1':\n      s3.create_bucket(Bucket=bucket_name)\n   else: \n      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\nprint('S3 bucket created successfully')\nexcept Exception as e:\n    print('S3 error: ',e)\n<\/code><\/pre>\n\n<p>I have a csv in my local and I want to load that into the bucket I created.<\/p>\n\n<p>All the links I have referred  have directions to load it from a link and unzip it. Is there a way to load the data into the bucket from the local.<\/p>",
        "Challenge_closed_time":1545428602016,
        "Challenge_comment_count":0,
        "Challenge_created_time":1545025564273,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is exploring AWS Sagemaker for ML and has created an S3 bucket. They want to load a CSV file from their local machine into the bucket, but are unable to find any resources that provide directions for doing so.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53809556",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":111.9549286111,
        "Challenge_title":"Load csv into S3 from local",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1703.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455496483356,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco, CA, USA",
        "Poster_reputation_count":3912.0,
        "Poster_view_count":311.0,
        "Solution_body":"<p>If you are using Amazon SageMaker you can use the SageMaker python library that is implementing the most useful commands for data scientists, including the upload of files to S3. It is already installed on your SageMaker notebook instance by default. <\/p>\n\n<pre><code>import sagemaker\nsess = sagemaker.Session()\n\n# Uploading the local file to S3\nsess.upload_data(path='local-file.txt', bucket=bucket_name, key_prefix='input')    \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.2,
        "Solution_reading_time":5.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2683527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have imported packages:    <\/p>\n<p>library(dplyr)    <\/p>\n<p>Uploaded my dataset:    <\/p>\n<p>bike &lt;- readRDS(&quot;bike.rds&quot;)    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/16798-image.png?platform=QnA\" alt=\"16798-image.png\" \/>    <\/p>\n<p>But when I try simple &quot;filter&quot; it is not working:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/16867-image.png?platform=QnA\" alt=\"16867-image.png\" \/>    <\/p>",
        "Challenge_closed_time":1597097170387,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597096204317,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with a simple filter not working in Azure notebook for R, despite importing necessary packages and uploading the dataset.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/63901\/simple-filter-is-not-working-in-azure-notebook-for",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":6.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.2683527778,
        "Challenge_title":"Simple filter is not working in Azure notebook for R",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":43,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Fixed.  <\/p>\n<p>It looks azure notebook clean the session after some period of inactivity, there the package dplyr was not loaded after some time<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":1.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":70.6579766667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a file dataset from a data lake folder on Azure ML Studio,  at the moment I\u00b4m able to download the data from the dataset to the compute instance with this code:<\/p>\n<pre><code>subscription_id = 'xxx'\nresource_group = 'luisdatapipelinetest'\nworkspace_name = 'ml-pipelines'\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndataset = Dataset.get_by_name(workspace, name='files_test')\npath = &quot;\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/demo1231\/code\/Users\/luis.rramirez\/test\/&quot;\ndataset.download(target_path=path, overwrite=True)\n<\/code><\/pre>\n<p>With that I'm able to access the files from the notebook.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8q8y2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8q8y2.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But copying the data from the data lake to the compute instance is not efficient, how can I mount the data lake directory in the vm instead of copying the data each time?<\/p>",
        "Challenge_closed_time":1630298993132,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630008530263,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a file dataset from a data lake folder on Azure ML Studio and is able to download the data from the dataset to the compute instance. However, copying the data from the data lake to the compute instance is not efficient, and the user is seeking a way to mount the data lake directory in the VM instead of copying the data each time.",
        "Challenge_last_edit_time":1630044624416,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68944750",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":13.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":80.6841302778,
        "Challenge_title":"Mount a datalake storage in azure ML studio",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":258.0,
        "Challenge_word_count":112,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423439611840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":8349.0,
        "Poster_view_count":949.0,
        "Solution_body":"<p>MOUNTING ADLS2 to AML so you can save files into your mountPoint directly. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-data-lake-storage-generation-2\" rel=\"nofollow noreferrer\">Here<\/a> is the example of registering the storage and <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.file_dataset.filedataset?view=azure-ml-py#mount-mount-point-none----kwargs-\" rel=\"nofollow noreferrer\">here<\/a> shows how to mount your registered datastore.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.0,
        "Solution_reading_time":7.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":70.57987,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I already post my problem <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/aff7df3f-afbc-4abc-8fb0-5597184fa6c1\/export-data-blob-storage-v2?forum=MachineLearning\" rel=\"nofollow noreferrer\">here<\/a> and they suggested me to post it here.\nI am trying to export data from Azure ML to Azure Storage but I have this error:<\/p>\n\n<p>Error writing to cloud storage: The remote server returned an error: (400) Bad Request.. Please check the url. . ( Error 0151 )<\/p>\n\n<p>My blob storage configuration is Storage v2 \/ Standard and  Require secure transfer set as enabled.<\/p>\n\n<p>If I set the Require secure transfer set as disabled, the export works fine.<\/p>\n\n<p><strong>How can I export data to my blob storage with the require secure transfer set as enabled ?<\/strong><\/p>",
        "Challenge_closed_time":1550477536900,
        "Challenge_comment_count":1,
        "Challenge_created_time":1550223149923,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while exporting data from Azure ML to Azure Storage V2. The error message \"Error writing to cloud storage: The remote server returned an error: (400) Bad Request\" is displayed when the user tries to export data with \"Require secure transfer\" enabled. However, the export works fine when \"Require secure transfer\" is disabled. The user is seeking a solution to export data with \"Require secure transfer\" enabled.",
        "Challenge_last_edit_time":1550223449368,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54706312",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":10.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":70.6630491667,
        "Challenge_title":"Azure ML studio export data Azure Storage V2",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":819.0,
        "Challenge_word_count":109,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521189557296,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":160.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>According to the offical tutorial <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-to-azure-blob-storage\" rel=\"nofollow noreferrer\"><code>Export to Azure Blob Storage<\/code><\/a>, there are two authentication types for exporting data to Azure Blob Storage: SAS and Account. The description for them as below.<\/p>\n\n<blockquote>\n  <ol start=\"4\">\n  <li><p>For <strong>Authentication type<\/strong>, choose <strong>Public (SAS URL)<\/strong> if you know that the storage supports access via a SAS URL.<\/p>\n  \n  <p>A SAS URL is a special type of URL that can be generated by using an Azure storage utility, and is available for only a limited time. It contains all the information that is needed for authentication and download.<\/p>\n  \n  <p>For <strong>URI<\/strong>, type or paste the full URI that defines the account and the public blob.<\/p><\/li>\n  <li><p>For private accounts, choose <strong>Account<\/strong>, and provide the account name and the account key, so that the experiment can write to the storage account.<\/p>\n  \n  <ul>\n  <li><p><strong>Account name<\/strong>: Type or paste the name of the account where you want to save the data. For example, if the full URL of the storage account is <a href=\"http:\/\/myshared.blob.core.windows.net\" rel=\"nofollow noreferrer\">http:\/\/myshared.blob.core.windows.net<\/a>, you would type myshared.<\/p><\/li>\n  <li><p><strong>Account key<\/strong>: Paste the storage access key that is associated with the account.<\/p><\/li>\n  <\/ul><\/li>\n  <\/ol>\n<\/blockquote>\n\n<p>I try to use a simple module combination as the figure and Python code below to test the issue you got.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9XXPV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9XXPV.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<pre><code>import pandas as pd\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    dataframe1 = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})\n    return dataframe1,\n<\/code><\/pre>\n\n<p>When I tried to use the authentication type <code>Account<\/code> of my Blob Storage V2 account, I got the same issue as yours which the error code is <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/errors\/error-0151\" rel=\"nofollow noreferrer\">Error 0151<\/a> as below via click the <code>View error log<\/code> Button under the link of <code>View output log<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/TFQgO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TFQgO.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<blockquote>\n  <p><strong>Error 0151<\/strong><\/p>\n  \n  <p>There was an error writing to cloud storage. Please check the URL.<\/p>\n  \n  <p>This error in Azure Machine Learning occurs when the module tries to write data to cloud storage but the URL is unavailable or invalid.<\/p>\n  \n  <p><strong>Resolution<\/strong>\n  Check the URL and verify that it is writable.<\/p>\n  \n  <p><strong>Exception Messages<\/strong><\/p>\n  \n  <ul>\n  <li>Error writing to cloud storage (possibly a bad url).<\/li>\n  <li>Error writing to cloud storage: {0}. Please check the url.<\/li>\n  <\/ul>\n<\/blockquote>\n\n<p>Based on the error description above, the error should be caused by the blob url with SAS incorrectly generated by the <code>Export Data<\/code> module code with account information. May I think the code is old and not compatible with the new V2 storage API or API version information. You can report it to <code>feedback.azure.com<\/code>.<\/p>\n\n<p>However, I switched to use <code>SAS<\/code> authentication type to type a blob url with a SAS query string of my container which I generated via <a href=\"https:\/\/azure.microsoft.com\/en-us\/features\/storage-explorer\/\" rel=\"nofollow noreferrer\">Azure Storage Explorer<\/a> tool as below, it works fine.<\/p>\n\n<p>Fig 1: Right click on the container of your Blob Storage account, and click the <code>Get Shared Access Signature<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/1fyFL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1fyFL.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 2: Enable the permission <code>Write<\/code> (recommended to use UTC timezone) and click <code>Create<\/code> button<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/IsbQQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IsbQQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 3: Copy the <code>Query string<\/code> value, and build a blob url with a container SAS query string like <code>https:\/\/&lt;account name&gt;.blob.core.windows.net\/&lt;container name&gt;\/&lt;blob name&gt;&lt;query string&gt;<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RGnxn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RGnxn.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Note: The blob must be not exist in the container, otherwise an <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/errors\/error-0057\" rel=\"nofollow noreferrer\">Error 0057<\/a> will be caused.<\/em><\/strong><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":17.0,
        "Solution_readability":10.9,
        "Solution_reading_time":65.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":42.0,
        "Solution_word_count":563.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1593579580856,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":14.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":0.8406536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I am trying iris to get acquainted with was sagemaker I am following simple tutorials from <a href=\"https:\/\/towardsdatascience.com\/training-and-deploying-custom-tensorflow-models-with-aws-sagemaker-72027722ad76\" rel=\"nofollow noreferrer\">link<\/a>. I have created a bucket named &quot;tf-practise-iris-data&quot; and gave the IAM role of Sagemaker access to the s3 bucket as mentioned in the tutorial. I also tried creating a new bucket with a different name thinking there might be some problem with a bucket but still it is having the same issue, this is the snippet of my code <a href=\"https:\/\/i.stack.imgur.com\/FgjVK.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FgjVK.png\" alt=\"enter image description here\" \/><\/a>. And I have turned off Block all public access from the bucket but still nothing.<\/p>",
        "Challenge_closed_time":1642446326470,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642443300117,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to upload data using sagemaker_session.upload_data in the S3 bucket that they created. The data is getting stored in the default S3 bucket instead. The user has given the IAM role of Sagemaker access to the S3 bucket and turned off Block all public access, but the issue persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70745798",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":12.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.8406536111,
        "Challenge_title":"Unable to upload data using sagemaker_session.upload_data in s3 bucket that I created, it is getting stored in default s3 bucket",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":242.0,
        "Challenge_word_count":124,
        "Platform":"Stack Overflow",
        "Poster_created_time":1593579580856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":14.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Solved it!!!!!!!!!!!!<\/p>\n<pre><code>prefix = &quot;checking-with-new-bucket&quot;\ntraining_input_path = sagemaker_session.upload_data('train.csv', bucket = 'checking-with-new-bucket',key_prefix = prefix + &quot;\/training&quot;)\ntraining_input_path\n<\/code><\/pre>\n<p>Which gave output as<\/p>\n<pre><code>'s3:\/\/checking-with-new-bucket\/checking-with-new-bucket\/training\/train.csv'\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":31.6,
        "Solution_reading_time":5.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":1.2687819444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose I have a server where many users run different experiments, possibly with different Trains Servers.<\/p>\n<p>I know about the <code>TRAINS_CONFIG_FILE<\/code> environment variable, but I wonder if this can be made more flexible in one of the following ways:<\/p>\n<ol>\n<li>Specifying the Trains config file dynamically, i.e. during runtime of the training script?<\/li>\n<li>Storing a config file in each of the training repos and specifying its path relatively to the running script path (instead of relatively to <code>~\/<\/code>)?<\/li>\n<\/ol>",
        "Challenge_closed_time":1592997291032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592992723417,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is wondering if the Trains config file can be specified dynamically during runtime of the training script or if it can be stored in each of the training repositories and specified relatively to the running script path instead of relatively to ~\/ directory.",
        "Challenge_last_edit_time":1609531417760,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62552414",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":7.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.2687819444,
        "Challenge_title":"Can Trains config file be specified dynamically or relative to the running script path?",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":129.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>Disclaimer: I'm a member of Allegro Trains team<\/p>\n<ol>\n<li>Loading of the configuration is done at import time. This means that if you set the os environment before importing the package, you should be fine:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>os.environ['TRAINS_CONFIG_FILE']='~\/repo\/trains.conf'\nfrom trains import Task\n<\/code><\/pre>\n<ol start=\"2\">\n<li>The configuration file is loaded based on the current working directory, this means that if you have <code>os.environ['TRAINS_CONFIG_FILE']='trains.conf'<\/code> the trains.conf file will be loaded from the running directory at the time the import happens (which usually is the folder where your script is executed from). This means you can have it as part of the repository, and always set the <code>TRAINS_CONFIG_FILE<\/code> to point to it.<\/li>\n<\/ol>\n<p>A few notes:<\/p>\n<ul>\n<li>What is the use case for different configuration files ?<\/li>\n<li>Notice that when running with <a href=\"https:\/\/github.com\/allegroai\/trains-agent\" rel=\"nofollow noreferrer\">trains-agent<\/a> , this method will override the configuration that the <em>trains-agent<\/em> passes to the code.<\/li>\n<\/ul>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.1,
        "Solution_reading_time":14.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":149.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1430233500800,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":212.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":21511.1452602778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I suspect this has to more to do with IAM roles than Sagemaker.<\/p>\n\n<p>I'm following the example <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"noreferrer\">here<\/a><\/p>\n\n<p>Specifically, when it makes this call<\/p>\n\n<pre><code>tf_estimator.fit('s3:\/\/bucket\/path\/to\/training\/data')\n<\/code><\/pre>\n\n<p>I get this error<\/p>\n\n<pre><code>ClientError: An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:sts::013772784144:assumed-role\/AmazonSageMaker-ExecutionRole-20181022T195630\/SageMaker is not authorized to perform: iam:GetRole on resource: role SageMakerRole\n<\/code><\/pre>\n\n<p>My notebook instance has an IAM role attached to it.\nThat role has the <code>AmazonSageMakerFullAccess<\/code> policy. It also has a custom policy that looks like this<\/p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:GetObject\",\n            \"s3:PutObject\",\n            \"s3:DeleteObject\",\n            \"s3:ListBucket\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::*\"\n        ]\n    }\n]\n<\/code><\/pre>\n\n<p>}<\/p>\n\n<p>My input files and .py script is in an s3 bucket with the phrase <code>sagemaker<\/code> in it.<\/p>\n\n<p>What else am I missing?<\/p>",
        "Challenge_closed_time":1543010904776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1542853625410,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an IAM role error while using AWS Sagemaker. The error message indicates that the user's role does not have the necessary permissions to perform the GetRole operation on the SageMakerRole resource. The user's notebook instance has an IAM role attached to it with the AmazonSageMakerFullAccess policy and a custom policy that allows access to S3 resources. The user's input files and .py script are in an S3 bucket with the phrase \"sagemaker\" in it. The user is seeking advice on what else they may be missing.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53423061",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.6,
        "Challenge_reading_time":16.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":43.6887127778,
        "Challenge_title":"How do I make this IAM role error in aws sagemaker go away?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":8160.0,
        "Challenge_word_count":130,
        "Platform":"Stack Overflow",
        "Poster_created_time":1319234288808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4966.0,
        "Poster_view_count":304.0,
        "Solution_body":"<p>If you're running the example code on a SageMaker notebook instance, you can use the execution_role which has the <code>AmazonSageMakerFullAccess<\/code> attached.<\/p>\n<pre><code>from sagemaker import get_execution_role\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n<\/code><\/pre>\n<p>And you can pass this role when initializing <code>tf_estimator<\/code>.\nYou can check out the example <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html\" rel=\"nofollow noreferrer\">here<\/a> for using <code>execution_role<\/code> with S3 on notebook instance.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1620293748347,
        "Solution_link_count":1.0,
        "Solution_readability":18.4,
        "Solution_reading_time":8.09,
        "Solution_score_count":8.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":57.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":31.5833333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Under the Vertex AI - a dataset failed to create due to a constraint applied to the organization. It does not allow for the deletion of the dataset, I attempted using python (Delete a dataset \u00a0|\u00a0 Vertex AI \u00a0|\u00a0 Google Cloud) and the response was -\u00a0\"...is in failure state and cannot be deleted. It will be deleted automatically after a few days.\"\u00a0\u00a0but it didn't delete. There is not a gcloud command to correct. Short of a support request..how can the dataset be removed as I foresee this occuring as others attempt experiments. I have addressed the issue with the constraint.",
        "Challenge_closed_time":1677681300000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677567600000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to delete a failed dataset in Vertex AI due to a constraint applied to the organization. The user attempted to delete the dataset using Python and received a response that it will be deleted automatically after a few days, but it did not delete. The user is seeking a solution to remove the dataset without a support request.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Deleting-a-failed-dataset\/m-p\/527077#M1360",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.9,
        "Challenge_reading_time":7.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":31.5833333333,
        "Challenge_title":"Deleting a failed dataset",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":96.0,
        "Challenge_word_count":101,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I tried running the same\u00a0code you used and I was able to delete a dataset that was successfully created. I suspect in your case, the failure state of the dataset is the problem. Also, there is indeed no gcloud command to manually delete it. I would still suggest you file a\u00a0ticket here so\u00a0Google Cloud's engineering team can further investigate.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":4.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":67.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.9168608333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I\u2019m using machine learning studio, I have uploaded my file to data, how can I read that, what is the path?<\/p>",
        "Challenge_closed_time":1672154615572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1672151314873,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing difficulty in finding the path to read the data they have uploaded to machine learning studio.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1143409\/what-is-the-path-to-read-the-data",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.2,
        "Challenge_reading_time":1.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.9168608333,
        "Challenge_title":"what is the path to read the data?",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=fd6c7c6c-1300-405c-8e72-0e5b3e2887f1\">@peter  <\/a>     <\/p>\n<p>Thanks for reaching out to us in Microsoft Q&amp;A, you can follow below step to check your data path -     <\/p>\n<ol>\n<li> go to the portal and select the data you upload    <\/li>\n<li> Click &quot;Consume&quot; and you will see the way you can use your data     <\/li>\n<\/ol>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/274361-image.png?platform=QnA\" alt=\"274361-image.png\" \/>    <\/p>\n<p>You could leverage it directly by using it name.    <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.6,
        "Solution_reading_time":8.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.4670722222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>My current email id will be deactivated soon, so I need to change the email id associated with W&amp;B. Following the advice from this <a href=\"https:\/\/community.wandb.ai\/t\/possible-to-add-options-to-edit-profile\/123\/7\">thread<\/a>, I have created a new account. Can someone help me on this? (Had emailed someone from the Community team last week as well)<\/p>",
        "Challenge_closed_time":1636496146540,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636458465080,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user needs to change the email associated with their W&B account as their current email will be deactivated soon. They have created a new account following advice from a community thread and are seeking help to transfer their old account to the new email.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/transfer-my-account-email-change\/1247",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":9.1,
        "Challenge_reading_time":5.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":10.4670722222,
        "Challenge_title":"Transfer my account (email change)",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":333.0,
        "Challenge_word_count":53,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dsteam\">@dsteam<\/a>!<\/p>\n<p>I will be happy to help you move your projects to your new account. Could you email us at <a href=\"mailto:support@wandb.com\">support@wandb.com<\/a> about this with the details of the move? Specifically:<\/p>\n<ul>\n<li>The entity name of the account you want projects moved from<\/li>\n<li>The entity name of the account you want projects moved into<\/li>\n<li>Which projects you want moved (or if you want all)<\/li>\n<\/ul>\n<p>Thanks,<br>\nRamit<br>\nWeights and Biases support<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.9,
        "Solution_reading_time":6.74,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":74.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1505264548896,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":343.0856288889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The MLFlow Tracking is great for monitoring experiments, but I wonder if there is a solution on MLFlow or another open-source platform that can be integrated to monitor data and model drift.<\/p>\n<p>There is a <a href=\"https:\/\/databricks.com\/blog\/2019\/09\/18\/productionizing-machine-learning-from-deployment-to-drift-detection.html\" rel=\"noreferrer\">post<\/a> from Databricks showing how to achieve that with Delta Lake, however, as you can deploy and serve models with MLFlow, it looks to me that it would be easy to monitor the predictions made by the model, the same way we monitor the experiments run.<\/p>",
        "Challenge_closed_time":1613074603467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611839495203,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a solution to monitor data and model drift using MLFlow or another open-source platform. They have come across a post from Databricks that shows how to achieve this with Delta Lake, but they are wondering if there is an easier way to monitor predictions made by the model using MLFlow.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65937786",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.2,
        "Challenge_reading_time":8.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":343.0856288889,
        "Challenge_title":"Data and model drift monitoring with MLflow",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":2539.0,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525393797416,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Philadelphia, USA",
        "Poster_reputation_count":130.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>My team has recently added integration between MLflow and our open source data monitoring library called <a href=\"https:\/\/github.com\/whylabs\/whylogs-python\" rel=\"nofollow noreferrer\">whylogs<\/a>. This lets you log statistical profiles of the data passing through the model and\/or the output of the model. You can then collect these profiles from MLflow run artifacts and analyze them for drift.<\/p>\n<p>We have a <a href=\"https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/MLFlow%20Integration%20Example.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a> that walks you through the integration process and a <a href=\"https:\/\/whylabs.ai\/blog\/posts\/on-model-lifecycle-and-monitoring\" rel=\"nofollow noreferrer\">blog post<\/a> to go along with it. Lmk if you have any questions or additional feature requests!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.4,
        "Solution_reading_time":10.72,
        "Solution_score_count":4.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":90.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1640731722280,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":0.0418091667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I Can not run <code>apt to install git-lfs<\/code> on sagemaker notebook instance. I want to run git commands in my notebook.<\/p>",
        "Challenge_closed_time":1640732355180,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640732204667,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is unable to install \"git-lfs\" on their AWS Sagemaker notebook instance and is seeking a solution to run git commands in their notebook.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70513398",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":2.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0418091667,
        "Challenge_title":"I can not install \"git-lfs\" on aws sagemaker notebook instance",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":489.0,
        "Challenge_word_count":30,
        "Platform":"Stack Overflow",
        "Poster_created_time":1596727881047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>use the following commands to install git-lfs<\/p>\n<pre><code>!curl -s https:\/\/packagecloud.io\/install\/repositories\/github\/git-lfs\/script.rpm.sh | sudo bash\n\n!sudo yum install git-lfs -y\n\n!git lfs install\n<\/code><\/pre>\n<p>that should make it work<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.9,
        "Solution_reading_time":3.29,
        "Solution_score_count":6.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1361156607787,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bergen, Norway",
        "Answerer_reputation_count":6168.0,
        "Answerer_view_count":334.0,
        "Challenge_adjusted_solved_time":0.3461716667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an AWS SageMaker domain in my account created via Terraform. The resource was modified outside of Terraform. The modification was the equivalent of the following:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>aws sagemaker update-domain --domain-id d-domainid123 --default-user-settings '{&quot;KernelGatewayAppSettings&quot;: { &quot;CustomImages&quot;: [ { ... } ] } }'\n<\/code><\/pre>\n<p>Ever since, all <code>terraform plan<\/code> operations want to replace the AWS SageMaker domain:<\/p>\n<pre><code>  # module.main.aws_sagemaker_domain.default must be replaced\n-\/+ resource &quot;aws_sagemaker_domain&quot; &quot;default&quot; {\n      ~ arn                                            = &quot;arn:aws:sagemaker:eu-central-1:000111222333:domain\/d-domainid123&quot; -&gt; (known after apply)\n      ...\n        # (6 unchanged attributes hidden)\n      ~ default_user_settings {\n            # (2 unchanged attributes hidden)\n          - kernel_gateway_app_settings { # forces replacement\n               - custom_images = [ ... ]\n            }\n        }\n    }\n<\/code><\/pre>\n<p>My goal is to reconcile the situation without Terraform or me needing to create a new domain. I can't modify the Terraform sources to match the state of the SageMaker domain because that would force the recreation of domains in other accounts provisioned from the same Terraform source code.<\/p>\n<p><strong>I want to issue an <code>aws<\/code> CLI command that updates the domain and removes the <code>&quot;KernelGatewayAppSettings&quot;: { ... }<\/code> key completely from the <code>&quot;DefaultUserSettings&quot;<\/code> of the SageMaker domain. Is there a way to do this?<\/strong><\/p>\n<p>I tried the following, but the empty object is still there, so they did not work.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>aws sagemaker update-domain --domain-id d-domainid123 --default-user-settings '{&quot;KernelGatewayAppSettings&quot;: {} }'\naws sagemaker update-domain --domain-id d-domainid123 --default-user-settings '{&quot;KernelGatewayAppSettings&quot;: null }'\n\n# Still:\naws sagemaker describe-domain --domain-id d-domainid123\n{\n    &quot;DomainArn&quot;: ...,\n    &quot;DomainId&quot;: ...,\n    ...\n    &quot;DefaultUserSettings&quot;: {\n        &quot;ExecutionRole&quot;: &quot;arn:aws:iam::0001112233444:role\/SageMakerStudioExecutionRole&quot;,\n        &quot;SecurityGroups&quot;: [\n            &quot;...&quot;\n        ],\n        &quot;KernelGatewayAppSettings&quot;: {\n            &quot;CustomImages&quot;: []\n        }\n    },\n    ...\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1662394449848,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662393203630,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has an AWS SageMaker domain created via Terraform, which was modified outside of Terraform using the AWS Update API. As a result, all Terraform plan operations want to replace the AWS SageMaker domain. The user wants to issue an AWS CLI command that updates the domain and removes the \"KernelGatewayAppSettings\" key completely from the \"DefaultUserSettings\" of the SageMaker domain without needing to create a new domain or modify the Terraform sources. The user has tried updating the domain with an empty object or null value, but the empty object is still present.",
        "Challenge_last_edit_time":1662455073636,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73611956",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.8,
        "Challenge_reading_time":31.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":0.3461716667,
        "Challenge_title":"Remove JSON object via AWS Update* API to prevent Terraform from recreating the resource",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":32.0,
        "Challenge_word_count":223,
        "Platform":"Stack Overflow",
        "Poster_created_time":1219760368600,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Miskolc, Hungary",
        "Poster_reputation_count":3743.0,
        "Poster_view_count":178.0,
        "Solution_body":"<p>One option you have is to use the <a href=\"https:\/\/www.terraform.io\/language\/meta-arguments\/lifecycle\" rel=\"nofollow noreferrer\">lifecycle meta argument<\/a> to ignore out-of-band changes to the resource.<\/p>\n<pre><code>  lifecycle {\n    ignore_changes = [\n      default_user_settings\n    ]\n  }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.8,
        "Solution_reading_time":3.86,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1288196087392,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Oslo, Norge",
        "Answerer_reputation_count":1010.0,
        "Answerer_view_count":119.0,
        "Challenge_adjusted_solved_time":1.8016833333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Azure Data Lake Gen2 with public endpoint and a standard Azure ML instance.\nI have created both components with my user and I am listed as Contributor.<\/p>\n<p>I want to use data from this data lake in Azure ML.<\/p>\n<p>I have added the data lake as a Datastore using Service Principal authentication.<\/p>\n<p>I then try to create a Tabular Dataset using the Azure ML GUI I get the following error:<\/p>\n<p>Access denied\nYou do not have permission to the specified path or file.<\/p>\n<pre><code>{\n  &quot;message&quot;: &quot;ScriptExecutionException was caused by StreamAccessException.\\n  StreamAccessException was caused by AuthenticationException.\\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for '[REDACTED]' on storage failed with status code 'Forbidden' (This request is not authorized to perform this operation using this permission.), client request ID '1f9e329b-2c2c-49d6-a627-91828def284e', request ID '5ad0e715-a01f-0040-24cb-b887da000000'. Error message: [REDACTED]\\n&quot;\n}\n<\/code><\/pre>\n<p>I have tried having our Azure Portal Admin, with Admin access to both Azure ML and Data Lake try the same and she gets the same error.<\/p>\n<p>I tried creating the Dataset using Python sdk and get a similar error:<\/p>\n<pre><code>ExecutionError: \nError Code: ScriptExecution.StreamAccess.Authentication\nFailed Step: 667ddfcb-c7b1-47cf-b24a-6e090dab8947\nError Message: ScriptExecutionException was caused by StreamAccessException.\n  StreamAccessException was caused by AuthenticationException.\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for 'https:\/\/mydatalake.dfs.core.windows.net\/mycontainer?directory=mydirectory\/csv&amp;recursive=true&amp;resource=filesystem' on storage failed with status code 'Forbidden' (This request is not authorized to perform this operation using this permission.), client request ID 'a231f3e9-b32b-4173-b631-b9ed043fdfff', request ID 'c6a6f5fe-e01f-0008-3c86-b9b547000000'. Error message: {&quot;error&quot;:{&quot;code&quot;:&quot;AuthorizationPermissionMismatch&quot;,&quot;message&quot;:&quot;This request is not authorized to perform this operation using this permission.\\nRequestId:c6a6f5fe-e01f-0008-3c86-b9b547000000\\nTime:2020-11-13T06:34:01.4743177Z&quot;}}\n| session_id=75ed3c11-36de-48bf-8f7b-a0cd7dac4d58\n<\/code><\/pre>\n<p>I have created Datastore and Datasets of both a normal blob storage and a managed sql database with no issues and I have only contributor access to those so I cannot understand why I should not be Authorized to add data lake. The fact that our admin gets the same error leads me to believe there are some other issue.<\/p>\n<p>I hope you can help me identify what it is or give me some clue of what more to test.<\/p>\n<p>Edit:\nI see I might have duplicated this post: <a href=\"https:\/\/stackoverflow.com\/questions\/63891547\/how-to-connect-amls-to-adls-gen-2\">How to connect AMLS to ADLS Gen 2?<\/a>\nI will test that solution and close this post if it works<\/p>",
        "Challenge_closed_time":1605260557303,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605250166337,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an AuthenticationException error when trying to create a Tabular Dataset in Azure ML using data from an Azure Data Lake Gen2 Datastore. The user has added the data lake as a Datastore using Service Principal authentication and has tried creating the Dataset using both the Azure ML GUI and Python SDK, but still gets the same error. The user has contributor access to other Datastores but cannot understand why they are not authorized to add data from the data lake. The user is seeking help to identify the issue or get some clues on what to test.",
        "Challenge_last_edit_time":1605254071243,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64816630",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":39.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":2.8863794445,
        "Challenge_title":"AuthenticationException when creating Azure ML Dataset from Azure Data Lake Gen2 Datastore",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":3179.0,
        "Challenge_word_count":360,
        "Platform":"Stack Overflow",
        "Poster_created_time":1288196087392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Oslo, Norge",
        "Poster_reputation_count":1010.0,
        "Poster_view_count":119.0,
        "Solution_body":"<p>This was actually a duplicate of <a href=\"https:\/\/stackoverflow.com\/questions\/63891547\/how-to-connect-amls-to-adls-gen-2\">How to connect AMLS to ADLS Gen 2?<\/a>.<\/p>\n<p>The solution is to give the service principal that Azure ML uses to access the data lake the Storage Blob Data Reader access. And note you have to wait at least some minutes for this to have effect.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.5,
        "Solution_reading_time":4.73,
        "Solution_score_count":4.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":53.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.3237788889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to register a data set via the Azure Machine Learning Studio designer but keep getting an error. Here is my code, used in a &quot;Execute Python Script&quot; module:<\/p>\n<pre><code>import pandas as pd\nfrom azureml.core.dataset import Dataset\nfrom azureml.core import Workspace\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    ws = Workspace.get(name = &lt;my_workspace_name&gt;, subscription_id = &lt;my_id&gt;, resource_group = &lt;my_RG&gt;)\n    ds = Dataset.from_pandas_dataframe(dataframe1)\n    ds.register(workspace = ws,\n                name = &quot;data set name&quot;,\n                description = &quot;example description&quot;,\n                create_new_version = True)\n    return dataframe1, \n<\/code><\/pre>\n<p>But I get the following error in the Workspace.get line:<\/p>\n<pre><code>Authentication Exception: Unknown error occurred during authentication. Error detail: Unexpected polling state code_expired.\n<\/code><\/pre>\n<p>Since I am inside the workspace and in the designer, I do not usually need to do any kind of authentication (or even reference the workspace). Can anybody offer some direction? Thanks!<\/p>",
        "Challenge_closed_time":1628038438487,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628037272883,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to register a data set via the Azure Machine Learning Studio designer. The error occurs in the Workspace.get line and states \"Authentication Exception: Unknown error occurred during authentication. Error detail: Unexpected polling state code_expired.\" The user is unsure why authentication is needed since they are already inside the workspace and in the designer.",
        "Challenge_last_edit_time":1628038626927,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68644137",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.3237788889,
        "Challenge_title":"Azure Machine Learning Studio Designer Error: code_expired",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":321.0,
        "Challenge_word_count":133,
        "Platform":"Stack Overflow",
        "Poster_created_time":1371499229816,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1111.0,
        "Poster_view_count":191.0,
        "Solution_body":"<p>when you're inside a &quot;Execute Python Script&quot; module or <code>PythonScriptStep<\/code>, the authentication for fetching the workspace is already done for you (unless you're trying to authenticate to different Azure ML workspace.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Run\nrun = Run.get_context()\n\nws = run.experiment.workspace\n<\/code><\/pre>\n<p>You should be able to use that <code>ws<\/code> object to register a Dataset.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":6.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":51.1702544445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a model with VPC, Private subnet, and appropriate security group. The endpoint URL can, however, be reached through the internet though failing due to the lack of security token<\/p>\n\n<p>Things I need clarification on now are<\/p>\n\n<ol>\n<li>Is there a way to avoid the URL being accessible from the internet<\/li>\n<li>Are we not charged for requests failed on AUTH(like for API Gateway)<\/li>\n<li>Does that make our deployment vulnerable to any attacks<\/li>\n<\/ol>",
        "Challenge_closed_time":1550250221603,
        "Challenge_comment_count":0,
        "Challenge_created_time":1550066008687,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a Sagemaker model with VPC, private subnet, and security group, but the endpoint URL can still be accessed from the internet without a security token. The user is seeking clarification on how to prevent internet access to the URL, whether failed AUTH requests are charged, and if this vulnerability makes their deployment susceptible to attacks.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54671841",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":6.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":51.1702544445,
        "Challenge_title":"Sagemaker endpoint(with VPC) url accessible from internet",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":679.0,
        "Challenge_word_count":81,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320419229500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4924.0,
        "Poster_view_count":358.0,
        "Solution_body":"<p>You are not hitting your endpoint, but the endpoint of AWS SageMaker runtime. This endpoint is checking all the permissions to access your hosted model, and only if the credentials and requirements are met, the request is forwarded to your instances and models. <\/p>\n\n<p>Therefore, you can't prevent this URL from being accessible from the Internet, but at the same time, you don't need to protect it or pay for it. AWS has a high level of security on these endpoints, and I don't think that you have a more secure way to protect these endpoints. <\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":6.71,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":97.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1320061998252,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":778.0,
        "Answerer_view_count":89.0,
        "Challenge_adjusted_solved_time":1075.1311988889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>when trying to pass data without doing anything in python, getting this error:<\/p>\n\n<pre><code>Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\nCaught exception while executing function: Traceback (most recent call last):\n  File \"C:\\server\\invokepy.py\", line 175, in batch\n    rutils.RUtils.DataFrameToRFile(outlist[i], outfiles[i])\n  File \"C:\\server\\RReader\\rutils.py\", line 28, in DataFrameToRFile\n    rwriter.write_attribute_list(attributes)\n  File \"C:\\server\\RReader\\rwriter.py\", line 59, in write_attribute_list\n    self.write_object(value);\n  File \"C:\\server\\RReader\\rwriter.py\", line 121, in write_object\n    write_function(flags, value.values())\n  File \"C:\\server\\RReader\\rwriter.py\", line 104, in write_objects\n    self.write_object(value)\n  File \"C:\\server\\RReader\\rwriter.py\", line 121, in write_object\n    write_function(flags, value.values())\n  File \"C:\\server\\RReader\\rwriter.py\", line 71, in write_integers\n    self.write_integer(value)\n  File \"C:\\server\\RReader\\rwriter.py\", line 147, in write_integer\n    self.writer.WriteInt32(value)\n  File \"C:\\server\\RReader\\BinaryIO\\binarywriter.py\", line 26, in WriteInt32\n    self.WriteData(self.Int32Format, data)\n  File \"C:\\server\\RReader\\BinaryIO\\binarywriter.py\", line 14, in WriteData\n    self.stream.write(pack(format, data))\nerror: cannot convert argument to integer\n\n---------- End of error message from Python  interpreter  ----------\nStart time: UTC 05\/26\/2016 13:16:01\nEnd time: UTC 05\/26\/2016 13:16:13\n<\/code><\/pre>\n\n<p>here is the data i'm trying to pass:\n<a href=\"https:\/\/i.stack.imgur.com\/ysG36.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ysG36.png\" alt=\"the data\"><\/a><\/p>\n\n<p>here is the experiment:\n<a href=\"https:\/\/i.stack.imgur.com\/vgdSn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vgdSn.png\" alt=\"the experiment\"><\/a><\/p>\n\n<p>and the python code:\n<a href=\"https:\/\/i.stack.imgur.com\/LRSAE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LRSAE.png\" alt=\"python code\"><\/a><\/p>",
        "Challenge_closed_time":1468139774183,
        "Challenge_comment_count":1,
        "Challenge_created_time":1464269301867,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to pass data without making any changes in Python in AzureML. The error message suggests that there is an issue with converting an argument to an integer. The user has provided the data and experiment details along with the Python code used.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37462268",
        "Challenge_link_count":6,
        "Challenge_participation_count":2,
        "Challenge_readability":11.8,
        "Challenge_reading_time":28.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":1075.1311988889,
        "Challenge_title":"Python in AzureML fail to pass dataframe without changes",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":739.0,
        "Challenge_word_count":191,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320061998252,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":778.0,
        "Poster_view_count":89.0,
        "Solution_body":"<p>after talking to Microsoft support, the problem was that the \"Execute Python Script\" module cannot return empty values.\nthis can be solved by adding a \"Clean Missing Data\" module before reading it from python:\n<a href=\"https:\/\/i.stack.imgur.com\/BzCUZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BzCUZ.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.0,
        "Solution_reading_time":4.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":12.1803666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What is the proper way to connect DVC to Min.IO that is connected to some buckets on S3.<\/p>\n<pre><code>AWS-S3(My_Bucket) &gt; Min.io(MY_Bucket aliased as S3)\n<\/code><\/pre>\n<p>Right now i am accessing my bucket by using mc for example <code>mc cp s3\/my_bucket\/datasets datasets<\/code> to copy stuff from there. But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC so i can use for example <code>&quot;DVC mc-S3 pull&quot;<\/code> and <code>&quot;DVC AWS-S3 pull&quot;<\/code>.<\/p>\n<p>How do i got for it because while googling i couldn't find anything that i could easily follow.<\/p>",
        "Challenge_closed_time":1627513406643,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627469557323,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to connect DVC to Min.IO, which is connected to some buckets on S3. They are currently accessing their bucket using mc, but need to set up DVC to work with Min.IO as a hub between AWS.S3 and DVC. The user is seeking guidance on how to achieve this as they have been unable to find any helpful resources online.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68559059",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":4.9,
        "Challenge_reading_time":8.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":12.1803666667,
        "Challenge_title":"DVC connect to Min.IO to access S3",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":375.0,
        "Challenge_word_count":101,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578574709920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Poland",
        "Poster_reputation_count":85.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>It looks like you are looking for a combination of things.<\/p>\n<p>First, Jorge mentioned you can set <code>endpointurl<\/code> to access Minio the same way as you would access regular S3:<\/p>\n<pre><code>dvc remote add -d minio-remote s3:\/\/mybucket\/path\ndvc remote modify minio-remote endpointurl https:\/\/minio.example.com                          \n<\/code><\/pre>\n<p>Second, it seems you can create <em>two<\/em> remotes - one for S3, one for Minio and use <code>-r<\/code> option that is available for many data management related commands:<\/p>\n<pre><code>dvc pull -r minio-remote\ndvc pull -r s3-remote\ndvc push -r minio-remote\n...\n<\/code><\/pre>\n<p>This way you could <code>push<\/code>\/<code>pull<\/code> data to\/from a specific storage.<\/p>\n<blockquote>\n<p>But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC<\/p>\n<\/blockquote>\n<p>There are other possible ways, I think to organize this. It indeed depends on what semantics you expect from <code>DVC mc-S3 pull<\/code>. Please let us know if <code>-r<\/code> is not enough and clarify the question- that would help us here.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.7,
        "Solution_reading_time":13.51,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":153.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1436696527987,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":724.0,
        "Answerer_view_count":87.0,
        "Challenge_adjusted_solved_time":17.01314,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>As mentioned in step-3 of <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">this blog by AWS<\/a>, I have created a role to invoke sagemaker endpoint. But, when I deploy the API to a stage, I get &quot;AWS ARN for integration contains invalid action&quot; and I can't deploy the stage.\n<a href=\"https:\/\/i.stack.imgur.com\/maMdl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/maMdl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>blog suggested to select API Gateway under services and to keep on next, but didn't mention which policy will be attached. and also that another inline policy to invoke a specific sagemaker endpoint to be created and attached.\n<a href=\"https:\/\/i.stack.imgur.com\/uQwx0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uQwx0.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>and as mentioned in <a href=\"https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/integration-request-basic-setup.html\" rel=\"nofollow noreferrer\">AWS Docs<\/a>:<\/p>\n<blockquote>\n<p>It must also have API Gateway declared (in the role's trust\nrelationship) as a trusted entity to assume the role.<\/p>\n<\/blockquote>\n<p>my role also have the trust-relationshp:\n<a href=\"https:\/\/i.stack.imgur.com\/VJ9aU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VJ9aU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What's missing in my role that led to the error?<\/p>",
        "Challenge_closed_time":1645781417427,
        "Challenge_comment_count":3,
        "Challenge_created_time":1645719715780,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while deploying an API Gateway with AWS SageMaker. The error message \"AWS ARN for integration contains invalid action\" is displayed, preventing the user from deploying the stage. The user followed the steps mentioned in an AWS blog but was not provided with information on which policy to attach. The user also created an inline policy to invoke a specific SageMaker endpoint and attached it to the role. The user's role has the required trust relationship with API Gateway, but the cause of the error is unclear.",
        "Challenge_last_edit_time":1645720170123,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71255132",
        "Challenge_link_count":8,
        "Challenge_participation_count":4,
        "Challenge_readability":14.4,
        "Challenge_reading_time":22.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":17.1393463889,
        "Challenge_title":"API Gateway + AWS SageMaker - AWS ARN for integration contains invalid action for integration with sagemaker",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":177.0,
        "Challenge_word_count":174,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559910246180,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":2046.0,
        "Poster_view_count":369.0,
        "Solution_body":"<p>Check in all your API methods that you haven't specified &quot;Use Action Name&quot; for any integration request, and then left the &quot;Action&quot; field blank. If you do the &quot;AWS ARN for integration contains invalid action&quot; error message will be shown.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EXEnQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EXEnQ.png\" alt=\"action type choice\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.5,
        "Solution_reading_time":5.63,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1446889503236,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":335.0,
        "Answerer_view_count":74.0,
        "Challenge_adjusted_solved_time":14.9678333333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Today the following error appeared in my study when I try to optimize it after loading <strong>on Colab<\/strong> on Optuna 2.8.0. I save a study with joblib each time a trial begins or ends in a separate file for each trial. Never have I had this problem before and not sure what actually causes it.<\/p>\n<p>Colab shows the following trace:<\/p>\n<pre><code>\/usr\/local\/lib\/python3.7\/dist-packages\/optuna\/study.py in optimize(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\n    408             callbacks=callbacks,\n    409             gc_after_trial=gc_after_trial,\n--&gt; 410             show_progress_bar=show_progress_bar,\n    411         )\n    412 \n\n\/usr\/local\/lib\/python3.7\/dist-packages\/optuna\/_optimize.py in _optimize(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\n     73                 reseed_sampler_rng=False,\n     74                 time_start=None,\n---&gt; 75                 progress_bar=progress_bar,\n     76             )\n     77         else:\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/optuna\/_optimize.py in _optimize_sequential(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\n    160 \n    161         try:\n--&gt; 162             trial = _run_trial(study, func, catch)\n    163         except Exception:\n    164             raise\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/optuna\/_optimize.py in _run_trial(study, func, catch)\n    195                 failed_trial_callback(study, failed_trial)\n    196 \n--&gt; 197     trial = study.ask()\n    198 \n    199     state: Optional[TrialState] = None\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/optuna\/study.py in ask(self, fixed_distributions)\n    485         if trial_id is None:\n    486             trial_id = self._storage.create_new_trial(self._study_id)\n--&gt; 487         trial = trial_module.Trial(self, trial_id)\n    488 \n    489         for name, param in fixed_distributions.items():\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/optuna\/trial\/_trial.py in __init__(self, study, trial_id)\n     55         self.storage = self.study._storage\n     56 \n---&gt; 57         self._init_relative_params()\n     58 \n     59     def _init_relative_params(self) -&gt; None:\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/optuna\/trial\/_trial.py in _init_relative_params(self)\n     65         self.relative_search_space = self.study.sampler.infer_relative_search_space(study, trial)\n     66         self.relative_params = self.study.sampler.sample_relative(\n---&gt; 67             study, trial, self.relative_search_space\n     68         )\n     69 \n\n\/usr\/local\/lib\/python3.7\/dist-packages\/optuna\/samplers\/_tpe\/sampler.py in sample_relative(self, study, trial, search_space)\n    327         self._raise_error_if_multi_objective(study)\n    328 \n--&gt; 329         if self._group:\n    330             assert self._search_space_group is not None\n    331             params = {}\n\nAttributeError: 'TPESampler' object has no attribute '_group'\n<\/code><\/pre>\n<p>PS. Interestingly, the problem with the study doesn't seem to exist on my local machine. I use version 2.5.0.<\/p>",
        "Challenge_closed_time":1623056322107,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623053333190,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while optimizing their study on Optuna 2.8.0 on Colab. The error message indicates that the 'TPESampler' object has no attribute '_group'. The user has not encountered this problem before and is unsure of the cause. Interestingly, the issue does not seem to exist on their local machine running version 2.5.0.",
        "Challenge_last_edit_time":1623053642967,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67868051",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":36.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":0.8302547222,
        "Challenge_title":"'TPESampler' object has no attribute '_group'",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":101.0,
        "Challenge_word_count":257,
        "Platform":"Stack Overflow",
        "Poster_created_time":1446889503236,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":335.0,
        "Poster_view_count":74.0,
        "Solution_body":"<p>Just got the answer from Optuna developers:<\/p>\n<blockquote>\n<p>A (private) attribute was introduced to the TPESampler in v2.8 and if\nyou've pickled or serialized the sampler before this introduction,\nyou'll encounter this AttributeError when unpickling it with v2.8.<\/p>\n<\/blockquote>\n<p>The solution to this problem is re-instantiating the sampler object and substitute it to the study.sampler:<\/p>\n<pre><code>study = optuna.load_study(...) \nsampler = optuna.samplers.TPESampler() \nstudy.sampler = sampler\nstudy.optimize(...)\n<\/code><\/pre>\n<p>PS. The same kind of problem may appear with a pruner. If so, re-instantiate also the pruner!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1623107527167,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":8.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1510527902860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Zurich, Switzerland",
        "Answerer_reputation_count":1078.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":42.4409011111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have to test if my kedro project works from github so I create a new environment, then :<\/p>\n<pre><code>git clone &lt;my_project&gt;\npip install kedro kedro[pandas] kedro-viz jupyter\nkedro build-reqs\nkedro install\n<\/code><\/pre>\n<p>and the install fails, then I retry a few time (sometimes 2 or 3) then the next attempt it is successful<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4KTui.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4KTui.png\" alt=\"see image\" \/><\/a><\/p>\n<p>EDIT:\npython -V : Python 3.7.10\nkedro --version : kedro, version 0.17.3<\/p>\n<p>i cant post my requirement.txt (post is mostly code) so here is my requirement.in<\/p>\n<pre><code>black==v19.10b0\nflake8&gt;=3.7.9, &lt;4.0\nipython==7.10\nisort&gt;=4.3.21, &lt;5.0\njupyter~=1.0\njupyter_client&gt;=5.1, &lt;7.0\njupyterlab==0.31.1\nkedro==0.17.3\nnbstripout==0.3.3\npytest-cov~=2.5\npytest-mock&gt;=1.7.1, &lt;2.0\npytest~=6.1.2\nwheel==0.32.2\nspacy&gt;=3.0.0,&lt;4.0.0\nscikit-learn == 0.24.2\nkedro-viz==3.11.0\nwordcloud== 1.8.1\nhttps:\/\/github.com\/explosion\/spacy-models\/releases\/download\/fr_core_news_sm-3.0.0\/fr_core_news_sm-3.0.0.tar.gz#egg=fr_core_news_sm\n<\/code><\/pre>",
        "Challenge_closed_time":1623941002452,
        "Challenge_comment_count":3,
        "Challenge_created_time":1623434458303,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an issue while installing Kedro for their project from GitHub. The installation failed initially, but after a few attempts, it was successful. The user has provided their Python and Kedro versions and a list of requirements.",
        "Challenge_last_edit_time":1623788215208,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67941614",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":11.0,
        "Challenge_reading_time":16.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":140.7067080556,
        "Challenge_title":"Kedro install fail to install, but few attempt later it is successful",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":594.0,
        "Challenge_word_count":122,
        "Platform":"Stack Overflow",
        "Poster_created_time":1596486679830,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>As indicated in the comment, I think there are two issues at play.<\/p>\n<h4>1. Decoding error<\/h4>\n<p>This is the main exception you're getting, i.e.:<\/p>\n<pre><code>UnicodeDecodeError: \u2018utf-8\u2019 codec can\u2019t decode byte 0xe8 in position 69: invalid continuation byte\n<\/code><\/pre>\n<p>This is unexpectedly raised while Kedro itself is handling the errors from <code>pip install<\/code> (see <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/blob\/master\/kedro\/framework\/cli\/project.py#L172\" rel=\"nofollow noreferrer\">this line of Kedro's source code<\/a>). I believe the cause might be that you have accented characters in your working directory, which can't be interpreted by Python's standard <code>decode()<\/code> (see <a href=\"https:\/\/stackoverflow.com\/questions\/49898909\/reading-a-file-with-french-characters-in-python\">this<\/a>). Example:<\/p>\n<pre><code>b'acc\u00e9l\u00e9ration'.decode()\n&gt;&gt; SyntaxError: bytes can only contain ASCII literal characters.\n<\/code><\/pre>\n<p>The decoding error is obscuring the actual <code>pip install<\/code> error.<\/p>\n<h4>2. <code>pip install<\/code> error<\/h4>\n<p>As you correctly pointed out, <code>kedro install<\/code> uses <code>pip install<\/code> under the hood. It's a bit difficult to pinpoint the exact cause without seeing the actual error. I could however reproduce a similar issue, in my case getting the following error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\users\\\\&lt;mu-user&gt;\\\\anaconda3\\\\&lt;my-env&gt;\\\\kedro_project_tests\\\\lib\\\\site-packages\\\\~ydantic\\\\annotated_types.cp37-win_amd64.pyd'\nConsider using the `--user` option or check the permissions.\n<\/code><\/pre>\n<p>I believe this is caused by interactions caused by between different versions Kedro and Kedro-Viz. Simply not <code>pip install<\/code>ing <code>kedro-viz<\/code> <em>before<\/em> doing <code>kedro install<\/code> fixed it for me.<\/p>\n<hr \/>\n<p><em>Note: Related to this, there will surely be an error if the version of Kedro installed through <code>pip<\/code> <em>before<\/em> doing <code>kedro install<\/code> is not the same as the version of Kedro specified in <code>requirements.in<\/code> or <code>requirements.txt<\/code>. This is obvious, as the package currently handling execution will be uninstalled. The error in this case will be something like this:<\/em><\/p>\n<pre><code>ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'c:\\\\users\\\\&lt;my-user&gt;\\\\anaconda3\\\\envs\\\\&lt;my-env&gt;\\\\scripts\\\\kedro.exe\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.6,
        "Solution_reading_time":34.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":296.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":0.1979908334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently trying to connect to a DocumentDb (MongoDb) using Azure Machine Learning Studio.<\/p>\n\n<p>I am currently following <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/import-from-cosmos-db\" rel=\"nofollow noreferrer\">this<\/a> guide, however it seems out of date already. The assumptions I have taken have lead me to get an <code>Error 1000: ... DocumentDb client threw an exception<\/code> <code>The underlying connection was closed. The connection was closed unexpectedly.<\/code><\/p>\n\n<p>The guide, and Azure Machine Learning Studio, outline the following parameters to make a connection.<\/p>\n\n<p>Endpoint URL, Database ID, DocumentDb Key, Collection ID. It also tells you to look under the <code>Keys<\/code> blade to find these, which does not exist anymore.<\/p>\n\n<p>These are the assumptions I have taken;<\/p>\n\n<ul>\n<li>Endpoint URL = host + port under the Connection String blade. <code>https:\/\/host.com:port\/<\/code><\/li>\n<li>Database ID = the database name listed under the Data Explorer blade.<\/li>\n<li>DocumentDb Key = Primary Password under the Connection String blade.<\/li>\n<li>Collection ID = the name of a collection in the db from the Data Explorer blade.<\/li>\n<\/ul>\n\n<p>I have, for now, also opened all connections to the database just to make sure I wasn't closing the network to outside requests which, I guess, means that at least the DocumentDb key is a poor assumption.<\/p>\n\n<hr>\n\n<p>After some input from Jon, below, here is the current state of things<\/p>\n\n<ul>\n<li>Endpoint URL = the Uri from the Overview blade.<\/li>\n<li>Database ID = the database name listed under the Data Explorer blade.<\/li>\n<li>DocumentDb Key = the Primary Password under the Connection String blade.<\/li>\n<li>Collection ID = the name of a collection in the db from the Data Explorer blade.<\/li>\n<li>Sql query = <code>select top 10 * from CollectionID<\/code><\/li>\n<li>Sql parameters = {}<\/li>\n<\/ul>",
        "Challenge_closed_time":1530573817947,
        "Challenge_comment_count":12,
        "Challenge_created_time":1530540068210,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while trying to connect to a DocumentDb (MongoDb) using Azure Machine Learning Studio. The guide they are following seems to be outdated, and they are getting an error message. The guide outlines parameters to make a connection, but some of them are not available anymore. The user has made some assumptions, but they are not working. They have received some input from someone and have updated the parameters, but it is unclear if the issue has been resolved.",
        "Challenge_last_edit_time":1530573105180,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51137973",
        "Challenge_link_count":2,
        "Challenge_participation_count":13,
        "Challenge_readability":9.7,
        "Challenge_reading_time":25.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":9.3749269445,
        "Challenge_title":"Azure Machine Learning Studio - Import from Cosmos Db",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":801.0,
        "Challenge_word_count":268,
        "Platform":"Stack Overflow",
        "Poster_created_time":1350771597060,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1758.0,
        "Poster_view_count":191.0,
        "Solution_body":"<p>Through discussion in the comments, it may be that the \"Endpoint URL\" just needed to be updated, but I'll go over all of the inputs in case anyone else needs a reference to it.<\/p>\n\n<ul>\n<li>Endpoint URL - Can use the URI in the CosmosDB \"Overview\" pane in the Azure Portal<\/li>\n<li>Database ID - The name of the database to connect to<\/li>\n<li>DocumentDB Key - The primary password from the \"Connection Strings\" pane in the Azure Portal<\/li>\n<li>Collection ID - The name of the collection to read data from<\/li>\n<\/ul>\n\n<p>And, for reference, here's what my data explorer looks like in CosmosDB (database ID then collection ID):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/7Z4Q7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7Z4Q7.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And the settings in Azure ML Studio to import the data:\n<a href=\"https:\/\/i.stack.imgur.com\/LheXz.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LheXz.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.1,
        "Solution_reading_time":13.02,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":132.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1336984204220,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wien, \u00d6sterreich",
        "Answerer_reputation_count":166.0,
        "Answerer_view_count":31.0,
        "Challenge_adjusted_solved_time":1.9699208333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am using sacred package in python, this allows to keep track of computational experiments i'm running. sacred allows to add observer (<code>mongodb<\/code>) which stores all sorts of information regarding the experiment (<code>configuration<\/code>, <code>source files<\/code> etc).\n<code>sacred<\/code> allows to add artifacts to the db bt using <code>sacred.Experiment.add_artifact(PATH_TO_FILE).<\/code><\/p>\n\n<p>This command essentially adds the file to the DB.<\/p>\n\n<p>I'm using MongoDB compass, I can access the experiment information and see that an artifact has been added. it contains two fields:\n'<code>name<\/code>' and '<code>file_id<\/code>' which contains an <code>ObjectId<\/code>. (see image)<\/p>\n\n<p>I am attempting to access the stored file itself. i have noticed that under my db there is an additional <code>sub-db<\/code> called <code>fs.files<\/code> in it i can filter to find my <code>ObjectId<\/code> but it does not seem to allow me to access to content of the file itself.<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/SBg8m.png\" alt=\"object id under .files\"><\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/B7ymG.png\" alt=\"file_id under artifact\/object\"><\/p>",
        "Challenge_closed_time":1513855746928,
        "Challenge_comment_count":0,
        "Challenge_created_time":1513848655213,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using the sacred package in Python to keep track of computational experiments and is storing experiment information, configuration, and source files in MongoDB. The user is able to add artifacts to the database but is facing challenges accessing the stored file itself. The user has noticed an additional sub-database called fs.files but is unable to access the content of the file.",
        "Challenge_last_edit_time":1525602466710,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47921875",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.0,
        "Challenge_reading_time":15.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1.9699208333,
        "Challenge_title":"Accessing files in Mongodb",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":2457.0,
        "Challenge_word_count":149,
        "Platform":"Stack Overflow",
        "Poster_created_time":1468845236196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>MongoDB file storage is handled by \"GridFS\" which basically splits up files in chunks and stores them in a collection (fs.files).<\/p>\n\n<p>Tutorial to access: <a href=\"http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html\" rel=\"nofollow noreferrer\">http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.5,
        "Solution_reading_time":4.32,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":1656427742040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":2.6754122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to test the remote connection of a Python data-science client with SQL Server Machine Learning Services following this guide: <a href=\"https:\/\/docs.microsoft.com\/en-us\/sql\/machine-learning\/python\/setup-python-client-tools-sql\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/sql\/machine-learning\/python\/setup-python-client-tools-sql<\/a> (section 6).\nRunning the following script<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def send_this_func_to_sql():\n    from revoscalepy import RxSqlServerData, rx_import\n    from pandas.tools.plotting import scatter_matrix\n    import matplotlib.pyplot as plt\n    import io\n    \n    # remember the scope of the variables in this func are within our SQL Server Python Runtime\n    connection_string = &quot;Driver=SQL Server;Server=localhost\\instance02;Database=testmlsiris;Trusted_Connection=Yes;&quot;\n    \n    # specify a query and load into pandas dataframe df\n    sql_query = RxSqlServerData(connection_string=connection_string, sql_query = &quot;select * from iris_data&quot;)\n    df = rx_import(sql_query)\n    \n    scatter_matrix(df)\n    \n    # return bytestream of image created by scatter_matrix\n    buf = io.BytesIO()\n    plt.savefig(buf, format=&quot;png&quot;)\n    buf.seek(0)\n    \n    return buf.getvalue()\n\nnew_db_name = &quot;testmlsiris&quot;\nconnection_string = &quot;driver={sql server};server=sqlrzs\\instance02;database=%s;trusted_connection=yes;&quot; \n\nfrom revoscalepy import RxInSqlServer, rx_exec\n\n# create a remote compute context with connection to SQL Server\nsql_compute_context = RxInSqlServer(connection_string=connection_string%new_db_name)\n\n# use rx_exec to send the function execution to SQL Server\nimage = rx_exec(send_this_func_to_sql, compute_context=sql_compute_context)[0]\n<\/code><\/pre>\n<p>yields the following error message returned by rx_exec (stored in the <em>image<\/em> variable)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>connection_string: &quot;driver={sql server};server=sqlrzs\\instance02;database=testmlsiris;trusted_connection=yes;&quot;\nnum_tasks: 1\nexecution_timeout_seconds: 0\nwait: True\nconsole_output: False\nauto_cleanup: True\npackages_to_load: []\ndescription: &quot;sqlserver&quot;\nversion: &quot;1.0&quot;\nXXX lineno: 2, opcode: 0\nTraceback (most recent call last):\n  File &quot;&lt;string&gt;&quot;, line 3, in &lt;module&gt;\n  File &quot;E:\\SQL\\MSSQL15.INSTANCE02\\PYTHON_SERVICES\\lib\\site-packages\\revoscalepy\\computecontext\\RxInSqlServer.py&quot;, line 664, in rx_sql_satellite_pool_call\n    exec(inputfile.read())\n  File &quot;&lt;string&gt;&quot;, line 34, in &lt;module&gt;\n  File &quot;E:\\SQL\\MSSQL15.INSTANCE02\\PYTHON_SERVICES\\lib\\site-packages\\revoscalepy\\computecontext\\RxInSqlServer.py&quot;, line 886, in rx_remote_call\n    results = rx_resumeexecution(state_file = inputfile, patched_server_name=args[&quot;hostname&quot;])\n  File &quot;E:\\SQL\\MSSQL15.INSTANCE02\\PYTHON_SERVICES\\lib\\site-packages\\revoscalepy\\computecontext\\RxInSqlServer.py&quot;, line 135, in rx_resumeexecution\n    return _state[&quot;function&quot;](**_state[&quot;args&quot;])\n  File &quot;C:\\Users\\username\\sendtosql.py&quot;, line 2, in send_this_func_to_sql\nSystemError: unknown opcode\n====== sqlrzs ( process 0 ) has started run at 2022-06-29 13:47:04 W. Europe Daylight Time ======\n{'local_state': {}, 'args': {}, 'function': &lt;function send_this_func_to_sql at 0x0000020F5810F1E0&gt;}\n<\/code><\/pre>\n<p>What is going wrong here? Line 2 in the script is just an import (which works when testing Python scripts on SQL Server directly). Any help is appreciated - thanks.<\/p>",
        "Challenge_closed_time":1656513437860,
        "Challenge_comment_count":2,
        "Challenge_created_time":1656491540923,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to test the remote connection of a Python data-science client with SQL Server Machine Learning Services. The error message is returned by rx_exec and the user is seeking help to understand what is going wrong. The error seems to be related to an unknown opcode in the send_this_func_to_sql function.",
        "Challenge_last_edit_time":1656503806376,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72798225",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":17.5,
        "Challenge_reading_time":48.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":6.0824825,
        "Challenge_title":"Remote Connection fails in setup of Python data-science client for SQL Server Machine Learning Services",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":54.0,
        "Challenge_word_count":291,
        "Platform":"Stack Overflow",
        "Poster_created_time":1656427742040,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>I just figured out the reason. As of today, the Python versions for the data clients in <a href=\"https:\/\/docs.microsoft.com\/de-de\/sql\/machine-learning\/python\/setup-python-client-tools-sql?view=sql-server-ver15\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/de-de\/sql\/machine-learning\/python\/setup-python-client-tools-sql?view=sql-server-ver15<\/a> are not the newest (revoscalepy Version 9.3), while the version of Machine Learning Services that we have running in our SQL Server is already 9.4.7.\nHowever, the revoscalepy libraries for the client and server must be the same, otherwise the deserialization fails server-sided.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.2,
        "Solution_reading_time":8.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1600124498003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":110.7464591667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've set up a ClearML server in GCP using the sub-domain approach. I can access all three domains (<code>https:\/\/app.clearml.mydomain.com<\/code>, <code>https:\/\/api.clearml.mydomain.com<\/code> and <code>https:\/\/files.clearml.mydomain.com<\/code>) in a browser and see what I think is the correct response, but when connecting with the python SDK via <code>clearml-init<\/code> I get the following error:<\/p>\n<pre><code>clearml.backend_api.session.session.LoginError: Failed getting token (error 400 from https:\/\/api.clearml.mydomain.com): Bad Request\n<\/code><\/pre>\n<p>Are there any likely causes of this error?<\/p>",
        "Challenge_closed_time":1640162037740,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639763350487,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has set up a ClearML server in GCP using the sub-domain approach and can access all three domains in a browser. However, when connecting with the python SDK via clearml-init, the user is encountering a LoginError with an error 400 message. The user is seeking possible causes for this error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70397010",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":8.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":110.7464591667,
        "Challenge_title":"What would stop credentials from validation on a ClearML server?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":194.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600124498003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":46.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Following the discussion <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/517\" rel=\"nofollow noreferrer\">here<\/a>, it seemed that the load balancer being used was blocking <code>GET<\/code> requests with a payload which are used by ClearML. A <a href=\"https:\/\/github.com\/allegroai\/clearml\/pull\/521\" rel=\"nofollow noreferrer\">fix<\/a> is being worked on to allow the method to be changed to a <code>POST<\/code> request via an environment variable.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":5.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":51.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1442816236550,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":230.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":0.3051033333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I just started with DVC. following are the steps I am doing to push my models on S3<\/p>\n<p>Initialize<\/p>\n<pre><code>dvc init\n<\/code><\/pre>\n<p>Add bucket url<\/p>\n<pre><code>dvc remote add -d storage s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>\n<p>add some files<\/p>\n<pre><code>dvc add somefiles\n<\/code><\/pre>\n<p>Add aws keys<\/p>\n<pre><code>dvc remote modify storage access_key_id AWS_ACCESS_KEY_ID\ndvc remote modify storage secret_access_key AWS_SECRET_ACCESS_KEY\n<\/code><\/pre>\n<p>now when I push<\/p>\n<pre><code>dvc push\n<\/code><\/pre>\n<p>it shows<\/p>\n<pre><code>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n<p>Am i missing something?<\/p>\n<p><strong>update1<\/strong><\/p>\n<p>result of <code>dvc doctor<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc doctor\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n<\/code><\/pre>\n<p>and the <code>dvc push-vv<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc push -vv  \n2021-09-21 13:21:38,382 TRACE: Namespace(all_branches=False, all_commits=False, all_tags=False, cd='.', cmd='push', cprofile=False, cprofile_dump=None, func=&lt;class 'dvc.command.data_sync.CmdDataPush'&gt;, glob=False, instrument=False, instrument_open=False, jobs=None, pdb=False, quiet=0, recursive=False, remote=None, run_cache=False, targets=[], verbose=2, version=None, with_deps=False)\n2021-09-21 13:21:39,293 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:39,296 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:40,114 DEBUG: Preparing to transfer data from '.dvc\\cache' to 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,117 DEBUG: Preparing to collect status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,119 DEBUG: Collecting status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,121 DEBUG: Querying 1 hashes via object_exists\n2021-09-21 13:21:44,840 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (InvalidAccessKeyId) when calling the ListObjectsV2 operation: The AWS Access Key Id you provided does not exist in our records.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1080, in _info\n    out = await self._simple_info(path)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 993, in _simple_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: The AWS Access Key Id you provided does not exist in our records.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\base.py&quot;, line 45, in do_run\n    return self.run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\push.py&quot;, line 48, in push\n    pushed += self.cloud.push(obj_ids, jobs, remote=remote, odb=odb)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\data_cloud.py&quot;, line 85, in push\n    return transfer(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\transfer.py&quot;, line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 160, in compare_status\n    dest_exists, dest_missing = status(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 122, in status\n    exists = hashes.intersection(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 48, in _indexed_dir_hashes\n    dir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 415, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 611, in result_iterator\n    yield fs.pop().result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 439, in result\n    return self.__get_result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 406, in exists_with_progress\n    ret = self.fs.exists(path_info)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\fs\\fsspec_wrapper.py&quot;, line 97, in exists\n    return self.fs.exists(self._with_bucket(path_info))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 88, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 69, in sync\n    raise result[0]\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 25, in _runner\n    result[0] = await coro\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 820, in _exists\n    await self._info(path, bucket, key, version_id=version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1084, in _info\n    out = await self._version_aware_info(path, version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1027, in _version_aware_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: Forbidden\n------------------------------------------------------------\n2021-09-21 13:21:45,178 DEBUG: Version info for developers:\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-09-21 13:21:45,185 DEBUG: Analytics is enabled.\n2021-09-21 13:21:45,446 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n2021-09-21 13:21:45,456 DEBUG: Spawned '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n<\/code><\/pre>",
        "Challenge_closed_time":1632210565512,
        "Challenge_comment_count":0,
        "Challenge_created_time":1632209467140,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a 403 Forbidden error when trying to push their models to S3 using DVC. They have added the S3 bucket URL, AWS keys, and some files, but are still unable to push. The error message suggests that the AWS Access Key ID provided does not exist in their records. The user has also provided the result of `dvc doctor` and `dvc push -vv`.",
        "Challenge_last_edit_time":1632236873616,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69265000",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":17.2,
        "Challenge_reading_time":127.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":85,
        "Challenge_solved_time":0.3051033333,
        "Challenge_title":"DVC - Forbidden: An error occurred (403) when calling the HeadObject operation",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":816.0,
        "Challenge_word_count":719,
        "Platform":"Stack Overflow",
        "Poster_created_time":1363322632587,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chandigarh, India",
        "Poster_reputation_count":13237.0,
        "Poster_view_count":2675.0,
        "Solution_body":"<p>Could you please run <code>dvc doctor<\/code> and rerun <code>dvc push<\/code> and add <code>-vv<\/code> flag. And give the two results?<\/p>\n<pre><code>PermissionError: The AWS Access Key Id you provided does not exist in our records.\n<\/code><\/pre>\n<p>Does the <code>aws cli<\/code> works correctly for you? First setup <code>AWS_ACCESS_KEY_ID<\/code> and <code>AWS_SECRET_ACCESS_KEY<\/code> in envs then<\/p>\n<pre><code>aws s3 ls s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1632211831430,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":6.01,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":26.85,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi all,\n\nI've got a compute instance that cannot access the documentai processor.\u00a0 The compute engine is in the same project as the processor, and I've given the service account the roles\n\n\"Document AI API User\" and\n\"Document AI Viewer\"\u00a0\n\nThe error I receive is\n\n\"7 PERMISSION_DENIED: Request had insufficient authentication scopes.\"\u00a0\n\nwhich feels like an Oauth issue, but my reading leads me to believe that documentAI uses Application Default Credentials, and that my compute instance should use the service account for the request.\u00a0\u00a0\n\nthanks in advance for any insight.",
        "Challenge_closed_time":1675190760000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675094100000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to access the DocumentAI processor from their compute instance despite being in the same project and having given the service account the necessary roles. The error message received indicates an insufficient authentication scope, which may be related to an OAuth issue. The user believes that DocumentAI uses Application Default Credentials and that the compute instance should use the service account for the request.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/cannot-access-documentai-api-from-compute\/m-p\/515739#M1154",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.9,
        "Challenge_reading_time":7.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":26.85,
        "Challenge_title":"cannot access documentai api from compute",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":103.0,
        "Challenge_word_count":95,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\n\nthanks, I had tried adding scopes, but the solution was to add a key to the service account\u00a0 add the json config to the file system and add the environment variable\u00a0\n\nGOOGLE_APPLICATION_CREDENTIALS\n\nas per\u00a0\n\nhttps:\/\/stackoverflow.com\/questions\/65703339\/fixedcredentialsprovider-gives-unauthorized-exception-w...\n\n\u00a0\n\n\u00a0\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.8,
        "Solution_reading_time":4.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":41.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1436432728608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Colleferro, Italy",
        "Answerer_reputation_count":809.0,
        "Answerer_view_count":361.0,
        "Challenge_adjusted_solved_time":142.5591269445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an experiment in which a module R script uses functions defined in a zip source (Data Exploration). <a href=\"https:\/\/blogs.msdn.microsoft.com\/benjguin\/2014\/09\/24\/how-to-upload-an-r-package-to-azure-machine-learning\/\" rel=\"nofollow noreferrer\">Here<\/a> it's described how to do about the packages not already existing in the Azure environment.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/jKLlP.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jKLlP.png\" alt=\"enter image description here\"><\/a> <\/p>\n\n<p>The DataExploration module has been imported from a file Azure.zip containing all the packages and functions I need (as shown in the next picture).<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WlrVE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WlrVE.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I run the experiment nothing goes wrong. At the contrary, watching the log it seems clear that Azure is able to manage the source.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/AuJLD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AuJLD.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The problem is that, when I deploy the web service (classic), if I run the experiment I get the following error:<\/p>\n\n<blockquote>\n  <p>FailedToEvaluateRScript: The following error occurred during\n  evaluation of R script: R_tryEval: return error: Error in\n  .zip.unpack(pkg, tmpDir) : zip file 'src\/scales_0.4.0.zip' not found ,\n  Error code: LibraryExecutionError, Http status code: 400, Timestamp:\n  Thu, 21 Jul 2016 09:05:25 GMT<\/p>\n<\/blockquote>\n\n<p>It's like he cannot see the scales_0.4.0.zip into the 'src' folder.<\/p>\n\n<p>The strange fact is that all used to work until some days ago. Then I have copied the experiment on a second workspace and it gives me the above error. <\/p>\n\n<p>I have also tried to upload again the DataExploration module on the new workspace, but it's the same.<\/p>",
        "Challenge_closed_time":1469606718630,
        "Challenge_comment_count":1,
        "Challenge_created_time":1469093505773,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while deploying a web service on Azure. The experiment runs without any errors, but when the user tries to run the experiment after deploying the web service, an error occurs stating that the zip file 'src\/scales_0.4.0.zip' is not found. The user has tried uploading the DataExploration module again, but the issue persists. The problem seems to be that Azure cannot see the scales_0.4.0.zip file in the 'src' folder.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38500359",
        "Challenge_link_count":7,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":25.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":142.5591269445,
        "Challenge_title":"Azure: importing not already existing packages in 'src'",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":144.0,
        "Challenge_word_count":241,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436432728608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation_count":809.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>I have \"solved\" thanks to the help of the AzureML support: it is a bug they are trying to solve right now.<\/p>\n\n<p>The bug shows up when you have <strong>more R script modules<\/strong>, and the <strong>first has no a zip<\/strong> input module while the following have. <\/p>\n\n<p><em>Workaround<\/em>: connect the zip input module to the first R script module too.\n<a href=\"https:\/\/i.stack.imgur.com\/C4fV9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/C4fV9.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.5,
        "Solution_reading_time":6.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3294444444,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm trying to figure out a minimally permissive yet operational network configuration for Amazon SageMaker training to train on data from Amazon FSx for Lustre. My understanding is that both the file system and the SageMaker instance can have their own security groups and that FSx uses TCP on ports 988 and 1021-1023. Therefore, I think a good network configuration for using SageMaker with FSx is the following:\n* SageMaker EC2 equipped with the security group SM-SG that allows Inbound only with TCP on 988 and 1021-1023 from FSX-SG only.\n* Amazon FSx equipped with the security group FSX-SG that allows outbound only with TCP on 988 and 1021-1023 towards SM-SG only.\nIs this configuration enough for the training to work? Do FSx and SageMaker need other ports and sources to be opened to operate normally?",
        "Challenge_closed_time":1605281179000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605279993000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to configure a secure network for Amazon SageMaker training to use data from Amazon FSx for Lustre. They propose a network configuration that involves setting up security groups for both SageMaker and FSx, allowing inbound and outbound traffic on specific TCP ports. The user is unsure if this configuration is sufficient for the training to work and if any additional ports or sources need to be opened.",
        "Challenge_last_edit_time":1668217529291,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUrTkxH_kIT-a_LJSGYS5SXA\/how-do-i-achieve-the-least-access-secure-networking-for-sagemaker-training-on-amazon-fsx-for-lustre",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":11.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3294444444,
        "Challenge_title":"How do I achieve the least-access secure networking for SageMaker Training on Amazon FSx for Lustre?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":195.0,
        "Challenge_word_count":149,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"For the security group for Amazon FSx (Example: FSx-SG), you need to add the following additional rules:\n\n1. FSx-SG needs inbound access from the security group for SageMaker (Example: SM-SG). The SageMaker instance needs to initiate a connection to the Amazon FSx file system, which is an inbound TCP packet to FSx.\n2. FSx-SG needs inbound and outbound access to itself. This is because, Amazon FSx for Lustre is a clustered file system, where each file system is typically powered by multiple file servers, and the file servers need to  communicate with one another.\n\nFor more information on the minimum set of rules required for FSx-SG, see [File system access control with Amazon VPC][1].\n[1]: https:\/\/docs.aws.amazon.com\/fsx\/latest\/LustreGuide\/limit-access-security-groups.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925559451,
        "Solution_link_count":1.0,
        "Solution_readability":12.0,
        "Solution_reading_time":9.75,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":116.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":69.30036,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>User is not authorized to query provided resources due to s2s call not providing any active baggage to verify role-based access.  <\/p>",
        "Challenge_closed_time":1614577233696,
        "Challenge_comment_count":2,
        "Challenge_created_time":1614327752400,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue while trying to automate a machine learning model on Azure portal. The problem was caused by the lack of authorization to query resources due to the absence of active baggage to verify role-based access.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/290347\/automate-machine-learning-model-on-azure-portal-fa",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.1,
        "Challenge_reading_time":2.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":69.30036,
        "Challenge_title":"Automate machine learning model on Azure portal failed",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":29,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>No update yet.  <\/p>\n<p>I have been receiving emails from Microsoft that they will disable my account for going against their policy.   <\/p>\n<p>Reason was because I am frequently using the Azure platform.  <\/p>\n<p>I decided to take a break from the platform to avoid them deleting my account.  <\/p>\n<p>Don't know what to do \ud83d\ude2d next<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.3,
        "Solution_reading_time":4.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":56.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1517578984080,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2090.0,
        "Answerer_view_count":163.0,
        "Challenge_adjusted_solved_time":7.2704063889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have a .pkl which I would like to put into production. I would like to do a daily query of my SQL server and do a prediction on about 1000 rows. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">documentation<\/a> implies I have to load the daily data into s3. Is there a way around this? It should be able to fit in memory no problem. <\/p>\n\n<p>The answer to \" <a href=\"https:\/\/stackoverflow.com\/questions\/48319893\/is-there-some-kind-of-persistent-local-storage-in-aws-sagemaker-model-training\">is there some kind of persistent local storage in aws sagemaker model training?<\/a> \" says that \"<em>The notebook instance is coming with a local EBS (5GB) that you can use to copy some data into it and run the fast development iterations without copying the data every time from S3.<\/em>\" The 5GB could be enough but I am not sure you can actually run from a notebook in this manner. If I set up a VPN could I just query using pyodbc?<\/p>\n\n<p>Is there sagemaker integration with AWS Lambda? That in combination with a docker container would suit my needs.<\/p>",
        "Challenge_closed_time":1531919252070,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531871216253,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a .pkl file that they want to put into production for daily predictions on about 1000 rows from their SQL server. The documentation implies that the daily data needs to be loaded into s3, but the user wants to know if there is a way around this since the data can fit in memory. They are considering using a local EBS or setting up a VPN to query using pyodbc. They are also wondering if there is sagemaker integration with AWS Lambda and a docker container that would suit their needs.",
        "Challenge_last_edit_time":1531893078607,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51391639",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":14.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":13.3432825,
        "Challenge_title":"Is it possible to predict in sagemaker without using s3",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1576.0,
        "Challenge_word_count":176,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400315847156,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4342.0,
        "Poster_view_count":315.0,
        "Solution_body":"<p>While you need to to specify a s3 \"folder\" as input, this folder can contain only a dummy file. \nAlso if you bring your own docker container for training like in <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">this example<\/a>, you can do pretty much everthing in it. So you could do your daily query inside your docker container, because they have access to the internet. <\/p>\n\n<p>Also inside this container you have access to all the other aws services. Your access is defined by the role you're passing to your training job.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":7.96,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.9089091667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a compute instance:    <\/p>\n<p><strong>Virtual machine size<\/strong>    <br \/>\nSTANDARD_DS3_V2 (4 Cores, 14 GB RAM, 28 GB Disk)    <\/p>\n<p><strong>Processing Unit<\/strong>    <br \/>\nCPU - General purpose    <\/p>\n<p>But, I'm not able to access it when trying to set it for data drift monitoring.    <br \/>\nThe dropdown list is empty. I can't understand why. Can you help me please?    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/28475-datadrift.png?platform=QnA\" alt=\"28475-datadrift.png\" \/>    <\/p>",
        "Challenge_closed_time":1601279018463,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601275746390,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has created a compute instance in ML studio but is unable to access it when trying to set it for data drift monitoring as the \"compute target\" field is still blank and the dropdown list is empty. The user is seeking help to understand why this is happening.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/109397\/why-is-the-field-compute-target-for-data-drift-mon",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":8.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.9089091667,
        "Challenge_title":"Why is the field \"compute target\" for data drift monitoring in ML studio still blank whereas I have a compute instance?",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":84,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I found the answer. You must give a <strong>cluster<\/strong> compute instance to do data drift in Azure Machine Learning Studio.  <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":1.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.0041666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I see in this page of documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-interface-endpoint.html that:\n>\"*You can connect to your notebook instance from your VPC through an interface endpoint in your Virtual Private Cloud (VPC) instead of connecting over the internet. When you use a VPC interface endpoint, communication between your VPC and the notebook instance is conducted entirely and securely within the AWS network.*\"\n\nHow would customer interact on their laptop with the UI of a notebook instance sitting in a VPC?",
        "Challenge_closed_time":1541516577000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541494962000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to connect to a SageMaker Notebook instance through a VPC interface endpoint instead of over the internet. They are specifically asking how they can interact with the UI of the notebook instance on their laptop while it is located in a VPC.",
        "Challenge_last_edit_time":1668438538512,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU5z-7bQ9zQOi_NrVlHy_5oA\/which-connection-method-when-using-sagemaker-notebook-through-vpc-interface-endpoint",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":8.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":6.0041666667,
        "Challenge_title":"Which connection method when using SageMaker Notebook through VPC Interface Endpoint?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":466.0,
        "Challenge_word_count":88,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"If you are trying to access from within VPC, you'll have a direct connection. Otherwise, you'll need a configuration in place, such as Amazon VPN or AWS Direct Connect, to connect to your notebooks. Here is the blog post where we tried to explain how to set up AWS PrivateLink for Amazon SageMaker notebooks: https:\/\/aws.amazon.com\/blogs\/machine-learning\/direct-access-to-amazon-sagemaker-notebooks-from-amazon-vpc-by-using-an-aws-privatelink-endpoint\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925550451,
        "Solution_link_count":1.0,
        "Solution_readability":14.0,
        "Solution_reading_time":5.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":28.0968075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>ML studio is, by default picking up Python 3.6 kernel, even when I'm specifying use Python 3.8 AzureML kernel. In UI, it's changed but not actually.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/183312-image.png?platform=QnA\" alt=\"183312-image.png\" \/>    <\/p>",
        "Challenge_closed_time":1647451118727,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647349970220,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure ML Studio where it defaults to Python 3.6 kernel instead of the specified Python 3.8 AzureML kernel. The UI shows the change but it does not actually work. The user is unable to create a Microsoft ticket under MSDN and is seeking suggestions to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/772790\/azure-ml-studio-is-bugged-out-and-can-not-create-a",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":4.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":28.0968075,
        "Challenge_title":"Azure ML Studio is bugged out and can not create a Microsoft ticket under MSDN. Need a few suggestions",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":49,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out.  It looks like the command you ran isn't supported. A better command to test kernel changes is shown below:  <\/p>\n<pre><code>from platform import python_version\nprint(python_version())\n<\/code><\/pre>\n<p>Hope this helps!<\/p>\n",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":3.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1435524174732,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bursa, Turkey",
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":19.1911183333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an aws sagemaker end-point which need to be called from .Net core client, I have used the AWS SDK that deals with SageMaker and provided the required credentials however, always it keeps saying : <\/p>\n\n<p>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.<\/p>\n\n<p>var requestBody = \"{'url':'\"+\"<a href=\"https:\/\/cdn.pixabay.com\/photo\/2018\/05\/28\/22\/11\/message-in-a-bottle-3437294_960_720.jpg\" rel=\"nofollow noreferrer\">https:\/\/cdn.pixabay.com\/photo\/2018\/05\/28\/22\/11\/message-in-a-bottle-3437294_960_720.jpg<\/a>\" + \"'}\";<\/p>\n\n<pre><code>        var request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest()\n        {\n            EndpointName = \"CG-model-v1-endpoint\",\n            ContentType = \"application\/json;utf-8\",\n            Body = new MemoryStream(Encoding.ASCII.GetBytes(JsonConvert.SerializeObject(requestBody)))\n\n        };\n\n\n        var awsClient = new AmazonSageMakerRuntimeClient(awsAccessKeyId: \"XXXX\", awsSecretAccessKey: \"XXX\", region: RegionEndpoint.EUCentral1);\n\n        try\n        {\n            var resposnse = await awsClient.InvokeEndpointAsync(request);\n\n        }\n        catch (Exception ex)\n        {\n\n            return ApiResponse&lt;bool&gt;.Create(false);\n        }\n<\/code><\/pre>",
        "Challenge_closed_time":1563872105436,
        "Challenge_comment_count":3,
        "Challenge_created_time":1563803017410,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while calling an AWS SageMaker end-point from a .Net core client using the AWS SDK. The error message \"The request signature we calculated does not match the signature you provided\" is displayed despite providing the required credentials. The user has shared the code snippet used for the request.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57147396",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":16.4,
        "Challenge_reading_time":16.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":19.1911183333,
        "Challenge_title":"Amazon Sage Maker: How to authenticate AWS SageMaker End-Point Request",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":222.0,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1435524174732,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bursa, Turkey",
        "Poster_reputation_count":11.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>I found the error , it was simply because of the request content-type,it had to be application\/json instead of application\/json;utf-8<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.4,
        "Solution_reading_time":1.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":23.5220988889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I get the following error inside the child runs in ML studio while doing an Automated ML experiment.  <\/p>\n<p>&quot;Identity does not have permissions for Microsoft.MachineLearningServices\/workspaces\/metadata\/artifacts\/write actions.&quot;  <\/p>\n<p>I am the owner of the resource group so I am not sure what the issue is.  <\/p>\n<p>Thanks  <\/p>",
        "Challenge_closed_time":1621971310163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621886630607,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a permission error while finishing an Automated ML experiment in ML studio. The error message stated that the user did not have permissions for Microsoft.MachineLearningServices\/workspaces\/metadata\/artifacts\/write actions, despite being the owner of the resource group.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/407580\/permission-error-while-finishing-auto-ml-run",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.7,
        "Challenge_reading_time":4.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":23.5220988889,
        "Challenge_title":"Permission error while finishing auto ml run",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":54,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello everyone, <a href=\"\/users\/na\/?userid=1321ce4c-8332-49c7-b902-2bcd4256debc\">@Shubham Miglani  <\/a> <a href=\"\/users\/na\/?userid=8886df29-ba7f-42f0-a932-a7883bbe54ea\">@Nick Schafer  <\/a>     <\/p>\n<p>We have identified the issue and a hot fix is rolling out.  It will be fixed in all regions by end of today. Sorry for the experience.     <\/p>\n<p>Regards,    <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":4.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1500629225150,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5939.0,
        "Answerer_view_count":886.0,
        "Challenge_adjusted_solved_time":0.9110955556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is the terraform shown in the docs:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.example.id\n  }\n}\n<\/code><\/pre>\n<p>I created a service catalog product with id: &quot;prod-xxxxxxxxxxxxx&quot;.\nWhen I substitute the service catalog product id into the above template,\nto get the following:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.prod-xxxxxxxxxxxxx\n  }\n}\n<\/code><\/pre>\n<p>I run terraform plan, but the following error occurs:<\/p>\n<pre><code>A managed resource &quot;aws_servicecatalog_product&quot; &quot;prod-xxxxxxxxxxxxx&quot; has not been declared in the root module.\n\n<\/code><\/pre>\n<p>What do I need to do to fix this error?<\/p>",
        "Challenge_closed_time":1654269294487,
        "Challenge_comment_count":2,
        "Challenge_created_time":1654264489180,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create an AWS Sagemaker project using Terraform, but encounters an error stating that a managed resource for the service catalog product has not been declared in the root module. The user is seeking guidance on how to fix this error.",
        "Challenge_last_edit_time":1654266014543,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72490682",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":16.3,
        "Challenge_reading_time":13.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.3348075,
        "Challenge_title":"How to create an aws sagemaker project using terraform?",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":298.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1653511725307,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>Since the documentation is lacking a bit of clarity, in order to have this work as in the example, you would first have to create the Service Catalog product in Terraform as well, e.g.:<\/p>\n<pre><code>resource &quot;aws_servicecatalog_product&quot; &quot;example&quot; {\n  name  = &quot;example&quot;\n  owner = [aws_security_group.example.id] # &lt;---- This would need to be created first\n  type  = aws_subnet.main.id # &lt;---- This would need to be created first\n\n  provisioning_artifact_parameters {\n    template_url = &quot;https:\/\/s3.amazonaws.com\/cf-templates-ozkq9d3hgiq2-us-east-1\/temp1.json&quot;\n  }\n\n  tags = {\n    foo = &quot;bar&quot;\n  }\n}\n<\/code><\/pre>\n<p>You can reference it then in the SageMaker project the same way as in the example:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.example.id\n  }\n}\n<\/code><\/pre>\n<p>Each of the resources that gets created has a set of attributes that can be accessed as needed by other resources, data sources or outputs. In order to understand how this works, I strongly suggest reading the documentation about referencing values [1]. Since you already created the Service Catalog product, the only thing you need to do is provide the string value for the product ID:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = &quot;prod-xxxxxxxxxxxxx&quot;\n  }\n}\n<\/code><\/pre>\n<p>When I can't understand what value is expected by an argument (e.g., <code>product_id<\/code> in this case), I usually read the docs and look for examples like in [2]. Note: That example is CloudFormation, but it can help you understand what type of a value is expected (e.g., string, number, bool).<\/p>\n<p>You could also import the created Service Catalog product into Terraform so you can manage it with IaC [3]. You should understand all the implications of <code>terraform import<\/code> though before trying it [4].<\/p>\n<hr \/>\n<p>[1] <a href=\"https:\/\/www.terraform.io\/language\/expressions\/references\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/expressions\/references<\/a><\/p>\n<p>[2] <a href=\"https:\/\/docs.amazonaws.cn\/en_us\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-project.html#aws-resource-sagemaker-project--examples--SageMaker_Project_Example\" rel=\"nofollow noreferrer\">https:\/\/docs.amazonaws.cn\/en_us\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-project.html#aws-resource-sagemaker-project--examples--SageMaker_Project_Example<\/a><\/p>\n<p>[3] <a href=\"https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/servicecatalog_product#import\" rel=\"nofollow noreferrer\">https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/servicecatalog_product#import<\/a><\/p>\n<p>[4] <a href=\"https:\/\/www.terraform.io\/cli\/commands\/import\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/cli\/commands\/import<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":9.0,
        "Solution_readability":17.8,
        "Solution_reading_time":40.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":270.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1631803441500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"mexico",
        "Answerer_reputation_count":1258.0,
        "Answerer_view_count":685.0,
        "Challenge_adjusted_solved_time":28.3038852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been using <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/08-model-monitoring.ipynb\" rel=\"nofollow noreferrer\">this<\/a> example of creating a Vertex AI monitoring job. It sends an email.<\/p>\n<pre><code>alerting_config = vertex_ai_beta.ModelMonitoringAlertConfig(\nemail_alert_config=vertex_ai_beta.ModelMonitoringAlertConfig.EmailAlertConfig(\n    user_emails=NOTIFY_EMAILS\n)\n<\/code><\/pre>\n<p>Is there any way to instead send a Pubsub message?<\/p>",
        "Challenge_closed_time":1660246628340,
        "Challenge_comment_count":1,
        "Challenge_created_time":1660144734353,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using an example of creating a Vertex AI monitoring job that sends an email. They are looking for a way to send a Pubsub message instead of an email.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73308825",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":18.1,
        "Challenge_reading_time":7.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":28.3038852778,
        "Challenge_title":"Can Vertex AI model monitoring job send a pubsub message instead of email?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":86.0,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>You can configure the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/using-model-monitoring#set-up-alerts\" rel=\"nofollow noreferrer\">alert to be sent to Cloud Logging<\/a>. To enable Cloud Logging alerts you have to set the <code>enableLogging<\/code> field on your <code>ModelMonitoringAlertConfig<\/code> configuration to <code>TRUE<\/code>. Then you can forward the logs to any service that Cloud Logging supports, Pub\/Sub is one of these.<\/p>\n<p>For this you\u2019ll need one of the following permissions:<\/p>\n<ul>\n<li>Owner (roles\/owner)<\/li>\n<li>Logging Admin (roles\/logging.admin)<\/li>\n<li>Logs Configuration Writer (roles\/logging.configWriter)<\/li>\n<\/ul>\n<p>Then you need to <a href=\"https:\/\/cloud.google.com\/logging\/docs\/export\/configure_export_v2#creating_sink\" rel=\"nofollow noreferrer\">create a sink<\/a>.<\/p>\n<p>After that you have created the sink you\u2019ll need to set the <a href=\"https:\/\/cloud.google.com\/logging\/docs\/export\/configure_export_v2#dest-auth\" rel=\"nofollow noreferrer\">destination permissions<\/a>.<\/p>\n<p>While Cloud Logging provides you with the ability to exclude logs from being ingested, you might want to consider keeping logs that help with supportability. Using these logs can help you quickly troubleshoot and identify issues with your applications.<\/p>\n<p>Logs routed to Pub\/Sub are generally available within seconds, with 99% of logs available in less than 60 seconds.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.7,
        "Solution_reading_time":18.63,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":155.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1286692213960,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":328.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":1559.6549622222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a MongoDB database (the Bitnami one) hosted on Azure. I want to import the data there to use it in my Azure Machine Learning experiment.<\/p>\n\n<p>Currently, I am exporting the data to <strong>.csv<\/strong> using <strong>mongoexport<\/strong> and then copy\/pasting it to the <strong>\"Enter Manually Data\"<\/strong> module. This is fine for small amounts of data but I would prefer to have a more robust technique for larger databases.<\/p>\n\n<p>I also thought about using the <strong>\"Import Data\"<\/strong> module from http url along with the <strong>http port (28017) of my mongodb<\/strong> instance but read this was not the recommended use of the http mongodb feature.<\/p>\n\n<p>Finally, I have installed <strong>cosmosDB<\/strong> instead of my bitnami MongoDB and it worked fine but this thing <strong>costs an arm<\/strong> when used with sitecore (it reaches around 100\u20ac per day) and we can't afford it so I switched back to by Mongo.<\/p>\n\n<p>So is there a better way to export data from Mongo to Azure ML ?<\/p>",
        "Challenge_closed_time":1504686538047,
        "Challenge_comment_count":0,
        "Challenge_created_time":1499071780183,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in importing data from their MongoDB database hosted on Azure to use it in their Azure Machine Learning experiment. They are currently exporting the data to .csv using mongoexport and manually entering it into the module, which is not feasible for larger databases. They have also considered using the \"Import Data\" module from http url along with the http port of their mongodb instance, but it is not recommended. They have tried using cosmosDB, but it is expensive, so they switched back to MongoDB. The user is seeking a better way to export data from Mongo to Azure ML.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44881303",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":13.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1559.6549622222,
        "Challenge_title":"Best way to import MongoDB data in Azure Machine Learning",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":724.0,
        "Challenge_word_count":169,
        "Platform":"Stack Overflow",
        "Poster_created_time":1441267698016,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":781.0,
        "Poster_view_count":97.0,
        "Solution_body":"<p>one way is to use a Python code block in AzureML, something like this:<\/p>\n\n<pre><code>import pandas as p\nimport pymongo as m\n\ndef azureml_main():\n    c = m.MongoClient(host='host_IP')\n    a = p.DataFrame(c.database_names())\n    return a\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":3.06,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.1427777778,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer is trying to setup Sagemaker studio. He is following our published instructions to set up using IAM: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/onboard-iam.html\n\nBut is getting an error: User:  arn:aws:iam:xxxx:user\/user1 is not authorized to perform: sagemaker:CreateDomain on resource: arn:aws:sagemaker: us-east-2:xxxx:domain\/yyyy\n\nHe has admin priviledges on the account and AmazonSageMakerFullAccess. We noticed that the AmazonSageMakerFullAccess policy actually has a limitation. You can perform all sagemaker actions, but not on a resource with arn \u201carn:aws:sagemaker:*:*:domain\/*\u201d. \nWe confirmed there are no other domains in that region with the CLI as you are only allowed one \u2013 so that isn\u2019t blocking.\nAnd aws sagemaker list-user-profiles returns no user profiles. \n\nHas anyone seen that error before or know the workaround? Should he create a custom policy to enable creating domains or would there be any implications of that? Are there specific permissions he should have so as to onboard using IAM?",
        "Challenge_closed_time":1586807470000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586796156000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while setting up Sagemaker Studio using IAM, specifically while trying to create a domain. Despite having admin privileges and AmazonSageMakerFullAccess, the policy has a limitation that prevents actions on resources with the arn \"arn:aws:sagemaker:*:*:domain\/*\". There are no other domains in the region, and the CLI returns no user profiles. The user is seeking a workaround and wondering if creating a custom policy would have any implications.",
        "Challenge_last_edit_time":1668609159168,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUyWQfPusnSHG6Ujfzx27o1w\/sagemaker-studio-create-domain-error",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":13.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":3.1427777778,
        "Challenge_title":"Sagemaker Studio - create domain error",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1619.0,
        "Challenge_word_count":146,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"A user with admin privileges would have access to `\"iam:CreateServiceLinkedRole\"` and `\"sagemaker:CreateDomain\"` actions, unless SCPs or permissions boundaries are involved. However, for the purpose of onboarding Amazon SageMaker Studio with limited permissions, I would grant the user [least privilege](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security_iam_id-based-policy-examples.html#security_iam_service-with-iam-policy-best-practices) by reviewing [Control Access to the Amazon SageMaker API by Using Identity-based Policies](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security_iam_id-based-policy-examples.html#api-access-policy) and [Actions, Resources, and Condition Keys for Amazon SageMaker](https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/list_amazonsagemaker.html) documentation:\n\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": \"sagemaker:CreateDomain\",\n        \"Resource\": \"arn:aws:sagemaker:<REGION>:<ACCOUNT-ID>:domain\/*\"\n    }\n\nNOTE: An AWS account is limited to one Domain, per region, see [CreateDomain](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateDomain.html).\n\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": \"iam:CreateServiceLinkedRole\",\n        \"Resource\": \"*\",\n        \"Condition\": {\n            \"StringEquals\": {\n                \"iam:AWSServiceName\": \"sagemaker.amazonaws.com\"\n            }\n        }\n    }\n\nCheers!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925577936,
        "Solution_link_count":4.0,
        "Solution_readability":28.1,
        "Solution_reading_time":17.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.3700394444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Having read Erland's article     <\/p>\n<p><a href=\"https:\/\/www.sqlservergeeks.com\/a-tip-about-using-python-for-regular-expressions-in-t-sql-by-erland-sommarskog\/\">https:\/\/www.sqlservergeeks.com\/a-tip-about-using-python-for-regular-expressions-in-t-sql-by-erland-sommarskog\/<\/a>    <\/p>\n<p>on <em>regular expressions<\/em> for SQL Server and the advantage of enabling sp_execute_external_script.  This works with the version of Anaconda3 that installs with SQL Server 2019.   There is an issue on the laptop used here because group policy (via the government policy) demands the use of FIPS.  For this reason, installing R or Java will fail (I suspect it doesn't sit well with managed copies of encryption).  Anaconda requires many over-rides during installation via <em>Privileged Management Administrator<\/em> and is installed locally through a painstaking process.       <\/p>\n<p>My bigger fear is that if I remove Anaconda3 to install using SQL, it will either fail similarly as did R and Java, or worse, I'll have trouble re-installing the version of Anaconda currently on the machine.    <\/p>\n<p>So again, the question is whether or not it is possible to enable sp_execute_external_script on SQL without installing R, Python, or Java.   I tried Java and R and both fail to install.   The Java and Python are already installed.<\/p>",
        "Challenge_closed_time":1670970968172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670958836030,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in enabling sp_execute_external_script on SQL Server due to the government policy that demands the use of FIPS, which causes the installation of R or Java to fail. The user has installed Anaconda3 locally through a painstaking process and is concerned about removing it to install using SQL, as it may fail similarly as did R and Java, or have trouble re-installing the current version of Anaconda. The user is seeking a solution to enable sp_execute_external_script on SQL without installing R, Python, or Java.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1127660\/will-sql-server-enable-sp-execute-external-script",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":18.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":3.3700394444,
        "Challenge_title":"Will SQL Server enable  sp_execute_external_script to work with a previously installed Anaconda3",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":181,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I am not sure that I understand the question. To use Python from SQL Server, you need to do one of:<\/p>\n<p>1) Use the Python that comes with SQL 2017 or SQL 2019.  <br \/>\n2) Install any version of Python you like as described on <a href=\"https:\/\/learn.microsoft.com\/en-us\/sql\/machine-learning\/install\/sql-machine-learning-services-windows-install-sql-2022\">https:\/\/learn.microsoft.com\/en-us\/sql\/machine-learning\/install\/sql-machine-learning-services-windows-install-sql-2022<\/a>. (That page is for SQL 2022, but it should be good for SQL 2019 as well.)<\/p>\n<p>If you don't have any external languages installed, you can still enable sp_execute_external_script, but you don't have much use for it, obviously.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.9,
        "Solution_reading_time":9.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":426.9047222222,
        "Challenge_answer_count":0,
        "Challenge_body":"When you have a global variable in the mlflow.yml file (e.g `mlruns: ${USER}\/mlruns`), the global variable is not replaced by its value even if the user has [registered a TemplatedConfigLoader](https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.config.TemplatedConfigLoader.html) in his project. This is due to `get_mlflow_config()` to manually recreate the default ConfigLoader.\r\n\r\nThis is part of the numerous issues that will  be fixed by #66.\r\n\r\n",
        "Challenge_closed_time":1602948810000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601411953000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where running mlflow projects from remote sources since version 1.28 causes a \"mlflow: not found\" error. The expected behavior is for remote-sourced mlflow projects to be supported as previously.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/72",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":9.2,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":426.9047222222,
        "Challenge_title":"mlflow.yml is not parsed properly when using TemplatedConfigLoader",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":64,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":11.5363191667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm Stumped.  <\/p>\n\n<p>I took my TensorFlow model and moved it up into SageMaker to try it out.  I put my own data up into an s3 bucket, set all the IAM roles\/access (or so I think).  I can read a file from s3.  I can push a new file to s3. I can read local directories from my SageMaker local directories.<\/p>\n\n<p><strong>I cannot traverse my s3 bucket directories.<\/strong>  I turned on logging and I get AccessDenied messages whenever I try access a URI of this format <strong>'s3:\/\/my_bucketName_here\/Directory_of_my_data\/'<\/strong>.<\/p>\n\n<p>Here is what I've done:\nI've confirmed that my notebook uses the AmazonSageMaker-ExecutionRole-***\nI've added AmazonSageMakerFullAccess Policy to that default role\nI've subsequently added AmazonS3FullAccess Policy as well<\/p>\n\n<p>I then created a bucket policy specifically granting s3:* access on the specific bucket to that specific role.<\/p>\n\n<p>Heck, I eventually made the bucket public with ListObjects = Yes.<\/p>\n\n<p>os.listdir() simply fails with file or directory not found and a lot message is created with AccessDenied. (TensorFlow libraries just didn't work, so I went with os.listdir() to simplify things.<\/p>\n\n<p>Finally, I test my access from the Policy Simulator - I selected the Role mentioned above, selected to test s3 and selected all 69 items and they all passed.<\/p>\n\n<p>But I continue to log AccessDenied and cannot actually list the contents of a directory from my SageMaker jupyter notebook.<\/p>\n\n<p>I'm at a loss.  Thoughts?<\/p>\n\n<p>EDIT:\nPer suggestion below, I have the following:\nbucket name contains sagemaker: '[redacted]-test-sagemaker'\nPublic access is off, and the only account is my root account.\n<code>\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::[redacted]-test-sagemaker\"\n            ]\n        },\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::[redacted]-test-sagemaker\/*\"\n            ]\n        }\n    ]\n}<\/code>\nand\narn:aws:iam::aws:policy\/AmazonSageMakerFullAccess<\/p>\n\n<p>Finally the bucket policy after the above failed:\n<code>{\n  \"Id\": \"Policy1534116031672\",\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Stmt1534116026409\",\n      \"Action\": \"s3:*\",\n      \"Effect\": \"Allow\",\n      \"Resource\": \"arn:aws:s3:::[redacted]-test-sagemaker\",\n      \"Principal\": {\n        \"AWS\": [\n          \"arn:aws:iam::[id]:role\/service-role\/AmazonSageMaker-ExecutionRole-***\"\n        ]\n      }\n    }\n  ]\n}<\/code><\/p>",
        "Challenge_closed_time":1534057177852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1534015647103,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with AWS SageMaker S3 os.listdir() access denied. The user has moved their TensorFlow model to SageMaker and uploaded their data to an S3 bucket. They have set all the IAM roles\/access, but they cannot traverse their S3 bucket directories. The user has tried adding AmazonSageMakerFullAccess Policy and AmazonS3FullAccess Policy to the default role, created a bucket policy, and made the bucket public with ListObjects = Yes. However, os.listdir() fails with file or directory not found, and a lot of messages are created with AccessDenied. The user has tested their access from the Policy Simulator, but they continue to log AccessDenied and cannot list the contents of a directory from their Sage",
        "Challenge_last_edit_time":1534116796512,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51803032",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":31.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":11.5363191667,
        "Challenge_title":"AWS SageMaker S3 os.listdir() Access denied",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":2961.0,
        "Challenge_word_count":304,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449682589303,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Phoenix, AZ, United States",
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>So you need to troubleshoot. Here are a few things to check:   <\/p>\n\n<p>0) Make sure the bucket is in the SageMaker region.<\/p>\n\n<p>1) Include the string \"sagemaker\" in your bucket name (e.g., <em>my_bucketName_here-sagemaker<\/em>, SageMaker has out of the box access to buckets named this way.<\/p>\n\n<p>2) Try using the SageMaker S3 <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py\" rel=\"nofollow noreferrer\">default_bucket()<\/a>:<\/p>\n\n<pre><code>import sagemaker\ns = sagemaker.Session()\ns.upload_data(path='somefile.csv', bucket=s.default_bucket(), key_prefix='data\/train')\n<\/code><\/pre>\n\n<p>3) Open terminal on the Notebook instance, to try to list your bucket using AWS CLI in bash:<\/p>\n\n<pre><code>aws iam get-user\naws s3 ls my_bucketName_here\n<\/code><\/pre>\n\n<p>Finally, pasting the bucket's access and resource policy in your question could help others to answer you.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.8,
        "Solution_reading_time":11.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":107.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1467035387036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":1277.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":191.1962875,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create an Azure DEVOPS ML Pipeline. The following code works 100% fine on Jupyter Notebooks, but when I run it in Azure Devops I get this error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;src\/my_custom_package\/data.py&quot;, line 26, in &lt;module&gt;\n    ws = Workspace.from_config()\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.8.7\/x64\/lib\/python3.8\/site-packages\/azureml\/core\/workspace.py&quot;, line 258, in from_config\n    raise UserErrorException('We could not find config.json in: {} or in its parent directories. '\nazureml.exceptions._azureml_exception.UserErrorException: UserErrorException:\n    Message: We could not find config.json in: \/home\/vsts\/work\/1\/s or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;We could not find config.json in: \/home\/vsts\/work\/1\/s or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.&quot;\n    }\n}\n<\/code><\/pre>\n<p>The code is:<\/p>\n<pre><code>#import\nfrom sklearn.model_selection import train_test_split\nfrom azureml.core.workspace import Workspace\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.core.experiment import Experiment\nfrom datetime import date\nfrom azureml.core import Workspace, Dataset\n\n\n\nimport pandas as pd\nimport numpy as np\nimport logging\n\n#getdata\nsubscription_id = 'mysubid'\nresource_group = 'myrg'\nworkspace_name = 'mlplayground'\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndataset = Dataset.get_by_name(workspace, name='correctData')\n\n\n#auto ml\nws = Workspace.from_config()\n\n\nautoml_settings = {\n    &quot;iteration_timeout_minutes&quot;: 2880,\n    &quot;experiment_timeout_hours&quot;: 48,\n    &quot;enable_early_stopping&quot;: True,\n    &quot;primary_metric&quot;: 'spearman_correlation',\n    &quot;featurization&quot;: 'auto',\n    &quot;verbosity&quot;: logging.INFO,\n    &quot;n_cross_validations&quot;: 5,\n    &quot;max_concurrent_iterations&quot;: 4,\n    &quot;max_cores_per_iteration&quot;: -1,\n}\n\n\n\ncpu_cluster_name = &quot;computecluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\nprint(compute_target)\nautoml_config = AutoMLConfig(task='regression',\n                             compute_target = compute_target,\n                             debug_log='automated_ml_errors.log',\n                             training_data = dataset,\n                             label_column_name=&quot;paidInDays&quot;,\n                             **automl_settings)\n\ntoday = date.today()\nd4 = today.strftime(&quot;%b-%d-%Y&quot;)\n\nexperiment = Experiment(ws, &quot;myexperiment&quot;+d4)\nremote_run = experiment.submit(automl_config, show_output = True)\n\nfrom azureml.widgets import RunDetails\nRunDetails(remote_run).show()\n\nremote_run.wait_for_completion()\n<\/code><\/pre>",
        "Challenge_closed_time":1612865840128,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612177533493,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while creating an Azure DevOps ML Pipeline. The error message indicates that the config.json file cannot be found in the specified directory or its parent directories. The code works fine on Jupyter Notebooks but not on Azure DevOps. The user has provided the code used for the pipeline.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65991587",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":16.0,
        "Challenge_reading_time":40.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":191.1962875,
        "Challenge_title":"AzureDevOPS ML Error: We could not find config.json in: \/home\/vsts\/work\/1\/s or in its parent directories",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":2339.0,
        "Challenge_word_count":265,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>There is something weird happening on your code, you are getting the data from a first workspace (<code>workspace = Workspace(subscription_id, resource_group, workspace_name)<\/code>), then using the resources from a second one (<code>ws = Workspace.from_config()<\/code>). I would suggest avoiding having code relying on two different workspaces, especially when you know that an underlying datasource can be registered (linked) to multiple workspaces (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#create-and-register-datastores\" rel=\"nofollow noreferrer\">documentation<\/a>).<\/p>\n<p>In general using a <code>config.json<\/code> file when instantiating a <code>Workspace<\/code> object will result in an interactive authentication. When your code will be processed and you will have a log asking you to reach a specific URL and enter a code. This will use your Microsoft account to verify that you are authorized to access the Azure resource (in this case your <code>Workspace('mysubid', 'myrg', 'mlplayground')<\/code>). This has its limitations when you start deploying the code onto virtual machines or agents, you will not always manually check the logs, access the URL and authenticate yourself.<\/p>\n<p>For this matter it is strongly recommended setting up more advanced authentication methods and personally I would suggest using the service principal one since it is simple, convinient and secure if done properly.\nYou can follow Azure's official documentation <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication#configure-a-service-principal\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":21.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":197.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1589738451347,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":179.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":0.1002166667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am having trouble contacting an AMLS web service hosted on AKS in a vnet. I am able to successfully provision AKS and deploy the models, but I am not able to access the web service using the Python requests module:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>headers = {'Content-Type':'application\/json',\n           'Authorization': 'Bearer ' + &lt;AKS_KEY&gt;}\nresp = requests.post(&lt;AKS_URI&gt;, json={&quot;data&quot;:{&quot;x&quot;: &quot;1&quot;}}, headers=headers)\nprint(resp.text)\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<blockquote>\n<p>Error: HTTPConnectionPool(host='', port=80): Max retries exceeded with url: &lt;AKS_URL&gt; (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f33f6035a10&gt;: Failed to establish a new connection: [Errno 110] Connection timed out'))<\/p>\n<\/blockquote>\n<p>However, I am able to successfully connect to the web service using Postman:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>curl --location --request POST &lt;AKS_URI&gt; \\\n--header 'Authorization: Bearer &lt;AKS_KEY&gt;' \\\n--header 'Content-Type: application\/json' \\\n--data-raw '{&quot;data&quot;: {&quot;x&quot;: &quot;1&quot;}}'\n<\/code><\/pre>\n<p>If I load the AKS service in my AMLS workspace <code>aks_service.run()<\/code> also gives me the same error message. I don't have these problems when I deploy without vnet integration.<\/p>\n<p>What could be causing this?<\/p>",
        "Challenge_closed_time":1600397953580,
        "Challenge_comment_count":4,
        "Challenge_created_time":1600383486567,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble connecting to an AMLS web service hosted on AKS in a vnet using Python requests module. They are able to provision AKS and deploy the models, but not able to access the web service. The error message received is \"Max retries exceeded with url\" and \"Failed to establish a new connection\". However, they are able to connect to the web service using Postman. The same error message is received when loading the AKS service in AMLS workspace. The issue is not encountered when deploying without vnet integration.",
        "Challenge_last_edit_time":1600397592800,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63947132",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":13.9,
        "Challenge_reading_time":19.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":4.0186147222,
        "Challenge_title":"Trouble connecting to AMLS web service on AKS using Python requests",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":76.0,
        "Challenge_word_count":164,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589738451347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":179.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>I fixed this by adding an inbound security rule enabled for the scoring endpoint in the NSG group that controls the virtual network.<\/p>\n<p>This should be done so that the scoring endpoint can be called from outside the virtual network (see <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-inferencing-vnet\" rel=\"nofollow noreferrer\">documentation<\/a>), but apparently Postman can figure out how to access the endpoint without this security rule!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":6.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1421803794992,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"St. Louis, MO, USA",
        "Answerer_reputation_count":373.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":16.7299083334,
        "Challenge_answer_count":4,
        "Challenge_body":"<p><strong>Problem:<\/strong>\nI am trying to setup a model in Sagemaker, however it fails when it comes to downloading the data.\nDoes anyone know what I am doing wrong?<\/p>\n\n<p><strong>What I did so far<\/strong>:\nIn order to avoid any mistakes on my side I decided to use the AWS tutorial:\ntensorflow_iris_dnn_classifier_using_estimators<\/p>\n\n<p>And I made only two changes:<\/p>\n\n<ol>\n<li>I copied the dataset to my own S3 instance. --> I tested if I could access \/ show the data and it worked.<\/li>\n<li>I edited the path to point to the new folder.<\/li>\n<\/ol>\n\n<p>This is the AWS source code:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators<\/a><\/p>\n\n<pre><code>%%time\nimport boto3\n\n# use the region-specific sample data bucket\nregion = boto3.Session().region_name\n#train_data_location = 's3:\/\/sagemaker-sample-data-{}\/tensorflow\/iris'.format(region)\ntrain_data_location = 's3:\/\/my-s3-bucket'\n\niris_estimator.fit(train_data_location)\n<\/code><\/pre>\n\n<p>And this is the error I get:<\/p>\n\n<pre><code>\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/interactiveshell.pyc in run_cell_magic(self, magic_name, line, cell)\n   2115             magic_arg_s = self.var_expand(line, stack_depth)\n   2116             with self.builtin_trap:\n-&gt; 2117                 result = fn(magic_arg_s, cell)\n   2118             return result\n   2119 \n\n&lt;decorator-gen-60&gt; in time(self, line, cell, local_ns)\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/magic.pyc in &lt;lambda&gt;(f, *a, **k)\n    186     # but it's overkill for just that one bit of state.\n    187     def magic_deco(arg):\n--&gt; 188         call = lambda f, *a, **k: f(*a, **k)\n    189 \n    190         if callable(arg):\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/magics\/execution.pyc in time(self, line, cell, local_ns)\n   1191         else:\n   1192             st = clock2()\n-&gt; 1193             exec(code, glob, local_ns)\n   1194             end = clock2()\n   1195             out = None\n\n&lt;timed exec&gt; in &lt;module&gt;()\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/tensorflow\/estimator.pyc in fit(self, inputs, wait, logs, job_name, run_tensorboard_locally)\n    314                 tensorboard.join()\n    315         else:\n--&gt; 316             fit_super()\n    317 \n    318     @classmethod\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/tensorflow\/estimator.pyc in fit_super()\n    293 \n    294         def fit_super():\n--&gt; 295             super(TensorFlow, self).fit(inputs, wait, logs, job_name)\n    296 \n    297         if run_tensorboard_locally and wait is False:\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/estimator.pyc in fit(self, inputs, wait, logs, job_name)\n    232         self.latest_training_job = _TrainingJob.start_new(self, inputs)\n    233         if wait:\n--&gt; 234             self.latest_training_job.wait(logs=logs)\n    235 \n    236     def _compilation_job_name(self):\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/estimator.pyc in wait(self, logs)\n    571     def wait(self, logs=True):\n    572         if logs:\n--&gt; 573             self.sagemaker_session.logs_for_job(self.job_name, wait=True)\n    574         else:\n    575             self.sagemaker_session.wait_for_job(self.job_name)\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/session.pyc in logs_for_job(self, job_name, wait, poll)\n   1126 \n   1127         if wait:\n-&gt; 1128             self._check_job_status(job_name, description, 'TrainingJobStatus')\n   1129             if dot:\n   1130                 print()\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/session.pyc in _check_job_status(self, job, desc, status_key_name)\n    826             reason = desc.get('FailureReason', '(No reason provided)')\n    827             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 828             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    829 \n    830     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Training job sagemaker-tensorflow-2019-01-03-16-32-16-435: Failed Reason: ClientError: Data download failed:S3 key: s3:\/\/my-s3-bucket\/\/sagemaker-tensorflow-2019-01-03-14-02-39-959\/source\/sourcedir.tar.gz has an illegal char sub-sequence '\/\/' in it\n<\/code><\/pre>",
        "Challenge_closed_time":1546594981603,
        "Challenge_comment_count":2,
        "Challenge_created_time":1546534753933,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while setting up a model in AWS Sagemaker. The error occurs when the data is being downloaded and the user has copied the dataset to their own S3 instance and edited the path to point to the new folder. The error message indicates that the data download failed due to an illegal character sub-sequence in the S3 key.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54026623",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":15.9,
        "Challenge_reading_time":57.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":9.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":16.7299083334,
        "Challenge_title":"AWS Sagemaker - ClientError: Data download failed",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":5145.0,
        "Challenge_word_count":367,
        "Platform":"Stack Overflow",
        "Poster_created_time":1546261116430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>The script is expecting 'bucket' to be bucket = Session().default_bucket() or your own. Have you tried setting bucket equal to your personal bucket?<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":1.95,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.103265,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have tried to read the dataset from datastore. Also tried to create the dataset also.<\/p>\n<p>The code for reading the dataset is below<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.from_config()\ndatastore = Datastore.get(ws, 'qdataset')\n<\/code><\/pre>\n<p>It works fine still now.<\/p>\n<pre><code>from azureml.core.dataset import Dataset\nsix_dataset = Dataset.get_by_name(workspace=ws, name='combined_classifier')\n<\/code><\/pre>\n<p>Also i have tried from <code>azureml.core import Dataset<\/code><\/p>\n<p>It shows the following error:<\/p>\n<p>2021-04-29 11:56:47.284077 | ActivityCompleted: Activity=_dataflow, HowEnded=Failure, Duration=0.0 [ms], Info = {'activity_id': 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'activity_name': '_dataflow', 'activity_type': 'InternalCall', 'app_name': 'dataset', 'source': 'azureml.dataset', 'version': '1.27.0', 'dataprepVersion': '2.14.2', 'subscription': '', 'run_id': '', 'resource_group': '', 'workspace_name': '', 'experiment_id': '', 'location': '', 'completionStatus': 'Failure', 'durationMs': 962.01}, Exception=AttributeError; module 'azureml.dataprep' has no attribute 'api'<\/p>\n<hr \/>\n<p>AttributeError Traceback (most recent call last)  <br \/>\n&lt;ipython-input-34-ac7a8d35da4d&gt; in &lt;module&gt;  <br \/>\n1 from azureml.core.dataset import Dataset  <br \/>\n----&gt; 2 six_dataset = Dataset.get_by_name(workspace=ws, name='combined_classifier')<\/p>\n<p>~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(*args, **kwargs)  <br \/>\n127 with _LoggerFactory.track_activity(logger, func.<strong>name<\/strong>, activity_type, custom_dimensions) as al:  <br \/>\n128 try:  <br \/>\n--&gt; 129 return func(*args, **kwargs)  <br \/>\n130 except Exception as e:  <br \/>\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):<\/p>\n<p>~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in get_by_name(workspace, name, version)  <br \/>\n87 :rtype: typing.Union[azureml.data.TabularDataset, azureml.data.FileDataset]  <br \/>\n88 &quot;&quot;&quot;  <br \/>\n---&gt; 89 dataset = AbstractDataset._get_by_name(workspace, name, version)  <br \/>\n90 AbstractDataset._track_lineage([dataset])  <br \/>\n91 return dataset<\/p>\n<p>~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _get_by_name(workspace, name, version)  <br \/>\n652 if not success:  <br \/>\n653 raise result  <br \/>\n--&gt; 654 dataset = _dto_to_dataset(workspace, result)  <br \/>\n655 warn_deprecated_blocks(dataset)  <br \/>\n656 return dataset<\/p>\n<p>~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_dataset_rest_helper.py in _dto_to_dataset(workspace, dto)  <br \/>\n93 registration=registration)  <br \/>\n94 if dto.dataset_type == _DATASET_TYPE_FILE:  <br \/>\n---&gt; 95 return FileDataset._create(  <br \/>\n96 definition=dataflow_json,  <br \/>\n97 properties=dto.latest.properties,<\/p>\n<p>~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(*args, **kwargs)  <br \/>\n127 with _LoggerFactory.track_activity(logger, func.<strong>name<\/strong>, activity_type, custom_dimensions) as al:  <br \/>\n128 try:  <br \/>\n--&gt; 129 return func(*args, **kwargs)  <br \/>\n130 except Exception as e:  <br \/>\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):<\/p>\n<p>~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _create(cls, definition, properties, registration, telemetry_info)  <br \/>\n555 from azureml.data._partition_format import parse_partition_format  <br \/>\n556  <br \/>\n--&gt; 557 steps = dataset._dataflow._get_steps()  <br \/>\n558 partition_keys = []  <br \/>\n559 for step in steps:<\/p>\n<p>~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(*args, **kwargs)  <br \/>\n127 with _LoggerFactory.track_activity(logger, func.<strong>name<\/strong>, activity_type, custom_dimensions) as al:  <br \/>\n128 try:  <br \/>\n--&gt; 129 return func(*args, **kwargs)  <br \/>\n130 except Exception as e:  <br \/>\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):<\/p>\n<p>~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _dataflow(self)  <br \/>\n215 raise UserErrorException('Dataset definition is missing. Please check how the dataset is created.')  <br \/>\n216 if self._registration and self._registration.workspace:  <br \/>\n--&gt; 217 dataprep().api._datastore_helper._set_auth_type(self._registration.workspace)  <br \/>\n218 if not isinstance(self._definition, dataprep().Dataflow):  <br \/>\n219 try:<\/p>\n<p>AttributeError: module 'azureml.dataprep' has no attribute 'api'<\/p>\n<p>Please give a solution to solve this<\/p>",
        "Challenge_closed_time":1619702571567,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619698599813,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while trying to access a dataset from a datastore using the Azure Machine Learning service. The error occurred when trying to read the dataset using the code provided, and the error message suggests that there is an issue with the dataprep module. The user is seeking a solution to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/377203\/error-while-accessing-the-dataset-from-a-datastore",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":62.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":1.103265,
        "Challenge_title":"Error while accessing the dataset from a datastore",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":393,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>It now worked..   <br \/>\nWe need to install azure-ml-api-sdk using this command  <\/p>\n<p>pip install azure-ml-api-sdk  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.4,
        "Solution_reading_time":1.54,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4111.2283333333,
        "Challenge_answer_count":0,
        "Challenge_body":"I'm not exactly sure on how to interpret this and what to refine. The error I'm getting is in the azureml.PipelineStep automl step. Here is the [link](https:\/\/mlworkspace.azure.ai\/portal\/subscriptions\/ff2e23ae-7d7c-4cbd-99b8-116bb94dca6e\/resourceGroups\/RG-ITSMLTeam-Dev\/providers\/Microsoft.MachineLearningServices\/workspaces\/avadevitsmlsvc\/experiments\/deal-deal-nema\/runs\/7abe9617-ac79-413b-8843-7fd3878313f0).\r\n\r\nWhen my dataset has more than ~1200 features, I consistently get this error, but when there are fewer features it works fine. Is there some limitation here?",
        "Challenge_closed_time":1581034133000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1566233711000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to register a model using Jupyter Notebook and is receiving an error message stating \"HttpOperationError: Operation returned an invalid status code 'Service invocation failed!Request: GET https:\/\/cert-westeurope.experiments.azureml.net\/rp\/workspaces'\". The code being used to register the model is provided in the post.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/534",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.08,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":4111.2283333333,
        "Challenge_title":"Azureml Automl \"Error: Null\" Vague Error",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":59,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Nema. Unfortunately I don't have access to your workspace. Would you provide more details on the failed experiment such as experiment\/pipeline id so that I can take a look at the logs of it? Hi Sonny, here is the run id: `eb6f111d-1251-40d2-b745-e3c4fbb31fcf` Thank you for the runid. I found automl setup has been timed out after some time. I will work with automl team for more details.  It seems to have been a one-off random occurrence. Considering solved. Somehow I lost track on this. You can let me know if you have any further issues.  ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.4,
        "Solution_reading_time":6.6,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":96.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1508836189288,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Finland",
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":5.5372291667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need to connect with Azure ML Workspace during deployment over container instance.<\/p>\n<pre><code>ws = Workspace(subscription_id=&quot;your-sub-id&quot;,\n              resource_group=&quot;your-resource-group-id&quot;,\n              workspace_name=&quot;your-workspace-name&quot;\n              )\n<\/code><\/pre>\n<p>Interactive Authentication to the ML Workspace prompts to login and then fails with below error message.<\/p>\n<pre><code>AttributeError: 'BasicTokenAuthentication' object has no attribute 'get_token'\n<\/code><\/pre>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication#interactive-authentication\" rel=\"nofollow noreferrer\">i have been following this Azure Authentication document.<\/a><\/p>\n<p>Any help is much appreciated.<\/p>",
        "Challenge_closed_time":1600775316867,
        "Challenge_comment_count":3,
        "Challenge_created_time":1600761544463,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an authentication error while trying to connect with Azure ML Workspace during deployment over container instance. The error message \"AttributeError: 'BasicTokenAuthentication' object has no attribute 'get_token'\" is displayed while attempting interactive authentication to the ML Workspace. The user has been following the Azure Authentication document for help.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64005433",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":20.4,
        "Challenge_reading_time":10.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":3.8256677778,
        "Challenge_title":"Azure ML operations : workspace authentication error",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":706.0,
        "Challenge_word_count":63,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530258066240,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":125.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>For me this was fixed by updating azureml-core from 1.13.0 to 1.14.0.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1600781478488,
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":0.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645519217332,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":336.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":166.7237425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How can I store additional information in an <code>optuna trial<\/code> when using it via the Hydra sweep plugin?<\/p>\n<p>My use case is as follows:\nI want to optimize a bunch of hyperparameters. I am storing all reproducibility information of all experiments (i.e., trials) in a separate database.\nI know I can get the best values via <code>optuna.load_study().best_params<\/code> or even <code>best_trial<\/code>. However, that only allows me to replicate the experiment - potentially this takes quite some time. To overcome this issue, I need to somehow link it to my own database. I would like to store the ID of my own database somewhere in the <code>trial<\/code> object.<\/p>\n<p>Without using Hydra, I suppose I'd set <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/003_attributes.html#sphx-glr-tutorial-20-recipes-003-attributes-py\" rel=\"nofollow noreferrer\">User Attributes<\/a>. However, with Hydra <a href=\"https:\/\/github.com\/facebookresearch\/hydra\/blob\/535dc7aacfe607e25848b2c4b8068317095a730b\/plugins\/hydra_optuna_sweeper\/hydra_plugins\/hydra_optuna_sweeper\/_impl.py#L183\" rel=\"nofollow noreferrer\">abstracting all that away<\/a>, there seems no option to do so.<\/p>\n<p>I know that I can just query my own database for the exact combination of best params that optuna found, but that just seems like a difficult solution to a simple problem.<\/p>\n<p>Some minimal code:<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>from dataclasses import dataclass\n\nimport hydra\nfrom hydra.core.config_store import ConfigStore\nfrom omegaconf import MISSING\n\n\n@dataclass\nclass TrainConfig:\n    x: float | int = MISSING\n    y: int = MISSING\n    z: int | None = None\n\n\nConfigStore.instance().store(name=&quot;config&quot;, node=TrainConfig)\n\n\n@hydra.main(version_base=None, config_path=&quot;conf&quot;, config_name=&quot;sweep&quot;)\ndef sphere(cfg: TrainConfig) -&gt; float:\n    x: float = cfg.x\n    y: float = cfg.y\n    return x**2 + y**2\n\n\nif __name__ == &quot;__main__&quot;:\n    sphere()\n<\/code><\/pre>\n<pre class=\"lang-yaml prettyprint-override\"><code>defaults:\n  - override hydra\/sweeper: optuna\n  - override hydra\/sweeper\/sampler: tpe\n\nhydra:\n  sweeper:\n    sampler:\n      seed: 123\n    direction: minimize\n    study_name: sphere\n    storage: sqlite:\/\/\/trials.db\n    n_trials: 20\n    n_jobs: 1\n    params:\n      x: range(-5.5, 5.5, step=0.5)\n      y: choice(-5 ,0 ,5)\n      z: choice(0, 3, 5)\n\nx: 1\ny: 1\nz: 1\n<\/code><\/pre>",
        "Challenge_closed_time":1657793209687,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657194655610,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to store additional information in an Optuna trial when using it via the Hydra sweep plugin. They are optimizing a bunch of hyperparameters and storing all reproducibility information of all experiments in a separate database. However, they need to link it to their own database and would like to store the ID of their own database somewhere in the trial object. Without using Hydra, they would set User Attributes, but with Hydra abstracting all that away, there seems to be no option to do so.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72897321",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":31.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":166.2650213889,
        "Challenge_title":"Store user attributes in Optuna Sweeper plugin for Hydra",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":83.0,
        "Challenge_word_count":275,
        "Platform":"Stack Overflow",
        "Poster_created_time":1645519217332,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":336.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>A hacky solution via the <a href=\"https:\/\/hydra.cc\/docs\/plugins\/optuna_sweeper\/#experimental--custom-search-space-optimization\" rel=\"nofollow noreferrer\"><code>custom_search_space<\/code><\/a>.<\/p>\n<pre><code>hydra:\n  sweeper:\n    sampler:\n      seed: 123\n    direction: minimize\n    study_name: sphere\n    storage: sqlite:\/\/\/trials.db\n    n_trials: 20\n    n_jobs: 1\n    params:\n      x: range(-5.5, 5.5, step=0.5)\n      y: choice(-5 ,0 ,5)\n      z: choice([0, 1], [2, 3], [2, 5])\n    custom_search_space: package.run.configure\n<\/code><\/pre>\n<pre><code>def configure(_, trial: Trial) -&gt; None:\n    trial.set_user_attr(&quot;experiment_db_id&quot;, 123456)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1657794861083,
        "Solution_link_count":1.0,
        "Solution_readability":15.6,
        "Solution_reading_time":8.21,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":52.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1443426419048,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Challenge_adjusted_solved_time":7.5235611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Since connecting to Azure SQL database from \u201cExecute R Script\u201d module in \u201cAzure Machine Learning Studio\u201d is not possible, and using Import Data modules (a.k.a Readers) is the only recommended approach, my question is that what can I do when I need more than 2 datasets as input for \"Execute R Script module\"?<\/p>\n\n<pre><code>\/\/ I'm already doing the following to get first 2 datasets,\ndataset1 &lt;- maml.mapInputPort(1)\ndataset2 &lt;- maml.mapInputPort(2)\n<\/code><\/pre>\n\n<p>How can I \"import\" a dataset3?<\/p>",
        "Challenge_closed_time":1491492855983,
        "Challenge_comment_count":1,
        "Challenge_created_time":1491465771163,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge while using the \"Execute R Script\" module in \"Azure Machine Learning Studio\" as they are unable to connect to Azure SQL database from the module. They are using Import Data modules to get the datasets, but they need more than 2 datasets as input for the module and are unsure how to import a third dataset.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/43249220",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.0,
        "Challenge_reading_time":7.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":7.5235611111,
        "Challenge_title":"Need more than 2 datasets for \u201cExecute R Script\u201d module in \u201cAzure Machine Learning Studio\u201d",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":337.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1487901477287,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>One thing you can do is combining two data-sets together and selecting the appropriate fields using the R script. That would be an easy workaround.   <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":1.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.7854666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to export the data from my batch flow with the use of the data export module. Tried multiple file shares but get the following error.    <\/p>\n<p>User program failed with UserError: ScriptExecutionException was caused by WriteStreamsException.    <br \/>\n  WriteStreamsException was caused by UnexpectedException.    <br \/>\n    Unexpected exception while writing files with writer 'delimited'.  <br \/>\n      StreamAccessException was caused by NotFoundException.  <br \/>\n        File Share '[REDACTED]' does not exist at '[REDACTED]'.  <br \/>\n| session_id=e1ee8699-3a94-4ea6-ab5d-f5bb945d56f3    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/32402-image.png?platform=QnA\" alt=\"32402-image.png\" \/>    <\/p>\n<p>The file share name is the Azure named file share name or is this something else. Cannot find an eample.    <\/p>\n<p>The export works to local ML workspace, but this doesn't accept folders creation.    <\/p>",
        "Challenge_closed_time":1602726430747,
        "Challenge_comment_count":1,
        "Challenge_created_time":1602687603067,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the data export module in Azure machine learning while trying to export data from a batch flow. The error message indicates that the file share specified does not exist. The user is unsure if the file share name is the Azure named file share name or something else. The export works to the local ML workspace but does not accept folder creation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/126502\/azure-machine-learning-data-export-module-failure",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":12.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":10.7854666667,
        "Challenge_title":"Azure machine learning data export module failure",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":116,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,    <\/p>\n<p>Thanks for reaching out to us. The file share name is the name of FILE_SHARE_CONTAINER     <\/p>\n<p>Please refer to below document for more details:    <\/p>\n<p>Export data module: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/export-data\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/export-data<\/a>    <\/p>\n<p>Data storage - Azure File Share: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-file-share\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-file-share<\/a>    <\/p>\n<p>Please let me know if you have more questions.    <\/p>\n<p>Regards,    <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.9,
        "Solution_reading_time":9.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1344510903550,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hannover, Germany",
        "Answerer_reputation_count":33554.0,
        "Answerer_view_count":2182.0,
        "Challenge_adjusted_solved_time":3240.8772311111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The experiment software <a href=\"http:\/\/sacred.readthedocs.io\/en\/latest\/quickstart.html\" rel=\"nofollow noreferrer\">sacred<\/a> was run without MongoDB in the background with a configured <a href=\"http:\/\/sacred.readthedocs.io\/en\/latest\/observers.html#mongo-observer\" rel=\"nofollow noreferrer\">mongo-observer<\/a>. When it tried to write the settings to MongoDB, this failed, creating the file <code>\/tmp\/sacred_mongo_fail__eErwU.pickle<\/code>, with the message<\/p>\n\n<pre><code>Warning: saving to MongoDB failed! Stored experiment entry in \/tmp\/sacred_mongo_fail__eErwU.pickle\nTraceback (most recent calls WITHOUT Sacred internals):\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sacred\/observers\/mongo.py\", line 127, in started_event\n    self.run_entry[experiment][sources] = self.save_sources(ex_info)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sacred\/observers\/mongo.py\", line 239, in save_sources\n    file = self.fs.find_one({filename: abs_path, md5: md5})\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/gridfs\/__init__.py\", line 261, in find_one\n    for f in self.find(filter, *args, **kwargs):\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/gridfs\/grid_file.py\", line 658, in next\n    next_file = super(GridOutCursor, self).next()\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 1114, in next\n    if len(self.__data) or self._refresh():\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 1036, in _refresh\n    self.__collation))\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 873, in __send_message\n    **kwargs)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/mongo_client.py\", line 888, in _send_message_with_response\n    server = topology.select_server(selector)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/topology.py\", line 214, in select_server\n    address))\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/topology.py\", line 189, in select_servers\n    self._error_message(selector))\nServerSelectionTimeoutError: localhost:27017: [Errno 111] Connection refused\n<\/code><\/pre>\n\n<p>How can this pickle file be imported into MongoDB manually?<\/p>",
        "Challenge_closed_time":1510651584203,
        "Challenge_comment_count":0,
        "Challenge_created_time":1498984762217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an issue while running the experiment software 'sacred' without MongoDB in the background. As a result, the software failed to write the settings to MongoDB, creating a pickle file instead. The user is seeking guidance on how to import this pickle file into MongoDB manually.",
        "Challenge_last_edit_time":1498985131100,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44868932",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.5,
        "Challenge_reading_time":29.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":3240.783885,
        "Challenge_title":"How to import the pickle file if sacred failed to connect to MongoDB",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":537.0,
        "Challenge_word_count":171,
        "Platform":"Stack Overflow",
        "Poster_created_time":1344510903550,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hannover, Germany",
        "Poster_reputation_count":33554.0,
        "Poster_view_count":2182.0,
        "Solution_body":"<ol>\n<li>Load the pickle file, <\/li>\n<li>set the <code>_id<\/code>,<\/li>\n<li>insert <\/li>\n<\/ol>\n\n<p><\/p>\n\n<pre><code>db = pymongo.MongoClient().sacred\nentry = pickle.load(open('\/tmp\/sacred_mongo_fail__eErwU.pickle'))\nentry['_id'] = list(db.runs.find({}, {\"_id\": 1}))[-1]['_id']\ndb.runs.insert_one(entry)\n<\/code><\/pre>\n\n<p>This is quick and dirty, depends on the <code>find<\/code> to list objects in order, and could use <a href=\"https:\/\/stackoverflow.com\/questions\/2138873\/cleanest-way-to-get-last-item-from-python-iterator\">Cleanest way to get last item from Python iterator<\/a> instead of <code>list(...)[-1]<\/code>, but it should work.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1510652289132,
        "Solution_link_count":1.0,
        "Solution_readability":10.0,
        "Solution_reading_time":8.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":57.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":1587438027383,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":7334.0,
        "Answerer_view_count":674.0,
        "Challenge_adjusted_solved_time":1.7229975,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to register a dataset from ADLS Gen2 in my Azure Machine Learning workspace (<code>azureml-core==1.12.0<\/code>). Given that service principal information is not required in the Python SDK <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py#register-azure-data-lake-gen2-workspace--datastore-name--filesystem--account-name--tenant-id-none--client-id-none--client-secret-none--resource-url-none--authority-url-none--protocol-none--endpoint-none--overwrite-false-\" rel=\"noreferrer\">documentation<\/a> for <code>.register_azure_data_lake_gen2()<\/code>, I successfully used the following code to register ADLS gen2 as a datastore:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Datastore\n\nadlsgen2_datastore_name = os.environ['adlsgen2_datastore_name']\naccount_name=os.environ['account_name'] # ADLS Gen2 account name\nfile_system=os.environ['filesystem']\n\nadlsgen2_datastore = Datastore.register_azure_data_lake_gen2(\n    workspace=ws,\n    datastore_name=adlsgen2_datastore_name,\n    account_name=account_name, \n    filesystem=file_system\n)\n<\/code><\/pre>\n<p>However, when I try to register a dataset, using<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Dataset\nadls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)\ndata = Dataset.Tabular.from_delimited_files((adls_ds, 'folder\/data.csv'))\n<\/code><\/pre>\n<p>I get an error<\/p>\n<blockquote>\n<p>Cannot load any data from the specified path. Make sure the path is accessible and contains data.\n<code>ScriptExecutionException<\/code> was caused by <code>StreamAccessException<\/code>.\nStreamAccessException was caused by AuthenticationException.\n<code>'AdlsGen2-ReadHeaders'<\/code> for '[REDACTED]' on storage failed with status code 'Forbidden' (This request is not authorized to perform this operation using this permission.), client request ID &lt;CLIENT_REQUEST_ID&gt;, request ID &lt;REQUEST_ID&gt;. Error message: [REDACTED]\n| session_id=&lt;SESSION_ID&gt;<\/p>\n<\/blockquote>\n<p>Do I need the to enable the service principal to get this to work? Using the ML Studio UI, it appears that the service principal is required even to register the datastore.<\/p>\n<p>Another issue I noticed is that AMLS is trying to access the dataset here:\n<code>https:\/\/adls_gen2_account_name.**dfs**.core.windows.net\/container\/folder\/data.csv<\/code> whereas the actual URI in ADLS Gen2 is: <code>https:\/\/adls_gen2_account_name.**blob**.core.windows.net\/container\/folder\/data.csv<\/code><\/p>",
        "Challenge_closed_time":1600155716360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600115991930,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to register a dataset from ADLS Gen2 in their Azure Machine Learning workspace using Python SDK. They successfully registered ADLS Gen2 as a datastore but encountered an error when trying to register a dataset. The error message suggests an authentication issue, and the user is unsure if they need to enable the service principal. Additionally, the user noticed that AMLS is trying to access the dataset using the wrong URI.",
        "Challenge_last_edit_time":1600160631356,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63891547",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":18.0,
        "Challenge_reading_time":34.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":11.0345638889,
        "Challenge_title":"How to connect AMLS to ADLS Gen 2?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":3331.0,
        "Challenge_word_count":219,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589738451347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":179.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>According to this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-data-lake-storage-generation-2\" rel=\"noreferrer\">documentation<\/a>,you need to enable the service principal.<\/p>\n<p>1.you need to register your application and grant the service principal with <strong>Storage Blob Data Reader access<\/strong>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/FZl8O.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FZl8O.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>2.try this code:<\/p>\n<pre><code>adlsgen2_datastore = Datastore.register_azure_data_lake_gen2(workspace=ws,\n                                                             datastore_name=adlsgen2_datastore_name,\n                                                             account_name=account_name,\n                                                             filesystem=file_system,\n                                                             tenant_id=tenant_id,\n                                                             client_id=client_id,\n                                                             client_secret=client_secret\n                                                             )\n\nadls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)\ndataset = Dataset.Tabular.from_delimited_files((adls_ds,'sample.csv'))\nprint(dataset.to_pandas_dataframe())\n<\/code><\/pre>\n<p><strong>Result:<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/50mit.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/50mit.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1600166834147,
        "Solution_link_count":5.0,
        "Solution_readability":25.5,
        "Solution_reading_time":16.17,
        "Solution_score_count":9.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":16.8847241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How to mount the dataset in ML Studio using python sdk ?    <br \/>\nWhat are the different ways and which is the right one ?    <\/p>\n<p>Can we create dataset pointing to two different stores ?    <\/p>\n<p>Also where can I learn more about azure machine learning ?<\/p>",
        "Challenge_closed_time":1661469512520,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661408727513,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to mount a dataset in ML Studio using Python SDK and is inquiring about the different methods available. They also want to know if it is possible to create a dataset that points to two different stores. Additionally, they are looking for resources to learn more about Azure Machine Learning.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/981126\/mounting-the-dataset-in-ml-workspace",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":2.5,
        "Challenge_reading_time":3.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":16.8847241667,
        "Challenge_title":"Mounting the dataset in ML Workspace",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":50,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=b2d28b4e-27a0-4d31-9f64-5723bfe88382\">@V JEEVA  <\/a>     <\/p>\n<p>Thanks for using Microsoft Q&amp;A platform. Let me answer your questions one by one.    <\/p>\n<p><em>where can I learn more about azure machine learning<\/em>    <br \/>\nThe best way to learn Azure Machine Learning is the documentation, please refer to - <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning\/#documentation\">https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning\/#documentation<\/a>    <\/p>\n<p><em>How to mount the dataset in ML Studio using python sdk ?<\/em>    <br \/>\nGenerally there are two ways to work with data in Azure Machine Learning -    <br \/>\nUse datastores - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-datastore?tabs=cli-identity-based-access%2Ccli-adls-identity-based-access%2Ccli-azfiles-account-key%2Ccli-adlsgen1-identity-based-access\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-datastore?tabs=cli-identity-based-access%2Ccli-adls-identity-based-access%2Ccli-azfiles-account-key%2Ccli-adlsgen1-identity-based-access<\/a>    <br \/>\nUse data assets - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-datastore?tabs=cli-identity-based-access%2Ccli-adls-identity-based-access%2Ccli-azfiles-account-key%2Ccli-adlsgen1-identity-based-access\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-datastore?tabs=cli-identity-based-access%2Ccli-adls-identity-based-access%2Ccli-azfiles-account-key%2Ccli-adlsgen1-identity-based-access<\/a>    <\/p>\n<p><em>What are the different ways and which is the right one ?<\/em>    <br \/>\nIt depends on your need. Compared to data assets and datastore, the benefits of creating data assets are:    <br \/>\nYou can share and reuse data with other members of the team such that they do not need to remember file locations.    <br \/>\nYou can seamlessly access data during model training (on any supported compute type) without worrying about connection strings or data paths.    <br \/>\nYou can version the data.    <\/p>\n<p><em>Can we create dataset pointing to two different stores ?<\/em>    <br \/>\nIf you need to assembly data, you may want to consider data assets. By creating a data asset, you create a reference to the data source location, along with a copy of its metadata. Because the data remains in its existing location, you incur no extra storage cost, and don't risk the integrity of your data sources. You can create Data from datastores, Azure Storage, public URLs, and local files.    <\/p>\n<p>I hope this helps!    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.9,
        "Solution_reading_time":35.07,
        "Solution_score_count":2.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":277.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":32.7005666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/GU4v0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/GU4v0.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>How is the above distance calculated and can I set my own threshold?<\/p>",
        "Challenge_closed_time":1662747458550,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662629736510,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on how the Baseline drift distance is calculated in data quality monitoring in Sagemaker and whether they can set their own threshold.",
        "Challenge_last_edit_time":1663220877288,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73646830",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":4.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":32.7005666667,
        "Challenge_title":"What inference can be made out of Baseline drift distance in data quality monitoring sagemaker?",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":55.0,
        "Challenge_word_count":37,
        "Platform":"Stack Overflow",
        "Poster_created_time":1660119311500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>You can find information on how the distributions are compared here (see <code>distribution_constraints<\/code> in the table):<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-byoc-constraints.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-byoc-constraints.html<\/a><\/p>\n<p>You can change the threshold in the constraint file to what you would like.<\/p>\n<p>The baseline computes baseline schema constraints and statistics for each feature using Deequ. If youwould like more details you can take a look at the implementation here:<\/p>\n<p><a href=\"https:\/\/github.com\/awslabs\/deequ\/blob\/master\/src\/main\/scala\/com\/amazon\/deequ\/analyzers\/Distance.scala\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/deequ\/blob\/master\/src\/main\/scala\/com\/amazon\/deequ\/analyzers\/Distance.scala<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":22.8,
        "Solution_reading_time":11.71,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1435766573232,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":10645.0,
        "Answerer_view_count":1173.0,
        "Challenge_adjusted_solved_time":37311.0782480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am sort of new to Python, so I probably don't understand fully how to exactly import the libraries correctly into Azure ML.<\/p>\n\n<p>I have a bunch of data stored in Table storage which I have local Python code to successfully join all of them as a preparation for the ML experiment. I learned that AzureML environment does not have the Azure-Storage libraries installed, and therefore procceded the steps according <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn955437.aspx\" rel=\"nofollow noreferrer\">this<\/a> to upload a ZIP file containing the Azure-storage libraries that I found under anaconda3\\lib\\site-packages. I took all of the azure directories and shoved them under one single zip file and followed the bottom of the document in the link to upload the zip file as a DataSet and attach the dataset to an Execute Python script node in ML.<\/p>\n\n<p>I am getting errors like this when I try to run the node:<\/p>\n\n<pre><code>requestId = 825883c7ccb74f7e869e68e60d3cd919 errorComponent=Module. taskStatusCode=400. e \"C:\\pyhome\\lib\\ssl.py\", line 856, in send return self._sslobj.write(data) File \"C:\\pyhome\\lib\\ssl.py\", line 581, in write return self._sslobj.write(data)socket.timeout: The write operation timed outDuring handling of the above exception, another exception occurred:Traceback (most recent call last): File \"C:\\pyhome\\lib\\site-packages\\requests\\adapters.py\", line 376, in send timeout=timeout File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 609, in urlopen _stacktrace=sys.exc_info()[2]) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\util\\retry.py\", line 247, in increment raise six.reraise(type(error), error, _stacktrace) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\packages\\six.py\", line 309, in reraise raise value.with_traceback(tb) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 559, in urlopen body=body, headers=headers) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 353, in _make_request conn.request(method, url, **httplib_request_kw) File \"C:\\pyhome\\lib\\http\\client.py\", line 1083, in request self._send_request(method, url, body, headers) File \"C:\\pyhome\\lib\\http\\client.py\", line 1128, in _send_request self.endheaders(body) File \"C:\\pyhome\\lib\\http\\client.py\", line 1079, in endheaders self._send_output(message_body) File \"C:\\pyhome\\lib\\http\\client.py\", line 911, in _send_output self.send(msg) File \"C:\\pyhome\\lib\\http\\client.py\", line 885, in send self.sock.sendall(data) File \"C:\\pyhome\\lib\\ssl.py\", line 886, in sendall v = self.send(data[count:]) File \"C:\\pyhome\\lib\\ssl.py\", line 856, in send return self._sslobj.write(data) File \"C:\\pyhome\\lib\\ssl.py\", line 581, in write return self._sslobj.write(data)requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', timeout('The write operation timed out',))During handling of the above exception, another exception occurred:Traceback (most recent call last): File \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 221, in _perform_request response = self._httpclient.perform_request(request) File \"c:\\temp\\script bundle\\azure\\storage\\_http\\httpclient.py\", line 114, in perform_request proxies=self.proxies) File \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 468, in request resp = self.send(prep, **send_kwargs) File \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 576, in send r = adapter.send(request, **kwargs) File \"C:\\pyhome\\lib\\site-packages\\requests\\adapters.py\", line 426, in send raise ConnectionError(err, request=request)requests.exceptions.ConnectionError: ('Connection aborted.', timeout('The write operation timed out',))During handling of the above exception, another exception occurred:Traceback (most recent call last): File \"C:\\server\\invokepy.py\", line 199, in batch odfs = mod.azureml_main(*idfs) File \"C:\\temp\\fa22884a19884f658d411dc0bdf05715.py\", line 33, in azureml_main data = table_service.query_entities(table_name) File \"c:\\temp\\script bundle\\azure\\storage\\table\\tableservice.py\", line 728, in query_entities resp = self._query_entities(*args, **kwargs) File \"c:\\temp\\script bundle\\azure\\storage\\table\\tableservice.py\", line 795, in _query_entities operation_context=_context) File \"c:\\temp\\script bundle\\azure\\storage\\table\\tableservice.py\", line 1093, in _perform_request return super(TableService, self)._perform_request(request, parser, parser_args, operation_context) File \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 279, in _perform_request raise ex File \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 251, in _perform_request raise AzureException(ex.args[0])azure.common.AzureException: ('Connection aborted.', timeout('The write operation timed out',))Process returned with non-zero exit code \n<\/code><\/pre>\n\n<p>I am not sure what I am doing wrong<\/p>",
        "Challenge_closed_time":1511661620620,
        "Challenge_comment_count":0,
        "Challenge_created_time":1511629802230,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble importing Azure Storage libraries into AzureML. They have uploaded a ZIP file containing the libraries as a DataSet and attached it to an Execute Python script node, but are encountering errors related to connection timeouts when trying to run the node. The user is unsure of what they are doing wrong.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47488544",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":65.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":62,
        "Challenge_solved_time":8.8384416667,
        "Challenge_title":"Using Azure Storage libraries in AzureML - Custom python library",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":150.0,
        "Challenge_word_count":471,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340380852680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":73.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>In Azure ML Studio, Python scripts (and R scripts for that matter) run in a sandbox so cannot access resources over the network. See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts?#limitations\" rel=\"nofollow noreferrer\">limitations<\/a>:<\/p>\n<blockquote>\n<p>The Execute Python Script currently has the following limitations:<\/p>\n<ol>\n<li>Sandboxed execution. The Python runtime is currently sandboxed and, as\na result, does not allow access to the network...<\/li>\n<\/ol>\n<\/blockquote>\n<p>So if you want to read from a blob use the separate Import Data module and if you want to write to a blob use the separate Export Data module.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1645949683923,
        "Solution_link_count":1.0,
        "Solution_readability":11.5,
        "Solution_reading_time":8.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":72.28,
        "Challenge_answer_count":0,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nI have a problem saving xgboost run in mlflow server. The run has a status of UNFINISHED, no metrics or artifacts are created. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/101572186\/183577670-53398204-debf-428b-8b0c-3c7ca83f4785.png)\r\n\r\nWhen I use `mlflow ui` everything is fine, but when I run mlflow server with SQLite as backend store the problem occurs.\r\nCommand used to run mlflow server- `mlflow server --host 0.0.0.0 --port 5000 --default-artifact-root \/mlflow\/artifacts\/ --backend-store-uri sqlite:\/\/\/\/\/mlflow\/experiments\/mlflow.db`\n\n### Reproducible Example\n\n```python\nimport mlflow\r\nfrom pycaret.classification import *\r\nimport pandas as pd\r\n\r\nmlflow.set_tracking_uri('http:\/\/localhost:5000')\r\n\r\ndata = pd.DataFrame({'V1': [-1.34419, -1.89211, 1.69421, 0.263328, 0.107918, 0.154241, 0.33468, 1.447778, -0.918269, 0.86319, -1.630049, 1.643798, 1.274341, -1.296742, -0.193585, 1.627422, -0.66805, -1.664491, -1.86911, 0.892885],\r\n                     'V2': [0.85556, -1.70503, -0.02896, 1.746258, -0.084151, 1.673185, 1.113326, -0.23231, 1.054817, -1.407584, 0.474997, 0.150687, -0.738246, -0.045513, 1.58637, 0.984249, 0.624333, 0.298866, 0.662204, 0.967942],\r\n                     'V3': [1.768638, -0.503169, -0.25622, -0.937752, -0.062189, -0.820652, -1.786942, -1.770495, 1.808681, -0.280286, -1.389736, 0.182212, -0.602959, -0.354683, -1.065631, 1.649264, 0.389538, -1.674815, 0.281824, -1.683662],\r\n                     'V4': [1.512828, 1.177697, -1.156862, -1.877876, 1.526013, 1.644001, -1.282481, -0.720543, 0.323963, -1.931616, 1.632839, 1.706752, 1.895627, 1.860705, -1.559702, 1.517466, 1.254323, 1.84415, -1.175013, -1.600652],\r\n                     'V5': [0.820483, -1.20923, -0.012221, 1.682836, 0.104248, 1.258085, 0.404062, 0.18019, 1.352545, -0.497071, 0.771277, 1.614052, -0.693854, 0.002655, 0.277743, -0.977744, -0.97259, -1.501586, -0.731194, -0.551264],\r\n                     'V6': [1.079115, -0.734152, -1.630816, -1.877664, 1.577477, -1.902078, 1.012828, -1.107726, 1.742781, -1.338595, 1.788969, -0.851507, 1.061596, -0.635559, -1.171469, -1.001642, 1.493507, 0.732088, 1.565327, -1.845441],\r\n                     'V7': [1.165929, 1.804607, 0.886589, -0.027458, -1.444197, -0.415643, 0.863924, -1.177661, 1.684514, 1.023797, -1.234116, -0.989024, 0.815575, -0.668453, 0.591911, -0.798925, 1.024032, -1.983963, 1.900752, 1.201001],\r\n                     'V8': [-0.536923, 0.641581, -0.585228, 1.061145, -0.303192, -0.652068, 0.858556, 0.11012, 1.839738, -1.51798, -0.942028, -0.736386, -0.098261, 0.699127, 0.173854, -1.16775, -0.417662, 0.021639, 1.745042, -1.119667],\r\n                     'V9': [0.643498, -1.090347, 0.120182, -0.819219, -1.296763, 0.530723, -1.367664, -0.708116, -1.304274, 1.486166, 1.656498, 1.645308, -0.257558, 0.400849, 1.356781, 1.693433, 0.42606, 0.370683, -0.239278, -0.541334],\r\n                     'V10': [-0.744989, 0.506658, 1.15586, 1.461127, 1.928769, -0.330472, 1.514159, -1.209056, -0.741453, -1.479674, 1.92057, -1.148481, 0.949433, 0.674107, -1.410627, 1.497083, -1.262624, -0.856706, -1.708155, 0.93153],\r\n                     'V11': [0.967242, 1.968385, -1.362337, -0.46194, 0.809224, 0.226177, 1.782128, -0.114595, 0.698243, -0.141743, -0.117251, 1.762656, -0.068839, 0.648945, -1.497037, -1.455443, -0.291242, 1.806048, -1.945438, 0.251282],\r\n                     'V12': [0.010432, -0.101522, -1.764095, 1.326967, -1.299122, -0.549148, 0.807092, -0.75387, 0.955056, 0.640369, -0.917832, 0.250338, 0.624729, 1.566922, 0.118619, 1.907585, -0.919995, 0.868393, -1.103909, 0.347108],\r\n                     'V13': [0.122315, -1.140017, -0.876424, -1.075771, 0.668814, 1.916654, -0.864906, 0.132892, 0.740058, 0.94469, -0.260381, 0.92833, -1.186423, -0.18321, 1.99266, -0.779091, -1.649025, -1.688821, 1.075145, -1.988603],\r\n                     'V14': [-1.494, 0.679776, 0.813194, 1.8687, -0.20273, -0.363265, 1.98902, 0.100025, 1.462866, 0.561017, 0.418922, 1.981837, -1.834009, -1.657952, 0.585069, -0.898764, 0.683234, 0.743215, -0.050289, -0.668302], \r\n                     'V15': [0.199787, 0.81829, 1.200156, -1.684249, 0.847466, 1.326102, 0.323103, -1.010648, -1.868355, -1.204467, 1.777393, 0.375692, -1.654002, 0.50357, -1.372448, -0.522425, 0.360716, 1.007605, 1.009369, -0.353638],\r\n                     'V16': [1.535552, -0.082278, -0.083154, 0.069432, 1.356735, -0.042527, -0.462543, 1.813852, -1.664882, 0.408013, -1.802172, -1.920202, 1.987332, -1.126771, 1.485496, 1.972345, -0.33345, 1.414685, -0.06674, 1.383197],\r\n                     'V17': [-0.249929, 1.668129, 0.860046, 0.013955, 0.085628, 1.285539, -0.754444, -0.306815, -1.244118, -0.61328, 0.711952, 1.384674, 1.710264, 1.337836, -0.029678, -1.382343, -1.963618, 0.088497, -0.110544, 0.954066],\r\n                     'V18': [0.665032, -1.214589, 0.486172, 1.184611, 1.152936, -0.192168, -1.096281, -0.762198, -0.338583, 0.170551, -0.045797, -0.897271, 0.433204, -0.986375, 0.430157, 1.846751, -0.905146, -1.398763, 1.790667, -1.580808],\r\n                     'V19': [1.347637, -0.356925, 0.414118, 0.277104, 0.41587, -1.237646, 0.580625, 1.468221, -0.254781, 0.245683, -1.25356, 0.241325, 1.15677, -1.74525, 1.970698, -0.038675, -0.314979, 0.114507, 1.378524, -0.139709],\r\n                     'V20': [-1.291686, -1.714475, 0.012188, 1.002238, -1.587334, 1.408967, 1.055095, -1.356865, 1.307388, 0.697003, -0.112676, 1.762375, 0.82697, 1.084934, 1.656421, 0.786079, -1.580991, 1.753751, -0.242525, 1.854008],\r\n                     'Class': [1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1]})\r\n\r\nsetup(data = data,\r\ntarget = 'Class', \r\nexperiment_name = 'xgb_test', \r\nfix_imbalance = True,\r\nlog_experiment = True, \r\nsilent=True, \r\nuse_gpu=True,\r\nfold=5,\r\npreprocess=False)\r\n\r\nmodels = ['xgboost','knn','rf']\r\ntop_models = compare_models(include = model)\r\ndd = pull()\n```\n\n\n### Expected Behavior\n\nArtifacts and metrics should be crated. \n\n### Actual Results\n\n```python-traceback\nError from logs.log:\r\n\r\n2022-08-09 06:11:05,384:ERROR:dashboard_logger.log_model() for XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\r\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\r\n              early_stopping_rounds=None, enable_categorical=False,\r\n              eval_metric=None, feature_types=None, gamma=0, gpu_id=0,\r\n              grow_policy='depthwise', importance_type=None,\r\n              interaction_constraints='', learning_rate=0.300000012,\r\n              max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=6,\r\n              max_leaves=0, min_child_weight=1, missing=nan,\r\n              monotone_constraints='()', n_estimators=100, n_jobs=-1,\r\n              num_parallel_tree=1, objective='binary:logistic',\r\n              predictor='auto', random_state=989, ...) raised an exception:\r\n2022-08-09 06:11:05,385:ERROR:Traceback (most recent call last):\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/internal\/tabular.py\", line 2362, in compare_models\r\n    dashboard_logger.log_model(\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/loggers\/__init__.py\", line 93, in log_model\r\n    logger.log_params(params, model_name=full_name)\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/loggers\/mlflow_logger.py\", line 46, in log_params\r\n    mlflow.log_params(params)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/fluent.py\", line 675, in log_params\r\n    MlflowClient().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[])\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/client.py\", line 918, in log_batch\r\n    self._tracking_client.log_batch(run_id, metrics, params, tags)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 315, in log_batch\r\n    self.store.log_batch(\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 309, in log_batch\r\n    self._call_endpoint(LogBatch, req_body)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/utils\/rest_utils.py\", line 256, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/utils\/rest_utils.py\", line 185, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Invalid value [{'key': 'objective', 'value': 'binary:logistic'}, {'key': 'use_label_encoder', 'value': 'None'}, {'key': 'base_score', 'value': '0.5'}, {'key': 'booster', 'value': 'gbtree'}, {'key': 'callbacks', 'value': 'None'}, {'key': 'colsample_bylevel', 'value': '1'}, {'key': 'colsample_bynode', 'value': '1'}, {'key': 'colsample_bytree', 'value': '1'}, {'key': 'early_stopping_rounds', 'value': 'None'}, {'key': 'enable_categorical', 'value': 'False'}, {'key': 'eval_metric', 'value': 'None'}, {'key': 'feature_types', 'value': 'None'}, {'key': 'gamma', 'value': '0'}, {'key': 'gpu_id', 'value': '0'}, {'key': 'grow_policy', 'value': 'depthwise'}, {'key': 'importance_type', 'value': 'None'}, {'key': 'interaction_constraints', 'value': ''}, {'key': 'learning_rate', 'value': '0.300000012'}, {'key': 'max_bin', 'value': '256'}, {'key': 'max_cat_to_onehot', 'value': '4'}, {'key': 'max_delta_step', 'value': '0'}, {'key': 'max_depth', 'value': '6'}, {'key': 'max_leaves', 'value': '0'}, {'key': 'min_child_weight', 'value': '1'}, {'key': 'missing', 'value': 'nan'}, {'key': 'monotone_constraints', 'value': '()'}, {'key': 'n_estimators', 'value': '100'}, {'key': 'n_jobs', 'value': '-1'}, {'key': 'num_parallel_tree', 'value': '1'}, {'key': 'predictor', 'value': 'auto'}, {'key': 'random_state', 'value': '989'}, {'key': 'reg_alpha', 'value': '0'}, {'key': 'reg_lambda', 'value': '1'}, {'key': 'sampling_method', 'value': 'uniform'}, {'key': 'scale_pos_weight', 'value': '1'}, {'key': 'subsample', 'value': '1'}, {'key': 'tree_method', 'value': 'gpu_hist'}, {'key': 'validate_parameters', 'value': '1'}, {'key': 'verbosity', 'value': '0'}] for parameter 'params' supplied. Hint: Value was of type 'list'. See the API docs for more information about request parameters.\n```\n\n\n### Installed Versions\n\n<details>\r\npycaret- Version: 2.3.10 <\/br>\r\nmlflow- Version: 1.27.0 <\/br>\r\nxgboost-  Version: 2.0.0.dev0 <\/br>\r\n<\/details>\r\n",
        "Challenge_closed_time":1660286399000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1660026191000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while running \"mlflow ui\" which results in a \"FileNotFoundError\". The expected behavior is for it to run without any issues. The version being used is 2.3.10.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2838",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":137.91,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":66,
        "Challenge_solved_time":72.28,
        "Challenge_title":"[BUG]: MLflow server integration",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":925,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"With new mlflow release-1.28.0- and **[Tracking \/ Model Registry] Fix an mlflow server bug that rejected parameters and tags with empty string values (https:\/\/github.com\/mlflow\/mlflow\/pull\/6179, @dbczumar)** bug fixed, the problem no longer occurs and artifacts are saved correctly",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.0,
        "Solution_reading_time":3.6,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1574678086832,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Amstelveen, Netherlands",
        "Answerer_reputation_count":3917.0,
        "Answerer_view_count":640.0,
        "Challenge_adjusted_solved_time":6.4474319444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have installed mlflow on GCP VM instance, \nnow I want to access mlflow UI with external IP.\nI tried setting up a firewall rule and opening the default port for mlflow, but not able to access it.\nCan someone give step by step process for just running mlflow on VM instance?<\/p>",
        "Challenge_closed_time":1583767598368,
        "Challenge_comment_count":3,
        "Challenge_created_time":1583744387613,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has installed MLFlow on a GCP VM instance and is trying to access the MLFlow UI with an external IP. Despite setting up a firewall rule and opening the default port for MLFlow, the user is unable to access it and is seeking a step-by-step process for running MLFlow on a VM instance.",
        "Challenge_last_edit_time":1583832420660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60597319",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.2,
        "Challenge_reading_time":3.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":6.4474319444,
        "Challenge_title":"Running MLFlow on GCP VM",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1537.0,
        "Challenge_word_count":56,
        "Platform":"Stack Overflow",
        "Poster_created_time":1451124057623,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":736.0,
        "Poster_view_count":234.0,
        "Solution_body":"<p>I've decided to check on my test VM and run mlflow server on GCE VM. Have a look at my steps below:<\/p>\n\n<ol>\n<li>create VM instance based on Ubuntu Linux 18.04 LTS<\/li>\n<li><p><a href=\"https:\/\/www.mlflow.org\/docs\/latest\/quickstart.html\" rel=\"noreferrer\">install MLflow<\/a>:<\/p>\n\n<pre><code>$ sudo apt update\n$ sudo apt upgrade\n$ cd ~\n$ git clone https:\/\/github.com\/mlflow\/mlflow\n$ cd mlflow\n$ sudo apt install python3-pip\n$ pip3 install mlflow\n$ python3 setup.py build\n$ sudo python3 setup.py install\n$ mlflow --version\nmlflow, version 1.7.1.dev0\n<\/code><\/pre><\/li>\n<li><p>run mlflow server on internal IP of VM instance (default 127.0.0.1):<\/p>\n\n<pre><code>$ ifconfig \nens4: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1460\ninet 10.XXX.15.XXX  netmask 255.255.255.255  broadcast 0.0.0.0\n...\n\n$ mlflow server --host 10.XXX.15.XXX\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Starting gunicorn 20.0.4\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Listening at: http:\/\/10.128.15.211:5000 (8631)\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Using worker: sync\n[2020-03-09 15:05:50 +0000] [8634] [INFO] Booting worker with pid: 8634\n[2020-03-09 15:05:51 +0000] [8635] [INFO] Booting worker with pid: 8635\n[2020-03-09 15:05:51 +0000] [8636] [INFO] Booting worker with pid: 8636\n[2020-03-09 15:05:51 +0000] [8638] [INFO] Booting worker with pid: 8638\n<\/code><\/pre><\/li>\n<li><p>check from VM instance (from second connection):<\/p>\n\n<pre><code>$ curl -I http:\/\/10.XXX.15.XXX:5000\nHTTP\/1.1 200 OK\nServer: gunicorn\/20.0.4\nDate: Mon, 09 Mar 2020 15:06:08 GMT\nConnection: close\nContent-Length: 853\nContent-Type: text\/html; charset=utf-8\nLast-Modified: Mon, 09 Mar 2020 14:57:11 GMT\nCache-Control: public, max-age=43200\nExpires: Tue, 10 Mar 2020 03:06:08 GMT\nETag: \"1583765831.3202355-853-3764264575\"\n<\/code><\/pre><\/li>\n<li><p><a href=\"https:\/\/cloud.google.com\/vpc\/docs\/add-remove-network-tags\" rel=\"noreferrer\">set network tag<\/a> <code>mlflow-server<\/code> <\/p><\/li>\n<li><p><a href=\"https:\/\/cloud.google.com\/vpc\/docs\/using-firewalls#creating_firewall_rules\" rel=\"noreferrer\">create firewall rule<\/a> to allow access on port 5000<\/p>\n\n<pre><code>$ gcloud compute --project=test-prj firewall-rules create mlflow-server --direction=INGRESS --priority=999 --network=default --action=ALLOW --rules=tcp:5000 --source-ranges=0.0.0.0\/0 --target-tags=mlflow-server\n<\/code><\/pre><\/li>\n<li><p>check from on-premises Linux machine <code>nmap -Pn 35.225.XXX.XXX<\/code><\/p>\n\n<pre><code>Starting Nmap 7.80 ( https:\/\/nmap.org ) at 2020-03-09 16:20 CET\nNmap scan report for 74.123.225.35.bc.googleusercontent.com (35.225.XXX.XXX)\nHost is up (0.20s latency).\nNot shown: 993 filtered ports\nPORT     STATE  SERVICE\n...\n5000\/tcp open   upnp\n...\n<\/code><\/pre><\/li>\n<li><p>go to web browser <a href=\"http:\/\/35.225.XXX.XXX:5000\/\" rel=\"noreferrer\">http:\/\/35.225.XXX.XXX:5000\/<\/a><\/p><\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/u2aFt.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/u2aFt.png\" alt=\"mlflow\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":11.0,
        "Solution_readability":8.4,
        "Solution_reading_time":39.18,
        "Solution_score_count":5.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":297.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1401427814950,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":749.0,
        "Answerer_view_count":50.0,
        "Challenge_adjusted_solved_time":19.2463625,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have two computers: Ubuntu1 and Ubuntu2. \nUbuntu1 runs MongoDB with database Sacred3. \nI want to connect from U2 to U1 via ssh and store there my experiment results.<\/p>\n\n<p>What I tried and failed:\n1. I installed mongo DB, created sacred3, I have ssh key to it. \nI edited <code>\/etc\/mongod.conf<\/code> adding:<\/p>\n\n<p><code># network interfaces\nnet:\n  port: 27017\n  bindIp: 0.0.0.0<\/code><\/p>\n\n<p>Then I enabled port forwarding with<\/p>\n\n<p><code>ssh -fN  -i ~\/.ssh\/sacred_key-pair.pem -L 6666:localhost:27017 ubuntu@106.969.696.969<\/code> \/\/ (with proper ip)<\/p>\n\n<p>so, as I undertstand, if I connect to my localhost:6666 it will be forwarded to 106.969.696.969:27017 <\/p>\n\n<p>So after that, I'm runnig an experiment with <a href=\"https:\/\/sacred.readthedocs.io\/en\/stable\/observers.html\" rel=\"nofollow noreferrer\">Sacred framework<\/a>:<\/p>\n\n<p>python exp1.py -m localhost:6666:sacred3<\/p>\n\n<p>and this should write experiment to remote DB, HOWEVER i I get:<\/p>\n\n<p><code>pymongo.errors.ServerSelectionTimeoutError: localhost:27017: [Errno 111] Connection refused<\/code><\/p>\n\n<p>which is driving me mad. please help!<\/p>\n\n#\n\n<p>below contents of exp1.py:<\/p>\n\n<pre><code>from sacred import Experiment\nfrom sacred.observers import MongoObserver\n\nex = Experiment()\nex.observers.append(MongoObserver.create())\n\ndef compute():\n    summ = layer1 - layer2\n    return summ\n\n\n@ex.config\ndef my_config():\n\n    hp_list = [{\"neurons\" : [32,32] , \"dropout\": 1.0},\n            {\"neurons\" : [32,32] , \"dropout\": 0.7},\n            {\"neurons\" : [32,16] , \"dropout\": 0.9},\n            {\"neurons\" : [24,16] , \"dropout\": 0.9},\n            {\"neurons\" : [24,8] , \"dropout\":  0.9},\n            {\"neurons\" : [16,8] , \"dropout\":  0.9},\n            {\"neurons\" : [64,64] , \"dropout\": 0.9},\n            {\"neurons\" : [64,64] , \"dropout\": 0.7},\n            {\"neurons\" : [64,32] , \"dropout\": 0.9},\n            {\"neurons\" : [64,32] , \"dropout\": 0.7},\n            {\"neurons\" : [48,32] , \"dropout\": 0.9},\n            {\"neurons\" : [48,32] , \"dropout\": 0.7},\n            {\"neurons\" : [48,16] , \"dropout\": 0.9},\n            {\"neurons\" : [48,16] , \"dropout\": 0.7},]\n\n    n_epochs = 2 \n\n\n@ex.capture\ndef training_loop(hp_list, n_epochs):\n    for j in hp_list:\n        print(\"Epoch: \", n_epochs)\n#       layer1 = random.randint(18,68)\n#       layer2 = random.randint(18,68)\n#       layer3 = random.randint(18,68)\n        layer1 = j[\"neurons\"][0]\n        layer2 = j[\"neurons\"][1]\n        dropout_ratio = j[\"dropout\"]\n\n\n        print(\"WHATS UUUUUP\",j, layer1, layer2, dropout_ratio, sep=\"_\")\n        # vae_training_loop_NN_DO(i, layer1, layer2, dropout_ratio )\n\n\n@ex.automain\ndef my_main():\n    training_loop()\n\n<\/code><\/pre>",
        "Challenge_closed_time":1571149528216,
        "Challenge_comment_count":4,
        "Challenge_created_time":1571145034667,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to save experiment results to a remote MongoDB database on Ubuntu1 from Ubuntu2 via an SSH tunnel. The user has installed MongoDB, created a database, and edited the mongod.conf file to enable port forwarding. However, when running an experiment with the Sacred framework, the user encounters a ServerSelectionTimeoutError, indicating that the connection to the remote database is being refused.",
        "Challenge_last_edit_time":1571149003303,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58395547",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":8.3,
        "Challenge_reading_time":31.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":1.2482080556,
        "Challenge_title":"How to save data to remote mongoDB via ssh tunnel? (connection refused)",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":329.0,
        "Challenge_word_count":266,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501710879087,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Warszawa, Polska",
        "Poster_reputation_count":300.0,
        "Poster_view_count":42.0,
        "Solution_body":"<p>According to the documentation <a href=\"https:\/\/sacred.readthedocs.io\/en\/stable\/observers.html\" rel=\"nofollow noreferrer\">supplied<\/a>, it looks like you're creating two observers, or overriding the connection argument you passed with <code>-m<\/code>, with the <code>MongoObserver.create()<\/code>specified in the code which uses the default mongo host and port <code>localhost:27017<\/code>. You either supply the observer connection via the <code>-m<\/code> argument or in code, not both.<\/p>\n\n<p>Try removing the <code>MongoObserver.create()<\/code> line altogether, or hardcoding the connection arguments: <code>MongoObserver(url='localhost:6666', db_name='sacred3')<\/code> <\/p>\n\n<p>Also, it looks like your mongo host is <a href=\"https:\/\/serverfault.com\/questions\/489192\/ssh-tunnel-refusing-connections-with-channel-2-open-failed\">not liking the binding to localhost<\/a> so you should also replace <code>localhost<\/code> in your ssh command with <code>127.0.0.1<\/code> or <code>[::1]<\/code>, e.g <code>ssh -fN -i ~\/.ssh\/sacred_key-pair.pem -L 6666:127.0.0.1:27017 ubuntu@106.969.696.969<\/code> or <code>ssh -fN -i ~\/.ssh\/sacred_key-pair.pem -L 6666:[::1]:27017 ubuntu@106.969.696.969<\/code><\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1571218290208,
        "Solution_link_count":2.0,
        "Solution_readability":12.9,
        "Solution_reading_time":15.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":113.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":1475181309096,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Brazil",
        "Answerer_reputation_count":4242.0,
        "Answerer_view_count":421.0,
        "Challenge_adjusted_solved_time":19.7379844444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am creating my own Docker image so that I can use my own models in AWS SageMaker. I sucessfully created a Docker image using command line inside the Jupyter Notebook in SageMaker ml.t2.medium instance using a customized Dockerfile:<\/p>\n\n<pre><code>REPOSITORY            TAG                 IMAGE ID            CREATED             SIZE\nsklearn               latest              01234212345        6 minutes ago       1.23GB\n<\/code><\/pre>\n\n<p>But when I run in Jupyter:<\/p>\n\n<pre><code>! aws ecr create-repository --repository-name sklearn\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>An error occurred (AccessDeniedException) when calling the CreateRepository operation: User: arn:aws:sts::1234567:assumed-role\/AmazonSageMaker-ExecutionRole-12345\/SageMaker is not authorized to perform: ecr:CreateRepository on resource: *\n<\/code><\/pre>\n\n<p>I already set up SageMaker, EC2, EC2ContainerService permissions and the following policy for EC2Container but I still get the same error.<\/p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"sagemaker:*\",\n        \"ec2:*\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n<\/code><\/pre>\n\n<p>Any idea on how I can solve this issue?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1528126261856,
        "Challenge_comment_count":3,
        "Challenge_created_time":1528052776997,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an \"AccessDeniedException\" error when trying to create a repository using AWS ECR in SageMaker. The user has already set up permissions for SageMaker, EC2, and EC2ContainerService, but is still unable to perform the action. The user is seeking advice on how to resolve the issue.",
        "Challenge_last_edit_time":1528055205112,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50669991",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":13.9,
        "Challenge_reading_time":15.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":20.4124608333,
        "Challenge_title":"AWS SageMaker is not authorized to perform: ecr:CreateRepository on resource: *",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":4509.0,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1475181309096,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brazil",
        "Poster_reputation_count":4242.0,
        "Poster_view_count":421.0,
        "Solution_body":"<p>I solved the problem. We must set a permission at SageMaker Execution Role as following:<\/p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"ecr:*\"            ],\n        \"Resource\": \"*\"\n    }\n]}\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":2.88,
        "Solution_score_count":5.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":1.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":26.0827755556,
        "Challenge_answer_count":1,
        "Challenge_body":"We are in a process to move all of our IAM users to aws SSO \nwe used to have this policy for sagemaker :\n\n\n\"\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListTags\",\n                \"sagemaker:DeleteNotebookInstance\",\n                \"sagemaker:StopNotebookInstance\",\n                \"sagemaker:CreatePresignedNotebookInstanceUrl\",\n                \"sagemaker:DescribeNotebookInstance\",\n                \"sagemaker:StartNotebookInstance\",\n                \"sagemaker:UpdateNotebookInstance\"\n            ],\n            \"Resource\": \"arn:aws:sagemaker:::notebook-instance\/${aws:username}*\"\n        },\n        {\n            \"Sid\": \"VisualEditor1\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListNotebookInstanceLifecycleConfigs\",\n                \"sagemaker:ListNotebookInstances\",\n                \"sagemaker:ListCodeRepositories\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n```\n\n\"\n\n\nthis would give access to each user to use his\\hers own notebook\nnow on the new SSO permission set i gave this \n\n\n\n```\n\"\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"glue:CreateScript\",\n                \"secretsmanager:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListTags\",\n                \"sagemaker:DeleteNotebookInstance\",\n                \"sagemaker:StopNotebookInstance\",\n                \"sagemaker:CreatePresignedNotebookInstanceUrl\",\n                \"sagemaker:Describe*\",\n                \"sagemaker:StartNotebookInstance\",\n                \"sagemaker:UpdateNotebookInstance\",\n                \"sagemaker:CreatePresignedDomainUrl\",\n                \"sagemaker:*\"\n            ],\n            \"Resource\": \"arn:aws:sagemaker:::notebook-instance\/*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"aws:ResourceTag\/Owner\": \"${identitystore:UserId}\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListTags\",\n                \"sagemaker:Describe*\",\n                \"sagemaker:StartNotebookInstance\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\"\n```\n\n\n\n\nthis is what i tried but i cant make it work \nplease assist?",
        "Challenge_closed_time":1658148377804,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658054479812,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to move all IAM users to AWS SSO and has encountered an issue with a policy that allows only one SSO user to access a resource. The user has provided two policies, one for Sagemaker that gives access to each user to use their own notebook, and another for the new SSO permission set that is not working. The user is seeking assistance to make the new policy work.",
        "Challenge_last_edit_time":1668442079408,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUruheXJHaQVu_S9LIzUyDAw\/policy-that-allows-only-one-sso-user-to-access-a-resource",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":38.9,
        "Challenge_reading_time":22.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":26.0827755556,
        "Challenge_title":"Policy that allows only one SSO user to access a resource",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":164.0,
        "Challenge_word_count":128,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello, \n\nI understand that you are currently trying to restrict access to Sagemaker notebook using SSO identity's UserID. \n\nCurrently, I leveraged your provided SSO Permission set and tweaked it out as you can see below, and finally tested it out on AWS SageMaker Console by logging in as an AWS SSO User, and was able to see successful start\/stop\/describing of the SageMaker notebook (with Tags - Owner:UserId) corresponding to the SSO UserId.\n\n```\n{\n\t\"Version\": \"2012-10-17\",\n\t\"Statement\": [\n\t\t{\n\t\t\t\"Effect\": \"Allow\",\n\t\t\t\"Action\": [\n\t\t\t\t\"glue:CreateScript\",\n\t\t\t\t\"secretsmanager:*\"\n\t\t\t],\n\t\t\t\"Resource\": \"*\"\n\t\t},\n\t\t{\n\t\t\t\"Effect\": \"Allow\",\n\t\t\t\"Action\": [\n\t\t\t\t\"sagemaker:ListTags\",\n\t\t\t\t\"sagemaker:DeleteNotebookInstance\",\n\t\t\t\t\"sagemaker:StopNotebookInstance\",\n\t\t\t\t\"sagemaker:CreatePresignedNotebookInstanceUrl\",\n\t\t\t\t\"sagemaker:Describe*\",\n\t\t\t\t\"sagemaker:StartNotebookInstance\",\n\t\t\t\t\"sagemaker:UpdateNotebookInstance\",\n\t\t\t\t\"sagemaker:CreatePresignedDomainUrl\"\n\t\t\t],\n\t\t\t\"Resource\": \"arn:aws:sagemaker:us-east-1:7XXXXXXXXX:notebook-instance\/*\",\n\t\t\t\"Condition\": {\n\t\t\t\t\"StringEquals\": {\n\t\t\t\t\t\"sagemaker:ResourceTag\/Owner\": \"${identitystore:UserId}\"\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t{\n\t\t\t\"Sid\": \"VisualEditor1\",\n\t\t\t\"Effect\": \"Allow\",\n\t\t\t\"Action\": [\n\t\t\t\t\"sagemaker:ListNotebookInstanceLifecycleConfigs\",\n\t\t\t\t\"sagemaker:ListNotebookInstances\",\n\t\t\t\t\"sagemaker:ListCodeRepositories\"\n\t\t\t],\n\t\t\t\"Resource\": \"*\"\n\t\t}\n\t]\n}\n```\n________________________________\n\nHowever, in case if this SSO User tried to stop any other Sagemaker notebooks, which didn't have the tags corresponding to their UserId, then the following errors were observed as expected behavior -\n\n```\nUser: arn:aws:sts::7XXXXXXXXX:assumed-role\/AWSReservedSSO_SageMXXXXXXXXXbe\/test1 is not authorized to perform: sagemaker:StopNotebookInstance on resource: arn:aws:sagemaker:us-east-1:7XXXXXXXXX:notebook-instance\/userachecking because no identity-based policy allows the sagemaker:StopNotebookInstance action\n\nor \n\nUser: arn:aws:sts::7XXXXXXXXX:assumed-role\/AWSReservedSSO_SageMXXXXXXXXXbe\/test1 is not authorized to perform: sagemaker:DescribeNotebookInstance on resource: arn:aws:sagemaker:us-east-1:7XXXXXXXXX:notebook-instance\/Test1Check because no identity-based policy allows the sagemaker:DescribeNotebookInstance action\n```\n\nAlso, please note that unlike your provided IAM policy, your SSO permission set policy was missing the action - `sagemaker:ListNotebookInstances` which also raised an error for not being able to list out the notebook instances on AWS SageMaker Console in my testing. Hence, I had added the appropriate Sagemaker list actions to your permission set as well. \n\n**Additional Information** - \n\na. ${identitystore:UserId} -> Each user in the AWS SSO identity store is assigned a unique UserId. You can view the UserId for your users by using the AWS SSO console and navigating to each user or by using the DescribeUser API action. [1]\n\nb. ListNotebookInstances -> Returns a list of the SageMaker notebook instances in the requester's account in an AWS Region. [2]\n\nc. ResourceTag -> You can use the ResourceTag\/key-name condition key to determine whether to allow access to the resource based on the tags that are attached to the resource. [3][4]\n\nd. sagemaker:ResourceTag\/ -> Filters access by the preface string for a tag key and value pair attached to a resource [5]\n\ne. sagemaker:ResourceTag\/${TagKey} -> Filters access by a tag key and value pair [5]\n____________________________________\nI hope the shared information is insightful to your query. In case, if you have any other queries or concerns regarding AWS SSO or Sagemaker services or any account specific configuration that you would like to discuss, then please feel free to reach out to our team directly by creating a support case with our premium support team. \n\nHave a wonderful day ahead and stay safe. \n____________________________________\nReferences: \n\n[1] https:\/\/docs.aws.amazon.com\/singlesignon\/latest\/userguide\/using-predefined-attributes.html\n\n[2] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ListNotebookInstances.html\n\n[3] https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_tags.html\n\n[4] https:\/\/aws.amazon.com\/blogs\/security\/simplify-granting-access-to-your-aws-resources-by-using-tags-on-aws-iam-users-and-roles\/\n\n[5] https:\/\/docs.aws.amazon.com\/service-authorization\/latest\/reference\/list_amazonsagemaker.html#amazonsagemaker-policy-keys",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1658148839830,
        "Solution_link_count":5.0,
        "Solution_readability":18.7,
        "Solution_reading_time":55.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":437.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1508797229168,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":1115.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":80.8031436111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Databricks and have a column in a dataframe that I need to update for every record with an external web service call. In this case it is using the Azure Machine Learning Service SDK and does a service call. This code works fine when not run as a UDF in spark (ie. just python) however it throws a serialization error when I try to call it as a UDF. The same happens if I use a lambda and a map with an rdd.<\/p>\n\n<p>The model uses fastText and can be invoked fine from Postman or python via a normal http call or using the WebService SDK from AMLS - it's just when it is a UDF that it fails with this message:<\/p>\n\n<p>TypeError: can't pickle _thread._local objects<\/p>\n\n<p>The only workaround I can think of is to loop through each record in the dataframe sequentially and update the record with a call, however this is not very efficient. I don't know if this is a spark error or because the service is loading a fasttext model. When I use the UDF and mock a return value it works though.<\/p>\n\n<p>Error at bottom...<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice, AciWebservice\nfrom azureml.core import Workspace\n\ndef predictModelValue2(summary, modelName, modelLabel):  \n    raw_data = '[{\"label\": \"' + modelLabel + '\", \"model\": \"' + modelName + '\", \"as_full_account\": \"' + summary + '\"}]'\n    prediction = service.run(raw_data)\n    return prediction\n\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql.functions import udf\n\npredictModelValueUDF = udf(predictModelValue2)\n\nDVIRCRAMFItemsDFScored1 = DVIRCRAMFItemsDF.withColumn(\"Result\", predictModelValueUDF(\"Summary\", \"ModelName\", \"ModelLabel\"))\n<\/code><\/pre>\n\n<blockquote>\n  <p>TypeError: can't pickle _thread._local objects<\/p>\n  \n  <p>During handling of the above exception, another exception occurred:<\/p>\n  \n  <p>PicklingError                             Traceback (most recent call\n  last)  in \n  ----> 2 x = df.withColumn(\"Result\", predictModelValueUDF(\"Summary\",\n  \"ModelName\", \"ModelLabel\"))<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in wrapper(*args)\n      194         @functools.wraps(self.func, assigned=assignments)\n      195         def wrapper(*args):\n  --> 196             return self(*args)\n      197 \n      198         wrapper.<strong>name<\/strong> = self._name<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in <strong>call<\/strong>(self, *cols)\n      172 \n      173     def <strong>call<\/strong>(self, *cols):\n  --> 174         judf = self._judf\n      175         sc = SparkContext._active_spark_context\n      176         return Column(judf.apply(_to_seq(sc, cols, _to_java_column)))<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in _judf(self)\n      156         # and should have a minimal performance impact.\n      157         if self._judf_placeholder is None:\n  --> 158             self._judf_placeholder = self._create_judf()\n      159         return self._judf_placeholder\n      160 <\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in _create_judf(self)\n      165         sc = spark.sparkContext\n      166 \n  --> 167         wrapped_func = _wrap_function(sc, self.func, self.returnType)\n      168         jdt = spark._jsparkSession.parseDataType(self.returnType.json())\n      169         judf = sc._jvm.org.apache.spark.sql.execution.python.UserDefinedPythonFunction(<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in _wrap_function(sc,\n  func, returnType)\n       33 def _wrap_function(sc, func, returnType):\n       34     command = (func, returnType)\n  ---> 35     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n       36     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n       37                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/rdd.py in _prepare_for_python_RDD(sc,\n  command)    2461     # the serialized command will be compressed by\n  broadcast    2462     ser = CloudPickleSerializer()\n  -> 2463     pickled_command = ser.dumps(command)    2464     if len(pickled_command) >\n  sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  # Default 1M<br>\n  2465         # The broadcast will have same life cycle as created\n  PythonRDD<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/serializers.py in dumps(self, obj)\n      709                 msg = \"Could not serialize object: %s: %s\" % (e.<strong>class<\/strong>.<strong>name<\/strong>, emsg)\n      710             cloudpickle.print_exec(sys.stderr)\n  --> 711             raise pickle.PicklingError(msg)\n      712 \n      713 <\/p>\n  \n  <p>PicklingError: Could not serialize object: TypeError: can't pickle\n  _thread._local objects<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1573841167916,
        "Challenge_comment_count":2,
        "Challenge_created_time":1573553894003,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a serialization error when trying to call an external web service as a UDF in Databricks. The error message states that \"_thread._local objects\" cannot be pickled. The user has tried using a lambda and a map with an RDD, but the error persists. The only workaround the user has found is to loop through each record in the dataframe sequentially, which is not efficient. The issue may be related to the service loading a fasttext model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58816515",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":55.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":79.7983091667,
        "Challenge_title":"Databricks UDF calling an external web service cannot be serialised (PicklingError)",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":931.0,
        "Challenge_word_count":449,
        "Platform":"Stack Overflow",
        "Poster_created_time":1256089885500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney, Australia",
        "Poster_reputation_count":4947.0,
        "Poster_view_count":531.0,
        "Solution_body":"<p>I am not expert in DataBricks or Spark, but pickling functions from the local notebook context is always problematic when you are touching complex objects like the <code>service<\/code> object. In this particular case, I would recommend removing the dependency on the azureML <code>service<\/code> object and just use <code>requests<\/code> to call the service. <\/p>\n\n<p>Pull the key from the service:<\/p>\n\n<pre><code># retrieve the API keys. two keys were generated.\nkey1, key2 = service.get_keys()\nscoring_uri = service.scoring_uri\n<\/code><\/pre>\n\n<p>You should be able to use these strings in the UDF directly without pickling issues -- <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/9233ce089afb81d466076e36e7e61c3ce4cfafec\/how-to-use-azureml\/ml-frameworks\/chainer\/deployment\/train-hyperparameter-tune-deploy-with-chainer\/train-hyperparameter-tune-deploy-with-chainer.ipynb\" rel=\"nofollow noreferrer\">here is an example<\/a> of  how you would call the service with just requests. Below applied to your UDF:<\/p>\n\n<pre><code>import requests, json\ndef predictModelValue2(summary, modelName, modelLabel):  \n  input_data = json.dumps({\"summary\": summary, \"modelName\":, ....})\n\n  headers = {'Content-Type':'application\/json', 'Authorization': 'Bearer ' + key1}\n\n  # call the service for scoring\n  resp = requests.post(scoring_uri, input_data, headers=headers)\n\n  return resp.text[1]\n\n<\/code><\/pre>\n\n<p>On a side node, though: your UDF will be called for each row in your data frame and each time it will make a network call -- that will be very slow. I would recommend looking for ways to batch the execution. As you can see from your constructed json <code>service.run<\/code> will accept an array of items, so you should call it in batches of 100s or so.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1573844785320,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":22.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":206.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1395431574432,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Islamabad Capital Territory, Pakistan",
        "Answerer_reputation_count":1520.0,
        "Answerer_view_count":214.0,
        "Challenge_adjusted_solved_time":2.3479152778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I started to work with AWS SageMaker. I have an AWS Starter Account. I have been trying to deploy a built-in algorithm for 2 days but I always get <code>AccessDeniedException<\/code> despite the fact that I created IAM role according to <a href=\"https:\/\/aws.amazon.com\/tr\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/tr\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a><\/p>\n\n<blockquote>\n  <p>ClientError: An error occurred (AccessDeniedException) when calling the CreateTrainingJob operation: User: arn:aws:sts::161745376217:assumed-role\/AmazonSageMaker-ExecutionRole-20200203T194557\/SageMaker is not authorized to perform: sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:us-east-1:161745376217:training-job\/blazingtext-2020-02-03-18-12-14-017 with an explicit deny<\/p>\n<\/blockquote>\n\n<p>Could you help me to solve this problem ?\nThank you so much<\/p>",
        "Challenge_closed_time":1580764036812,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580755584317,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an \"AccessDeniedException\" error while trying to deploy a built-in algorithm on AWS SageMaker despite creating an IAM role as per the tutorial. The error message suggests that the user is not authorized to perform the \"CreateTrainingJob\" action on the resource. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1580790326547,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60045326",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.9,
        "Challenge_reading_time":13.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2.3479152778,
        "Challenge_title":"AWS SageMaker Access Denied",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":3755.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1481983208856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>You have created a role for SageMaker to access S3 bucket, but it seems your IAM user doesn't have access to SageMaker service. Please make sure your IAM user has permission to SageMaker.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":2.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1626973312768,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":92.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":4407.5466925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm having the error mentioned in the title when trying to upload a large file (15gb) to my s3 bucket from a Sagemaker notebook instance.<\/p>\n<p>I know that there are some similar questions here that i have already visited. I have gone through <a href=\"https:\/\/stackoverflow.com\/questions\/52541933\/accessdenied-when-calling-the-createmultipartupload-operation-in-django-using-dj\">this<\/a>, <a href=\"https:\/\/stackoverflow.com\/questions\/37630635\/createmultipartupload-operation-aws-policy-items-needed\">this<\/a>, and <a href=\"https:\/\/stackoverflow.com\/questions\/36272286\/getting-access-denied-when-calling-the-putobject-operation-with-bucket-level-per\">this<\/a> question, but after following the steps mentioned, and applying the policies described in these questions i still have the same error.<\/p>\n<p>I have also come to <a href=\"https:\/\/aws.amazon.com\/es\/premiumsupport\/knowledge-center\/s3-access-denied-error-kms\/#:%7E:text=%22An%20error%20occurred%20(AccessDenied)%20when%20calling%20the%20CreateMultipartUpload%20operation,GenerateDataKey%20and%20kms%3ADecrypt%20actions.\" rel=\"nofollow noreferrer\">this<\/a> documentation page eventually. The problem is that when i go into my users page in the IAM section, i see no users. I can see some roles but no users and i don't know which role should i edit following the steps mentioned in the documentation page. Also, my bucket DON'T have encryption enabled so i'm not really sure that the steps in the documentation page will fix the error for me.<\/p>\n<p>This is the policy in currently using for my bucket:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Id&quot;: &quot;Policy1&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Principal&quot;: {\n                &quot;AWS&quot;: &quot;arn:aws:iam::XXXX:root&quot;\n            },\n            &quot;Action&quot;: &quot;s3:*&quot;,\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::bauer-bucket&quot;,\n                &quot;arn:aws:s3:::bauer-bucket\/*&quot;\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n<p>I'm totally lost with this, i need to upload that file to my bucket. Please help.<\/p>\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1645238505323,
        "Challenge_comment_count":2,
        "Challenge_created_time":1629371337230,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an \"AccessDenied\" error when trying to upload a large file (15gb) to their S3 bucket from a Sagemaker notebook instance. They have tried following steps from similar questions and documentation pages, but the error persists. The user is unable to find any users in their IAM section and is unsure which role to edit. The user's bucket does not have encryption enabled, and they are using a policy that allows all S3 actions for the root user. The user is seeking help to resolve the issue and upload the file to their bucket.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68846704",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":16.2,
        "Challenge_reading_time":29.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":4407.5466925,
        "Challenge_title":"An error occurred (AccessDenied) when calling the CreateMultipartUpload operation: Access Denied",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":774.0,
        "Challenge_word_count":210,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521854999168,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Spain",
        "Poster_reputation_count":1338.0,
        "Poster_view_count":265.0,
        "Solution_body":"<p>The access is dictated by the execution role that is attached to the SageMaker notebook. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html<\/a> goes through how add additional s3 permissions to a SageMaker execution role.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.3,
        "Solution_reading_time":4.72,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":0.4647708333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>This is my first time trying to use the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Kedro<\/a> package.<\/p>\n<p>I have a list of .wav files in an s3 bucket, and I'm keen to know how I can have them available within the Kedro data catalog.<\/p>\n<p>Any thoughts?<\/p>",
        "Challenge_closed_time":1611661436392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611660152193,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to add a directory of .wav files from an s3 bucket to the Kedro data catalogue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65900415",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":4.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3567219444,
        "Challenge_title":"How do I add a directory of .wav files to the Kedro data catalogue?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":294.0,
        "Challenge_word_count":56,
        "Platform":"Stack Overflow",
        "Poster_created_time":1500383313376,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":851.0,
        "Poster_view_count":86.0,
        "Solution_body":"<p>I don't believe there's currently a dataset format that handles <code>.wav<\/code> files. You'll need to build a <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/03_custom_datasets.html\" rel=\"nofollow noreferrer\">custom dataset<\/a> that uses something like <a href=\"https:\/\/docs.python.org\/3\/library\/wave.html\" rel=\"nofollow noreferrer\">Wave<\/a> - not as much work as it sounds!<\/p>\n<p>This will enable you to do something like this in your catalog:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>dataset:\n  type: my_custom_path.WaveDataSet\n  filepath: path\/to\/individual\/wav_file.wav # this can be a s3:\/\/url\n<\/code><\/pre>\n<p>and you can then access your WAV data natively within your Kedro pipeline. You can do this for each <code>.wav<\/code> file you have.<\/p>\n<p>If you wanted to be able to access a whole folders worth of wav files, you might want to explore the notion of a &quot;wrapper&quot; dataset like the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.PartitionedDataSet.html\" rel=\"nofollow noreferrer\">PartitionedDataSet<\/a> whose <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html#partitioned-dataset-definition\" rel=\"nofollow noreferrer\">usage guide<\/a> can be found in the documentation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1611661825368,
        "Solution_link_count":4.0,
        "Solution_readability":12.2,
        "Solution_reading_time":16.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":129.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.3832386111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\nAre there any drawbacks I should be aware of if we restrict user access to only a single region? \n\nWe use a variety of AWS services but mainly S3 and Sagemaker Studio. Our team is located in various locations so their default regions are different.  It has been a challenge to keep track of studio instances when they are created in different regions so we are now considering restricting access to a single region. Are there issues that we may face in that case? Any services we may miss?",
        "Challenge_closed_time":1642705784219,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642700804560,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is considering restricting user access to a single region due to difficulties in keeping track of instances created in different regions. They are mainly using AWS services such as S3 and Sagemaker Studio and are concerned about missing out on any services or facing issues if they restrict access.",
        "Challenge_last_edit_time":1667921325307,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUxt7fqO9HQrKWDfi4V4Lagg\/pros-and-cons-of-restricting-user-access-to-certain-regions",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":6.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.3832386111,
        "Challenge_title":"Pros and cons of restricting user access to certain regions",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":100.0,
        "Challenge_word_count":99,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I would take a look at [this](https:\/\/docs.aws.amazon.com\/organizations\/latest\/userguide\/orgs_manage_policies_scps_examples_general.html#example-scp-deny-region) for some potential edge cases. In summary, you may need to allow us-east-1 and us-west-2 in addition to whatever regions your team is in since they host some of the global service endpoints (like IAM, Route 53, Global Accelerator, and a few others). For STS, I would use the [regional endpoints](https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/id_credentials_temp_enable-regions.html) if you aren't already.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1642705784219,
        "Solution_link_count":2.0,
        "Solution_readability":15.2,
        "Solution_reading_time":7.48,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1534058291092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":33.0,
        "Challenge_adjusted_solved_time":55.4162013889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create sagemaker studio project using aws cdk following below steps:<\/p>\n<p>create domain (<a href=\"https:\/\/github.com\/aws-samples\/aws-cdk-sagemaker-studio.git\" rel=\"nofollow noreferrer\">using this example<\/a>)\ncreate user (<a href=\"https:\/\/github.com\/aws-samples\/aws-cdk-sagemaker-studio.git\" rel=\"nofollow noreferrer\">using this example<\/a>)\ncreate jupyter app\ncreate project<\/p>\n<p>Code for creating jupyter app:<\/p>\n<pre><code>\ndef __init__(self, scope: Construct,\n             construct_id: str, *,\n             app_name: str,\n             app_type: str,\n             domain_id: str,\n             user_profile_name: str,\n             depends_on=None, **kwargs) -&gt; None:\n    super().__init__(scope, construct_id)\n\n    sagemaker_jupyter_app = sg.CfnApp(self, construct_id,\n                                      app_name=app_name,\n                                      app_type=app_type,\n                                      domain_id=domain_id,\n                                      user_profile_name=user_profile_name\n                                      )\n    sagemaker_jupyter_app.add_depends_on(depends_on_user_creation)\n<\/code><\/pre>\n<p>Code for creating project:<\/p>\n<pre><code>\ndef __init__(self, scope: Construct,\n             construct_id: str, *,\n             project_name: str,\n             project_description: str,\n             product_id: str,\n             depends_on=None,\n             **kwargs) -&gt; None:\n    super().__init__(scope, construct_id)\n\n    sagemaker_studio_project = sg.CfnProject(self, construct_id,\n                                             project_name=project_name,\n                                             service_catalog_provisioning_details={\n                                                 &quot;ProductId&quot;: &quot;prod-7tjedn5dz4jrw&quot;\n                                             },\n                                             project_description=project_description\n                                             )\n<\/code><\/pre>\n<p>Domain, user, jupyter app all gets created successfully. The problem comes in with project.\nBelow is the error :<\/p>\n<blockquote>\n<p>Resource handler returned message: &quot;Product prod-7tjedn5dz4jrw does\nnot exist or access was denied (Service: SageMaker, Status Code: 400,\nRequest ID: 768116aa-e77b-4691-a972-38b83093fdc4)&quot; (RequestToken:\n45ca2a0c-3f03-e3e0-f29d-d9443ff4dfc1, HandlerErrorCode:\nGeneralServiceException)<\/p>\n<\/blockquote>\n<p>I am running this code from an ec2 instance that has SagemakerFullAccess\nI also tried attaching SagemakerFullAccess execution role with project...but got the same error.\nI have also attached below policy to my domain:<\/p>\n<ul>\n<li>AmazonSageMakerAdmin-ServiceCatalogProductsServiceRolePolicy<\/li>\n<\/ul>",
        "Challenge_closed_time":1650740870208,
        "Challenge_comment_count":1,
        "Challenge_created_time":1650541371883,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a Sagemaker studio project using AWS CDK, but is encountering an error message stating that the product does not exist or access was denied. The user has created the domain, user, and Jupyter app successfully, but is unable to create the project. The user has tried running the code from an EC2 instance with SagemakerFullAccess and attaching the SagemakerFullAccess execution role to the project, but the error persists. The user has also attached the AmazonSageMakerAdmin-ServiceCatalogProductsServiceRolePolicy to the domain.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71953876",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":18.5,
        "Challenge_reading_time":29.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":55.4162013889,
        "Challenge_title":"How to create Sagemaker studio project using aws cdk",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":491.0,
        "Challenge_word_count":193,
        "Platform":"Stack Overflow",
        "Poster_created_time":1534058291092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":116.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>Basically this was an issue related to IAM.\nRunning cdk program requires bootstrapping it using the command <code>cdk bootstrap<\/code>\nAfter running this command cdk was creating a bunch of roles out of which one role will be related to cloudformation's execution role. Something like<\/p>\n<blockquote>\n<p>cdk-serialnumber-cfn-exec-role-Id-region<\/p>\n<\/blockquote>\n<p>Now this role was used by cloudformation to run the stack.<\/p>\n<p>Using sagemaker from console automatically adds the role associated with domain\/user at<\/p>\n<blockquote>\n<p>ServiceCatalog -&gt; Portfolios -&gt; Imported -&gt; Amazon SageMaker Solutions and ML Ops products -&gt; Groups, roles, and users<\/p>\n<\/blockquote>\n<p>Thats was the reason why product id was accessible from console.<\/p>\n<p>After adding the role created by cdk bootsrap to the above path I was able to run my stack.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":10.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":120.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1375058329287,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":762.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":0.1746386111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a project with DVC (Data Version Control), when I push files in my remote storage, the name of the files are changed. How I can conserve the names?<\/p>",
        "Challenge_closed_time":1620171651067,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620170453717,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with DVC where the names of files are changed when pushed to remote storage. They are seeking a solution to conserve the original file names.",
        "Challenge_last_edit_time":1620171022368,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67393339",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.9,
        "Challenge_reading_time":2.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.3325972222,
        "Challenge_title":"dvc push, change the names of files on the remote storage",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":447.0,
        "Challenge_word_count":41,
        "Platform":"Stack Overflow",
        "Poster_created_time":1379685720448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>Short answer: there is no way to do that.<\/p>\n<p>Long answer:\nDvc remote is a content-based storage, so names are not preserved. Dvc creates metafiles (*.dvc files) in your workspace that contain names and those files are usually tracked by git, so you need to use git remote and dvc remote together to have both filenames and their contents. Here is a more detailed explanation about the format of local and remote storage <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a> . Also, checkout <a href=\"https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.5,
        "Solution_reading_time":10.87,
        "Solution_score_count":5.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":81.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1614873430827,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":169.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":14.7412641667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to setup DVC with Google Drive storage as shown <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#url-format\" rel=\"nofollow noreferrer\">here<\/a>. So far, I've been unsuccessful in pushing data to the remote. I tried both with and without the Google App setup.<\/p>\n<p>After running a <code>dvc push -v<\/code>, the following exception is shown:<\/p>\n<pre><code>  File &quot;(...)\/anaconda3\/lib\/python3.8\/site-packages\/googleapiclient\/discovery.py&quot;, line 387, in _retrieve_discovery_doc\n    raise UnknownApiNameOrVersion(&quot;name: %s  version: %s&quot; % (serviceName, version))\ngoogleapiclient.errors.UnknownApiNameOrVersion: name: drive  version: v2\n<\/code><\/pre>\n<p>DVC was installed via <code>pip install dvc[gdrive]<\/code>. The <code>pip freeze<\/code> of the concerning packages is:<\/p>\n<pre><code>oauth2client==4.1.3\ngoogle-api-python-client==2.0.1\ndvc==2.0.1\n<\/code><\/pre>\n<p>Any help is thoroughly appreciated.<\/p>",
        "Challenge_closed_time":1614873489452,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614869465907,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is attempting to set up Data Version Control (DVC) with Google Drive storage but is unable to push data to the remote. They have tried both with and without the Google App setup. After running a \"dvc push -v\" command, an exception is shown indicating an \"UnknownApiNameOrVersion\" error with the Google Drive API version. The user has installed DVC via pip and the concerning packages are oauth2client, google-api-python-client, and dvc.",
        "Challenge_last_edit_time":1614874338736,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66477468",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":14.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.1176513889,
        "Challenge_title":"Data Version Control with Google Drive Remote: \"googleapiclient.errors.UnknownApiNameOrVersion: name: drive version: v2\"",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":979.0,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1593006899208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":704.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>Can you try to install <code>google-api-python-client==1.12.8<\/code> and test in that way?<\/p>\n<p>Edit:<\/p>\n<p>It appears to be that, this was a bug in the 2.0.0-2.0.1 of google-api-client and resolved in 2.0.2. So this should also work <code>google-api-python-client&gt;=2.0.2<\/code><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1614927407287,
        "Solution_link_count":0.0,
        "Solution_readability":3.7,
        "Solution_reading_time":3.76,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":8.8217069444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_mscoco_multi_label\/Image-classification-multilabel-lst.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a> with my custom data and my custom S3 buckets where train and validation data are. I am getting the following error:<\/p>\n<pre><code>Customer Error: imread read blank (None) image for file: \/opt\/ml\/input\/data\/train\/s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n<\/code><\/pre>\n<p>I have all my training data are in one folder named '<code>train<\/code>' I have set up my <code>lst<\/code> file like this suggested by <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">doc<\/a>,<\/p>\n<pre><code>22  1   s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n86  0   s3:\/\/image-classification\/image_classification_model_data\/train\/img-002.png\n...\n<\/code><\/pre>\n<p>My other configurations:<\/p>\n<pre><code>s3_bucket = 'image-classification'\nprefix =  'image_classification_model_data'\n\n\ns3train = 's3:\/\/{}\/{}\/train\/'.format(s3_bucket, prefix)\ns3validation = 's3:\/\/{}\/{}\/validation\/'.format(s3_bucket, prefix)\n\ns3train_lst = 's3:\/\/{}\/{}\/train_lst\/'.format(s3_bucket, prefix)\ns3validation_lst = 's3:\/\/{}\/{}\/validation_lst\/'.format(s3_bucket, prefix)\n\n\n\ntrain_data = sagemaker.inputs.TrainingInput(s3train, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data = sagemaker.inputs.TrainingInput(s3validation, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\ntrain_data_lst = sagemaker.inputs.TrainingInput(s3train_lst, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data_lst = sagemaker.inputs.TrainingInput(s3validation_lst, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\n\ndata_channels = {'train': train_data, 'validation': validation_data, 'train_lst': train_data_lst, \n                 'validation_lst': validation_data_lst}\n<\/code><\/pre>\n<p>I checked the images downloaded and checked physically, I see the image. Now sure what this error gets thrown out as <code>blank<\/code>. Any suggestion would be great.<\/p>",
        "Challenge_closed_time":1616684272012,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616651745117,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while following an image classification tutorial on Amazon Sagemaker with custom data and S3 buckets. The error message states that the image read is blank (None) for a specific file. The user has set up the lst file and other configurations as suggested by the documentation. The user has physically checked the downloaded images and is unsure why the error is being thrown.",
        "Challenge_last_edit_time":1616652513867,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66793845",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":30.4,
        "Challenge_reading_time":32.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":9.0352486111,
        "Challenge_title":"Customer Error: imread read blank (None) image for file- Sagemaker AWS",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":164.0,
        "Challenge_word_count":160,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519936486960,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Minneapolis, MN, USA",
        "Poster_reputation_count":1113.0,
        "Poster_view_count":122.0,
        "Solution_body":"<p>Sagemaker copies the input data you specify in <code>s3train<\/code> into the instance in <code>\/opt\/ml\/input\/data\/train\/<\/code> and that's why you have an error, because as you can see from the error message is trying to concatenate the filename in the <code>lst<\/code> file with the path where it expect the image to be. So just put only the filenames in your <code>lst<\/code>and should be fine (remove the s3 path).<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.0,
        "Solution_reading_time":5.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1540117021307,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":226.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":5.1148786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Let me prefix this by saying I'm very new to tensorflow and even newer to AWS Sagemaker.<\/p>\n\n<p>I have some tensorflow\/keras code that I wrote and tested on a local dockerized Jupyter notebook and it runs fine. In it, I import a csv file as my input.<\/p>\n\n<p>I use Sagemaker to spin up a jupyter notebook instance with conda_tensorflow_p36. I modified the pandas.read_csv() code to point to my input file, now hosted on a S3 bucket.<\/p>\n\n<p>So I changed this line of code from<\/p>\n\n<pre><code>import pandas as pd\n\ndata = pd.read_csv(\"\/input.csv\", encoding=\"latin1\")\n<\/code><\/pre>\n\n<p>to this<\/p>\n\n<pre><code>import pandas as pd\n\ndata = pd.read_csv(\"https:\/\/s3.amazonaws.com\/my-sagemaker-bucket\/input.csv\", encoding=\"latin1\")\n<\/code><\/pre>\n\n<p>and I get this error<\/p>\n\n<pre><code>AttributeError: module 'pandas' has no attribute 'core'\n<\/code><\/pre>\n\n<p>I'm not sure if it's a permissions issue. I read that as long as I name my bucket with the string \"sagemaker\" it should have access to it.<\/p>",
        "Challenge_closed_time":1540283269856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1540264856293,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to import a CSV file using pandas.read_csv() in AWS Sagemaker. The error message says \"AttributeError: module 'pandas' has no attribute 'core'\". The user modified the code to point to the input file hosted on an S3 bucket, but it did not resolve the issue. The user is unsure if it's a permissions issue and has named the bucket with the string \"sagemaker\" to ensure access.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52940677",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":13.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":5.1148786111,
        "Challenge_title":"AWS Sagemaker: AttributeError: module 'pandas' has no attribute 'core'",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1028.0,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Poster_created_time":1319234288808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4966.0,
        "Poster_view_count":304.0,
        "Solution_body":"<p>Pull our data from S3 for example:<\/p>\n\n<pre><code>import boto3\nimport io\nimport pandas as pd\n\n\n# Set below parameters\nbucket = '&lt;bucket name&gt;'\nkey = 'data\/training\/iris.csv'\nendpointName = 'decision-trees'\n\n# Pull our data from S3\ns3 = boto3.client('s3')\nf = s3.get_object(Bucket=bucket, Key=key)\n\n# Make a dataframe\nshape = pd.read_csv(io.BytesIO(f['Body'].read()), header=None)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.9,
        "Solution_reading_time":5.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":42.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2183.8986427778,
        "Challenge_answer_count":2,
        "Challenge_body":"We have a gitlab repo within a private VPN and would like to setup Studio to clone that repo and to push and pull updates. Is that possible yet from within Studio?",
        "Challenge_closed_time":1650994900956,
        "Challenge_comment_count":1,
        "Challenge_created_time":1643132865842,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to connect a Sagemaker Studio user to a gitlab repo within a private VPN and is seeking guidance on how to set up Studio to clone the repo and push and pull updates.",
        "Challenge_last_edit_time":1668578369590,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QURGs7VOVlTzKCG7H2AFLWww\/how-can-we-connect-a-sagemaker-studio-user-to-a-gitlab-repo-within-a-private-vpn",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.2,
        "Challenge_reading_time":2.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2183.8986427778,
        "Challenge_title":"How can we connect a Sagemaker Studio user to a gitlab repo within a private VPN?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":376.0,
        "Challenge_word_count":47,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thank you for your response. For those looking to do the same thing, according to AWS Support AWS SageMakers does NOT support GitLab yet and there is no ETA for that feature.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1650994900956,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":2.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1270568377790,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Philadelphia",
        "Answerer_reputation_count":13056.0,
        "Answerer_view_count":354.0,
        "Challenge_adjusted_solved_time":1.5477955556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using AWS sagemaker, I have some secret keys and access keys to access some APIs that I don't want to expose directly in code.<\/p>\n<p>What are the ways like environment variables etc., that can be used to hide these keys and I can use them securely, and how to set them.<\/p>",
        "Challenge_closed_time":1625064328872,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625052591003,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking ways to securely hide secret keys and access keys for APIs while using AWS sagemaker. They are looking for methods such as environment variables to set and use these keys securely.",
        "Challenge_last_edit_time":1625058756808,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68193944",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.4,
        "Challenge_reading_time":3.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":3.2605191667,
        "Challenge_title":"Set custom environment variables in AWS",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":277.0,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Poster_created_time":1567880532003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lahore, Pakistan",
        "Poster_reputation_count":137.0,
        "Poster_view_count":100.0,
        "Solution_body":"<p>AWS System Manager (SSM) is designed to store keys and tokens securely.<\/p>\n<p>Depending on how your notebook is defined, you could <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">use the 'env' property<\/a> directly or in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-environment-variables\" rel=\"nofollow noreferrer\">training data<\/a>, or you could access SSM directly from sagemaker. For example this Snowflake KB article explains how to fetch auth info from ssm: <a href=\"https:\/\/community.snowflake.com\/s\/article\/Connecting-a-Jupyter-Notebook-Part-3\" rel=\"nofollow noreferrer\">https:\/\/community.snowflake.com\/s\/article\/Connecting-a-Jupyter-Notebook-Part-3<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":23.8,
        "Solution_reading_time":11.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1411212343683,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, BC, Canada",
        "Answerer_reputation_count":3592.0,
        "Answerer_view_count":268.0,
        "Challenge_adjusted_solved_time":1.5132372222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've uploaded my own Jupyter notebook to Sagemaker, and am trying to create an iterator for my training \/ validation data which is in S3, as follow:<\/p>\n\n<pre><code>train = mx.io.ImageRecordIter(\n        path_imgrec         = \u2018s3:\/\/bucket-name\/train.rec\u2019 \u2026\u2026 )\n<\/code><\/pre>\n\n<p>I receive the following exception: <\/p>\n\n<pre><code>MXNetError: [04:33:32] src\/io\/s3_filesys.cc:899: Need to set enviroment variable AWS_SECRET_ACCESS_KEY to use S3\n<\/code><\/pre>\n\n<p>I've checked that the IAM role attached with this notebook instance has S3 access. Any clues on what might be needed to fix this?<\/p>",
        "Challenge_closed_time":1522305475727,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522300028073,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is trying to create an iterator for their training\/validation data in S3 using AWS Sagemaker, but is encountering an exception that requires setting the AWS_SECRET_ACCESS_KEY environment variable to use S3. The user has checked that the IAM role attached to the notebook instance has S3 access and is seeking help to fix the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49548422",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":7.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.5132372222,
        "Challenge_title":"Training data in S3 in AWS Sagemaker",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1353.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449031669083,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":777.0,
        "Poster_view_count":103.0,
        "Solution_body":"<p>If your IAM roles are setup correctly, then you need to download the file to the Sagemaker instance first and then work on it. Here's how:<\/p>\n\n<pre><code># Import roles\nimport sagemaker\nrole = sagemaker.get_execution_role()\n\n# Download file locally\ns3 = boto3.resource('s3')\ns3.Bucket(bucket).download_file('your_training_s3_file.rec', 'training.rec')\n\n#Access locally\ntrain = mx.io.ImageRecordIter(path_imgrec=\u2018training.rec\u2019 \u2026\u2026 )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.6,
        "Solution_reading_time":5.8,
        "Solution_score_count":4.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":49.0639686111,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hello,<\/p>\n<p>After several tries I am getting stuck when finishing a run about data uploading. It always throws the same error and it seems it \u2018kills\u2019 my network, consuming all resources as when I try to access other pages the internet is very slow.<\/p>\n<p>When I try to do the tests I always have a good connection (around 600Mb), and I tried on different days. I never have connection cuts, just when doing the run finish.<\/p>\n<p>The error I have is the following:<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/a27d197a4c8825679f7e5c459ef89d5f87a2a192.png\" alt=\"Selection_001\" data-base62-sha1=\"nbrnemiqAtyylTXSAgkOD1SYaSS\" width=\"573\" height=\"182\"><\/p>",
        "Challenge_closed_time":1675199007678,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675022377391,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a network error (ConnectTimeout) while finishing a run about data uploading. The error is consuming all network resources and slowing down the internet. The user has a good connection while doing tests and never experiences connection cuts, only when finishing the run.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/network-error-connecttimeout-entering-retry-loop\/3772",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":49.0639686111,
        "Challenge_title":"Network error (ConnectTimeout), entering retry loop",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":220.0,
        "Challenge_word_count":94,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey Mario!<\/p>\n<p>We have been having an influx of traffic lately. Please try again and if you still run into the ConnectTimeout Network error, please send us your debug.log so we can take a closer look.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":2.57,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1550902509267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2669.0,
        "Answerer_view_count":3292.0,
        "Challenge_adjusted_solved_time":17.0441908334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I had configured my databricks workspace in local using,<\/p>\n<p><code>databricks configure --profile &lt;profile_name&gt; --token<\/code><\/p>\n<p>by which I am able to list the clusters and create secret scope.<\/p>\n<p>But I am unable to create mlflow experiments. I had set the tracking uri to &quot;databricks&quot; and also tested with &quot;databricks\/&lt;profile_name&quot; and tested but i am unable to create or track any experiments on my databricks workspace.<\/p>\n<p>I get this following error;<\/p>\n<p><code>from mlflow.tracking import MlflowClient client = MlflowClient() mlflow.set_tracking_uri(&quot;databricks&quot;) experiment =  client.get_experiment_by_name('\/Shared\/test')<\/code><\/p>\n<p>MlflowException: API request to endpoint was successful but the response body was not in a valid JSON format. Response body: '&lt;!doctype html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;\/&gt;&lt;meta http-equiv=&quot;Content-Language&quot; content=&quot;en&quot;\/&gt;&lt;title&gt;Databricks - Sign In&lt;\/title&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=960&quot;\/&gt;&lt;link rel=&quot;icon&quot; type=&quot;image\/png&quot; href=&quot;\/favicon.ico&quot;\/&gt;&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text\/html; charset=UTF8&quot;\/&gt;&lt;link rel=&quot;icon&quot; href=&quot;\/favicon.ico&quot;&gt;&lt;script defer=&quot;defer&quot; src=&quot;\/login\/login.0ceb14c0.js&quot;&gt;&lt;\/script&gt;&lt;\/head&gt;&lt;body class=&quot;light-mode&quot;&gt;&lt;uses-legacy-bootstrap&gt;&lt;div id=&quot;login-page&quot;&gt;&lt;\/div&gt;&lt;\/uses-legacy-bootstrap&gt;&lt;\/body&gt;&lt;\/html&gt;'<\/p>\n<p>Could someone help me on what I am missing here?<\/p>\n<p>I am expecting to create\/track mlflow experiements in databricks workspace via dev-tools(vscode).<\/p>",
        "Challenge_closed_time":1649225844710,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649164485623,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has configured their Databricks workspace in local using a token and is able to list clusters and create secret scope, but they are unable to create MLflow experiments. They have set the tracking URI to \"databricks\" and tested with \"databricks\/<profile_name>\" but are unable to create or track any experiments on their Databricks workspace. They are getting an error message that the response body was not in a valid JSON format. The user is seeking help to create\/track MLflow experiments in Databricks workspace via dev-tools (VSCode).",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71752458",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.7,
        "Challenge_reading_time":25.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":17.0441908334,
        "Challenge_title":"MLflow Experiments Tracking : local (dev tools - vscode) to databricks workspace",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":220.0,
        "Challenge_word_count":150,
        "Platform":"Stack Overflow",
        "Poster_created_time":1515569029903,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":5.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>I had the same problem while trying to load a model from model registry with mismatching versions (client 1.22.0).<\/p>\n<p>I had to downgrade the client version to make it work.<\/p>\n<p>Downgraded first the client to 1.21 and then server to 1.20<\/p>\n<p>Refer - <a href=\"https:\/\/docs.databricks.com\/dev-tools\/api\/latest\/mlflow.html#operation\/transition-model-version-stage\" rel=\"nofollow noreferrer\">https:\/\/docs.databricks.com\/dev-tools\/api\/latest\/mlflow.html#operation\/transition-model-version-stage<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.4,
        "Solution_reading_time":6.8,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":46.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1422222222,
        "Challenge_answer_count":0,
        "Challenge_body":"Hello,\r\n\r\nAfter I tried to build a Conda environment using mlu-tab.yml I was ran out of space with no environment created. After I deleted all files from my home folder I still had 95% of my space used. There is no way to \"reimage\" my Studio Lab instance and get back the initial 30Gb of space.\r\n\r\nI followed the AWS Machine Learning University course and cloned the examples for Tabular data course: [(https:\/\/github.com\/aws-samples\/aws-machine-learning-university-accelerated-tab)]\r\n\r\nAfter that I was stupid enough to try creating the Conda environment using the mlu-tab.yml file. the environment creation ate all my space available and creation was failed.\r\nCurrently I have 95% space usage of my \/home\/studio-lab-user folder with no files in it.\r\n\r\nHow can I reimage SageMaker Studio Lab instance to get the space back or uninstall all libraries installed by creating the Conda environment?\r\n\r\nOS: Windows 10\r\nBrowser: Chrome 107.0.5304.107\r\n\r\n![space issue1](https:\/\/user-images.githubusercontent.com\/12427856\/202601233-b7378b40-17d6-4ea3-8e8c-c96bebde0010.png)\r\n![space issue2](https:\/\/user-images.githubusercontent.com\/12427856\/202601236-c6fe41d5-0171-4539-8d82-3eaf0577f427.png)\r\n",
        "Challenge_closed_time":1668738306000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668737794000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"SageMaker Studio Lab is experiencing elevated errors starting runtimes since November 16, 2022, at 04:00 PM PST. Users are unable to open projects and are receiving an ERR_EMPTY_RESPONSE error in the browser. The SageMaker Studio Lab team is working to restore the service, and users are advised to try again later.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/167",
        "Challenge_link_count":3,
        "Challenge_participation_count":0,
        "Challenge_readability":10.1,
        "Challenge_reading_time":15.87,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.1422222222,
        "Challenge_title":"inability to reimage SageMaker Studio Lab instance to get the space back",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":160,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":326.7363888889,
        "Challenge_answer_count":0,
        "Challenge_body":"When I tried to run benchmark on sagemaker with anubis, it showed processing benchmark submission request and then cannot execute the requested benchmark. \r\n<img width=\"1038\" alt=\"smmrcnn\" src=\"https:\/\/user-images.githubusercontent.com\/54413235\/66169329-e0ee3800-e5f4-11e9-887f-8e6fce87a917.png\">\r\n\r\nI also tried to run the sample for sagemaker https:\/\/github.com\/MXNetEdge\/benchmark-ai\/blob\/master\/sample-benchmarks\/sagemaker\/horovod.toml   and it showed with the same error\r\n<img width=\"1018\" alt=\"smsample\" src=\"https:\/\/user-images.githubusercontent.com\/54413235\/66169407-201c8900-e5f5-11e9-9de7-b46a7e9501a4.png\">\r\n\r\n\r\nBTW, when we wanna run with sagemaker, besides specify  execution_engine = \"aws.sagemaker\" and framework , is there anything else we need to specify or change?\r\n",
        "Challenge_closed_time":1571326528000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1570150277000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with the data path inside a Sagemaker notebook as the bucket of processed data does not exist, resulting in a NoSuchBucket error when calling the ListObjectsV2 operation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/benchmark-ai\/issues\/907",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":10.68,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":1054.0,
        "Challenge_repo_star_count":11.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":326.7363888889,
        "Challenge_title":"Cannot run benchmark for sagemaker",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":75,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"3 tactics were used to address this issue (by @perdasilva): \r\nStop gap, watcher, error reporting in the status.\r\n@haohanchen-yagao - Please confirm and the close this issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":2.12,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1657058369727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":2.7470963889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am calling <code>Sagemaker API<\/code> from python script inside <code>EC2<\/code> instance to create online feature store. I gave required permission and its creating feature group.\nHowever I observed that key I'm passing in below program (<code>online_store_kms_key_id = 'arn:aws:kms:us-east-1:1234:key\/1111'<\/code>) is not being used to write objects to s3 bucket instead it's using default bucket key.\nI'm not sure what is causing this to happen? Why its not using key given in create feature group config? Any idea?<\/p>\n<p>code snippet:<\/p>\n<pre><code>customer_data = pd.read_csv(&quot;data.csv&quot;,dtype={'customer_id': int,'city_code': int, 'state_code': int, 'country_code': int, 'eventtime': float })\n\n    customers_feature_group_name = &quot;customers-fg-01&quot;\n    customers_feature_group = FeatureGroup(name=customers_feature_group_name, sagemaker_session=sagemaker_session\n                                           )\n\n    current_time_sec = int(round(time.time()))\n\n    record_identifier_feature_name = &quot;customer_id&quot;\n\n    customers_feature_group.load_feature_definitions(data_frame=customer_data)\n\n    customers_feature_group.create(\n        s3_uri=&quot;s3:\/\/xxxx\/sagemaker-featurestore\/&quot;,\n        record_identifier_name=record_identifier_feature_name,\n        event_time_feature_name=&quot;eventtime&quot;,\n        role_arn='arn:aws:iam::1234:role\/role-1234',\n        enable_online_store=True,\n        online_store_kms_key_id = 'arn:aws:kms:us-east-1:1234:key\/1111'\n    )\n<\/code><\/pre>",
        "Challenge_closed_time":1659050391287,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659040058783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while creating an online feature store using Sagemaker API from a Python script inside an EC2 instance. The key provided in the create feature group configuration is not being used to write objects to the S3 bucket, instead, the default bucket key is being used. The user is seeking help to understand why this is happening.",
        "Challenge_last_edit_time":1659040501740,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73158818",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":18.4,
        "Challenge_reading_time":19.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":2.87014,
        "Challenge_title":"Sagemaker API online feature store creation not using given kms key",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":47.0,
        "Challenge_word_count":116,
        "Platform":"Stack Overflow",
        "Poster_created_time":1365570541220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":519.0,
        "Poster_view_count":576.0,
        "Solution_body":"<p>For encryption of data stored in s3 ( offline store ) you need to add a field\n'offline_store_kms_key_id ' to the create() method call, please refer the document below<\/p>\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/prep_data\/feature_store.html#sagemaker.feature_store.feature_group.FeatureGroup.create\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/prep_data\/feature_store.html#sagemaker.feature_store.feature_group.FeatureGroup.create<\/a><\/p>\n<p>Also please go through the below document to check the policies and also to verify if you have a symmetric customer managed keys or asymmetric customer managed keys as feature store only supports symmetric keys.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/feature-store-security.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/feature-store-security.html<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":22.8,
        "Solution_reading_time":12.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":69.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1415722650716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Verona, VR, Italy",
        "Answerer_reputation_count":4811.0,
        "Answerer_view_count":713.0,
        "Challenge_adjusted_solved_time":20.2557158334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to connect <strong>Azure SQL Database<\/strong> from <strong>Azure Machine Learning service<\/strong>, but I got the below error.<\/p>\n\n<p><strong>Please check Error: -<\/strong><\/p>\n\n<pre><code>**('IM002', '[IM002] [unixODBC][Driver Manager]Data source name not found and no default driver specified (0) (SQLDriverConnect)')**\n<\/code><\/pre>\n\n<p>Please Check the below code that I have used for database connection: -<\/p>\n\n<pre><code>import pyodbc\n\nclass DbConnect:\n    # This class is used for azure database connection using pyodbc\n    def __init__(self):\n        try:\n            self.sql_db = pyodbc.connect(SERVER=&lt;servername&gt;;PORT=1433;DATABASE=&lt;databasename&gt;;UID=&lt;username&gt;;PWD=&lt;password&gt;')\n\n            get_name_query = \"select name from contacts\"\n            names = self.sql_db.execute(get_name_query)\n            for name in names:\n                print(name)\n\n        except Exception as e:\n            print(\"Error in azure sql server database connection : \", e)\n            sys.exit()\n\nif __name__ == \"__main__\":\n    class_obj = DbConnect()\n<\/code><\/pre>\n\n<p>Is there any way to solve the above error? Please let me know if there is any way.<\/p>",
        "Challenge_closed_time":1569108921710,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569073789150,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to connect Azure SQL Database from Azure Machine Learning service using Python. The error message indicates that the data source name is not found and no default driver is specified. The user has provided the code used for database connection and is seeking a solution to the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58040933",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":14.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":9.7590444445,
        "Challenge_title":"Error in connecting Azure SQL database from Azure Machine Learning Service using python",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1013.0,
        "Challenge_word_count":132,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554466050936,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":219.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>I'd consider using <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-dataprep\/azureml.dataprep?view=azure-dataprep-py\" rel=\"nofollow noreferrer\"><code>azureml.dataprep<\/code><\/a> over pyodbc for this task (the API may change, but this worked last time I tried):<\/p>\n\n<pre><code>import azureml.dataprep as dprep\n\nds = dprep.MSSQLDataSource(server_name=&lt;server-name,port&gt;,\n                           database_name=&lt;database-name&gt;,\n                           user_name=&lt;username&gt;,\n                           password=&lt;password&gt;)\n<\/code><\/pre>\n\n<p>You should then be able to collect the result of an SQL query in pandas e.g. via<\/p>\n\n<pre><code>dataflow = dprep.read_sql(ds, \"SELECT top 100 * FROM [dbo].[MYTABLE]\")\ndataflow.to_pandas_dataframe()\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1569146709727,
        "Solution_link_count":1.0,
        "Solution_readability":12.5,
        "Solution_reading_time":9.52,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":59.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1653511725307,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":26.0,
        "Challenge_adjusted_solved_time":165.2682675,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I created a sagemaker project with a terraform template which successfully created with a stack successfully created and associated with it. However, there is no repository associated or pipeline associated with the sagemaker project despite there being both in the cloudformation template I used. Can someone help with this?<\/p>\n<p>Is there a way to manually link a sagemaker project with a code commit repository? I see that succesfully linked repositories have the tag: <code>sagemaker:project-name<\/code> with the correct project name.<\/p>",
        "Challenge_closed_time":1657906025030,
        "Challenge_comment_count":2,
        "Challenge_created_time":1657295938437,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has successfully created a Sagemaker project using a Terraform template, but there are no linked pipelines or repositories associated with it, despite being present in the CloudFormation template used. The user is seeking help to manually link the Sagemaker project with a CodeCommit repository.",
        "Challenge_last_edit_time":1657311059267,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72914046",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":11.5,
        "Challenge_reading_time":8.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":169.4684980556,
        "Challenge_title":"Sagemaker Project successfully creates but there are no linked pipelines or repositories",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":124.0,
        "Challenge_word_count":90,
        "Platform":"Stack Overflow",
        "Poster_created_time":1653511725307,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>Using a different cloudformation template fixed the issue. Not sure why.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1381858437316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":2720.0,
        "Answerer_view_count":482.0,
        "Challenge_adjusted_solved_time":0.8205025,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to query a MS Access Web App (SQL Azure) using the Azure ML platform. The field I'm trying to capture is type <code>Fixed-point number (6 decimal places)<\/code>, the default numeric field type in Azure SQL. When I try to query this field, I get the error:<\/p>\n\n<p><code>Error 1000: AFx Library library exception: Type Decimal is not supported<\/code><\/p>\n\n<p>I have tried casting it to another form like follows:<\/p>\n\n<p><code>select cast(a) FROM b<\/code><\/p>\n\n<p>And got the error:<\/p>\n\n<p><code>Error 0069: SQL query \"select cast(\"a\" as float) from \"b\"\" is not correct:\nColumn names cannot be null or empty.<\/code><\/p>\n\n<p>What gives?<\/p>\n\n<p>Furthermore, how isn't the default on Azure SQL supported in Azure ML???<\/p>",
        "Challenge_closed_time":1471362745296,
        "Challenge_comment_count":3,
        "Challenge_created_time":1471359791487,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error (Error 1000: AFx Library library exception: Type Decimal is not supported) while trying to query a Fixed-point number (6 decimal places) field in an MS Access Web App (SQL Azure) using the Azure ML platform. The user has tried casting the field to another form but is still getting an error (Error 0069: SQL query \"select cast(\"a\" as float) from \"b\"\" is not correct: Column names cannot be null or empty). The user is questioning why the default on Azure SQL is not supported in Azure ML.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38978361",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.8205025,
        "Challenge_title":"Error 1000: AFx Library library exception: Type Decimal is not supported",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":2105.0,
        "Challenge_word_count":122,
        "Platform":"Stack Overflow",
        "Poster_created_time":1381858437316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":2720.0,
        "Poster_view_count":482.0,
        "Solution_body":"<p>As per serhiyb's answer, the win was to assign it to another variable:<\/p>\n\n<p><code>Select cast(\"field\" as float) as 'someAlias' FROM \"Table\"<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":1.98,
        "Solution_score_count":4.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":21.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":50.0496855556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was wondering whether there is some size limit for storing a folder in sagemaker studio?\nI have this dataset stored in s3 but I want to download that dataset in my sagemaker studio environment to train my model. Is there some kind of limit in the size of a file i can download?<\/p>",
        "Challenge_closed_time":1638800978928,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638620800060,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the size limit for storing a folder in Sagemaker Studio and whether there is a limit on the size of a file that can be downloaded for training a model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70225564",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.5,
        "Challenge_reading_time":3.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":50.0496855556,
        "Challenge_title":"Sagemaker file size limit?",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":398.0,
        "Challenge_word_count":57,
        "Platform":"Stack Overflow",
        "Poster_created_time":1638359312992,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tasks-manage-storage.html\" rel=\"nofollow noreferrer\">home directory in Amazon SageMaker Studio is stored in Amazon EFS file system<\/a>. The file system grows and shrink as you add and remove files.<br \/>\nAccording to <a href=\"https:\/\/docs.aws.amazon.com\/efs\/latest\/ug\/limits.html#limits-fs-specific\" rel=\"nofollow noreferrer\">Amazon EFS limits<\/a> the <strong>maximum size for a single file is 47.9TiB.<\/strong><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.5,
        "Solution_reading_time":6.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0197222222,
        "Challenge_answer_count":0,
        "Challenge_body":"Archived from slack discussion!\n\nI\u2019m receiving the following error recently, but it only occurs when I use a VPN, if I\u2019m in the office I don\u2019t get this issue.\nHas anyone received similar or have any clues on what might be the problem?\n\n File \"\/usr\/local\/opt\/python@3.8\/Frameworks\/Python.framework\/Versions\/3.8\/lib\/python3.8\/ssl.py\", line 1019, in _create\n    self.getpeername()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', OSError(22, 'Invalid argument'))",
        "Challenge_closed_time":1667914455000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667914384000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while installing the Polyaxon Python client. The error message is \"urllib3.exceptions.ProtocolError: ('Connection aborted.', OSError(22, 'Invalid argument'))\". The error only occurs when using a VPN and not in the office. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1519",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.5,
        "Challenge_reading_time":7.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.0197222222,
        "Challenge_title":"Issue installing polyaxon python client urllib3.exceptions.ProtocolError: OSError",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":62,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Sometimes the CA bundle is not up-to date and needs to be installed manually, especially if it's a new Python version. Please run the following commands:\n\ncd \/Applications\/Python\\ 3.8\/\n\nthen\n\n.\/Install\\ Certificates.command",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":2.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1406731060412,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":139.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":268.3228897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am setting up SageMaker for a group outside my organization, and wondering how I provide access to SageMaker Studio (and any dependent AWS resources SageMaker would use) to these users.\nThe idea is to have then use SageMaker studio to do train and test models.<\/p>\n<p>Ideally, it would be great if I simply send them a link where they sign in, and are then granted access to SageMaker hosted from my AWS account.<\/p>\n<p>I believe to grant temporary access I need to attach a <strong>role<\/strong> to a group of <strong>users<\/strong>, and also attach a <strong>policy<\/strong> to this group, them finally associate this role with SageMaker. But how do I then create a <strong>link<\/strong> to SageMaker so these users can sign in?<\/p>\n<p>The other option would be to set this up using cross-account access, since those I want to give access to also have their own AWS account. But again, how does one generate a link to direct these users to the SageMaker on my AWS account?<\/p>\n<p>There appears to be 2 options, as per <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-onboard.html\" rel=\"nofollow noreferrer\">SageMaker Onboarding<\/a>:<\/p>\n<ol>\n<li><strong>AWS SSO authentication<\/strong><\/li>\n<\/ol>\n<ul>\n<li>access to Studio via unique sign-in URL that directly opens Studio<\/li>\n<li>sign in with their SSO credentials<\/li>\n<li>organizations manages members in AWS SSO instead of Studio<\/li>\n<li>can assign multiple members access to Studio at the same time<\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>IAM authentication<\/strong><\/li>\n<\/ol>\n<ul>\n<li>sign in through the SageMaker console<\/li>\n<li>must add and manage members manually one at time using the Studio Control Panel<\/li>\n<\/ul>\n<p>I don't understand the 2nd approach, since wouldn't this mean users would have to sign-in as root to the console anyway and then have full access. I could edit the policy attached to IAM users, but this begs another question:<\/p>\n<p>...does SageMaker come pre-baked with policies that include its dependent AWS services? For example, SageMaker will use S3 for storage and EC2 for processing; do I need to set these individually in the policy, or can I simply use a SageMaker policy, which will include all those dependencies by default?<\/p>",
        "Challenge_closed_time":1626467994683,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625496004123,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to provide access to AWS SageMaker Studio to a group outside their organization. They are looking for a way to grant temporary access by attaching a role and policy to a group of users and then associating it with SageMaker. The user is also exploring two options for authentication: AWS SSO authentication and IAM authentication. They are unsure about the second approach and whether SageMaker comes with pre-baked policies that include its dependent AWS services.",
        "Challenge_last_edit_time":1625502032280,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68258003",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":29.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":269.9973777778,
        "Challenge_title":"How does one make AWS SageMaker available to people outside their organization?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1359.0,
        "Challenge_word_count":352,
        "Platform":"Stack Overflow",
        "Poster_created_time":1346443720088,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11650.0,
        "Poster_view_count":977.0,
        "Solution_body":"<p>First of all, you can generate &quot;disposable&quot; (called pre-signed) URLs which can be used for accessing SageMaker Studio User Profiles without any AWS credentials. These URls can be valid for max 5 minutes and can be generated with a single AWS API call.<\/p>\n<p>One approach to provide Studio access to your users is to set up a service which can authenticate your Studio users and then calls the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePresignedDomainUrl.html\" rel=\"nofollow noreferrer\">CreatePresignedDomainUrl SageMaker API<\/a> method to sends back the generated pre-signed URL to the user.<\/p>\n<p>Alternatively, you can use AWS SSO as well, which can do most of the heavy lifting for you, especially if you'd like to integrate with a single sign-on service. AWS SSO integrates with SageMaker Studio and you can assign Studio user profiles to your onboarded users. Your users then can go through your single sign-on service and can launch the Studio without logging into the AWS Console.<\/p>\n<p>An another approach is to use IAM Federation where you basically provide access to the AWS API and\/or to the Console to your users which authenticated by an (external) identity provider. Federated users can assume specific roles to operate with the AWS API or the Management Console. For accessing SageMaker Studio, users just need to have the CreatePresignedDomainUrl access policy which allows them to create the pre-signed URL by themselves. If you want to isolate your SageMaker user profiles and ensure each federated user can access just those user profiles which are assigned to them, please see the following <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/configuring-amazon-sagemaker-studio-for-teams-and-groups-with-complete-resource-isolation\/\" rel=\"nofollow noreferrer\">blog post<\/a> for more information.<\/p>\n<p>And finally, please note that, once the user has logged in to Studio, the Execution Role configured for the specific user profile will determine what the Studio user can access and is able to do (e.g. spinning up SageMaker training jobs, deploying models, accessing S3, etc). Thus, you don't need to set up these policies for your IAM users or roles used by the federated users.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.9,
        "Solution_reading_time":28.44,
        "Solution_score_count":3.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":325.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1515682479260,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":754.0,
        "Answerer_view_count":113.0,
        "Challenge_adjusted_solved_time":1.4016325,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I create one compute instance 'yhd-notebook' in Azure Machine Learning compute with user1. When I login with user2, and try to open the JupyterLab of this compute instance, it shows an error message like below.<\/p>\n\n<blockquote>\n  <p>User user2 does not have access to compute instance yhd-notebook.<\/p>\n  \n  <p>Only the creator can access a compute instance.<\/p>\n  \n  <p>Click here to sign out and sign in again with a different account.<\/p>\n<\/blockquote>\n\n<p>Is it possible to share compute instance with another user? BTW, both user1 and user2 have Owner role with the Azure subscription.<\/p>",
        "Challenge_closed_time":1583393458820,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583388412943,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user created a compute instance in Azure Machine Learning with user1, but when trying to access it with user2, an error message appears stating that only the creator can access the instance. The user is wondering if it is possible to share the compute instance with another user, even though both users have Owner role with the Azure subscription.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60539094",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.9,
        "Challenge_reading_time":7.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.4016325,
        "Challenge_title":"Is it possible to share compute instance with other user?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":3705.0,
        "Challenge_word_count":99,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582361231692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Guangzhou, China",
        "Poster_reputation_count":393.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>According to MS, all users in the workspace contributor and owner role can create, delete, start, stop, and restart compute instances across the workspace. However, only the creator of a specific compute instance is allowed to access Jupyter, JupyterLab, and RStudio on that compute instance. The creator of the compute instance has the compute instance dedicated to them, have root access, and can terminal in through Jupyter. Compute instance will have single-user login of creator user and all actions will use that user\u2019s identity for RBAC and attribution of experiment runs. SSH access is controlled through public\/private key mechanism.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":8.09,
        "Solution_score_count":7.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":99.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":83.6952572222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am getting this error on a Linux box (Gentoo w\/ .NET via Mono properly installed)<\/p>\n<p><strong>&quot;Unable to retrieve .NET dependencies. Please make sure you are connected to the Internet and have a stable network connection.&quot;<\/strong><\/p>\n<p>The error is triggered when creating a dataset from a directory using<\/p>\n<p>&quot;dataset = Dataset.File.from_files(path=(datastore, path_to_dataset_in_datastore))&quot;<\/p>\n<p>Some system info:  <br \/>\nPython: 3.8.8.  <br \/>\nazureml-automl-core 1.26.0  <br \/>\nazureml-core 1.26.0  <br \/>\nazureml-dataprep 2.13.2  <br \/>\nazureml-dataprep-native 32.0.0  <br \/>\nazureml-dataprep-rslex 1.11.2  <br \/>\nazureml-dataset-runtime 1.26.0  <br \/>\nazureml-pipeline 1.26.0  <br \/>\nazureml-pipeline-core 1.26.0  <br \/>\nazureml-pipeline-steps 1.26.0  <br \/>\nazureml-sdk 1.26.0  <br \/>\nazureml-telemetry 1.26.0  <br \/>\nazureml-train 1.26.0  <br \/>\nazureml-train-automl-client 1.26.0  <br \/>\nazureml-train-core 1.26.0  <br \/>\nazureml-train-restclients-hyperdrive 1.26.0<\/p>\n<p>.NET Info:  <br \/>\nMono JIT compiler version 6.6.0.161 (tarball Sat Apr 10 16:41:12 PDT 2021)  <br \/>\nCopyright (C) 2002-2014 Novell, Inc, Xamarin Inc and Contributors. <a href=\"https:\/\/www.mono-project.com\">www.mono-project.com<\/a>  <br \/>\nTLS: __thread  <br \/>\nSIGSEGV: altstack  <br \/>\nNotifications: epoll  <br \/>\nArchitecture: amd64  <br \/>\nDisabled: none  <br \/>\nMisc: softdebug  <br \/>\nInterpreter: yes  <br \/>\nLLVM: supported, not enabled.  <br \/>\nSuspend: hybrid  <br \/>\nGC: sgen (concurrent by default)<\/p>",
        "Challenge_closed_time":1619068615363,
        "Challenge_comment_count":2,
        "Challenge_created_time":1618767312437,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error on a Linux box while creating a dataset from a directory using AzureML. The error message states that it is unable to retrieve .NET dependencies and suggests checking the internet connection and network stability. The system information and .NET info are also provided.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/361522\/azureml-error-on-linux-unable-to-retrieve-net-depe",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":7.2,
        "Challenge_reading_time":20.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":83.6952572222,
        "Challenge_title":"AzureML Error on Linux: \"Unable to retrieve .NET dependencies. Please make sure you are connected ...\"",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":179,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=48487b4e-cd4d-4b5e-aae5-dcf8dfaf50c4\">@Victor Fragoso  <\/a>  Thanks for the details. Gentoo is not a 'natively' supported distribution of linux for Datasets. The Exception message doesn't link to a .NET docs page with instructions on installing the system dependencies required for .NET to work. Though it seems a different one is being thrown related to not being able to connect to out blob storage which has pre-prepared dependency sets for some linux distros (not gentoo).    <\/p>\n<p>This page <a href=\"https:\/\/learn.microsoft.com\/en-us\/dotnet\/core\/install\/linux\">Install .NET on Linux Distributions | Microsoft Learn<\/a> does not detail support for .NET on gentoo.    <br \/>\nYou can get the names of the missing dependencies themselves by running:    <\/p>\n<pre><code>from dotnetcore2 import runtime  \nruntime._enable_debug_logging()  \nruntime.ensure_dependencies()  \n<\/code><\/pre>\n<p>This code snippet should print the libraies missing required by .NET core 2.1.    <br \/>\nIf the above does not print anything, other than the Exception, then instead this should:    <\/p>\n<pre><code>from dotnetcore2 import runtime  \nprint(runtime._gather_dependencies(runtime._get_bin_folder()))  \n<\/code><\/pre>\n",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":15.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":148.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5196519444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Perform AZURE SQL query by joining table 1 from (Azure Prodution) database 1 to table 2 from (Azure Production) database 2?   <br \/>\nThe result to be saved as a table in database 3 (Development)  <\/p>\n<p>Eg.   <\/p>\n<p>SELECT  T1.CustomerID, T2.CustomerName  <br \/>\nFROM database1.SalesTable AS T1  <br \/>\nLEFT JOIN database2.CustomerTable AS T2  <\/p>\n<p>ON database1.SalesTable.CustomerID = database2.CustomerTable  <\/p>",
        "Challenge_closed_time":1643780375360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643778504613,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to know if it is possible to perform an Azure SQL query by joining tables from two different production databases and save the result as a table in a development database. An example query is provided.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/719171\/can-i-query-perform-sql-query-by-joining-table-1-f",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":7.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.5196519444,
        "Challenge_title":"Can I query perform SQL query by joining table 1 from (Prodution) database 1 to table 2 from (Production) databse 2? The result to be saved as a table in database 3 (Development)",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":88,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey,  <br \/>\nIt is possible by couple of ways:<\/p>\n<p>1) Create an external table in either of the Production database:  <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/azure-sql\/database\/elastic-query-getting-started-vertical\">https:\/\/learn.microsoft.com\/en-us\/azure\/azure-sql\/database\/elastic-query-getting-started-vertical<\/a><\/p>\n<p>Then in Azure data factory use a copy activity to have your query as source and your sink as the table 3 in database dev.  <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/connector-sql-server\">https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/connector-sql-server<\/a><\/p>\n<p>But based on the amount of data there might be performance issues due to elastic query so we can go with the below approach :<\/p>\n<p>2) use data flow to join both the data from table 1 and table 2 and copy into table 3<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.0,
        "Solution_reading_time":11.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":96.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1508517418056,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":156.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":3.9598897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to access the Workspace object in my <code>train.py<\/code> script, when running in an Estimator.  <\/p>\n\n<p>I currently can access the Run object, using the following code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>run = Run.get_context()\n<\/code><\/pre>\n\n<p>But I cannot seem to get my hands on the Workspace object in my training script.  I would use this mostly to get access to the Datastores and Datasets (as I would hope to keep all data set references inside the training script, instead of passing them as input datasets)<\/p>\n\n<p>Any idea if\/how this is possible ?<\/p>",
        "Challenge_closed_time":1591197955080,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591183699477,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to access the Workspace object in their training script in AzureML, specifically to get access to Datastores and Datasets. They are currently able to access the Run object but are unsure how to access the Workspace object.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62171724",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":8.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":3.9598897222,
        "Challenge_title":"How can I access the Workspace object from a training script in AzureML?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1148.0,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>Sure, try this:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\nws = run.experiment.workspace\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.8,
        "Solution_reading_time":1.78,
        "Solution_score_count":12.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":12.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1481732617127,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dallas, TX, United States",
        "Answerer_reputation_count":70265.0,
        "Answerer_view_count":9379.0,
        "Challenge_adjusted_solved_time":1.3378227778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have these two datasets defined:<\/p>\n<pre><code>flp_test_query:\n  type: pandas.SQLQueryDataSet\n  credentials: dw_dev_credentials\n  sql: select numero from dwdb.dwschema.flp_tst\n  load_args:\n    index_col: [numero]\n\nflp_test:\n  type: pandas.SQLTableDataSet\n  credentials: dw_dev_credentials\n  table_name: flp_tst\n  load_args:\n    index_col: ['numero']\n    columns: ['numero']\n  save_args:\n    if_exists: 'append'\n<\/code><\/pre>\n<p>However, I only manged to get <code>flp_test_query<\/code> working, as when I try to access <code>flp_tst<\/code> I get this error:<\/p>\n<blockquote>\n<p>ValueError: Table flp_tst not found<\/p>\n<\/blockquote>\n<p>I did try to define table name as <code>table_name: dwschema.flp_tst<\/code> and <code>table_name: dwdb.dwschema.flp_tst<\/code> but all trew the same error. What am I missing?<\/p>",
        "Challenge_closed_time":1626124577252,
        "Challenge_comment_count":2,
        "Challenge_created_time":1626119761090,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with Kedro where they are unable to access a SQL Server table named \"flp_tst\" using the defined dataset \"flp_test\". They have tried defining the table name in different ways but are still receiving a \"Table flp_tst not found\" error. They are able to access another dataset \"flp_test_query\" successfully.",
        "Challenge_last_edit_time":1644169948123,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68353241",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.1,
        "Challenge_reading_time":10.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.3378227778,
        "Challenge_title":"Kedro can not find SQL Server table",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":117.0,
        "Challenge_word_count":90,
        "Platform":"Stack Overflow",
        "Poster_created_time":1271930452580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":5469.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>From the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.extras.datasets.pandas.SQLTableDataSet.html\" rel=\"nofollow noreferrer\">docs<\/a> it looks like you can specify the schema in <code>load_args<\/code>, eg<\/p>\n<pre><code>  load_args:\n    index_col: ['numero']\n    columns: ['numero']\n    schema: 'dwschema'\n<\/code><\/pre>\n<p>or<\/p>\n<pre><code>load_args = {&quot;schema&quot;,&quot;dwschema&quot;}\ndata_set = SQLTableDataSet(table_name=table_name,\n                           credentials=credentials,\n                            load_args=load_args)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.4,
        "Solution_reading_time":6.87,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":58.1621369444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a CDM folder with data coming from Dynamics 365 Business Central.  <br \/>\nI need to do some data cleaning\/preprocessing and then apply my models on that data, but I didn't find a proper way to read CDM folders.  <br \/>\nI found some code on the Microsoft github repository, but is marked as obsolete.  <\/p>\n<p><a href=\"https:\/\/github.com\/Azure-Samples\/cdm-azure-data-services-integration\">Azure-Samples\/cdm-azure-data-services-integration<\/a>  <\/p>\n<p>I'm searching for something like the <strong>Apache Spark CDM connector<\/strong> but to use within Azure Machine Learning service.  <\/p>\n<p>ps: I know that is possible to copy\/transform files with <em>Azure Data Factory<\/em> and that is supports CDM folders too, but is not what I want. I want to read CDM folder from python, do my stuff (data cleaning, preprocessing, applying models, ecc) then save the results.  <\/p>\n<p>Is there any way?   <br \/>\nAny advice is welcome.  <\/p>\n<p>Thanks.  <\/p>",
        "Challenge_closed_time":1620687553120,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620478169427,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in reading data from a CDM folder that contains data from Dynamics 365 Business Central. They need to perform data cleaning and preprocessing before applying their models, but they have not found a proper way to read CDM folders. The user is searching for a solution similar to the Apache Spark CDM connector to use within Azure Machine Learning service. They do not want to use Azure Data Factory to copy\/transform files. The user is seeking advice and any possible solutions.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/387697\/read-data-from-cdm-folder",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":12.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":58.1621369444,
        "Challenge_title":"Read data from CDM folder",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":142,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. The data source you specified isn't a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#supported-data-storage-service-types\">supported storage type<\/a> in AML. If you're using unsupported storage, we recommend that you <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/quickstart-create-data-factory-copy-data-tool\">move<\/a> your data to supported Azure storage solutions. Currently, we don't have a python connector for connecting to CRMs. A workaround would be to load your data to a database and connect to the database using python. Hope this helps, sorry for any inconvenience.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.6,
        "Solution_reading_time":8.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1597855076910,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Delft, Netherlands",
        "Answerer_reputation_count":60.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":0.3015636111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am unable to perform the simple action:<\/p>\n<pre><code>import sagemaker\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n<\/code><\/pre>\n<p>because my notebook instance is not connected to the internet. I have an STS endpoint interface in the same subnet as my notebook instance but I thought the sagemaker API is using the global endpoint. I actually get the following error message after a while:<\/p>\n<pre><code>ConnectTimeoutError: Connect timeout on endpoint URL: &quot;https:\/\/sts.us-east-1.amazonaws.com\/&quot;\n<\/code><\/pre>\n<p>How do I fix this? Or does one need to update the sagemaker module?<\/p>",
        "Challenge_closed_time":1608634938236,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608633852607,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with AWS SageMaker as they are unable to connect to STS due to the internet being disabled. They are receiving a ConnectTimeoutError message and are seeking a solution to fix the problem or update the sagemaker module.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65407274",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.3015636111,
        "Challenge_title":"AWS SageMaker (with internet disabled) unable to connect to STS",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":510.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597855076910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delft, Netherlands",
        "Poster_reputation_count":60.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>So the solution is to include a VPC endpoint for the sagemaker API (api.sagemaker...) as well as STS.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":1.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1541523185320,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Ottawa, ON, Canada",
        "Answerer_reputation_count":1003.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":1.5810183334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What is the difference between these two? We used git-lfs in my previous job and we are starting to use dvc alongside git in my current one. They both place some kind of index instead of file and can be downloaded on demand. Has dvc some improvements over the former one?<\/p>",
        "Challenge_closed_time":1571925277763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1571919586097,
        "Challenge_favorite_count":5.0,
        "Challenge_gpt_summary_original":"The user is seeking to understand the difference between git-lfs and dvc, as they have used git-lfs in a previous job and are now using dvc alongside git in their current job. They note that both tools place an index instead of a file and can be downloaded on demand, and are wondering if dvc has any improvements over git-lfs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58541260",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.2,
        "Challenge_reading_time":3.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":27.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.5810183334,
        "Challenge_title":"Difference between git-lfs and dvc",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":6255.0,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373630643248,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":382.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>DVC is a better replacement for <code>git-lfs<\/code>. <\/p>\n\n<p>Unlike git-lfs, DVC doesn't require installing a dedicated server; It can be used on-premises (NAS, SSH, for example) or with any major cloud provider (S3, Google Cloud, Azure).<\/p>\n\n<p>For more information: <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-and-model-files-versioning\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/data-and-model-files-versioning<\/a><\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.7,
        "Solution_reading_time":5.71,
        "Solution_score_count":10.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":42.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":6.6123980556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I work on a project with DVC (Data version control). Let's say I make a lot of local commits. Something like this:<\/p>\n\n<pre><code># make changes for experiment 1\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 1\"\n\n# make changes for experiment 2\n# which change both code and data\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 2\"\n\n# make changes for experiment 3\n# which change both code and data\ndvc add my_data_file\ngit add my_data_file.dvc\ngit commit -m \"Experiment 3\"\n\n# Finally I'm done\n# push changes:\ndvc push\ngit push\n<\/code><\/pre>\n\n<p>However there is one problem: <code>dvc push<\/code> will only push data from experiment 3. Is there any way to push data from all local commits (i.e. starting from the first commit diverged from remote branch)?<\/p>\n\n<p>Currently I see two options:<\/p>\n\n<ol>\n<li>Tag each commit and push it with <code>dvc push -T<\/code><\/li>\n<li>After \"expermient 3\" commit execute <code>git checkout commit-hash &amp;&amp; dvc push<\/code> for all local commits not yet pushed to remote.<\/li>\n<\/ol>\n\n<p>Both these options seem cumbersome and error-prone. Is there any better way to do it?<\/p>",
        "Challenge_closed_time":1561842649416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561823734517,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with the \"dvc push\" command in a project that uses Data Version Control (DVC). The command only pushes data from the latest commit, and the user wants to push data from all local commits. The user is considering two options, tagging each commit and pushing it with \"dvc push -T\" or executing \"git checkout commit-hash && dvc push\" for all local commits not yet pushed to remote. The user is looking for a better way to push data from all local commits.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56818930",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":14.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":5.2541386111,
        "Challenge_title":"\"dvc push\" after several local commits",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":916.0,
        "Challenge_word_count":185,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522254698710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Russia",
        "Poster_reputation_count":784.0,
        "Poster_view_count":77.0,
        "Solution_body":"<p>@NShiny, there is a related ticket:<\/p>\n\n<p><a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1691\" rel=\"nofollow noreferrer\">support push\/pull\/metrics\/gc, etc across different commits<\/a>.<\/p>\n\n<p>Please, give it a vote so that we know how to prioritize it.<\/p>\n\n<p>As a workaround, I would recommend to run <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/install\" rel=\"nofollow noreferrer\"><code>dvc install<\/code><\/a>. It installs a <code>pre-push<\/code> GIt hook and runs <code>dvc push<\/code> automatically:<\/p>\n\n<pre><code>Git pre-push hook executes dvc push before git push to upload files and directories under DVC control to remote.\n<\/code><\/pre>\n\n<p>It means, though you need to run <code>git push<\/code> after every <code>git commit<\/code> :(<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1561847539150,
        "Solution_link_count":2.0,
        "Solution_readability":12.8,
        "Solution_reading_time":9.83,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":86.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":21.5217911111,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have a project \u201cfoo\u201d on my personal wandb account (entity \u201cuser\u201d). However, I am also a member of a team (\u201cteam\u201d). When I try to sync an offline run using <code>wandb sync path\/to\/foo\/run<\/code>, I want it to be saved in project \u201cfoo\u201d on my personal account. However, wandb creates a new project \u201cfoo\u201d that is owned by \u201cteam\u201d.<\/p>\n<p>Is there any way I can fix this? Do I need to change the way I\u2019m logged in to my wandb account? wandb says that I am logged in as <code>user (team)<\/code>, but I\u2019m not sure how to change that.<\/p>",
        "Challenge_closed_time":1683034536819,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682957058371,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has a personal project named \"foo\" on their Wandb account, but when they try to sync an offline run using \"wandb sync path\/to\/foo\/run\", it creates a new project named \"foo\" owned by their team instead of saving it in their personal account. The user is looking for a solution to fix this issue and is unsure if they need to change their login credentials.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-sync-confusing-personal-project-for-team-project\/4315",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":4.9,
        "Challenge_reading_time":7.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":21.5217911111,
        "Challenge_title":"Wandb sync confusing personal project for team project",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":103.0,
        "Challenge_word_count":106,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/adamoyoung\">@adamoyoung<\/a> thanks for reporting this issue. Could you please provide the <code>--entity<\/code> and <code>--project<\/code> arguments as follows:<br>\n<code>wandb sync -e personal -p foo path\/to\/foo\/run<\/code><\/p>\n<p>Would this work for you? There\u2019s a <code>Project Defaults<\/code> section in your <a href=\"https:\/\/wandb.ai\/settings\">personal settings page<\/a> where this in your case seems to be configured for your team entity. You may change that if you wanted the default to be your personal account.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.9,
        "Solution_reading_time":7.15,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":70.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1298484007147,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, United States",
        "Answerer_reputation_count":9271.0,
        "Answerer_view_count":1819.0,
        "Challenge_adjusted_solved_time":119.2915472222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to add an IAM user for using sagemaker. I used the <code>AmazonSageMakerFullAccess<\/code> policy. But when I log in as this user I can see all of the s3 buckets of the root account and download files from them.<\/p>\n<p>The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\" rel=\"nofollow noreferrer\">sagemaker documentation<\/a> states<\/p>\n<blockquote>\n<p>When attaching the AmazonSageMakerFullAccess policy to a role, you must do one of the following to allow Amazon SageMaker to access your S3 bucket:<\/p>\n<p>Include the string &quot;SageMaker&quot; or &quot;sagemaker&quot; in the name of the bucket where you store training data, or the model artifacts resulting from model training, or both.<\/p>\n<p>Include the string &quot;SageMaker&quot; or &quot;sagemaker&quot; in the object name of the training data object(s).<\/p>\n<p>Tag the S3 object with &quot;sagemaker=true&quot;. The key and value are case sensitive. For more information, see Object Tagging in the Amazon Simple Storage Service Developer Guide.<\/p>\n<p>Add a bucket policy that allows access for the execution role. For more information, see Using Bucket Policies and User Policies in the Amazon Simple Storage Service Developer Guide.<\/p>\n<\/blockquote>\n<p>This seems to be inaccurate the user can access s3 buckets lacking <code>sagemaker<\/code> in the name. How do I limit the access?<\/p>\n<p>the full policy is below<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;sagemaker:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;ecr:GetAuthorizationToken&quot;,\n                &quot;ecr:GetDownloadUrlForLayer&quot;,\n                &quot;ecr:BatchGetImage&quot;,\n                &quot;ecr:BatchCheckLayerAvailability&quot;,\n                &quot;cloudwatch:PutMetricData&quot;,\n                &quot;cloudwatch:PutMetricAlarm&quot;,\n                &quot;cloudwatch:DescribeAlarms&quot;,\n                &quot;cloudwatch:DeleteAlarms&quot;,\n                &quot;ec2:CreateNetworkInterface&quot;,\n                &quot;ec2:CreateNetworkInterfacePermission&quot;,\n                &quot;ec2:DeleteNetworkInterface&quot;,\n                &quot;ec2:DeleteNetworkInterfacePermission&quot;,\n                &quot;ec2:DescribeNetworkInterfaces&quot;,\n                &quot;ec2:DescribeVpcs&quot;,\n                &quot;ec2:DescribeDhcpOptions&quot;,\n                &quot;ec2:DescribeSubnets&quot;,\n                &quot;ec2:DescribeSecurityGroups&quot;,\n                &quot;application-autoscaling:DeleteScalingPolicy&quot;,\n                &quot;application-autoscaling:DeleteScheduledAction&quot;,\n                &quot;application-autoscaling:DeregisterScalableTarget&quot;,\n                &quot;application-autoscaling:DescribeScalableTargets&quot;,\n                &quot;application-autoscaling:DescribeScalingActivities&quot;,\n                &quot;application-autoscaling:DescribeScalingPolicies&quot;,\n                &quot;application-autoscaling:DescribeScheduledActions&quot;,\n                &quot;application-autoscaling:PutScalingPolicy&quot;,\n                &quot;application-autoscaling:PutScheduledAction&quot;,\n                &quot;application-autoscaling:RegisterScalableTarget&quot;,\n                &quot;logs:CreateLogGroup&quot;,\n                &quot;logs:CreateLogStream&quot;,\n                &quot;logs:DescribeLogStreams&quot;,\n                &quot;logs:GetLogEvents&quot;,\n                &quot;logs:PutLogEvents&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;,\n                &quot;s3:PutObject&quot;,\n                &quot;s3:DeleteObject&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::*SageMaker*&quot;,\n                &quot;arn:aws:s3:::*Sagemaker*&quot;,\n                &quot;arn:aws:s3:::*sagemaker*&quot;\n            ]\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:CreateBucket&quot;,\n                &quot;s3:GetBucketLocation&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:ListAllMyBuckets&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEqualsIgnoreCase&quot;: {\n                    &quot;s3:ExistingObjectTag\/SageMaker&quot;: &quot;true&quot;\n                }\n            }\n        },\n        {\n            &quot;Action&quot;: &quot;iam:CreateServiceLinkedRole&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:iam::*:role\/aws-service-role\/sagemaker.application-autoscaling.amazonaws.com\/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringLike&quot;: {\n                    &quot;iam:AWSServiceName&quot;: &quot;sagemaker.application-autoscaling.amazonaws.com&quot;\n                }\n            }\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEquals&quot;: {\n                    &quot;iam:PassedToService&quot;: &quot;sagemaker.amazonaws.com&quot;\n                }\n            }\n        }\n    ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1525129267447,
        "Challenge_comment_count":1,
        "Challenge_created_time":1524699817877,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue where an IAM user with the AmazonSageMakerFullAccess policy can access all S3 buckets of the root account and download files from them, even if the buckets do not have \"SageMaker\" in their name. The user is seeking a way to limit this access.",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50032795",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":27.9,
        "Challenge_reading_time":64.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":119.2915472222,
        "Challenge_title":"prevent access to s3 buckets for sagemaker users",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1703.0,
        "Challenge_word_count":302,
        "Platform":"Stack Overflow",
        "Poster_created_time":1298484007147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, NY, United States",
        "Poster_reputation_count":9271.0,
        "Poster_view_count":1819.0,
        "Solution_body":"<p>looks like the sagemaker notebook wizard has you create a role that has limited s3 access. If I add this and the default <code>AmazonSageMakerFullAccess<\/code> the user is properly restricted. <a href=\"https:\/\/i.stack.imgur.com\/9BjRD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9BjRD.png\" alt=\"Amazon make sagemaker role\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/IM7WW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IM7WW.png\" alt=\"choose iam roles\"><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.6,
        "Solution_reading_time":6.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":511.6538888889,
        "Challenge_answer_count":0,
        "Challenge_body":"Error in pipeline in GithubActions\r\n`14:25:43.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDOUT: Starting Mlflow UI on port 5000\r\n14:25:46.430 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\r\n14:25:46.453 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\r\n14:25:46.468 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 ERROR mlflow.cli: Error initializing backend store\r\n14:25:46.480 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: ]\r\n14:25:46.483 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.484 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: )\r\n14:25:46.485 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tFOREIGN KEY(experiment_id) REFERENCES experiments (experiment_id)\r\n14:25:46.487 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT runs_lifecycle_stage CHECK (lifecycle_stage IN ('active', 'deleted')), \r\n14:25:46.489 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT status CHECK (status IN ('SCHEDULED', 'FAILED', 'FINISHED', 'RUNNING')), \r\n14:25:46.491 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT source_type CHECK (source_type IN ('NOTEBOOK', 'JOB', 'LOCAL', 'UNKNOWN', 'PROJECT')), \r\n14:25:46.493 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT run_pk PRIMARY KEY (run_uuid), \r\n14:25:46.495 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \texperiment_id INTEGER, \r\n14:25:46.496 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tartifact_uri VARCHAR(200), \r\n14:25:46.497 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tlifecycle_stage VARCHAR(20), \r\n14:25:46.500 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_version VARCHAR(50), \r\n14:25:46.505 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tend_time BIGINT, \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstart_time BIGINT, \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstatus VARCHAR(20), \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tuser_id VARCHAR(256), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tentry_point_name VARCHAR(50), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_name VARCHAR(500), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_type VARCHAR(20), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tname VARCHAR(250), \r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \trun_uuid VARCHAR(32) NOT NULL, \r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: (Background on this error at: http:\/\/sqlalche.me\/e\/gkpj)\r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.516 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: The above exception was the direct cause of the following exception:\r\n14:25:46.516 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\r\n14:25:46.517 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1618, in _run_visitor\r\n14:25:46.517 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     conn._run_visitor(visitorcallable, element, **kwargs)\r\n14:25:46.518 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     visitorcallable(self.dialect, self, **kwargs).traverse_single(element)\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: (Background on this error at: http:\/\/sqlalche.me\/e\/gkpj)\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: ]\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: )\r\n14:25:46.542 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tFOREIGN KEY(experiment_id) REFERENCES experiments (experiment_id)\r\n14:25:46.542 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT runs_lifecycle_stage CHECK (lifecycle_stage IN ('active', 'deleted')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Updating database tables\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Will assume transactional DDL.\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT status CHECK (status IN ('SCHEDULED', 'FAILED', 'FINISHED', 'RUNNING')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT source_type CHECK (source_type IN ('NOTEBOOK', 'JOB', 'LOCAL', 'UNKNOWN', 'PROJECT')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT run_pk PRIMARY KEY (run_uuid), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \texperiment_id INTEGER, \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tartifact_uri VARCHAR(200), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tlifecycle_stage VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_version VARCHAR(50), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tend_time BIGINT, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstart_time BIGINT, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstatus VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tuser_id VARCHAR(256), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tentry_point_name VARCHAR(50), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_name VARCHAR(500), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_type VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tname VARCHAR(250), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \trun_uuid VARCHAR(32) NOT NULL, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: CREATE TABLE runs (\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: [SQL: \r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     cursor.execute(statement, parameters)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/default.py\", line 588, in do_execute\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.dialect.do_execute(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1245, in _execute_context\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     raise value.with_traceback(tb)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/util\/compat.py\", line 152, in reraise\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     reraise(type(exception), exception, tb=exc_tb, cause=cause)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/util\/compat.py\", line 398, in raise_from_cause\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     util.raise_from_cause(sqlalchemy_exception, exc_info)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1476, in _handle_dbapi_exception\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self._handle_dbapi_exception(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1249, in _execute_context\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     ret = self._execute_context(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1039, in _execute_ddl\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return connection._execute_ddl(self, multiparams, params)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 72, in _execute_on_connection\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(self, multiparams, params)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 982, in execute\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.connection.execute(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 821, in visit_table\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(obj, **kw)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/visitors.py\", line 138, in traverse_single\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.traverse_single(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 777, in visit_metadata\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(obj, **kw)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/visitors.py\", line 138, in traverse_single\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 2049, in _run_visitor\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     bind._run_visitor(\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/schema.py\", line 4315, in create_all\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     InitialBase.metadata.create_all(engine)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/db\/utils.py\", line 30, in _initialize_tables\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     mlflow.store.db.utils._initialize_tables(self.engine)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/sqlalchemy_store.py\", line 99, in __init__\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return SqlAlchemyStore(store_uri, artifact_uri)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 64, in _get_sqlalchemy_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return builder(store_uri=store_uri, artifact_uri=artifact_uri)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 37, in get_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 91, in _get_tracking_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     _get_tracking_store(backend_store_uri, default_artifact_root)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 105, in initialize_backend_stores\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     initialize_backend_stores(backend_store_uri, default_artifact_root)\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/cli.py\", line 291, in server\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: Traceback (most recent call last):\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     cursor.execute(statement, parameters)\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/default.py\", line 588, in do_execute\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.dialect.do_execute(\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1245, in _execute_context\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: Traceback (most recent call last):\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: CREATE TABLE runs (\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: [SQL: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 ERROR mlflow.cli: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"`\r\nwhich causes test to not pass",
        "Challenge_closed_time":1610230183000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1608388229000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an issue with MLFlow where the name of the bucket is hardcoded, making it impossible to use MLFlow with AWS S3. This poses a challenge for those using Minio in Gateway mode with MLFlow on AWS as S3 buckets are globally unique.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/prinz-nussknacker\/prinz\/issues\/78",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":23.7,
        "Challenge_reading_time":251.52,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":210.0,
        "Challenge_repo_star_count":8.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":290,
        "Challenge_solved_time":511.6538888889,
        "Challenge_title":"Error when starting new experiment in mlflow",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":1131,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Already fixed in #79 by introducing delay between mlflow server start and starting experiments in mlflow",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":1.31,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":93.5734752778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Azure Machine learning Studio, I have imported a dataset from a locally stored spreadsheet. In the designer, I drag the dataset into the workspace, right click, and select 'Visualize. I get the following error:   <\/p>\n<p>&quot;Unable to visualize this dataset. This might be because your data is stored behind a virtual network or your data does not support profile&quot;. I've searched for hours for a remedy, but find nothing.   <\/p>\n<p>What do I do to fix this error?<\/p>",
        "Challenge_closed_time":1603128837008,
        "Challenge_comment_count":5,
        "Challenge_created_time":1602791972497,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to visualize a dataset in Microsoft Azure Machine Learning Studio. The error message states that the dataset cannot be visualized, possibly due to the data being stored behind a virtual network or not supporting profile. The user is seeking a solution to fix this error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/127980\/error-when-visualizing-dataset-in-microsoft-azure",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":93.5734752778,
        "Challenge_title":"Error when Visualizing Dataset in Microsoft Azure Machine Learning Studio",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":88,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=ab00ff52-eb99-4909-97c6-13620f09e957\">@Dana Shields  <\/a> I have tried this scenario with my workspace and i was able to replicate the message you have seen. It looks like you are using the Dataset type as File while creating the dataset which is causing the issue. Please register the dataset as Tabular type and then use the dataset in designer. This should show you the preview of the data. Here is a screen shot from my workspace of the designer.    <\/p>\n<p><img src=\"\/answers\/storage\/temp\/33346-image.png\" alt=\"33346-image.png\" \/>    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":7.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":83.5716872222,
        "Challenge_answer_count":1,
        "Challenge_body":"The objective is to replicate \"MLOps template for model building, training, and deployment with third-party Git repositories using Jenkins\" builtin Sagemaker Project template. I want to feed custom seed code to the Github repository each time a project is created using my organization custom template instead of the default seed code that the builtin template feeds.\n\nI am able to create the custom template using service catalog but I could not find a solution for feeding the seed code to github repo. So, I decided to see how the built in project template is doing this and it is using resources from this bucket \"s3:\/\/sagemaker-servicecatalog-seedcode-us-east-1\/bootstrap\/GitRepositorySeedCodeCheckinCodeBuildProject-v1.0.zip\" but I could not access it. I am not sure how to achieve the objective?",
        "Challenge_closed_time":1657560195771,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657259337697,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to feed custom seed code to a GitHub repository each time a project is created using their organization's custom template in Sagemaker Projects. They have created the custom template using service catalog but are unable to find a solution for feeding the seed code to the GitHub repo. They have tried to replicate the built-in project template but are unable to access the resources from the bucket used by the template. The user is seeking guidance on how to achieve their objective.",
        "Challenge_last_edit_time":1668617915380,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU_Y4T-A3aQySFeRr3feBscA\/how-to-feed-seed-code-to-github-repository-from-sagemaker-projects-organization-template-created-with-service-catalog",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":11.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":83.5716872222,
        "Challenge_title":"How to feed seed code to GitHub Repository from Sagemaker Projects Organization Template created with Service Catalog?",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":416.0,
        "Challenge_word_count":136,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You can download the seed package using awscli s3 cp <s3_uri> <target_path> or by using this URL: https:\/\/sagemaker-servicecatalog-seedcode-us-east-1.s3.amazonaws.com\/bootstrap\/GitRepositorySeedCodeCheckinCodeBuildProject-v1.0.zip\n\nThis .zip is used by CodeBuild that is called when the template is deployed (by a lambda mapped to a CFN custom component). If you take a look in the template you'll find a component named \"SageMakerModelBuildSeedCodeCheckinProjectTriggerLambdaInvoker\". You can find some env vars defined for this component like: SEEDCODE_BUCKET_NAME and SEEDCODE_BUCKET_KEY. These vars point to an S3 uri that has another .zip file with the content of the seed for the git repo. If you get the default values defined there you can re-create the URL and download the .zip file as well:\nhttps:\/\/sagemaker-servicecatalog-seedcode-us-east-1.s3.amazonaws.com\/toolchain\/model-building-workflow-jenkins-v1.0.zip\n\nSo, in the end, if you want to change the content that is pushed to the git repo, you can redefine these 2 vars and point to an S3 path that contains a .zip file you created.\n\nBonus: If you're a curious person, I recommend you to take a look at the .java file (src\/main\/java\/GitRepositorySeedCodeBootStrapper.java) inside the .zip of the CodeBuild .zip for you to understand what it does to prepare the git repo like: download a .zip, unpack it, commit\/push to the git repo.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1657560195771,
        "Solution_link_count":2.0,
        "Solution_readability":9.3,
        "Solution_reading_time":17.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":197.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.2444444444,
        "Challenge_answer_count":0,
        "Challenge_body":"## Expected Behavior\r\nIt seems new version on AzureML extension to VS Code doesn't have this option in settings. I needed to downgrade to 0.6x.\r\n\r\n## Actual Behavior\r\nCurrent version 0.10.0 doesn't have the option. Cannot locally debug or documentation doesn't provide info about that.\r\n\r\n## Specifications\r\n\r\n  - Version: 0.10.0\r\n  - Platform: VS Code, Windows\r\n",
        "Challenge_closed_time":1654701310000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1654678830000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a reoccurring error message when opening a remote connection to Azure Machine Learning Compute Instance. The error message is related to a failed request to a specific URL and is causing annoyance to the user. The error type is REQUEST_SEND_ERROR and the user is running Visual Studio Code version 1.66.1 on a Linux OS.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1589",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":5.42,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":6.2444444444,
        "Challenge_title":"Run and debug experiments locally - azureML.CLI Compatibility Mode for CLI v1 - cannot find",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":63,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@michalmar We have completely deprecated the v1 CLI Compatibility mode settings from v0.8.0 onwards and v2 mode will be the way going forward :).",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":1.79,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1373545891008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Europe",
        "Answerer_reputation_count":4792.0,
        "Answerer_view_count":1984.0,
        "Challenge_adjusted_solved_time":0.1783527778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am wondering what is the difference between Cortana Analytics and Azure ML ?<\/p>\n\n<ul>\n<li>those are 2 distincts solutions ? <\/li>\n<li>one is part of the other ?<\/li>\n<\/ul>",
        "Challenge_closed_time":1451558834643,
        "Challenge_comment_count":0,
        "Challenge_created_time":1451558192573,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the difference between Cortana Analytics and Azure ML. They are unsure if these are two separate solutions or if one is a part of the other.",
        "Challenge_last_edit_time":1543674264230,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34545078",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.6,
        "Challenge_reading_time":2.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1783527778,
        "Challenge_title":"Azure ML vs Cortana Analytics Suite",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1150.0,
        "Challenge_word_count":34,
        "Platform":"Stack Overflow",
        "Poster_created_time":1425637780190,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":559.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>Azure Machine Learning is part of the Cortana analytics suite<\/p>\n\n<p>You will find more info with the link below<\/p>\n\n<p><a href=\"http:\/\/www.sqlchick.com\/entries\/2015\/8\/22\/what-is-the-cortana-analytics-suite\" rel=\"nofollow\">All the details on the Cortana link here<\/a><\/p>\n\n<p>All the best<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1457447097076,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":3.86,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1512520584492,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bloomington, IN, USA",
        "Answerer_reputation_count":868.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":7347.4097222222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to run a machine learning experiment in azureml.<\/p>\n<p>I can't figure out how to get the workspace context from the control script.  Examples like <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-bring-data#control-script\" rel=\"nofollow noreferrer\">this one<\/a> in the microsoft docs use Workspace.from_config().  When I use this in the control script I get the following error:<\/p>\n<blockquote>\n<p>&quot;message&quot;: &quot;We could not find config.json in: [path] or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.&quot;<\/p>\n<\/blockquote>\n<p>I've also tried including my subscription id and the resource specs like so:<\/p>\n<pre><code>subscription_id = 'id'\nresource_group = 'name'\nworkspace_name = 'name'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n<\/code><\/pre>\n<p>In this case I have to monitor the log and authenticate on each run as I would locally.<\/p>\n<p>How do you get the local workspace from a control script for azureml?<\/p>",
        "Challenge_closed_time":1641958092267,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615507417267,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble getting the workspace context from the control script while running a machine learning experiment in AzureML. They have tried using Workspace.from_config() and including subscription id and resource specs, but both methods have not worked. The user is seeking help to get the local workspace from a control script for AzureML.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66592313",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":9.5,
        "Challenge_reading_time":14.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":7347.4097222222,
        "Challenge_title":"Get local workspace in azureml",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":333.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1512520584492,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bloomington, IN, USA",
        "Poster_reputation_count":868.0,
        "Poster_view_count":51.0,
        "Solution_body":"<p>This had no answers for 10 months, and now they are coming in :).  I figuerd this out quite a while ago but haven't gotten around to posting the answer.  Here it is.<\/p>\n<p>From the training script, you can get the workspace from the run context as follows:<\/p>\n<pre><code>from azureml.core import Run\nRun.get_context()\nws = run.experiment.workspace\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.5,
        "Solution_reading_time":4.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":19.6539680556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi, I am new to Azure ML, and I have been trying to replicate the same structure presented in the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-train-models-with-aml\">MNIST tutorial<\/a>, but I don't understand how to adapt it to my case.     <\/p>\n<p>I am running a python file from the experiment, but I don't understand how I can access data that is currently in a folder in the cloud file system from the script running in the experiment.     <br \/>\nI have found many examples about accessing one single .csv file, but my data is made of many images.    <\/p>\n<p>From my understanding I should first load the folder to a datastore, then use Dataset.File.upload_directory to create a dataset containing my folder, and here is how I tried to do it:     <\/p>\n<pre><code># Create dataset from data directory  \ndatastore = Datastore.get(ws, 'workspaceblobstore')  \ndataset = Dataset.File.upload_directory(path_data, target, pattern=None, overwrite=False, show_progress=True)  \n  \nfile_dataset = dataset.register(workspace=ws, name='reduced_classification_dataset',  \n                                                 description='reduced_classification_dataset',  \n                                                 create_new_version=True)  \n<\/code><\/pre>\n<p>But then I don't understand if and how I can access this data like a normal file system from my python script, or I need further steps to be able to do that.     <\/p>",
        "Challenge_closed_time":1614833313088,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614762558803,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is new to Azure ML and is trying to access data from a folder in the cloud file system for a python script running in an experiment. They have tried to create a dataset using Dataset.File.upload_directory, but are unsure how to access the data like a normal file system from their script.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/296661\/azureml-notebooks-how-to-access-data-from-an-exper",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":17.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":19.6539680556,
        "Challenge_title":"AzureML Notebooks: how to access data from an experiment",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":187,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=bf39b622-b8af-42d8-809c-296225cdbb39\">@Matzof  <\/a> Thanks for the question. Please follow the below code for writing.    <\/p>\n<pre><code>   datastore = ## get your defined in Workspace as Datastore   \ndatastore.upload(src_dir='.\/files\/to\/copy\/...',  \n                 target_path='target\/directory',  \n                 overwrite=True)  \n<\/code><\/pre>\n<p>Datastore.upload only support blob and fileshare. For adlsgen2 upload, you can try our new dataset upload API:    <\/p>\n<pre><code>from azureml.core import Dataset, Datastore  \ndatastore = Datastore.get(workspace, 'mayadlsgen2')  \nDataset.File.upload_directory(src_dir='.\/data', target=(datastore,'data'))  \n<\/code><\/pre>\n<p>Pandas is integrated with fsspec which provides Pythonic implementation for filesystems including s3, gcs, and Azure. You can check the source for Azure here: <a href=\"https:\/\/github.com\/dask\/adlfs\">dask\/adlfs: fsspec-compatible Azure Datake and Azure Blob Storage access (github.com)<\/a>. With this you can use normal filesystem operations like ls, glob, info, etc.     <\/p>\n<p>You can find an example (for reading data) here: <a href=\"https:\/\/github.com\/Azure\/azureml-examples\/blob\/main\/tutorials\/using-dask\/1.intro-to-dask.ipynb\">azureml-examples\/1.intro-to-dask.ipynb at main \u00b7 Azure\/azureml-examples (github.com)<\/a>     <\/p>\n<p>Writing is essentially the same as reading, you need to switch the protocol to abfs (or az), slightly modify how you're accessing the data, and provide credentials unless your blob has public write access.     <\/p>\n<p>You can use the Azure ML Datastore to retrieve credentials like this (taken from example):     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/74112-2.png?platform=QnA\" alt=\"74112-2.png\" \/>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.5,
        "Solution_reading_time":22.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":177.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":13.2357005556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a machine learning workspace in West Europe region. But the storage account, key vault and application insights got created in East US region. All these got created by default with creation on ML workspace.  <br \/>\nSo I want to know the reason for different region and also want to move the storage account to West Europe region.<\/p>",
        "Challenge_closed_time":1613167744632,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613120096110,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user created a machine learning workspace in West Europe region, but the associated storage account, key vault, and application insights were created in East US region by default. The user wants to know the reason for the different region and how to move the storage account to West Europe region.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/270693\/why-the-storage-account-assosiated-to-azure-machin",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":5.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":13.2357005556,
        "Challenge_title":"Why the storage account assosiated to azure machine learning has differenent region compared ML workspace?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":74,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, this is unusual. There's no way to move your default AML storage account to a different region. I recommend creating a new workspace or contacting <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/azure-portal\/supportability\/how-to-create-azure-support-request\">Azure Support<\/a> to investigate further.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":4.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1491556112892,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Milano, MI, Italia",
        "Answerer_reputation_count":611.0,
        "Answerer_view_count":111.0,
        "Challenge_adjusted_solved_time":239.8928277778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run R script in <strong>Azure ML studio<\/strong> that transposes\/reshapes the dataframe from long to wide format (<a href=\"https:\/\/stackoverflow.com\/questions\/11322801\/transpose-reshape-dataframe-without-timevar-from-long-to-wide-format\">example<\/a>). My script runs very fine in Rstudio. But the same does not run in Azure ML studio and throws the following error - could not find function &quot;rowid&quot;. It would be great to know how can I get rid of this and what exactly is causing this error despite it being good enough to run neatly in Rstudio.<\/p>\n<pre><code>#Error: Error 0063: The following error occurred during evaluation of R script:\n# ---------- Start of error message from R ----------\n      could not find function &quot;rowid&quot;\n# ----------- End of error message from R -----------\n<\/code><\/pre>\n<p>I've tried the code in both R versions <em>CRAN R 3.1.0<\/em> &amp; <em>Microsoft R open 3.2.2<\/em>.\nThank you very much in advance.<\/p>",
        "Challenge_closed_time":1513962447063,
        "Challenge_comment_count":6,
        "Challenge_created_time":1513098832883,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to run an R script in Azure ML studio that transposes\/reshapes a dataframe from long to wide format. The error message says \"could not find function 'rowid'\". The script runs fine in Rstudio but not in Azure ML studio. The user has tried running the code in both CRAN R 3.1.0 and Microsoft R open 3.2.2.",
        "Challenge_last_edit_time":1612471487528,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47778076",
        "Challenge_link_count":1,
        "Challenge_participation_count":7,
        "Challenge_readability":9.1,
        "Challenge_reading_time":13.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":239.8928277778,
        "Challenge_title":"Azure Machine Learning execute R script - Could not find function \"rowid\" error",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":803.0,
        "Challenge_word_count":142,
        "Platform":"Stack Overflow",
        "Poster_created_time":1503486557670,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":533.0,
        "Poster_view_count":163.0,
        "Solution_body":"<p>Hi I had the same problem 2 days ago with the function <code>pull()<\/code>, always of the package <code>dplyr<\/code>.\nThe problem is that the both version of R (CRAN R 3.1.0 and Microsoft R open 3.2.2) supported by Azure Machine Learning Studio, does not support the version <code>0.7.4<\/code> of package <code>dplyr<\/code>.\nIf you read the <a href=\"https:\/\/cran.r-project.org\/web\/packages\/dplyr\/dplyr.pdf\" rel=\"nofollow noreferrer\">documentation<\/a> related to the package <code>dplyr<\/code> you can see that the package is installable only for R versions >= 3.1.2.<\/p>\n\n<p>Then you must wait for the R version used by Azure Machine Learning Studio be updated, or find an alternative solution to your function.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":9.06,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":100.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.1363888889,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nXCom return value of `SageMakerTransformOperatorAsync`  and `SageMakerTrainingOperatorAsync` does not produce the expected output.\r\n\r\nIt seems like some key(s) don't match the non-async operator output.\r\n\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run a dag with traditional operators\r\n2. Run same dag with Async operators\r\n3. Compare outputs\r\n\r\n**Expected behavior**\r\nThe Xcom keys and values should match whatever the traditional non-async version of the operators output.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n",
        "Challenge_closed_time":1666957435000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1666892144000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering token errors with the new Sagemaker Async Operators despite using personal Access Key, Secret, and Session Token for authentication. The error message states that the security token included in the request is invalid. The user expects the operators to work without any authentication or token errors. The user has also noticed that the traditional operators work fine with the same authentication credentials.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/736",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":7.76,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":22.0,
        "Challenge_repo_issue_count":807.0,
        "Challenge_repo_star_count":97.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":18.1363888889,
        "Challenge_title":"XCom Output of Sagemaker Async Operators",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":84,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@bharanidharan14  I tested locally with the branch for the fix and seems to be fixed with your patch. Thank you!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":1.35,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1324988509368,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Moscow, Russia",
        "Answerer_reputation_count":1593.0,
        "Answerer_view_count":93.0,
        "Challenge_adjusted_solved_time":0.0671325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed an Endpoint on Amazon SageMaker.\nNow I am trying to Invoke it.<\/p>\n\n<p>If I run this code in Sagemaker's Jupyter Notebook: <\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime')\nendpoint_name = 'DEMO-XGBoostEndpoint'\nbody = ','.join(['1.0'] * 6)\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=body)\nresponse['Body'].read()\n<\/code><\/pre>\n\n<p>it works properly.<\/p>\n\n<p>But if I run the same code, with added credentials for boto3 client, from my machine:<\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime', \n                       aws_access_key_id=ACCESS_ID,\n                       aws_secret_access_key= ACCESS_KEY)\nendpoint_name = 'DEMO-XGBoostEndpoint'\nbody = ','.join(['1.0'] * 6)\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=body)\nresponse['Body'].read()\n<\/code><\/pre>\n\n<p>I get this error:<\/p>\n\n<blockquote>\n  <p>ClientError: An error occurred (AccessDeniedException) when calling the InvokeEndpoint operation: User: arn:aws:iam::249707424405:user\/yury.logachev is not authorized to perform: sagemaker:InvokeEndpoint on resource: arn:aws:sagemaker:us-east-1:249707424405:endpoint\/demo-xgboostendpoint-2018-12-12-22-07-28 with an explicit deny<\/p>\n<\/blockquote>\n\n<p>If I run the latter piece of code (with added credentials as a parameters of client) on Sagemaker's Jupyter Notebook, I also get the same error.<\/p>\n\n<p>I understand that the solution should be linked with roles, policies etc, but could not find out it.<\/p>",
        "Challenge_closed_time":1547664263500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547411152043,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an AccessDeniedException error when trying to invoke an Endpoint on Amazon SageMaker using boto3 client with added credentials. The error message indicates that the user is not authorized to perform the sagemaker:InvokeEndpoint operation on the specified resource. The user suspects that the issue is related to roles and policies but is unsure of the solution.",
        "Challenge_last_edit_time":1547664021823,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54172907",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":20.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":70.3087380556,
        "Challenge_title":"Amazon Sagemaker. AccessDeniedException when calling the InvokeEndpoint operation",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":2435.0,
        "Challenge_word_count":155,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324988509368,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Moscow, Russia",
        "Poster_reputation_count":1593.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>The problem was with the MFA autharization. \nWhen I invoked the model from inside the model, the MFA was passed. \nBut when I tried to invoke the model from my machine, the MFA was not passed, so the access was denied.<\/p>\n\n<p>I created special user without MFA to debug the model, and that solved my problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":3.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416648155470,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":14749.0,
        "Answerer_view_count":968.0,
        "Challenge_adjusted_solved_time":76.2321219444,
        "Challenge_answer_count":1,
        "Challenge_body":"<h1>Question<\/h1>\n<p>Please advise how to trouble shoot the problem.<\/p>\n<h1>Problem<\/h1>\n<p>Cannot access the RedShift cluster endpoint from the SageMaker studio instance.<\/p>\n<pre><code>import socket\nsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nresult = sock.connect_ex(('dsoaws.cw7xniw3gvef.us-east-2.redshift.amazonaws.com',5439))\nif result == 0:\n   print(&quot;Port is open&quot;)\nelse:\n   print(&quot;Port is not open&quot;)\nsock.close()\n---\n\nPort is not open\n<\/code><\/pre>\n<h1>RedShift Cluster<\/h1>\n<p>Endpoint is <code>dsoaws.cw7xniw3gvef.us-east-2.redshift.amazonaws.com:5439\/dsoaws<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/t4w92.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t4w92.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The network setting shows the VPC is vpc-5b123432 allowing access from sg-56cb133e.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/nu5kM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/nu5kM.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<hr \/>\n<h1>SageMaker Studio<\/h1>\n<p>The SageMaker Studio instance is in the save VPC vpc-5b123432. However, not sure if sg-56cb133e is actually attached to the SageMaker studio instance. Please advise how to confirm if the sg-56cb133e is attached to the instance.<\/p>\n<pre><code>import json\nimport boto3\nfrom botocore.exceptions import ClientError\nfrom botocore.config import Config\n\nconfig = Config(\n   retries = {\n      'max_attempts': 10,\n      'mode': 'adaptive'\n   }\n)\n\n\niam = boto3.client('iam', config=config)\nsts = boto3.client('sts')\nredshift = boto3.client('redshift')\nsm = boto3.client('sagemaker')\nec2 = boto3.client('ec2')\n\ntry:\n    domain_id = sm.list_domains()['Domains'][0]['DomainId'] #['NotebookInstances'][0]['NotebookInstanceName']\n    describe_domain_response = sm.describe_domain(DomainId=domain_id)\n    vpc_id = describe_domain_response['VpcId']\n    print(vpc_id)\n    security_groups = ec2.describe_security_groups()['SecurityGroups']\n    for security_group in security_groups:\n        if vpc_id == security_group['VpcId']:\n            security_group_id = security_group['GroupId']\n    print(security_group_id)\nexcept:\n    pass\n-----\nvpc-5b123432\nsg-56cb133e\n<\/code><\/pre>\n<hr \/>\n<h1>Security Group<\/h1>\n<p><a href=\"https:\/\/i.stack.imgur.com\/JgCgi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JgCgi.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gjmLh.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gjmLh.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h1>IAM<\/h1>\n<p>The IAM role <code>SageMaker<\/code> is attached to the SageMaker Studio.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oY9ix.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oY9ix.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1628118669360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1627882823273,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to access the RedShift cluster endpoint from the SageMaker Studio instance. The network settings show that the VPC is allowing access from the security group, but it is unclear if the security group is attached to the SageMaker Studio instance. The IAM role \"SageMaker\" is attached to the SageMaker Studio.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68616817",
        "Challenge_link_count":10,
        "Challenge_participation_count":1,
        "Challenge_readability":15.4,
        "Challenge_reading_time":38.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":65.5128019444,
        "Challenge_title":"AWS - Cannot access RedShift endpoint from the SageMaker Studio",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":434.0,
        "Challenge_word_count":229,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416648155470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":14749.0,
        "Poster_view_count":968.0,
        "Solution_body":"<h1>Cause<\/h1>\n<p>Did not use VPC Only sagemaker deployment as having used the Quick Start onboard.<\/p>\n<h1>Fix<\/h1>\n<ol>\n<li>Deleted the SageMaker Studio. R<\/li>\n<li>Recreated by using the Standard Setup + VPC only\n<a href=\"https:\/\/i.stack.imgur.com\/zZ7be.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zZ7be.png\" alt=\"enter image description here\" \/><\/a><\/li>\n<li>Added the NAT and configured the routing tables.<\/li>\n<\/ol>\n<h1>References<\/h1>\n<ul>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/securing-amazon-sagemaker-studio-connectivity-using-a-private-vpc\/\" rel=\"nofollow noreferrer\">Securing Amazon SageMaker Studio connectivity using a private VPC<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-studio-vpc-networkfirewall\" rel=\"nofollow noreferrer\">Amazon SageMaker Studio in a private VPC with NAT Gateway and Network Firewall<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1628157258912,
        "Solution_link_count":4.0,
        "Solution_readability":17.8,
        "Solution_reading_time":12.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":2.8828916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is regard to ML Feature Stores, is Feast the recommended option today for Feature Store with Azure ML or is there any other options?<\/p>",
        "Challenge_closed_time":1643267855687,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643257477277,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking recommendations for a feature store in Azure ML and wants to know if Feast is the recommended option or if there are other options available.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70873347",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":2.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":2.8828916667,
        "Challenge_title":"Recommended options for Feature store in Azure ML",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":327.0,
        "Challenge_word_count":32,
        "Platform":"Stack Overflow",
        "Poster_created_time":1632461310820,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":107.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>We have roadmap to support that is something more native and also tightly integrates into Azure ML.<\/p>\n<p>Here is <a href=\"https:\/\/techcommunity.microsoft.com\/t5\/ai-customer-engineering-team\/bringing-feature-store-to-azure-from-microsoft-azure-redis-and\/ba-p\/2918917\" rel=\"nofollow noreferrer\">doc<\/a> to integration with OSS tool such as Hopsworks\/Feast and leveraging existing functionalities (designer\/pipelines, dataset) for an end-to-end &quot;feature store&quot; solution.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.1,
        "Solution_reading_time":6.52,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1614882423070,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":10.1961591667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Earlier when using AzureML from the Notebooks blade of Azure ML UI, we could access the local files in AzureML using simple relative paths:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bKZ0W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bKZ0W.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For example, in the above image to access the CSV from the <code>test.ipynb<\/code> we could just mention the relative path:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pandas.read_csv('WHO-COVID-19-global-data.csv')\n<\/code><\/pre>\n<p>However, we are not able to do that anymore.<\/p>\n<p>Also when we run<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nos.getcwd()\n<\/code><\/pre>\n<p>We see the output as\n<code>'\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/&lt;cluster-name&gt;'<\/code>.<\/p>\n<p>Hence, we are unable to access the files in the FileStore which was not the case earlier.<\/p>\n<p>When we run the same from the JuyterLab environment of the compute environment we get:<\/p>\n<p><code>'\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/&lt;cluster-name&gt;\/code\/Users\/&lt;current-user-name&gt;\/temp'<\/code>.<\/p>\n<p>We can easily solve it by adding the path <code>'\/code\/Users\/&lt;current-user-name&gt;\/temp'<\/code> at the base and use that instead. But this is not recommended as with a change in the environment we are using the code needs to change every time. How do we resolve this issue without going through this path appending method.<\/p>",
        "Challenge_closed_time":1632845944176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1632809238003,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue in accessing local files from AzureML File Share. Earlier, they were able to access the files using simple relative paths, but now they are unable to do so. The output of the command \"os.getcwd()\" shows a different path, which is causing the problem. The user has found a solution by adding a specific path, but it is not recommended as it requires changing the code every time the environment changes. The user is seeking a solution to this issue without using the path appending method.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69356567",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":20.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":10.1961591667,
        "Challenge_title":"How to access local files from AzureML File Share?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":375.0,
        "Challenge_word_count":177,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>I work on the Notebooks team in AzureML, I just tried this. Did this just start happening today?<\/p>\n<p>It seems like things are working as expected: <a href=\"https:\/\/i.stack.imgur.com\/xeDIT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xeDIT.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":4.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1351080779276,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Leuven, Belgium",
        "Answerer_reputation_count":3126.0,
        "Answerer_view_count":262.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>My training data looks like <\/p>\n\n<pre><code>df = pd.DataFrame({'A' : [2, 5], 'B' : [1, 7]})\n<\/code><\/pre>\n\n<p>I have trained a model in AWS Sagemaker and I deployed the model behind an endpoint.\nThe endpoint accepts the payload as \"text\/csv\".<\/p>\n\n<p>to invoke the endpoint using boto3 you can do:<\/p>\n\n<pre><code>import boto3\nclient = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(\n    EndpointName=\"my-sagemaker-endpoint-name\",\n    Body= my_payload_as_csv,\n    ContentType = 'text\/csv')\n<\/code><\/pre>\n\n<p>How do i construct the payload \"my_payload_as_csv\" from my Dataframe in order to invoke the Sagemaker Endpoint correctly?<\/p>",
        "Challenge_closed_time":1590329636843,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590329636843,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to construct a \"text\/csv\" payload from a pandas DataFrame in order to invoke a Sagemaker endpoint correctly using boto3. The user has trained a model in AWS Sagemaker and deployed it behind an endpoint that accepts the payload as \"text\/csv\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61987233",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":9.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.0,
        "Challenge_title":"How to construct a \"text\/csv\" payload when invoking a sagemaker endpoint",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":2627.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351080779276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Leuven, Belgium",
        "Poster_reputation_count":3126.0,
        "Poster_view_count":262.0,
        "Solution_body":"<p>if you start from the dataframe example<\/p>\n\n<pre><code>df = pd.DataFrame({'A' : [2, 5], 'B' : [1, 7]})\n<\/code><\/pre>\n\n<p>you take a row<\/p>\n\n<pre><code>df_1_record = df[:1]\n<\/code><\/pre>\n\n<p>and convert <code>df_1_record<\/code> to a csv like this:<\/p>\n\n<pre><code>import io\nfrom io import StringIO\ncsv_file = io.StringIO()\n# by default sagemaker expects comma seperated\ndf_1_record.to_csv(csv_file, sep=\",\", header=False, index=False)\nmy_payload_as_csv = csv_file.getvalue()\n<\/code><\/pre>\n\n<p><code>my_payload_as_csv<\/code> looks like<\/p>\n\n<pre><code>'2,1\\n'\n<\/code><\/pre>\n\n<p>then you can invoke the sagemaker endpoint<\/p>\n\n<pre><code>import boto3\nclient = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(\n    EndpointName=\"my-sagemaker-endpoint-name\",\n    Body= my_payload_as_csv,\n    ContentType = 'text\/csv')\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.2,
        "Solution_reading_time":10.94,
        "Solution_score_count":7.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":24.0520611111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Can we connect Azure ML Notebooks directly to Snowflake using Private end-points, my ML Workspace is inside a VNet.<\/p>",
        "Challenge_closed_time":1653034845860,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652948258440,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to know if it is possible to connect Azure ML Notebooks to Snowflake using Private endpoints, as their ML Workspace is located inside a VNet.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/855820\/connect-azure-ml-with-snowflake-using-private-endp",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.2,
        "Challenge_reading_time":2.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":24.0520611111,
        "Challenge_title":"Connect Azure ML with Snowflake using Private endpoint?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=03343194-9922-4c28-abc7-1d7c46b6d2d6\">@Varun  <\/a>     <\/p>\n<p>Thanks for reaching out to us, currently there is no internal way in Azure Machine Learning Studio to connect to Snowflake. I am sorry for all inconveniences.     <\/p>\n<p>But you can run a  Python 3 code to use the Snowflake python connector - <a href=\"https:\/\/docs.snowflake.com\/en\/user-guide\/python-connector.html\">https:\/\/docs.snowflake.com\/en\/user-guide\/python-connector.html<\/a>    <\/p>\n<p>With Azure ML Studio, there's no built-in support for SnowFlake, I will forward your feedback to product group to see if there any plan in the future.     <\/p>\n<p>Hope this helps!    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p><em>-Please kindly accept the answer if you feel helpful, thanks a lot for supporting the community.<\/em>     <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.0,
        "Solution_reading_time":10.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":103.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":79.8691547222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I\u2019m trying to setup a self hosted wandb on k8s using helm charts. Unfortunately, I am not able to connect to my Amazon S3.<\/p>\n<p>I tried two ways:<\/p>\n<ol>\n<li>\n<p>Based on the example here <a href=\"https:\/\/docs.wandb.ai\/guides\/self-hosted\/setup\/on-premise-baremetal\" class=\"inline-onebox\">On Prem \/ Baremetal - Documentation<\/a>, I used the format:<br>\ns3:\/\/myaccess:myseceret@s3.amazonaws.com\/ofer-bucket-1<br>\nHowever when the wandb pod starts, it says that the URL is not valid, as \u201c:mysecret\u201d is not a valid port.<br>\nFor some reason it considers the secret to indicate URL port and not secret<\/p>\n<\/li>\n<li>\n<p>I also tried changing my bucket to public,  but wandb pod failed to initialize again, this time with error 403 access denied.<\/p>\n<\/li>\n<\/ol>\n<p>Anyone has an example for the correct format of the BUCKET value or can explain how it should be structured? I prefer to have it with access\/secret key. But I\u2019m ok with public as well.<\/p>",
        "Challenge_closed_time":1668315898264,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668028369307,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in connecting to Amazon S3 from a self-hosted wandb on k8s using helm charts. The user tried two ways, one with access\/secret key and the other with a public bucket, but both attempts failed. The user is seeking help to understand the correct format of the BUCKET value.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/cannot-connect-to-amazon-s3-from-self-hosted-wandb\/3399",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":9.1,
        "Challenge_reading_time":12.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":79.8691547222,
        "Challenge_title":"Cannot connect to Amazon S3 from self hosted wandb",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":124.0,
        "Challenge_word_count":150,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I\u2019ll post the reply of Chris Van Pelt from WandB support, to assist anyone who encounters this:<br>\nThe correct format is indeed s3:\/\/access:secret@host\/bucket<br>\nHowever, each component (access, secret, etc) needs to be url encoded as special characters within them can interfere with the parsing.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":21.6,
        "Solution_reading_time":3.86,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":44.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1391261341596,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":5196.6227847222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I wants to create azure machine learning workspace using terraform scripts.Is there any terraform provider to achieve this.<\/p>",
        "Challenge_closed_time":1600285333648,
        "Challenge_comment_count":1,
        "Challenge_created_time":1581577491623,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to create an Azure machine learning workspace using Terraform scripts and is seeking information on whether there is a Terraform provider available for this purpose.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60202189",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":5.7,
        "Challenge_reading_time":2.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":5196.6227847222,
        "Challenge_title":"How to create azure machine learning resource using terraform resource providers?",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1152.0,
        "Challenge_word_count":28,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565633099383,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":110.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>In the meantime Microsoft has added a Terraform resource for ML Workspace in the Azure Provider. This should make any custom scripting obsolete.<\/p>\n<p><a href=\"https:\/\/www.terraform.io\/docs\/providers\/azurerm\/r\/machine_learning_workspace.html\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/docs\/providers\/azurerm\/r\/machine_learning_workspace.html<\/a><\/p>\n<pre><code>resource &quot;azurerm_machine_learning_workspace&quot; &quot;example&quot; {\n  name                    = &quot;example-workspace&quot;\n  location                = azurerm_resource_group.example.location\n  resource_group_name     = azurerm_resource_group.example.name\n  application_insights_id = azurerm_application_insights.example.id\n  key_vault_id            = azurerm_key_vault.example.id\n  storage_account_id      = azurerm_storage_account.example.id\n\n  identity {\n    type = &quot;SystemAssigned&quot;\n  }\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":25.1,
        "Solution_reading_time":11.31,
        "Solution_score_count":4.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":285.5134875,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I can't see a data drift module anywhere in v2 of the Azure ML Python SDK. Is this missing or what's the deal? If so, are there any plans of bringing it into v2?<\/p>",
        "Challenge_closed_time":1658311324112,
        "Challenge_comment_count":1,
        "Challenge_created_time":1657283475557,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to locate the data drift module in version 2 of the Azure ML Python SDK and is seeking information on whether it is missing or if there are plans to include it in the future.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/919651\/datadrift-in-azure-ml-sdk-v2",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":2.4,
        "Challenge_reading_time":2.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":285.5134875,
        "Challenge_title":"Datadrift in Azure ML SDK v2",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=1dc2a0bd-ac4b-413b-bae7-930e0079e70d\">@SH  <\/a>     <\/p>\n<p>I have a good news for you, I just got confirmation from product team, the datadrift function will be in SDK V2 for sure. But for now we don't have an exact date for when. I have forwarded this feedback to product group and we hope we can bring this feature in near future.     <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.3,
        "Solution_reading_time":6.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1565289301123,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":79.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":3320.1937519445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What are the different data sources we can import data into Azure Machine Learning Services storage or notebook. I mean from Salesforce or any ERP or any website? As of now I have seen importing data using URL or getting it from data location in storage where notebook will also be stored.<\/p>\n\n<p>I have not got anything to try on. I googled for different methods, but couldn't find relevant link. So I didn't try much there.<\/p>",
        "Challenge_closed_time":1566330717407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554359325097,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on the various data sources that can be imported into Azure Machine Learning Services storage or notebook, including from Salesforce, ERP, or websites. They have attempted to find information online but have been unsuccessful.",
        "Challenge_last_edit_time":1554378019900,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55509207",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":5.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3325.3867527778,
        "Challenge_title":"Data sources in Azure Machine Learning Services",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1140.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545289999703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Thanks for your question. You can import data from Azure Blob, Azure File, ADLS Gen1, ADLS Gen2, Azure SQL, Azure PostgreSQL. \nFor more information: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data<\/a><\/p>\n\n<p>You can create an Azure ML Dataset for your training scenarios. Dataset can be created either from the data store mentioned above or from public urls.\nFor more information: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-register-datasets\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-register-datasets<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.7,
        "Solution_reading_time":10.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1267440784443,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Somewhere",
        "Answerer_reputation_count":15705.0,
        "Answerer_view_count":2150.0,
        "Challenge_adjusted_solved_time":0.2702147222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created a workspace in our azure environment and try to run this code:<\/p>\n<pre><code>library(azuremlsdk)\n\nws &lt;- get_workspace(\n    name = &quot;someworkspace&quot;, \n    subscription_id = &quot;si1&quot;, \n    resource_group =&quot;rg1&quot;\n)\n<\/code><\/pre>\n<p>Some interactive authenticator opens in my browser, which I think is intended behaviour as I have no tenantdid. However, I get this:<\/p>\n<pre><code>Performing interactive authentication. Please follow the instructions on the terminal.\nNote, we have launched a browser for you to login. For old experience with device code, use &quot;az login --use-device-code&quot;\nYou have logged in. Now let us find all the subscriptions to which you have access...\nInteractive authentication successfully completed.\nPerforming interactive authentication. Please follow the instructions on the terminal.\nNote, we have launched a browser for you to login. For old experience with device code, use &quot;az login --use-device-code&quot;\nYou have logged in. Now let us find all the subscriptions to which you have access...\nInteractive authentication successfully completed.\nAuthenticationException: AuthenticationException:\n        Message: Could not retrieve user token. Please run 'az login'\n        InnerException It is required that you pass in a value for the &quot;algorithms&quot; argument when calling decode().\n        ErrorResponse\n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n<\/code><\/pre>\n<p>I also tried:<\/p>\n<pre><code>az login\n<\/code><\/pre>\n<p>This works fine. So for me all this is very confusing!<\/p>",
        "Challenge_closed_time":1620752159740,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620740709577,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to run a code to get a workspace in Azure environment using azuremlsdk. An interactive authenticator opens in the browser, but the user gets an AuthenticationException error message stating that the user token could not be retrieved. The user has tried running 'az login', which works fine, but finds the entire process confusing.",
        "Challenge_last_edit_time":1620751186967,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67488064",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.3,
        "Challenge_reading_time":22.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":3.1806008333,
        "Challenge_title":"get workspace failed azuremlsdk - AuthenticationException",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":530.0,
        "Challenge_word_count":216,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>So I tried the same in Python and had a similar error and came across this:<\/p>\n<p><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/16035\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/16035<\/a><\/p>\n<p>Downgrading:<\/p>\n<pre><code> PyJWT \n<\/code><\/pre>\n<p>helped. The bizarre world of open source and its web of interdependencies!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.3,
        "Solution_reading_time":5.08,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":2.7001211111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to ask if it is possible to use DVC with several accounts on the same machine. At the moment, all commands (<code>dvc pull<\/code>, <code>dvc push<\/code>, ...) are executed under my name. But after several people joined this project too, I do not want them to execute commands under my name.<\/p>\n<p>When I was alone on this project I generated ssh key:<\/p>\n<pre><code>ssh-keygen\n<\/code><\/pre>\n<p>Connected to server where DVC remote data is stored:<\/p>\n<pre><code>ssh-copy-id username@server_IP\n<\/code><\/pre>\n<p>Created config file which lets me execute all <code>dvc<\/code> commands using ssh:<\/p>\n<pre><code>[core]\n    remote = storage_server\n['remote &quot;storage_server&quot;']\n    url = ssh:\/\/username@server_IP:\/home\/DVC_remote\/DVC_project\n<\/code><\/pre>\n<p>What I should do so that several people could execute commands on their own name?<\/p>",
        "Challenge_closed_time":1643651143316,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643641422880,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is asking if it is possible to use DVC with multiple accounts on the same machine. They have been executing all commands under their name, but with other people joining the project, they want to avoid this. The user has generated an SSH key, connected to the server where DVC remote data is stored, and created a config file to execute all DVC commands using SSH. The user is seeking advice on how to allow multiple people to execute commands under their own names.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70928144",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":11.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2.7001211111,
        "Challenge_title":"Multiple users in DVC",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":181.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457469301700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":585.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>You need to make the &quot;username&quot; part of the config personalized based on who is running the command. There are a few options to do this (based on <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type\" rel=\"nofollow noreferrer\">this document<\/a>, see the SSH part):<\/p>\n<h2>Basic options are:<\/h2>\n<ul>\n<li>User defined in the SSH config file (e.g. <code>~\/.ssh\/config<\/code>) for this host (URL);<\/li>\n<li>Current system user;<\/li>\n<\/ul>\n<p>So, the simplest even options could be just remove it from the URL and rely on the current system user?<\/p>\n<h2>Local (git-ignored or per-project DVC config) config<\/h2>\n<p>You could do is to remove the <code>username<\/code> part from the <code>url<\/code> and run something like this:<\/p>\n<pre><code>dvc remote modify --local storage_server user username\n<\/code><\/pre>\n<p><code>--local<\/code> here means that DVC will create a separate additional config that will be ignored by Git. This way if every user runs this command in every project they use they will customize the username.<\/p>\n<hr \/>\n<p>Let me know if that helps or something doesn't work. I'll try to help.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":14.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":160.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1429262032907,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Manchester, United Kingdom",
        "Answerer_reputation_count":11490.0,
        "Answerer_view_count":2150.0,
        "Challenge_adjusted_solved_time":0.3111347222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Are these roles to be deleted?<\/p>\n<ol>\n<li>AmazonSageMakerServiceCatalogProductsLaunchRole<\/li>\n<li>AmazonSageMakerServiceCatalogProductsUseRole<\/li>\n<li>AWSServiceRoleForAmazonSageMakerNotebooks<\/li>\n<\/ol>\n<p>Are these roles to be deleted?<\/p>\n<ol>\n<li>AmazonSageMakerServiceCatalogProductsUseRole<\/li>\n<li>Plus some execution policies<\/li>\n<\/ol>\n<p>Is Jupyter server within sagemaker studio also be stopped for not being charged?<\/p>",
        "Challenge_closed_time":1633709785048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633708664963,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking advice on which IAM roles and policies to delete in order to avoid being charged by AWS. They have listed some roles and policies and are asking if they should be deleted. They also inquire about stopping the Jupyter server within SageMaker Studio to avoid charges.",
        "Challenge_last_edit_time":1634116759467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69498670",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":6.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.3111347222,
        "Challenge_title":"Which IAM roles and policies should I delete to not being charged by AWS?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":105.0,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Poster_created_time":1623330365063,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":75.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p><strong>AWS IAM is a free service<\/strong> - you do not get charged for roles, policies or any other aspect of IAM.<\/p>\n<p>From <a href=\"https:\/\/aws.amazon.com\/iam\/#:%7E:text=IAM%20is%20a%20feature%20of,AWS%20services%20by%20your%20users.\" rel=\"nofollow noreferrer\">the documentation<\/a>:<\/p>\n<blockquote>\n<p>IAM is a feature of your AWS account offered at no additional charge. You will be charged only for use of other AWS services by your users.<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.0,
        "Solution_reading_time":6.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":9614.8936591666,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I uploaded a model with<\/p>\n<pre><code>gcloud beta ai models upload --artifact-uri\n<\/code><\/pre>\n<p>And in the docker I access <code>AIP_STORAGE_URI<\/code>.\nI see that <code>AIP_STORAGE_URI<\/code> is another Google Storage location so I try to download the files using <code>storage.Client()<\/code> but then it says that I don't have access:<\/p>\n<pre><code>google.api_core.exceptions.Forbidden: 403 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/caip-tenant-***-***-*-*-***?projection=noAcl&amp;prettyPrint=false: custom-online-prediction@**.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket\n<\/code><\/pre>\n<p>I am running this endpoint with the default service account.<\/p>\n<p><a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#artifacts\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#artifacts<\/a><\/p>\n<p>According to the above link:\n<code>The service account that your container uses by default has permission to read from this URI. <\/code><\/p>\n<p>What am I doing wrong?<\/p>",
        "Challenge_closed_time":1627666181500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626854597743,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to access AIP_STORAGE_URI in Vertex AI after uploading a model using gcloud beta ai models upload --artifact-uri. However, when attempting to download the files using storage.Client(), they receive a 403 Forbidden error indicating that the default service account does not have access to the Google Cloud Storage bucket. The user is seeking assistance in resolving this issue.",
        "Challenge_last_edit_time":1626858370043,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68465990",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":15.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":225.4399325,
        "Challenge_title":"How do I access AIP_STORAGE_URI in Vertex AI?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":742.0,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1466977784156,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":117.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>The reason behind the error being, the default service account that Vertex AI uses has the \u201c<a href=\"https:\/\/cloud.google.com\/storage\/docs\/access-control\/iam-roles#standard-roles\" rel=\"nofollow noreferrer\">Storage Object Viewer<\/a>\u201d role which excludes the <code>storage.buckets.get<\/code> permission. At the same time, the <code>storage.Client()<\/code> part of the code makes a <code>storage.buckets.get<\/code> request to the Vertex AI managed bucket for which the default service account does not have permission to.<\/p>\n<p>To resolve the issue, I would suggest you to follow the below steps -<\/p>\n<ol>\n<li><p>Make changes in the custom code to access the bucket with the model artifacts in your project instead of using the environment variable <code>AIP_STORAGE_URI<\/code> which points to the model location in the Vertex AI managed bucket.<\/p>\n<\/li>\n<li><p>Create your own service account and grant the service account with all the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/custom-service-account\" rel=\"nofollow noreferrer\">permissions<\/a> needed by the custom code. For this specific error, a role with the <code>storage.buckets.get<\/code> permission, eg. <a href=\"https:\/\/cloud.google.com\/storage\/docs\/access-control\/iam-roles#standard-roles\" rel=\"nofollow noreferrer\">Storage Admin<\/a> (&quot;roles\/storage.admin&quot;) has to be granted to the service account.<\/p>\n<\/li>\n<li><p>Provide the newly created service account in the &quot;Service Account&quot; field when deploying the model.<\/p>\n<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1661471987216,
        "Solution_link_count":3.0,
        "Solution_readability":11.7,
        "Solution_reading_time":19.89,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":176.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1305851487736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Challenge_adjusted_solved_time":32.1755194445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose I run the following commands:<\/p>\n<pre><code># set up DVC\n\nmkdir foo\ncd foo &amp;&amp; git init\ndvc init\ngit add * &amp;&amp; git commit -m &quot;dvc init&quot;\n\n\n# make a data file\n\nmkdir -p bar\/biz\ntouch bar\/biz\/boz\n\n\n# add the data file\n\ndvc add bar\/biz\/boz\n<\/code><\/pre>\n<p>And DVC outputs the following:<\/p>\n<pre><code>To track the changes with git, run:\n\n  git add bar\/biz\/.gitignore bar\/biz\/boz.dvc\n<\/code><\/pre>\n<hr \/>\n<p>This last part is what I would like to avoid.  Preferably, DVC would only change the top level <code>.gitignore<\/code> (located at the project root, where <code>git init<\/code> was executed), and will change only DVC files at the top level.<\/p>\n<p><strong>And here's why:<\/strong><\/p>\n<p>I have a rather large dataset developed in an original work more or less ad-hoc. This data is not systematically organized, nor do I want to organize it as-is.<\/p>\n<p>Instead, I want to incrementally add this old, bespoke data to the DVC directory tree.  And each time I add some of the data to the tree, I want to check it in with DVC as I would if I were modifying code or mixing one project's code into another.<\/p>\n<p>However, DVC wants to create a local file and gitignore at every location I add.  This creates a mess and I have no reasonable faith that it will be easy to maintain all of these atomic and distributed datastores.<\/p>\n<hr \/>\n<p><strong>The question:<\/strong><\/p>\n<p>What is the preferred way to incrementally add data in DVC so that DVC uses the root gitignore and root DVC files\/items?<\/p>",
        "Challenge_closed_time":1655349731160,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655234204993,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to add individual files with DVC without creating a local file and gitignore at every location they add. They have a large dataset that they want to incrementally add to the DVC directory tree, but DVC wants to create a local file and gitignore at every location they add, which creates a mess and is difficult to maintain. The user is asking for the preferred way to incrementally add data in DVC so that DVC uses the root gitignore and root DVC files\/items.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72622280",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":19.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":32.0906019444,
        "Challenge_title":"How does one add individual files with DVC?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":112.0,
        "Challenge_word_count":254,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405262190020,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Atlanta, GA",
        "Poster_reputation_count":26244.0,
        "Poster_view_count":1383.0,
        "Solution_body":"<p>Assuming bar\/ is the dataset directory you're incrementally adding to, you can instead<\/p>\n<pre><code>dvc add bar\n<\/code><\/pre>\n<p>This creates a bar.dvc file and writes to .gitignore at the top level.<\/p>\n<p>When you update content in bar\/, <code>dvc add<\/code> it again or use <code>dvc commit<\/code> to register the new dataset version. The new files get added to the project cache and the .dvc file gets an updated <code>md5<\/code> hash that identifies to the latest directory structure.<\/p>\n<p>Some docs:<br \/>\n<a href=\"https:\/\/dvc.org\/doc\/start\/data-management#making-changes\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/start\/data-management#making-changes<\/a><br \/>\n<a href=\"https:\/\/dvc.org\/doc\/command-reference\/add\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/add<\/a><br \/>\n<a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1655350036863,
        "Solution_link_count":6.0,
        "Solution_readability":16.8,
        "Solution_reading_time":14.03,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":89.0,
        "Tool":"DVC"
    }
]