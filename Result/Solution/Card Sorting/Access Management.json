[
    {
        "Answerer_created_time":1502379091636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1886.0,
        "Answerer_view_count":145.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm currently trying to script our Azure Machine Learning infrastructure, using ARM templates and running through Terraform.  In order to ensure that the template works, I'm first running it from a file using the Az CLI.<\/p>\n\n<p>I'm running this on Ubuntu, with the below version of the Az CLI:-<\/p>\n\n<pre><code>azure-cli                         2.0.78\n\ncommand-modules-nspkg              2.0.3\ncore                              2.0.78\nnspkg                              3.0.4\ntelemetry                          1.0.4\n\nPython location '\/opt\/az\/bin\/python3'\nExtensions directory '\/home\/blah\/.azure\/cliextensions'\n\nPython (Linux) 3.6.5 (default, Dec 12 2019, 11:11:33) \n[GCC 8.3.0]\n<\/code><\/pre>\n\n<p>I have already created the Storage Account, App Insights and Key Vault using terraform.<\/p>\n\n<p>When trying to run the template using the Az CLI with the command:-<\/p>\n\n<pre><code>az group deployment create --name MachineLearning --resource-group data-science --template-file ML_ARM.json --parameters appInsightsName=machine-learning-dev storageAccountName=machinelearningdev keyVaultName=data-science-dev mlApiVersion=2018-11-19 mlWorkspaceName=machine-learning-dev location=uksouth\n<\/code><\/pre>\n\n<p>I receive the following error:-<\/p>\n\n<p><code>Make sure to create your workspace using a client which support MSI<\/code><\/p>\n\n<p>The ARM template is below:-<\/p>\n\n<pre><code>{\n    \"$schema\": \"https:\/\/schema.management.azure.com\/schemas\/2015-01-01\/deploymentTemplate.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"storageAccountName\": {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"The name of the storage account\"\n            }\n        },\n        \"appInsightsName\" : {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"The name of the app insights account\"\n            }\n        },\n        \"keyVaultName\": {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"The name of the keyvault resource\"\n            }\n        },\n        \"mlApiVersion\": {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"The api version of the ML workspace\"\n            }\n        },\n        \"mlWorkspaceName\": {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"The name of the Machine Learning Workspace\"\n            }\n        },\n        \"location\": {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"Resource location\"\n            }\n        }\n    },\n  \"resources\": [\n        {\n            \"apiVersion\": \"[parameters('mlApiVersion')]\",\n            \"type\": \"Microsoft.MachineLearningServices\/workspaces\",\n            \"name\": \"[parameters('mlWorkspaceName')]\",\n            \"location\": \"[parameters('location')]\",\n            \"sku\": {\n              \"tier\": \"enterprise\",\n              \"name\": \"enterprise\"\n            },\n            \"properties\": {\n                \"storageAccount\": \"[resourceId('Microsoft.Storage\/storageAccounts',parameters('storageAccountName'))]\",\n                \"applicationInsights\": \"[resourceId('Microsoft.Insights\/components',parameters('appInsightsName'))]\",\n                \"keyVault\": \"[resourceId('Microsoft.KeyVault\/vaults',parameters('keyVaultName'))]\"\n            }\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>Some rudimentary googling hasn't really been enlightening into what the issue might be with this; the documentation and guide templates for the Machine Learning Service are linked below:-<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-workspace-template\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-workspace-template<\/a>\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/templates\/microsoft.machinelearningservices\/2019-11-01\/workspaces\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/templates\/microsoft.machinelearningservices\/2019-11-01\/workspaces<\/a><\/p>\n\n<p>Any idea what the issue might be?  Thanks in advance for any pointers!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576767488223,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59412213",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":15.2,
        "Challenge_reading_time":46.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":null,
        "Challenge_title":"Error: \"Make sure to create your workspace using a client which support MSI\" when deploying Azure ARM template for Machine Learning Services Workpsace",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":422.0,
        "Challenge_word_count":307,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562705135403,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":327.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>I am not familar with Terraform or that robust on ML Services; however, the error you provided lends itself to needing to have MSI authentication configured which is configured in the link you provided.<\/p>\n\n<p>Try updating your ARM to include the identity section like this:<\/p>\n\n<pre><code>   ...  },\n\"identity\": {\n        \"type\": \"systemAssigned\"\n      },\n                \"properties\": {\n                    \"storageAccount\": \"[resourceId('Microsoft.Storage\/storageAccounts',parameters('storageAccountName'))]\",\n                    \"applicationInsights\": \"[resourceId('Microsoft.Insights\/components',parameters('appInsightsName'))]\",\n                    \"keyVault\": \"[resourceId('Microsoft.KeyVault\/vaults',parameters('keyVaultName'))]\"\n                }\n<\/code><\/pre>\n\n<p>This will create the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/active-directory\/managed-identities-azure-resources\/overview\" rel=\"nofollow noreferrer\">Managed Service Identity<\/a>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":20.0,
        "Solution_reading_time":11.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any possibility of doing the following operations in Azure ML Studio through REST calls?\n1) Create and upload a new dataset.\n2) Create a new Automated ML run selecting an already created dataset, configuring the experiment name, target column and training cluster and selecting the task type (e.g. Classification\/Regression).\n3) Deploy the run on a container and retrieve the container endpoint URL.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586328317900,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61094767",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":6.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Doing the following operations in Azure ML Studio through REST calls",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":91.0,
        "Challenge_word_count":74,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Please follow the APIs available at the <a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/<\/a> \n for Azure ML studio through REST API calls, but other than dataset-related API.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/rTTNz.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rTTNz.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.9,
        "Solution_reading_time":5.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How to use safe and protected communication?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656963784913,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/913567\/problems-secure-connection",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":1.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"problems secure connection",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":9,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Your question is too vague for anyone to answer. Can you please elaborate?    <\/p>\n<p>If you are talking about securing Azure Machine Learning you will need to use Virtual Networks (VNet) to protect.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-network-security-overview\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-network-security-overview<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":5.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":46.8695180555,
        "Challenge_answer_count":2,
        "Challenge_body":"I want to allowlist the Sagemaker studio IP so people can access certain allowlisted services from Sagemaker. I created a sagemaker domain in my private subnet of my VPC, so theoretically it should use the IP of the associated NAT gateway, right? But I see a different IP \ud83e\udd14",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1674683552396,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1675031393875,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUX7n9V0osTAmdmLYM21vmKQ\/how-to-allowlist-sagemaker-ip",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":3.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to allowlist sagemaker IP?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":131.0,
        "Challenge_word_count":52,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I should've read through my terraform code that created Sagemaker more carefully, I specified a VPC so I thought it would be *in* the VPC but it turns out I needed to specify the AppNetworkAccessType too.\n\nbad:\n```hcl\nresource \"aws_sagemaker_domain\" \"my_domain\" {\n  domain_name = var.domain_name\n  auth_mode   = \"IAM\"\n  vpc_id      = var.vpc_id\n  subnet_ids  = var.subnet_ids\n```\n\ngood:\n```hcl\nresource \"aws_sagemaker_domain\" \"my_domain\" {\n  domain_name = var.domain_name\n  auth_mode   = \"IAM\"\n  vpc_id      = var.vpc_id\n  subnet_ids  = var.subnet_ids\n  app_network_access_type = \"VpcOnly\"\n```",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1675200124140,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":7.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to understand how DEP works in ML.<\/p>\n<p>The Microsoft recommended architecture states that I must use a service endpoint along with a service endpoint policy to prevent ML compute subnets from gaining access to non-white listed storage accounts (<a href=\"\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-network-isolation-planning#recommended-architecture-with-data-exfiltration-prevention<\/a>)<\/p>\n<p>Some other examples I found on the web don't use service endpoints and instead prefer private endpoints for storage accounts. Does using PEs alone prevent data exfiltration? I'm not sure, because from what I've seen so far, it's possible to add any storage account as a datastore through the ML workspace as long as you have the appropriate access rights for the storage account.<\/p>\n<p>So I'm a bit confused and would appreciate if someone could shed some light on this.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677415689026,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1184373\/azure-ml-and-data-exfiltration-prevention",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":12.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML and Data exfiltration prevention",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":127,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=c7bbbc2c-bba8-4500-a3a8-df7a87f8e72a\">Plodie<\/a> Thanks, using private endpoints alone may not prevent data exfiltration, but it can reduce the attack surface and the chances of data exfiltration. It is recommended to use a combination of Azure Virtual Network, Azure Private Link, and <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/governance\/policy\/overview\">Azure Policy<\/a> to secure your Azure Machine Learning resources.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.7,
        "Solution_reading_time":6.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hello,<\/p>\n<p>After several tries I am getting stuck when finishing a run about data uploading. It always throws the same error and it seems it \u2018kills\u2019 my network, consuming all resources as when I try to access other pages the internet is very slow.<\/p>\n<p>When I try to do the tests I always have a good connection (around 600Mb), and I tried on different days. I never have connection cuts, just when doing the run finish.<\/p>\n<p>The error I have is the following:<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/a27d197a4c8825679f7e5c459ef89d5f87a2a192.png\" alt=\"Selection_001\" data-base62-sha1=\"nbrnemiqAtyylTXSAgkOD1SYaSS\" width=\"573\" height=\"182\"><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675022377391,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/network-error-connecttimeout-entering-retry-loop\/3772",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Network error (ConnectTimeout), entering retry loop",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":220.0,
        "Challenge_word_count":94,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey Mario!<\/p>\n<p>We have been having an influx of traffic lately. Please try again and if you still run into the ConnectTimeout Network error, please send us your debug.log so we can take a closer look.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":2.57,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1523192621643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":1229.0,
        "Answerer_view_count":175.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to create a dataset on my azure ML workspace from a GitHub action<\/p>\n<p>I've created a datastore and uploaded data to that datastore\nwhen I try to create a dataset using the cli, I get this error:<\/p>\n<p><code>'create' is misspelled or not recognized by the system.<\/code><\/p>\n<p>this is the command i use:<\/p>\n<pre><code>&gt; az ml dataset create \n          -n insurance_dataset \n          --resource-group rg-name \n          --workspace-name ml-ws-name \n          -p 'file:azureml\/datastore\/$(az ml datastore show-default -w ml-ws-name -g rg-name --query name -o tsv)\/insurance\/insurance.csv'\n<\/code><\/pre>\n<p>any idea what am I doing wrong?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1658758142813,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73110661",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":13.0,
        "Challenge_reading_time":8.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"'create' is misspelled or not recognized by the system on az ml dataset create",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":99,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1523192621643,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":1229.0,
        "Poster_view_count":175.0,
        "Solution_body":"<p>in my case, the issue was solved by upgrading the ml extension to <code>azure-cli-ml v2<\/code><\/p>\n<p>Remove any existing installation of the of <code>ml<\/code> extension and also the CLI v1 <code>azure-cli-ml<\/code> extension:<\/p>\n<pre><code>az extension remove -n azure-cli-ml\naz extension remove -n ml\n<\/code><\/pre>\n<p>Now, install the ml extension:<\/p>\n<pre><code>az extension add -n ml -y\n<\/code><\/pre>\n<p>which still doesn't explain why the <code>create<\/code> command wasn't recognized, but the v2 behavior works fine for me.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":32.6,
        "Solution_reading_time":6.87,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1423640080283,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Lyon, France",
        "Answerer_reputation_count":457.0,
        "Answerer_view_count":125.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a compute instance:<\/p>\n<p>Virtual machine size\nSTANDARD_DS3_V2 (4 Cores, 14 GB RAM, 28 GB Disk)<\/p>\n<p>Processing Unit\nCPU - General purpose<\/p>\n<p>But, I'm not able to access it when trying to set it for data drift monitoring.\nThe dropdown list is empty. I can't understand why. Can you help me please?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Pf10h.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Pf10h.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601276103867,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64097278",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":7.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Why is the field \"compute target\" for data drift monitoring in Azure ML studio still blank whereas I have a compute instance?",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":88.0,
        "Challenge_word_count":84,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423640080283,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lyon, France",
        "Poster_reputation_count":457.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>I found the answer. You must give a <strong>cluster<\/strong> compute instance to do data drift in Azure Machine Learning Studio. As it is not clear, I'm planning to add something in the documentation of Azure.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":2.67,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":35.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"load_dataset function from hugging face can't access the dvc tracked data directory \r\n--> OSError: [Errno 30] Read-only file system: '\/data'",
        "Challenge_closed_time":1642070.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641729327000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/johannespischinger\/senti_anal\/issues\/11",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":2.07,
        "Challenge_repo_contributor_count":2.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":95.0,
        "Challenge_repo_star_count":2.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"data loading bug with dvc",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":23,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"What command are you using? Note `\/data` is not same as `.\/data`",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":0.78,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":0.4647708333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>This is my first time trying to use the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Kedro<\/a> package.<\/p>\n<p>I have a list of .wav files in an s3 bucket, and I'm keen to know how I can have them available within the Kedro data catalog.<\/p>\n<p>Any thoughts?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611660152193,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65900415",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":4.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I add a directory of .wav files to the Kedro data catalogue?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":294.0,
        "Challenge_word_count":56,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1500383313376,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":851.0,
        "Poster_view_count":86.0,
        "Solution_body":"<p>I don't believe there's currently a dataset format that handles <code>.wav<\/code> files. You'll need to build a <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/03_custom_datasets.html\" rel=\"nofollow noreferrer\">custom dataset<\/a> that uses something like <a href=\"https:\/\/docs.python.org\/3\/library\/wave.html\" rel=\"nofollow noreferrer\">Wave<\/a> - not as much work as it sounds!<\/p>\n<p>This will enable you to do something like this in your catalog:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>dataset:\n  type: my_custom_path.WaveDataSet\n  filepath: path\/to\/individual\/wav_file.wav # this can be a s3:\/\/url\n<\/code><\/pre>\n<p>and you can then access your WAV data natively within your Kedro pipeline. You can do this for each <code>.wav<\/code> file you have.<\/p>\n<p>If you wanted to be able to access a whole folders worth of wav files, you might want to explore the notion of a &quot;wrapper&quot; dataset like the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.PartitionedDataSet.html\" rel=\"nofollow noreferrer\">PartitionedDataSet<\/a> whose <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html#partitioned-dataset-definition\" rel=\"nofollow noreferrer\">usage guide<\/a> can be found in the documentation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1611661825368,
        "Solution_link_count":4.0,
        "Solution_readability":12.2,
        "Solution_reading_time":16.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":129.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1442422586352,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Atlanta, United States",
        "Answerer_reputation_count":20328.0,
        "Answerer_view_count":2380.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to use terraform to create a model on SageMaker by following <a href=\"https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/sagemaker_model\" rel=\"nofollow noreferrer\">this page<\/a>\nI can't assign a full access policy to the sagemaker role due to permission constrains, so I created a role and attached a policy with part of the permissions<\/p>\n<p>When I tested <code>Terraform plan<\/code>, it gave me this:<\/p>\n<pre><code>Error: Invalid template interpolation value\n...\n<\/code><\/pre>\n<pre><code>..........................\n 141:                 &quot;ecr:GetRepositoryPolicy&quot;\n 142:             ],\n 143:             &quot;Resource&quot;: [\n 144:                 &quot;arn:aws:s3:::${aws_s3_bucket.xx_xxxxxxxxxx_xxx_bucket}&quot;,\n 145:                 &quot;arn:aws:s3:::${local.binaries_bucket_name}&quot;,\n 146:                 &quot;arn:aws:s3:::${aws_s3_bucket.xx_xxxxxxxxxx_xxx_bucket}\/*&quot;,\n 147:                 &quot;arn:aws:s3:::${local.binaries_bucket_name}\/*&quot;,\n 148:                 &quot;arn:aws:ecr:us-east-1:*:repository\/*&quot;,\n 149.....................\n 157:         }\n 158:     ]\n 159: }\n 160: POLICY\n    |----------------\n    | aws_s3_bucket.xx_xxxxxxxxxx_xxx_bucket is object with 25 attributes\n\nCannot include the given value in a string template: string required.\n<\/code><\/pre>\n<p>I'm new to this, just wondering if this is complaining the bucket name is too long or something else? What should I do to fix this, I'm a bit confused. Many thanks.<\/p>\n<p>(PS: Terraform version <code>v0.13.4<\/code> + provider registry.terraform.io\/hashicorp\/aws <code>v3.20.0<\/code>)<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1607622036003,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1607700252620,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65239565",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":12.3,
        "Challenge_reading_time":20.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Terraform - Cannot include the given value in a string template: string required",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":8882.0,
        "Challenge_word_count":152,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>It appears what you want here is the ARN of the S3 bucket, which is provided by <a href=\"https:\/\/www.terraform.io\/docs\/configuration\/blocks\/resources\/behavior.html#accessing-resource-attributes\" rel=\"noreferrer\">exported resource attributes<\/a>. Specifically, you probably want the <code>arn<\/code> resource attribute.<\/p>\n<p>Updating your policy like:<\/p>\n<pre><code> 144:             &quot;${aws_s3_bucket.xx_xxxxxxxxxx_xxx_bucket.arn}&quot;,\n 146:             &quot;${aws_s3_bucket.xx_xxxxxxxxxx_xxx_bucket.arn}\/*&quot;,\n<\/code><\/pre>\n<p>will provide you with the String that you need by accessing the <code>arn<\/code> attribute. The currently written policy is accessing <code>aws_s3_bucket.xx_xxxxxxxxxx_xxx_bucket<\/code>, which is a Map (possibly Object) of every argument and attribute for that resource, and will not interpolate within the string of your policy.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.1,
        "Solution_reading_time":11.28,
        "Solution_score_count":5.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":85.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We need to connect Azure Data Lake Storage Gen2 to Azure Machine Learning by means of a datastore. For security reasons we do not want to provide the credential-based authentication credentials (service principal or SAS token). Instead we want to connect with identity based access.  <\/p>\n<p>The problem we face is that we are not able to assign a managed identity to a compute instance, so we can connect from notebooks to the Data Lake. In the documentation is explained how to assign a managed identity to a cluster, but we need the same for the compute instance, as it is the only way to run commands directly from the notebook.  <\/p>\n<p>Is there a way to assign managed identity to an Azure Machine Learning Compute Instance? Otherwise, we would like to know the best approach to overcome this issue, considering that we do not want to introduce the credentials in the code.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1637143820660,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/630520\/azure-ml-managed-identity-for-compute-instance",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.9,
        "Challenge_reading_time":11.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML - Managed Identity for Compute Instance",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":159,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=cbcc7cd1-be1d-4352-9648-87343bfaafff\">@Nimbeo  <\/a> Thanks for the question. Currently It\u2019s not supported yet  to assign managed identity to an Azure Machine Learning Compute Instance, you\u2019d need to use credential-based access. We have forwarded to the product team to support in the near future.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":4.22,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1391261341596,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I wants to create azure machine learning workspace using terraform scripts.Is there any terraform provider to achieve this.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1581577491623,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60202189",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":5.7,
        "Challenge_reading_time":2.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How to create azure machine learning resource using terraform resource providers?",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1152.0,
        "Challenge_word_count":28,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565633099383,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":110.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>In the meantime Microsoft has added a Terraform resource for ML Workspace in the Azure Provider. This should make any custom scripting obsolete.<\/p>\n<p><a href=\"https:\/\/www.terraform.io\/docs\/providers\/azurerm\/r\/machine_learning_workspace.html\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/docs\/providers\/azurerm\/r\/machine_learning_workspace.html<\/a><\/p>\n<pre><code>resource &quot;azurerm_machine_learning_workspace&quot; &quot;example&quot; {\n  name                    = &quot;example-workspace&quot;\n  location                = azurerm_resource_group.example.location\n  resource_group_name     = azurerm_resource_group.example.name\n  application_insights_id = azurerm_application_insights.example.id\n  key_vault_id            = azurerm_key_vault.example.id\n  storage_account_id      = azurerm_storage_account.example.id\n\n  identity {\n    type = &quot;SystemAssigned&quot;\n  }\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":25.1,
        "Solution_reading_time":11.31,
        "Solution_score_count":4.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I created a batch endpoint, deployment and now creating deployment job in Machine Learning Workspace. I am providing the input data via datastore to read data from an external data storage. I followed the following tutorial <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data-batch-endpoints-jobs?view=azureml-api-2&amp;tabs=sdk\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data-batch-endpoints-jobs?view=azureml-api-2&amp;tabs=sdk<\/a> and under the title <strong>Security considerations when reading data<\/strong> within the table, in the second row, identity of the job should be enough. But I am still getting authorization error when I try to read the data. Is it because I want to read from an external storage account, meaning that compute cluster should be authenticated by the external storage account as well? <\/p>\n<p>Thank you for your time and help.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684230280513,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1285828\/ml-batch-endpoint-using-datastore-to-access-an-ext",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":13.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"ML Batch Endpoint, using datastore to access an external data storage",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":120,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Yes, you are correct. In order to read data from an external storage account, the compute cluster must be authenticated by the external storage account as well. This is because the compute cluster needs to have permission to access the data in the external storage account.<\/p>\n<p>There are a few ways to authenticate the compute cluster with the external storage account. One way is to use a service principal. A service principal is an identity that can be used to access Azure resources. To create a service principal, you can use the Azure portal or the Azure CLI.<\/p>\n<p>Once you have created a service principal, you need to grant it access to the external storage account. You can do this by assigning the service principal a role in the external storage account. The role that you assign will determine what level of access the service principal has to the external storage account.<\/p>\n<p>Once you have granted the service principal access to the external storage account, you need to configure the compute cluster to use the service principal. You can do this by setting the <code>AZURE_STORAGE_ACCOUNT_CONNECTION_STRING<\/code> environment variable on the compute cluster. The value of this environment variable should be the connection string for the external storage account.<\/p>\n<p>Once you have configured the compute cluster to use the service principal, you should be able to read data from the external storage account. If you are still getting an authorization error, you can try the following:<\/p>\n<ul>\n<li> Make sure that the service principal has the correct permissions to access the external storage account.<\/li>\n<li> Make sure that the <code>AZURE_STORAGE_ACCOUNT_CONNECTION_STRING<\/code> environment variable is set correctly on the compute cluster.<\/li>\n<li> Restart the compute cluster.<\/li>\n<\/ul>\n<p>If you are still having trouble, you can contact Azure support for help.<\/p>\n<p>Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/><\/p>\n<p>and upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/><\/p>\n<p>button if you find this helpful.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.4,
        "Solution_reading_time":28.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":318.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":1.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"We are in a process to move all of our IAM users to aws SSO \nwe used to have this policy for sagemaker :\n\n\n\"\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListTags\",\n                \"sagemaker:DeleteNotebookInstance\",\n                \"sagemaker:StopNotebookInstance\",\n                \"sagemaker:CreatePresignedNotebookInstanceUrl\",\n                \"sagemaker:DescribeNotebookInstance\",\n                \"sagemaker:StartNotebookInstance\",\n                \"sagemaker:UpdateNotebookInstance\"\n            ],\n            \"Resource\": \"arn:aws:sagemaker:::notebook-instance\/${aws:username}*\"\n        },\n        {\n            \"Sid\": \"VisualEditor1\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListNotebookInstanceLifecycleConfigs\",\n                \"sagemaker:ListNotebookInstances\",\n                \"sagemaker:ListCodeRepositories\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n```\n\n\"\n\n\nthis would give access to each user to use his\\hers own notebook\nnow on the new SSO permission set i gave this \n\n\n\n```\n\"\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"glue:CreateScript\",\n                \"secretsmanager:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListTags\",\n                \"sagemaker:DeleteNotebookInstance\",\n                \"sagemaker:StopNotebookInstance\",\n                \"sagemaker:CreatePresignedNotebookInstanceUrl\",\n                \"sagemaker:Describe*\",\n                \"sagemaker:StartNotebookInstance\",\n                \"sagemaker:UpdateNotebookInstance\",\n                \"sagemaker:CreatePresignedDomainUrl\",\n                \"sagemaker:*\"\n            ],\n            \"Resource\": \"arn:aws:sagemaker:::notebook-instance\/*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"aws:ResourceTag\/Owner\": \"${identitystore:UserId}\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListTags\",\n                \"sagemaker:Describe*\",\n                \"sagemaker:StartNotebookInstance\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\"\n```\n\n\n\n\nthis is what i tried but i cant make it work \nplease assist?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658054479812,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1668442079408,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUruheXJHaQVu_S9LIzUyDAw\/policy-that-allows-only-one-sso-user-to-access-a-resource",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":38.9,
        "Challenge_reading_time":22.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Policy that allows only one SSO user to access a resource",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":164.0,
        "Challenge_word_count":128,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello, \n\nI understand that you are currently trying to restrict access to Sagemaker notebook using SSO identity's UserID. \n\nCurrently, I leveraged your provided SSO Permission set and tweaked it out as you can see below, and finally tested it out on AWS SageMaker Console by logging in as an AWS SSO User, and was able to see successful start\/stop\/describing of the SageMaker notebook (with Tags - Owner:UserId) corresponding to the SSO UserId.\n\n```\n{\n\t\"Version\": \"2012-10-17\",\n\t\"Statement\": [\n\t\t{\n\t\t\t\"Effect\": \"Allow\",\n\t\t\t\"Action\": [\n\t\t\t\t\"glue:CreateScript\",\n\t\t\t\t\"secretsmanager:*\"\n\t\t\t],\n\t\t\t\"Resource\": \"*\"\n\t\t},\n\t\t{\n\t\t\t\"Effect\": \"Allow\",\n\t\t\t\"Action\": [\n\t\t\t\t\"sagemaker:ListTags\",\n\t\t\t\t\"sagemaker:DeleteNotebookInstance\",\n\t\t\t\t\"sagemaker:StopNotebookInstance\",\n\t\t\t\t\"sagemaker:CreatePresignedNotebookInstanceUrl\",\n\t\t\t\t\"sagemaker:Describe*\",\n\t\t\t\t\"sagemaker:StartNotebookInstance\",\n\t\t\t\t\"sagemaker:UpdateNotebookInstance\",\n\t\t\t\t\"sagemaker:CreatePresignedDomainUrl\"\n\t\t\t],\n\t\t\t\"Resource\": \"arn:aws:sagemaker:us-east-1:7XXXXXXXXX:notebook-instance\/*\",\n\t\t\t\"Condition\": {\n\t\t\t\t\"StringEquals\": {\n\t\t\t\t\t\"sagemaker:ResourceTag\/Owner\": \"${identitystore:UserId}\"\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t{\n\t\t\t\"Sid\": \"VisualEditor1\",\n\t\t\t\"Effect\": \"Allow\",\n\t\t\t\"Action\": [\n\t\t\t\t\"sagemaker:ListNotebookInstanceLifecycleConfigs\",\n\t\t\t\t\"sagemaker:ListNotebookInstances\",\n\t\t\t\t\"sagemaker:ListCodeRepositories\"\n\t\t\t],\n\t\t\t\"Resource\": \"*\"\n\t\t}\n\t]\n}\n```\n________________________________\n\nHowever, in case if this SSO User tried to stop any other Sagemaker notebooks, which didn't have the tags corresponding to their UserId, then the following errors were observed as expected behavior -\n\n```\nUser: arn:aws:sts::7XXXXXXXXX:assumed-role\/AWSReservedSSO_SageMXXXXXXXXXbe\/test1 is not authorized to perform: sagemaker:StopNotebookInstance on resource: arn:aws:sagemaker:us-east-1:7XXXXXXXXX:notebook-instance\/userachecking because no identity-based policy allows the sagemaker:StopNotebookInstance action\n\nor \n\nUser: arn:aws:sts::7XXXXXXXXX:assumed-role\/AWSReservedSSO_SageMXXXXXXXXXbe\/test1 is not authorized to perform: sagemaker:DescribeNotebookInstance on resource: arn:aws:sagemaker:us-east-1:7XXXXXXXXX:notebook-instance\/Test1Check because no identity-based policy allows the sagemaker:DescribeNotebookInstance action\n```\n\nAlso, please note that unlike your provided IAM policy, your SSO permission set policy was missing the action - `sagemaker:ListNotebookInstances` which also raised an error for not being able to list out the notebook instances on AWS SageMaker Console in my testing. Hence, I had added the appropriate Sagemaker list actions to your permission set as well. \n\n**Additional Information** - \n\na. ${identitystore:UserId} -> Each user in the AWS SSO identity store is assigned a unique UserId. You can view the UserId for your users by using the AWS SSO console and navigating to each user or by using the DescribeUser API action. [1]\n\nb. ListNotebookInstances -> Returns a list of the SageMaker notebook instances in the requester's account in an AWS Region. [2]\n\nc. ResourceTag -> You can use the ResourceTag\/key-name condition key to determine whether to allow access to the resource based on the tags that are attached to the resource. [3][4]\n\nd. sagemaker:ResourceTag\/ -> Filters access by the preface string for a tag key and value pair attached to a resource [5]\n\ne. sagemaker:ResourceTag\/${TagKey} -> Filters access by a tag key and value pair [5]\n____________________________________\nI hope the shared information is insightful to your query. In case, if you have any other queries or concerns regarding AWS SSO or Sagemaker services or any account specific configuration that you would like to discuss, then please feel free to reach out to our team directly by creating a support case with our premium support team. \n\nHave a wonderful day ahead and stay safe. \n____________________________________\nReferences: \n\n[1] https:\/\/docs.aws.amazon.com\/singlesignon\/latest\/userguide\/using-predefined-attributes.html\n\n[2] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ListNotebookInstances.html\n\n[3] https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_tags.html\n\n[4] https:\/\/aws.amazon.com\/blogs\/security\/simplify-granting-access-to-your-aws-resources-by-using-tags-on-aws-iam-users-and-roles\/\n\n[5] https:\/\/docs.aws.amazon.com\/service-authorization\/latest\/reference\/list_amazonsagemaker.html#amazonsagemaker-policy-keys",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1658148839830,
        "Solution_link_count":5.0,
        "Solution_readability":18.7,
        "Solution_reading_time":55.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":437.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hello Microsoft Q&amp;A,\nwhen running azure ml pipelines I got the following error:\n&quot; permission denied when access stream. Reason: Some(This request is not authorized to perform this operation using this permission.) &quot;\nWhen I checked the data assets for the pipeline, I got the follwoing error:<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/37d53c8a-2bec-4fa5-b769-3d57010a96f7?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p>The Azure machine learning workspace is inside a vnet and I'm using a service principal for the data store authentification:\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/aa1dd74a-f306-4ddc-a8ef-97b8fc6ae98a?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p>I allready granted the workspace managed identity AND the service principal the reader role for ALL private endpointes. Moreover I checked all other permissions, for example that the workspace managed identiy has the blob storage reader role for the adls gen2 storage.\nDoes this has something to do with these changes:\n &quot;Azure Machine Learning Network Isolation Changes with Compute Instance and Compute Cluster&quot;<\/p>\n<p>Could you please help me.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1681228111046,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1220935\/azure-ml-workspace-unable-to-get-access-token-for",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":13.2,
        "Challenge_reading_time":16.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Workspace - Unable to get access token for ADLS Gen2",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":154,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello @Lukas\nThanks for reaching out to us. I haven't seen the same error, but based on some researches and my personal experience, the error message you're seeing indicates that the user or service principal running the Azure ML pipeline does not have sufficient permissions to access the data assets required by the pipeline. It's possible that the recent changes to Azure Machine Learning Network Isolation could be a factor in this issue.<\/p>\n<p>Here are some steps you can take to further troubleshoot the issue:<\/p>\n<p>Check the credentials being used to access the data assets: Verify that the credentials being used to access the data assets are correct and have sufficient permissions to read the data. You can check this by attempting to manually access the data assets using the same credentials and seeing if you encounter any issues.<\/p>\n<p>Verify RBAC permissions: Make sure that the user or service principal running the pipeline has the necessary RBAC permissions to access the data assets. You can check this by reviewing the access policies and roles associated with the data assets, and making sure that the user or service principal is included in the appropriate role(s) with sufficient access.<\/p>\n<p>Check firewall and network settings: If the data assets are hosted in a private network, make sure that the firewall and network settings allow the pipeline to access the data. You may need to configure virtual network peering or VPN connections to enable access.<\/p>\n<p>Review pipeline configuration: Double-check the pipeline configuration to ensure that the correct data asset paths and permissions are specified. You can also try re-creating the pipeline from scratch to see if that resolves the issue.<\/p>\n<p>Review the Network Isolation changes: Review the recent Azure Machine Learning Network Isolation changes and ensure that they are not impacting your pipeline. You may need to update your pipeline configuration to account for any changes in network isolation.<\/p>\n<p>If none of the above steps resolve the issue, you can contact Microsoft support for further assistance. Please raise a support ticket if you have a support plan, please let us know if you have tried all above items but nothing works, I am happy to enable you a free ticket for this issue. <\/p>\n<p>I hope this helps! Let me know if you have any further questions.<\/p>\n<p>Regards,\nYutong<\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.0,
        "Solution_reading_time":30.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":401.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have tried to read the dataset from datastore. Also tried to create the dataset also.<\/p>\n<p>The code for reading the dataset is below<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.from_config()\ndatastore = Datastore.get(ws, 'qdataset')\n<\/code><\/pre>\n<p>It works fine still now.<\/p>\n<pre><code>from azureml.core.dataset import Dataset\nsix_dataset = Dataset.get_by_name(workspace=ws, name='combined_classifier')\n<\/code><\/pre>\n<p>Also i have tried from <code>azureml.core import Dataset<\/code><\/p>\n<p>It shows the following error:<\/p>\n<p>2021-04-29 11:56:47.284077 | ActivityCompleted: Activity=_dataflow, HowEnded=Failure, Duration=0.0 [ms], Info = {'activity_id': 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'activity_name': '_dataflow', 'activity_type': 'InternalCall', 'app_name': 'dataset', 'source': 'azureml.dataset', 'version': '1.27.0', 'dataprepVersion': '2.14.2', 'subscription': '', 'run_id': '', 'resource_group': '', 'workspace_name': '', 'experiment_id': '', 'location': '', 'completionStatus': 'Failure', 'durationMs': 962.01}, Exception=AttributeError; module 'azureml.dataprep' has no attribute 'api'<\/p>\n<hr \/>\n<p>AttributeError Traceback (most recent call last)  <br \/>\n&lt;ipython-input-34-ac7a8d35da4d&gt; in &lt;module&gt;  <br \/>\n1 from azureml.core.dataset import Dataset  <br \/>\n----&gt; 2 six_dataset = Dataset.get_by_name(workspace=ws, name='combined_classifier')<\/p>\n<p>~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(*args, **kwargs)  <br \/>\n127 with _LoggerFactory.track_activity(logger, func.<strong>name<\/strong>, activity_type, custom_dimensions) as al:  <br \/>\n128 try:  <br \/>\n--&gt; 129 return func(*args, **kwargs)  <br \/>\n130 except Exception as e:  <br \/>\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):<\/p>\n<p>~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in get_by_name(workspace, name, version)  <br \/>\n87 :rtype: typing.Union[azureml.data.TabularDataset, azureml.data.FileDataset]  <br \/>\n88 &quot;&quot;&quot;  <br \/>\n---&gt; 89 dataset = AbstractDataset._get_by_name(workspace, name, version)  <br \/>\n90 AbstractDataset._track_lineage([dataset])  <br \/>\n91 return dataset<\/p>\n<p>~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _get_by_name(workspace, name, version)  <br \/>\n652 if not success:  <br \/>\n653 raise result  <br \/>\n--&gt; 654 dataset = _dto_to_dataset(workspace, result)  <br \/>\n655 warn_deprecated_blocks(dataset)  <br \/>\n656 return dataset<\/p>\n<p>~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_dataset_rest_helper.py in _dto_to_dataset(workspace, dto)  <br \/>\n93 registration=registration)  <br \/>\n94 if dto.dataset_type == _DATASET_TYPE_FILE:  <br \/>\n---&gt; 95 return FileDataset._create(  <br \/>\n96 definition=dataflow_json,  <br \/>\n97 properties=dto.latest.properties,<\/p>\n<p>~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(*args, **kwargs)  <br \/>\n127 with _LoggerFactory.track_activity(logger, func.<strong>name<\/strong>, activity_type, custom_dimensions) as al:  <br \/>\n128 try:  <br \/>\n--&gt; 129 return func(*args, **kwargs)  <br \/>\n130 except Exception as e:  <br \/>\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):<\/p>\n<p>~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _create(cls, definition, properties, registration, telemetry_info)  <br \/>\n555 from azureml.data._partition_format import parse_partition_format  <br \/>\n556  <br \/>\n--&gt; 557 steps = dataset._dataflow._get_steps()  <br \/>\n558 partition_keys = []  <br \/>\n559 for step in steps:<\/p>\n<p>~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(*args, **kwargs)  <br \/>\n127 with _LoggerFactory.track_activity(logger, func.<strong>name<\/strong>, activity_type, custom_dimensions) as al:  <br \/>\n128 try:  <br \/>\n--&gt; 129 return func(*args, **kwargs)  <br \/>\n130 except Exception as e:  <br \/>\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):<\/p>\n<p>~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _dataflow(self)  <br \/>\n215 raise UserErrorException('Dataset definition is missing. Please check how the dataset is created.')  <br \/>\n216 if self._registration and self._registration.workspace:  <br \/>\n--&gt; 217 dataprep().api._datastore_helper._set_auth_type(self._registration.workspace)  <br \/>\n218 if not isinstance(self._definition, dataprep().Dataflow):  <br \/>\n219 try:<\/p>\n<p>AttributeError: module 'azureml.dataprep' has no attribute 'api'<\/p>\n<p>Please give a solution to solve this<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619698599813,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/377203\/error-while-accessing-the-dataset-from-a-datastore",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":62.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":null,
        "Challenge_title":"Error while accessing the dataset from a datastore",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":393,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>It now worked..   <br \/>\nWe need to install azure-ml-api-sdk using this command  <\/p>\n<p>pip install azure-ml-api-sdk  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.4,
        "Solution_reading_time":1.54,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":11,
        "Challenge_body":"> These are reported by @tapadipti (thanks). I'm moving here to discuss and follow: \r\n\r\nI was running experiments by following the docs (https:\/\/dvc.org\/doc\/start\/experiments) and encountered the following issues. Sharing here for any required action.\r\n1. dvc is not installed by `pip install -r requirements.txt`. So, if someone is trying to use a new virtual env, they need to install dvc separately. Would be good to include `dvc` in `requirements.txt`.\r\n2. `dvc pull` gave this error:\r\n   ```\r\n   ERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n   models\/model.h5\r\n   metrics\r\n   Is your cache up to date?\r\n   <https:\/\/error.dvc.org\/missing-files>\r\n   ```\r\n\r\n3. `dvc exp run` lists all the image when running the `extract` stage. Would be good to remove `-v` from `tar -xvzf data\/images.tar.gz --directory data`\r\n4. `If you used dvc repro before` section in the doc is a little unclear. Does `dvc exp run` replace `dvc repro`? If yes, can we state this clearly? Also would be great to change this statement `We use dvc repro to run the pipeline...` to `dvc repro runs the pipeline...`",
        "Challenge_closed_time":1642605.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638880026000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/98",
        "Challenge_link_count":2,
        "Challenge_participation_count":11,
        "Challenge_readability":5.9,
        "Challenge_reading_time":13.98,
        "Challenge_repo_contributor_count":17.0,
        "Challenge_repo_fork_count":11.0,
        "Challenge_repo_issue_count":154.0,
        "Challenge_repo_star_count":15.0,
        "Challenge_repo_watch_count":14.0,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Various issues in `example-dvc-experiments`",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":176,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This seems high priority. We can remove `bug` and change to `p1` after 2. is addressed at least, I think. > 1. dvc is not installed by `pip install -r requirements.txt`. So, if someone is trying to use a new virtual env, they need to install dvc separately. Would be good to include `dvc` in `requirements.txt`.\r\n\r\nThis was a bit intentional to let the users install DVC themselves, and a bit to prevent version conflicts. There are some conditions (like installing DVC to system and venv both with different dependencies) that cause weird behavior. \r\n\r\nWe can go on to this route though, it's a single line of change. Is it better to add `dvc` to the `requirements.txt` @shcheklein?  If this was intentional and we don't want to include `dvc` in `requirements.txt`, then we should add an instruction that the user should install `dvc`. Currently, such an instruction is missing. It is unlikely that many people will reach the experiments page of the tutorial without first having installed `dvc`. But in case they try to work a new venv, it can be a `lil confusing. I remembered why I left `-v` in `tar`, it was taking some time after `extract` to start running and the experiment looks like it's frozen. I've now updated the project not to use `-v` in `tar`, and also updated `model.h5` in the remote. (We had a bug in DVC that was preventing to upload experiments.) Could you now check whether the project works as intended? @tapadipti \r\n\r\nI'll create separate PRs in the docs for content updates. Thank you.  Thanks @iesahin \r\n\r\n`dvc pull` gave this error:\r\n```                                                                                                                    \r\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n\/Users\/tapadiptisitaula\/Documents\/test\/example-dvc-experiments\/models\/model.h5\r\nIs your cache up to date?\r\n<https:\/\/error.dvc.org\/missing-files>\r\n```\r\nSo looks like `metrics` worked but not `model.h5`. And this time, the full file path is displayed.\r\n\r\nRemoving `-v` worked. The files are not listed anymore.\r\n\r\n ```\r\n> ERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n\/Users\/tapadiptisitaula\/Documents\/test\/example-dvc-experiments\/models\/model.h5\r\n```\r\n\r\nInteresting. I double checked yesterday that the script pushing the artifacts has completed successfully. Now, I've checked again and it says:\r\n\r\n```\r\ndvc push\r\nEverything is up to date.\r\n```\r\n\r\nCould you check the MD5 line in `dvc.lock`, corresponding to this line: https:\/\/github.com\/iterative\/example-dvc-experiments\/blob\/main\/dvc.lock#L36\r\n\r\nWhat's the MD5 hash value there, in your installation?\r\n Also, I've checked after cloning the repository: \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/476310\/145449841-16ae8f43-7ce3-4459-a0d3-225d67214ab0.png)\r\n\r\n@tapadipti  The current staging version in https:\/\/github.com\/iterative\/example-dvc-staging resolves all of these issues. I think we can push it to `example-dvc-experiments`.  @iesahin sounds good. The most recent https:\/\/github.com\/iterative\/example-dvc-experiments resolves all these issues. The codification changes are in #97. Closing this. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":7.3,
        "Solution_reading_time":37.61,
        "Solution_score_count":null,
        "Solution_sentence_count":41.0,
        "Solution_word_count":426.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I need to move my Machine Learning Studio workspace to a new region. I am aware that the move function <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-workspace#limitations\">doesn't allow automatically moving to a new region<\/a>, so I'll have to create a new workspace. That's not a big problem, but I still want to keep my job\/experiment history (in my new workspace). How can I do that?    <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1662467585447,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/995833\/moving-azure-machine-learning-studio-jobs-to-a-new",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Moving Azure Machine Learning Studio jobs to a new region",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":70,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=a645d9b8-7c24-4419-8686-bc144a45c4f1\">@David-3633  <\/a>     <\/p>\n<p>Sorry, I just got confimation from product team, this is currently impossible. I am sorry for the inconvenience.     <\/p>\n<p>A near future workaround which could let users at least share some experiment outputs\/inputs like environments, models, datasets cross region, but not the jobs\/metrics\/logs themselves. This feature is in private preview now and will be in public preview soon.    <\/p>\n<p>I hope this information helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":8.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1454593215100,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So my question is this, <\/p>\n\n<p>When creating a notebook in <code>Sagemaker<\/code> <code>AWS<\/code> I need to help the devEngineer keep his secret key in <code>.ssh\/id_rsa<\/code> as the file after every instance reboot becomes empty. \nHe requires a <code>github<\/code> repo to be downloaded and he has to work on the code and then push the updates as needed. \nPlease let me know what details I need to provide to help you help me. \nThanks. <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548941458327,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1548941932587,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54461730",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.8,
        "Challenge_reading_time":6.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to create a permanent login from jupyter notebook to github with ssh_rsa key pair",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1856.0,
        "Challenge_word_count":88,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1541484812983,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":33.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>This is the filesystems for my notebook instance:<\/p>\n<pre><code>sh-4.2$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\ndevtmpfs         16G   76K   16G   1% \/dev\ntmpfs            16G     0   16G   0% \/dev\/shm\n\/dev\/nvme0n1p1   94G   76G   19G  81% \/\n\/dev\/nvme1n1     99G   40G   55G  43% \/home\/ec2-user\/SageMaker\n<\/code><\/pre>\n<p>Note that one pointing to <code>\/home\/ec2-user\/SageMaker<\/code> is the only one which is saved between reboots. Since ssh keys are stored in <code>\/home\/ec2-user\/.ssh<\/code>, they are lost after reboot.<\/p>\n<p>The way I make it work is:<\/p>\n<ol>\n<li>Create the folder <code>\/home\/ec2-user\/SageMaker\/.ssh<\/code><\/li>\n<li>Run <code>ssh-keygen<\/code> and set the location <code>\/home\/ec2-user\/SageMaker\/.ssh\/id_rsa<\/code><\/li>\n<li>Clone repo with <code>GIT_SSH_COMMAND=&quot;ssh -i ~\/SageMaker\/.ssh\/id_rsa -F \/dev\/null&quot; git clone git@domain:account\/repo.git<\/code><\/li>\n<li>cd repo<\/li>\n<li>Set your repo to use the custom location with <code>git config core.sshCommand &quot;ssh -i ~\/SageMaker\/.ssh\/id_rsa -F \/dev\/null&quot;<\/code><\/li>\n<\/ol>\n<p>Based on <a href=\"https:\/\/superuser.com\/a\/912281\">https:\/\/superuser.com\/a\/912281<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.0,
        "Solution_reading_time":14.69,
        "Solution_score_count":3.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":123.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1577353307072,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":491.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>after download a dataset, convert to dataframe and manipulate it.. how can I upload again as new dataset in Azure Machine Learning? <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582561736370,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60380154",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.8,
        "Challenge_reading_time":2.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Upload dataframe as dataset in Azure Machine Learning",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":3736.0,
        "Challenge_word_count":30,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1348126905516,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Barcelona",
        "Poster_reputation_count":327.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>You can follow the steps below: <br>\n1. write dataframe to a local file (e.g. csv, parquet)<\/p>\n\n<pre><code>local_path = 'data\/prepared.csv'\ndf.to_csv(local_path)\n<\/code><\/pre>\n\n<ol start=\"2\">\n<li>upload the local file to a datastore on the cloud<\/li>\n<\/ol>\n\n<pre><code># azureml-core of version 1.0.72 or higher is required\n# azureml-dataprep[pandas] of version 1.1.34 or higher is required\nfrom azureml.core import Workspace, Dataset\n\nsubscription_id = 'xxxxxxxxxxxxxxxxxxxxx'\nresource_group = 'xxxxxx'\nworkspace_name = 'xxxxxxxxxxxxxxxx'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n# get the datastore to upload prepared data\ndatastore = workspace.get_default_datastore()\n# upload the local file from src_dir to the target_path in datastore\ndatastore.upload(src_dir='data', target_path='data')\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li>create a dataset referencing the cloud location<\/li>\n<\/ol>\n\n<pre><code>ds = Dataset.Tabular.from_delimited_files(datastore.path('data\/prepared.csv'))\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.4,
        "Solution_reading_time":13.32,
        "Solution_score_count":7.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1363409191768,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":404.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I would like to update previous runs done with MLFlow, ie. changing\/updating a parameter value to accommodate a change in the implementation. Typical uses cases:<\/p>\n<ul>\n<li>Log runs using a parameter A, and much later, log parameters A and B. It would be useful to update the value of parameter B of previous runs using its default value.<\/li>\n<li>&quot;Specialize&quot; a parameter. Implement a model using a boolean flag as a parameter. Update the implementation to take a string instead. Now we need to update the values of the parameter for the previous runs so that it stays consistent with the new behavior.<\/li>\n<li>Correct a wrong parameter value loggued in the previous runs.<\/li>\n<\/ul>\n<p>It is not always easy to trash the whole experiment as I need to keep the previous runs for statistical purpose. I would like also not to generate new experiments just for a single new parameter, to keep a single database of runs.<\/p>\n<p>What is the best way to do this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1601903047533,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1607788863848,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64209196",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.4,
        "Challenge_reading_time":12.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"How to update a previous run into MLFlow?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":2834.0,
        "Challenge_word_count":171,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1347312347147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1022.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>To add or correct a parameter, metric or artifact of an existing run, pass run_id instead of experiment_id to mlflow.start_run function<\/p>\n<pre><code>with mlflow.start_run(run_id=&quot;your_run_id&quot;) as run:\n    mlflow.log_param(&quot;p1&quot;,&quot;your_corrected_value&quot;)\n    mlflow.log_metric(&quot;m1&quot;,42.0) # your corrected metrics\n    mlflow.log_artifact(&quot;data_sample.html&quot;) # your corrected artifact file\n<\/code><\/pre>\n<p>You can correct, add to, or delete any MLflow run any time after it is complete. Get the run_id either from the UI or by using <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.search_runs\" rel=\"noreferrer\">mlflow.search_runs<\/a>.<\/p>\n<p>Source: <a href=\"https:\/\/towardsdatascience.com\/5-tips-for-mlflow-experiment-tracking-c70ae117b03f\" rel=\"noreferrer\">https:\/\/towardsdatascience.com\/5-tips-for-mlflow-experiment-tracking-c70ae117b03f<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.6,
        "Solution_reading_time":12.37,
        "Solution_score_count":10.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":69.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an experiment (exp) which is published as a web service (exp [Predictive Exp.])  in the azure machine learning studio, the data used by this experiment was pushed by R using AzureML package<\/p>\n\n<pre><code>library(AzureML)\n\nws &lt;- workspace(\n  id = 'xxxxxxxxx',\n  auth = 'xxxxxxxxxxx'\n)\n\nupload.dataset(data_for_azure, ws, \"data_for_azure\")\n<\/code><\/pre>\n\n<p>The above thing worked, but lets say I want to update the dataset(same schema just added more rows)<\/p>\n\n<p>I tired this but this does not work:<\/p>\n\n<pre><code>delete.datasets(ws, \"data_for_azure\")\n\nrefresh(ws, what = c(\"everything\", \"data_for_azure\", \"exp\", \"exp [Predictive Exp.]\")) \n<\/code><\/pre>\n\n<p>I get the error stating the following:<\/p>\n\n<pre><code>Error: AzureML returns error code:\nHTTP status code : 409\nUnable to delete dataset due to lingering dependants\n<\/code><\/pre>\n\n<p>I went through the documentation, and I know that a simple refresh is not possible(same name), the only alternative I see is to delete the web service and perform everything again<\/p>\n\n<p>Any solution will be greatly helped!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1458545373340,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1458567175056,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36125274",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":14.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Refresh the dataset in Azure machine learning",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1196.0,
        "Challenge_word_count":151,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1406266059940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Link\u00f6ping, Sweden",
        "Poster_reputation_count":1677.0,
        "Poster_view_count":221.0,
        "Solution_body":"<p>From the R doc.<\/p>\n\n<blockquote>\n  <p>The AzureML API does not support uploads for <em>replacing<\/em> datasets with\n  new data by re-using a name. If you need to do this, first delete the\n  dataset from the AzureML Studio interface, then upload a new version.<\/p>\n<\/blockquote>\n\n<p>Now, I think this is particular for the R sdk, as the Python SDK, and the AzureML Studio UI lets you upload a new dataset. Will check in with the R team about this.<\/p>\n\n<p>I would recommend uploading it as a new dataset with a new name, and then replacing the dataset in your experiment with this new dataset. Sorry this seem's round about, but I think is the easier option.<\/p>\n\n<p>Unless you want to upload the new version using the AzureML Studio, in which case go to +NEW, Dataset, select your file and select the checkbox that says this is an existing dataset. The filename should be the same. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":10.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":154.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently pursuing 'Microsoft Azure AI Fundamentals' course and I am currently stuck on unit 6 of 8 (titled-Explore automated machine learning in azure ML) as I am unable to find the administrative access into azure ML to create the resource and I am getting redirected to payment page for subscription. Please help me to get into Azure ML using administrative access. <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684995150610,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1291402\/stuck-on-azure-ai-fundamentals-course",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":5.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Stuck on Azure AI Fundamentals course",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":69,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=eb5a735f-18f5-4565-8bc6-44e2094e0255\">@Soumyadeep Podder  <\/a>Thanks for the question. If you\u2019re being redirected to the payment page for subscription, it could mean that you don\u2019t have an active Azure subscription. You can try signing up for a free trial of Azure.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":3.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":1791.6976897222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I created Compute instance in Azure Machine Learning in the Edge browser right after logging in. When it was started, I clicked on the Jupyter link. <\/p>\n\n<p>I got the following authentication error: \"User live.com#myname@outlook.com does not have access to compute instance vm-aml-lab4.\nOnly the creator can access a compute instance.\"<\/p>\n\n<p>Is there a way to avoid this error?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587659313437,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1591885030160,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61392212",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":5.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Authentication Error: Compute instance for Azure Machine Learning",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":953.0,
        "Challenge_word_count":66,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330373362200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kennett Square, PA",
        "Poster_reputation_count":445.0,
        "Poster_view_count":104.0,
        "Solution_body":"<p>Currently the AML compute instance only allows the creator to access the CI.It's known bug, once it's fixed we will update you. We think it is related to MSA accounts.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1598335141843,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":2.13,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1475181309096,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Brazil",
        "Answerer_reputation_count":4242.0,
        "Answerer_view_count":421.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am creating my own Docker image so that I can use my own models in AWS SageMaker. I sucessfully created a Docker image using command line inside the Jupyter Notebook in SageMaker ml.t2.medium instance using a customized Dockerfile:<\/p>\n\n<pre><code>REPOSITORY            TAG                 IMAGE ID            CREATED             SIZE\nsklearn               latest              01234212345        6 minutes ago       1.23GB\n<\/code><\/pre>\n\n<p>But when I run in Jupyter:<\/p>\n\n<pre><code>! aws ecr create-repository --repository-name sklearn\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>An error occurred (AccessDeniedException) when calling the CreateRepository operation: User: arn:aws:sts::1234567:assumed-role\/AmazonSageMaker-ExecutionRole-12345\/SageMaker is not authorized to perform: ecr:CreateRepository on resource: *\n<\/code><\/pre>\n\n<p>I already set up SageMaker, EC2, EC2ContainerService permissions and the following policy for EC2Container but I still get the same error.<\/p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"sagemaker:*\",\n        \"ec2:*\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n<\/code><\/pre>\n\n<p>Any idea on how I can solve this issue?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1528052776997,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1528055205112,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50669991",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":13.9,
        "Challenge_reading_time":15.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS SageMaker is not authorized to perform: ecr:CreateRepository on resource: *",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":4509.0,
        "Challenge_word_count":145,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1475181309096,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brazil",
        "Poster_reputation_count":4242.0,
        "Poster_view_count":421.0,
        "Solution_body":"<p>I solved the problem. We must set a permission at SageMaker Execution Role as following:<\/p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"ecr:*\"            ],\n        \"Resource\": \"*\"\n    }\n]}\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":2.88,
        "Solution_score_count":5.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1336984204220,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wien, \u00d6sterreich",
        "Answerer_reputation_count":166.0,
        "Answerer_view_count":31.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am using sacred package in python, this allows to keep track of computational experiments i'm running. sacred allows to add observer (<code>mongodb<\/code>) which stores all sorts of information regarding the experiment (<code>configuration<\/code>, <code>source files<\/code> etc).\n<code>sacred<\/code> allows to add artifacts to the db bt using <code>sacred.Experiment.add_artifact(PATH_TO_FILE).<\/code><\/p>\n\n<p>This command essentially adds the file to the DB.<\/p>\n\n<p>I'm using MongoDB compass, I can access the experiment information and see that an artifact has been added. it contains two fields:\n'<code>name<\/code>' and '<code>file_id<\/code>' which contains an <code>ObjectId<\/code>. (see image)<\/p>\n\n<p>I am attempting to access the stored file itself. i have noticed that under my db there is an additional <code>sub-db<\/code> called <code>fs.files<\/code> in it i can filter to find my <code>ObjectId<\/code> but it does not seem to allow me to access to content of the file itself.<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/SBg8m.png\" alt=\"object id under .files\"><\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/B7ymG.png\" alt=\"file_id under artifact\/object\"><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1513848655213,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1525602466710,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47921875",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.0,
        "Challenge_reading_time":15.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Accessing files in Mongodb",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":2457.0,
        "Challenge_word_count":149,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1468845236196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>MongoDB file storage is handled by \"GridFS\" which basically splits up files in chunks and stores them in a collection (fs.files).<\/p>\n\n<p>Tutorial to access: <a href=\"http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html\" rel=\"nofollow noreferrer\">http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.5,
        "Solution_reading_time":4.32,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":1554060427012,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2433.0,
        "Answerer_view_count":228.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have <code>parquet<\/code> files that are generated via <code>spark<\/code> and the filename (key) in <code>s3<\/code> will always change post ETL job.  This is the code I use to read the <code>parquet<\/code> files via <code>boto3<\/code> in <code>sagemaker<\/code>.  Looking for a way to dynamically read the <code>S3<\/code> filename (key) since hard-coding the key will fail the read since it changes every time.  How can this be achieved?  Thanks.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>filename = \"datasets\/randomnumbergenerator.parquet\"\nbucketName = \"bucket-name\"\n\nbuffer = io.BytesIO()\nclient = boto3.resource(\"s3\")\nobj = client.Object(bucketName, filename)\nobj.download_fileobj(buffer)\ndf = pd.read_parquet(buffer)\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583849751003,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1583863687520,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60619460",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.9,
        "Challenge_reading_time":10.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Dynamically read changing filename key",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":280.0,
        "Challenge_word_count":87,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554060427012,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2433.0,
        "Poster_view_count":228.0,
        "Solution_body":"<p>This solution is working for me.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\nimport pandas as pd\nimport io\nimport pyarrow\nimport fastparquet\n\ndef dynamically_read_filename_key(bucket, prefix='', suffix=''):\n    s3 = boto3\\\n    .client(\"s3\",\\\n            region_name=os.environ['AWS_DEFAULT_REGION'],\\\n            aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\\\n            aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'])\n    kwargs = {'Bucket': bucket}\n    if isinstance(prefix, str):\n        kwargs['Prefix'] = prefix\n    resp = s3\\\n    .list_objects_v2(**kwargs)\n    for obj in resp['Contents']:\n        key = obj['Key']\n    if key.startswith(prefix) and key.endswith(suffix):\n        return key\n\nfilename = \"\".join(i for i in dynamically_read_filename_key\\\n                   (bucket=\"my-bucket\",\\\n                    prefix=\"datasets\/\",\\\n                    suffix=\".parquet\"))\n\nbucket = \"my-bucket\"\n\ndef parquet_read_filename_key(bucket, filename):\n    client = boto3\\\n    .resource(\"s3\",\\\n            region_name=os.environ['AWS_DEFAULT_REGION'],\\\n            aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\\\n            aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'])\n    buffer = io.BytesIO()\n    obj = client.Object(bucket, filename)\n    obj.download_fileobj(buffer)\n    df = pd.read_parquet(buffer)\n    return df\n\ndf = parquet_read_filename_key(bucket, filename)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.9,
        "Solution_reading_time":16.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1355002392776,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bolzano, Italia",
        "Answerer_reputation_count":530.0,
        "Answerer_view_count":91.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After running <code>dvc push data.csv<\/code> (to ssh-remote), when i try to dvc-pull the same file on another machine from the same remote, it won't get pulled. Below are the logs and the error:<\/p>\n<pre><code>2021-01-21 22:17:26,643 DEBUG: checking if 'data.csv'('HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)') has changed.\n2021-01-21 22:17:26,643 DEBUG: 'data.csv' doesn't exist.\n2021-01-21 22:17:26,644 WARNING: Cache 'HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)' not found. File 'data.csv' won't be created.\n2021-01-21 22:17:26,644 DEBUG: cache '\/usr\/src\/bohr\/.dvc\/cache\/27\/9936268f488e1e613f81a537f29055' expected 'HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)' actual 'None'\n...\n2021-01-21 22:17:26,660 ERROR: failed to pull data from the cloud - Checkout failed for following targets:\ndata.csv\n<\/code><\/pre>\n<p>However, the file is present on the remote:<\/p>\n<pre><code>$ ls -la ~\/.dvcstorage\/bohr\/27\/9936268f488e1e613f81a537f29055\n-rw-rw-r-- 1 hbabii hbabii 1458311 Jan 22 00:19 \/home\/hbabii\/.dvcstorage\/bohr\/27\/9936268f488e1e613f81a537f29055\n<\/code><\/pre>\n<p>I double-checked that I am pulling from and pushing to the same remote. I am using DVC v1.11.11.<\/p>\n<p>Could you please give me any hints on what could be wrong?<\/p>\n<p>Cheers, Hlib<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":4,
        "Challenge_created_time":1611327752360,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65847574",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":5.5,
        "Challenge_reading_time":19.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Failed to pull existing files from SSH DVC Remote",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1715.0,
        "Challenge_word_count":159,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1355002392776,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bolzano, Italia",
        "Poster_reputation_count":530.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p>In the end, the problem was that I indeed was pulling from the wrong remote (I had multiple remotes, their configuration was tricky, and local configurations differed on different machines).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.2,
        "Solution_reading_time":2.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":30.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nThere are several areas in the code where we have an explicit check for the `neptune.amazonaws.com` DNS suffix; this is used to determine if we need to use Neptune-specific configuration options and request URI elements. \r\n\r\nHowever, these checks misidentify endpoints of Neptune clusters in AWS CN regions, which use the `neptune.<region>.amazonaws.com.cn` DNS suffix instead, as non-AWS endpoints. As a result, required config options such as `auth_mode` and `region` are not set correctly.\r\n\r\nAll of the following checks need to be changed to \"amazonaws.com\":\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/magics\/graph_magic.py#L160\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/neptune\/client.py#L129\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/configuration\/generate_config.py#L54\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/68e888def530be70e08b5250c8146292fb49cfa1\/src\/graph_notebook\/configuration\/get_config.py#L14",
        "Challenge_closed_time":1635986.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635879966000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/222",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":16.11,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Configuration options not being set correctly when using CN region Neptune endpoint as host",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":103,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Resolved as of release 3.0.8",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":0.35,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":5.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi, I'm wondering if I can register azure cosmos db as a datastore in azure machine learning?     <br \/>\nFrom your documentation, it seems not <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore%28class%29?view=azure-ml-py\">https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore%28class%29?view=azure-ml-py<\/a>    <\/p>\n<ul>\n<li> Do you have a plan to implement the feature in near future?     <\/li>\n<li> Any recommended alternative solutions for now?     <\/li>\n<\/ul>\n<p>Thanks.    <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597331768270,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/66297\/azure-cosmos-db-as-a-datastore-in-ml",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.7,
        "Challenge_reading_time":7.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"azure cosmos db as a datastore in ml",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. Currently, Cosmos DB isn't a supported datasource when using <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#supported-data-storage-service-types\">Azure ML datastores<\/a>. However, the product team are aware of this request and will provide updates accordingly. An alternative for now will be to use <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/import-from-cosmos-db\">Azure ML Studio (Classic)<\/a> which supports Cosmos DB as data source. You can also try a heuristic approach via <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/execute-python-script\">Execute Python Script module<\/a> in Designer to import data using <a href=\"https:\/\/stackoverflow.com\/questions\/44249604\/how-to-read-data-from-azures-cosmosdb-in-python\">python<\/a>. Hope this helps.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":16.1,
        "Solution_reading_time":12.3,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":77.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Unit 4 of 7  <br \/>\nExercise - Back up an Azure virtual machine  <br \/>\nCreate a backup for Azure virtual machines  <\/p>\n<p>I am unable to run the following command in cloud shell to set up the environment:  <br \/>\nRGROUP=$(az group create --name vmbackups --location westus2 --output tsv --query name)  <\/p>\n<p>Following error pop up:  <br \/>\nERROR: (AuthorizationFailed) The client 'live.com#...... does not have authorization to perform action 'Microsoft.Resources\/subscriptions\/resourcegroups\/write' over scope '\/subscriptions\/.......\/resourcegroups\/vmbackups' or the scope is invalid. If access was recently granted, please refresh your credentials.  <br \/>\nCode: AuthorizationFailed  <br \/>\nMessage: The client 'live.com#l...... with object id '.......' does not have authorization to perform action 'Microsoft.Resources\/subscriptions\/resourcegroups\/write' over scope '\/subscriptions\/.....\/resourcegroups\/vmbackups' or the scope is invalid. If access was recently granted, please refresh your credentials.  <\/p>\n<p>When I do refresh, sign out or sign in do not helps. Anybody has any idea what to do?  <br \/>\nThank you  <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651917005187,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/840311\/code-authorizationfailed",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":14.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Code: AuthorizationFailed",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":146,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=854a8d3c-feda-48d5-ba7c-744d587335c9\">@Krisztian  <\/a>     <\/p>\n<p>Welcome to Microsoft Q&amp;A community.     <\/p>\n<p>Have you tried to do this first?    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/199884-image.png?platform=QnA\" alt=\"199884-image.png\" \/>    <\/p>\n<p>Cheers,    <\/p>\n<p>Please &quot;Accept the answer&quot; if the information helped you. This will help us and others in the community as well.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":6.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":9614.8936591666,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I uploaded a model with<\/p>\n<pre><code>gcloud beta ai models upload --artifact-uri\n<\/code><\/pre>\n<p>And in the docker I access <code>AIP_STORAGE_URI<\/code>.\nI see that <code>AIP_STORAGE_URI<\/code> is another Google Storage location so I try to download the files using <code>storage.Client()<\/code> but then it says that I don't have access:<\/p>\n<pre><code>google.api_core.exceptions.Forbidden: 403 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/caip-tenant-***-***-*-*-***?projection=noAcl&amp;prettyPrint=false: custom-online-prediction@**.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket\n<\/code><\/pre>\n<p>I am running this endpoint with the default service account.<\/p>\n<p><a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#artifacts\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#artifacts<\/a><\/p>\n<p>According to the above link:\n<code>The service account that your container uses by default has permission to read from this URI. <\/code><\/p>\n<p>What am I doing wrong?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626854597743,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1626858370043,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68465990",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":15.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I access AIP_STORAGE_URI in Vertex AI?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":742.0,
        "Challenge_word_count":111,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1466977784156,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":117.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>The reason behind the error being, the default service account that Vertex AI uses has the \u201c<a href=\"https:\/\/cloud.google.com\/storage\/docs\/access-control\/iam-roles#standard-roles\" rel=\"nofollow noreferrer\">Storage Object Viewer<\/a>\u201d role which excludes the <code>storage.buckets.get<\/code> permission. At the same time, the <code>storage.Client()<\/code> part of the code makes a <code>storage.buckets.get<\/code> request to the Vertex AI managed bucket for which the default service account does not have permission to.<\/p>\n<p>To resolve the issue, I would suggest you to follow the below steps -<\/p>\n<ol>\n<li><p>Make changes in the custom code to access the bucket with the model artifacts in your project instead of using the environment variable <code>AIP_STORAGE_URI<\/code> which points to the model location in the Vertex AI managed bucket.<\/p>\n<\/li>\n<li><p>Create your own service account and grant the service account with all the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/custom-service-account\" rel=\"nofollow noreferrer\">permissions<\/a> needed by the custom code. For this specific error, a role with the <code>storage.buckets.get<\/code> permission, eg. <a href=\"https:\/\/cloud.google.com\/storage\/docs\/access-control\/iam-roles#standard-roles\" rel=\"nofollow noreferrer\">Storage Admin<\/a> (&quot;roles\/storage.admin&quot;) has to be granted to the service account.<\/p>\n<\/li>\n<li><p>Provide the newly created service account in the &quot;Service Account&quot; field when deploying the model.<\/p>\n<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1661471987216,
        "Solution_link_count":3.0,
        "Solution_readability":11.7,
        "Solution_reading_time":19.89,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":176.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1298484007147,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, United States",
        "Answerer_reputation_count":9271.0,
        "Answerer_view_count":1819.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Cannot read AWS open data datasets into Sagemaker. Error is<\/p>\n\n<pre><code>download failed: s3:\/\/fast-ai-imageclas\/cifar100.tgz to ..\/..\/..\/tmp\/fastai-images\/cifar100.tgz An error occurred (AccessDenied) when calling the GetObject operation: Access Denied\n<\/code><\/pre>\n\n<p>code\n<a href=\"https:\/\/i.stack.imgur.com\/2b73H.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2b73H.png\" alt=\"sagemaker notebook s3 download access denied\"><\/a><\/p>\n\n<p>The user has the s3:getObjects * permission<\/p>\n\n<p>The user's permissions are the full s3 read policy and the full Sagemaker policies. The policies are<\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:Get*\",\n                \"s3:List*\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"application-autoscaling:DeleteScalingPolicy\",\n                \"application-autoscaling:DeleteScheduledAction\",\n                \"application-autoscaling:DeregisterScalableTarget\",\n                \"application-autoscaling:DescribeScalableTargets\",\n                \"application-autoscaling:DescribeScalingActivities\",\n                \"application-autoscaling:DescribeScalingPolicies\",\n                \"application-autoscaling:DescribeScheduledActions\",\n                \"application-autoscaling:PutScalingPolicy\",\n                \"application-autoscaling:PutScheduledAction\",\n                \"application-autoscaling:RegisterScalableTarget\",\n                \"aws-marketplace:ViewSubscriptions\",\n                \"cloudwatch:DeleteAlarms\",\n                \"cloudwatch:DescribeAlarms\",\n                \"cloudwatch:GetMetricData\",\n                \"cloudwatch:GetMetricStatistics\",\n                \"cloudwatch:ListMetrics\",\n                \"cloudwatch:PutMetricAlarm\",\n                \"cloudwatch:PutMetricData\",\n                \"codecommit:BatchGetRepositories\",\n                \"codecommit:CreateRepository\",\n                \"codecommit:GetRepository\",\n                \"codecommit:ListBranches\",\n                \"codecommit:ListRepositories\",\n                \"cognito-idp:AdminAddUserToGroup\",\n                \"cognito-idp:AdminCreateUser\",\n                \"cognito-idp:AdminDeleteUser\",\n                \"cognito-idp:AdminDisableUser\",\n                \"cognito-idp:AdminEnableUser\",\n                \"cognito-idp:AdminRemoveUserFromGroup\",\n                \"cognito-idp:CreateGroup\",\n                \"cognito-idp:CreateUserPool\",\n                \"cognito-idp:CreateUserPoolClient\",\n                \"cognito-idp:CreateUserPoolDomain\",\n                \"cognito-idp:DescribeUserPool\",\n                \"cognito-idp:DescribeUserPoolClient\",\n                \"cognito-idp:ListGroups\",\n                \"cognito-idp:ListIdentityProviders\",\n                \"cognito-idp:ListUserPoolClients\",\n                \"cognito-idp:ListUserPools\",\n                \"cognito-idp:ListUsers\",\n                \"cognito-idp:ListUsersInGroup\",\n                \"cognito-idp:UpdateUserPool\",\n                \"cognito-idp:UpdateUserPoolClient\",\n                \"ec2:CreateNetworkInterface\",\n                \"ec2:CreateNetworkInterfacePermission\",\n                \"ec2:CreateVpcEndpoint\",\n                \"ec2:DeleteNetworkInterface\",\n                \"ec2:DeleteNetworkInterfacePermission\",\n                \"ec2:DescribeDhcpOptions\",\n                \"ec2:DescribeNetworkInterfaces\",\n                \"ec2:DescribeRouteTables\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeVpcEndpoints\",\n                \"ec2:DescribeVpcs\",\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:BatchGetImage\",\n                \"ecr:CreateRepository\",\n                \"ecr:GetAuthorizationToken\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:Describe*\",\n                \"elastic-inference:Connect\",\n                \"glue:CreateJob\",\n                \"glue:DeleteJob\",\n                \"glue:GetJob\",\n                \"glue:GetJobRun\",\n                \"glue:GetJobRuns\",\n                \"glue:GetJobs\",\n                \"glue:ResetJobBookmark\",\n                \"glue:StartJobRun\",\n                \"glue:UpdateJob\",\n                \"groundtruthlabeling:*\",\n                \"iam:ListRoles\",\n                \"kms:DescribeKey\",\n                \"kms:ListAliases\",\n                \"lambda:ListFunctions\",\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:DescribeLogStreams\",\n                \"logs:GetLogEvents\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ecr:SetRepositoryPolicy\",\n                \"ecr:CompleteLayerUpload\",\n                \"ecr:BatchDeleteImage\",\n                \"ecr:UploadLayerPart\",\n                \"ecr:DeleteRepositoryPolicy\",\n                \"ecr:InitiateLayerUpload\",\n                \"ecr:DeleteRepository\",\n                \"ecr:PutImage\"\n            ],\n            \"Resource\": \"arn:aws:ecr:*:*:repository\/*sagemaker*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"codecommit:GitPull\",\n                \"codecommit:GitPush\"\n            ],\n            \"Resource\": [\n                \"arn:aws:codecommit:*:*:*sagemaker*\",\n                \"arn:aws:codecommit:*:*:*SageMaker*\",\n                \"arn:aws:codecommit:*:*:*Sagemaker*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:CreateSecret\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:ListSecrets\",\n                \"secretsmanager:TagResource\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetSecretValue\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"secretsmanager:ResourceTag\/SageMaker\": \"true\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"robomaker:CreateSimulationApplication\",\n                \"robomaker:DescribeSimulationApplication\",\n                \"robomaker:DeleteSimulationApplication\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"robomaker:CreateSimulationJob\",\n                \"robomaker:DescribeSimulationJob\",\n                \"robomaker:CancelSimulationJob\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::*SageMaker*\",\n                \"arn:aws:s3:::*Sagemaker*\",\n                \"arn:aws:s3:::*sagemaker*\",\n                \"arn:aws:s3:::*aws-glue*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:CreateBucket\",\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\",\n                \"s3:ListAllMyBuckets\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEqualsIgnoreCase\": {\n                    \"s3:ExistingObjectTag\/SageMaker\": \"true\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"lambda:InvokeFunction\"\n            ],\n            \"Resource\": [\n                \"arn:aws:lambda:*:*:function:*SageMaker*\",\n                \"arn:aws:lambda:*:*:function:*sagemaker*\",\n                \"arn:aws:lambda:*:*:function:*Sagemaker*\",\n                \"arn:aws:lambda:*:*:function:*LabelingFunction*\"\n            ]\n        },\n        {\n            \"Action\": \"iam:CreateServiceLinkedRole\",\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:iam::*:role\/aws-service-role\/sagemaker.application-autoscaling.amazonaws.com\/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"iam:AWSServiceName\": \"sagemaker.application-autoscaling.amazonaws.com\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:CreateServiceLinkedRole\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"iam:AWSServiceName\": \"robomaker.amazonaws.com\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:PassRole\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"iam:PassedToService\": [\n                        \"sagemaker.amazonaws.com\",\n                        \"glue.amazonaws.com\",\n                        \"robomaker.amazonaws.com\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>The Sagemaker instance is in us-east-1 same as the dataset.<\/p>\n\n<p>The dataset is <a href=\"https:\/\/registry.opendata.aws\/fast-ai-imageclas\/\" rel=\"nofollow noreferrer\">https:\/\/registry.opendata.aws\/fast-ai-imageclas\/<\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1549841280153,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54622191",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":40.9,
        "Challenge_reading_time":86.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Access Denied read open data into Sagemaker",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1192.0,
        "Challenge_word_count":309,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1298484007147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, NY, United States",
        "Poster_reputation_count":9271.0,
        "Poster_view_count":1819.0,
        "Solution_body":"<p>thanks to Matthew I looked into the permissions of the notebook itself, not just the user using Sagemaker.<\/p>\n\n<p>The policies on the notebook look like this and I can download from the aws open data datasets!<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/s0jwV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s0jwV.png\" alt=\"notebook settings\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/OqEHi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OqEHi.png\" alt=\"notebook permissions\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.1,
        "Solution_reading_time":7.05,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"My customer asks that:\n\n\n* Container images must be registered and deployments tracked\n\n* Containers must be registered within a private customer-owned registry prior to deployment\n\n* Only registered containers are to be deployed.\n\t\n* Part of the registration process must include verification the containers have comes from a trusted source and that they have been scanned and found to be free of malware and vulnerabilities.\n\n* An inventory of all deployed containers must be maintained at all times. \n\n\n* The inventory must include:\n\t Software installed within the container\n\tversion of all software and patch level\n\t. Where the container has been deployed\n\t. Owner of the container\n\nDo we do any of these? Please provide documentation on AWS\/SageMaker vs custom container provider's responsibilities.",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607710961000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668553545196,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUdJqWN_WJQeuYrkZMikkibQ\/custom-amazon-sagemaker-container-registration-and-deployment-tracking",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":10.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Custom Amazon SageMaker container registration and deployment tracking",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":123,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Amazon Elastic Container Registry (Amazon ECR) enables customers to store images, secure their images using AWS Identity and Access Management (IAM), and scan their containers for vulnerabilities. Open Policy Agent (OPA) is an open-source project focused on codifying policy such as the approved image registries. OPA is integrated with Kubernetes via Gatekeeper, an admission controller that checks if the image is from an approved registry prior to allowing it to be deployed on the cluster. For more details see: https:\/\/aws.amazon.com\/blogs\/containers\/designing-a-secure-container-image-registry",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1608576929504,
        "Solution_link_count":1.0,
        "Solution_readability":14.6,
        "Solution_reading_time":7.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1507661294190,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":98.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We would like to enforce specific security groups to be set on the SageMaker training jobs (XGBoost in script mode).\nHowever, distributed training, in this case, won\u2019t work out of the box, since the containers need to communicate with each other. What are the minimum inbound\/outbound rules (ports) that we need to specify for training jobs so that they can communicate?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662733216593,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73663585",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":5.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Add Security groups in Amazon SageMaker for distributed training jobs",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":19.0,
        "Challenge_word_count":70,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1662621266503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":48.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>setting up training in VPC including specifying security groups is documented here:\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-vpc.html#train-vpc-groups\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-vpc.html#train-vpc-groups<\/a><\/p>\n<p>Normally you would allow all communication between the training nodes. To do this you specify the security group source and destination to the name of the security group itself, and allow all IPv4 traffic. If you want to figure out what ports are used, you could: 1\/ define the permissive security group. 2\/ Turn on VPC flow logs 3\/ run training. 4\/ examine VPC Flow logs 5\/ update the security group only to the required ports.<\/p>\n<p>I must say restricting communication between the training nodes might be an extreme, so I would challenge the customer why it's really needed, as all nodes carry the same job, have the same IAM role, and are transiate by nature.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.1,
        "Solution_reading_time":12.21,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":132.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am a network admin and know very little about Machine Learning<\/p>\n<p>We setup a private ML studio workspace for our user who is assigned the Owner role to the resource group the workspace uses.<\/p>\n<p>First we had issue with creating compute and user had to be assigned some network join permissions at the Vnet, as he could not even get a list of subnets. I could not find any ML role at the Vnet level that needs to be assigned.<\/p>\n<p>After assigning those permissions, user can create compute but not able to click terminal as all the applications are greyed out. That may be because compute is not assigned to me.<\/p>\n<p>I created a test compute myself. and even added a dns entry pointing to compute instance as we have custom dns, terminal still does not open.<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/c03305a2-59dc-4c88-9454-fe171d86a1c5?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p>What all permissions that we need to assign user and at what resource so they can use ML studio without our intervention.<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677773626496,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1185943\/ml-studio-roles-permissions-issue",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":13.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"ML studio Roles\/Permissions issue",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":169,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=b845915b-4001-0003-0000-000000000000\">@R.T  <\/a><\/p>\n<p>Thanks for reaching out to us, I understand you are building your compute within your VNET for your team. There are two roles related to VNET you may want to add, could you please take a look and have a try? Please let me know if below roles are still not working and we can discuss the next step.<\/p>\n<p>To deploy your compute resources inside a VNet, you need to explicitly have permissions for the following actions:<\/p>\n<ul>\n<li> <code>Microsoft.Network\/virtualNetworks\/*\/read<\/code> on the VNet resources.<\/li>\n<li> <code>Microsoft.Network\/virtualNetworks\/subnets\/join\/action<\/code> on the subnet resource.   For more information on Azure RBAC with networking, see the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/role-based-access-control\/built-in-roles#networking\">Networking built-in roles<\/a>.<\/li>\n<\/ul>\n<p>Since you ask all relative permissions, I want to share some common Common scenarios for roles\/permissions management like below screenshot-<\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-assign-roles?tabs=labeler#common-scenarios\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-assign-roles?tabs=labeler#common-scenarios<\/a><\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/dbf3883e-5d1b-4294-88b1-f31f79efd7a5?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p>And also some examples of custom role for different scenarios like below screenshot- <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-assign-roles?tabs=labeler#example-custom-roles\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-assign-roles?tabs=labeler#example-custom-roles<\/a><\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/fd4691a6-36f5-4eb8-ba40-f58460a1efd4?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p>Please let me know if you need further help.<\/p>\n<p>Regards,<\/p>\n<p>Yutong<\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":14.9,
        "Solution_reading_time":28.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":184.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"",
        "Challenge_closed_time":1638706.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638704752000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/se4ai2122-cs-uniba\/CT-COVID\/issues\/30",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":5.2,
        "Challenge_reading_time":0.66,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":66.0,
        "Challenge_repo_star_count":1.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":null,
        "Challenge_title":"Missing params field for evaluate stage in dvc.yaml",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":8,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1444418094503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":762.0,
        "Answerer_view_count":26.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a folder with predicted masks on AWS Sagemaker. ( It has 4 folders inside it and lot of files inside those folders. ) I want to download the entire folder to my laptop. \nThis might sound so simple and easy, but I could not find a way to do it. Appreciate any help.<\/p>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1551375381970,
        "Challenge_favorite_count":8.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54931270",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.9,
        "Challenge_reading_time":4.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":17.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Download an entire folder from AWS sagemaker to laptop",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":13015.0,
        "Challenge_word_count":62,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1366530725212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"California, USA",
        "Poster_reputation_count":440.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>You can do that by opening a terminal on sagemaker. Navigate to the path where your folder is. Run the command to zip it<\/p>\n\n<pre><code>zip -r -X archive_name.zip folder_to_compress\n<\/code><\/pre>\n\n<p>You will find the zipped folder. You can then select it and download it.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.5,
        "Solution_reading_time":3.45,
        "Solution_score_count":35.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1638293279416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following the example <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/community\/matching_engine\/matching_engine_for_indexing.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a> as per GCP <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/matching-engine\/using-matching-engine#example_notebook\" rel=\"nofollow noreferrer\">docs<\/a> to test Vertex Matching Engine. I have deployed an index but while trying to query the index I am getting <code>_InactiveRpcError<\/code>. The VPC network is in <code>us-west2<\/code> with private service access enabled and the Index is deployed in <code>us-central1<\/code>. My VPC network contains the <a href=\"https:\/\/cloud.google.com\/vpc\/docs\/firewalls#more_rules_default_vpc\" rel=\"nofollow noreferrer\">pre-populated firewall rules<\/a>.<\/p>\n<p>Index<\/p>\n<pre><code>createTime: '2021-11-23T15:25:53.928606Z'\ndeployedIndexes:\n- deployedIndexId: brute_force_glove_deployed_v3\n  indexEndpoint: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexEndpoints\/XXXXXXXXXXXX\ndescription: testing python script for creating index\ndisplayName: glove_100_brute_force_20211123152551\netag: AMEw9yOVPWBOTpbAvJLllqxWMi2YurEV_sad2n13QvbIlqjOdMyiq_j20gG1ldhdZNTL\nmetadata:\n  config:\n    algorithmConfig:\n      bruteForceConfig: {}\n    dimensions: 100\n    distanceMeasureType: DOT_PRODUCT_DISTANCE\nmetadataSchemaUri: gs:\/\/google-cloud-aiplatform\/schema\/matchingengine\/metadata\/nearest_neighbor_search_1.0.0.yaml\nname: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexes\/XXXXXXXXXXXX\nupdateTime: '2021-11-23T16:04:17.993730Z'\n<\/code><\/pre>\n<p>Index-Endpoint<\/p>\n<pre><code>createTime: '2021-11-24T10:59:51.975949Z'\ndeployedIndexes:\n- automaticResources:\n    maxReplicaCount: 1\n    minReplicaCount: 1\n  createTime: '2021-11-30T15:16:12.323028Z'\n  deploymentGroup: default\n  displayName: brute_force_glove_deployed_v3\n  enableAccessLogging: true\n  id: brute_force_glove_deployed_v3\n  index: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexes\/XXXXXXXXXXXX\n  indexSyncTime: '2021-11-30T16:37:35.597200Z'\n  privateEndpoints:\n    matchGrpcAddress: 10.242.4.5\ndisplayName: index_endpoint_for_demo\netag: AMEw9yO6cuDfgpBhGVw7-NKnlS1vdFI5nnOtqVgW1ddMP-CMXM7NfGWVpqRpMRPsNCwc\nname: projects\/XXXXXXXXXXXX\/locations\/us-central1\/indexEndpoints\/XXXXXXXXXXXX\nnetwork: projects\/XXXXXXXXXXXX\/global\/networks\/XXXXXXXXXXXX\nupdateTime: '2021-11-24T10:59:53.271100Z'\n<\/code><\/pre>\n<p>Code<\/p>\n<pre><code>\nimport grpc\n\n# import the generated classes\nimport match_service_pb2\nimport match_service_pb2_grpc\n\nDEPLOYED_INDEX_SERVER_IP = '10.242.0.5'\nDEPLOYED_INDEX_ID = 'brute_force_glove_deployed_v3'\n\nquery = [-0.11333, 0.48402, 0.090771, -0.22439, 0.034206, -0.55831, 0.041849, -0.53573, 0.18809, -0.58722, 0.015313, -0.014555, 0.80842, -0.038519, 0.75348, 0.70502, -0.17863, 0.3222, 0.67575, 0.67198, 0.26044, 0.4187, -0.34122, 0.2286, -0.53529, 1.2582, -0.091543, 0.19716, -0.037454, -0.3336, 0.31399, 0.36488, 0.71263, 0.1307, -0.24654, -0.52445, -0.036091, 0.55068, 0.10017, 0.48095, 0.71104, -0.053462, 0.22325, 0.30917, -0.39926, 0.036634, -0.35431, -0.42795, 0.46444, 0.25586, 0.68257, -0.20821, 0.38433, 0.055773, -0.2539, -0.20804, 0.52522, -0.11399, -0.3253, -0.44104, 0.17528, 0.62255, 0.50237, -0.7607, -0.071786, 0.0080131, -0.13286, 0.50097, 0.18824, -0.54722, -0.42664, 0.4292, 0.14877, -0.0072514, -0.16484, -0.059798, 0.9895, -0.61738, 0.054169, 0.48424, -0.35084, -0.27053, 0.37829, 0.11503, -0.39613, 0.24266, 0.39147, -0.075256, 0.65093, -0.20822, -0.17456, 0.53571, -0.16537, 0.13582, -0.56016, 0.016964, 0.1277, 0.94071, -0.22608, -0.021106]\n\nchannel = grpc.insecure_channel(&quot;{}:10000&quot;.format(DEPLOYED_INDEX_SERVER_IP))\nstub = match_service_pb2_grpc.MatchServiceStub(channel)\n\nrequest = match_service_pb2.MatchRequest()\nrequest.deployed_index_id = DEPLOYED_INDEX_ID\nfor val in query:\n    request.float_val.append(val)\n\nresponse = stub.Match(request)\nresponse\n<\/code><\/pre>\n<p>Error<\/p>\n<pre><code>_InactiveRpcError                         Traceback (most recent call last)\n\/tmp\/ipykernel_3451\/467153318.py in &lt;module&gt;\n    108     request.float_val.append(val)\n    109 \n--&gt; 110 response = stub.Match(request)\n    111 response\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\n    944         state, call, = self._blocking(request, timeout, metadata, credentials,\n    945                                       wait_for_ready, compression)\n--&gt; 946         return _end_unary_response_blocking(state, call, False, None)\n    947 \n    948     def with_call(self,\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\n    847             return state.response\n    848     else:\n--&gt; 849         raise _InactiveRpcError(state)\n    850 \n    851 \n\n_InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\n    status = StatusCode.UNAVAILABLE\n    details = &quot;failed to connect to all addresses&quot;\n    debug_error_string = &quot;{&quot;created&quot;:&quot;@1638277076.941429628&quot;,&quot;description&quot;:&quot;Failed to pick subchannel&quot;,&quot;file&quot;:&quot;src\/core\/ext\/filters\/client_channel\/client_channel.cc&quot;,&quot;file_line&quot;:3093,&quot;referenced_errors&quot;:[{&quot;created&quot;:&quot;@1638277076.941428202&quot;,&quot;description&quot;:&quot;failed to connect to all addresses&quot;,&quot;file&quot;:&quot;src\/core\/lib\/transport\/error_utils.cc&quot;,&quot;file_line&quot;:163,&quot;grpc_status&quot;:14}]}&quot;\n&gt;\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638291169620,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1639486727367,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70173096",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":74.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":null,
        "Challenge_title":"_InactiveRpcError while querying Vertex AI Matching Engine Index",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":350.0,
        "Challenge_word_count":372,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1463607987528,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lahore, Pakistan",
        "Poster_reputation_count":143.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>Currently, Matching Engine only supports Query from the same region. Can you try running the code from VM in <code>us-central1<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":1.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1582301202872,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When deploying a real-time inferencing pipeline in Azure ML (as per <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy#deploy-the-real-time-endpoint\" rel=\"nofollow noreferrer\">this<\/a> tutorial), I receive the below error. I've tried forcibly logging out using OAuth. Tried creating a new Azure workspace but continue to receive the same error.<\/p>\n\n<p>It looks like the tenant id causing the problem is example.onmicrosoft.com (72f988bf-86f1-41af-91ab-2d7cd011db47)<\/p>\n\n<hr>\n\n<p><em>Deploy: Failed on step CreateServiceFromModels. Details: AzureML service API error. Error calling ServiceCreate: {\"code\":\"Unauthorized\",\"statusCode\":401,\"message\":\"Unauthorized\",\"details\":[{\"code\":\"EmptyOrInvalidToken\",\"message\":\"Error: Service invocation failed!\\r\\nRequest: GET <a href=\"https:\/\/management.azure.com\/subscriptions\/subscription_id\/resourceGroups\/dev-rg\/providers\/Microsoft.MachineLearningServices\/workspaces\/dev-ws\/providers\/Microsoft.Authorization\/permissions?api-version=2015-07-01\" rel=\"nofollow noreferrer\">https:\/\/management.azure.com\/subscriptions\/subscription_id\/resourceGroups\/dev-rg\/providers\/Microsoft.MachineLearningServices\/workspaces\/dev-ws\/providers\/Microsoft.Authorization\/permissions?api-version=2015-07-01<\/a>\\r\\nStatus Code: 401 Unauthorized\\r\\nReason Phrase: Unauthorized\\r\\nResponse Body: {\\\"error\\\":{\\\"code\\\":\\\"InvalidAuthenticationTokenTenant\\\",\\\"message\\\":\\\"The access token is from the wrong issuer '<a href=\"https:\/\/sts.windows.net\/72f988bf-86f1-41af-91ab-2d7cd011db47\/\" rel=\"nofollow noreferrer\">https:\/\/sts.windows.net\/72f988bf-86f1-41af-91ab-2d7cd011db47\/<\/a>'. It must match the tenant '<a href=\"https:\/\/sts.windows.net\/correct_tenant_id\/\" rel=\"nofollow noreferrer\">https:\/\/sts.windows.net\/correct_tenant_id\/<\/a>' associated with this subscription. Please use the authority (URL) '<a href=\"https:\/\/login.windows.net\/correct_tenant_id\" rel=\"nofollow noreferrer\">https:\/\/login.windows.net\/correct_tenant_id<\/a>' to get the token. Note, if the subscription is transferred to another tenant there i<\/em><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1582302156893,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1582395411243,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60342645",
        "Challenge_link_count":9,
        "Challenge_participation_count":3,
        "Challenge_readability":23.7,
        "Challenge_reading_time":29.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML inference pipeline deployment authorization token error",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":481.0,
        "Challenge_word_count":138,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582301202872,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>I appear to have had User Access Administrator role only (in addition to Classic Service Administrator). As soon as I added myself to the Owner role in the Access Control (IAM) section of the Azure Portal, the deployment succeeded.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.1,
        "Solution_reading_time":2.94,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1587438027383,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":7334.0,
        "Answerer_view_count":674.0,
        "Challenge_adjusted_solved_time":1.7229975,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to register a dataset from ADLS Gen2 in my Azure Machine Learning workspace (<code>azureml-core==1.12.0<\/code>). Given that service principal information is not required in the Python SDK <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py#register-azure-data-lake-gen2-workspace--datastore-name--filesystem--account-name--tenant-id-none--client-id-none--client-secret-none--resource-url-none--authority-url-none--protocol-none--endpoint-none--overwrite-false-\" rel=\"noreferrer\">documentation<\/a> for <code>.register_azure_data_lake_gen2()<\/code>, I successfully used the following code to register ADLS gen2 as a datastore:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Datastore\n\nadlsgen2_datastore_name = os.environ['adlsgen2_datastore_name']\naccount_name=os.environ['account_name'] # ADLS Gen2 account name\nfile_system=os.environ['filesystem']\n\nadlsgen2_datastore = Datastore.register_azure_data_lake_gen2(\n    workspace=ws,\n    datastore_name=adlsgen2_datastore_name,\n    account_name=account_name, \n    filesystem=file_system\n)\n<\/code><\/pre>\n<p>However, when I try to register a dataset, using<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Dataset\nadls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)\ndata = Dataset.Tabular.from_delimited_files((adls_ds, 'folder\/data.csv'))\n<\/code><\/pre>\n<p>I get an error<\/p>\n<blockquote>\n<p>Cannot load any data from the specified path. Make sure the path is accessible and contains data.\n<code>ScriptExecutionException<\/code> was caused by <code>StreamAccessException<\/code>.\nStreamAccessException was caused by AuthenticationException.\n<code>'AdlsGen2-ReadHeaders'<\/code> for '[REDACTED]' on storage failed with status code 'Forbidden' (This request is not authorized to perform this operation using this permission.), client request ID &lt;CLIENT_REQUEST_ID&gt;, request ID &lt;REQUEST_ID&gt;. Error message: [REDACTED]\n| session_id=&lt;SESSION_ID&gt;<\/p>\n<\/blockquote>\n<p>Do I need the to enable the service principal to get this to work? Using the ML Studio UI, it appears that the service principal is required even to register the datastore.<\/p>\n<p>Another issue I noticed is that AMLS is trying to access the dataset here:\n<code>https:\/\/adls_gen2_account_name.**dfs**.core.windows.net\/container\/folder\/data.csv<\/code> whereas the actual URI in ADLS Gen2 is: <code>https:\/\/adls_gen2_account_name.**blob**.core.windows.net\/container\/folder\/data.csv<\/code><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600115991930,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1600160631356,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63891547",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":18.0,
        "Challenge_reading_time":34.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"How to connect AMLS to ADLS Gen 2?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":3331.0,
        "Challenge_word_count":219,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589738451347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":179.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>According to this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-data-lake-storage-generation-2\" rel=\"noreferrer\">documentation<\/a>,you need to enable the service principal.<\/p>\n<p>1.you need to register your application and grant the service principal with <strong>Storage Blob Data Reader access<\/strong>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/FZl8O.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FZl8O.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>2.try this code:<\/p>\n<pre><code>adlsgen2_datastore = Datastore.register_azure_data_lake_gen2(workspace=ws,\n                                                             datastore_name=adlsgen2_datastore_name,\n                                                             account_name=account_name,\n                                                             filesystem=file_system,\n                                                             tenant_id=tenant_id,\n                                                             client_id=client_id,\n                                                             client_secret=client_secret\n                                                             )\n\nadls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)\ndataset = Dataset.Tabular.from_delimited_files((adls_ds,'sample.csv'))\nprint(dataset.to_pandas_dataframe())\n<\/code><\/pre>\n<p><strong>Result:<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/50mit.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/50mit.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1600166834147,
        "Solution_link_count":5.0,
        "Solution_readability":25.5,
        "Solution_reading_time":16.17,
        "Solution_score_count":9.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1305851487736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Challenge_adjusted_solved_time":18966.6006311111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want my data and models stored in separate Google Cloud buckets. The idea is that I want to be able to share the data with others without sharing the models.<\/p>\n\n<p>One idea I can think of is using separate git submodules for data and models. But that feels cumbersome and imposes some additional requirements from the end user (e.g. having to do <code>git submodule update<\/code>).<\/p>\n\n<p>So can I do this without using git submodules?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574248229420,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58952962",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":12.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use different remotes for different folders?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1984.0,
        "Challenge_word_count":82,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>You can first add the different <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\" rel=\"nofollow noreferrer\">DVC remotes<\/a> you want to establish (let's say you call them <code>data<\/code> and <code>models<\/code>, each one pointing to a different <a href=\"https:\/\/cloud.google.com\/storage\/docs\/json_api\/v1\/buckets\" rel=\"nofollow noreferrer\">GC bucket<\/a>). <strong>But don't set any remote as the project's default<\/strong>; This way, <a href=\"https:\/\/dvc.org\/doc\/command-reference\/push\" rel=\"nofollow noreferrer\"><code>dvc push<\/code><\/a> won't work without the <code>-r<\/code> (or <code>--remote<\/code>) option.<\/p>\n<p>You would then need to push each directory or file individually to the appropriate remote, like <code>dvc push data\/ -r data<\/code> and <code>dvc push model.dat -r models<\/code>.<\/p>\n<p>Note that a feature request to configure this exists on the DVC repo too. See <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2095\" rel=\"nofollow noreferrer\">Specify file types that can be pushed to remote<\/a>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1642527991692,
        "Solution_link_count":4.0,
        "Solution_readability":13.6,
        "Solution_reading_time":13.5,
        "Solution_score_count":13.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":112.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was wondering whether there is some size limit for storing a folder in sagemaker studio?\nI have this dataset stored in s3 but I want to download that dataset in my sagemaker studio environment to train my model. Is there some kind of limit in the size of a file i can download?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638620800060,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70225564",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.5,
        "Challenge_reading_time":3.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker file size limit?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":398.0,
        "Challenge_word_count":57,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1638359312992,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tasks-manage-storage.html\" rel=\"nofollow noreferrer\">home directory in Amazon SageMaker Studio is stored in Amazon EFS file system<\/a>. The file system grows and shrink as you add and remove files.<br \/>\nAccording to <a href=\"https:\/\/docs.aws.amazon.com\/efs\/latest\/ug\/limits.html#limits-fs-specific\" rel=\"nofollow noreferrer\">Amazon EFS limits<\/a> the <strong>maximum size for a single file is 47.9TiB.<\/strong><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.5,
        "Solution_reading_time":6.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":7270.5901094445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there no way to connect to a URL from azure ml and get it's content <\/p>\n\n<p><strong>my code:<\/strong><\/p>\n\n<pre><code>import requests\ndef azureml_main(dataframe1 = None, dataframe2 = None):    \n    b= requests.get(\"http:\/\/www.google.com\",timeout=30)\n    dataframe1 = b.content\n    return dataframe1\n<\/code><\/pre>\n\n<p>Is there any change need to be made to connect to URL <\/p>\n\n<p><strong>ERROR:<\/strong><\/p>\n\n<pre><code>Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n ---------- Start of error message from Python interpreter ----------\ndata:text\/plain,Caught exception while executing function: Traceback (most recent call last):\nFile \"C:\\server\\invokepy.py\", line 167, in batch\nodfs = mod.azureml_main(*idfs)\nFile \"C:\\temp\\azuremod.py\", line 24, in azureml_main\nb= requests.get(\"http:\/\/www.google.com\",timeout=30)\nFile \"C:\\pyhome\\lib\\site-packages\\requests\\api.py\", line 55, in get\nreturn request('get', url, **kwargs)\nFile \"C:\\pyhome\\lib\\site-packages\\requests\\api.py\", line 44, in request\nreturn session.request(method=method, url=url, **kwargs)\nFile \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 456, in request\nresp = self.send(prep, **send_kwargs)\nFile \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 559, in send\nr = adapter.send(request, **kwargs)\nFile \"C:\\pyhome\\lib\\site-packages\\requests\\adapters.py\", line 375, in send\nraise ConnectionError(e, request=request)\nConnectionError: HTTPConnectionPool(host='www.google.com', port=80): Max retries exceeded with url: \/ (Caused by &lt;class 'socket.gaierror'&gt;: [Errno 11001] getaddrinfo failed)\n\n---------- End of error message from Python  interpreter  ---------\n<\/code><\/pre>\n\n<p>Or is there any change needed to be made on the azure ml settings<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1427449480153,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1427810420956,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/29297579",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":24.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Execute Python Module: Network I\/O Disabled?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":525.0,
        "Challenge_word_count":189,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><em>UPDATE<\/em> 1\/28\/2016<\/p>\n\n<p>Network I\/O for <code>Execute Python Script<\/code> is now supported.<\/p>\n\n<p><em>Out of date<\/em><\/p>\n\n<p>Network I\/O is not support from Execute Python Modules. In order to execute such a program, you should instead launch a virtual machine(Windows or Linux your choice). <\/p>\n\n<p>Windows:<\/p>\n\n<ol>\n<li>RDP into Virtual Machine<\/li>\n<li>Install your choice of Python<\/li>\n<li>You can drag and drop your Python program from your Local Windows machine onto your RDP screen to transfer your code<\/li>\n<li>Then run your program <\/li>\n<\/ol>\n\n<p>Ubuntu:<\/p>\n\n<ol>\n<li>SSH into your virtual machine using Cygwin or Putty(Windows) or Terminal SSH (mac) <code>ssh yourUserName@yourAzureVM.cloudapps.net<\/code><\/li>\n<li>install Python <code>sudo apt-get install python<\/code><\/li>\n<li>open your preferred Linux text editor <code>vi myProgram.py<\/code><\/li>\n<li>Copy and Paste your code into the editor (leave vi with esc <code>:wq<\/code> )<\/li>\n<li>Run your code <code>python myProgram.py<\/code><\/li>\n<\/ol>\n\n<p>To move data from your VM to AzureML please check out the <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\" rel=\"nofollow\">Azure-MachineLearning-ClientLibrary-Python<\/a> on Github<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1453984545350,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":16.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":148.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi;  <\/p>\n<p>First off, where can I find the costs for all the different things I can run in Azure ML? Not just a compute, but editing a notebook, connecting to a datastore, splitting a datastore, etc. Basically where is the price list?  <\/p>\n<p>Second, where can I find what I will be charged for things I ran in the last hour? I want to see what I'm spending before a month is up and the charge is then 100x what I expected (and can afford).  <\/p>\n<p>thanks - dave<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634318912257,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/592299\/cost-of-running-a-compute-other-tasks",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":6.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Cost of running a compute, other tasks",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":95,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, you can use <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/cost-management-billing-overview\">Azure Cost Management<\/a> to manage Azure costs, please review the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/costs\/quick-acm-cost-analysis\">quickstart<\/a> document. Also, the following document provides detailed information on how to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-plan-manage-cost\">plan and manage cost for AML<\/a>.<\/p>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":19.7,
        "Solution_reading_time":8.39,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a model with VPC, Private subnet, and appropriate security group. The endpoint URL can, however, be reached through the internet though failing due to the lack of security token<\/p>\n\n<p>Things I need clarification on now are<\/p>\n\n<ol>\n<li>Is there a way to avoid the URL being accessible from the internet<\/li>\n<li>Are we not charged for requests failed on AUTH(like for API Gateway)<\/li>\n<li>Does that make our deployment vulnerable to any attacks<\/li>\n<\/ol>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1550066008687,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54671841",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":6.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker endpoint(with VPC) url accessible from internet",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":679.0,
        "Challenge_word_count":81,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320419229500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4924.0,
        "Poster_view_count":358.0,
        "Solution_body":"<p>You are not hitting your endpoint, but the endpoint of AWS SageMaker runtime. This endpoint is checking all the permissions to access your hosted model, and only if the credentials and requirements are met, the request is forwarded to your instances and models. <\/p>\n\n<p>Therefore, you can't prevent this URL from being accessible from the Internet, but at the same time, you don't need to protect it or pay for it. AWS has a high level of security on these endpoints, and I don't think that you have a more secure way to protect these endpoints. <\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":6.71,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":97.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1565289301123,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":79.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking for a working example how to access data on a <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data#access-datastores-during-training\" rel=\"nofollow noreferrer\">Azure Machine Learning managed data store<\/a> from within a train.py script. I followed the instructions in the link and my script is able to resolve the datastore.<\/p>\n\n<p>However, whatever I tried (<code>as_download(), as_mount()<\/code>) the only thing I always got was a <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.data_reference.datareference?view=azure-ml-py\" rel=\"nofollow noreferrer\">DataReference<\/a> object. Or maybe I just don't understand how actually read data from a file with that.<\/p>\n\n<pre><code>run = Run.get_context()\nexp = run.experiment\nws = run.experiment.workspace\n\nds = Datastore.get(ws, datastore_name='mydatastore')\ndata_folder_mount = ds.path('mnist').as_mount()\n\n# So far this all works. But how to go from here?\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":4,
        "Challenge_created_time":1559718840513,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56455761",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":12.2,
        "Challenge_reading_time":13.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Access data on AML datastore from training script",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":866.0,
        "Challenge_word_count":107,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1342685175156,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":12103.0,
        "Poster_view_count":1451.0,
        "Solution_body":"<p>You can pass in the DataReference object you created as the input to your training product (scriptrun\/estimator\/hyperdrive\/pipeline). Then in your training script, you can access the mounted path via argument.\nfull tutorial: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.0,
        "Solution_reading_time":6.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645048145367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"S\u00e3o Paulo, Brazil",
        "Answerer_reputation_count":146.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":72.2380869444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a custom job with<\/p>\n<pre><code>gcloud ai custom-jobs create --region=us-west1 --display-name=test-job --config=trainjob.yaml\n<\/code><\/pre>\n<p>where <code>trainjob.yaml<\/code> is<\/p>\n<pre><code>workerPoolSpecs:\n  machineSpec:\n    machineType: n1-standard-4\n  replicaCount: 1\n  containerSpec:\n    imageUri: eu.gcr.io\/myproject\/myimage\n<\/code><\/pre>\n<p>I can see the list of the job via<\/p>\n<pre><code>gcloud ai custom-jobs list --region=us-west1\n<\/code><\/pre>\n<p>. Can this list seen in the UI? For AI Platform product there is jobs but I don't see anything like this in Vertex AI<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657900255003,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72996624",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":8.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Can the list of custom jobs in vertex AI custom seen in the UI?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":81,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1485529624880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1991.0,
        "Poster_view_count":107.0,
        "Solution_body":"<p>I don't know if it is exactly what you are looking for, but you can see the custom training jobs details using the UI at <code>Console<\/code> &gt; <code>Vertex AI<\/code> &gt; <code>Training<\/code> &gt; <code>Custom Jobs<\/code> or following the next <a href=\"https:\/\/console.cloud.google.com\/vertex-ai\/training\/custom-jobs\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1658160312116,
        "Solution_link_count":1.0,
        "Solution_readability":15.0,
        "Solution_reading_time":4.76,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":42.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been storing my large files in CLOBs within Oracle, but I am thinking of storing my large files in a shared drive, then having a column in Oracle contain pointers to the files. This would use DVC.<\/p>\n<p>When I do this,<\/p>\n<p>(a) are the paths in Oracle paths that point to the files in my shared drive, as in, the actual files themselves?<\/p>\n<p>(b) or do the paths in Oracle point somehow to the DVC metafile?<\/p>\n<p>Any insight would help me out!<\/p>\n<p>Thanks :)\nJustin<\/p>\n<hr \/>\n<p>EDIT to provide more clarity:<\/p>\n<p>I checked here (<a href=\"https:\/\/dvc.org\/doc\/api-reference\/open\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/api-reference\/open<\/a>), and it helped, but I'm not fully there yet ...<\/p>\n<p>I want to pull a file from a remote dvc repository using python (which I have connected to the Oracle database). So, if we can make that work, I think I will be good. But, I am confused. If I specify 'remote' below, then how do I name the file (e.g., 'activity.log') when the remote files are all encoded?<\/p>\n<pre><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>(NOTE: For testing purposes, my &quot;remote&quot; DVC directory is just another folder on my MacBook.)<\/p>\n<p>I feel like I'm missing a key concept about getting remote files ...<\/p>\n<p>I hope that adds more clarity. Any help figuring out remote file access is appreciated! :)<\/p>\n<p>Justin<\/p>\n<hr \/>\n<p>EDIT to get insights on 'rev' parameter:<\/p>\n<p>Before my question, some background\/my setup:\n(a) I have a repo on my MacBook called 'basics'.\n(b) I copied into 'basics' a directory of 501 files (called 'surface_files') that I subsequently pushed to a remote storage folder called 'gss'. After the push, 'gss' contains 220 hash directories.<\/p>\n<p>The steps I used to get here are as follows:<\/p>\n<pre><code>&gt; cd ~\/Desktop\/Work\/basics\n&gt; git init\n&gt; dvc init\n&gt; dvc add ~\/Desktop\/Work\/basics\/surface_files\n&gt; git add .gitignore surface_files.dvc\n&gt; git commit -m &quot;Add raw data&quot;\n&gt; dvc remote add -d remote_storage ~\/Desktop\/Work\/gss\n&gt; git commit .dvc\/config -m &quot;Configure remote storage&quot;\n&gt; dvc push\n&gt; rm -rf .\/.dvc\/cache\n&gt; rm -rf .\/surface_files\n<\/code><\/pre>\n<p>Next, I ran the following Python code to take one of my surface files, named <code>surface_100141.dat<\/code>, and used <code>dvc.api.get_url()<\/code> to get the corresponding remote storage file name. I then copied this remote storage file into my desktop under the file's original name, i.e., <code>surface_100141.dat<\/code>.<\/p>\n<p>The code that does all this is as follows, but FIRST, MY QUESTION --- when I run the code as it is shown below, no problems; but when I uncomment the 'rev=' line, it fails. I am not sure why this is happening. I used <code>git log<\/code> and <code>cat .git\/refs\/heads\/master<\/code> to make sure that I was getting the right hash. WHY IS THIS FAILING? That is my question.<\/p>\n<p>(In full disclosure, my git knowledge is not too strong yet. I'm getting there, but it's still a work in progress! :))<\/p>\n<pre><code>import dvc.api\nimport os.path\nfrom os import path\nimport shutil\n\nfilename = 'surface_100141.dat' # This file name would be stored in my Oracle database\nhome_dir = os.path.expanduser('~')+'\/' # This simply expanding '~' into '\/Users\/ricej\/'\n\nresource_url = dvc.api.get_url(\n    path=f'surface_files\/{filename}', # Works when 'surface_files.dvc' exists, even when 'surface_files' directory and .dvc\/cache do not\n    repo=f'{home_dir}Desktop\/Work\/basics',\n    # rev='5c92710e68c045d75865fa24f1b56a0a486a8a45', # Commit hash, found using 'git log' or 'cat .git\/refs\/heads\/master'\n    remote='remote_storage')\nresource_url = home_dir+resource_url\nprint(f'Remote file: {resource_url}')\n\nnew_dir = f'{home_dir}Desktop\/' # Will copy fetched file to desktop, for demonstration\nnew_file = new_dir+filename\nprint(f'Remote file copy: {new_file}')\n\nif path.exists(new_file):\n    os.remove(new_file)\n    \ndest = shutil.copy(resource_url, new_file) # Check your desktop after this to see remote file copy\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1617399923477,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1617643343263,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66925614",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":7.8,
        "Challenge_reading_time":53.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":50,
        "Challenge_solved_time":null,
        "Challenge_title":"How to access DVC-controlled files from Oracle?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":389.0,
        "Challenge_word_count":582,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1407091594728,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>I'm not 100% sure that I understand the question (it would be great to expand it a bit on the actual use case you are trying to solve with this database), but I can share a few thoughts.<\/p>\n<p>When we talk about DVC, I think you need to specify a few things to identify the file\/directory:<\/p>\n<ol>\n<li>Git commit + path (actual path like <code>data\/data\/xml<\/code>). Commit (or to be precise any Git revision) is needed to identify the version of the data file.<\/li>\n<li>Or path in the DVC storage (<code>\/mnt\/shared\/storage\/00\/198493ef2343ao<\/code> ...<code>) + actual name of this file. This way you would be saving info that <\/code>.dvc` files have.<\/li>\n<\/ol>\n<p>I would say that second way is <em>not<\/em> recommended since to some extent it's an implementation detail - how does DVC store files internally. The public interface to DVC organized data storage is its repository URL + commit + file name.<\/p>\n<p>Edit (example):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p><code>location\/of\/dvc\/project<\/code> this path must point to an actual Git repo. This repo should have a <code>.dvc<\/code> or <code>dvc.lock<\/code> file that has <code>activity.log<\/code> name in it + its hash in the remote storage:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n  - md5: a304afb96060aad90176268345e10355\n    path: activity.log\n<\/code><\/pre>\n<p>By reading this Git repo and analyzing let's say <code>activity.log.dvc<\/code> DVC will be able to create the right path <code>s3:\/\/my-bucket\/storage\/a3\/04afb96060aad90176268345e10355<\/code><\/p>\n<p><code>remote='my-s3-bucket'<\/code> argument is optional. By default it will use the one that is defined in the repo itself.<\/p>\n<p>Let's take another real example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'get-started\/data.xml',\n        repo='https:\/\/github.com\/iterative\/dataset-registry'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>In the <code>https:\/\/github.com\/iterative\/dataset-registry<\/code> you could find the <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/get-started\/data.xml.dvc\" rel=\"nofollow noreferrer\"><code>.dvc<\/code> file<\/a> that is enough for DVC to create a path to the file by also analyzing its <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/.dvc\/config\" rel=\"nofollow noreferrer\">config<\/a><\/p>\n<pre><code>https:\/\/remote.dvc.org\/dataset-registry\/a3\/04afb96060aad90176268345e10355\n<\/code><\/pre>\n<p>you could run <code>wget<\/code> on this file to download it<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1617478114567,
        "Solution_link_count":5.0,
        "Solution_readability":11.4,
        "Solution_reading_time":35.95,
        "Solution_score_count":2.0,
        "Solution_sentence_count":27.0,
        "Solution_word_count":314.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1388584380832,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":396.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm researching the use of MLflow as part of our data science initiatives and I wish to set up a minimum working example of remote execution on databricks from windows.<\/p>\n\n<p>However, when I perform the remote execution a path is created locally on windows in the MLflow package which is sent to databricks. This path specifies the upload location of the '.tar.gz' file corresponding to the Github repo containing the MLflow Project. In cmd this has a combination of '\\' and '\/', but on databricks there are no separators at all in this path, which raises the 'rsync: No such file or directory (2)' error.<\/p>\n\n<p>To be more general, I reproduced the error using an MLflow standard example and following this <a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/projects.html\" rel=\"nofollow noreferrer\">guide<\/a> from databricks. The MLflow example is the <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/sklearn_elasticnet_wine\" rel=\"nofollow noreferrer\">sklearn_elasticnet_wine<\/a>, but I had to add a default value to a parameter so I forked it and the MLproject which can be executed remotely can be found at (<a href=\"https:\/\/github.com\/aestene\/mlflow\/tree\/master\/examples\/sklearn_elasticnet_wine\" rel=\"nofollow noreferrer\">forked repo<\/a>).<\/p>\n\n<p>The Project can be executed remotely by the following command (assuming a databricks instance has been set up)<\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/aestene\/mlflow#examples\/sklearn_elasticnet_wine -b databricks -c db-clusterconfig.json --experiment-id &lt;insert-id-here&gt;\n<\/code><\/pre>\n\n<p>where \"db-clusterconfig.json\" correspond to the cluster to set up in databricks and is in this example set to<\/p>\n\n<pre><code>{\n    \"autoscale\": {\n        \"min_workers\": 1,\n        \"max_workers\": 2\n    },\n    \"spark_version\": \"5.5.x-scala2.11\",\n    \"node_type_id\": \"Standard_DS3_v2\",\n    \"driver_node_type_id\": \"Standard_DS3_v2\",\n    \"ssh_public_keys\": [],\n    \"custom_tags\": {},\n    \"spark_env_vars\": {\n        \"PYSPARK_PYTHON\": \"\/databricks\/python3\/bin\/python3\"\n    }\n}\n<\/code><\/pre>\n\n<p>When running the project remotely, this is the output in cmd:<\/p>\n\n<pre><code>2019\/10\/04 10:09:50 INFO mlflow.projects: === Fetching project from https:\/\/github.com\/aestene\/mlflow#examples\/sklearn_elasticnet_wine into C:\\Users\\ARNTS\\AppData\\Local\\Temp\\tmp2qzdyq9_ ===\n2019\/10\/04 10:10:04 INFO mlflow.projects.databricks: === Uploading project to DBFS path \/dbfs\\mlflow-experiments\\3947403843428882\\projects-code\\aa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz ===\n2019\/10\/04 10:10:05 INFO mlflow.projects.databricks: === Finished uploading project to \/dbfs\\mlflow-experiments\\3947403843428882\\projects-code\\aa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz ===\n2019\/10\/04 10:10:05 INFO mlflow.projects.databricks: === Running entry point main of project https:\/\/github.com\/aestene\/mlflow#examples\/sklearn_elasticnet_wine on Databricks ===\n2019\/10\/04 10:10:06 INFO mlflow.projects.databricks: === Launched MLflow run as Databricks job run with ID 8. Getting run status page URL... ===\n2019\/10\/04 10:10:18 INFO mlflow.projects.databricks: === Check the run's status at https:\/\/&lt;region&gt;.azuredatabricks.net\/?o=&lt;databricks-id&gt;#job\/8\/run\/1 ===\n<\/code><\/pre>\n\n<p>Where the DBFS path has a leading '\/' before the remaining are '\\'. <\/p>\n\n<p>The command spins up a cluster in databricks and is ready to execute the job, but ends up with the following error message on the databricks side:<\/p>\n\n<pre><code>rsync: link_stat \"\/dbfsmlflow-experiments3947403843428882projects-codeaa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz\" failed: No such file or directory (2)\nrsync error: some files\/attrs were not transferred (see previous errors) (code 23) at main.c(1183) [sender=3.1.1]\n<\/code><\/pre>\n\n<p>Where we can see the same path but without the '\\' inserted. I narrowed down the creation of this path to this <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/projects\/databricks.py\" rel=\"nofollow noreferrer\">file<\/a> in the MLflow Github repo, where the following code creates the path (line 133):<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>dbfs_path = os.path.join(DBFS_EXPERIMENT_DIR_BASE, str(experiment_id),\n                                     \"projects-code\", \"%s.tar.gz\" % tarfile_hash)\ndbfs_fuse_uri = os.path.join(\"\/dbfs\", dbfs_path)\n<\/code><\/pre>\n\n<p>My current hypothesis is that <code>os.path.join()<\/code> in the first line joins the string together in a \"windows fashion\" such that they have backslashes. Then the following call to <code>os.path.join()<\/code> adds a '\/'. The databricks file system is then unable to handle this path and something causes the 'tar.gz' file to not be properly uploaded or to be accessed at the wrong path. <\/p>\n\n<p>It should also be mentioned that the project runs fine locally.<\/p>\n\n<p>I'm running the following versions:<\/p>\n\n<p>Windows 10<\/p>\n\n<p>Python 3.6.8<\/p>\n\n<p>MLflow 1.3.0 (also replicated the fault with 1.2.0)<\/p>\n\n<p>Any feedback or suggestions are greatly appreciated!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570185592420,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58234777",
        "Challenge_link_count":8,
        "Challenge_participation_count":2,
        "Challenge_readability":10.1,
        "Challenge_reading_time":66.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow remote execution on databricks from windows creates an invalid dbfs path",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":350.0,
        "Challenge_word_count":537,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1570173110492,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Thanks for the catch, you're right that using <code>os.path.join<\/code> when working with DBFS paths is incorrect, resulting in a malformed path that breaks project execution. I've filed to <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1926\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1926<\/a> track this, if you're interested in making a bugfix PR (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst\" rel=\"nofollow noreferrer\">see the MLflow contributor guide for info on how to do this<\/a>) to replace <code>os.path.join<\/code> here with <code>os.posixpath.join<\/code> I'd be happy to review :)<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":7.8,
        "Solution_reading_time":8.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":69.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Bunch of secrets being automatically created in key vault that is integrated into Azure ML workspace(s). These secrets seem to be generated by the ML resource\/service itself and continues to generate new secrets.<\/p>\n<p>\u00a0Can you please help with the document for these secrets?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1682857366816,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1270782\/secrets-automatically-created-in-key-vault",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.4,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Secrets automatically created in key vault",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":49,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@<a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=dfa9d536-725c-462d-87c8-47fbafb1a2bc\">D-0887<\/a> Thanks, When you perform operations in Azure ML that require secret values to be stored like creating connections, datastores, or workspace management operations, the key vault instance associated to the workspace is used to store those secrets.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":22.0,
        "Solution_reading_time":4.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to execute ClearML task remotely. According to docs there are 2 options: 1) execute single python file; 2) ClearML would identify that script is part of repo, that repo will be cloned and installed into docker and executed on the worker.<\/p>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<p>I think it is somewhat extending scenario 1, where not a single file is passed for execution but whole directory with file in it.<\/p>\n<p>PS: i understand reproducibility concerns that arise, but repo is really not accessible from worker :(<\/p>\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653499480970,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72381916",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Remotely execute ClearML task using local-only repo",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":25.0,
        "Challenge_word_count":131,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1653498830776,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<blockquote>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<\/blockquote>\n<p>well, no :( if your code is a single script, then yes ClearML would store the entire script, then the worker will reproduce it on the remote machine. But if your code base is composed of more than a single file, then why not use git? it is free hosted by GitHub, Bitbucket, GitLab etc.<\/p>\n<p>In theory this is doable and if you feel the need, I urge you to PR this feature. Basically you would store the entire folder as an artifact (ClearML will auto zip it for you), then the agent needs to unzip the artifact and run it. The main issue would be that cloning the Task will not clone the artifact...<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":10.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":162.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a machine learning workspace in West Europe region. But the storage account, key vault and application insights got created in East US region. All these got created by default with creation on ML workspace.  <br \/>\nSo I want to know the reason for different region and also want to move the storage account to West Europe region.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613120096110,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/270693\/why-the-storage-account-assosiated-to-azure-machin",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":5.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Why the storage account assosiated to azure machine learning has differenent region compared ML workspace?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":74,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, this is unusual. There's no way to move your default AML storage account to a different region. I recommend creating a new workspace or contacting <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/azure-portal\/supportability\/how-to-create-azure-support-request\">Azure Support<\/a> to investigate further.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":4.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nWhen using the Neptune ML widget to export data like the command below from the 01- Node Classification notebook:\r\n```\r\n%%neptune_ml export start --export-url {neptune_ml.get_export_service_host()} --export-iam --wait --store-to export_results\r\n${export_params}\r\n```\r\nThe following error is thrown\r\n```\r\n{\r\n  \"message\": \"Credential should be scoped to correct service: 'execute-api'. \"\r\n}  \r\n```\r\n\r\n**Expected behavior**\r\nThe export should run to completion\r\n\r\n\r\n",
        "Challenge_closed_time":1628716.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1627938048000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/167",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Neptune ML Export widget throwing error",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This issue occurs on a cluster created using the default CFN script with IAM disabled\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":1.04,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":15.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":1626973312768,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":92.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm having the error mentioned in the title when trying to upload a large file (15gb) to my s3 bucket from a Sagemaker notebook instance.<\/p>\n<p>I know that there are some similar questions here that i have already visited. I have gone through <a href=\"https:\/\/stackoverflow.com\/questions\/52541933\/accessdenied-when-calling-the-createmultipartupload-operation-in-django-using-dj\">this<\/a>, <a href=\"https:\/\/stackoverflow.com\/questions\/37630635\/createmultipartupload-operation-aws-policy-items-needed\">this<\/a>, and <a href=\"https:\/\/stackoverflow.com\/questions\/36272286\/getting-access-denied-when-calling-the-putobject-operation-with-bucket-level-per\">this<\/a> question, but after following the steps mentioned, and applying the policies described in these questions i still have the same error.<\/p>\n<p>I have also come to <a href=\"https:\/\/aws.amazon.com\/es\/premiumsupport\/knowledge-center\/s3-access-denied-error-kms\/#:%7E:text=%22An%20error%20occurred%20(AccessDenied)%20when%20calling%20the%20CreateMultipartUpload%20operation,GenerateDataKey%20and%20kms%3ADecrypt%20actions.\" rel=\"nofollow noreferrer\">this<\/a> documentation page eventually. The problem is that when i go into my users page in the IAM section, i see no users. I can see some roles but no users and i don't know which role should i edit following the steps mentioned in the documentation page. Also, my bucket DON'T have encryption enabled so i'm not really sure that the steps in the documentation page will fix the error for me.<\/p>\n<p>This is the policy in currently using for my bucket:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Id&quot;: &quot;Policy1&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Principal&quot;: {\n                &quot;AWS&quot;: &quot;arn:aws:iam::XXXX:root&quot;\n            },\n            &quot;Action&quot;: &quot;s3:*&quot;,\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::bauer-bucket&quot;,\n                &quot;arn:aws:s3:::bauer-bucket\/*&quot;\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n<p>I'm totally lost with this, i need to upload that file to my bucket. Please help.<\/p>\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1629371337230,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68846704",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":16.2,
        "Challenge_reading_time":29.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"An error occurred (AccessDenied) when calling the CreateMultipartUpload operation: Access Denied",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":774.0,
        "Challenge_word_count":210,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521854999168,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Spain",
        "Poster_reputation_count":1338.0,
        "Poster_view_count":265.0,
        "Solution_body":"<p>The access is dictated by the execution role that is attached to the SageMaker notebook. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html<\/a> goes through how add additional s3 permissions to a SageMaker execution role.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.3,
        "Solution_reading_time":4.72,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1450057717008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using AzureMLBatchExecution activity in Azure Data Factory, is it secure to pass the DB query as a global parameter to the AzureML web service? <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1476354083450,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40018320",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Is it secure to pass the DB query to AzureML as a global parameter?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":39,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1452608563363,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":105.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>When you talk about \"secure\", are you worried about secure transmission between AML and ADF, or secure storage of your DB query information? For the former, all communication between these two services will be done with HTTPS. For the latter, our production storage has its strict access control. Besides, we only log the count of the global parameters and never the values. I believe it's secure to pass your DB query as a global parameter to the AzureML web service.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":5.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1421803794992,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"St. Louis, MO, USA",
        "Answerer_reputation_count":373.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p><strong>Problem:<\/strong>\nI am trying to setup a model in Sagemaker, however it fails when it comes to downloading the data.\nDoes anyone know what I am doing wrong?<\/p>\n\n<p><strong>What I did so far<\/strong>:\nIn order to avoid any mistakes on my side I decided to use the AWS tutorial:\ntensorflow_iris_dnn_classifier_using_estimators<\/p>\n\n<p>And I made only two changes:<\/p>\n\n<ol>\n<li>I copied the dataset to my own S3 instance. --> I tested if I could access \/ show the data and it worked.<\/li>\n<li>I edited the path to point to the new folder.<\/li>\n<\/ol>\n\n<p>This is the AWS source code:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators<\/a><\/p>\n\n<pre><code>%%time\nimport boto3\n\n# use the region-specific sample data bucket\nregion = boto3.Session().region_name\n#train_data_location = 's3:\/\/sagemaker-sample-data-{}\/tensorflow\/iris'.format(region)\ntrain_data_location = 's3:\/\/my-s3-bucket'\n\niris_estimator.fit(train_data_location)\n<\/code><\/pre>\n\n<p>And this is the error I get:<\/p>\n\n<pre><code>\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/interactiveshell.pyc in run_cell_magic(self, magic_name, line, cell)\n   2115             magic_arg_s = self.var_expand(line, stack_depth)\n   2116             with self.builtin_trap:\n-&gt; 2117                 result = fn(magic_arg_s, cell)\n   2118             return result\n   2119 \n\n&lt;decorator-gen-60&gt; in time(self, line, cell, local_ns)\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/magic.pyc in &lt;lambda&gt;(f, *a, **k)\n    186     # but it's overkill for just that one bit of state.\n    187     def magic_deco(arg):\n--&gt; 188         call = lambda f, *a, **k: f(*a, **k)\n    189 \n    190         if callable(arg):\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/magics\/execution.pyc in time(self, line, cell, local_ns)\n   1191         else:\n   1192             st = clock2()\n-&gt; 1193             exec(code, glob, local_ns)\n   1194             end = clock2()\n   1195             out = None\n\n&lt;timed exec&gt; in &lt;module&gt;()\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/tensorflow\/estimator.pyc in fit(self, inputs, wait, logs, job_name, run_tensorboard_locally)\n    314                 tensorboard.join()\n    315         else:\n--&gt; 316             fit_super()\n    317 \n    318     @classmethod\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/tensorflow\/estimator.pyc in fit_super()\n    293 \n    294         def fit_super():\n--&gt; 295             super(TensorFlow, self).fit(inputs, wait, logs, job_name)\n    296 \n    297         if run_tensorboard_locally and wait is False:\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/estimator.pyc in fit(self, inputs, wait, logs, job_name)\n    232         self.latest_training_job = _TrainingJob.start_new(self, inputs)\n    233         if wait:\n--&gt; 234             self.latest_training_job.wait(logs=logs)\n    235 \n    236     def _compilation_job_name(self):\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/estimator.pyc in wait(self, logs)\n    571     def wait(self, logs=True):\n    572         if logs:\n--&gt; 573             self.sagemaker_session.logs_for_job(self.job_name, wait=True)\n    574         else:\n    575             self.sagemaker_session.wait_for_job(self.job_name)\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/session.pyc in logs_for_job(self, job_name, wait, poll)\n   1126 \n   1127         if wait:\n-&gt; 1128             self._check_job_status(job_name, description, 'TrainingJobStatus')\n   1129             if dot:\n   1130                 print()\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/session.pyc in _check_job_status(self, job, desc, status_key_name)\n    826             reason = desc.get('FailureReason', '(No reason provided)')\n    827             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 828             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    829 \n    830     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Training job sagemaker-tensorflow-2019-01-03-16-32-16-435: Failed Reason: ClientError: Data download failed:S3 key: s3:\/\/my-s3-bucket\/\/sagemaker-tensorflow-2019-01-03-14-02-39-959\/source\/sourcedir.tar.gz has an illegal char sub-sequence '\/\/' in it\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1546534753933,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54026623",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":15.9,
        "Challenge_reading_time":57.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":9.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker - ClientError: Data download failed",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":5145.0,
        "Challenge_word_count":367,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1546261116430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>The script is expecting 'bucket' to be bucket = Session().default_bucket() or your own. Have you tried setting bucket equal to your personal bucket?<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":1.95,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I came across <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-datasets#mount-vs-download\">this page<\/a> which describes how to work with AML datasets. I'm specifically interested in mounting. It states that &quot;Mounting is supported for Linux-based computes&quot;. Is there no way to do this on Windows?    <\/p>\n<p>Thanks,    <br \/>\nYordan    <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653326462743,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/860774\/mount-aml-dataset-on-windows",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":5.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Mount AML dataset on Windows",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":46,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@YordanZaykov-7763 Yes, currently this is only supported for linux based computes for Azure ML. Windows only supports download option.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.3,
        "Solution_reading_time":6.76,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1584379010350,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm having an issue with AWS when I try to create a device fleet with sagemaker :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nsagemaker_client = boto3.client('sagemaker', region_name=AWS_REGION)\nsagemaker_client.create_device_fleet(\n    DeviceFleetName=device_fleet_name,\n    RoleArn=iot_role_arn,\n    OutputConfig={\n        'S3OutputLocation': s3_device_fleet_output\n    }\n)\n\n<\/code><\/pre>\n<p>It raises the following exception:<\/p>\n<blockquote>\n<p>ClientError: An error occurred (ValidationException) when calling the CreateDeviceFleet operation: The account id &lt;my-account-id&gt; does not have ownership on bucket: &lt;bucket-name&gt;<\/p>\n<\/blockquote>\n<p>I dont get it because I created the bucket so I should be the owner. I have not found how to check or change bucket ownership.<\/p>\n<p>I tried changing the bucket policy as follows but it didn't help.<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Principal&quot;: {\n                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;id&gt;:user\/&lt;user&gt;&quot;\n            },\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;*&quot;,\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;&quot;,\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;\/*&quot;\n            ]\n        }\n    ]\n}\n\n<\/code><\/pre>\n<p>I also tried with sagemaker's GUI, it fails for the same reason (ValidationException, the account id &lt;my-account-id&gt; does not have ownership on bucket : &lt;bucket-name&gt;).<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1641413429643,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70599052",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":21.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS CreateDeviceFleet operation fail because \"the account id does not have ownership on bucket\"",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":153,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1584379010350,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":36.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>This bucket policy made it work :<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Principal&quot;: {\n                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;account-id&gt;:role\/&lt;iot-role&gt;&quot;\n            },\n            &quot;Action&quot;: &quot;*&quot;,\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;&quot;,\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;\/*&quot;\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n<p>I still don't fully get it, because the role had full access on s3 buckets so i don't know why editing the bucket's policy changed something, but it works.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":29.5,
        "Solution_reading_time":9.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a file dataset from a data lake folder on Azure ML Studio,  at the moment I\u00b4m able to download the data from the dataset to the compute instance with this code:<\/p>\n<pre><code>subscription_id = 'xxx'\nresource_group = 'luisdatapipelinetest'\nworkspace_name = 'ml-pipelines'\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndataset = Dataset.get_by_name(workspace, name='files_test')\npath = &quot;\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/demo1231\/code\/Users\/luis.rramirez\/test\/&quot;\ndataset.download(target_path=path, overwrite=True)\n<\/code><\/pre>\n<p>With that I'm able to access the files from the notebook.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8q8y2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8q8y2.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But copying the data from the data lake to the compute instance is not efficient, how can I mount the data lake directory in the vm instead of copying the data each time?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630008530263,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1630044624416,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68944750",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":13.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Mount a datalake storage in azure ML studio",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":258.0,
        "Challenge_word_count":112,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423439611840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":8349.0,
        "Poster_view_count":949.0,
        "Solution_body":"<p>MOUNTING ADLS2 to AML so you can save files into your mountPoint directly. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-data-lake-storage-generation-2\" rel=\"nofollow noreferrer\">Here<\/a> is the example of registering the storage and <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.file_dataset.filedataset?view=azure-ml-py#mount-mount-point-none----kwargs-\" rel=\"nofollow noreferrer\">here<\/a> shows how to mount your registered datastore.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.0,
        "Solution_reading_time":7.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1261400320736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Antwerp, Belgium",
        "Answerer_reputation_count":7876.0,
        "Answerer_view_count":924.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've installed <a href=\"https:\/\/mlflow.org\/\" rel=\"nofollow noreferrer\">MLflow<\/a> on Ubuntu Server 18.04 LTS, in a virtual environment (Python 3), using its <a href=\"https:\/\/mlflow.org\/docs\/latest\/quickstart.html\" rel=\"nofollow noreferrer\">Quickstart documentation<\/a>:<\/p>\n\n<pre><code>$ python3 -m venv mlflow\n$ source \/home\/emre\/mlflow\/bin\/activate\n$ pip install mlflow\n<\/code><\/pre>\n\n<p>that gave the following output during install:<\/p>\n\n<pre><code>Collecting mlflow\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e8\/b3\/cf358e182be34a62fcd6843e5df793f278bd9d24f78f565509cb927c6a22\/mlflow-0.1.0.tar.gz (4.3MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3MB 323kB\/s\nCollecting Flask (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7f\/e7\/08578774ed4536d3242b14dacb4696386634607af824ea997202cd0edb4b\/Flask-1.0.2-py2.py3-none-any.whl (91kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 9.4MB\/s\nCollecting awscli (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ba\/32\/d6d254f6ccc2ed21f02d81f38709ff06feca9cbdb2e68ea90635fa483a73\/awscli-1.15.46-py2.py3-none-any.whl (1.3MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3MB 1.0MB\/s\nCollecting boto3 (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/24\/e0\/a98898b94d8093bbd8fd4576fb2e89620adac1e24a2bfc28d11c4ce29a5b\/boto3-1.7.46-py2.py3-none-any.whl (128kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 8.8MB\/s\nCollecting click&gt;=6.7 (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/34\/c1\/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77\/click-6.7-py2.py3-none-any.whl (71kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 9.3MB\/s\nCollecting databricks-cli (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/58\/78\/4bda6f29a091ab7b0ad29efdba2491e5d0b56bd09d608857e6f0b799be48\/databricks-cli-0.7.2.tar.gz\nCollecting gitpython (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ac\/c9\/96d7c86c623cb065976e58c0f4898170507724d6b4be872891d763d686f4\/GitPython-2.1.10-py2.py3-none-any.whl (449kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 450kB 2.9MB\/s\nCollecting numpy (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/68\/1e\/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2\/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.2MB 110kB\/s\nCollecting pandas (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/57\/eb\/6ab533ea8e35e7dd159af6922ac1123d4565d89f3926ad9a6aa46530978f\/pandas-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11.8MB 116kB\/s\nCollecting protobuf (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/fc\/f0\/db040681187496d10ac50ad167a8fd5f953d115b16a7085e19193a6abfd2\/protobuf-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.1MB 177kB\/s\nCollecting pygal (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/5f\/b7\/201c9254ac0d2b8ffa3bb2d528d23a4130876d9ba90bc28e99633f323f17\/pygal-2.4.0-py2.py3-none-any.whl (127kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 9.7MB\/s\nCollecting python-dateutil (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/cf\/f5\/af2b09c957ace60dcfac112b669c45c8c97e32f94aa8b56da4c6d1682825\/python_dateutil-2.7.3-py2.py3-none-any.whl (211kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 6.0MB\/s\nCollecting pyyaml (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/10\/7d\/6efe0bd69580fecd40adf47ebaf8d807238308ccb851f0549881fa7605aa\/PyYAML-4.1.tar.gz (153kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 7.8MB\/s\nCollecting querystring_parser (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/57\/64\/3086a9a991ff3aca7b769f5b0b51ff8445a06337ae2c58f215bcee48f527\/querystring_parser-1.2.3.tar.gz\nCollecting requests&gt;=2.17.3 (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/65\/47\/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda\/requests-2.19.1-py2.py3-none-any.whl (91kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 8.2MB\/s\nCollecting scikit-learn (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/3d\/2d\/9fbc7baa5f44bc9e88ffb7ed32721b879bfa416573e85031e16f52569bc9\/scikit_learn-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.4MB 108kB\/s\nCollecting scipy (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a8\/0b\/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730\/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 31.2MB 42kB\/s\nCollecting six&gt;=1.10.0 (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/67\/4b\/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a\/six-1.11.0-py2.py3-none-any.whl\nCollecting uuid (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ce\/63\/f42f5aa951ebf2c8dac81f77a8edcc1c218640a2a35a03b9ff2d4aa64c3d\/uuid-1.30.tar.gz\nCollecting zipstream (from mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/1a\/a4\/58f0709cef999db1539960aa2ae77100dc800ebb8abb7afc97a1398dfb2f\/zipstream-1.1.4.tar.gz\nCollecting itsdangerous&gt;=0.24 (from Flask-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/dc\/b4\/a60bcdba945c00f6d608d8975131ab3f25b22f2bcfe1dab221165194b2d4\/itsdangerous-0.24.tar.gz (46kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 10.4MB\/s\nCollecting Werkzeug&gt;=0.14 (from Flask-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/20\/c4\/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243\/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 327kB 4.0MB\/s\nCollecting Jinja2&gt;=2.10 (from Flask-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7f\/ff\/ae64bacdfc95f27a016a7bed8e8686763ba4d277a78ca76f32659220a731\/Jinja2-2.10-py2.py3-none-any.whl (126kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 8.2MB\/s\nCollecting rsa&lt;=3.5.0,&gt;=3.1.2 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e1\/ae\/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e\/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 10.5MB\/s\nCollecting botocore==1.10.46 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b4\/04\/ddaad5574f70a539d106e8d53b4685e3de4387de7a16884a95459f8c7691\/botocore-1.10.46-py2.py3-none-any.whl (4.4MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.4MB 314kB\/s\nCollecting s3transfer&lt;0.2.0,&gt;=0.1.12 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/d7\/14\/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d\/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 10.6MB\/s\nCollecting colorama&lt;=0.3.9,&gt;=0.2.5 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/db\/c8\/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf\/colorama-0.3.9-py2.py3-none-any.whl\nCollecting docutils&gt;=0.10 (from awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/36\/fa\/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d\/docutils-0.14-py3-none-any.whl (543kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 552kB 2.5MB\/s\nCollecting jmespath&lt;1.0.0,&gt;=0.7.1 (from boto3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b7\/31\/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365\/jmespath-0.9.3-py2.py3-none-any.whl\nCollecting configparser&gt;=0.3.5 (from databricks-cli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7c\/69\/c2ce7e91c89dc073eb1aa74c0621c3eefbffe8216b3f9af9d3885265c01c\/configparser-3.5.0.tar.gz\nCollecting tabulate&gt;=0.7.7 (from databricks-cli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/12\/c2\/11d6845db5edf1295bc08b2f488cf5937806586afe42936c3f34c097ebdc\/tabulate-0.8.2.tar.gz (45kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 7.9MB\/s\nCollecting gitdb2&gt;=2.0.0 (from gitpython-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e0\/95\/c772c13b7c5740ec1a0924250e6defbf5dfdaee76a50d1c47f9c51f1cabb\/gitdb2-2.0.3-py2.py3-none-any.whl (63kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 11.2MB\/s\nCollecting pytz&gt;=2011k (from pandas-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/dc\/83\/15f7833b70d3e067ca91467ca245bae0f6fe56ddc7451aa0dc5606b120f2\/pytz-2018.4-py2.py3-none-any.whl (510kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 512kB 421kB\/s\nRequirement already satisfied: setuptools in .\/mlflow\/lib\/python3.6\/site-packages (from protobuf-&gt;mlflow)\nCollecting chardet&lt;3.1.0,&gt;=3.0.2 (from requests&gt;=2.17.3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/bc\/a9\/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8\/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 143kB 8.7MB\/s\nCollecting idna&lt;2.8,&gt;=2.5 (from requests&gt;=2.17.3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4b\/2a\/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165\/idna-2.7-py2.py3-none-any.whl (58kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 10.3MB\/s\nCollecting urllib3&lt;1.24,&gt;=1.21.1 (from requests&gt;=2.17.3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/bd\/c9\/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb\/urllib3-1.23-py2.py3-none-any.whl (133kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 143kB 8.3MB\/s\nCollecting certifi&gt;=2017.4.17 (from requests&gt;=2.17.3-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7c\/e6\/92ad559b7192d846975fc916b65f667c7b8c3a32bea7372340bfe9a15fa5\/certifi-2018.4.16-py2.py3-none-any.whl (150kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 8.0MB\/s\nCollecting MarkupSafe&gt;=0.23 (from Jinja2&gt;=2.10-&gt;Flask-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4d\/de\/32d741db316d8fdb7680822dd37001ef7a448255de9699ab4bfcbdf4172b\/MarkupSafe-1.0.tar.gz\nCollecting pyasn1&gt;=0.1.3 (from rsa&lt;=3.5.0,&gt;=3.1.2-&gt;awscli-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a0\/70\/2c27740f08e477499ce19eefe05dbcae6f19fdc49e9e82ce4768be0643b9\/pyasn1-0.4.3-py2.py3-none-any.whl (72kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 10.9MB\/s\nCollecting smmap2&gt;=2.0.0 (from gitdb2&gt;=2.0.0-&gt;gitpython-&gt;mlflow)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e3\/59\/4e22f692e65f5f9271252a8e63f04ce4ad561d4e06192478ee48dfac9611\/smmap2-2.0.3-py2.py3-none-any.whl\nBuilding wheels for collected packages: mlflow, databricks-cli, pyyaml, querystring-parser, uuid, zipstream, itsdangerous, configparser, tabulate, MarkupSafe\n  Running setup.py bdist_wheel for mlflow ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/mlflow\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmp10fdrz2ypip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for mlflow\n  Running setup.py clean for mlflow\n  Running setup.py bdist_wheel for databricks-cli ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/databricks-cli\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpy_2acqi3pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for databricks-cli\n  Running setup.py clean for databricks-cli\n  Running setup.py bdist_wheel for pyyaml ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/pyyaml\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmp4bs2fwrtpip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for pyyaml\n  Running setup.py clean for pyyaml\n  Running setup.py bdist_wheel for querystring-parser ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/querystring-parser\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmp_cnm9w_tpip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for querystring-parser\n  Running setup.py clean for querystring-parser\n  Running setup.py bdist_wheel for uuid ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/uuid\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpenr2igaxpip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for uuid\n  Running setup.py clean for uuid\n  Running setup.py bdist_wheel for zipstream ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/zipstream\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpnzsjh5e2pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for zipstream\n  Running setup.py clean for zipstream\n  Running setup.py bdist_wheel for itsdangerous ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/itsdangerous\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmp7imi3zv2pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for itsdangerous\n  Running setup.py clean for itsdangerous\n  Running setup.py bdist_wheel for configparser ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/configparser\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpyk9qtmi1pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for configparser\n  Running setup.py clean for configparser\n  Running setup.py bdist_wheel for tabulate ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/tabulate\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpjim2qr00pip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for tabulate\n  Running setup.py clean for tabulate\n  Running setup.py bdist_wheel for MarkupSafe ... error\n  Complete output from command \/home\/emre\/mlflow\/bin\/python3 -u -c \"import setuptools, tokenize;__file__='\/tmp\/pip-build-s7vrp5z7\/MarkupSafe\/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d \/tmp\/tmpsdpdd8ulpip-wheel- --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\n\n  error: invalid command 'bdist_wheel'\n\n  ----------------------------------------\n  Failed building wheel for MarkupSafe\n  Running setup.py clean for MarkupSafe\nFailed to build mlflow databricks-cli pyyaml querystring-parser uuid zipstream itsdangerous configparser tabulate MarkupSafe\nInstalling collected packages: click, itsdangerous, Werkzeug, MarkupSafe, Jinja2, Flask, pyasn1, rsa, jmespath, six, python-dateutil, docutils, botocore, s3transfer, colorama, pyyaml, awscli, boto3, configparser, chardet, idna, urllib3, certifi, requests, tabulate, databricks-cli, smmap2, gitdb2, gitpython, numpy, pytz, pandas, protobuf, pygal, querystring-parser, scikit-learn, scipy, uuid, zipstream, mlflow\n  Running setup.py install for itsdangerous ... done\n  Running setup.py install for MarkupSafe ... done\n  Running setup.py install for pyyaml ... done\n  Running setup.py install for configparser ... done\n  Running setup.py install for tabulate ... done\n  Running setup.py install for databricks-cli ... done\n  Running setup.py install for querystring-parser ... done\n  Running setup.py install for uuid ... done\n  Running setup.py install for zipstream ... done\n  Running setup.py install for mlflow ... done\nSuccessfully installed Flask-1.0.2 Jinja2-2.10 MarkupSafe-1.0 Werkzeug-0.14.1 awscli-1.15.46 boto3-1.7.46 botocore-1.10.46 certifi-2018.4.16 chardet-3.0.4 click-6.7 colorama-0.3.9 configparser-3.5.0 databricks-cli-0.7.2 docutils-0.14 gitdb2-2.0.3 gitpython-2.1.10 idna-2.7 itsdangerous-0.24 jmespath-0.9.3 mlflow-0.1.0 numpy-1.14.5 pandas-0.23.1 protobuf-3.6.0 pyasn1-0.4.3 pygal-2.4.0 python-dateutil-2.7.3 pytz-2018.4 pyyaml-4.1 querystring-parser-1.2.3 requests-2.19.1 rsa-3.4.2 s3transfer-0.1.13 scikit-learn-0.19.1 scipy-1.1.0 six-1.11.0 smmap2-2.0.3 tabulate-0.8.2 urllib3-1.23 uuid-1.30 zipstream-1.1.4\n<\/code><\/pre>\n\n<p>After that I checked the following didn't give any errors:<\/p>\n\n<pre><code>import os\nfrom mlflow import log_metric, log_param, log_artifact\n<\/code><\/pre>\n\n<p>But when I try to run the web-based user interface, I get the following errors:<\/p>\n\n<pre><code>$ mlflow ui\nTraceback (most recent call last):\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 574, in _build_master\n    ws.require(__requires__)\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 892, in require\n    needed = self.resolve(parse_requirements(requirements))\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 783, in resolve\n    raise VersionConflict(dist, req).with_context(dependent_req)\npkg_resources.ContextualVersionConflict: (PyYAML 4.1 (\/home\/emre\/mlflow\/lib\/python3.6\/site-packages), Requirement.parse('PyYAML&lt;=3.12,&gt;=3.10'), {'awscli'})\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/emre\/mlflow\/bin\/mlflow\", line 6, in &lt;module&gt;\n    from pkg_resources import load_entry_point\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 3088, in &lt;module&gt;\n    @_call_aside\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 3072, in _call_aside\n    f(*args, **kwargs)\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 3101, in _initialize_master_working_set\n    working_set = WorkingSet._build_master()\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 576, in _build_master\n    return cls._build_from_requirements(__requires__)\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 589, in _build_from_requirements\n    dists = ws.resolve(reqs, Environment())\n  File \"\/home\/emre\/mlflow\/lib\/python3.6\/site-packages\/pkg_resources\/__init__.py\", line 783, in resolve\n    raise VersionConflict(dist, req).with_context(dependent_req)\npkg_resources.ContextualVersionConflict: (PyYAML 4.1 (\/home\/emre\/mlflow\/lib\/python3.6\/site-packages), Requirement.parse('PyYAML&lt;=3.12,&gt;=3.10'), {'awscli'})\n<\/code><\/pre>\n\n<p>Any ideas how I can fix this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530106954863,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1530112131487,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51064366",
        "Challenge_link_count":42,
        "Challenge_participation_count":1,
        "Challenge_readability":15.2,
        "Challenge_reading_time":293.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":213,
        "Challenge_solved_time":null,
        "Challenge_title":"Can't run MLflow web-based user interface",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":791.0,
        "Challenge_word_count":1336,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1261400320736,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Antwerp, Belgium",
        "Poster_reputation_count":7876.0,
        "Poster_view_count":924.0,
        "Solution_body":"<p>Apparently I had to install the <code>wheel<\/code> module inside my virtual environment. I deleted the virtual environment, re-created it, and then installed the <code>wheel<\/code> module:<\/p>\n\n<pre><code>pip install wheel\n<\/code><\/pre>\n\n<p>after that <code>pip install mlflow<\/code>, as well as <code>mlflow ui<\/code> worked successfully.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.8,
        "Solution_reading_time":4.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":41.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1597855076910,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Delft, Netherlands",
        "Answerer_reputation_count":60.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am unable to perform the simple action:<\/p>\n<pre><code>import sagemaker\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n<\/code><\/pre>\n<p>because my notebook instance is not connected to the internet. I have an STS endpoint interface in the same subnet as my notebook instance but I thought the sagemaker API is using the global endpoint. I actually get the following error message after a while:<\/p>\n<pre><code>ConnectTimeoutError: Connect timeout on endpoint URL: &quot;https:\/\/sts.us-east-1.amazonaws.com\/&quot;\n<\/code><\/pre>\n<p>How do I fix this? Or does one need to update the sagemaker module?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608633852607,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65407274",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS SageMaker (with internet disabled) unable to connect to STS",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":510.0,
        "Challenge_word_count":91,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597855076910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delft, Netherlands",
        "Poster_reputation_count":60.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>So the solution is to include a VPC endpoint for the sagemaker API (api.sagemaker...) as well as STS.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":1.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently using react-native to build a mobile application. I need to access a machine learning model in order to send pictures for segmentation. I want to be able to receive a segmented picture back to have the background of the picture cut out. I am trying to use Amazon Sagemaker (because it seems to be a easy to work with package, but if there are other ways to do it, please let me know).<\/p>\n\n<p>On <a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/?sc_icampaign=pac-sagemaker-console-tutorial&amp;sc_ichannel=ha&amp;sc_icontent=awssm-2276&amp;sc_iplace=console-body&amp;trk=ha_awssm-2276\" rel=\"nofollow noreferrer\">this<\/a> Sagemaker quick-start guide, on step 5a, it states:<\/p>\n\n<blockquote>\n  <p>5a. To deploy the model on a server and create an endpoint that you can access, copy the following code into the next code cell and select Run:\n  xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')<\/p>\n<\/blockquote>\n\n<p>I want to host everything on AWS and not have to run a separate server. What service\/process could I use that would allow me to create an endpoint that I can access through react-native?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":4,
        "Challenge_created_time":1569478549487,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58110595",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":12.0,
        "Challenge_reading_time":16.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I access Amazon Sagemaker through React Native?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":717.0,
        "Challenge_word_count":163,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1488334447848,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>To summarize the conversation in the comments:<\/p>\n\n<p>Once you have your model trained, tuned, and deployed (which is not a simple process), you can call the endpoint of the model using the <a href=\"https:\/\/github.com\/aws\/aws-sdk-js\" rel=\"nofollow noreferrer\">AWS SDK for JavaScript<\/a>, that you install by:<\/p>\n\n<pre><code>npm install aws-sdk\nvar AWS = require('aws-sdk\/dist\/aws-sdk-react-native');\n<\/code><\/pre>\n\n<p>you include in the HTML as:<\/p>\n\n<pre><code>&lt;script src=\"https:\/\/sdk.amazonaws.com\/js\/aws-sdk-2.538.0.min.js\"&gt;&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>And when you want to call the endpoint you invoke it like that:<\/p>\n\n<pre><code>var params = {\n  Body: Buffer.from('...') || 'STRING_VALUE' \/* Strings will be Base-64 encoded on your behalf *\/, \/* required *\/\n  EndpointName: 'STRING_VALUE', \/* required *\/\n  Accept: 'STRING_VALUE',\n  ContentType: 'STRING_VALUE',\n  CustomAttributes: 'STRING_VALUE'\n};\nsagemakerruntime.invokeEndpoint(params, function(err, data) {\n  if (err) console.log(err, err.stack); \/\/ an error occurred\n  else     console.log(data);           \/\/ successful response\n});\n<\/code><\/pre>\n\n<p>You can check out the <a href=\"https:\/\/aws-amplify.github.io\" rel=\"nofollow noreferrer\">Amplify Library<\/a> that can take some of the heavy liftings such as getting IAM permissions to call the API, a user log in and many others. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.9,
        "Solution_reading_time":17.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":145.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1614882423070,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Earlier when using AzureML from the Notebooks blade of Azure ML UI, we could access the local files in AzureML using simple relative paths:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bKZ0W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bKZ0W.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For example, in the above image to access the CSV from the <code>test.ipynb<\/code> we could just mention the relative path:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pandas.read_csv('WHO-COVID-19-global-data.csv')\n<\/code><\/pre>\n<p>However, we are not able to do that anymore.<\/p>\n<p>Also when we run<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nos.getcwd()\n<\/code><\/pre>\n<p>We see the output as\n<code>'\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/&lt;cluster-name&gt;'<\/code>.<\/p>\n<p>Hence, we are unable to access the files in the FileStore which was not the case earlier.<\/p>\n<p>When we run the same from the JuyterLab environment of the compute environment we get:<\/p>\n<p><code>'\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/&lt;cluster-name&gt;\/code\/Users\/&lt;current-user-name&gt;\/temp'<\/code>.<\/p>\n<p>We can easily solve it by adding the path <code>'\/code\/Users\/&lt;current-user-name&gt;\/temp'<\/code> at the base and use that instead. But this is not recommended as with a change in the environment we are using the code needs to change every time. How do we resolve this issue without going through this path appending method.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1632809238003,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69356567",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":20.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"How to access local files from AzureML File Share?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":375.0,
        "Challenge_word_count":177,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>I work on the Notebooks team in AzureML, I just tried this. Did this just start happening today?<\/p>\n<p>It seems like things are working as expected: <a href=\"https:\/\/i.stack.imgur.com\/xeDIT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xeDIT.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":4.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>I have create a compute instance through the Azure ML Studio. Using the &quot;Applications&quot; list, I can access Jupyter, RStudio, a Terminal and connect via VS Code. But how can I just connect via SSH? I have added my SSH key to the instance while creating it, but cannot connect via the public IP address listed in the node details. I tried this as <code>ssh azureuser@&lt;compute-ip&gt;<\/code>  <\/p>\n<p>In the output of the <code>azure ml compute show<\/code> command in the cloud shell I noticed the values <code>&quot;enable_public_ip&quot;: false<\/code> and <code>&quot;ssh_public_access&quot;: &quot;False&quot;<\/code>, but couldn't find anything regarding how to change these settings. Or is there any way to connect to them without a public ip?  <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1626870444287,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/484265\/connect-to-compute-instance-via-ssh",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":10.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Connect to compute instance via ssh",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":121,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey Yutong,  <br \/>\nI was just asking about compute instances created though the Azure ML Studio in general. Like the ones you create when you use the ML Studio, then select <em>Compute<\/em> in the sidebar and under <em>Compute Instances<\/em> create a new compute node. During creation, I selected the &quot;Enable SSH access&quot; option.  <\/p>\n<p>I did figure out a solution to my problem of connecting to such an instance a bit later, and will leave it here for others with the same problem. As long as you select &quot;Enable SSH access&quot;, the SSH access does work even though the <code>azure ml compute show<\/code> says <code>&quot;ssh_public_access&quot;: &quot;False&quot;<\/code>. My problem just was that I did not know the user name I had to log in as and also did not know which port I had to connect to. It turns out that the user name is always set to <code>azureuser<\/code> and the port seems to be either <code>50000<\/code> or <code>50001<\/code>. You should therefore be able to connect via <code>ssh  azureuser@&lt;PUBLIC_IP&gt; -p 50000<\/code> or with <code>-p 50001<\/code>. You can find the public IP on the details page of the compute instance. If you want to know for sure which port it is, you can query the azure CLI with   <\/p>\n<pre><code>az resource show --ids \/subscriptions\/{subscriptionId}\/resourceGroups\/{resourceGroupName}\/providers\/Microsoft.MachineLearningServices\/workspaces\/{workspaceName}\/computes\/{computeName}\n<\/code><\/pre>\n<p>and in the JSON response look under <em>properties.sshSettings<\/em> for <em>adminUserName<\/em> and <em>sshPort<\/em>.  <\/p>\n<p>I just wished that this was documented somewhere in the Azure ML docs. I had to read through the source code of how the Azure ML VSCode extension connects to a compute instance to find this out.  <\/p>\n<p>Is there any easier way to accomplish this? I needed the SSH access to forward specific ports from the compute instance to local and also to set up a Python SSH remote interpreter in IntelliJ<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.4,
        "Solution_reading_time":24.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":299.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I\u2019m having an issue importing the data into my Machine Learning Studio. It shows me a Red Cross with the error 0030 - which means that there\u2019s an issue in downloading the data. For background, I\u2019m importing data from the Web URL via HTTP option. I looked up the issue on the troubleshooting page, followed the advice, which shows I\u2019ve done everything correctly. My data link works perfectly fine in my browser. When I enter the http link into my browser, it immediately downloads the csv file. However, my studio is not downloading the data. Importing the data is the first step in my experiment, and I can\u2019t move forward without it. Immediate help would be greatly appreciated! I\u2019ve attached pictures for reference. [1]: \/api\/attachments\/72499-0ebb78a4-4805-46e8-a7f1-fbf99682af5f.png?platform=QnA <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1614362680897,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/291213\/importing-data-in-azure-ml-studio-experiment",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":10.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Importing Data in Azure ML Studio Experiment",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":131,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,  <\/p>\n<p>This exception in Azure Machine Learning occurs when it is not possible to download a file. You will receive this exception when an attempted read from an HTTP source has failed after three (3) retry attempts.  <\/p>\n<p>Resolution: Verify that the URI to the HTTP source is correct and that the site is currently accessible via the Internet.  <\/p>\n<p>Is this file on any place need authentication?   <\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":5.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1577353307072,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":491.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to connect to an Azure SQL Database from inside Azure Machine Learning Studio. Based on <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py<\/a>, it seems that the recommended pattern is to create a Datastore using the Datastore.register_azure_sql_database method as follows:<\/p>\n<pre><code>import os\nfrom azureml.core import Workspace, Datastore\n\nws = Workspace.from_config() # asks for interactive authentication the first time\n\nsql_datastore_name  = &quot;datastore_test_01&quot; # any name should be fine\nserver_name         = os.getenv(&quot;SQL_SERVERNAME&quot;    , &quot;{SQL_SERVERNAME}&quot;) # Name of the Azure SQL server\ndatabase_name       = os.getenv(&quot;SQL_DATABASENAME&quot;  , &quot;{SQL_DATABASENAME}&quot;) # Name of the Azure SQL database\nusername            = os.getenv(&quot;SQL_USER_NAME&quot;     , &quot;{SQL_USER_NAME}&quot;) # The username of the database user.\npassword            = os.getenv(&quot;SQL_USER_PASSWORD&quot; , &quot;{SQL_USER_PASSWORD}&quot;) # The password of the database user.\n\nsql_datastore = Datastore.register_azure_sql_database(workspace      = ws,\n                                                      datastore_name = sql_datastore_name,\n                                                      server_name    = server_name,\n                                                      database_name  = database_name,\n                                                      username       = username,\n                                                      password       = password)\n<\/code><\/pre>\n<p>I am pretty sure I have set all parameters right, having copied them from the ADO.NET connection string at my SQL Database resource --&gt; Settings --&gt; Connection strings:<\/p>\n<pre><code>Server=tcp:{SQL_SERVERNAME}.database.windows.net,1433;Initial Catalog={SQL_DATABASENAME};Persist Security Info=False;User ID={SQL_USER_NAME};Password={SQL_USER_PASSWORD};MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;\n<\/code><\/pre>\n<p>However, I get the following error:<\/p>\n<pre><code>Registering datastore failed with a 400 error code and error message 'Azure SQL Database Error -2146232060: Please check the correctness of the datastore information.'\n<\/code><\/pre>\n<p>Am I missing something? E.g., a firewall rule? I have also tried adding the Azure ML compute resource's public IP address to the list of allowed IP addresses in my SQL Database resource, but still no success.<\/p>\n<hr \/>\n<p><strong>UPDATE<\/strong>: adding <code>skip_validation = True<\/code> to <code>Datastore.register_azure_sql_database<\/code> solves the issue. I can then query the data with<\/p>\n<pre><code>from azureml.core import Dataset\nfrom azureml.data.datapath import DataPath\n\nquery   = DataPath(sql_datastore, 'SELECT * FROM my_table')\ntabular = Dataset.Tabular.from_sql_query(query, query_timeout = 10)\ndf = tabular.to_pandas_dataframe()\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598976793243,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1599032818923,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63691515",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":37.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Studio: cannot create Datastore from Azure SQL Database",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":793.0,
        "Challenge_word_count":262,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1502454917960,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Milano, MI, Italia",
        "Poster_reputation_count":132.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>is the datastore behind vnet? where are you running the registration code above? On a compute instance behind the same vnet?\nhere is the doc that describe what you need to do to connect to data behind vnet:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-virtual-network#use-datastores-and-datasets\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-virtual-network#use-datastores-and-datasets<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.0,
        "Solution_reading_time":6.46,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1508517418056,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":156.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to access the Workspace object in my <code>train.py<\/code> script, when running in an Estimator.  <\/p>\n\n<p>I currently can access the Run object, using the following code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>run = Run.get_context()\n<\/code><\/pre>\n\n<p>But I cannot seem to get my hands on the Workspace object in my training script.  I would use this mostly to get access to the Datastores and Datasets (as I would hope to keep all data set references inside the training script, instead of passing them as input datasets)<\/p>\n\n<p>Any idea if\/how this is possible ?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591183699477,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62171724",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":8.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I access the Workspace object from a training script in AzureML?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1148.0,
        "Challenge_word_count":102,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>Sure, try this:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\nws = run.experiment.workspace\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.8,
        "Solution_reading_time":1.78,
        "Solution_score_count":12.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":12.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1653511725307,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":26.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I created a sagemaker project with a terraform template which successfully created with a stack successfully created and associated with it. However, there is no repository associated or pipeline associated with the sagemaker project despite there being both in the cloudformation template I used. Can someone help with this?<\/p>\n<p>Is there a way to manually link a sagemaker project with a code commit repository? I see that succesfully linked repositories have the tag: <code>sagemaker:project-name<\/code> with the correct project name.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1657295938437,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1657311059267,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72914046",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":11.5,
        "Challenge_reading_time":8.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker Project successfully creates but there are no linked pipelines or repositories",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":124.0,
        "Challenge_word_count":90,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1653511725307,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>Using a different cloudformation template fixed the issue. Not sure why.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What is the proper way to connect DVC to Min.IO that is connected to some buckets on S3.<\/p>\n<pre><code>AWS-S3(My_Bucket) &gt; Min.io(MY_Bucket aliased as S3)\n<\/code><\/pre>\n<p>Right now i am accessing my bucket by using mc for example <code>mc cp s3\/my_bucket\/datasets datasets<\/code> to copy stuff from there. But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC so i can use for example <code>&quot;DVC mc-S3 pull&quot;<\/code> and <code>&quot;DVC AWS-S3 pull&quot;<\/code>.<\/p>\n<p>How do i got for it because while googling i couldn't find anything that i could easily follow.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627469557323,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68559059",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":4.9,
        "Challenge_reading_time":8.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"DVC connect to Min.IO to access S3",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":375.0,
        "Challenge_word_count":101,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578574709920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Poland",
        "Poster_reputation_count":85.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>It looks like you are looking for a combination of things.<\/p>\n<p>First, Jorge mentioned you can set <code>endpointurl<\/code> to access Minio the same way as you would access regular S3:<\/p>\n<pre><code>dvc remote add -d minio-remote s3:\/\/mybucket\/path\ndvc remote modify minio-remote endpointurl https:\/\/minio.example.com                          \n<\/code><\/pre>\n<p>Second, it seems you can create <em>two<\/em> remotes - one for S3, one for Minio and use <code>-r<\/code> option that is available for many data management related commands:<\/p>\n<pre><code>dvc pull -r minio-remote\ndvc pull -r s3-remote\ndvc push -r minio-remote\n...\n<\/code><\/pre>\n<p>This way you could <code>push<\/code>\/<code>pull<\/code> data to\/from a specific storage.<\/p>\n<blockquote>\n<p>But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC<\/p>\n<\/blockquote>\n<p>There are other possible ways, I think to organize this. It indeed depends on what semantics you expect from <code>DVC mc-S3 pull<\/code>. Please let us know if <code>-r<\/code> is not enough and clarify the question- that would help us here.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.7,
        "Solution_reading_time":13.51,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":153.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Is there any overlap of permissions between a space execution role and a user profile execution role, if those two are given different permissions? Does it behave the same as with service control policies (SCPs) and policies on IAM roles, where the overlap is what takes effect? For example, if the space execution role has an explicit deny (or even an implicit deny for that matter!) for `CreateApp`, but the user profile execution role has an explicit allow for `CreateApp`, then that user profile won't be able to CreateApp?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671475979923,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1671823545396,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUUTo01GFWQlyRYDXAKQInyA\/venn-diagram-of-permissions-in-sagemaker-studio",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":7.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Venn diagram of permissions in SageMaker Studio?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":167.0,
        "Challenge_word_count":95,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Yann, the user profile's execution role is what will be used within the context of the private app of the user profile. So, once a user hits Launch -> Studio or is redirected to Studio UI through SSO, the user's execution role will allow them to launch apps such as data science app, data wrangler app, etc.\n\nThe default space execution role is what is assumed by the user once they are in a shared space. So, once the user is in the UI by clicking Launch -> Spaces, the user cannot create apps to run notebooks in, within that space.\n\nTL;DR - they are two distinct roles, each giving the user permissions on a private space or a shared space, and do not work like SCPs. Any user in the shared space within a domain, will share the same execution role as of today. However, for the private space, each user profile can have their own role if needed.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1671492806348,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":10.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":158.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1491898605956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sydney NSW, Australia",
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We have applications for multiple tenants on our AWS account and would like to distinguish between them in different IAM roles. In most places this is already possible by limiting resource access based on naming patterns.<\/p>\n<p>For CloudWatch log groups of SageMaker training jobs however I have not seen a working solution yet. The tenants can choose the job name arbitrarily, and hence the only part of the LogGroup name that is available for pattern matching would be the prefix before the job name. This prefix however seems to be fixed to <code>\/aws\/sagemaker\/TrainingJobs<\/code>.<\/p>\n<p>Is there a way to change or extend this prefix in order to make such limiting possible? Say, for example <code>\/aws\/sagemaker\/TrainingJobs\/&lt;product&gt;-&lt;stage&gt;-&lt;component&gt;\/&lt;training-job-name&gt;-...<\/code> so that a resource limitation like <code>\/aws\/sagemaker\/TrainingJobs\/&lt;product&gt;-*<\/code> becomes possible?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649316153020,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71777914",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":12.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Change AWS SageMaker LogGroup Prefix?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":127,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1257535237563,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":1904.0,
        "Poster_view_count":321.0,
        "Solution_body":"<p>I think it is not possible to change the log streams names for any of the SageMaker services.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1601993658523,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Calgary, AB, Canada",
        "Answerer_reputation_count":592.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Just a conceptual question here.<\/p>\n<p>I'm a newbie to aws. I have a node app and a python file that is currently on a Flask server. The node app sends data to the Py server and gets data back. This takes approx 3.2 secs to happen. I am not sure how I can apply this to AWS. I tried sagemaker but it was really costly for me. Is there anyway I can create a Python server with an endpoint in AWS within the free tier?<\/p>\n<p>Thanks<\/p>\n<p>Rushi<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602699610963,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64359321",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.1,
        "Challenge_reading_time":6.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"creating python server in AWS without breaching free tier",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":451.0,
        "Challenge_word_count":95,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600997541087,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4019.0,
        "Poster_view_count":212.0,
        "Solution_body":"<p>You do not need to use sagemaker to deploy your flask application to AWS. AWS has a nice documentation to deploy a Flask Application to an <a href=\"https:\/\/docs.aws.amazon.com\/elasticbeanstalk\/latest\/dg\/create-deploy-python-flask.html\" rel=\"nofollow noreferrer\">AWS Elastic Beanstalk<\/a> environment.<\/p>\n<p>Other than that you can also deploy the application using two methods.<\/p>\n<ol>\n<li>via EC2<\/li>\n<li>via Lambda<\/li>\n<\/ol>\n<h3>EC2 Instances<\/h3>\n<p>You can launch the ec2 instance with public IP with SSH enabled from your IP address. Then SSH into the instance and install the python, it's libraries and your application.<\/p>\n<h3>Lambda<\/h3>\n<p>AWS lambda is the perfect solution. It scales automatically, depends upon the requests your application will receive.\nAs lambda needs your dependencies be available in the package, so you need to install them using <code>--target<\/code> parameter, zip the python code along with the installed packages and then upload to the Lambda.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install --target .\/package Flask\ncd package\nzip -r9 function.zip . # Create a ZIP archive of the dependencies.\ncd .. &amp;&amp; zip -g function.zip lambda_function.py # Add your function code to the archive.\n<\/code><\/pre>\n<p>For more detailed instructions you can read these documentations<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/lambda-python.html\" rel=\"nofollow noreferrer\">Lambda<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.1,
        "Solution_reading_time":18.86,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":183.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1411212343683,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, BC, Canada",
        "Answerer_reputation_count":3592.0,
        "Answerer_view_count":268.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've uploaded my own Jupyter notebook to Sagemaker, and am trying to create an iterator for my training \/ validation data which is in S3, as follow:<\/p>\n\n<pre><code>train = mx.io.ImageRecordIter(\n        path_imgrec         = \u2018s3:\/\/bucket-name\/train.rec\u2019 \u2026\u2026 )\n<\/code><\/pre>\n\n<p>I receive the following exception: <\/p>\n\n<pre><code>MXNetError: [04:33:32] src\/io\/s3_filesys.cc:899: Need to set enviroment variable AWS_SECRET_ACCESS_KEY to use S3\n<\/code><\/pre>\n\n<p>I've checked that the IAM role attached with this notebook instance has S3 access. Any clues on what might be needed to fix this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522300028073,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49548422",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":7.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Training data in S3 in AWS Sagemaker",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1353.0,
        "Challenge_word_count":80,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449031669083,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":777.0,
        "Poster_view_count":103.0,
        "Solution_body":"<p>If your IAM roles are setup correctly, then you need to download the file to the Sagemaker instance first and then work on it. Here's how:<\/p>\n\n<pre><code># Import roles\nimport sagemaker\nrole = sagemaker.get_execution_role()\n\n# Download file locally\ns3 = boto3.resource('s3')\ns3.Bucket(bucket).download_file('your_training_s3_file.rec', 'training.rec')\n\n#Access locally\ntrain = mx.io.ImageRecordIter(path_imgrec=\u2018training.rec\u2019 \u2026\u2026 )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.6,
        "Solution_reading_time":5.8,
        "Solution_score_count":4.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1454844135036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"T\u00fcrkiye",
        "Answerer_reputation_count":462.0,
        "Answerer_view_count":83.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying out <strong>Amazon Sagemaker<\/strong>, I haven't figured out how we can have Continuous training.\n<br>\nFor example if i have a CSV file in s3 and I want to train each time the CSV file is updated.<\/p>\n\n<p>I know we can go again to the notebook and re-run the whole notebook to make this happen.\n<br>\nBut i am looking for an automated way, with some python scripts or using a lambda function with s3 events etc<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1529654311630,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1529654668140,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50983316",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":7.3,
        "Challenge_reading_time":5.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Continuous Training in Sagemaker",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1191.0,
        "Challenge_word_count":82,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1440734188430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":1491.0,
        "Poster_view_count":112.0,
        "Solution_body":"<p>You can use boto3 sdk for python to start training on lambda then you need to trigger the lambda when csv is update.<\/p>\n\n<blockquote>\n  <p><a href=\"http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/sagemaker.html\" rel=\"nofollow noreferrer\">http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/sagemaker.html<\/a><\/p>\n<\/blockquote>\n\n<p>Example python code<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a><\/p>\n<\/blockquote>\n\n<p>Addition: You dont need to use lambda you just start\/cronjob the python script any kind of instance which has python and aws sdk in it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.0,
        "Solution_reading_time":10.3,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1342713532568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seoul, South Korea",
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I've been working on the automation of processes and it includes fetching data from an external source through DVC(data version control) for which I am using SSH client to pull and push changes. For automation, I'm using <strong>Jenkins<\/strong> and the problem I'm facing is that for ssh we need to give a password on runtime, and in automation that's not an option. I've tried multiple ways to specify passwords for ssh like sshpass and ssh config but it turns out Jenkins when building creates some file name <strong>script.sh<\/strong> in a directory <em>repoName@tmp<\/em> in var\/lib\/jenkins\/.... and therefore it is giving permission denied error. no matter what I try. If anyone could give any suggestions to this problem it would be appreciated.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611222255303,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65824766",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":9.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"SSH automation in jenkins",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":124,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1493101921288,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pakistan",
        "Poster_reputation_count":133.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>You could use key-based auth for SSH instead instead of password auth so that your Jenkins user can access your SSH DVC remote without needing to specify a password.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":2.12,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":29.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently trying to connect to a DocumentDb (MongoDb) using Azure Machine Learning Studio.<\/p>\n\n<p>I am currently following <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/import-from-cosmos-db\" rel=\"nofollow noreferrer\">this<\/a> guide, however it seems out of date already. The assumptions I have taken have lead me to get an <code>Error 1000: ... DocumentDb client threw an exception<\/code> <code>The underlying connection was closed. The connection was closed unexpectedly.<\/code><\/p>\n\n<p>The guide, and Azure Machine Learning Studio, outline the following parameters to make a connection.<\/p>\n\n<p>Endpoint URL, Database ID, DocumentDb Key, Collection ID. It also tells you to look under the <code>Keys<\/code> blade to find these, which does not exist anymore.<\/p>\n\n<p>These are the assumptions I have taken;<\/p>\n\n<ul>\n<li>Endpoint URL = host + port under the Connection String blade. <code>https:\/\/host.com:port\/<\/code><\/li>\n<li>Database ID = the database name listed under the Data Explorer blade.<\/li>\n<li>DocumentDb Key = Primary Password under the Connection String blade.<\/li>\n<li>Collection ID = the name of a collection in the db from the Data Explorer blade.<\/li>\n<\/ul>\n\n<p>I have, for now, also opened all connections to the database just to make sure I wasn't closing the network to outside requests which, I guess, means that at least the DocumentDb key is a poor assumption.<\/p>\n\n<hr>\n\n<p>After some input from Jon, below, here is the current state of things<\/p>\n\n<ul>\n<li>Endpoint URL = the Uri from the Overview blade.<\/li>\n<li>Database ID = the database name listed under the Data Explorer blade.<\/li>\n<li>DocumentDb Key = the Primary Password under the Connection String blade.<\/li>\n<li>Collection ID = the name of a collection in the db from the Data Explorer blade.<\/li>\n<li>Sql query = <code>select top 10 * from CollectionID<\/code><\/li>\n<li>Sql parameters = {}<\/li>\n<\/ul>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":12,
        "Challenge_created_time":1530540068210,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1530573105180,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51137973",
        "Challenge_link_count":2,
        "Challenge_participation_count":13,
        "Challenge_readability":9.7,
        "Challenge_reading_time":25.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Studio - Import from Cosmos Db",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":801.0,
        "Challenge_word_count":268,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1350771597060,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1758.0,
        "Poster_view_count":191.0,
        "Solution_body":"<p>Through discussion in the comments, it may be that the \"Endpoint URL\" just needed to be updated, but I'll go over all of the inputs in case anyone else needs a reference to it.<\/p>\n\n<ul>\n<li>Endpoint URL - Can use the URI in the CosmosDB \"Overview\" pane in the Azure Portal<\/li>\n<li>Database ID - The name of the database to connect to<\/li>\n<li>DocumentDB Key - The primary password from the \"Connection Strings\" pane in the Azure Portal<\/li>\n<li>Collection ID - The name of the collection to read data from<\/li>\n<\/ul>\n\n<p>And, for reference, here's what my data explorer looks like in CosmosDB (database ID then collection ID):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/7Z4Q7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7Z4Q7.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And the settings in Azure ML Studio to import the data:\n<a href=\"https:\/\/i.stack.imgur.com\/LheXz.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LheXz.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.1,
        "Solution_reading_time":13.02,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":132.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1565289301123,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":79.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What are the different data sources we can import data into Azure Machine Learning Services storage or notebook. I mean from Salesforce or any ERP or any website? As of now I have seen importing data using URL or getting it from data location in storage where notebook will also be stored.<\/p>\n\n<p>I have not got anything to try on. I googled for different methods, but couldn't find relevant link. So I didn't try much there.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554359325097,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1554378019900,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55509207",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":5.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Data sources in Azure Machine Learning Services",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1140.0,
        "Challenge_word_count":82,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545289999703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Thanks for your question. You can import data from Azure Blob, Azure File, ADLS Gen1, ADLS Gen2, Azure SQL, Azure PostgreSQL. \nFor more information: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data<\/a><\/p>\n\n<p>You can create an Azure ML Dataset for your training scenarios. Dataset can be created either from the data store mentioned above or from public urls.\nFor more information: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-register-datasets\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-register-datasets<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.7,
        "Solution_reading_time":10.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nXCom return value of `SageMakerTransformOperatorAsync`  and `SageMakerTrainingOperatorAsync` does not produce the expected output.\r\n\r\nIt seems like some key(s) don't match the non-async operator output.\r\n\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run a dag with traditional operators\r\n2. Run same dag with Async operators\r\n3. Compare outputs\r\n\r\n**Expected behavior**\r\nThe Xcom keys and values should match whatever the traditional non-async version of the operators output.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n",
        "Challenge_closed_time":1666957.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666892144000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/736",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":7.76,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":22.0,
        "Challenge_repo_issue_count":807.0,
        "Challenge_repo_star_count":97.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"XCom Output of Sagemaker Async Operators",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":84,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@bharanidharan14  I tested locally with the branch for the fix and seems to be fixed with your patch. Thank you!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":1.35,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Archived from slack discussion!\n\nI\u2019m receiving the following error recently, but it only occurs when I use a VPN, if I\u2019m in the office I don\u2019t get this issue.\nHas anyone received similar or have any clues on what might be the problem?\n\n File \"\/usr\/local\/opt\/python@3.8\/Frameworks\/Python.framework\/Versions\/3.8\/lib\/python3.8\/ssl.py\", line 1019, in _create\n    self.getpeername()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', OSError(22, 'Invalid argument'))",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667914384000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1519",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.5,
        "Challenge_reading_time":7.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Issue installing polyaxon python client urllib3.exceptions.ProtocolError: OSError",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":62,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Sometimes the CA bundle is not up-to date and needs to be installed manually, especially if it's a new Python version. Please run the following commands:\n\ncd \/Applications\/Python\\ 3.8\/\n\nthen\n\n.\/Install\\ Certificates.command",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":2.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1394547235287,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Warsaw, Poland",
        "Answerer_reputation_count":352.0,
        "Answerer_view_count":22.0,
        "Challenge_adjusted_solved_time":2.3974027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>dvc documentation for <code>dvc gc<\/code> command states, the <code>-r<\/code> option indicates \"Remote storage to collect garbage in\" but I'm not sure if I understand it correctly. For example I execute this command:<\/p>\n\n<pre><code>dvc gc -r myremote\n<\/code><\/pre>\n\n<p>What exactly happens if I execute this command? I have 2 possible answers:<\/p>\n\n<ol>\n<li>dvc checks which files should be deleted, then moves these files to \"myremote\" and then deletes all these files in local cache but not in remote.<\/li>\n<li>dvc checks which files should be deleted and deletes these files both in local cache and \"myremote\"<\/li>\n<\/ol>\n\n<p>Which one of them is correct?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569828438393,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58163305",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":8.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"dvc gc and files in remote cache",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1617.0,
        "Challenge_word_count":107,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522254698710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Russia",
        "Poster_reputation_count":784.0,
        "Poster_view_count":77.0,
        "Solution_body":"<p>one of DVC maintainers here.<\/p>\n\n<p>Short answer: 2. is correct.<\/p>\n\n<p>A bit of additional information:\nPlease be careful when using <code>dvc gc<\/code>. It will clear your cache from all dependencies that are not mentioned in the current HEAD of your git repository. \nWe are working on making <code>dvc gc<\/code> preserving whole history by default. <\/p>\n\n<p>So if you don't want to delete files from your history commits, it would be better to wait for completion of <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2325\" rel=\"nofollow noreferrer\">this<\/a> task.<\/p>\n\n<p>[EDIT]\nPlease see comment below.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1569837069043,
        "Solution_link_count":1.0,
        "Solution_readability":7.5,
        "Solution_reading_time":7.71,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":86.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1340063804276,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":298.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So our team created a new Azure <strong>Machine Learning<\/strong> resource, but whenever I try to add a new notebook and try to edit it using &quot;JUPYTERLAB&quot; i get <code>ERR_HTTP2_PROTOCOL_ERROR<\/code> error, but the same notebook, when edited using <code>EDIT IN JUPYTER<\/code> works perfectly.<\/p>\n<p>This is a blank and clean notebook, I also tried 2 different laptops and multiple browsers per laptop, same error. I also tried incognito and clearing cookies, but to no avail.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/IevSG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IevSG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>update: I seem to have accidentally replicated the issue and I now know what is causing it, the situation is that Im using my work laptop and constantly switching VPN connections, and some times, connecting to the AZURE PORTAl OUTSIDE the VPN. So, when you've worked on a notebook while inside a VPN, then you disconnected, and tried loading the notebook sometime later, you will encounter this<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596642453543,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1597056501367,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63268849",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.2,
        "Challenge_reading_time":14.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"ERR_HTTP2_PROTOCOL_ERROR when opening Notebook in JUPYTERLAB Azure ML Studio",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":246.0,
        "Challenge_word_count":158,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340063804276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":298.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>This problem has stomped me for hours, but I was finally able to fix it. What I did was I opened a terminal and did a Jupyter lab rebuild &quot;jupyter lab build&quot;<\/p>\n<p><a href=\"https:\/\/imgur.com\/aRB8GWS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/imgur.com\/aRB8GWS.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/IceQO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IceQO.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1596669031916,
        "Solution_link_count":4.0,
        "Solution_readability":10.2,
        "Solution_reading_time":6.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is regard to ML Feature Stores, is Feast the recommended option today for Feature Store with Azure ML or is there any other options?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643257477277,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70873347",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":2.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Recommended options for Feature store in Azure ML",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":327.0,
        "Challenge_word_count":32,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1632461310820,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":107.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>We have roadmap to support that is something more native and also tightly integrates into Azure ML.<\/p>\n<p>Here is <a href=\"https:\/\/techcommunity.microsoft.com\/t5\/ai-customer-engineering-team\/bringing-feature-store-to-azure-from-microsoft-azure-redis-and\/ba-p\/2918917\" rel=\"nofollow noreferrer\">doc<\/a> to integration with OSS tool such as Hopsworks\/Feast and leveraging existing functionalities (designer\/pipelines, dataset) for an end-to-end &quot;feature store&quot; solution.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.1,
        "Solution_reading_time":6.52,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1534058291092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":33.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create sagemaker studio project using aws cdk following below steps:<\/p>\n<p>create domain (<a href=\"https:\/\/github.com\/aws-samples\/aws-cdk-sagemaker-studio.git\" rel=\"nofollow noreferrer\">using this example<\/a>)\ncreate user (<a href=\"https:\/\/github.com\/aws-samples\/aws-cdk-sagemaker-studio.git\" rel=\"nofollow noreferrer\">using this example<\/a>)\ncreate jupyter app\ncreate project<\/p>\n<p>Code for creating jupyter app:<\/p>\n<pre><code>\ndef __init__(self, scope: Construct,\n             construct_id: str, *,\n             app_name: str,\n             app_type: str,\n             domain_id: str,\n             user_profile_name: str,\n             depends_on=None, **kwargs) -&gt; None:\n    super().__init__(scope, construct_id)\n\n    sagemaker_jupyter_app = sg.CfnApp(self, construct_id,\n                                      app_name=app_name,\n                                      app_type=app_type,\n                                      domain_id=domain_id,\n                                      user_profile_name=user_profile_name\n                                      )\n    sagemaker_jupyter_app.add_depends_on(depends_on_user_creation)\n<\/code><\/pre>\n<p>Code for creating project:<\/p>\n<pre><code>\ndef __init__(self, scope: Construct,\n             construct_id: str, *,\n             project_name: str,\n             project_description: str,\n             product_id: str,\n             depends_on=None,\n             **kwargs) -&gt; None:\n    super().__init__(scope, construct_id)\n\n    sagemaker_studio_project = sg.CfnProject(self, construct_id,\n                                             project_name=project_name,\n                                             service_catalog_provisioning_details={\n                                                 &quot;ProductId&quot;: &quot;prod-7tjedn5dz4jrw&quot;\n                                             },\n                                             project_description=project_description\n                                             )\n<\/code><\/pre>\n<p>Domain, user, jupyter app all gets created successfully. The problem comes in with project.\nBelow is the error :<\/p>\n<blockquote>\n<p>Resource handler returned message: &quot;Product prod-7tjedn5dz4jrw does\nnot exist or access was denied (Service: SageMaker, Status Code: 400,\nRequest ID: 768116aa-e77b-4691-a972-38b83093fdc4)&quot; (RequestToken:\n45ca2a0c-3f03-e3e0-f29d-d9443ff4dfc1, HandlerErrorCode:\nGeneralServiceException)<\/p>\n<\/blockquote>\n<p>I am running this code from an ec2 instance that has SagemakerFullAccess\nI also tried attaching SagemakerFullAccess execution role with project...but got the same error.\nI have also attached below policy to my domain:<\/p>\n<ul>\n<li>AmazonSageMakerAdmin-ServiceCatalogProductsServiceRolePolicy<\/li>\n<\/ul>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1650541371883,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71953876",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":18.5,
        "Challenge_reading_time":29.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"How to create Sagemaker studio project using aws cdk",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":491.0,
        "Challenge_word_count":193,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1534058291092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":116.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>Basically this was an issue related to IAM.\nRunning cdk program requires bootstrapping it using the command <code>cdk bootstrap<\/code>\nAfter running this command cdk was creating a bunch of roles out of which one role will be related to cloudformation's execution role. Something like<\/p>\n<blockquote>\n<p>cdk-serialnumber-cfn-exec-role-Id-region<\/p>\n<\/blockquote>\n<p>Now this role was used by cloudformation to run the stack.<\/p>\n<p>Using sagemaker from console automatically adds the role associated with domain\/user at<\/p>\n<blockquote>\n<p>ServiceCatalog -&gt; Portfolios -&gt; Imported -&gt; Amazon SageMaker Solutions and ML Ops products -&gt; Groups, roles, and users<\/p>\n<\/blockquote>\n<p>Thats was the reason why product id was accessible from console.<\/p>\n<p>After adding the role created by cdk bootsrap to the above path I was able to run my stack.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":10.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":120.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Azure Machine learning Studio, I have imported a dataset from a locally stored spreadsheet. In the designer, I drag the dataset into the workspace, right click, and select 'Visualize. I get the following error:   <\/p>\n<p>&quot;Unable to visualize this dataset. This might be because your data is stored behind a virtual network or your data does not support profile&quot;. I've searched for hours for a remedy, but find nothing.   <\/p>\n<p>What do I do to fix this error?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":5,
        "Challenge_created_time":1602791972497,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/127980\/error-when-visualizing-dataset-in-microsoft-azure",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Error when Visualizing Dataset in Microsoft Azure Machine Learning Studio",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":88,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=ab00ff52-eb99-4909-97c6-13620f09e957\">@Dana Shields  <\/a> I have tried this scenario with my workspace and i was able to replicate the message you have seen. It looks like you are using the Dataset type as File while creating the dataset which is causing the issue. Please register the dataset as Tabular type and then use the dataset in designer. This should show you the preview of the data. Here is a screen shot from my workspace of the designer.    <\/p>\n<p><img src=\"\/answers\/storage\/temp\/33346-image.png\" alt=\"33346-image.png\" \/>    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":7.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1395431574432,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Islamabad Capital Territory, Pakistan",
        "Answerer_reputation_count":1520.0,
        "Answerer_view_count":214.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I started to work with AWS SageMaker. I have an AWS Starter Account. I have been trying to deploy a built-in algorithm for 2 days but I always get <code>AccessDeniedException<\/code> despite the fact that I created IAM role according to <a href=\"https:\/\/aws.amazon.com\/tr\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/tr\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a><\/p>\n\n<blockquote>\n  <p>ClientError: An error occurred (AccessDeniedException) when calling the CreateTrainingJob operation: User: arn:aws:sts::161745376217:assumed-role\/AmazonSageMaker-ExecutionRole-20200203T194557\/SageMaker is not authorized to perform: sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:us-east-1:161745376217:training-job\/blazingtext-2020-02-03-18-12-14-017 with an explicit deny<\/p>\n<\/blockquote>\n\n<p>Could you help me to solve this problem ?\nThank you so much<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580755584317,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1580790326547,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60045326",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.9,
        "Challenge_reading_time":13.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS SageMaker Access Denied",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":3755.0,
        "Challenge_word_count":86,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1481983208856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>You have created a role for SageMaker to access S3 bucket, but it seems your IAM user doesn't have access to SageMaker service. Please make sure your IAM user has permission to SageMaker.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":2.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to build a machine learning model locally using AWS SageMaker, but I got a validation error on IAM Role name. Although it's the exact role name that I created on the console.<\/p>\n<p>This is my code<\/p>\n<pre><code>    import boto3\n    import sagemaker\n    from sagemaker import get_execution_role\n    from sagemaker.amazon.amazon_estimator import image_uris\n    from sagemaker.amazon.amazon_estimator import RecordSet\n\n    sess = sagemaker.Session()\n\n\n    bucket = sagemaker.Session().default_bucket()\n    prefix = 'sagemaker\/ccard19'\n\n    role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access '\n\n    container = image_uris.retrieve('linear-learner',boto3.Session().region_name)\n    \n    # Some other code\n\n   linear = sagemaker.LinearLearner(role=role,\n                                               instance_count=1,\n                                               instance_type='ml.m4.xlarge',\n                                               predictor_type='binary_classifier')\n  \n  # Some other code\n\n  ### Fit the classifier\n  linear.fit([train_records,val_records,test_records], wait=True, logs='All')\n\n<\/code><\/pre>\n<p>And this is the error message<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: 1 validation error detected: Value 'arn:aws:iam::949010940542:role\/SageMaker-Full-Access ' at 'roleArn' failed to satisfy constraint: Member must satisfy regular expression pattern: ^arn:aws[a-z\\-]*:iam::\\d{12}:role\/?[a-zA-Z_0-9+=,.@\\-_\/]+$\n<\/code><\/pre>\n<p>Any Help please?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617256728180,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66899120",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.7,
        "Challenge_reading_time":19.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Validation error on Role name when running AWS SageMaker linear-learner locally",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":560.0,
        "Challenge_word_count":132,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378039539503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":340.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>You have <strong>space<\/strong> in the name. It should be:<\/p>\n<pre><code>role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access'\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.7,
        "Solution_reading_time":2.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer is using SageMaker Studio in VpcOnly mode (VPC, protected subnets **without** internet access, **NO** NAT gateways). \nThe all functionality is fine. However, when I try create a SageMaker projects - as described [here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-projects-create.html), SageMaker Studio is unable to list the project templates (timeout and unspecified error) resulting in empty list of the available project templates.\n\nProjects are enabled for the users - as described [here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-projects-studio-updates.html). The problem is with project creation.\n\nIs internet access (e.g. via NAT gateways) is needed for SageMaker projects?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615480055000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668618206192,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUcyhpq1pxRTmtjkDRAh_MDA\/sagemaker-studio-projects-in-vpconly-mode-without-internet-access",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":10.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker Studio projects in VpcOnly mode without internet access",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":678.0,
        "Challenge_word_count":91,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Figured it out. SageMaker Studio projects need Service Catalog access and VPCE for `com.amazonaws.${AWS::Region}.servicecatalog`",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925560363,
        "Solution_link_count":0.0,
        "Solution_readability":15.5,
        "Solution_reading_time":1.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":14.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to register a data set via the Azure Machine Learning Studio designer but keep getting an error. Here is my code, used in a &quot;Execute Python Script&quot; module:<\/p>\n<pre><code>import pandas as pd\nfrom azureml.core.dataset import Dataset\nfrom azureml.core import Workspace\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    ws = Workspace.get(name = &lt;my_workspace_name&gt;, subscription_id = &lt;my_id&gt;, resource_group = &lt;my_RG&gt;)\n    ds = Dataset.from_pandas_dataframe(dataframe1)\n    ds.register(workspace = ws,\n                name = &quot;data set name&quot;,\n                description = &quot;example description&quot;,\n                create_new_version = True)\n    return dataframe1, \n<\/code><\/pre>\n<p>But I get the following error in the Workspace.get line:<\/p>\n<pre><code>Authentication Exception: Unknown error occurred during authentication. Error detail: Unexpected polling state code_expired.\n<\/code><\/pre>\n<p>Since I am inside the workspace and in the designer, I do not usually need to do any kind of authentication (or even reference the workspace). Can anybody offer some direction? Thanks!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628037272883,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1628038626927,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68644137",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Studio Designer Error: code_expired",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":321.0,
        "Challenge_word_count":133,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1371499229816,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1111.0,
        "Poster_view_count":191.0,
        "Solution_body":"<p>when you're inside a &quot;Execute Python Script&quot; module or <code>PythonScriptStep<\/code>, the authentication for fetching the workspace is already done for you (unless you're trying to authenticate to different Azure ML workspace.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Run\nrun = Run.get_context()\n\nws = run.experiment.workspace\n<\/code><\/pre>\n<p>You should be able to use that <code>ws<\/code> object to register a Dataset.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":6.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\nAre there any drawbacks I should be aware of if we restrict user access to only a single region? \n\nWe use a variety of AWS services but mainly S3 and Sagemaker Studio. Our team is located in various locations so their default regions are different.  It has been a challenge to keep track of studio instances when they are created in different regions so we are now considering restricting access to a single region. Are there issues that we may face in that case? Any services we may miss?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642700804560,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667921325307,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUxt7fqO9HQrKWDfi4V4Lagg\/pros-and-cons-of-restricting-user-access-to-certain-regions",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":6.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Pros and cons of restricting user access to certain regions",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":100.0,
        "Challenge_word_count":99,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I would take a look at [this](https:\/\/docs.aws.amazon.com\/organizations\/latest\/userguide\/orgs_manage_policies_scps_examples_general.html#example-scp-deny-region) for some potential edge cases. In summary, you may need to allow us-east-1 and us-west-2 in addition to whatever regions your team is in since they host some of the global service endpoints (like IAM, Route 53, Global Accelerator, and a few others). For STS, I would use the [regional endpoints](https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/id_credentials_temp_enable-regions.html) if you aren't already.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1642705784219,
        "Solution_link_count":2.0,
        "Solution_readability":15.2,
        "Solution_reading_time":7.48,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an AWS SageMaker notebook running some ML stuff for work, and I have a private github repo with some of my commonly used functions which is formatted in such a way to be pip install-able, so I set up an SSH key by doing this:<\/p>\n<pre><code>ssh-keygen \n\n-t rsa -b 4096 -C &quot;danielwarfield1@gmail.com&quot;\n<\/code><\/pre>\n<p>enter, enter, enter (default save location no password)<\/p>\n<pre><code>eval $(ssh-agent -s)\nssh-add ~\/.ssh\/id_rs\n<\/code><\/pre>\n<p>then I copy the public key into github, then I run this to install my library<\/p>\n<pre><code>$PWD\/pip install git+ssh:\/\/git@github.com\/...\n<\/code><\/pre>\n<p>where <code>$PWD<\/code> is the directory containing pip for the conda env I'm using (tensorflow2_p36 specifically, the one that AWS provides)<\/p>\n<p>this works fine, until I restart the EC2, then it appears my shh key (along with all my other installs) are lost, and I have to repeat the process. I expect the modules to be lost, I know SageMaker manages the environments, but me loosing my ssh key seems peculiar, is there a place I can save my ssh key wher it wont get lost, but I can still find it when I pip install?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596230563253,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63199239",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":14.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Why are shh keys lost on reboot of AWS ec2 instance (sage maker)?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":278.0,
        "Challenge_word_count":196,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545360696800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Earth",
        "Poster_reputation_count":1011.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>The <code>\/home\/ec2-user\/SageMaker<\/code> location is persisted even when you switch down the notebook instance, you can try saving things here to get them persisted. Things saved elsewhere will be lost when you switch off the instance<\/p>\n<p>Regarding private git integration, you can use the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-repo.html\" rel=\"nofollow noreferrer\">SageMaker git Notebook integration<\/a>, which uses Secrets Manager to safely handle your credentials<\/p>\n<p>You can perform steps automatically when the notebook starts with a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">lifecycle configuration<\/a>. This is useful for example to standardise and automatise copying of data and environment customization<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.4,
        "Solution_reading_time":10.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":90.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"## Expected Behavior\r\nIt seems new version on AzureML extension to VS Code doesn't have this option in settings. I needed to downgrade to 0.6x.\r\n\r\n## Actual Behavior\r\nCurrent version 0.10.0 doesn't have the option. Cannot locally debug or documentation doesn't provide info about that.\r\n\r\n## Specifications\r\n\r\n  - Version: 0.10.0\r\n  - Platform: VS Code, Windows\r\n",
        "Challenge_closed_time":1654701.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654678830000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1589",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":5.42,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Run and debug experiments locally - azureML.CLI Compatibility Mode for CLI v1 - cannot find",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":63,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@michalmar We have completely deprecated the v1 CLI Compatibility mode settings from v0.8.0 onwards and v2 mode will be the way going forward :).",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":1.79,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am struggling to find a way to register multiple gateways. I have a local instance of my SQL server and have created a gateway to access to it from the AML Studio workspace. It works fine but now I would like to access to the same SQL server instance from another workspace. So the question is: how to register a new gateway without removing the previous one?\nI followed this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/use-data-from-an-on-premises-sql-server\" rel=\"nofollow noreferrer\">documentation<\/a>.\nDoes the following explanation mean that there is no way to do that?<\/p>\n\n<blockquote>\n  <p>You can create and set up multiple gateways in Studio for each workspace. For example, you may have a gateway that you want to connect to your test data sources during development, and a different gateway for your production data sources. Azure Machine Learning gives you the flexibility to set up multiple gateways depending upon your corporate environment. Currently you can\u2019t share a gateway between workspaces and only one gateway can be installed on a single computer.<\/p>\n<\/blockquote>\n\n<p>It is quite limiting as connecting to the same server from multiple workspaces may be sometimes crucial.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538466090420,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1538485474347,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52603929",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":16.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"AML Studio: Register mutliple gateways on the same server",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":40.0,
        "Challenge_word_count":192,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1528790837107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Paris, France",
        "Poster_reputation_count":610.0,
        "Poster_view_count":203.0,
        "Solution_body":"<p>Well, finally I have found a way to bypass this limitation. From this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/use-data-from-an-on-premises-sql-server\" rel=\"nofollow noreferrer\">documentation<\/a> I have found that: <\/p>\n\n<blockquote>\n  <p>The IR does not need to be on the same machine as the data source. But staying closer to the data source reduces the time for the gateway to connect to the data source. We recommend that you install the IR on a machine that's different from the one that hosts the on-premises data source so that the gateway and data source don't compete for resources.<\/p>\n<\/blockquote>\n\n<p>So the  logic is pretty simple. You provide access to your local server to another machine on vpn and install your gateway there. Important: I have set up the firewall rules on the server before, to be able to establish the connection remotely.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":11.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":133.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1468179475927,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation_count":795.0,
        "Answerer_view_count":210.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi i am started to learning the azure data lake and azure machine learning ,i need to use the azure data lake storage as a azure machine learning studio input data .There have a any options are there, i gone through the azure data lake and machine learning documentation but i can't reach that,finally i got one solution on this \n<a href=\"https:\/\/stackoverflow.com\/questions\/36127510\/how-to-use-azure-data-lake-store-as-an-input-data-set-for-azure-ml\">link<\/a> but they are mentioning there is no option for it,but this post is old one,so might be the Microsoft people added the future  on it if it's please let me know, let me know Thank you. <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1488899776040,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1495535398003,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42651900",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.6,
        "Challenge_reading_time":8.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to connect Azure Data lake storage to Azure ML?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":4966.0,
        "Challenge_word_count":108,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1487413134923,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gurugram, Haryana, India",
        "Poster_reputation_count":321.0,
        "Poster_view_count":65.0,
        "Solution_body":"<p>I recommend the following:<\/p>\n\n<ul>\n<li>Get a tenant ID, client ID, and client secret for your ADLS using the tutorial <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-lake-store\/data-lake-store-authenticate-using-active-directory#step-2-get-client-id-client-secret-and-tenant-id\" rel=\"nofollow noreferrer\">here<\/a>.<\/li>\n<li>Install the <a href=\"https:\/\/github.com\/Azure\/azure-data-lake-store-python\" rel=\"nofollow noreferrer\"><code>azure-datalake-store<\/code><\/a> Python package on AML Studio by attaching it as a Script Bundle to an Execute Python Script module.<\/li>\n<li>In the Execute Python Script module, import the <code>azure-datalake-store<\/code> package and connect to the ADLS with your tenant ID, client ID, and client secret.<\/li>\n<li>Download the data you need from ADLS and convert it into a dataframe within the Python Script module; return that dataframe to make the data available in the rest of AML Studio.<\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.7,
        "Solution_reading_time":12.34,
        "Solution_score_count":4.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":105.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1443426419048,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Since connecting to Azure SQL database from \u201cExecute R Script\u201d module in \u201cAzure Machine Learning Studio\u201d is not possible, and using Import Data modules (a.k.a Readers) is the only recommended approach, my question is that what can I do when I need more than 2 datasets as input for \"Execute R Script module\"?<\/p>\n\n<pre><code>\/\/ I'm already doing the following to get first 2 datasets,\ndataset1 &lt;- maml.mapInputPort(1)\ndataset2 &lt;- maml.mapInputPort(2)\n<\/code><\/pre>\n\n<p>How can I \"import\" a dataset3?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1491465771163,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/43249220",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.0,
        "Challenge_reading_time":7.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Need more than 2 datasets for \u201cExecute R Script\u201d module in \u201cAzure Machine Learning Studio\u201d",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":337.0,
        "Challenge_word_count":91,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1487901477287,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>One thing you can do is combining two data-sets together and selecting the appropriate fields using the R script. That would be an easy workaround.   <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":1.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1436696527987,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":724.0,
        "Answerer_view_count":87.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>As mentioned in step-3 of <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">this blog by AWS<\/a>, I have created a role to invoke sagemaker endpoint. But, when I deploy the API to a stage, I get &quot;AWS ARN for integration contains invalid action&quot; and I can't deploy the stage.\n<a href=\"https:\/\/i.stack.imgur.com\/maMdl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/maMdl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>blog suggested to select API Gateway under services and to keep on next, but didn't mention which policy will be attached. and also that another inline policy to invoke a specific sagemaker endpoint to be created and attached.\n<a href=\"https:\/\/i.stack.imgur.com\/uQwx0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uQwx0.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>and as mentioned in <a href=\"https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/integration-request-basic-setup.html\" rel=\"nofollow noreferrer\">AWS Docs<\/a>:<\/p>\n<blockquote>\n<p>It must also have API Gateway declared (in the role's trust\nrelationship) as a trusted entity to assume the role.<\/p>\n<\/blockquote>\n<p>my role also have the trust-relationshp:\n<a href=\"https:\/\/i.stack.imgur.com\/VJ9aU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VJ9aU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What's missing in my role that led to the error?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1645719715780,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1645720170123,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71255132",
        "Challenge_link_count":8,
        "Challenge_participation_count":4,
        "Challenge_readability":14.4,
        "Challenge_reading_time":22.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"API Gateway + AWS SageMaker - AWS ARN for integration contains invalid action for integration with sagemaker",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":177.0,
        "Challenge_word_count":174,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559910246180,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":2046.0,
        "Poster_view_count":369.0,
        "Solution_body":"<p>Check in all your API methods that you haven't specified &quot;Use Action Name&quot; for any integration request, and then left the &quot;Action&quot; field blank. If you do the &quot;AWS ARN for integration contains invalid action&quot; error message will be shown.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EXEnQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EXEnQ.png\" alt=\"action type choice\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.5,
        "Solution_reading_time":5.63,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442816236550,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":230.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I just started with DVC. following are the steps I am doing to push my models on S3<\/p>\n<p>Initialize<\/p>\n<pre><code>dvc init\n<\/code><\/pre>\n<p>Add bucket url<\/p>\n<pre><code>dvc remote add -d storage s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>\n<p>add some files<\/p>\n<pre><code>dvc add somefiles\n<\/code><\/pre>\n<p>Add aws keys<\/p>\n<pre><code>dvc remote modify storage access_key_id AWS_ACCESS_KEY_ID\ndvc remote modify storage secret_access_key AWS_SECRET_ACCESS_KEY\n<\/code><\/pre>\n<p>now when I push<\/p>\n<pre><code>dvc push\n<\/code><\/pre>\n<p>it shows<\/p>\n<pre><code>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n<p>Am i missing something?<\/p>\n<p><strong>update1<\/strong><\/p>\n<p>result of <code>dvc doctor<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc doctor\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n<\/code><\/pre>\n<p>and the <code>dvc push-vv<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc push -vv  \n2021-09-21 13:21:38,382 TRACE: Namespace(all_branches=False, all_commits=False, all_tags=False, cd='.', cmd='push', cprofile=False, cprofile_dump=None, func=&lt;class 'dvc.command.data_sync.CmdDataPush'&gt;, glob=False, instrument=False, instrument_open=False, jobs=None, pdb=False, quiet=0, recursive=False, remote=None, run_cache=False, targets=[], verbose=2, version=None, with_deps=False)\n2021-09-21 13:21:39,293 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:39,296 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:40,114 DEBUG: Preparing to transfer data from '.dvc\\cache' to 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,117 DEBUG: Preparing to collect status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,119 DEBUG: Collecting status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,121 DEBUG: Querying 1 hashes via object_exists\n2021-09-21 13:21:44,840 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (InvalidAccessKeyId) when calling the ListObjectsV2 operation: The AWS Access Key Id you provided does not exist in our records.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1080, in _info\n    out = await self._simple_info(path)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 993, in _simple_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: The AWS Access Key Id you provided does not exist in our records.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\base.py&quot;, line 45, in do_run\n    return self.run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\push.py&quot;, line 48, in push\n    pushed += self.cloud.push(obj_ids, jobs, remote=remote, odb=odb)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\data_cloud.py&quot;, line 85, in push\n    return transfer(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\transfer.py&quot;, line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 160, in compare_status\n    dest_exists, dest_missing = status(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 122, in status\n    exists = hashes.intersection(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 48, in _indexed_dir_hashes\n    dir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 415, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 611, in result_iterator\n    yield fs.pop().result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 439, in result\n    return self.__get_result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 406, in exists_with_progress\n    ret = self.fs.exists(path_info)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\fs\\fsspec_wrapper.py&quot;, line 97, in exists\n    return self.fs.exists(self._with_bucket(path_info))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 88, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 69, in sync\n    raise result[0]\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 25, in _runner\n    result[0] = await coro\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 820, in _exists\n    await self._info(path, bucket, key, version_id=version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1084, in _info\n    out = await self._version_aware_info(path, version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1027, in _version_aware_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: Forbidden\n------------------------------------------------------------\n2021-09-21 13:21:45,178 DEBUG: Version info for developers:\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-09-21 13:21:45,185 DEBUG: Analytics is enabled.\n2021-09-21 13:21:45,446 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n2021-09-21 13:21:45,456 DEBUG: Spawned '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1632209467140,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1632236873616,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69265000",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":17.2,
        "Challenge_reading_time":127.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":85,
        "Challenge_solved_time":null,
        "Challenge_title":"DVC - Forbidden: An error occurred (403) when calling the HeadObject operation",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":816.0,
        "Challenge_word_count":719,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1363322632587,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chandigarh, India",
        "Poster_reputation_count":13237.0,
        "Poster_view_count":2675.0,
        "Solution_body":"<p>Could you please run <code>dvc doctor<\/code> and rerun <code>dvc push<\/code> and add <code>-vv<\/code> flag. And give the two results?<\/p>\n<pre><code>PermissionError: The AWS Access Key Id you provided does not exist in our records.\n<\/code><\/pre>\n<p>Does the <code>aws cli<\/code> works correctly for you? First setup <code>AWS_ACCESS_KEY_ID<\/code> and <code>AWS_SECRET_ACCESS_KEY<\/code> in envs then<\/p>\n<pre><code>aws s3 ls s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1632211831430,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":6.01,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1324988509368,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Moscow, Russia",
        "Answerer_reputation_count":1593.0,
        "Answerer_view_count":93.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed an Endpoint on Amazon SageMaker.\nNow I am trying to Invoke it.<\/p>\n\n<p>If I run this code in Sagemaker's Jupyter Notebook: <\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime')\nendpoint_name = 'DEMO-XGBoostEndpoint'\nbody = ','.join(['1.0'] * 6)\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=body)\nresponse['Body'].read()\n<\/code><\/pre>\n\n<p>it works properly.<\/p>\n\n<p>But if I run the same code, with added credentials for boto3 client, from my machine:<\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime', \n                       aws_access_key_id=ACCESS_ID,\n                       aws_secret_access_key= ACCESS_KEY)\nendpoint_name = 'DEMO-XGBoostEndpoint'\nbody = ','.join(['1.0'] * 6)\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                               ContentType='text\/csv',\n                               Body=body)\nresponse['Body'].read()\n<\/code><\/pre>\n\n<p>I get this error:<\/p>\n\n<blockquote>\n  <p>ClientError: An error occurred (AccessDeniedException) when calling the InvokeEndpoint operation: User: arn:aws:iam::249707424405:user\/yury.logachev is not authorized to perform: sagemaker:InvokeEndpoint on resource: arn:aws:sagemaker:us-east-1:249707424405:endpoint\/demo-xgboostendpoint-2018-12-12-22-07-28 with an explicit deny<\/p>\n<\/blockquote>\n\n<p>If I run the latter piece of code (with added credentials as a parameters of client) on Sagemaker's Jupyter Notebook, I also get the same error.<\/p>\n\n<p>I understand that the solution should be linked with roles, policies etc, but could not find out it.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547411152043,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1547664021823,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54172907",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":20.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Amazon Sagemaker. AccessDeniedException when calling the InvokeEndpoint operation",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":2435.0,
        "Challenge_word_count":155,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324988509368,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Moscow, Russia",
        "Poster_reputation_count":1593.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>The problem was with the MFA autharization. \nWhen I invoked the model from inside the model, the MFA was passed. \nBut when I tried to invoke the model from my machine, the MFA was not passed, so the access was denied.<\/p>\n\n<p>I created special user without MFA to debug the model, and that solved my problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":3.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1394547235287,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Warsaw, Poland",
        "Answerer_reputation_count":352.0,
        "Answerer_view_count":22.0,
        "Challenge_adjusted_solved_time":19.9708936111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've recently started to play with <a href=\"https:\/\/dvc.org\" rel=\"nofollow noreferrer\">DVC<\/a>, and I was a bit surprised to see the <a href=\"https:\/\/dvc.org\/doc\/start\/data-and-model-versioning#storing-and-sharing\" rel=\"nofollow noreferrer\">getting started docs<\/a> are suggesting to store <code>.dvc\/config<\/code> in git.<\/p>\n<p>This seemed like a fine idea at first, but then I noticed that my Azure Blob Storage account (i.e. my Azure username) is also stored in .dvc\/config, which means it would end up in git. Making it not ideal for team collaboration scenarios.<\/p>\n<p>What's even less ideal (read: really scary) is that connection strings entered using <code>dvc remote modify blah connection_string ...<\/code> also end up in <code>.dvc\/config<\/code>, making them end up in git and, in the case of open source projects, making them end up in <strong>very<\/strong> interesting places.<\/p>\n<p>Am I doing something obviously wrong? I wouldn't expect the getting started docs to go very deep into security issues, but I wouldn't expect them to store connection strings in source control either.<\/p>\n<p>My base assumption is that I'm misunderstanding\/misconfiguring something, I'd be curious to know what.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635260868803,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69725612",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":16.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Is the default DVC behavior to store connection data in git?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":178,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1250158552416,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Romania",
        "Poster_reputation_count":7916.0,
        "Poster_view_count":801.0,
        "Solution_body":"<p>DVC has few &quot;levels&quot; of config, that can be controlled with proper flag:<\/p>\n<ul>\n<li><code>--local<\/code> - repository level, ignored by git by default - designated for project-scope, sensitive data<\/li>\n<li>project - same as above, not ignored - designated to specify non-sensitive data (it is the default)<\/li>\n<li><code>--global<\/code> \/ <code>--system<\/code> - for common config for more repositories.<\/li>\n<\/ul>\n<p>More information can be found in the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1635332764020,
        "Solution_link_count":1.0,
        "Solution_readability":17.3,
        "Solution_reading_time":7.49,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":62.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>I understand there was a process how to connect to on-prem sql db from Azure ML studio, but with the transition to the new UI, I don't see the option to connect to the gateway. I have it successfully installed and registered in MS Azure, but from Studio it simply does not offer it as a dataset type when using the Import Data module.  <br \/>\nI can't find any documentation regarding the new UI nor any useful guides for this.  <\/p>\n<p>Would anybody know whether this function is still available in the new studio and if so how can an on-prem gateway be connected?  <\/p>\n<p>Thank you,  <br \/>\nVS<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638277834353,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/646058\/new-azure-ml-vs-on-prem-sql",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.3,
        "Challenge_reading_time":7.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"NEW Azure ML vs On-Prem SQL",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":116,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=63e55afc-7396-4eb1-8eec-945a013b20aa\">@sorcrow  <\/a>     <\/p>\n<p>Thanks for reaching out to us. I just got confirmation from the pm of AML, on-prem SQL is not supported in AML yet, but it's now on our plan.     <\/p>\n<p>I will forward your feedback to product team as well.    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":11.1,
        "Solution_reading_time":18.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":152.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I\u2019m using machine learning studio, I have uploaded my file to data, how can I read that, what is the path?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1672151314873,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1143409\/what-is-the-path-to-read-the-data",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.2,
        "Challenge_reading_time":1.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"what is the path to read the data?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=fd6c7c6c-1300-405c-8e72-0e5b3e2887f1\">@peter  <\/a>     <\/p>\n<p>Thanks for reaching out to us in Microsoft Q&amp;A, you can follow below step to check your data path -     <\/p>\n<ol>\n<li> go to the portal and select the data you upload    <\/li>\n<li> Click &quot;Consume&quot; and you will see the way you can use your data     <\/li>\n<\/ol>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/274361-image.png?platform=QnA\" alt=\"274361-image.png\" \/>    <\/p>\n<p>You could leverage it directly by using it name.    <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.6,
        "Solution_reading_time":8.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1435766573232,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":10645.0,
        "Answerer_view_count":1173.0,
        "Challenge_adjusted_solved_time":37311.0782480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am sort of new to Python, so I probably don't understand fully how to exactly import the libraries correctly into Azure ML.<\/p>\n\n<p>I have a bunch of data stored in Table storage which I have local Python code to successfully join all of them as a preparation for the ML experiment. I learned that AzureML environment does not have the Azure-Storage libraries installed, and therefore procceded the steps according <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn955437.aspx\" rel=\"nofollow noreferrer\">this<\/a> to upload a ZIP file containing the Azure-storage libraries that I found under anaconda3\\lib\\site-packages. I took all of the azure directories and shoved them under one single zip file and followed the bottom of the document in the link to upload the zip file as a DataSet and attach the dataset to an Execute Python script node in ML.<\/p>\n\n<p>I am getting errors like this when I try to run the node:<\/p>\n\n<pre><code>requestId = 825883c7ccb74f7e869e68e60d3cd919 errorComponent=Module. taskStatusCode=400. e \"C:\\pyhome\\lib\\ssl.py\", line 856, in send return self._sslobj.write(data) File \"C:\\pyhome\\lib\\ssl.py\", line 581, in write return self._sslobj.write(data)socket.timeout: The write operation timed outDuring handling of the above exception, another exception occurred:Traceback (most recent call last): File \"C:\\pyhome\\lib\\site-packages\\requests\\adapters.py\", line 376, in send timeout=timeout File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 609, in urlopen _stacktrace=sys.exc_info()[2]) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\util\\retry.py\", line 247, in increment raise six.reraise(type(error), error, _stacktrace) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\packages\\six.py\", line 309, in reraise raise value.with_traceback(tb) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 559, in urlopen body=body, headers=headers) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 353, in _make_request conn.request(method, url, **httplib_request_kw) File \"C:\\pyhome\\lib\\http\\client.py\", line 1083, in request self._send_request(method, url, body, headers) File \"C:\\pyhome\\lib\\http\\client.py\", line 1128, in _send_request self.endheaders(body) File \"C:\\pyhome\\lib\\http\\client.py\", line 1079, in endheaders self._send_output(message_body) File \"C:\\pyhome\\lib\\http\\client.py\", line 911, in _send_output self.send(msg) File \"C:\\pyhome\\lib\\http\\client.py\", line 885, in send self.sock.sendall(data) File \"C:\\pyhome\\lib\\ssl.py\", line 886, in sendall v = self.send(data[count:]) File \"C:\\pyhome\\lib\\ssl.py\", line 856, in send return self._sslobj.write(data) File \"C:\\pyhome\\lib\\ssl.py\", line 581, in write return self._sslobj.write(data)requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', timeout('The write operation timed out',))During handling of the above exception, another exception occurred:Traceback (most recent call last): File \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 221, in _perform_request response = self._httpclient.perform_request(request) File \"c:\\temp\\script bundle\\azure\\storage\\_http\\httpclient.py\", line 114, in perform_request proxies=self.proxies) File \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 468, in request resp = self.send(prep, **send_kwargs) File \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 576, in send r = adapter.send(request, **kwargs) File \"C:\\pyhome\\lib\\site-packages\\requests\\adapters.py\", line 426, in send raise ConnectionError(err, request=request)requests.exceptions.ConnectionError: ('Connection aborted.', timeout('The write operation timed out',))During handling of the above exception, another exception occurred:Traceback (most recent call last): File \"C:\\server\\invokepy.py\", line 199, in batch odfs = mod.azureml_main(*idfs) File \"C:\\temp\\fa22884a19884f658d411dc0bdf05715.py\", line 33, in azureml_main data = table_service.query_entities(table_name) File \"c:\\temp\\script bundle\\azure\\storage\\table\\tableservice.py\", line 728, in query_entities resp = self._query_entities(*args, **kwargs) File \"c:\\temp\\script bundle\\azure\\storage\\table\\tableservice.py\", line 795, in _query_entities operation_context=_context) File \"c:\\temp\\script bundle\\azure\\storage\\table\\tableservice.py\", line 1093, in _perform_request return super(TableService, self)._perform_request(request, parser, parser_args, operation_context) File \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 279, in _perform_request raise ex File \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 251, in _perform_request raise AzureException(ex.args[0])azure.common.AzureException: ('Connection aborted.', timeout('The write operation timed out',))Process returned with non-zero exit code \n<\/code><\/pre>\n\n<p>I am not sure what I am doing wrong<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1511629802230,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47488544",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":65.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":62,
        "Challenge_solved_time":null,
        "Challenge_title":"Using Azure Storage libraries in AzureML - Custom python library",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":150.0,
        "Challenge_word_count":471,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340380852680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":73.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>In Azure ML Studio, Python scripts (and R scripts for that matter) run in a sandbox so cannot access resources over the network. See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts?#limitations\" rel=\"nofollow noreferrer\">limitations<\/a>:<\/p>\n<blockquote>\n<p>The Execute Python Script currently has the following limitations:<\/p>\n<ol>\n<li>Sandboxed execution. The Python runtime is currently sandboxed and, as\na result, does not allow access to the network...<\/li>\n<\/ol>\n<\/blockquote>\n<p>So if you want to read from a blob use the separate Import Data module and if you want to write to a blob use the separate Export Data module.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1645949683923,
        "Solution_link_count":1.0,
        "Solution_readability":11.5,
        "Solution_reading_time":8.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"When you have a global variable in the mlflow.yml file (e.g `mlruns: ${USER}\/mlruns`), the global variable is not replaced by its value even if the user has [registered a TemplatedConfigLoader](https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.config.TemplatedConfigLoader.html) in his project. This is due to `get_mlflow_config()` to manually recreate the default ConfigLoader.\r\n\r\nThis is part of the numerous issues that will  be fixed by #66.\r\n\r\n",
        "Challenge_closed_time":1602948.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601411953000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/72",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":9.2,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"mlflow.yml is not parsed properly when using TemplatedConfigLoader",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":64,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1314094431590,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Ghent, Belgium",
        "Answerer_reputation_count":2648.0,
        "Answerer_view_count":181.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm following the automate_model_retraining_workflow example from SageMaker examples, and I'm running that in AWS SageMaker Jupyter notebook. I followed all the steps given in the example for creating the roles and policies.<\/p>\n<p>But when I try to run the following block of code to creat a Glue job, I ran into an error:<\/p>\n<pre><code>glue_script_location = S3Uploader.upload(\n    local_path=&quot;.\/code\/glue_etl.py&quot;,\n    desired_s3_uri=&quot;s3:\/\/{}\/{}&quot;.format(bucket, project_name),\n    sagemaker_session=session,\n)\nglue_client = boto3.client(&quot;glue&quot;)\n\nresponse = glue_client.create_job(\n    Name=job_name,\n    Description=&quot;PySpark job to extract the data and split in to training and validation data sets&quot;,\n    Role=glue_role,  # you can pass your existing AWS Glue role here if you have used Glue before\n    ExecutionProperty={&quot;MaxConcurrentRuns&quot;: 2},\n    Command={&quot;Name&quot;: &quot;glueetl&quot;, &quot;ScriptLocation&quot;: glue_script_location, &quot;PythonVersion&quot;: &quot;3&quot;},\n    DefaultArguments={&quot;--job-language&quot;: &quot;python&quot;},\n    GlueVersion=&quot;1.0&quot;,\n    WorkerType=&quot;Standard&quot;,\n    NumberOfWorkers=2,\n    Timeout=60,\n)\n<\/code><\/pre>\n<blockquote>\n<p>An error occurred (AccessDeniedException) when calling the CreateJob\noperation: User:\narn:aws:sts::############:assumed-role\/AmazonSageMaker-ExecutionRole-############\/SageMaker is not authorized to perform: iam:PassRole on resource:\narn:aws:iam::############:role\/AWS-Glue-S3-Bucket-Access<\/p>\n<\/blockquote>\n<p>This is how AmazonSageMaker-ExecutionPolicy-############ looks like:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;############&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;lambda:CreateFunction&quot;,\n                &quot;glue:UpdateCrawler&quot;,\n                &quot;glue:UpdateTrigger&quot;,\n                &quot;lambda:DeleteFunction&quot;,\n                &quot;glue:DeleteCrawler&quot;,\n                &quot;glue:UpdateSchema&quot;,\n                &quot;lambda:UpdateFunctionCode&quot;,\n                &quot;glue:DeleteConnection&quot;,\n                &quot;glue:UseMLTransforms&quot;,\n                &quot;glue:BatchDeleteConnection&quot;,\n                &quot;lambda:PutProvisionedConcurrencyConfig&quot;,\n                &quot;glue:StartCrawlerSchedule&quot;,\n                &quot;glue:UpdateMLTransform&quot;,\n                &quot;lambda:PublishVersion&quot;,\n                &quot;lambda:DeleteEventSourceMapping&quot;,\n                &quot;glue:CreateMLTransform&quot;,\n                &quot;glue:CreateRegistry&quot;,\n                &quot;glue:StartMLEvaluationTaskRun&quot;,\n                &quot;glue:DeleteTableVersion&quot;,\n                &quot;glue:CreateTrigger&quot;,\n                &quot;glue:BatchDeletePartition&quot;,\n                &quot;glue:StopTrigger&quot;,\n                &quot;glue:CreateUserDefinedFunction&quot;,\n                &quot;glue:StopCrawler&quot;,\n                &quot;lambda:InvokeAsync&quot;,\n                &quot;glue:DeleteJob&quot;,\n                &quot;glue:DeleteDevEndpoint&quot;,\n                &quot;glue:DeleteMLTransform&quot;,\n                &quot;glue:CreateJob&quot;,\n                &quot;glue:ResetJobBookmark&quot;,\n                &quot;glue:CreatePartition&quot;,\n                &quot;lambda:PutFunctionCodeSigningConfig&quot;,\n                &quot;glue:UpdatePartition&quot;,\n                &quot;glue:RegisterSchemaVersion&quot;,\n                &quot;glue:ResumeWorkflowRun&quot;,\n                &quot;lambda:UpdateEventSourceMapping&quot;,\n                &quot;lambda:UpdateFunctionCodeSigningConfig&quot;,\n                &quot;lambda:UpdateFunctionConfiguration&quot;,\n                &quot;glue:StartMLLabelingSetGenerationTaskRun&quot;,\n                &quot;lambda:UpdateCodeSigningConfig&quot;,\n                &quot;glue:CreateDatabase&quot;,\n                &quot;glue:BatchDeleteTableVersion&quot;,\n                &quot;lambda:DeleteAlias&quot;,\n                &quot;glue:DeleteSchemaVersions&quot;,\n                &quot;glue:BatchCreatePartition&quot;,\n                &quot;glue:CreateClassifier&quot;,\n                &quot;glue:UpdateTable&quot;,\n                &quot;lambda:DeleteProvisionedConcurrencyConfig&quot;,\n                &quot;glue:DeleteTable&quot;,\n                &quot;glue:DeleteWorkflow&quot;,\n                &quot;glue:DeleteSchema&quot;,\n                &quot;glue:UpdateWorkflow&quot;,\n                &quot;glue:CreateScript&quot;,\n                &quot;glue:StartWorkflowRun&quot;,\n                &quot;glue:StopCrawlerSchedule&quot;,\n                &quot;lambda:UpdateFunctionEventInvokeConfig&quot;,\n                &quot;lambda:DeleteFunctionCodeSigningConfig&quot;,\n                &quot;glue:UpdateDatabase&quot;,\n                &quot;glue:CreateTable&quot;,\n                &quot;lambda:InvokeFunction&quot;,\n                &quot;glue:BatchStopJobRun&quot;,\n                &quot;glue:DeleteUserDefinedFunction&quot;,\n                &quot;glue:CreateConnection&quot;,\n                &quot;glue:CreateCrawler&quot;,\n                &quot;lambda:UpdateAlias&quot;,\n                &quot;glue:DeleteSecurityConfiguration&quot;,\n                &quot;glue:CreateSchema&quot;,\n                &quot;glue:StartJobRun&quot;,\n                &quot;glue:BatchDeleteTable&quot;,\n                &quot;glue:UpdateClassifier&quot;,\n                &quot;glue:CreateWorkflow&quot;,\n                &quot;glue:DeletePartition&quot;,\n                &quot;lambda:CreateAlias&quot;,\n                &quot;glue:CreateSecurityConfiguration&quot;,\n                &quot;glue:PutWorkflowRunProperties&quot;,\n                &quot;glue:DeleteDatabase&quot;,\n                &quot;glue:RemoveSchemaVersionMetadata&quot;,\n                &quot;lambda:PublishLayerVersion&quot;,\n                &quot;lambda:CreateEventSourceMapping&quot;,\n                &quot;glue:StartTrigger&quot;,\n                &quot;glue:DeleteRegistry&quot;,\n                &quot;lambda:PutFunctionConcurrency&quot;,\n                &quot;lambda:DeleteCodeSigningConfig&quot;,\n                &quot;glue:ImportCatalogToGlue&quot;,\n                &quot;glue:PutDataCatalogEncryptionSettings&quot;,\n                &quot;glue:UpdateRegistry&quot;,\n                &quot;glue:StartCrawler&quot;,\n                &quot;lambda:DeleteLayerVersion&quot;,\n                &quot;lambda:PutFunctionEventInvokeConfig&quot;,\n                &quot;glue:UpdateJob&quot;,\n                &quot;lambda:DeleteFunctionEventInvokeConfig&quot;,\n                &quot;lambda:CreateCodeSigningConfig&quot;,\n                &quot;glue:StartImportLabelsTaskRun&quot;,\n                &quot;glue:DeleteClassifier&quot;,\n                &quot;glue:StartExportLabelsTaskRun&quot;,\n                &quot;glue:UpdateUserDefinedFunction&quot;,\n                &quot;glue:CancelMLTaskRun&quot;,\n                &quot;glue:StopWorkflowRun&quot;,\n                &quot;glue:PutSchemaVersionMetadata&quot;,\n                &quot;glue:UpdateCrawlerSchedule&quot;,\n                &quot;glue:UpdateConnection&quot;,\n                &quot;glue:CreateDevEndpoint&quot;,\n                &quot;glue:UpdateDevEndpoint&quot;,\n                &quot;lambda:DeleteFunctionConcurrency&quot;,\n                &quot;glue:DeleteTrigger&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Sid&quot;: &quot;VisualEditor1&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:PutObject&quot;,\n                &quot;s3:GetObject&quot;,\n                &quot;iam:PassRole&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:DeleteObject&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::*&quot;,\n                &quot;arn:aws:iam::############:role\/query_training_status-role&quot;\n            ]\n        }\n    ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628668794403,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68738148",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":46.2,
        "Challenge_reading_time":87.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker is not authorized to perform: iam:PassRole",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1684.0,
        "Challenge_word_count":274,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1450889293150,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Finland",
        "Poster_reputation_count":398.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>It's clear from the IAM policy that you've posted that you're only allowed to do an <code>iam:PassRole<\/code> on <code>arn:aws:iam::############:role\/query_training_status-role<\/code> while Glue is trying to use the <code>arn:aws:iam::############:role\/AWS-Glue-S3-Bucket-Access<\/code>. So you'll just need to update your IAM policy to allow <code>iam:PassRole<\/code> role as well for the other role.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":5.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1401427814950,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":749.0,
        "Answerer_view_count":50.0,
        "Challenge_adjusted_solved_time":19.2463625,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have two computers: Ubuntu1 and Ubuntu2. \nUbuntu1 runs MongoDB with database Sacred3. \nI want to connect from U2 to U1 via ssh and store there my experiment results.<\/p>\n\n<p>What I tried and failed:\n1. I installed mongo DB, created sacred3, I have ssh key to it. \nI edited <code>\/etc\/mongod.conf<\/code> adding:<\/p>\n\n<p><code># network interfaces\nnet:\n  port: 27017\n  bindIp: 0.0.0.0<\/code><\/p>\n\n<p>Then I enabled port forwarding with<\/p>\n\n<p><code>ssh -fN  -i ~\/.ssh\/sacred_key-pair.pem -L 6666:localhost:27017 ubuntu@106.969.696.969<\/code> \/\/ (with proper ip)<\/p>\n\n<p>so, as I undertstand, if I connect to my localhost:6666 it will be forwarded to 106.969.696.969:27017 <\/p>\n\n<p>So after that, I'm runnig an experiment with <a href=\"https:\/\/sacred.readthedocs.io\/en\/stable\/observers.html\" rel=\"nofollow noreferrer\">Sacred framework<\/a>:<\/p>\n\n<p>python exp1.py -m localhost:6666:sacred3<\/p>\n\n<p>and this should write experiment to remote DB, HOWEVER i I get:<\/p>\n\n<p><code>pymongo.errors.ServerSelectionTimeoutError: localhost:27017: [Errno 111] Connection refused<\/code><\/p>\n\n<p>which is driving me mad. please help!<\/p>\n\n#\n\n<p>below contents of exp1.py:<\/p>\n\n<pre><code>from sacred import Experiment\nfrom sacred.observers import MongoObserver\n\nex = Experiment()\nex.observers.append(MongoObserver.create())\n\ndef compute():\n    summ = layer1 - layer2\n    return summ\n\n\n@ex.config\ndef my_config():\n\n    hp_list = [{\"neurons\" : [32,32] , \"dropout\": 1.0},\n            {\"neurons\" : [32,32] , \"dropout\": 0.7},\n            {\"neurons\" : [32,16] , \"dropout\": 0.9},\n            {\"neurons\" : [24,16] , \"dropout\": 0.9},\n            {\"neurons\" : [24,8] , \"dropout\":  0.9},\n            {\"neurons\" : [16,8] , \"dropout\":  0.9},\n            {\"neurons\" : [64,64] , \"dropout\": 0.9},\n            {\"neurons\" : [64,64] , \"dropout\": 0.7},\n            {\"neurons\" : [64,32] , \"dropout\": 0.9},\n            {\"neurons\" : [64,32] , \"dropout\": 0.7},\n            {\"neurons\" : [48,32] , \"dropout\": 0.9},\n            {\"neurons\" : [48,32] , \"dropout\": 0.7},\n            {\"neurons\" : [48,16] , \"dropout\": 0.9},\n            {\"neurons\" : [48,16] , \"dropout\": 0.7},]\n\n    n_epochs = 2 \n\n\n@ex.capture\ndef training_loop(hp_list, n_epochs):\n    for j in hp_list:\n        print(\"Epoch: \", n_epochs)\n#       layer1 = random.randint(18,68)\n#       layer2 = random.randint(18,68)\n#       layer3 = random.randint(18,68)\n        layer1 = j[\"neurons\"][0]\n        layer2 = j[\"neurons\"][1]\n        dropout_ratio = j[\"dropout\"]\n\n\n        print(\"WHATS UUUUUP\",j, layer1, layer2, dropout_ratio, sep=\"_\")\n        # vae_training_loop_NN_DO(i, layer1, layer2, dropout_ratio )\n\n\n@ex.automain\ndef my_main():\n    training_loop()\n\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":4,
        "Challenge_created_time":1571145034667,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1571149003303,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58395547",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":8.3,
        "Challenge_reading_time":31.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":null,
        "Challenge_title":"How to save data to remote mongoDB via ssh tunnel? (connection refused)",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":329.0,
        "Challenge_word_count":266,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501710879087,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Warszawa, Polska",
        "Poster_reputation_count":300.0,
        "Poster_view_count":42.0,
        "Solution_body":"<p>According to the documentation <a href=\"https:\/\/sacred.readthedocs.io\/en\/stable\/observers.html\" rel=\"nofollow noreferrer\">supplied<\/a>, it looks like you're creating two observers, or overriding the connection argument you passed with <code>-m<\/code>, with the <code>MongoObserver.create()<\/code>specified in the code which uses the default mongo host and port <code>localhost:27017<\/code>. You either supply the observer connection via the <code>-m<\/code> argument or in code, not both.<\/p>\n\n<p>Try removing the <code>MongoObserver.create()<\/code> line altogether, or hardcoding the connection arguments: <code>MongoObserver(url='localhost:6666', db_name='sacred3')<\/code> <\/p>\n\n<p>Also, it looks like your mongo host is <a href=\"https:\/\/serverfault.com\/questions\/489192\/ssh-tunnel-refusing-connections-with-channel-2-open-failed\">not liking the binding to localhost<\/a> so you should also replace <code>localhost<\/code> in your ssh command with <code>127.0.0.1<\/code> or <code>[::1]<\/code>, e.g <code>ssh -fN -i ~\/.ssh\/sacred_key-pair.pem -L 6666:127.0.0.1:27017 ubuntu@106.969.696.969<\/code> or <code>ssh -fN -i ~\/.ssh\/sacred_key-pair.pem -L 6666:[::1]:27017 ubuntu@106.969.696.969<\/code><\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1571218290208,
        "Solution_link_count":2.0,
        "Solution_readability":12.9,
        "Solution_reading_time":15.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":113.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":1255292583096,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Udaipur, Rajasthan, India",
        "Answerer_reputation_count":119996.0,
        "Answerer_view_count":13669.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I've created some <a href=\"https:\/\/studio.azureml.net\" rel=\"nofollow\">Azure Machine Learning<\/a> Workspaces and associated them with \"classic\" storage accounts; but would like to have them associated with \"not-classic\" (or whatever the term is) storage accounts.<\/p>\n\n<p>Is there a way to convert the storage accounts from \"classic\", or to change the storage account associated with a Machine Learning Workspace?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1463237356133,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37228077",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":10.2,
        "Challenge_reading_time":5.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Converting Azure \"classic\" storage accounts",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1254.0,
        "Challenge_word_count":59,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1299959670312,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United States",
        "Poster_reputation_count":41475.0,
        "Poster_view_count":1912.0,
        "Solution_body":"<p>As of today, there's no automatic way of converting a \"Classic\" storage account into \"Azure Resource Manager (ARM)\" storage account. Today, you would need to copy data from a classic storage account to a new storage account.<\/p>\n\n<p>Having said that, there's no difference in how the data is stored in both kinds of storage accounts. Both of them support connecting via account name\/key and\/or shared access signature. The difference is how these storage account themselves are managed. In ARM storage accounts, you can assign granular role-based access control (RBAC) to control what a user can do as far as managing the storage accounts (like updating, deleting, viewing\/regenerating keys).<\/p>\n\n<p>Regarding your question about using new storage accounts with ML workspace, I don't think it's possible today (I may be wrong though). Reason being, ML is still managed via old portal which doesn't have the capability to manage ARM storage accounts.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.5,
        "Solution_reading_time":11.85,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":149.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1656427742040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to test the remote connection of a Python data-science client with SQL Server Machine Learning Services following this guide: <a href=\"https:\/\/docs.microsoft.com\/en-us\/sql\/machine-learning\/python\/setup-python-client-tools-sql\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/sql\/machine-learning\/python\/setup-python-client-tools-sql<\/a> (section 6).\nRunning the following script<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def send_this_func_to_sql():\n    from revoscalepy import RxSqlServerData, rx_import\n    from pandas.tools.plotting import scatter_matrix\n    import matplotlib.pyplot as plt\n    import io\n    \n    # remember the scope of the variables in this func are within our SQL Server Python Runtime\n    connection_string = &quot;Driver=SQL Server;Server=localhost\\instance02;Database=testmlsiris;Trusted_Connection=Yes;&quot;\n    \n    # specify a query and load into pandas dataframe df\n    sql_query = RxSqlServerData(connection_string=connection_string, sql_query = &quot;select * from iris_data&quot;)\n    df = rx_import(sql_query)\n    \n    scatter_matrix(df)\n    \n    # return bytestream of image created by scatter_matrix\n    buf = io.BytesIO()\n    plt.savefig(buf, format=&quot;png&quot;)\n    buf.seek(0)\n    \n    return buf.getvalue()\n\nnew_db_name = &quot;testmlsiris&quot;\nconnection_string = &quot;driver={sql server};server=sqlrzs\\instance02;database=%s;trusted_connection=yes;&quot; \n\nfrom revoscalepy import RxInSqlServer, rx_exec\n\n# create a remote compute context with connection to SQL Server\nsql_compute_context = RxInSqlServer(connection_string=connection_string%new_db_name)\n\n# use rx_exec to send the function execution to SQL Server\nimage = rx_exec(send_this_func_to_sql, compute_context=sql_compute_context)[0]\n<\/code><\/pre>\n<p>yields the following error message returned by rx_exec (stored in the <em>image<\/em> variable)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>connection_string: &quot;driver={sql server};server=sqlrzs\\instance02;database=testmlsiris;trusted_connection=yes;&quot;\nnum_tasks: 1\nexecution_timeout_seconds: 0\nwait: True\nconsole_output: False\nauto_cleanup: True\npackages_to_load: []\ndescription: &quot;sqlserver&quot;\nversion: &quot;1.0&quot;\nXXX lineno: 2, opcode: 0\nTraceback (most recent call last):\n  File &quot;&lt;string&gt;&quot;, line 3, in &lt;module&gt;\n  File &quot;E:\\SQL\\MSSQL15.INSTANCE02\\PYTHON_SERVICES\\lib\\site-packages\\revoscalepy\\computecontext\\RxInSqlServer.py&quot;, line 664, in rx_sql_satellite_pool_call\n    exec(inputfile.read())\n  File &quot;&lt;string&gt;&quot;, line 34, in &lt;module&gt;\n  File &quot;E:\\SQL\\MSSQL15.INSTANCE02\\PYTHON_SERVICES\\lib\\site-packages\\revoscalepy\\computecontext\\RxInSqlServer.py&quot;, line 886, in rx_remote_call\n    results = rx_resumeexecution(state_file = inputfile, patched_server_name=args[&quot;hostname&quot;])\n  File &quot;E:\\SQL\\MSSQL15.INSTANCE02\\PYTHON_SERVICES\\lib\\site-packages\\revoscalepy\\computecontext\\RxInSqlServer.py&quot;, line 135, in rx_resumeexecution\n    return _state[&quot;function&quot;](**_state[&quot;args&quot;])\n  File &quot;C:\\Users\\username\\sendtosql.py&quot;, line 2, in send_this_func_to_sql\nSystemError: unknown opcode\n====== sqlrzs ( process 0 ) has started run at 2022-06-29 13:47:04 W. Europe Daylight Time ======\n{'local_state': {}, 'args': {}, 'function': &lt;function send_this_func_to_sql at 0x0000020F5810F1E0&gt;}\n<\/code><\/pre>\n<p>What is going wrong here? Line 2 in the script is just an import (which works when testing Python scripts on SQL Server directly). Any help is appreciated - thanks.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1656491540923,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1656503806376,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72798225",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":17.5,
        "Challenge_reading_time":48.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"Remote Connection fails in setup of Python data-science client for SQL Server Machine Learning Services",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":54.0,
        "Challenge_word_count":291,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1656427742040,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>I just figured out the reason. As of today, the Python versions for the data clients in <a href=\"https:\/\/docs.microsoft.com\/de-de\/sql\/machine-learning\/python\/setup-python-client-tools-sql?view=sql-server-ver15\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/de-de\/sql\/machine-learning\/python\/setup-python-client-tools-sql?view=sql-server-ver15<\/a> are not the newest (revoscalepy Version 9.3), while the version of Machine Learning Services that we have running in our SQL Server is already 9.4.7.\nHowever, the revoscalepy libraries for the client and server must be the same, otherwise the deserialization fails server-sided.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.2,
        "Solution_reading_time":8.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1430233500800,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":212.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":21511.1452602778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I suspect this has to more to do with IAM roles than Sagemaker.<\/p>\n\n<p>I'm following the example <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"noreferrer\">here<\/a><\/p>\n\n<p>Specifically, when it makes this call<\/p>\n\n<pre><code>tf_estimator.fit('s3:\/\/bucket\/path\/to\/training\/data')\n<\/code><\/pre>\n\n<p>I get this error<\/p>\n\n<pre><code>ClientError: An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:sts::013772784144:assumed-role\/AmazonSageMaker-ExecutionRole-20181022T195630\/SageMaker is not authorized to perform: iam:GetRole on resource: role SageMakerRole\n<\/code><\/pre>\n\n<p>My notebook instance has an IAM role attached to it.\nThat role has the <code>AmazonSageMakerFullAccess<\/code> policy. It also has a custom policy that looks like this<\/p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:GetObject\",\n            \"s3:PutObject\",\n            \"s3:DeleteObject\",\n            \"s3:ListBucket\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::*\"\n        ]\n    }\n]\n<\/code><\/pre>\n\n<p>}<\/p>\n\n<p>My input files and .py script is in an s3 bucket with the phrase <code>sagemaker<\/code> in it.<\/p>\n\n<p>What else am I missing?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1542853625410,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53423061",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.6,
        "Challenge_reading_time":16.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I make this IAM role error in aws sagemaker go away?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":8160.0,
        "Challenge_word_count":130,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1319234288808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4966.0,
        "Poster_view_count":304.0,
        "Solution_body":"<p>If you're running the example code on a SageMaker notebook instance, you can use the execution_role which has the <code>AmazonSageMakerFullAccess<\/code> attached.<\/p>\n<pre><code>from sagemaker import get_execution_role\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n<\/code><\/pre>\n<p>And you can pass this role when initializing <code>tf_estimator<\/code>.\nYou can check out the example <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html\" rel=\"nofollow noreferrer\">here<\/a> for using <code>execution_role<\/code> with S3 on notebook instance.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1620293748347,
        "Solution_link_count":1.0,
        "Solution_readability":18.4,
        "Solution_reading_time":8.09,
        "Solution_score_count":8.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":57.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1614711784088,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/edge-device-fleet-create.html#edge-device-fleet-create-console\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/edge-device-fleet-create.html#edge-device-fleet-create-console<\/a> docs to create device fleet. In this console, Role ARN is optional but it throws <code>RoleARN is required<\/code>. If I provide proper RoleArn it throws <code>Failed to create\/modify RoleAlias. Check your IAM role permission<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/84UEW.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/84UEW.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wKuTV.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wKuTV.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I have no idea what is going wrong. Any hint would be appreciable.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607600991993,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65233943",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":12.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to create Device Fleet",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":79,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426675778223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Coimbatore, Tamil Nadu, India",
        "Poster_reputation_count":10189.0,
        "Poster_view_count":1471.0,
        "Solution_body":"<p>Mohamed, this means that Sagemaker Edge Manager was unable to use the RoleAlias you provided to take the necessary actions when creating a DeviceFleet. It needs to have the AmazonSageMakerEdgeDeviceFleetPolicy attached (or have similar permissions granted) and it needs to trust both SageMaker and IoT Core.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":3.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I already post my problem <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/aff7df3f-afbc-4abc-8fb0-5597184fa6c1\/export-data-blob-storage-v2?forum=MachineLearning\" rel=\"nofollow noreferrer\">here<\/a> and they suggested me to post it here.\nI am trying to export data from Azure ML to Azure Storage but I have this error:<\/p>\n\n<p>Error writing to cloud storage: The remote server returned an error: (400) Bad Request.. Please check the url. . ( Error 0151 )<\/p>\n\n<p>My blob storage configuration is Storage v2 \/ Standard and  Require secure transfer set as enabled.<\/p>\n\n<p>If I set the Require secure transfer set as disabled, the export works fine.<\/p>\n\n<p><strong>How can I export data to my blob storage with the require secure transfer set as enabled ?<\/strong><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1550223149923,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1550223449368,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54706312",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":10.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML studio export data Azure Storage V2",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":819.0,
        "Challenge_word_count":109,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521189557296,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":160.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>According to the offical tutorial <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-to-azure-blob-storage\" rel=\"nofollow noreferrer\"><code>Export to Azure Blob Storage<\/code><\/a>, there are two authentication types for exporting data to Azure Blob Storage: SAS and Account. The description for them as below.<\/p>\n\n<blockquote>\n  <ol start=\"4\">\n  <li><p>For <strong>Authentication type<\/strong>, choose <strong>Public (SAS URL)<\/strong> if you know that the storage supports access via a SAS URL.<\/p>\n  \n  <p>A SAS URL is a special type of URL that can be generated by using an Azure storage utility, and is available for only a limited time. It contains all the information that is needed for authentication and download.<\/p>\n  \n  <p>For <strong>URI<\/strong>, type or paste the full URI that defines the account and the public blob.<\/p><\/li>\n  <li><p>For private accounts, choose <strong>Account<\/strong>, and provide the account name and the account key, so that the experiment can write to the storage account.<\/p>\n  \n  <ul>\n  <li><p><strong>Account name<\/strong>: Type or paste the name of the account where you want to save the data. For example, if the full URL of the storage account is <a href=\"http:\/\/myshared.blob.core.windows.net\" rel=\"nofollow noreferrer\">http:\/\/myshared.blob.core.windows.net<\/a>, you would type myshared.<\/p><\/li>\n  <li><p><strong>Account key<\/strong>: Paste the storage access key that is associated with the account.<\/p><\/li>\n  <\/ul><\/li>\n  <\/ol>\n<\/blockquote>\n\n<p>I try to use a simple module combination as the figure and Python code below to test the issue you got.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9XXPV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9XXPV.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<pre><code>import pandas as pd\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    dataframe1 = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})\n    return dataframe1,\n<\/code><\/pre>\n\n<p>When I tried to use the authentication type <code>Account<\/code> of my Blob Storage V2 account, I got the same issue as yours which the error code is <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/errors\/error-0151\" rel=\"nofollow noreferrer\">Error 0151<\/a> as below via click the <code>View error log<\/code> Button under the link of <code>View output log<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/TFQgO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TFQgO.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<blockquote>\n  <p><strong>Error 0151<\/strong><\/p>\n  \n  <p>There was an error writing to cloud storage. Please check the URL.<\/p>\n  \n  <p>This error in Azure Machine Learning occurs when the module tries to write data to cloud storage but the URL is unavailable or invalid.<\/p>\n  \n  <p><strong>Resolution<\/strong>\n  Check the URL and verify that it is writable.<\/p>\n  \n  <p><strong>Exception Messages<\/strong><\/p>\n  \n  <ul>\n  <li>Error writing to cloud storage (possibly a bad url).<\/li>\n  <li>Error writing to cloud storage: {0}. Please check the url.<\/li>\n  <\/ul>\n<\/blockquote>\n\n<p>Based on the error description above, the error should be caused by the blob url with SAS incorrectly generated by the <code>Export Data<\/code> module code with account information. May I think the code is old and not compatible with the new V2 storage API or API version information. You can report it to <code>feedback.azure.com<\/code>.<\/p>\n\n<p>However, I switched to use <code>SAS<\/code> authentication type to type a blob url with a SAS query string of my container which I generated via <a href=\"https:\/\/azure.microsoft.com\/en-us\/features\/storage-explorer\/\" rel=\"nofollow noreferrer\">Azure Storage Explorer<\/a> tool as below, it works fine.<\/p>\n\n<p>Fig 1: Right click on the container of your Blob Storage account, and click the <code>Get Shared Access Signature<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/1fyFL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1fyFL.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 2: Enable the permission <code>Write<\/code> (recommended to use UTC timezone) and click <code>Create<\/code> button<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/IsbQQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IsbQQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 3: Copy the <code>Query string<\/code> value, and build a blob url with a container SAS query string like <code>https:\/\/&lt;account name&gt;.blob.core.windows.net\/&lt;container name&gt;\/&lt;blob name&gt;&lt;query string&gt;<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RGnxn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RGnxn.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Note: The blob must be not exist in the container, otherwise an <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/errors\/error-0057\" rel=\"nofollow noreferrer\">Error 0057<\/a> will be caused.<\/em><\/strong><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":17.0,
        "Solution_readability":10.9,
        "Solution_reading_time":65.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":42.0,
        "Solution_word_count":563.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1289236634983,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":315.0,
        "Answerer_view_count":82.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am moving data into Azure Data Lake Store and processing it using Azure Data Lake Analytics. Data is in form of XML and I am reading it through <a href=\"https:\/\/github.com\/Azure\/usql\/tree\/master\/Examples\/DataFormats\/Microsoft.Analytics.Samples.Formats\" rel=\"nofollow\">XML Extractor<\/a>. Now I want to access this data from Azure ML and it looks like Azure Data Lake store is not directly supported at the moment. <\/p>\n\n<p>What are the possible ways to use Azure Data Lake Store with Azure ML?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1458553338210,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36127510",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":7.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use Azure Data Lake Store as an input data set for Azure ML?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":963.0,
        "Challenge_word_count":87,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1274637971227,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pakistan",
        "Poster_reputation_count":838.0,
        "Poster_view_count":101.0,
        "Solution_body":"<p>Right now, Azure Data Lake Store is not a supported source, as you note.  That said, Azure Data Lake Analytics can also be used to write data out to Azure Blob Store, and so you can use that as an approach to process the data in U-SQL and then stage it for Azure Machine Learning to process it from Blob store.  When Azure ML supports Data Lake store, then you can switch that over. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":4.61,
        "Solution_score_count":4.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1516367794196,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":1055.1677944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>System information\nOS Platform and Distribution: Windows 10\nMLflow installed: using pip\nMLflow version: version 1.24.0\n**Python version: Python 3.9.7 **<\/p>\n<p>Describe the problem\nI have created a docker-compose system with a backend\/artifact storages, mlflow server and nginx to add an authentication layer.<\/p>\n<pre><code>...\nmlflow:\n        restart: always\n        build: .\n        environment:\n            - AWS_ACCESS_KEY_ID=${MINIO_USR}\n            - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       \n        expose:\n            - '5000'\n        networks:\n            - frontend\n            - backend\n        depends_on:\n            - storage                       \n        image: 'mlflow:Dockerfile'\n        container_name: mlflow_server_nginx\n\n    nginx:\n        restart: always\n        build: .\/nginx\n        container_name: mlflow_nginx\n        ports:\n            - 5043:443\n        links:\n            - mlflow:mlflow\n        volumes:\n            - 'path\/to\/nginx\/auth:\/etc\/nginx\/conf.d'\n            - 'path\/to\/nginx\/nginx.conf:\/etc\/nginx\/nginx.conf:ro'\n        networks:\n            - frontend\n        depends_on:\n            - mlflow\n<\/code><\/pre>\n<p>I have created an user\/password via htpasswd and a custom SSL CA (.pem\/.key) using openssl and my-mlflow.com server-name.<\/p>\n<p>When the docker-compose system is built i can access to mlflow UI via my browser. But when i try to create a new experiment using python trying diferent approaches, i get next errors:\nExecuted code 1:<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\n#os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n#os.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate (_ssl.c:1108)')))\n<\/code><\/pre>\n<p>After read some notes in the documentation and realated issues I tryed next<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\n#os.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\nos.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLError(9, '[SSL] PEM lib (_ssl.c:4012)')))\n<\/code><\/pre>\n<p>Finally<\/p>\n<pre><code># Setting the requried environment variables\nos.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https:\/\/localhost:9000'\nos.environ['AWS_ACCESS_KEY_ID'] = 'user'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'password'\n# Set username and password for added authentication\n#os.environ['MLFLOW_TRACKING_URI '] = 'https:\/\/localhost:5043\/'\n#os.environ['MLFLOW_TRACKING_USERNAME '] = 'user'\n#os.environ['MLFLOW_TRACKING_PASSWORD '] = 'password'\nos.environ['MLFLOW_TRACKING_SERVER_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n#os.environ['MLFLOW_TRACKING_CLIENT_CERT_PATH'] = 'path\/to\/nginx\/auth\/domain.pem'\n# MLflow enviroment\nremote_server_uri = &quot;https:\/\/user:password@localhost:5043\/&quot; # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\n\nmlflow.set_experiment(&quot;MLflow_demo&quot;)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>MlflowException: API request to https:\/\/user:password@localhost:5043\/api\/2.0\/mlflow\/experiments\/list failed with exception HTTPSConnectionPool(host='localhost', port=5043): Max retries exceeded with url: \/api\/2.0\/mlflow\/experiments\/list?view_type=ALL (Caused by SSLError(SSLCertVerificationError(&quot;hostname 'localhost' doesn't match '*.my-mlflow.com'&quot;)))\n<\/code><\/pre>\n<p>Can you give me some hints about how to solve it?<\/p>\n<p>Thank you very much!\nFernando....<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648650339347,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71679081",
        "Challenge_link_count":12,
        "Challenge_participation_count":1,
        "Challenge_readability":18.4,
        "Challenge_reading_time":68.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I connect mlflow server via nginx ssl authentication?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":625.0,
        "Challenge_word_count":377,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1580841805372,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seville, Spain",
        "Poster_reputation_count":33.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>You can set:<\/p>\n<pre><code>os.environ['MLFLOW_TRACKING_INSECURE_TLS'] = 'true'\n<\/code><\/pre>\n<p>And then try to get your cert-chain straight from there for production use.<\/p>\n<p>Also see Documentation: <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#id19\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tracking.html#id19<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1652448943407,
        "Solution_link_count":2.0,
        "Solution_readability":14.1,
        "Solution_reading_time":4.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":26.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In the API docs about <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.html#\" rel=\"nofollow noreferrer\"><code>kedro.io<\/code><\/a> and <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.contrib.io.html\" rel=\"nofollow noreferrer\"><code>kedro.contrib.io<\/code><\/a> I could not find info about how to read\/write data from\/to network attached storage such as e.g. <a href=\"https:\/\/en.avm.de\/guide\/using-the-fritzbox-nas-function\/\" rel=\"nofollow noreferrer\">FritzBox NAS<\/a>.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589441422903,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1589442901067,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61791713",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":14.1,
        "Challenge_reading_time":7.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I read\/write data from\/to network attached storage with kedro?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":675.0,
        "Challenge_word_count":46,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1441627039648,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Augsburg, Germany",
        "Poster_reputation_count":3635.0,
        "Poster_view_count":383.0,
        "Solution_body":"<p>So I'm a little rusty on network attached storage, but:<\/p>\n\n<ol>\n<li><p>If you can mount your network attached storage onto your OS and access it like a regular folder, then it's just a matter of providing the right <code>filepath<\/code> when writing the config for a given catalog entry. See for example: <a href=\"https:\/\/stackoverflow.com\/questions\/7169845\/using-python-how-can-i-access-a-shared-folder-on-windows-network\">Using Python, how can I access a shared folder on windows network?<\/a><\/p><\/li>\n<li><p>Otherwise, if accessing the network attached storage requires anything special, you might want to <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/14_create_a_new_dataset.html\" rel=\"nofollow noreferrer\">create a custom dataset<\/a> that uses a Python library for interfacing with your network attached storage. Something like <a href=\"https:\/\/pysmb.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">pysmb<\/a> comes to mind.<\/p><\/li>\n<\/ol>\n\n<p>The custom dataset could borrow heavily from the logic in existing <code>kedro.io<\/code> or <code>kedro.extras.datasets<\/code> datasets, but you replace the filepath\/fsspec handling code with <code>pysmb<\/code> instead.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.0,
        "Solution_reading_time":15.63,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":132.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>According to Kedro's <a href=\"https:\/\/kedro.readthedocs.io\/en\/0.15.7\/kedro.extras.datasets.html\" rel=\"nofollow noreferrer\">documentation<\/a>, Azure Blob Storage is one of the available data sources. Does this extend to ADLS Gen2 ?<\/p>\n<p>Haven't tried Kedro yet, but before I invest some time on it, I wanted to make sure I could connect to ADLS Gen2.<\/p>\n<p>Thank you in advance !<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636709020827,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69940562",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":5.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Data Lake Storage Gen2 (ADLS Gen2) as a data source for Kedro pipeline",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":350.0,
        "Challenge_word_count":65,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586517832390,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":127.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>Yes this works with Kedro. You're actually pointing a really old version of the docs, nowadays all filesystem based datasets in Kedro use <a href=\"https:\/\/github.com\/fsspec\/filesystem_spec\" rel=\"nofollow noreferrer\">fsspec<\/a> under the hood which means they work with S3, HDFS, local and many more filesystems seamlessly.<\/p>\n<p>The ADLS Gen2 is supported by <code>ffspec<\/code> via the underlying <code>adlfs<\/code> library which is <a href=\"https:\/\/github.com\/fsspec\/adlfs\" rel=\"nofollow noreferrer\">documented here<\/a>.<\/p>\n<p>From a Kedro point of view all you need to do is declare your catalog entry like so:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code> motorbikes:\n     type: pandas.CSVDataSet\n     filepath: abfs:\/\/your_bucket\/data\/02_intermediate\/company\/motorbikes.csv\n     credentials: dev_az\n<\/code><\/pre>\n<p>We also have more examples <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/01_data_catalog.html\" rel=\"nofollow noreferrer\">here<\/a>, particularly example 15.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.5,
        "Solution_reading_time":13.03,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":103.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1601729162436,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":887.0,
        "Answerer_view_count":130.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If we have an AzureML web service endpoint that is collecting data (for Data Drift Monitoring), does overwriting the web service endpoint with a new version of the model break links with the Dataset registered for collecting data.<\/p>\n<p>The relative path to this dataset is:\n<code>&lt;Subscription-ID&gt;\/&lt;Resource-Group&gt;\/&lt;Workspace&gt;\/&lt;Webservice-Name&gt;\/&lt;model-name&gt;\/&lt;version&gt;\/inputs\/**\/inputs*.csv<\/code><\/p>\n<p>If we redeploy a new version using <code>az ml model deploy ..... --overwrite<\/code>, will we need a new reference to a new Dataset for detecting Data Drift?<\/p>\n<p>If we use <code>az ml service update ..<\/code>, will the Dataset reference be kept intact?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1622187967130,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67734831",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.8,
        "Challenge_reading_time":9.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Does a AzureML webservice overwrite reset the Data Collection Dataset?",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":44.0,
        "Challenge_word_count":96,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>Since the Dataset Asset is a simple reference to a location in a Datastore. Assuming the model version and service name does not change, the Dataset reference also will not change. If however, with every Service Update - The model version changes then adding a Dataset with Relative Path:<\/p>\n<pre><code>&lt;Subscription-ID&gt;\/&lt;Resource-Group&gt;\/&lt;Workspace&gt;\/&lt;Webservice-Name&gt;\/&lt;model-name&gt;\/*\/inputs\/**\/inputs*.csv\n<\/code><\/pre>\n<p>Will solve the problem. Since Data Drift is another service referencing this Dataset asset, it will keep working as expected.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":7.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":70.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1593579580856,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":14.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I am trying iris to get acquainted with was sagemaker I am following simple tutorials from <a href=\"https:\/\/towardsdatascience.com\/training-and-deploying-custom-tensorflow-models-with-aws-sagemaker-72027722ad76\" rel=\"nofollow noreferrer\">link<\/a>. I have created a bucket named &quot;tf-practise-iris-data&quot; and gave the IAM role of Sagemaker access to the s3 bucket as mentioned in the tutorial. I also tried creating a new bucket with a different name thinking there might be some problem with a bucket but still it is having the same issue, this is the snippet of my code <a href=\"https:\/\/i.stack.imgur.com\/FgjVK.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FgjVK.png\" alt=\"enter image description here\" \/><\/a>. And I have turned off Block all public access from the bucket but still nothing.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642443300117,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70745798",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":12.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to upload data using sagemaker_session.upload_data in s3 bucket that I created, it is getting stored in default s3 bucket",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":242.0,
        "Challenge_word_count":124,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1593579580856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":14.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Solved it!!!!!!!!!!!!<\/p>\n<pre><code>prefix = &quot;checking-with-new-bucket&quot;\ntraining_input_path = sagemaker_session.upload_data('train.csv', bucket = 'checking-with-new-bucket',key_prefix = prefix + &quot;\/training&quot;)\ntraining_input_path\n<\/code><\/pre>\n<p>Which gave output as<\/p>\n<pre><code>'s3:\/\/checking-with-new-bucket\/checking-with-new-bucket\/training\/train.csv'\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":31.6,
        "Solution_reading_time":5.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1365101584443,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Munich, Germany",
        "Answerer_reputation_count":7203.0,
        "Answerer_view_count":445.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a step function statemachine which creates SageMaker batch transform job, the definition is written in Terraform, I wanted to add the stepfunction execution id to the batch transform job names:<\/p>\n<p>in stepfunction terraform file:<\/p>\n<pre><code>  definition = templatefile(&quot;stepfuntion.json&quot;,\n    {\n      xxxx\n)\n<\/code><\/pre>\n<p>in the &quot;stepfuntion.json&quot;:<\/p>\n<pre><code>{...\n          &quot;TransformJobName&quot;: &quot;jobname-$$.Execution.Id&quot;,\n  \n          }\n      },\n        &quot;End&quot;: true\n      }\n    }\n  }\n<\/code><\/pre>\n<p>But after terraform apply, it didn't generate the actual id, it gave me <code>jobname-$$.Execution.Id<\/code>, can anyone help with this please?<\/p>\n<p>Resources: <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/input-output-contextobject.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/input-output-contextobject.html<\/a>\n&quot;To access the context object, first specify the parameter name by appending .$ to the end, as you do when selecting state input with a path. Then, to access context object data instead of the input, prepend the path with $$.. This tells AWS Step Functions to use the path to select a node in the context object.&quot;<\/p>\n<p>Can someone tell me what I'm missing please?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610012100150,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1610014278923,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65609804",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":17.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"How to append stepfunction execution id to SageMaker job names?",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":488.0,
        "Challenge_word_count":148,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>The var you are trying to use terraform doesn't know about it<\/p>\n<blockquote>\n<p>jobname-$$.Execution.Id.<\/p>\n<\/blockquote>\n<p>That's something specific to the Step function and available within state machine not available for terraform.<\/p>",
        "Solution_comment_count":17.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":3.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1515682479260,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":754.0,
        "Answerer_view_count":113.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I create one compute instance 'yhd-notebook' in Azure Machine Learning compute with user1. When I login with user2, and try to open the JupyterLab of this compute instance, it shows an error message like below.<\/p>\n\n<blockquote>\n  <p>User user2 does not have access to compute instance yhd-notebook.<\/p>\n  \n  <p>Only the creator can access a compute instance.<\/p>\n  \n  <p>Click here to sign out and sign in again with a different account.<\/p>\n<\/blockquote>\n\n<p>Is it possible to share compute instance with another user? BTW, both user1 and user2 have Owner role with the Azure subscription.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583388412943,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60539094",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.9,
        "Challenge_reading_time":7.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Is it possible to share compute instance with other user?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":3705.0,
        "Challenge_word_count":99,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582361231692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Guangzhou, China",
        "Poster_reputation_count":393.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>According to MS, all users in the workspace contributor and owner role can create, delete, start, stop, and restart compute instances across the workspace. However, only the creator of a specific compute instance is allowed to access Jupyter, JupyterLab, and RStudio on that compute instance. The creator of the compute instance has the compute instance dedicated to them, have root access, and can terminal in through Jupyter. Compute instance will have single-user login of creator user and all actions will use that user\u2019s identity for RBAC and attribution of experiment runs. SSH access is controlled through public\/private key mechanism.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":8.09,
        "Solution_score_count":7.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":99.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>User is not authorized to query provided resources due to s2s call not providing any active baggage to verify role-based access.  <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1614327752400,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/290347\/automate-machine-learning-model-on-azure-portal-fa",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.1,
        "Challenge_reading_time":2.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Automate machine learning model on Azure portal failed",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":29,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>No update yet.  <\/p>\n<p>I have been receiving emails from Microsoft that they will disable my account for going against their policy.   <\/p>\n<p>Reason was because I am frequently using the Azure platform.  <\/p>\n<p>I decided to take a break from the platform to avoid them deleting my account.  <\/p>\n<p>Don't know what to do \ud83d\ude2d next<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.3,
        "Solution_reading_time":4.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":56.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi team im new to Azure ML studio in that i done trained data but i would like to deploy im in trial account i dont see option for deploy i can able to see only submit , share like that only please help<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/88acd6d3-f886-4193-abd3-dd22f6453378?platform=QnA\" alt=\"azure\" \/><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1680338490806,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1195304\/in-azure-ml-studio-deploy-option-is-not-there",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":4.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"In Azure ML studio deploy option is not there",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":54,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=b7477e61-4395-49cc-9454-e13b8a24bb87\">@manoj p  <\/a><\/p>\n<p>As you can read :<\/p>\n<p>In Azure Machine Learning Studio, the ability to deploy a model is only available in the paid tiers of the service. If you are using a trial account, you may not have access to the deploy functionality.<\/p>\n<p>To deploy a model in Azure Machine Learning Studio, you will need to upgrade to a paid subscription. The deploy functionality is available in the Standard and Enterprise tiers of the service.<\/p>\n<p>Once you have upgraded your subscription, you can follow these steps to deploy your trained model:<\/p>\n<p>Open the Azure Machine Learning Studio and navigate to your workspace.<\/p>\n<p>Navigate to the &quot;Models&quot; tab and select the trained model you want to deploy.<\/p>\n<p>Click on the &quot;Deploy&quot; button and select the deployment target, such as Azure Kubernetes Service (AKS) or Azure Container Instances (ACI).<\/p>\n<p>Configure the deployment settings, such as the number of nodes and the CPU and memory settings.<\/p>\n<p>Click on the &quot;Deploy&quot; button to start the deployment process.<\/p>\n<p>Once the deployment is complete, you can test the deployed model by sending requests to the endpoint.<\/p>\n<p>Please mark the answer as accepted if yu find it helpful !<\/p>\n<p>BR<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":16.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":197.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1370505440848,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to avoid to use the managed policies <code>AmazonSageMakerReadOnly<\/code> and <code>AmazonSageMakerFullAccess<\/code> because I only want the users to be able to start\/stop their own notebook instance and to open jupyter in their instance.<\/p>\n<p>So far the user role has the following permissions among others<\/p>\n<pre><code>...\n        {\n            &quot;Sid&quot;: &quot;&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;sagemaker:StopNotebookInstance&quot;,\n                &quot;sagemaker:StartNotebookInstance&quot;,\n                &quot;sagemaker:CreatePresignedNotebookInstanceUrl&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEquals&quot;: {\n                    &quot;aws:ResourceTag\/OwnerRole&quot;: &quot;${aws:userid}&quot;\n                }\n            }\n        },\n  \n<\/code><\/pre>\n<p>The policy does <strong>not<\/strong> have <code>sagemaker:CreatePresignedDomainUrl<\/code> but it has <code>sagemaker:CreatePresignedNotebookInstanceUrl<\/code>, when the user with this policy click on <code>Open Jupyter<\/code> in the AWS Sagemaker console , it opens an url <code>https:\/\/xxxxxx.notebook.eu-north-1.sagemaker.aws\/auth?authToken=xxxxx<\/code> but that url will return:<\/p>\n<p><code>403 Forbidden. Access to xxxxxx.notebook.eu-north-1.sagemaker.aws was denied. You don't have authorisation to view this page. HTTP ERROR 403<\/code><\/p>\n<p>As soon as I added <code>sagemaker:CreatePresignedDomainUrl<\/code> for resource <code>*<\/code> then the 403 error <strong>was gone<\/strong> and the user could open the jupyter notebook.<\/p>\n<p>My question is <strong>why is that needed<\/strong>, and <strong>what resource should I put<\/strong> instead of <code>*<\/code>, the documentation mentions <code>arn:aws:sagemaker:regionXXX:account-idXXX:app\/domain-id\/userProfileNameXXXX\/*<\/code> but I do not have any domain or user profile.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652690155553,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72256288",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.2,
        "Challenge_reading_time":25.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Is sagemaker:CreatePresignedDomainUrl required to open jupyter in SageMaker notebook instance?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":173.0,
        "Challenge_word_count":176,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1237484106592,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Stockholm, Sweden",
        "Poster_reputation_count":19085.0,
        "Poster_view_count":906.0,
        "Solution_body":"<p><code>CreatePresignedDomainUrl<\/code> statement allows the role to launch a SageMaker Studio app (and hence the <code>domain-id\/user-profile<\/code> ARN). Opening SageMaker notebook instance does not need the presigned domain url permission.<\/p>\n<p>You'll need to make sure you're tagging the notebook with an OwnerRole key, with value = userid (not username). In addition, you'll need to use the <code>sagemaker:ResourceTag<\/code> (instead of <code>aws:ResourceTag<\/code>).<\/p>\n<p>See the <a href=\"https:\/\/docs.aws.amazon.com\/service-authorization\/latest\/reference\/list_amazonsagemaker.html\" rel=\"nofollow noreferrer\">service authorization page<\/a> for a complete list of actions and condition keys.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.0,
        "Solution_reading_time":9.28,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":74.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1586263306992,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Krak\u00f3w, Poland",
        "Answerer_reputation_count":1622.0,
        "Answerer_view_count":654.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How can I find IP for vertex AI managed notebook instance? The service is differing from user managed notebooks in certain sense. The creation of an instance doesn't create a compute instance, so it's all managed by itself.<\/p>\n<p>My purpose is to whitelist the set of IPs in Mongo atlas. Set of IPs being of all the notebooks in that region. I'm using google-managed networks in this case.<\/p>\n<p>I've a few doubts here:<\/p>\n<ul>\n<li>Since within managed nb, I can change CPU consumption, will this reinstantiate a new cluster, with entirely new IP, or it will be 1 from among a group of IPs?<\/li>\n<li>Is it possible to add a custom init script?<\/li>\n<\/ul>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635836251457,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1636016613528,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69806432",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":8.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Vertex AI Managed Notebook, get subnet\/IP",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":665.0,
        "Challenge_word_count":119,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1353151867408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":2513.0,
        "Poster_view_count":251.0,
        "Solution_body":"<p>If you want to connect to a database service on GCP, create a network (or use the default) and instantiate the notebook using this network (<code>Advanced options<\/code>) and create the white list for this entire network . It's required because the managed notebook creates a peering network on the network you will use, you can check you in <code>VPC Network<\/code> \u279e <code>VPC Network Peering<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Pd1ui.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Pd1ui.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>If you want an external IP, it will not work. Google managed notebooks <strong>does not use external ips<\/strong>, they basically access the internet via NAT gateways (does not matter if you use google or own managed networks) so you will not be able to do what you want. Move for user managed notebooks (where you can assign a fixed external ip) or white list any IP on your Mongo db service if you are not in a production environment.<\/p>\n<p>About yous doubts:<\/p>\n<blockquote>\n<p>Since within managed nb, I can change CPU consumption, will this instantiate a new cluster, with entirely new IP, or it will be 1 from among a group of IPs<\/p>\n<\/blockquote>\n<p>For the internal network it may change when you restart or recreate the notebook instance. For an external network, it does not exists and explained.<\/p>\n<blockquote>\n<p>Is it possible to add a custom init script?<\/p>\n<\/blockquote>\n<p>Basically not. But you can provide custom docker images for the notebook.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1635872119870,
        "Solution_link_count":2.0,
        "Solution_readability":9.7,
        "Solution_reading_time":19.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":234.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Is it possible to access SageMaker Notebooks without accessing the console?\n\n**Do we have a best practice for that?**\nIn the [`create-presigned-notebook-instance-url`][1] command, what is the `--session-expiration-duration-in-seconds`: is it the validity duration of the URL or the max session duration once the URL has been clicked?\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-presigned-notebook-instance-url.html",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1543947347000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668556320118,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUp9lMw9-ESm-27BWY_RgCSg\/accessing-sagemaker-notebooks-without-accessing-the-console",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.5,
        "Challenge_reading_time":6.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Accessing SageMaker Notebooks without accessing the console",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":481.0,
        "Challenge_word_count":54,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I have experimented with CreatePresignedNotebookInstanceUrl a number of times. It returns an \"AuthorizedUrl\" string in the form:\n`https:\/\/<notebook_instance_name>.notebook.<region>.sagemaker.aws?authToken=<a_very_long_string>`\n\nI used the URL in another browser with no AWS console's session cookies (not logged in to the console) and it worked (could access my notebooks).\n\nThe parameter SessionExpirationDurationInSeconds is... well, exactly what it says, The number of seconds the presigned url is valid for. The API accepts a range of  `[1800, 43200]`  in seconds, which is equivalent to : 30 minutes to 12 hours .\n\nI hope this helps",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925565195,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":8.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":86.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Assume a data scientist who is coding inside a Synapse notebook, aims to submit his AutoML job to Azure ML. Also assume that we already created the Azure ML workspace, and linked it to Synapse, and also gave Synapse workspace the contributor access to Azure ML workspace. Also the data scientist has the Azure reader role at the synapse workspace level. Data scientist run the following code according to this link (<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/synapse-analytics\/spark\/apache-spark-azure-machine-learning-tutorial\">https:\/\/learn.microsoft.com\/en-us\/azure\/synapse-analytics\/spark\/apache-spark-azure-machine-learning-tutorial<\/a>)    <\/p>\n<p>from azureml.core import Workspace    <\/p>\n<p>subscription_id = &quot;xxxxxx&quot; #you should be owner or contributor    <br \/>\nresource_group = &quot;xxxxx&quot; #you should be owner or contributor    <br \/>\nworkspace_name = &quot;xxxxx&quot; #your workspace name    <br \/>\nworkspace_region = &quot;xxxxx&quot; #your region    <\/p>\n<p>ws = Workspace(workspace_name = workspace_name,    <br \/>\n               subscription_id = subscription_id,  <br \/>\n               resource_group = resource_group)  <\/p>\n<p>However, he receives an error that says he does not have the required contributor\/owner roles at the subscription and resource group level. But we (as the synapse administrators) we don't want to give him the contributor\/owner role at the subscription and resource group name    <\/p>\n<p>Question: How the data scientist can submit his job without letting him to have the required contributor\/owner role. Can he use the managed identity of the Synapse workspace to connect to the Azure ML workspace?    <\/p>\n<p>Thank you<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648588740927,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/792681\/submitting-a-job-to-azure-ml-from-synapse-workspac",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":21.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Submitting a job to Azure ML from Synapse workspace",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":209,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <em>anonymous user<\/em>,    <\/p>\n<p>Thanks for the question and using MS Q&amp;A platform.    <\/p>\n<blockquote>\n<p>Make sure your <code>Service principal<\/code> or <code>Managed Service Identity (MSI)<\/code> must have &quot;<strong>Contributor<\/strong>&quot; access to the AML workspace.    <\/p>\n<\/blockquote>\n<p> If the model is registered in Azure Machine Learning, then you can choose either of the following two supported ways of authentication.    <\/p>\n<ul>\n<li> <strong>Through service principal:<\/strong> You can use service principal client ID and secret directly to authenticate to AML workspace. Service principal must have &quot;Contributor&quot; access to the AML workspace.    <\/li>\n<li> <strong>Through linked service:<\/strong> You can use linked service to authenticate to AML workspace. Linked service can use &quot;service principal&quot; or Synapse workspace's &quot;Managed Service Identity (MSI)&quot; for authentication. &quot;Service principal&quot; or &quot;Managed Service Identity (MSI)&quot; must have &quot;Contributor&quot; access to the AML workspace.    <\/li>\n<\/ul>\n<p>Here is the complete walkthrough of authenticating AML workspace with Azure Synapse Analytics:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/188130-synapse-aml-predict.gif?platform=QnA\" alt=\"188130-synapse-aml-predict.gif\" \/>      <\/p>\n<p>For more details, refer to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\">Tutorial: Score machine learning models with PREDICT in serverless Apache Spark pools<\/a>.    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/support\/community-champions-program\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":13.0,
        "Solution_reading_time":35.73,
        "Solution_score_count":1.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":280.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1467237684900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":613.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":30715.9147230556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am a newbie when it comes to Python SageMaker (my background is C#). Currently, I have a problem because the last method call (I mean the fit method) results in a \"NoCredentialsError\". I do not understand that. The AWS credentials have been set and I do use them to communicate with AWS, for example to communicate with S3. How can I prevent this error? <\/p>\n\n<pre><code>import io\nimport os\nimport gzip\nimport pickle\nimport urllib.request\nimport boto3\nimport sagemaker\nimport sagemaker.amazon.common as smac\n\nDOWNLOADED_FILENAME = 'C:\/Users\/Daan\/PycharmProjects\/downloads\/mnist.pkl.gz'\nif not os.path.exists(DOWNLOADED_FILENAME):\n    urllib.request.urlretrieve(\"http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl.gz\", DOWNLOADED_FILENAME)\n\nwith gzip.open(DOWNLOADED_FILENAME, 'rb') as f:\n    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\nvectors = train_set[0].T\nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, vectors)\nbuf.seek(0)\nkey = 'recordio-pb-data'\nbucket_name = 'SOMEKINDOFBUCKETNAME'\nprefix = 'sagemaker\/pca'\npath = os.path.join(prefix, 'train', key)\nprint(path)\n\nsession = boto3.session.Session(aws_access_key_id='SECRET',aws_secret_access_key='SECRET',region_name='eu-west-1')\nclient = boto3.client('sagemaker',region_name='eu-west-1',aws_access_key_id='SECRET',aws_secret_access_key='SECRET')\nregion='eu-west-1'\nsagemakerSession= sagemaker.Session(sagemaker_client=client,boto_session=session)\ns3_resource=session.resource('s3')\nbucket = s3_resource.Bucket(bucket_name)\ncurrent_bucket = bucket.Object(path)\n\ntrain_data = 's3:\/\/{}\/{}\/train\/{}'.format(bucket_name, prefix, key)\nprint('uploading training data location: {}'.format(train_data))\ncurrent_bucket.upload_fileobj(buf)\n\noutput_location = 's3:\/\/{}\/{}\/output'.format('SOMEBUCKETNAME', prefix)\nprint('training artifacts will be uploaded to: {}'.format(output_location))\n\nregion='eu-west-1'\n\ncontainers = {'us-west-2': 'SOMELOCATION',\n              'us-east-1': 'SOMELOCATION',\n              'us-east-2': 'SOMELOCATION',\n              'eu-west-1': 'SOMELOCATION'}\ncontainer = containers[region]\n\nrole='AmazonSageMaker-ExecutionRole-SOMEVALUE'\npca = sagemaker.estimator.Estimator(container,\n                                    role,\n                                    train_instance_count=1,\n                                    train_instance_type='ml.c4.xlarge',\n                                    output_path=output_location,\n                                    sagemaker_session=sagemakerSession)\n\n\npca.set_hyperparameters(feature_dim=50000,\n                        num_components=10,\n                        subtract_mean=True,\n                        algorithm_mode='randomized',\n                        mini_batch_size=200)\n\npca.fit(inputs=train_data)\n\nprint('END')\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526393414957,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50352412",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":19.9,
        "Challenge_reading_time":33.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":null,
        "Challenge_title":"How to prevent a NoCredentialsError when calling the fit method in SageMaker?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":447.0,
        "Challenge_word_count":190,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1358429250663,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2120.0,
        "Poster_view_count":279.0,
        "Solution_body":"<p>I am not sure if you have masked the actual access id and key or this is what you are running.<\/p>\n<pre><code>session = boto3.session.Session(aws_access_key_id='SECRET',aws_secret_access_key='SECRET',region_name='eu-west-1')\nclient = boto3.client('sagemaker',region_name='eu-west-1',aws_access_key_id='SECRET',aws_secret_access_key='SECRET')\n<\/code><\/pre>\n<p>I am hoping you are providing the actual aws_access_key_id and aws_secret_access_key in the above lines of code.<\/p>\n<p>Another way of specifying the same and not hardcoding in the code is to create a credentials file in your profile directory i.e.<\/p>\n<p>in Mac    ~\/.aws\/<\/p>\n<p>and in Windows <code>&quot;%UserProfile%\\.aws&quot;<\/code><\/p>\n<p>the file is a plain text file with a name &quot;credentials&quot; (without the quotes).\nfile contains<\/p>\n<pre><code>[default]\naws_access_key_id=XXXXXXXXXXXXXX\naws_secret_access_key=YYYYYYYYYYYYYYYYYYYYYYYYYYY\n<\/code><\/pre>\n<p>AWS CLI would pick it up from the above location and use it. You can also use non-default profiles and pass on the profile with<\/p>\n<pre><code>os.environ[&quot;AWS_PROFILE&quot;] = &quot;profile-name&quot;\n<\/code><\/pre>\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1636970707960,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":15.42,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":124.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554298968016,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"wondeland",
        "Answerer_reputation_count":1540.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>As I am logging my entire models and params into mlflow I thought it will be a good idea to have  it protected under a user name and password.<\/p>\n\n<p>I use the following code to run the mlflow server<\/p>\n\n<p><code>mlflow server --host 0.0.0.0 --port 11111<\/code>\nworks perfect,in mybrowser i type <code>myip:11111<\/code> and i see everything (which eventually is the problem)<\/p>\n\n<p>If I understood the documentation and the following <a href=\"https:\/\/groups.google.com\/forum\/#!topic\/mlflow-users\/E9QW4HdS8a8\" rel=\"noreferrer\">https:\/\/groups.google.com\/forum\/#!topic\/mlflow-users\/E9QW4HdS8a8<\/a> link here correct, I should use nginx to create the authentication.<\/p>\n\n<p>I installed <code>nginx open sourcre<\/code>  and <code>apache2-utils<\/code><\/p>\n\n<p>created <code>sudo htpasswd -c \/etc\/apache2\/.htpasswd user1<\/code> user and passwords.<\/p>\n\n<p>I edited my <code>\/etc\/nginx\/nginx.conf<\/code> to the following:<\/p>\n\n<pre><code>server {\n        listen 80;\n        listen 443 ssl;\n\n        server_name my_ip;\n        root NOT_SURE_WHICH_PATH_TO_PUT_HERE, THE VENV?;\n        location \/ {\n            proxy_pass                      my_ip:11111\/;\n            auth_basic                      \"Restricted Content\";\n            auth_basic_user_file \/home\/path to the password file\/.htpasswd;\n        }\n    }\n<\/code><\/pre>\n\n<p><strong>but no authentication appears.<\/strong><\/p>\n\n<p>if I change the conf to listen to  <code>listen 11111<\/code>\nI get an error that the port is already in use ( of course, by the mlflow server....)<\/p>\n\n<p>my wish is to have a authentication window before anyone can enter by the mlflow with a browser.<\/p>\n\n<p>would be happy to hear any suggestions.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1574259400087,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58956459",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":9.9,
        "Challenge_reading_time":20.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":11.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"How to run authentication on a mlFlow server?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":13870.0,
        "Challenge_word_count":193,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554298968016,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"wondeland",
        "Poster_reputation_count":1540.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>the problem here is that both <code>mlflow<\/code> and <code>nginx<\/code> are trying to run on the <strong>same port<\/strong>... <\/p>\n\n<ol>\n<li><p>first lets deal with nginx:<\/p>\n\n<p>1.1 in \/etc\/nginx\/sites-enable make a new file <code>sudo nano mlflow<\/code> and delete the exist default.<\/p>\n\n<p>1.2 in mlflow file:<\/p><\/li>\n<\/ol>\n\n<pre><code>server {\n    listen YOUR_PORT;\n    server_name YOUR_IP_OR_DOMAIN;\n    auth_basic           \u201cAdministrator\u2019s Area\u201d;\n    auth_basic_user_file \/etc\/apache2\/.htpasswd; #read the link below how to set username and pwd in nginx\n\n    location \/ {\n        proxy_pass http:\/\/localhost:8000;\n        include \/etc\/nginx\/proxy_params;\n        proxy_redirect off;\n    }\n}\n<\/code><\/pre>\n\n<p>1.3.  restart nginx <code>sudo systemctl restart nginx<\/code><\/p>\n\n<ol start=\"2\">\n<li>on your server run mlflow  <code>mlflow server --host localhost --port 8000<\/code><\/li>\n<\/ol>\n\n<p>Now if you try access the YOUR_IP_OR_DOMAIN:YOUR_PORT within your browser an auth popup should appear, enter your host and pass and now you in mlflow<\/p>\n\n<ol start=\"3\">\n<li><p>now there are 2 options to tell the mlflow server about it:<\/p>\n\n<p>3.1 set username and pwd as environment variable \n<code>export MLFLOW_TRACKING_USERNAME=user export MLFLOW_TRACKING_PASSWORD=pwd<\/code><\/p>\n\n<p>3.2 edit in your <code>\/venv\/lib\/python3.6\/site-packages\/mlflowpackages\/mlflow\/tracking\/_tracking_service\/utils.py<\/code> the function <\/p><\/li>\n<\/ol>\n\n<pre><code>def _get_rest_store(store_uri, **_):\n    def get_default_host_creds():\n        return rest_utils.MlflowHostCreds(\n            host=store_uri,\n            username=replace with nginx user\n            password=replace with nginx pwd\n            token=os.environ.get(_TRACKING_TOKEN_ENV_VAR),\n            ignore_tls_verification=os.environ.get(_TRACKING_INSECURE_TLS_ENV_VAR) == 'true',\n        )\n<\/code><\/pre>\n\n<p>in your .py file where you work with mlflow:<\/p>\n\n<pre><code>import mlflow\nremote_server_uri = \"YOUR_IP_OR_DOMAIN:YOUR_PORT\" # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\nmlflow.set_experiment(\"\/my-experiment\")\nwith mlflow.start_run():\n    mlflow.log_param(\"a\", 1)\n    mlflow.log_metric(\"b\", 2)\n<\/code><\/pre>\n\n<p>A link to nginx authentication doc <a href=\"https:\/\/docs.nginx.com\/nginx\/admin-guide\/security-controls\/configuring-http-basic-authentication\/\" rel=\"noreferrer\">https:\/\/docs.nginx.com\/nginx\/admin-guide\/security-controls\/configuring-http-basic-authentication\/<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.7,
        "Solution_reading_time":30.69,
        "Solution_score_count":8.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":211.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1417158887760,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Gurgaon, India",
        "Answerer_reputation_count":872.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created AWS S3 bucket and tried sample kmeans example on Jupyter notebook.\nBeing account owner I have read\/write permissions but I am unable to write logs with following error, <\/p>\n\n<pre><code> ClientError: An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n<\/code><\/pre>\n\n<p>here's the kmeans sample code, <\/p>\n\n<pre><code> from sagemaker import get_execution_role\n role = get_execution_role()\n bucket='testingshk' \n\n import pickle, gzip, numpy, urllib.request, json\nurllib.request.urlretrieve(\"http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl.gz\", \"mnist.pkl.gz\")\n with gzip.open('mnist.pkl.gz', 'rb') as f:\n train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n\n from sagemaker import KMeans\n data_location = 's3:\/\/{}\/kmeans_highlevel_example\/data'.format(bucket)\n output_location = 's3:\/\/{}\/kmeans_example\/output'.format(bucket)\n\n print('training data will be uploaded to: {}'.format(data_location))\n print('training artifacts will be uploaded to: {}'.format(output_location))\n\n kmeans = KMeans(role=role,\n            train_instance_count=2,\n            train_instance_type='ml.c4.8xlarge',\n            output_path=output_location,\n            k=10,\n            data_location=data_location)\n kmeans.fit(kmeans.record_set(train_set[0]))\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1523658977623,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49826004",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":15.2,
        "Challenge_reading_time":16.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS S3 bucket write error",
        "Challenge_topic":"Bucket Access Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":915.0,
        "Challenge_word_count":112,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400307037672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Austria",
        "Poster_reputation_count":83.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>Even if you have all the access to the bucket, you need to provide access key and secret in order to put some object in bucket if it is private. Or if you make bucket access public to all then you can push object to bucket without any problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":2.97,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi all,\n\nI've got a compute instance that cannot access the documentai processor.\u00a0 The compute engine is in the same project as the processor, and I've given the service account the roles\n\n\"Document AI API User\" and\n\"Document AI Viewer\"\u00a0\n\nThe error I receive is\n\n\"7 PERMISSION_DENIED: Request had insufficient authentication scopes.\"\u00a0\n\nwhich feels like an Oauth issue, but my reading leads me to believe that documentAI uses Application Default Credentials, and that my compute instance should use the service account for the request.\u00a0\u00a0\n\nthanks in advance for any insight.",
        "Challenge_closed_time":1675190.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675094100000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/cannot-access-documentai-api-from-compute\/m-p\/515739#M1154",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.9,
        "Challenge_reading_time":7.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"cannot access documentai api from compute",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":103.0,
        "Challenge_word_count":95,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\n\nthanks, I had tried adding scopes, but the solution was to add a key to the service account\u00a0 add the json config to the file system and add the environment variable\u00a0\n\nGOOGLE_APPLICATION_CREDENTIALS\n\nas per\u00a0\n\nhttps:\/\/stackoverflow.com\/questions\/65703339\/fixedcredentialsprovider-gives-unauthorized-exception-w...\n\n\u00a0\n\n\u00a0\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.8,
        "Solution_reading_time":4.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":41.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking to use Azure Machine Learning Services (the one with the new drag and drop feature; still in preview) in a new data science project. <\/p>\n\n<p>I have realised that I can preview the data when I connect a data set; I am able to do this using the option 'Dataset output' which is available as part of the dataset.<\/p>\n\n<p>To be able to see this data, the data needs to be cached some where. <\/p>\n\n<p>Can someone advise where this is cached? <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582250022010,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60331084",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Where does Azure Machine Learning Service cache data?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":397.0,
        "Challenge_word_count":93,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456485566000,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Canberra, Australian Capital Territory, Australia",
        "Poster_reputation_count":214.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>Data is cached by default in a storage account that is created along with the the ML service workspace. It has the same name as the workspace plus some numbers. Inside the account there is a blobstore called <code>azureml-blobstore-{GUID}<\/code> Inside of that container your data is cached,  organized by runs.<\/p>\n\n<p>This data is made available to ML service as a <code>Datastore<\/code> that you can navigate to in the UI by clicking \"Datastores\" in the blade on the left-hand of the Studio.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YVwPl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YVwPl.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.3,
        "Solution_reading_time":8.46,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":16,
        "Challenge_body":"**Describe the bug**\r\nWe encountered an interesting issue regarding the auto stop script. We had no code changes, but suddenly, Sagemaker instances started hanging around for days, with no use. Looking into the instance, the cron job was failing, because the autostop.py script had a syntax error. When I look at the script, it has this line `print(f'Notebook idle state set as {idle} because no kernel has been detected.')` which caused the syntax error. However, the file on the repo, as well as the s3 bucket, does not contain this line. So, after some digging, I found that this line was introduced here, in this commit [aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/commit\/fdace58a6b9401c53dc17f5c64bef3ec40dbc70e). What I don't understand is how it got into the Sagemaker notebook, and why it's not being overridden by the custom config start we have here [sagemaker-notebook-instance.cfn.yml](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/addons\/addon-base-raas\/packages\/base-raas-cfn-templates\/src\/templates\/service-catalog\/sagemaker-notebook-instance.cfn.yml#L264-L272) This script and repo was updated in the last 16 hours to remove this syntax error.\r\n\r\n**To Reproduce**\r\nLaunch a Sagemaker instance. You can tell which version of the script it's using by looking at the autostop script, `less \/usr\/local\/bin\/autostop.py` and find lines 96-101.\r\n\r\nThe AWS version of the script on the `awslabs\/service-workbench-on-aws` repo has these lines, [reference](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/main\/solution\/post-deployment\/config\/environment-files\/offline-packages\/sagemaker\/autostop.py#L96-L100)\r\n```\r\nif notebook['kernel']['connections'] == 0:\r\n    if not is_idle(notebook['kernel']['last_activity']):\r\n        idle = False\r\nelse:\r\n    idle = False\r\n```\r\nAnd on the `aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples` repo, [reference](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/autostop.py#L96-L101)\r\n```\r\nif notebook['kernel']['connections'] == 0:\r\n    if not is_idle(notebook['kernel']['last_activity']):\r\n        idle = False\r\nelse:\r\n    idle = False\r\n    print('Notebook idle state set as %s because no kernel has been detected.' % idle)\r\n```\r\n\r\n**Expected behavior**\r\nThe autostop script in the s3 bucket should be the one used for SWB Sagemaker instances.\r\n\r\n**Screenshots**\r\n<img width=\"1510\" alt=\"Screen Shot 2022-11-16 at 12 14 21 PM\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/202251401-67da0253-e74e-40e9-8150-99a4a27017ff.png\">\r\n\r\n**Versions (please complete the following information):**\r\n - SWB 4.3.1\r\n",
        "Challenge_closed_time":1671215.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668620515000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1065",
        "Challenge_link_count":5,
        "Challenge_participation_count":16,
        "Challenge_readability":14.7,
        "Challenge_reading_time":36.84,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"[Bug] Sagemaker autostop script not pulling from s3 bucket",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":279,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Just want to verify that the autostop script in your bucket had not been updated at some point in the past unexpectedly. Is that correct? Yes-- none of the files on the s3 bucket were changed in several months. I also downloaded the autostop script from the bucket to verify manually that it matches the SWB repo version. I was not able to replicate this in v5.2.2:\r\n<img width=\"937\" alt=\"Screen Shot 2022-11-30 at 3 34 26 PM\" src=\"https:\/\/user-images.githubusercontent.com\/43092418\/204903064-013c1899-2763-4c88-8cce-7b39128b0240.png\">\r\n\r\nIf you look at the CloudWatch log group \/aws\/sagemaker\/NotebookInstances\/BasicNotebookInstance-<id>\/LifecycleConfigOnStart do you see the following output (would be towards the end):\r\n<img width=\"960\" alt=\"Screen Shot 2022-11-30 at 3 36 08 PM\" src=\"https:\/\/user-images.githubusercontent.com\/43092418\/204903303-d180144c-62d9-4775-a233-83687012715b.png\">\r\nThis is triggered on start of the instance. The screenshot you posted was from your instance, correct? Do you see the lines, \r\n```\r\nprint('Notebook is not idle:', notebook['kernel']['execution_state'])\r\nidle = False\r\n``` \r\nthis line comes from this repo [aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/autostop.py#L100-L101). It should be only the one line, like below\r\n```\r\nidle = False\r\n``` \r\nwhich comes from SWB here [awslabs\/service-workbench-on-aws](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/main\/solution\/post-deployment\/config\/environment-files\/offline-packages\/sagemaker\/autostop.py#L100)\r\n\r\nThe s3 file says it's downloaded, but for whatever reason it's not using that downloaded file, and instead using the file on aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples. This introduces SWB to vulnerabilities when this code is changes and a bug is introduced in that repo. SWB should instead use the autostop script that it has saved in s3, because that is locked and changes that aren't intended wouldn't be introduced. The screenshot is from my instance, yes. SWB repo contains the lines:\r\n```\r\nprint('Notebook is not idle:', notebook['kernel']['execution_state'])\r\nidle = False\r\n```\r\n[here](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/61200d06d1a607b9c0a209240813b261ade2c5e9\/main\/solution\/post-deployment\/config\/environment-files\/offline-packages\/sagemaker\/autostop.py#L105). It is the lines:\r\n```\r\nidle = False\r\nprint('Notebook idle state set as %s because no kernel has been detected.' % idle)\r\n```\r\nthat I thought you said were presenting the problem (that are in the samples repo but not SWB). Is that correct?\r\n\r\nHowever, I see that my instance is not stopping even though the autostop script is the same as the SWB repo.\r\n\r\nWhere did you see the error on the cron job for the autostop? Also, to clarify, you see that the Cloudwatch logs copy from the s3 bucket to local and that the s3 bucket file is the correct file? Yet, you see the wrong file when you less the file on the instance? Oh, you're right I was looking at the wrong line. My apologies! \r\n\r\nYes- for us it's successfully copying the correct file from s3 (which I downloaded to verify), but the autostop script (`\/usr\/local\/bin\/autostop.py`) is the wrong copy. Got it. And where do you see the cron job failing? Because it was not overwriting the autostop script with the one from our s3 bucket, and instead defaulting to the script from `aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples`,  when the repo owners introduced this commit: https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/commit\/fdace58a6b9401c53dc17f5c64bef3ec40dbc70e, the cron job failed on lines like `print(f'Notebook idle state set as {idle} since kernel connections are ignored.')`, stating that the `print(f` part was invalid syntax.\r\n\r\nI guess the key issue is really why it didn't overwrite this default script with the s3 one, considering it successfully downloaded the one from s3. Secondly, it seems odd that this repo is somehow the default autostop script that the Sagemaker system uses. This introduces bugs if there's a failure in that script or a malicious commit. Yes, I see the problem in the other repo's commit. I am still trying to debug how the script is on your Sagemaker instance.\r\n\r\nGot ~two~ three more questions:\r\n1. Where _in you account_ did you see that error message from the cron job? CloudWatch logs? Sagemaker? etc.\r\n2. Are you working with AppStream-enabled SWB? Does Sagemaker have to go through AppStream to connect?\r\n3. What is the output from running this command in a terminal on the sagemaker instance: `\/usr\/bin\/python \/usr\/local\/bin\/autostop.py --time 300 --ignore-connections`? Sure. :D Yeah I don't know how the script was there automatically. I didn't see any code in our repo that would cause it to pull from there.\r\n\r\nWe do not have appstream enabled, but we do send traffic through a proxy lambda as well as a firewall instance. However, all the environment files that get downloaded for the bootstrap process were successful, so I don't think it was a network issue.\r\n\r\nThe reason I checked the autostop script was because I saw no note in the `\/var\/log\/autostop.log` file that the script is sent to via cron job. So I ran the script by hand.\r\n\r\nFor instance, right now autostop is not working. There's no messaging that tells you it's not working. When you look at the cron logs it shows this, with no errors. The `\/var\/log\/autostop.log` script doesn't show any messages or errors.\r\n```\r\n[root@ip-10-10-57-235 ec2-user]# grep autostop \/var\/log\/cron | tail -n 1\r\nDec  1 18:14:01 ip-10-10-57-235 CROND[9860]: (root) CMD (\/usr\/bin\/python \/usr\/local\/bin\/autostop.py --time 3600 --ignore-connections >> \/var\/log\/autostop.log)\r\n```\r\n\r\nBut if you run the autostop script exactly as the cron job has it, like below, you get an error \/right now\/ related to a boto3 import issue.\r\n```\r\n[root@ip-10-10-57-235 ec2-user]# \/usr\/bin\/python \/usr\/local\/bin\/autostop.py --time 3600\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/bin\/autostop.py\", line 18, in <module>\r\n    import boto3\r\nImportError: No module named boto3\r\n```\r\n\r\nBoto3 changes were introduced in a commit here, [in the on-start script on aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/commit\/13b4023c9dca45fea58b2129fe5848619284653a#diff-54051e148aa00ee3fa158cc346d6c243418d14718a6760171ef562887977748f) Yup, so I also get that problem when I try to invoke the autostop script (and my autostop script matches the SWB one). I think that is the root cause of this problem. I will add a backlog item to figure out why boto3 is not being imported correctly so that sagemaker notebooks can use them for autostop. \r\n\r\nIt still does not explain why you got the amazon-sagemaker-notebook-instance-lifecycle-config-samples in the instance. Was that only present in one instance or all instances? Is it possible someone manually changed the files when trying to debug the autostop not working?\r\n\r\nThanks so much for working through this with me! Yeah, no problem-- thanks for your patience and attention! :D\r\n\r\nI don't believe that the files have been altered. I am currently the only person on my team actively responsible for doing admin\/infrastructure activities for these systems. I've tried this on new instances to rule out individual Sagemaker systems changes by users. We have two different version of SWB deployed-- maybe there's a difference between versions.  @srpiatt please upgrade your SWB installation to the latest release [v5.2.5](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v5.2.5). Sagemaker made a change that caused all new instances to be spun up with the AL2 operating system. New Sagemaker instances will no longer be able to mount studies or autostop without the fix in v5.2.5 Hi! I also want to note that you may need to stop and start any affected instances after upgrade and deploying SWB v5.2.5.\r\n\r\nIf this fixes your issue, please go ahead and close this issue. I am going to mark as closing-soon-if-no-response so we will close in about 7 days if we do not hear that this did not resolve the issue.\r\n\r\nThank you for the report! I pulled the changes committed for v5.2.5 into our forked repository, which is locked at 5.0.0 version. Auto stop works. Still not sure how the file got replaced with the one in that repo, but it's a non-issue at the moment. Thank you! :D",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":8.9,
        "Solution_reading_time":108.09,
        "Solution_score_count":null,
        "Solution_sentence_count":81.0,
        "Solution_word_count":1171.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Problem with <a href=\"https:\/\/learn.microsoft.com\/en-gb\/rest\/api\/azureml\/workspacesandcomputes\/workspaces\/createorupdate\">https:\/\/learn.microsoft.com\/en-gb\/rest\/api\/azureml\/workspacesandcomputes\/workspaces\/createorupdate<\/a> API... In the request body, Is it mandatory to create storage account, app insights, key vault, registration resources before? Ideally since these are dependent resources, shouldn\u2019t it be created as part of workflow creation?    <br \/>\nI get below response when dependent resources are not created prior.    <\/p>\n<pre><code>`{  \n  \u201cerror\u201d: {  \n    \u201ccode\u201d: \u201cValidationError\u201d,  \n    \u201cmessage\u201d: \u201cOne or more validation errors occured.\u201c,  \n    \u201cmessageFormat\u201d: null,  \n    \u201cmessageParameters\u201d: null,  \n    \u201creferenceCode\u201d: null,  \n    \u201cdetailsUri\u201d: null,  \n    \u201ctarget\u201d: \u201cCan not perform requested operation on nested resource. Parent resource \u2018&amp;lt;resourceid&amp;gt;\u2019 not found.\u201c,  \n    \u201cdetails\u201d: [],  \n    \u201cinnerError\u201d: null,  \n    \u201cdebugInfo\u201d: null  \n  },  \n  \u201ccorrelation\u201d: {  \n    \u201coperation\u201d: \u201c&amp;lt;opid&amp;gt;\u201c,  \n    \u201crequest\u201d: \u201c&amp;lt;reqid&amp;gt;\u201d  \n  },  \n  \u201cenvironment\u201d: \u201cwestus\u201d,  \n  \u201clocation\u201d: \u201cwestus\u201d,  \n  \u201ctime\u201d: \u201c2020-06-03T07:13:14.6463577+00:00&amp;#34;  \n}`  \n<\/code><\/pre>\n<p>I need an API which works similar to <a href=\"https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/ext\/azure-cli-ml\/ml\/workspace?view=azure-cli-latest\">https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/ext\/azure-cli-ml\/ml\/workspace?view=azure-cli-latest<\/a>    <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591181902550,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/31569\/rest-api-to-create-or-update-azure-ml-workspace-do",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":19.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Rest api to create or update azure ML workspace doesn't create dependant resources",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":124,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=3808bcc5-5aa6-41c8-bdfa-ab818e88679f\">@Harshini K S  <\/a>,    <\/p>\n<p>Yes, the REST API needs the other resource ids to be mentioned in the request body or they need to be created prior to this call unlike azure cli which provides the option to create them in a single request with input parameters. You could also try to use <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-workspace-template\">ARM<\/a> template to create all the resources by calling this action from PS or cli.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.4,
        "Solution_reading_time":6.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1611181716003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":119.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a Pipeline in Azure ML which makes calls to Azure Cognitive Services Text Analytics using its Python API. When I run the code I have written locally, it executes without error, but when run it in the pipeline it fails to perform the Sentiment Analysis and Key Phrase Extraction calls with a strange error message:<\/p>\n<blockquote>\n<p>Got exception when invoking script at line 243 in function azureml_main: 'ServiceRequestError: &lt;urllib3.connection.HTTPSConnection object at 0x7ff4dc727588&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'.<\/p>\n<\/blockquote>\n<p>Upon further testing, it appears that it is able to open the Text Analytics Client correctly (Or at least without throwing an error), but when it gets to the line that actually makes the call out using the Python API it throws the above error.<\/p>\n<p>I wondered if it was an Open SSL issue, but when I checked the version it had access to TLS 1.2: <code>OpenSSL 1.1.1k  25 Mar 2021<\/code><\/p>\n<p>It does not appear to be a temporary issue; I started seeing the issue last week, and I have seen it over a number of environments and with different input datasets.<\/p>\n<p>Has anyone seen a similar issue before? Any ideas on how it could be resolved?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1630894745580,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1630971784780,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69068520",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.6,
        "Challenge_reading_time":16.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Python Module failing to Execute Calls to Cognitive Services",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":299.0,
        "Challenge_word_count":217,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1611181716003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":119.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>After speaking with Microsoft Support, it turns out this error was a platform error introduced in a recent update of Azure ML. Their product team are currently investigating a solution.<\/p>\n<p>As a temporary fix, if you see this issue, you can try switching between using your personal endpoint and the generic regional endpoint; In this case, the error was only introduced for using personal endpoints. The endpoints in question have the following formats:<\/p>\n<ul>\n<li>Personal: <code>https:\/\/&lt;COGNITIVE-SERVICES-INSTANCE&gt;.cognitiveservices.azure.com\/<\/code><\/li>\n<li>Regional: <code>https:\/\/&lt;REGION&gt;.api.cognitive.microsoft.com\/<\/code><\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.0,
        "Solution_reading_time":8.61,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":79.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Based on this excerpt:\n\nNote:\u00a0Cloud TPU v4 capacity is located in us-central2 region. Currently, v4 is only available in zone\u00a0us-central2-b. See\u00a0Types and Topologies\u00a0for information about supported v4 TPU types and topologies.\n\nfrom https:\/\/cloud.google.com\/tpu\/docs\/regions-zones\n\n\u00a0\n\nI'm expecting to find the region however it's not present in the dropdown. CLI is also saying it's either forbidden or mistyped.",
        "Challenge_closed_time":1683617.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683373860000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Missing-us-central2-b-when-creating-TPU-VM\/m-p\/550953#M1811",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":5.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Missing `us-central2-b` when creating TPU VM",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":72.0,
        "Challenge_word_count":64,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Good day\u00a0@lukas0,\n\nWelcome to Google Cloud Community!\n\nAs of now, you need to reach out to Google Cloud Support in order to enable this feature in your Project. You can check the key note in this documentation:\u00a0https:\/\/cloud.google.com\/tpu\/docs\/system-architecture-tpu-vm#tpu-v4-config\n\nYou can use this link to reach out to Google Cloud Support:\u00a0https:\/\/cloud.google.com\/support\n\nHope this will help!\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":5.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":61.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"How can I extend the default time out period for SageMaker pre-signed URL ?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679526730196,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1679874833968,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUKDk-YzA5SlK_zw3jb5Ox3g\/sagemaker-pre-signed-url-timeout-extension",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.0,
        "Challenge_reading_time":1.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker pre-signed URL timeout extension",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":27.0,
        "Challenge_word_count":17,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The session timeout duration can specified using \n```\n`SessionExpirationDurationInSeconds`\n```\n in the API. Refer to https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePresignedDomainUrl.html#API_CreatePresignedDomainUrl_RequestParameters.\n This value defaults to 43200 which is the max allowed value.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1679527619211,
        "Solution_link_count":1.0,
        "Solution_readability":21.9,
        "Solution_reading_time":4.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1291793452900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berkeley, CA, United States",
        "Answerer_reputation_count":752.0,
        "Answerer_view_count":120.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are working on Azure ML and ADLS combination. Since HDInsight Cluster is working over ADLS, we are trying to use Hive Query and HDFS route and running into problems. \nRequest your help in solving the problem of reading data from hive query and writing to HDFS. Below is the error URL for reference:<\/p>\n\n<p><a href=\"https:\/\/studioapi.azureml.net\/api\/sharedaccess?workspaceId=025ba20578874d7086e6c495cc49a3f2&amp;signature=ZMUCNMwRjlrksrrmsrx5SaGedSgwMmO%2FfSHvq190%2F1I%3D&amp;sharedAccessUri=https%3A%2F%2Fesprodussouth001.blob.core.windows.net%2Fexperimentoutput%2Fccf9a206-730d-4773-b44e-a2dd8c6e87b9%2Fccf9a206-730d-4773-b44e-a2dd8c6e87b9.txt%3Fsv%3D2015-02-21%26sr%3Db%26sig%3DHkuFm8B2Ba1kEWWIwanqlv%2FcQPWVz0XYveSsZnEa0Wg%3D%26st%3D2017-10-16T18%3A31%3A06Z%26se%3D2017-10-17T18%3A36%3A06Z%26sp%3Dr\" rel=\"nofollow noreferrer\">https:\/\/studioapi.azureml.net\/api\/sharedaccess?workspaceId=025ba20578874d7086e6c495cc49a3f2&amp;signature=ZMUCNMwRjlrksrrmsrx5SaGedSgwMmO%2FfSHvq190%2F1I%3D&amp;sharedAccessUri=https%3A%2F%2Fesprodussouth001.blob.core.windows.net%2Fexperimentoutput%2Fccf9a206-730d-4773-b44e-a2dd8c6e87b9%2Fccf9a206-730d-4773-b44e-a2dd8c6e87b9.txt%3Fsv%3D2015-02-21%26sr%3Db%26sig%3DHkuFm8B2Ba1kEWWIwanqlv%2FcQPWVz0XYveSsZnEa0Wg%3D%26st%3D2017-10-16T18%3A31%3A06Z%26se%3D2017-10-17T18%3A36%3A06Z%26sp%3Dr<\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1508806505617,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1510470314876,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46900593",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":24.1,
        "Challenge_reading_time":19.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML - Import Hive Query Failing - Hive over ADLS",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":64.0,
        "Challenge_word_count":66,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419317006220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":97.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Azure Machine Learning supports Hive but not over ADLS. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1509134225963,
        "Solution_link_count":0.0,
        "Solution_readability":3.7,
        "Solution_reading_time":0.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1574678086832,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Amstelveen, Netherlands",
        "Answerer_reputation_count":3917.0,
        "Answerer_view_count":640.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have installed mlflow on GCP VM instance, \nnow I want to access mlflow UI with external IP.\nI tried setting up a firewall rule and opening the default port for mlflow, but not able to access it.\nCan someone give step by step process for just running mlflow on VM instance?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1583744387613,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1583832420660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60597319",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.2,
        "Challenge_reading_time":3.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Running MLFlow on GCP VM",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1537.0,
        "Challenge_word_count":56,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1451124057623,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":736.0,
        "Poster_view_count":234.0,
        "Solution_body":"<p>I've decided to check on my test VM and run mlflow server on GCE VM. Have a look at my steps below:<\/p>\n\n<ol>\n<li>create VM instance based on Ubuntu Linux 18.04 LTS<\/li>\n<li><p><a href=\"https:\/\/www.mlflow.org\/docs\/latest\/quickstart.html\" rel=\"noreferrer\">install MLflow<\/a>:<\/p>\n\n<pre><code>$ sudo apt update\n$ sudo apt upgrade\n$ cd ~\n$ git clone https:\/\/github.com\/mlflow\/mlflow\n$ cd mlflow\n$ sudo apt install python3-pip\n$ pip3 install mlflow\n$ python3 setup.py build\n$ sudo python3 setup.py install\n$ mlflow --version\nmlflow, version 1.7.1.dev0\n<\/code><\/pre><\/li>\n<li><p>run mlflow server on internal IP of VM instance (default 127.0.0.1):<\/p>\n\n<pre><code>$ ifconfig \nens4: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1460\ninet 10.XXX.15.XXX  netmask 255.255.255.255  broadcast 0.0.0.0\n...\n\n$ mlflow server --host 10.XXX.15.XXX\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Starting gunicorn 20.0.4\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Listening at: http:\/\/10.128.15.211:5000 (8631)\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Using worker: sync\n[2020-03-09 15:05:50 +0000] [8634] [INFO] Booting worker with pid: 8634\n[2020-03-09 15:05:51 +0000] [8635] [INFO] Booting worker with pid: 8635\n[2020-03-09 15:05:51 +0000] [8636] [INFO] Booting worker with pid: 8636\n[2020-03-09 15:05:51 +0000] [8638] [INFO] Booting worker with pid: 8638\n<\/code><\/pre><\/li>\n<li><p>check from VM instance (from second connection):<\/p>\n\n<pre><code>$ curl -I http:\/\/10.XXX.15.XXX:5000\nHTTP\/1.1 200 OK\nServer: gunicorn\/20.0.4\nDate: Mon, 09 Mar 2020 15:06:08 GMT\nConnection: close\nContent-Length: 853\nContent-Type: text\/html; charset=utf-8\nLast-Modified: Mon, 09 Mar 2020 14:57:11 GMT\nCache-Control: public, max-age=43200\nExpires: Tue, 10 Mar 2020 03:06:08 GMT\nETag: \"1583765831.3202355-853-3764264575\"\n<\/code><\/pre><\/li>\n<li><p><a href=\"https:\/\/cloud.google.com\/vpc\/docs\/add-remove-network-tags\" rel=\"noreferrer\">set network tag<\/a> <code>mlflow-server<\/code> <\/p><\/li>\n<li><p><a href=\"https:\/\/cloud.google.com\/vpc\/docs\/using-firewalls#creating_firewall_rules\" rel=\"noreferrer\">create firewall rule<\/a> to allow access on port 5000<\/p>\n\n<pre><code>$ gcloud compute --project=test-prj firewall-rules create mlflow-server --direction=INGRESS --priority=999 --network=default --action=ALLOW --rules=tcp:5000 --source-ranges=0.0.0.0\/0 --target-tags=mlflow-server\n<\/code><\/pre><\/li>\n<li><p>check from on-premises Linux machine <code>nmap -Pn 35.225.XXX.XXX<\/code><\/p>\n\n<pre><code>Starting Nmap 7.80 ( https:\/\/nmap.org ) at 2020-03-09 16:20 CET\nNmap scan report for 74.123.225.35.bc.googleusercontent.com (35.225.XXX.XXX)\nHost is up (0.20s latency).\nNot shown: 993 filtered ports\nPORT     STATE  SERVICE\n...\n5000\/tcp open   upnp\n...\n<\/code><\/pre><\/li>\n<li><p>go to web browser <a href=\"http:\/\/35.225.XXX.XXX:5000\/\" rel=\"noreferrer\">http:\/\/35.225.XXX.XXX:5000\/<\/a><\/p><\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/u2aFt.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/u2aFt.png\" alt=\"mlflow\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":11.0,
        "Solution_readability":8.4,
        "Solution_reading_time":39.18,
        "Solution_score_count":5.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":297.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1508836189288,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Finland",
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":5.5372291667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need to connect with Azure ML Workspace during deployment over container instance.<\/p>\n<pre><code>ws = Workspace(subscription_id=&quot;your-sub-id&quot;,\n              resource_group=&quot;your-resource-group-id&quot;,\n              workspace_name=&quot;your-workspace-name&quot;\n              )\n<\/code><\/pre>\n<p>Interactive Authentication to the ML Workspace prompts to login and then fails with below error message.<\/p>\n<pre><code>AttributeError: 'BasicTokenAuthentication' object has no attribute 'get_token'\n<\/code><\/pre>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication#interactive-authentication\" rel=\"nofollow noreferrer\">i have been following this Azure Authentication document.<\/a><\/p>\n<p>Any help is much appreciated.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1600761544463,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64005433",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":20.4,
        "Challenge_reading_time":10.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML operations : workspace authentication error",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":706.0,
        "Challenge_word_count":63,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530258066240,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":125.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>For me this was fixed by updating azureml-core from 1.13.0 to 1.14.0.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1600781478488,
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":0.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Perform AZURE SQL query by joining table 1 from (Azure Prodution) database 1 to table 2 from (Azure Production) database 2?   <br \/>\nThe result to be saved as a table in database 3 (Development)  <\/p>\n<p>Eg.   <\/p>\n<p>SELECT  T1.CustomerID, T2.CustomerName  <br \/>\nFROM database1.SalesTable AS T1  <br \/>\nLEFT JOIN database2.CustomerTable AS T2  <\/p>\n<p>ON database1.SalesTable.CustomerID = database2.CustomerTable  <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643778504613,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/719171\/can-i-query-perform-sql-query-by-joining-table-1-f",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":7.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Can I query perform SQL query by joining table 1 from (Prodution) database 1 to table 2 from (Production) databse 2? The result to be saved as a table in database 3 (Development)",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":88,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey,  <br \/>\nIt is possible by couple of ways:<\/p>\n<p>1) Create an external table in either of the Production database:  <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/azure-sql\/database\/elastic-query-getting-started-vertical\">https:\/\/learn.microsoft.com\/en-us\/azure\/azure-sql\/database\/elastic-query-getting-started-vertical<\/a><\/p>\n<p>Then in Azure data factory use a copy activity to have your query as source and your sink as the table 3 in database dev.  <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/connector-sql-server\">https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/connector-sql-server<\/a><\/p>\n<p>But based on the amount of data there might be performance issues due to elastic query so we can go with the below approach :<\/p>\n<p>2) use data flow to join both the data from table 1 and table 2 and copy into table 3<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.0,
        "Solution_reading_time":11.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":96.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Is it possible to get the public-ip of an amazon <code>sagemaker<\/code> notebook instance?<\/p>\n\n<p>I was wondering if I can ssh into it using the public ip for remote debugging purposes.<\/p>\n\n<p>I tried getting the public ip using the below curl command<\/p>\n\n<pre><code>$curl http:\/\/169.254.169.254\/latest\/meta-data\n<\/code><\/pre>\n\n<p>This just lists the local ip and not the public ip.<\/p>\n\n<p>I also tried the below command.<\/p>\n\n<pre><code>$curl ifconfig.me\n<\/code><\/pre>\n\n<p>This returns an ip address like <code>13.232.96.15<\/code>. If I try ssh into this it doesnt work.<\/p>\n\n<p>Is there any other way we can do this?<\/p>\n\n<p>Note : The ssh port 22 is open already in the security group<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563361386390,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57074382",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":5.8,
        "Challenge_reading_time":9.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"How to get the public ip of amazon sagemaker's notebook instance? Is it possible?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":4461.0,
        "Challenge_word_count":114,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455097846150,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":1347.0,
        "Poster_view_count":217.0,
        "Solution_body":"<p>I don't think you can ssh to notebook instances. You can either use open them from the console, or grab the url with an API, re: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-access-ws.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-access-ws.html<\/a><\/p>\n\n<p>If you need a terminal, then you can open one from Jupyter.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.4,
        "Solution_reading_time":5.01,
        "Solution_score_count":5.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"When I don't have the optional MLFlow dependency installed I get the following exception the first time I try to import the `numbertracker`.  The second time I run the import, everything works just fine.\r\n\r\n```python\r\nfrom whylogs.core.statistics import numbertracker\r\n\r\n\r\n\r\nFailed to import MLFLow\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n<ipython-input-1-3964e19b3cb4> in <module>\r\n----> 1 from whylogs.core.statistics import numbertracker\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/__init__.py in <module>\r\n      4 from .app.session import get_or_create_session\r\n      5 from .app.session import reset_default_session\r\n----> 6 from .mlflow import enable_mlflow\r\n      7 \r\n      8 __all__ = [\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/mlflow\/__init__.py in <module>\r\n----> 1 from .patcher import enable_mlflow\r\n      2 \r\n      3 __all__ = [\"enable_mlflow\"]\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/mlflow\/patcher.py in <module>\r\n    145 \r\n    146 _active_whylogs = []\r\n--> 147 _original_end_run = mlflow.tracking.fluent.end_run\r\n    148 \r\n    149 \r\n\r\nNameError: name 'mlflow' is not defined\r\n```",
        "Challenge_closed_time":1603222.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603138068000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/72",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.9,
        "Challenge_reading_time":14.1,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFlow NameError",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":108,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<br>\nsometimes I meet with this kind of error, when I run on a remote device:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/3\/3210bee6d41f470899393ebd804a54d0b8eacb85.png\" data-download-href=\"\/uploads\/short-url\/78TKyZQzgGPLZDlHXk14ZJlglYV.png?dl=1\" title=\"Screenshot from 2023-04-23 08-40-26\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/3\/3210bee6d41f470899393ebd804a54d0b8eacb85_2_690x93.png\" alt=\"Screenshot from 2023-04-23 08-40-26\" data-base62-sha1=\"78TKyZQzgGPLZDlHXk14ZJlglYV\" width=\"690\" height=\"93\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/3\/3210bee6d41f470899393ebd804a54d0b8eacb85_2_690x93.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/3\/3210bee6d41f470899393ebd804a54d0b8eacb85_2_1035x139.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/3\/3210bee6d41f470899393ebd804a54d0b8eacb85_2_1380x186.png 2x\" data-dominant-color=\"2A2319\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2023-04-23 08-40-26<\/span><span class=\"informations\">1482\u00d7200 40.2 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\n<code>Error: Run xxx errored: AssertionError()<\/code><br>\nDubugging tool in vscode cannot locate it. It just pops up and tells you the run is failed. I can find which line is the reason of this error, but have no idea what happens.<br>\nAfter checking for much time, I can do nothing but disconnect with remote device and connect again. And the problem is gone\u2026 Is there a more specific reason?<br>\nThanks a lot!<br>\nJialei Li<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682232309854,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-assertion-errror\/4260",
        "Challenge_link_count":5,
        "Challenge_participation_count":4,
        "Challenge_readability":16.1,
        "Challenge_reading_time":26.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Wandb assertion errror",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":55.0,
        "Challenge_word_count":139,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,<br>\nI found the reason now. It is relative with other part of my project\u2019s program. It has nothing to do with wandb actually\u2026 Still thanks a lot for your willingness to check error for me <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.8,
        "Solution_reading_time":4.77,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":44.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I can't see a data drift module anywhere in v2 of the Azure ML Python SDK. Is this missing or what's the deal? If so, are there any plans of bringing it into v2?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1657283475557,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/919651\/datadrift-in-azure-ml-sdk-v2",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":2.4,
        "Challenge_reading_time":2.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Datadrift in Azure ML SDK v2",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=1dc2a0bd-ac4b-413b-bae7-930e0079e70d\">@SH  <\/a>     <\/p>\n<p>I have a good news for you, I just got confirmation from product team, the datadrift function will be in SDK V2 for sure. But for now we don't have an exact date for when. I have forwarded this feedback to product group and we hope we can bring this feature in near future.     <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.3,
        "Solution_reading_time":6.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1270568377790,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Philadelphia",
        "Answerer_reputation_count":13056.0,
        "Answerer_view_count":354.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using AWS sagemaker, I have some secret keys and access keys to access some APIs that I don't want to expose directly in code.<\/p>\n<p>What are the ways like environment variables etc., that can be used to hide these keys and I can use them securely, and how to set them.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625052591003,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1625058756808,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68193944",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.4,
        "Challenge_reading_time":3.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Set custom environment variables in AWS",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":277.0,
        "Challenge_word_count":58,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1567880532003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lahore, Pakistan",
        "Poster_reputation_count":137.0,
        "Poster_view_count":100.0,
        "Solution_body":"<p>AWS System Manager (SSM) is designed to store keys and tokens securely.<\/p>\n<p>Depending on how your notebook is defined, you could <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">use the 'env' property<\/a> directly or in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-environment-variables\" rel=\"nofollow noreferrer\">training data<\/a>, or you could access SSM directly from sagemaker. For example this Snowflake KB article explains how to fetch auth info from ssm: <a href=\"https:\/\/community.snowflake.com\/s\/article\/Connecting-a-Jupyter-Notebook-Part-3\" rel=\"nofollow noreferrer\">https:\/\/community.snowflake.com\/s\/article\/Connecting-a-Jupyter-Notebook-Part-3<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":23.8,
        "Solution_reading_time":11.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1517578984080,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2090.0,
        "Answerer_view_count":163.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have a .pkl which I would like to put into production. I would like to do a daily query of my SQL server and do a prediction on about 1000 rows. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">documentation<\/a> implies I have to load the daily data into s3. Is there a way around this? It should be able to fit in memory no problem. <\/p>\n\n<p>The answer to \" <a href=\"https:\/\/stackoverflow.com\/questions\/48319893\/is-there-some-kind-of-persistent-local-storage-in-aws-sagemaker-model-training\">is there some kind of persistent local storage in aws sagemaker model training?<\/a> \" says that \"<em>The notebook instance is coming with a local EBS (5GB) that you can use to copy some data into it and run the fast development iterations without copying the data every time from S3.<\/em>\" The 5GB could be enough but I am not sure you can actually run from a notebook in this manner. If I set up a VPN could I just query using pyodbc?<\/p>\n\n<p>Is there sagemaker integration with AWS Lambda? That in combination with a docker container would suit my needs.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531871216253,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1531893078607,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51391639",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":14.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Is it possible to predict in sagemaker without using s3",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1576.0,
        "Challenge_word_count":176,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400315847156,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4342.0,
        "Poster_view_count":315.0,
        "Solution_body":"<p>While you need to to specify a s3 \"folder\" as input, this folder can contain only a dummy file. \nAlso if you bring your own docker container for training like in <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">this example<\/a>, you can do pretty much everthing in it. So you could do your daily query inside your docker container, because they have access to the internet. <\/p>\n\n<p>Also inside this container you have access to all the other aws services. Your access is defined by the role you're passing to your training job.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":7.96,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"'m going through this notebook: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb\r\n\r\nI need to start the training using the docker image from my local registry. I provided all required data in the environment I created:\r\n\r\nconda_env.docker.enabled = True\r\nconda_env.docker.base_image = \"tf_od_api:latest\"\r\nconda_env.docker.base_image_registry.address = \"mylocalacr.azurecr.io\"\r\nconda_env.docker.base_image_registry.username = \"MyToken\"\r\nconda_env.docker.base_image_registry.password = \"MyPassword\"\r\n\r\nconda_env.python.user_managed_dependencies = True\r\n\r\nsrc = ScriptRunConfig(source_directory='azureml-examples\/workflows\/train\/fastai\/pets\/src',\r\n                      script='aml_wrapper.py',\r\n                      compute_target=attached_dsvm_compute,\r\n                      environment=conda_env)\r\nrun = exp.submit(config=src)\r\nrun.wait_for_completion(show_output=True)\r\n\r\nAnd when I start the pipeline I got: \"FailedPullingImage: Unable to pull docker image\\n\\timageName: Run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"\r\n\r\nIf I set conda_env.python.user_managed_dependencies = False\r\n\r\nthen the pipeline can pull my image from my local registry, build a new image with all required python dependencies on top of my base image and push the new image to my local registry. But on the second step of the pipeline, when it tries to pull the image for running it, that was just created and pushed, it again crashes with the same error: \"Run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"",
        "Challenge_closed_time":1614604.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614273729000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1371",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.6,
        "Challenge_reading_time":21.99,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Run docker command to pull public image failed ",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":179,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"please try this \r\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1247#issuecomment-738887772 Seems like it solved the issue. Thanks!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.7,
        "Solution_reading_time":1.91,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1435524174732,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bursa, Turkey",
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an aws sagemaker end-point which need to be called from .Net core client, I have used the AWS SDK that deals with SageMaker and provided the required credentials however, always it keeps saying : <\/p>\n\n<p>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.<\/p>\n\n<p>var requestBody = \"{'url':'\"+\"<a href=\"https:\/\/cdn.pixabay.com\/photo\/2018\/05\/28\/22\/11\/message-in-a-bottle-3437294_960_720.jpg\" rel=\"nofollow noreferrer\">https:\/\/cdn.pixabay.com\/photo\/2018\/05\/28\/22\/11\/message-in-a-bottle-3437294_960_720.jpg<\/a>\" + \"'}\";<\/p>\n\n<pre><code>        var request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest()\n        {\n            EndpointName = \"CG-model-v1-endpoint\",\n            ContentType = \"application\/json;utf-8\",\n            Body = new MemoryStream(Encoding.ASCII.GetBytes(JsonConvert.SerializeObject(requestBody)))\n\n        };\n\n\n        var awsClient = new AmazonSageMakerRuntimeClient(awsAccessKeyId: \"XXXX\", awsSecretAccessKey: \"XXX\", region: RegionEndpoint.EUCentral1);\n\n        try\n        {\n            var resposnse = await awsClient.InvokeEndpointAsync(request);\n\n        }\n        catch (Exception ex)\n        {\n\n            return ApiResponse&lt;bool&gt;.Create(false);\n        }\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1563803017410,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57147396",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":16.4,
        "Challenge_reading_time":16.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Amazon Sage Maker: How to authenticate AWS SageMaker End-Point Request",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":222.0,
        "Challenge_word_count":111,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1435524174732,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bursa, Turkey",
        "Poster_reputation_count":11.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>I found the error , it was simply because of the request content-type,it had to be application\/json instead of application\/json;utf-8<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.4,
        "Solution_reading_time":1.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416648155470,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":14749.0,
        "Answerer_view_count":968.0,
        "Challenge_adjusted_solved_time":76.2321219444,
        "Challenge_answer_count":1,
        "Challenge_body":"<h1>Question<\/h1>\n<p>Please advise how to trouble shoot the problem.<\/p>\n<h1>Problem<\/h1>\n<p>Cannot access the RedShift cluster endpoint from the SageMaker studio instance.<\/p>\n<pre><code>import socket\nsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nresult = sock.connect_ex(('dsoaws.cw7xniw3gvef.us-east-2.redshift.amazonaws.com',5439))\nif result == 0:\n   print(&quot;Port is open&quot;)\nelse:\n   print(&quot;Port is not open&quot;)\nsock.close()\n---\n\nPort is not open\n<\/code><\/pre>\n<h1>RedShift Cluster<\/h1>\n<p>Endpoint is <code>dsoaws.cw7xniw3gvef.us-east-2.redshift.amazonaws.com:5439\/dsoaws<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/t4w92.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t4w92.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The network setting shows the VPC is vpc-5b123432 allowing access from sg-56cb133e.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/nu5kM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/nu5kM.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<hr \/>\n<h1>SageMaker Studio<\/h1>\n<p>The SageMaker Studio instance is in the save VPC vpc-5b123432. However, not sure if sg-56cb133e is actually attached to the SageMaker studio instance. Please advise how to confirm if the sg-56cb133e is attached to the instance.<\/p>\n<pre><code>import json\nimport boto3\nfrom botocore.exceptions import ClientError\nfrom botocore.config import Config\n\nconfig = Config(\n   retries = {\n      'max_attempts': 10,\n      'mode': 'adaptive'\n   }\n)\n\n\niam = boto3.client('iam', config=config)\nsts = boto3.client('sts')\nredshift = boto3.client('redshift')\nsm = boto3.client('sagemaker')\nec2 = boto3.client('ec2')\n\ntry:\n    domain_id = sm.list_domains()['Domains'][0]['DomainId'] #['NotebookInstances'][0]['NotebookInstanceName']\n    describe_domain_response = sm.describe_domain(DomainId=domain_id)\n    vpc_id = describe_domain_response['VpcId']\n    print(vpc_id)\n    security_groups = ec2.describe_security_groups()['SecurityGroups']\n    for security_group in security_groups:\n        if vpc_id == security_group['VpcId']:\n            security_group_id = security_group['GroupId']\n    print(security_group_id)\nexcept:\n    pass\n-----\nvpc-5b123432\nsg-56cb133e\n<\/code><\/pre>\n<hr \/>\n<h1>Security Group<\/h1>\n<p><a href=\"https:\/\/i.stack.imgur.com\/JgCgi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JgCgi.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gjmLh.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gjmLh.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h1>IAM<\/h1>\n<p>The IAM role <code>SageMaker<\/code> is attached to the SageMaker Studio.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oY9ix.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oY9ix.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1627882823273,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68616817",
        "Challenge_link_count":10,
        "Challenge_participation_count":1,
        "Challenge_readability":15.4,
        "Challenge_reading_time":38.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS - Cannot access RedShift endpoint from the SageMaker Studio",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":434.0,
        "Challenge_word_count":229,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416648155470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":14749.0,
        "Poster_view_count":968.0,
        "Solution_body":"<h1>Cause<\/h1>\n<p>Did not use VPC Only sagemaker deployment as having used the Quick Start onboard.<\/p>\n<h1>Fix<\/h1>\n<ol>\n<li>Deleted the SageMaker Studio. R<\/li>\n<li>Recreated by using the Standard Setup + VPC only\n<a href=\"https:\/\/i.stack.imgur.com\/zZ7be.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zZ7be.png\" alt=\"enter image description here\" \/><\/a><\/li>\n<li>Added the NAT and configured the routing tables.<\/li>\n<\/ol>\n<h1>References<\/h1>\n<ul>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/securing-amazon-sagemaker-studio-connectivity-using-a-private-vpc\/\" rel=\"nofollow noreferrer\">Securing Amazon SageMaker Studio connectivity using a private VPC<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-studio-vpc-networkfirewall\" rel=\"nofollow noreferrer\">Amazon SageMaker Studio in a private VPC with NAT Gateway and Network Firewall<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1628157258912,
        "Solution_link_count":4.0,
        "Solution_readability":17.8,
        "Solution_reading_time":12.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1491421190663,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Salt Lake City, UT, USA",
        "Answerer_reputation_count":426.0,
        "Answerer_view_count":89.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am having problems accessing tables in an Oracle database over a SQLAlchemy connection. Specifically, I am using Kedro <code>catalog.load('table_name')<\/code> and getting the error message <code>Table table_name not found<\/code>. So I decided to test my connection using the method listed in this answer: <a href=\"https:\/\/stackoverflow.com\/questions\/41887344\/how-to-verify-sqlalchemy-engine-object\">How to verify SqlAlchemy engine object<\/a>.<\/p>\n<pre><code>from sqlalchemy import create_engine\nengine = create_engine('oracle+cx_oracle:\/\/USER:PASSWORD@HOST:PORT\/?service_name=SERVICE_NAME')\nengine.connect()\n<\/code><\/pre>\n<p>Error: <code>InvalidRequestError: could not retrieve isolation level<\/code><\/p>\n<p>I have tried explicitly adding an isolation level as explained in the <a href=\"https:\/\/docs.sqlalchemy.org\/en\/14\/core\/connections.html#setting-transaction-isolation-levels-including-dbapi-autocommit\" rel=\"nofollow noreferrer\">documentation<\/a> like this:<\/p>\n<pre><code>engine = create_engine('oracle+cx_oracle:\/\/USER:PASSWORD@HOST:PORT\/?service_name=SERVICE_NAME', execution_options={'isolation_level': 'AUTOCOMMIT'})\n<\/code><\/pre>\n<p>and this:<\/p>\n<pre><code>engine.connect().execution_options(isolation_level='AUTOCOMMIT')\n<\/code><\/pre>\n<p>and this:<\/p>\n<pre><code>connection = engine.connect()\nconnection = connection.execution_options(\n    isolation_level=&quot;AUTOCOMMIT&quot;\n)\n<\/code><\/pre>\n<p>but I get the same error in all cases.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1611242774307,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1611265283776,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65830524",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":20.9,
        "Challenge_reading_time":20.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"SQLAlchemy Oracle - InvalidRequestError: could not retrieve isolation level",
        "Challenge_topic":"Database Connection",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":196.0,
        "Challenge_word_count":116,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1491421190663,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Salt Lake City, UT, USA",
        "Poster_reputation_count":426.0,
        "Poster_view_count":89.0,
        "Solution_body":"<p>Upgrading from SqlAlchemy 1.3.21 to 1.3.22 solved the problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":0.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":9.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Can Sagemaker Git Repositories use ssh secrets (no name and password)?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649790570262,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668627587288,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU-P1Hlk4OR6K6kAug-wHT_g\/can-sagemaker-git-repositories-use-ssh-secrets-no-name-and-password",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":1.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Can Sagemaker Git Repositories use ssh secrets (no name and password)?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":1708.0,
        "Challenge_word_count":21,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes, Sagemaker can use SSH for private repos. There are multiple options on how to connect to a repo in Sagemaker.\n\n** Option 1**: Using SSH to work with a private repo\nYou can follow the same steps you do in your local machine to connect to a private repo through SSH, steps to follow:\n    \n1. Open `Terminal` and type `ssh-keygen` to create an SSH key in your Amazon Sagemaker instance. \n2. Add the public key to your Git account (Github or Gitlab)\n3. Get the SSH url of your repo and git clone\n\n**Option 2**: Using AWS Secret Manager \nYou can follow the steps in AWS official documentation [here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-resource.html). \n\n\n**Option 3**: Using GitHub with Personal Access Tokens **Recommended**\n\nLet\u2019s assume you have already generated an Access Tokens through the GitHub\u2019s Settings \/ Developer Settings \/ Personal Access Tokens page.\n\nYou can just simply go ahead and clone the repository using Studio UI. When it asks your username and password, you can provide your GitHub username and the Personal Access Token. If you want to cache your credentials avoiding to type it every time when you\u2019re interacting with the GitHub server, you can cache or store it on your home folder with the following command issued in the Terminal:\n\n``` $ git config --global git credential.helper [cache|store] ```\n\nIf you choose to store your credentials, it will be written to the `~\/.git-credentials` file located in your home folder. The \u201ccache\u201d helper stores the credential in-memory only and never lands on disk. It also accepts the --timeout <seconds> option, which changes the amount of time its daemon is kept running (the default is \u201c900\u201d, or 15 minutes)\n\nBefore you make your first commit, you still need to configure the git client to use your identity when we\u2019re checking in some new code into the repository. You need to run the following two commands from the terminal:\n```\n$ git config --global user.email \u201cuser@email.com\u201d\n$ git config --global user.name \u201cUser Name\u201d\n```\n\nSagemaker Studio is fully integrated with git and you can do it through the UI.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1650645924384,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":25.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":335.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1568185673007,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Kuala Lumpur, Federal Territory of Kuala Lumpur, Malaysia",
        "Answerer_reputation_count":383.0,
        "Answerer_view_count":35.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Was working on az ml cli v2 to deploy real-time endpoint with command <code>az ml online-deployment<\/code> through Azure pipeline. had double confirmed that the service connection used in this pipeline task had added the permissions below in Azure Portal but still showing the same error.<\/p>\n<pre><code>ERROR: Error with code: You don't have permission to alter this storage account. Ensure that you have been assigned both Storage Blob Data Reader and Storage Blob Data Contributor roles.\n<\/code><\/pre>\n<p>Using the same service connection, we are able to perform the creation of online endpoint with <code>az ml online-endpoint create<\/code> in the same and other workspaces.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655363403560,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72641789",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":9.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning workspace's storage account permission issue",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":129.0,
        "Challenge_word_count":109,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568185673007,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kuala Lumpur, Federal Territory of Kuala Lumpur, Malaysia",
        "Poster_reputation_count":383.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>Issue was resolved. I did not change anything in the service principal and running it on second day using same yml got through the issue. I guess there might be some propagation issue, but longer than usual.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":2.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Trying to use clearml-server on own Ubuntu 18.04.5.<\/p>\n<p>I use env variables to set the IP Address of my clearml-server.<\/p>\n<pre><code>export CLEARML_HOST_IP=127.0.0.1\nexport TRAINS_HOST_IP=127.0.0.1\n<\/code><\/pre>\n<p>But it still is available thorugh the external server IP.\nHow can I deactivate the listeners for external IP in clearml-server config?<\/p>\n<p>Edit:\nAccording to this:\nI use SSH Port forward to access local instance from my computer outside of the network. But I can't access custom uploaded images (task-&gt; debug samples) as they will not use my port forwarded URLs.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610294996320,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1610296597728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65655382",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":8.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"ClearML server IP address not used with localhost and SSH port forwarding",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":261.0,
        "Challenge_word_count":97,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604391794420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":89.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Disclaimer: I'm a ClearML (Trains) team member<\/p>\n<p>Basically the docker-compose will expose only the API\/Web\/File server , you can further limit the exposure to your localhost only, by changing the following section in your ClearML server <a href=\"https:\/\/github.com\/allegroai\/clearml-server\/blob\/master\/docker\/docker-compose.yml\" rel=\"nofollow noreferrer\">docker-compose.yml<\/a><\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>networks:\n  backend:\n    driver:\n      bridge\n    driver_opts:\n      com.docker.network.bridge.host_binding_ipv4: &quot;127.0.0.1&quot;\n  frontend:\n    driver: \n      bridge\n    driver_opts:\n      com.docker.network.bridge.host_binding_ipv4: &quot;127.0.0.1&quot;\n\n<\/code><\/pre>\n<p>Based on docker's <a href=\"https:\/\/docs.docker.com\/engine\/reference\/commandline\/network_create\/\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.9,
        "Solution_reading_time":11.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":62.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1508797229168,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":1115.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have uploaded a big (10+gb) dataset into Azure Blob Storage, containing thousands of images (jpg) format.<br>\nI registered the blob container in Azure Machine Learning Service as a data store and I also registered a File Dataset, pointing to the actual blob container, containing the images. (showing there are 44440 images).<\/p>\n\n<p>Now, I want to run a notebook (in AzureML) that needs to read a specific image and load it into an image (using <code>cv2.imread()<\/code>).  However, I don't seem to find the right documentation for this...  The only option I see is to download the entire dataset onto the local temp storage, which I prefer not to do (multiple giga bytes).<\/p>\n\n<p>Is there an option I can use to access the actual file reference and pass it to my 3rd party method?<\/p>\n\n<p>Below you can find some code that is relevant:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># get workspace and datastore\nws = Workspace.from_config()\ndstore = ws.datastores[datastore_name]\nimage_dataset = ws.datasets[image_dataset_name]\n\nmounted_images = image_dataset.mount() \n\nimg = cv2.imread(mounted_images['my_file_name.jpg']) # this will not work\n<\/code><\/pre>\n\n<p>Any idea on how to get this to work?<\/p>\n\n<p>Thank you<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581343009563,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60151965",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.1,
        "Challenge_reading_time":16.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I access files (images) in an azureml FileDataSet?",
        "Challenge_topic":"Dataset Mounting",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":3000.0,
        "Challenge_word_count":184,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>dataset.mount() actually returns a MountContext which has a mount_point attribute. <\/p>\n\n<p>So:<\/p>\n\n<p>img = cv2.imread(mounted_images.mount_point +\u2019\/my_file_name.jpg')<\/p>\n\n<p>Should hopefully work.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.7,
        "Solution_reading_time":2.73,
        "Solution_score_count":4.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":18.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1487450813192,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Lviv, Ukraine",
        "Answerer_reputation_count":1047.0,
        "Answerer_view_count":88.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an issue about acessing data in a BigQuery table in one project using a VertexAI in another project.<\/p>\n<p>Now, I own both project and have service accounts in both project, which implies that I also have the key (credential.json in the project containing the data) which I can use to define my client:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\n\ncredentials = service_account.Credentials.from_service_account_file('credentials.json')\nproject_id = 'cloud-billing-XXXX'\nclient = bigquery.Client(credentials= credentials,project=project_id)\n<\/code><\/pre>\n<p>which should be enough to run:<\/p>\n<pre><code>%%bigquery\nSELECT * FROM `cloud-billing-XXXX.all_billing_detailed.gcp_billing_export` limit 100\n<\/code><\/pre>\n<p>but I get the error:<\/p>\n<pre><code>ERROR:\n 403 Access Denied: Table cloud-billing-XXXX:all_billing_detailed.gcp_billing_export: User does not have permission to query table cloud-billing-XXXX:all_billing_detailed.gcp_billing_export.\n<\/code><\/pre>\n<p>I can, from the project containing my notebook, query the table in BigQuery. This makes me think that the problem is a VertexAI permission issue. I read somewhere that the service account used when defining the notebook must match the service account in the project the data resides in. I tried to create a workbench notebook with the service account in the first project and it is created but when I try to open it it refuses to do so and get an error message.<\/p>\n<p>I've also tried to grant Editor and job user permissions across both project but that wouldn't work either.<\/p>\n<p>Any experiences and ideas on how to solve this would be greatly appreciated.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1659713227817,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73252066",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.2,
        "Challenge_reading_time":22.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Query BQ table with Jupyter Notebook across owned projects",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":67.0,
        "Challenge_word_count":237,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442929315876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Stockholm, Sverige",
        "Poster_reputation_count":4598.0,
        "Poster_view_count":855.0,
        "Solution_body":"<ol>\n<li>As a mentioned in the comment: add your notebook service account to the first project (which contains your BigQuery data) and grant it with <strong>Bigquery Job User<\/strong> and <strong>BigQuery Data Viewer<\/strong> permissions.<\/li>\n<li>You can query data directly in you python code (without using &quot;magic&quot; %%bigquery). Just add the next two rows:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>pct_overlap_terms_by_days_apart = client.query(&quot;SELECT * FROM `cloud-billing-XXXX.all_billing_detailed.gcp_billing_export` limit 100&quot;).to_dataframe()\npct_overlap_terms_by_days_apart.head()\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Check out table name. If table path is wrong, then you'll get the same error: <strong>403 Access Denied<\/strong>.<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":10.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":84.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to create a workspace in azure machine learning and receiving this error after 2 browser Windows open and I click log in.<\/p>\n<blockquote>\n<p>library(azuremlsdk)  <br \/>\nnew_ws &lt;- create_workspace(name = 'muffin',<\/p>\n<\/blockquote>\n<ul>\n<li>   subscription_id = 'XXXXXXXXXXXX',<\/li>\n<li>   resource_group = 'white',<\/li>\n<li>   location = 'eastus2',<\/li>\n<li>   create_resource_group = T)  <br \/>\n    Note, we have launched a browser for you to login. For old experience with device code, use &quot;az login --use-device-code&quot;  <br \/>\n    You have logged in. Now let us find all the subscriptions to which you have access...  <br \/>\n    Note, we have launched a browser for you to login. For old experience with device code, use &quot;az login --use-device-code&quot;  <br \/>\n    You have logged in. Now let us find all the subscriptions to which you have access...  <br \/>\n    Error in py_call_impl(callable, dots$args, dots$keywords) :  <br \/>\n    AuthenticationException: AuthenticationException:  <br \/>\n    Message: Could not retrieve user token. Please run 'az login'  <br \/>\n    InnerException It is required that you pass in a value for the &quot;algorithms&quot; argument when calling decode().  <br \/>\n    ErrorResponse  <br \/>\n    {  <br \/>\n    &quot;error&quot;: {  <br \/>\n    &quot;code&quot;: &quot;UserError&quot;,  <br \/>\n    &quot;inner_error&quot;: {  <br \/>\n    &quot;code&quot;: &quot;Authentication&quot;  <br \/>\n    },  <br \/>\n    &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;  <br \/>\n    }  <br \/>\n    }<\/li>\n<\/ul>\n<p>how do I get passed this error?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1621290302507,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/398420\/azuremlsdk-for-r-error-could-not-retrieve-user-tok",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.5,
        "Challenge_reading_time":20.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"azuremlsdk for R error Could not retrieve user token. Please run 'az login'",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":null,
        "Challenge_word_count":207,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>You have to use this command to make it install the correct version of miniconda reticulate::py_install(&quot;PyJWT==1.7.1&quot;). If you don't do that it seems to install the wrong version. I also had to manually delete the r-miniconda folder in \\appdata\\local\\r-miniconda which got installed previously to get it to install the correct version. It's pretty outrageous they leave that out of the tutorial when it ain't going to work otherwise.<\/p>\n<p>If you try to do the accident.R tutorial for azuremlsdk-r next make sure you add the line<\/p>\n<p>interactive_auth &lt;- interactive_login_authentication(tenant_id=&quot;&lt;tenant id&gt;&quot;)<\/p>\n<p>to your code otherwise you'll get a permissions error and it won't work.<\/p>\n<p>Then to the create_workspace or get_workspace function you have to add auth = interactive_auth after a comma.<\/p>\n<p>It should look like this<\/p>\n<p>new_ws &lt;- get_workspace(name = &quot;&lt;workspace name&gt;&quot;,  <br \/>\nsubscription_id = &quot;&lt;subscription id&gt;&quot;,  <br \/>\nresource_group = &quot;&lt;resource name&gt;&quot;,  <br \/>\nauth = interactive_auth)<\/p>\n<p>To find the tenant ID I had to download the azure CLI and run the command az login. Not sure if there is another way to find a tenant ID or not.<\/p>\n<p>To leave out critical steps from a tutorial is gross incompetence on the part of Azure. How anyone who isn't a comp sci phd uses this service is a mystery to me.<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.5,
        "Solution_reading_time":17.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":203.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I see in this page of documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-interface-endpoint.html that:\n>\"*You can connect to your notebook instance from your VPC through an interface endpoint in your Virtual Private Cloud (VPC) instead of connecting over the internet. When you use a VPC interface endpoint, communication between your VPC and the notebook instance is conducted entirely and securely within the AWS network.*\"\n\nHow would customer interact on their laptop with the UI of a notebook instance sitting in a VPC?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541494962000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668438538512,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU5z-7bQ9zQOi_NrVlHy_5oA\/which-connection-method-when-using-sagemaker-notebook-through-vpc-interface-endpoint",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":8.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Which connection method when using SageMaker Notebook through VPC Interface Endpoint?",
        "Challenge_topic":"Network Configuration",
        "Challenge_topic_macro":"Access Management",
        "Challenge_view_count":466.0,
        "Challenge_word_count":88,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"If you are trying to access from within VPC, you'll have a direct connection. Otherwise, you'll need a configuration in place, such as Amazon VPN or AWS Direct Connect, to connect to your notebooks. Here is the blog post where we tried to explain how to set up AWS PrivateLink for Amazon SageMaker notebooks: https:\/\/aws.amazon.com\/blogs\/machine-learning\/direct-access-to-amazon-sagemaker-notebooks-from-amazon-vpc-by-using-an-aws-privatelink-endpoint\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925550451,
        "Solution_link_count":1.0,
        "Solution_readability":14.0,
        "Solution_reading_time":5.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    }
]