[
    {
        "Challenge_adjusted_solved_time":3.1542194444,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Has anyone found a good way to host wandb local on singularity rather than Docker? My understanding is that many institutions, mine included, do not allow use of docker because of security flaws within it.<\/p>",
        "Challenge_closed_time":1643227334984,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643215979794,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-local-on-singularity\/1820",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":9.4,
        "Challenge_reading_time":2.98,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.1542194444,
        "Challenge_title":"Wandb Local on Singularity?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":145.0,
        "Challenge_word_count":38,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/evanv\">@evanv<\/a>,<\/p>\n<p>We do not have official support for singularity. You could, however, follow existing tutorials on running Docker containers with Singularity.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"direct tutori run docker singular host local docker",
        "Solution_link_count":0.0,
        "Solution_original_content":"evanv offici singular tutori run docker singular ramit",
        "Solution_preprocessed_content":"offici singular tutori run docker singular ramit",
        "Solution_readability":13.8,
        "Solution_reading_time":3.01,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":34.1587602778,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<br>\nMe and a colleague are sharing a remote server with 8 GPUs. We split them, 4 GPUs each. In the system panel at the WANDB page I currently see data of all 8 GPUs. Is it possible to filter some of those curves, so I\u2019ll only see the GPUs I\u2019m using?<\/p>\n<p>Many thanks<\/p>",
        "Challenge_closed_time":1663689312779,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663566341242,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/filter-gpu-curves-in-the-system-panel\/3152",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":3.7,
        "Challenge_reading_time":3.79,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":34.1587602778,
        "Challenge_title":"Filter GPU curves in the system panel",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":174.0,
        "Challenge_word_count":60,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/yonatan-shimoni\">@yonatan-shimoni<\/a> thank you for writing in! Just to clarify here, is this for the System view that you can access from the left panel of an individual Run, or is it at the System panels section in your Project\u2019s Workspace? You can select which GPUs to visualise there by editing the Chart (pencil icon) as in the attached screenshot. Would this help?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818.png\" data-download-href=\"\/uploads\/short-url\/uFNI8njAc6A8srPIjwcR86x4AZi.png?dl=1\" title=\"Screenshot 2022-09-20 at 16.50.40\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818_2_476x500.png\" alt=\"Screenshot 2022-09-20 at 16.50.40\" data-base62-sha1=\"uFNI8njAc6A8srPIjwcR86x4AZi\" width=\"476\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818_2_476x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818_2_714x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818_2_952x1000.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2022-09-20 at 16.50.40<\/span><span class=\"informations\">1098\u00d71153 76.4 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"colleagu select gpu visual edit chart panel section workspac colleagu screenshot process",
        "Solution_link_count":6.0,
        "Solution_original_content":"yonatan shimoni write clarifi access left panel individu run panel section workspac select gpu visualis edit chart pencil icon attach screenshot screenshot",
        "Solution_preprocessed_content":"write clarifi access left panel individu run panel section workspac select gpu visualis edit chart attach screenshot screenshot",
        "Solution_readability":22.0,
        "Solution_reading_time":26.25,
        "Solution_score_count":null,
        "Solution_sentence_count":13.0,
        "Solution_word_count":113.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":27.7923416667,
        "Challenge_answer_count":3,
        "Challenge_body":"I am running through the tutorial at ..https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/create-clustering-model-azure-machine-learning-designer\/explore-data\n\nWhen I submit my pipeline I am seeing the error ...\n\nAn error occurred while submitting pipeline run\nGraphDatasetNotFound: Request failed with status code 400\n\nThis is an incredibly unhelpful message. I believe I have followed the steps as per the tutorial.\n\nAny idea what is the cause of this error?\n\nThanks",
        "Challenge_closed_time":1620057090880,
        "Challenge_comment_count":3,
        "Challenge_created_time":1619957038450,
        "Challenge_favorite_count":20.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/379678\/receive-error-graphdatasetnotfound-request-failed.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":11.4,
        "Challenge_reading_time":7.14,
        "Challenge_score_count":4.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":27.7923416667,
        "Challenge_title":"Receive error GraphDatasetNotFound: Request failed with status code 400 when submitting pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Solution_body":"In dataset Version change from \"Always use latest\" to 1 or anyother version, worked for me",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"dataset version latest version",
        "Solution_link_count":0.0,
        "Solution_original_content":"dataset version latest anyoth version",
        "Solution_preprocessed_content":"dataset version latest anyoth version",
        "Solution_readability":6.0,
        "Solution_reading_time":1.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":7.7009416667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I used this code to create a confusion matrix:<\/p>\n<pre><code class=\"lang-auto\"># confusion matrix\n        wandb.log({\"confusion-matrix-test\": wandb.plot.confusion_matrix(\n            probs=None,\n            y_true=all_gt, preds=all_pre,\n            class_names=classes_names)})\n<\/code><\/pre>\n<p>However, Wanda\u2019s website only shows a table instead of the confusion matrix. This is a screenshot from the issue:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046.png\" data-download-href=\"\/uploads\/short-url\/8UGiFwpsOZ6Pivp7qmFXgL1OuvI.png?dl=1\" title=\"Screenshot from 2022-01-09 20-58-35\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_690x249.png\" alt=\"Screenshot from 2022-01-09 20-58-35\" data-base62-sha1=\"8UGiFwpsOZ6Pivp7qmFXgL1OuvI\" width=\"690\" height=\"249\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_690x249.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_1035x373.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_1380x498.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2022-01-09 20-58-35<\/span><span class=\"informations\">1741\u00d7629 29.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1641809553312,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641781829922,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744",
        "Challenge_link_count":6,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":27.3,
        "Challenge_reading_time":26.87,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":7.7009416667,
        "Challenge_title":"Wandb.plot.confusion_matrix() just show a Table!",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":565.0,
        "Challenge_word_count":92,
        "Platform":"Tool-specific",
        "Solution_body":"<aside class=\"quote no-group\" data-username=\"fdaliran\" data-post=\"1\" data-topic=\"1744\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/f\/73ab20\/40.png\" class=\"avatar\"> fdaliran:<\/div>\n<blockquote>\n<pre><code class=\"lang-auto\">        wandb.log({\"confusion-matrix-test\": wandb.plot.confusion_matrix(\n            probs=None,\n            y_true=all_gt, preds=all_pre,\n            class_names=classes_names)})\n<\/code><\/pre>\n<\/blockquote>\n<\/aside>\n<p>If you click the section called \u201cCustom Charts\u201d above the Table, it\u2019ll show the line plot that you\u2019ve logged.<\/p>\n<p>Logging the Table also is expected behaviour because this will allow users to interactively explore the logged data in a W&amp;B Table after logging it.<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"plot matrix function displai tabl matrix websit click section call chart tabl line plot log log tabl allow interact explor log data tabl log",
        "Solution_link_count":1.0,
        "Solution_original_content":"fdaliran log matrix test plot matrix prob pred pre class class click section call chart tabl itll line plot youv log log tabl allow interact explor log data tabl log",
        "Solution_preprocessed_content":"fdaliran click section call chart tabl itll line plot youv log log tabl allow interact explor log data tabl log",
        "Solution_readability":15.1,
        "Solution_reading_time":10.22,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":73.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":6.0913719444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello, I am trying to add a user Id column to my dataset but I don't want the user Id to impact the results of the ML.\n\nI am using Auto ML on my dataset to generate a model and then deployed the model to an endpoint.\n\nCurrently I am calling the endpoint like:\n\n {\"data\":[\n        {\n           \"TEMP\":\"X\",\n         }\n     ]\n }\n\n\n\nand I would like to call it like:\n\n {\"data\":[\n     {\n       \"TEMP\":\"X\",\n       \"userID\": 5434643\n      }\n   ]}\n\n\n\nI'm wondering if there is a way I can do this? I've seen about using Clear Feature in Edit Metadata for the Designer but I'm wondering if something similar can be done for automated ML?\n\nThanks so much!",
        "Challenge_closed_time":1627949805196,
        "Challenge_comment_count":0,
        "Challenge_created_time":1627927876257,
        "Challenge_favorite_count":8.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/498759\/clear-feature-with-auto-ml.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":7.07,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":6.0913719444,
        "Challenge_title":"Clear Feature with Auto ML",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":110,
        "Platform":"Tool-specific",
        "Solution_body":"Hi, thanks for reaching out. You can customize featurization in automl to only include features relevant for prediction. Here's the documentation. Hope it helps!",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"featur automl featur relev predict document",
        "Solution_link_count":0.0,
        "Solution_original_content":"reach featur automl featur relev predict document hope",
        "Solution_preprocessed_content":"reach featur automl featur relev predict document hope",
        "Solution_readability":8.0,
        "Solution_reading_time":2.03,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":0.2769444444,
        "Challenge_answer_count":1,
        "Challenge_body":"SageMaker can train on FSx data. One SageMaker SDK parameter for FSx training is directory_path. Where do we find that?",
        "Challenge_closed_time":1604679222000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604678225000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCaemzfoDRIy9AgLRW8suqw\/sage-maker-training-with-f-sx-what-is-directory-path",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":2.17,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.2769444444,
        "Challenge_title":"SageMaker training with FSx: what is \"directory_path\"",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":105.0,
        "Challenge_word_count":26,
        "Platform":"Tool-specific",
        "Solution_body":"FSx for Lustre is a file system that you can use to provide high performance for ML training workloads. The directory_path should point to the location on your file system where your dataset is stored.\n\nIn the example in the docs: directory_path='\/fsx\/tensorflow',\n\n\/fsx is the directory you define on your compute instances where you are mounting the file system \/tensorflow would represent a folder within the fsx directory\n\nIf you are using an S3-linked FSx for Lustre file system \/tensorflow would be a prefix within your S3-linked bucket.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"directori path paramet sdk train fsx data locat file dataset store document directori path defin fsx tensorflow fsx directori defin comput instanc file mount tensorflow repres folder fsx directori link fsx lustr file tensorflow prefix link bucket",
        "Solution_link_count":0.0,
        "Solution_original_content":"fsx lustr file high perform train workload directori path locat file dataset store doc directori path fsx tensorflow fsx directori defin comput instanc mount file tensorflow repres folder fsx directori link fsx lustr file tensorflow prefix link bucket",
        "Solution_preprocessed_content":"fsx lustr file high perform train workload locat file dataset store doc fsx directori defin comput instanc mount file tensorflow repres folder fsx directori fsx lustr file tensorflow prefix bucket",
        "Solution_readability":14.7,
        "Solution_reading_time":6.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":0.0055555556,
        "Challenge_answer_count":1,
        "Challenge_body":"After successfully uploading CSV files from S3 to SageMaker notebook instance, I am stuck on doing the reverse.\n\nI have a dataframe and want to upload that to S3 Bucket as CSV or JSON. The code that I have is below:\n\nbucket='bucketname'\ndata_key = 'test.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ndf.to_csv(data_location)\nI assumed since I successfully used pd.read_csv() while loading, using df.to_csv() would also work but it didn't. Probably it is generating error because this way I cannot pick the privacy options while uploading a file manually to S3. Is there a way to upload the data to S3 from SageMaker?",
        "Challenge_closed_time":1562042063000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562042043000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUfoMiB7A8SFOpr5uklZZuNg\/uploading-a-dataframe-to-aws-s-3-bucket-from-sage-maker",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":8.43,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.0055555556,
        "Challenge_title":"Uploading a Dataframe to AWS S3 Bucket from SageMaker",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":807.0,
        "Challenge_word_count":106,
        "Platform":"Tool-specific",
        "Solution_body":"One way to solve this would be to save the CSV to the local storage on the SageMaker notebook instance, and then use the S3 API's via boto3 to upload the file as an s3 object. S3 docs for upload_file() available here.\n\nNote, you'll need to ensure that your SageMaker hosted notebook instance has proper ReadWrite permissions in its IAM role, otherwise you'll receive a permissions error.\n\ncode you already have, saving the file locally to whatever directory you wish\n\nfile_name = \"mydata.csv\"\ndf.to_csv(file_name)\n\ninstantiate S3 client and upload to s3\n\nimport boto3\n\ns3 = boto3.resource('s3')\ns3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')\nAlternatively, upload_fileobj() may help for parallelizing as a multi-part upload.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"save csv local storag notebook instanc api boto upload file object host notebook instanc readwrit permiss iam role receiv permiss save file local upload boto",
        "Solution_link_count":0.0,
        "Solution_original_content":"save csv local storag notebook instanc api boto upload file object doc upload file note host notebook instanc readwrit permiss iam role receiv permiss save file local directori wish file mydata csv csv file instanti client upload import boto boto resourc meta client upload file file bucket desir object upload fileobj parallel multi upload",
        "Solution_preprocessed_content":"save csv local storag notebook instanc api boto upload file object doc note host notebook instanc readwrit permiss iam role receiv permiss save file local directori wish instanti client upload import boto parallel upload",
        "Solution_readability":11.4,
        "Solution_reading_time":9.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":107.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":0.5344444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nCan SageMaker Model Monitor be applied in NLP models? Is it necessary to do some preprocessing of the data? How can we use SageMaker Model Monitor? sentence length, unseen words, language etc. Any thoughts or experience on that?",
        "Challenge_closed_time":1605007442000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605005518000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUxCKLg-eiQ1mwZvzFyczBEg\/relevancy-of-sage-maker-model-monitor-for-nlp",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":3.44,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.5344444444,
        "Challenge_title":"Relevancy of SageMaker Model Monitor for NLP?",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":45,
        "Platform":"Tool-specific",
        "Solution_body":"Yes, You can use model monitor for data capture and scheduling in your own custom container with the relevant monitoring for NLP use case.\nFor example, there's a blog post for model monitor for computer vision classification prediction with defined *alert * of predict more than expected.\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/automated-monitoring-of-your-machine-learning-models-with-amazon-sagemaker-model-monitor-and-sending-predictions-to-human-review-workflows-using-amazon-a2i\/?nc1=b_rp",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"model monitor nlp model data captur schedul blog share model monitor vision classif predict defin alert advic factor sentenc length unseen languag",
        "Solution_link_count":1.0,
        "Solution_original_content":"model monitor data captur schedul relev monitor nlp blog model monitor vision classif predict defin alert predict http com blog autom monitor model model monitor send predict human review workflow",
        "Solution_preprocessed_content":"model monitor data captur schedul relev monitor nlp blog model monitor vision classif predict defin alert predict",
        "Solution_readability":25.4,
        "Solution_reading_time":6.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":47.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":15.4961111111,
        "Challenge_answer_count":2,
        "Challenge_body":"I have an Amazon SageMaker endpoint with A1, a model with data capture activated, and I want to update the endpoint with A2, a new model.\n\nHow do I track the Model Monitor Data Capture that captured data in Amazon S3, and identify which data referred to model A1 and which data referred to model A2?",
        "Challenge_closed_time":1602144072000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602088286000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUPWBH_xFoS4aq5i41Za5qaQ\/tracking-the-lineage-between-amazon-sage-maker-endpoint-model-and-model-monitor-captured-data",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.1,
        "Challenge_reading_time":4.76,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":15.4961111111,
        "Challenge_title":"Tracking the lineage between Amazon SageMaker endpoint model and Model Monitor captured data",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":84.0,
        "Challenge_word_count":68,
        "Platform":"Tool-specific",
        "Solution_body":"using boto3:\nwhen you update the model endpoint, you need to create a new EndpointConfig where you specify a new s3 uri where data capture will be stored and thats how you can see different data captures from different versions of the model.\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpointConfig.html",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"boto creat endpointconfig updat model endpoint endpointconfig uri specifi data captur store identifi data captur version model document link http doc com latest apirefer api updateendpoint html http doc com latest apirefer api createendpointconfig html",
        "Solution_link_count":2.0,
        "Solution_original_content":"boto updat model endpoint creat endpointconfig specifi uri data captur store that data captur version model http doc com latest apirefer api updateendpoint html http doc com latest apirefer api createendpointconfig html",
        "Solution_preprocessed_content":"boto updat model endpoint creat endpointconfig specifi uri data captur store that data captur version model",
        "Solution_readability":26.7,
        "Solution_reading_time":5.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":7.809835,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello experts, we are working on a medium size solution for our company and we are exploring basic estimate for SDK or studio decision. How I can know?",
        "Challenge_closed_time":1658757422623,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658729307217,
        "Challenge_favorite_count":11.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/940045\/estimate-the-cost-for-machine-learning-sdk-or-ui-p.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":2.51,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":7.809835,
        "Challenge_title":"Estimate the cost for Machine learning SDK or UI portal",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":37,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @Alexandre-2525\n\nThanks for reachin out to us, the Azure Machine Learnng pricing mainly is consist of CPU pricing and compute pricing, to get a better estimate pricing, a good way to calculate is using the calculator - https:\/\/azure.microsoft.com\/en-us\/pricing\/calculator\/\n\nYou can add your details into it and you will have a general idea about that.\n\nI hope this helps, thank you.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"price calcul estim cost sdk portal medium size compani",
        "Solution_link_count":1.0,
        "Solution_original_content":"alexandr reachin learnng price mainli consist cpu price comput price estim price calcul calcul http com price calcul add gener idea hope yutong kindli accept commun",
        "Solution_preprocessed_content":"reachin learnng price mainli consist cpu price comput price estim price calcul calcul add gener idea hope yutong kindli accept commun",
        "Solution_readability":10.9,
        "Solution_reading_time":6.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":78.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":13.1007069444,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi,\n\nI`m using aws textract to extract key-value pairs from an pdf. Because sometimes the accucary is low i use augmented AI (human review worflows) to involve a human worker. That works fine with png files, but when I use pdf files (which textract supports), I get an \"Failed to load image\". How do I get around this? I tried using a custom template, but can't find a way to insert the file type.\n\nBest regards,\n\nPaul",
        "Challenge_closed_time":1665462672324,
        "Challenge_comment_count":0,
        "Challenge_created_time":1665415509779,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUo48ev4bTTvO-GjsezfAmuQ\/aws-textract-human-review-flow-failed-to-load-image",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":5.6,
        "Challenge_reading_time":5.64,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":13.1007069444,
        "Challenge_title":"aws textract human review flow, failed to load image",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":85,
        "Platform":"Tool-specific",
        "Solution_body":"The underlying challenge here is that, while modern browsers can natively render PDFs, they require different embedding methods for PDFs vs images. To my knowledge there's no built-in SageMaker Crowd HTML Element that's capable of handling both types interchangeably - and your experience with the pre-built UI seems to confirm this.\n\nDisplaying PDFs in A2I\/SMGT\n\nThis simple sample suggests to use an <iframe type=\"application\/pdf\"> to display PDFs via the browser's native renderer. You could try this approach... but as of ~March 2022, I found support was patchy because some browsers' default security policies didn't like loading a cross-origin iframe with interactive content.\n\nIf relying on the browser native renderer won't work for your users, you can use the open-source PDF.js renderer instead. Here is a more complex sample template that does that. PDF.js is powerful, but can be pretty tricky to get started with from my experience... Note that the basic process in this sample is:\n\nTag the <script>s and stylesheets for PDF.js in from a CDN\nInclude a PDF viewer structure in your HTML\nPass your A2I object URL in through JavaScript and set up your viewer there - including any interactivity you need\n(The second inline script tag there you can probably ignore: It's specific to what that data that template collects)\nScaling template complexity\n\nAlthough the situation has improved a lot in recent years, writing direct-to-browser inline JavaScript in HTML can be tricky due to browser diversity and developer tooling limitations. If you want to build more advanced, interactive task templates, you might want to explore using front-end frameworks like React\/Angular\/Vue within A2I\/Ground Truth.\n\nThe above-mentioned PDF.js template is actually a legacy that's since been replaced by this VueJS app in the sample that uses it. In that case, the switch was made because we wanted to customize the PDF viewer (rendering detection boxes over the document), and the complexity of the app justified setting up a proper toolchain. You can find discussion there about using frameworks in general and VueJS in particular with A2I, and could use the app as a starting point for building your own complex template in advance. Note if I was re-building that from scratch, I'd probably use much less liquid templating, and implement more within the JS framework itself as discussed here.\n\nYou can see the complex template being built\/deployed from (SageMaker) Python notebook here, and a screenshot of it in action here. This end-to-end sample is discussed further in an AWS ML blog post.\n\nHandling mixed PDF\/Image content\n\nIf you need your template to handle both PDFs and images, this will add extra complexity. Could your JavaScript infer from the object URL (filename) which category the input object falls into, and dynamically set up either an <img> tag or a PDF viewer? Could you fetch the object from JS and check the Content-Type response header? Might it be simpler to add the file type as an input to your A2I loop, and pass it in that way? (e.g. using conditional liquid template to either render an <img> or not?)\n\nDepending on what points in the flow you know the file type, there are multiple different ways you might tackle this. Ultimately though, you'll probably be switching between either generating an img or a PDF viewer: Whether those HTML elements are created by static Liquid templating or by dynamic JS.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"textract extract kei valu pair pdf file displai pdf browser nativ render open sourc pdf render end framework react angular vue ground build advanc interact task templat mix pdf imag infer object url filenam input object fall dynam set tag pdf viewer file type input loop",
        "Solution_link_count":0.0,
        "Solution_original_content":"underli modern browser nativ render pdf embed pdf imag knowledg built crowd html element capabl type interchang pre built displai pdf smgt sampl displai pdf browser nativ render march patchi browser default secur polici load cross origin ifram interact reli browser nativ render open sourc pdf render complex sampl templat pdf power pretti tricki start note process sampl tag",
        "Solution_preprocessed_content":"underli modern browser nativ render pdf embed pdf imag knowledg crowd html element capabl type interchang displai pdf sampl displai pdf browser nativ render march patchi browser default secur polici load ifram interact reli browser nativ render render complex sampl templat power pretti tricki start note process sampl tag",
        "Solution_readability":10.5,
        "Solution_reading_time":42.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":26.0,
        "Solution_word_count":557.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":18.5982666667,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm trying to load files as a dataset in the GUI of Azure ML Studio. These parquet files have been created through Spark.\n\nIn my folder, Spark creates files such as \"_SUCCESS\" or \"_committed_8998000\".\n\nAzure ML Studio is not able to read them or ignore them and tells me:\n\nThe provided file(s) have invalid byte(s) for the specified file encoding.\n{\n  \"message\": \" \"\n}\n\n\n\n\nI selected \"Ignore unmatched files path\" and yet, it still does not work.\n\nIf I remove the \"_SUCCESS\" and other Spark files, it works.\n\nDoes anyone have an idea about a workaround?\n\nThank you.",
        "Challenge_closed_time":1601535069840,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601468116080,
        "Challenge_favorite_count":4.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/112778\/how-can-i-create-a-dataset-in-azure-ml-studio-thro.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":5.4,
        "Challenge_reading_time":7.99,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":18.5982666667,
        "Challenge_title":"How can I create a dataset in Azure ML studio (through the GUI) from a parquet file created with Azure Spark",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":115,
        "Platform":"Tool-specific",
        "Solution_body":"I used \"path\/\/.parquet\" in the \"Path\" field and now it works.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"path parquet path field allow studio read parquet file creat spark",
        "Solution_link_count":0.0,
        "Solution_original_content":"path parquet path field",
        "Solution_preprocessed_content":null,
        "Solution_readability":0.7,
        "Solution_reading_time":0.75,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":371.2785480556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What can I do when I forget my password in the local wandb?<br>\nIt seems that deleting or uninstalling  doesn\u2019t work.<\/p>",
        "Challenge_closed_time":1646784628352,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645448025579,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/forgot-password-in-local\/1959",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":5.2,
        "Challenge_reading_time":1.84,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":371.2785480556,
        "Challenge_title":"Forgot password in local",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":188.0,
        "Challenge_word_count":24,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/nightmare4214\">@nightmare4214<\/a> ,<\/p>\n<p>Could you try the following steps?<\/p>\n<ul>\n<li>Log into the docker container using <code>docker exec -it wandb-local bash<\/code>\n<\/li>\n<li>Type <code>\/usr\/local\/bin\/local password EMAIL@ADDRESS.com<\/code> (where <code>EMAIL@ADDRESS.com<\/code> is your email)<\/li>\n<\/ul>\n<p>This should let you manually reset your password for the local instance and you should be able to log in through this. Please let me know if this does not work for you.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"log docker docker exec local bash type usr local bin local password email address com email address com email manual reset password local instanc",
        "Solution_link_count":0.0,
        "Solution_original_content":"nightmar step log docker docker exec local bash type usr local bin local password email address com email address com email manual reset password local instanc log ramit",
        "Solution_preprocessed_content":"step log docker type manual reset password local instanc log ramit",
        "Solution_readability":7.8,
        "Solution_reading_time":7.04,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":69.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":0.5563888889,
        "Challenge_answer_count":1,
        "Challenge_body":"What are the advantages of using SageMaker jupyter instance instead of running it locally? Is there a special integration with SageMaker that we lose it if we do not use Sagemaker jupyer instance?",
        "Challenge_closed_time":1606709124000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606707121000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUIzWlfNVTSIWIqkVsIaNv2A\/sagemaker-jupyter-notebook",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":2.78,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.5563888889,
        "Challenge_title":"Sagemaker Jupyter notebook",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":62.0,
        "Challenge_word_count":35,
        "Platform":"Tool-specific",
        "Solution_body":"Some useful points:\n\nThe typical arguments of cloud vs local will apply (as with e.g. Cloud9, Workspaces, etc): Can de-couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, right-size the environment for what workloads you need to do on a given day, etc.\nSageMaker notebooks already run in an explicit IAM context (via assigned execution role) - so you don't need to log in e.g. as you would through the CLI on local machine... Can just run sagemaker.get_execution_role()\nPre-built environments for a range of use-cases (e.g. generic data science, TensorFlow, PyTorch, MXNet, etc) with libraries already installed, and easy wiping\/reset of the environment by stopping & starting the instance - no more \"environment soup\" on your local laptop.\nLinux-based environments, which typically makes for a shorter path to production code than Mac\/Windows.\nIf you started using SageMaker Studio, then yes there are some native integrations such as the UIs for experiment tracking and endpoint management\/monitoring; easy sharing of notebook snapshots; and whatever else might be announced over the next couple of weeks.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"jupyt instanc coupl lifetim laptop run local shut size environ workload dai notebook run explicit iam context assign execut role log cli local run execut role pre built environ rang gener data scienc tensorflow pytorch mxnet librari instal wipe reset environ",
        "Solution_link_count":0.0,
        "Solution_original_content":"typic argument cloud local appli cloud workspac coupl lifetim laptop run local shut size environ workload dai notebook run explicit iam context assign execut role log cli local run execut role pre built environ rang gener data scienc tensorflow pytorch mxnet librari instal wipe reset environ stop start instanc environ soup local laptop linux base environ typic shorter path mac window start studio nativ integr ui track endpoint monitor share notebook snapshot announc coupl week",
        "Solution_preprocessed_content":"typic argument cloud local appli lifetim laptop run local shut environ workload dai notebook run explicit iam context log cli local run environ rang librari instal environ stop start instanc environ soup local laptop environ typic shorter path start studio nativ integr ui track endpoint share notebook snapshot announc coupl week",
        "Solution_readability":11.1,
        "Solution_reading_time":14.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":179.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":16.8847241667,
        "Challenge_answer_count":1,
        "Challenge_body":"How to mount the dataset in ML Studio using python sdk ?\nWhat are the different ways and which is the right one ?\n\nCan we create dataset pointing to two different stores ?\n\nAlso where can I learn more about azure machine learning ?",
        "Challenge_closed_time":1661469512520,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661408727513,
        "Challenge_favorite_count":11.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/981126\/mounting-the-dataset-in-ml-workspace.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":3.3,
        "Challenge_reading_time":3.19,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":16.8847241667,
        "Challenge_title":"Mounting the dataset in ML Workspace",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":46,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @VJEEVA-5492\n\nThanks for using Microsoft Q&A platform. Let me answer your questions one by one.\n\nwhere can I learn more about azure machine learning\nThe best way to learn Azure Machine Learning is the documentation, please refer to - https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning\/#documentation\n\nHow to mount the dataset in ML Studio using python sdk ?\nGenerally there are two ways to work with data in Azure Machine Learning -\nUse datastores - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-datastore?tabs=cli-identity-based-access%2Ccli-adls-identity-based-access%2Ccli-azfiles-account-key%2Ccli-adlsgen1-identity-based-access\nUse data assets - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-datastore?tabs=cli-identity-based-access%2Ccli-adls-identity-based-access%2Ccli-azfiles-account-key%2Ccli-adlsgen1-identity-based-access\n\nWhat are the different ways and which is the right one ?\nIt depends on your need. Compared to data assets and datastore, the benefits of creating data assets are:\nYou can share and reuse data with other members of the team such that they do not need to remember file locations.\nYou can seamlessly access data during model training (on any supported compute type) without worrying about connection strings or data paths.\nYou can version the data.\n\nCan we create dataset pointing to two different stores ?\nIf you need to assembly data, you may want to consider data assets. By creating a data asset, you create a reference to the data source location, along with a copy of its metadata. Because the data remains in its existing location, you incur no extra storage cost, and don't risk the integrity of your data sources. You can create Data from datastores, Azure Storage, public URLs, and local files.\n\nI hope this helps!\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"document link http com servic document data datastor data asset benefit creat data asset share reus data team member seamless access data model train version data creat dataset store creat data asset creat data sourc locat copi metadata",
        "Solution_link_count":3.0,
        "Solution_original_content":"vjeeva platform document http com servic document mount dataset studio sdk gener data datastor http doc com datastor tab cli ident base access ccli adl ident base access ccli azfil account kei ccli adlsgen ident base access data asset http doc com datastor tab cli ident base access ccli adl ident base access ccli azfil account kei ccli adlsgen ident base access depend compar data asset datastor benefit creat data asset share reus data member team rememb file locat seamlessli access data model train comput type worri connect data path version data creat dataset store assembl data data asset creat data asset creat data sourc locat copi metadata data remain locat incur extra storag cost risk integr data sourc creat data datastor storag public url local file hope yutong kindli accept commun",
        "Solution_preprocessed_content":"platform document mount dataset studio sdk gener data datastor data asset depend compar data asset datastor benefit creat data asset share reus data member team rememb file locat seamlessli access data model train worri connect data path version data creat dataset store assembl data data asset creat data asset creat data sourc locat copi metadata data remain locat incur extra storag cost risk integr data sourc creat data datastor storag public url local file hope yutong kindli accept commun",
        "Solution_readability":12.2,
        "Solution_reading_time":24.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":249.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":5.7289875,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm doing research that aims to detect if social media users are depressed or\/and have mental health issues. Does Cognitive Service and its sentiment analysis models provide an API for this?",
        "Challenge_closed_time":1668715953528,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668695329173,
        "Challenge_favorite_count":20.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1093496\/does-azure-cognitive-service-provide-a-way-to-dete.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":3.54,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":5.7289875,
        "Challenge_title":"Does Azure Cognitive Service provide a way to detect depression (or mental health) from a text?",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":46,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @xiriro\n\nThanks for using Microsoft Q&A platform. Sentimnt Analysis of Azure Cognitive Service does not support Depression Detection at this moment, it can only returen positive, neutral and negative labels, which may not enough for depression from my personal experience.\n\nBut I do see Microsoft Research group is working on this case - https:\/\/www.microsoft.com\/en-us\/research\/project\/technology-for-mental-health-and-well-being-interventions\/\n\nAnd also I see some external Microsoft resource about this topic you may be interested in - https:\/\/www.youtube.com\/watch?v=HzlOkaGHZSg&t=1432s\n\nI hope this helps and thank you for your product feedback, I will bring this feature to product team for future considerations.\n\n\n\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"cognit servic depress detect sentiment analysi research group technolog mental health intervent extern resourc mental health technolog bias summari",
        "Solution_link_count":2.0,
        "Solution_original_content":"xiriro platform sentimnt analysi cognit servic depress detect moment returen posit neutral neg label depress research group http com research technolog mental health intervent extern resourc http youtub com watch hzlokaghzsg hope feedback featur team futur consider yutong kindli accept commun",
        "Solution_preprocessed_content":"platform sentimnt analysi cognit servic depress detect moment returen posit neutral neg label depress research group extern resourc hope feedback featur team futur consider yutong kindli accept commun",
        "Solution_readability":12.9,
        "Solution_reading_time":10.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":111.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":92.1785794445,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>The doc <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide\">Import &amp; Export Data<\/a> gives the way how to export data from cloud. Can I use api to export data from local run files? I tried use path to local run directory instread of <code>&lt;entity&gt;\/&lt;project&gt;\/&lt;run_id&gt;<\/code>, but it doesn\u2019t work.<\/p>",
        "Challenge_closed_time":1661211326167,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660879483281,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-export-data-from-local-run-files\/2959",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":8.3,
        "Challenge_reading_time":4.8,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":92.1785794445,
        "Challenge_title":"How to export data from local run files?",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":173.0,
        "Challenge_word_count":49,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/geyao\">@geyao<\/a> , this is currently not an available option. This functionality will be revisited in the future for consideration. Our API only works with runs logged to the cloud.<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"geyao option function revisit futur consider api run log cloud",
        "Solution_preprocessed_content":"option function revisit futur consider api run log cloud",
        "Solution_readability":7.3,
        "Solution_reading_time":2.78,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":723.7180555556,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer is using SageMaker Studio in VpcOnly mode (VPC, protected subnets without internet access, NO NAT gateways). The all functionality is fine. However, when I try create a SageMaker projects - as described here, SageMaker Studio is unable to list the project templates (timeout and unspecified error) resulting in empty list of the available project templates.\n\nProjects are enabled for the users - as described here. The problem is with project creation.\n\nIs internet access (e.g. via NAT gateways) is needed for SageMaker projects?",
        "Challenge_closed_time":1618085440000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615480055000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUcyhpq1pxRTmtjkDRAh_MDA\/sage-maker-studio-projects-in-vpc-only-mode-without-internet-access",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":7.54,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":723.7180555556,
        "Challenge_title":"SageMaker Studio projects in VpcOnly mode without internet access",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":323.0,
        "Challenge_word_count":91,
        "Platform":"Tool-specific",
        "Solution_body":"Figured it out. SageMaker Studio projects need Service Catalog access and VPCE for com.amazonaws.${AWS::Region}.servicecatalog",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"enabl servic catalog access vpce com amazonaw region servicecatalog studio list templat allow creation vpconli mode internet access",
        "Solution_link_count":0.0,
        "Solution_original_content":"figur studio servic catalog access vpce com amazonaw region servicecatalog",
        "Solution_preprocessed_content":"figur studio servic catalog access vpce",
        "Solution_readability":15.5,
        "Solution_reading_time":1.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":14.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":0.2683527778,
        "Challenge_answer_count":1,
        "Challenge_body":"I have imported packages:\n\nlibrary(dplyr)\n\nUploaded my dataset:\n\nbike <- readRDS(\"bike.rds\")\n\nBut when I try simple \"filter\" it is not working:",
        "Challenge_closed_time":1597097170387,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597096204317,
        "Challenge_favorite_count":2.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/63901\/simple-filter-is-not-working-in-azure-notebook-for.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":2.39,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.2683527778,
        "Challenge_title":"Simple filter is not working in Azure notebook for R",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":29,
        "Platform":"Tool-specific",
        "Solution_body":"Fixed.\n\nIt looks azure notebook clean the session after some period of inactivity, there the package dplyr was not loaded after some time",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"reload packag dplyr period inact notebook",
        "Solution_link_count":0.0,
        "Solution_original_content":"notebook clean session period inact packag dplyr load time",
        "Solution_preprocessed_content":"notebook clean session period inact packag dplyr load time",
        "Solution_readability":11.1,
        "Solution_reading_time":1.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":150.1254972222,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hi W&amp;B Community,<\/p>\n<p>Is there a possibility to get additional live system metrics like the network read\/write rates, disk read\/write rates, virtual memory major\/minor page faults, filesystem inodes, and system context switches?<\/p>\n<p>Basically, most of the metrics that dstat provides with the following flags:<\/p>\n<ul>\n<li>\u2013disk<\/li>\n<li>\u2013mem (memory)<\/li>\n<li>\u2013net (network)<\/li>\n<li>\u2013sys (system)<\/li>\n<li>\u2013fs (filesystem)<\/li>\n<li>\u2013vm (virtual memory)<\/li>\n<\/ul>\n<p>I\u2019m deep into pipeline profiling and found that having these helps a lot when looking for performance tuning opportunities. Also, allowing to add to the system metric log might be helpful generally to have everything related to actual ML in one log, and everything related to system metrics in another.<\/p>\n<p>I saw that the <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/system-metrics\">current documentation<\/a> suggests that you use this script - github(.)com\/nicolargo\/nvidia-ml-py3\/blob\/master\/pynvml.py - to get the GPU metrics, however, I did not find the system metrics there.<\/p>\n<p>The first workaround for me would be to run  <code>dstat<\/code> in parallel to the process, save the profiling log,<br>\ndownload your system metrics and join over the <code>_timestamp<\/code>. This, however, would negate your wonderful automatic visualization.<\/p>\n<p>The other solution would be to use some system monitoring library and add manually via <code>wandb.log({'my_metric': x})<\/code> to the \u201cML\u201d-log. This would show the metric in your visualization but not at the correct place and would not be easily compared to the other system metrics. I do not know how well this would work in practice as there would need to be additions to this log ideally every (few) seconds. This would be an asynchronous running thread that is not inside of the training loop. <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log\/logging-faqs#what-if-i-want-to-log-some-metrics-on-batches-and-some-metrics-only-on-epochs\">The solution proposed here<\/a>  seems like it could work if I use \u201ctimestamps\u201d as the X-axis? This still does not seem like a clean solution.<\/p>\n<p>What are your thoughts on this proposed feature? I\u2019m very much a novice regarding your service so I might not know the in\u2019s and out\u2019s, maybe I have overlooked some trivial solution.<\/p>",
        "Challenge_closed_time":1651620228352,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651079776562,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/additional-system-metrics-from-e-g-dstat\/2333",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":11.4,
        "Challenge_reading_time":29.92,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":150.1254972222,
        "Challenge_title":"Additional System Metrics From e.g., `dstat`",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":141.0,
        "Challenge_word_count":320,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hi Alex,<\/p>\n<p>Thank you for that very detailed and insightful response regarding your request! I definitely see why this could be useful for optimizing features now, I had never considered how the rate of context switches could have a performance impact on the performance of an ML pipeline.<\/p>\n<p>I\u2019ll definitely go ahead and make a feature request for this, and I\u2019ll keep you updated on the status of this request.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"propos featur add metric log featur request",
        "Solution_link_count":0.0,
        "Solution_original_content":"alex respons request definit optim featur rate context switch perform impact perform pipelin ill definit ahead featur request ill updat statu request ramit",
        "Solution_preprocessed_content":"alex respons request definit optim featur rate context switch perform impact perform pipelin ill definit ahead featur request ill updat statu request ramit",
        "Solution_readability":9.1,
        "Solution_reading_time":5.58,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":72.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":5.8236111111,
        "Challenge_answer_count":2,
        "Challenge_body":"What is the difference between SageMaker Pipelines and SageMaker Step Function SDK?\n\nThe official Pipelines notebook is basically only doing a workflow - pretty much a copy cat of what step functions has been doing for years. In the nice video from Julien Simon I see CICD capacities mentioned, where are those? any demos?",
        "Challenge_closed_time":1607015065000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606994100000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU2iheeTzhSTmWw4aqVEeqOQ\/what-is-the-difference-between-sage-maker-pipelines-and-sage-maker-step-function-sdk",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":5.01,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":5.8236111111,
        "Challenge_title":"What is the difference between SageMaker Pipelines and SageMaker Step Function SDK?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":673.0,
        "Challenge_word_count":64,
        "Platform":"Tool-specific",
        "Solution_body":"Hey, that demo is missing the project part of Pipelines and therefore the SM provided project templates. Go to SM studio and on the Studio summary hit edit settings and then enable access and provisioning of Service Catalog Portfolio of products in SM Studio. Then check your service catalog portfolios. Haven't tried it out yet though.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"miss pipelin templat studio studio summari hit edit set enabl access provis servic catalog portfolio studio servic catalog portfolio haven tri",
        "Solution_preprocessed_content":"miss pipelin templat studio studio summari hit edit set enabl access provis servic catalog portfolio studio servic catalog portfolio haven tri",
        "Solution_readability":7.6,
        "Solution_reading_time":4.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":0.6838888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I am trying to launch an endpoint locally, to do couple inferences from my dev notebook (without having to wait for instanciation time of actual endpoint or batch training). I am running the following code:\n\n# get trained model from s3\ntrained_S2S = SM.model.Model(\n    image=seq2seq,\n    model_data=('s3:\/\/XXXXXXXXXXXXX\/'\n        + 'output\/seq2seq-2018-07-30-16-55-12-521\/output\/model.tar.gz'),\n    role=role) \n\nS2S = trained_S2S.deploy(1, instance_type='local')    \n\n\nI get the following error (several hundreds of lines repeated):\n\nWARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa5c61eaac8>: Failed to establish a new connection: [Errno 111] Connection refused',)': \/ping \n\n\nRuntimeError: Giving up, endpoint: seq2seq-2018-08-01-14-18-06-555 didn't launch correctly",
        "Challenge_closed_time":1533135863000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533133401000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU54PWM3V9QoybCiO5GvB03g\/sagemaker-local-deployment-runtime-error-giving-up-endpoint-didnt-launch-correctly",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":12.97,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.6838888889,
        "Challenge_title":"Sagemaker local deployment: \"RuntimeError: Giving up, endpoint: didn't launch correctly\"",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":311.0,
        "Challenge_word_count":102,
        "Platform":"Tool-specific",
        "Solution_body":"Currently, SageMaker local is supported only for SageMaker framework containers (MXNet, TensorFlow, PyTorch, Chainer and Spark) and not for the Builtin algorithms",
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"local framework mxnet tensorflow pytorch chainer spark builtin algorithm",
        "Solution_preprocessed_content":"local framework builtin algorithm",
        "Solution_readability":15.4,
        "Solution_reading_time":2.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":3.7368033333,
        "Challenge_answer_count":1,
        "Challenge_body":"We need to connect Azure Data Lake Storage Gen2 to Azure Machine Learning by means of a datastore. For security reasons we do not want to provide the credential-based authentication credentials (service principal or SAS token). Instead we want to connect with identity based access.\n\nThe problem we face is that we are not able to assign a managed identity to a compute instance, so we can connect from notebooks to the Data Lake. In the documentation is explained how to assign a managed identity to a cluster, but we need the same for the compute instance, as it is the only way to run commands directly from the notebook.\n\nIs there a way to assign managed identity to an Azure Machine Learning Compute Instance? Otherwise, we would like to know the best approach to overcome this issue, considering that we do not want to introduce the credentials in the code.",
        "Challenge_closed_time":1637157273152,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637143820660,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/630520\/azure-ml-managed-identity-for-compute-instance.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":11.06,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":3.7368033333,
        "Challenge_title":"Azure ML - Managed Identity for Compute Instance",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":157,
        "Platform":"Tool-specific",
        "Solution_body":"@Nimbeo-7089 Thanks for the question. Currently It\u2019s not supported yet to assign managed identity to an Azure Machine Learning Compute Instance, you\u2019d need to use credential-based access. We have forwarded to the product team to support in the near future.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"assign ident comput instanc credenti base access forward team futur",
        "Solution_link_count":0.0,
        "Solution_original_content":"nimbeo assign ident comput instanc youd credenti base access forward team futur",
        "Solution_preprocessed_content":"assign ident comput instanc youd access forward team futur",
        "Solution_readability":8.5,
        "Solution_reading_time":3.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":16.6353427778,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi,\ni deployed a real-time inference pipeline using ML Designer. Training and deploying works fine. But when I'm consuming\/testing my API it doesn't work. Postman gives me Errorcode 500 and \"Internal Server Error. Run: Server internal error is from Module Extract N-Gram Features from Text\".\n\nThis is my training pipeline:\n\n\nI read this: https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/algorithm-module-reference\/extract-n-gram-features-from-text.md#score-or-publish-a-model-that-uses-n-grams\n\nBut I don't know how to achieve this.\n\nThanks in advance.",
        "Challenge_closed_time":1606367173387,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606307286153,
        "Challenge_favorite_count":6.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/175242\/how-to-deploy-ml-designer-pipeline-as-real-time-in.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.5,
        "Challenge_reading_time":8.59,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":16.6353427778,
        "Challenge_title":"How to deploy ML Designer pipeline as real-time inference pipeline using N-Gram",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":76,
        "Platform":"Tool-specific",
        "Solution_body":"Once you create a real-time inference pipeline, please make the further modifications below:\n\nFind the output Result_vocabulary dataset from Extract N-Gram Features from Text module.\n\n\n\nRegister the dataset as with a name\n\n\n\nUpdate real-time inference pipeline like below:\n\n\n\n\n\nWe will improve the documentation accordingly. Thanks for reporting the issue!",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"clear output vocabulari dataset extract gram featur text modul regist updat time infer pipelin accordingli document improv base report",
        "Solution_link_count":0.0,
        "Solution_original_content":"creat time infer pipelin modif output vocabulari dataset extract gram featur text modul regist dataset updat time infer pipelin improv document accordingli report",
        "Solution_preprocessed_content":"creat infer pipelin modif output dataset extract featur text modul regist dataset updat infer pipelin improv document accordingli report",
        "Solution_readability":12.0,
        "Solution_reading_time":4.35,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":49.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":8.7290330556,
        "Challenge_answer_count":5,
        "Challenge_body":"Is there a way in the AML designer to set a pipeline and\/or specific step to now allow reuse between runs? I've seen quite a few posts on how to do this in code, but I can't seem to find a way to set that property in the designer.",
        "Challenge_closed_time":1617671557012,
        "Challenge_comment_count":1,
        "Challenge_created_time":1617640132493,
        "Challenge_favorite_count":8.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/344626\/how-to-specify-do-not-allow-reuse-in-azure-machine.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":8.7,
        "Challenge_reading_time":3.53,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":8.7290330556,
        "Challenge_title":"How to specify do not allow reuse in Azure Machine Learning designer",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Platform":"Tool-specific",
        "Solution_body":"Hello,\n\nUse the following steps to update a module pipeline parameter:\n\nAt the top of the canvas, select the gear icon.\nIn the Pipeline parameters section, you can view and update the name and default value for all of your pipeline parameter.\n\n\n\n\nHope this helps. Thanks.\n\nRegards,\nYutong",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"updat modul pipelin paramet select gear icon canva updat default valu pipelin paramet pipelin paramet section set allow reus properti design",
        "Solution_link_count":0.0,
        "Solution_original_content":"step updat modul pipelin paramet canva select gear icon pipelin paramet section updat default valu pipelin paramet hope yutong",
        "Solution_preprocessed_content":"step updat modul pipelin paramet canva select gear icon pipelin paramet section updat default valu pipelin paramet hope yutong",
        "Solution_readability":8.4,
        "Solution_reading_time":3.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":2.9591811111,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm following this guide for transitioning to Amazon Linux 2 provided by AWS\n\nI've set up the two needed lifecycle configurations and created a new S3 Bucket to store the backup. I've also ensured the IAM roles have the required S3 permissions and updated the notebook with the ebs-backup-bucket tag per the instructions.\n\nWhen I run the notebook with the new configuration I get the following error: \"Notebook Instance Lifecycle Config [LIFECYCLE ARN] for Notebook Instance [NOTEBOOK ARN] took longer than 5 minutes. Please check your CloudWatch logs for more details if your Notebook Instance has Internet access.\n\nLooking at the logs I get the error: \/bin\/bash: \/tmp\/OnStart_2022-11-09-01-51ontlqcqt: \/bin\/bash^M: bad interpreter: No such file or directory\n\nAny thoughts on how to resolve this issue? The code for the backup lifecycle configuration can be found here",
        "Challenge_closed_time":1667972357492,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667961704440,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUPsaLux6EQcqvHW1HtNlkIw\/directory-error-when-running-sage-maker-backup-ebs-lifecycle-for-amazon-linux-2-transition",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":11.9,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2.9591811111,
        "Challenge_title":"Directory Error when running SageMaker backup-ebs lifecycle for Amazon Linux 2 transition",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":28.0,
        "Challenge_word_count":146,
        "Platform":"Tool-specific",
        "Solution_body":"The extra ^M symbol (i.e. Ctrl-M) stopped the whole scrip from being interpreted properly.\n\nThis issue is normally seen in scripts prepared in MSDOS\/Windows based system but used in Linux system due to difference of line endings.\n\nIn Unix based OS, lines end with \\n but MSDOS\/Win based system ends with \\r\\n\n\nIn Linux based system, you could show your prepared scripts by running\n\ncat -e some-script.sh \n\n\nThe results would be something similar to\n\n#!\/bin\/bash^M$\n... ...^M$\n\n\n$ is normal Unix end-of-line symbol. Windows uses an extra one ^M and this symbol is not recognized by Unix system. That's why, in SageMaker Notebook Lifecycle Configuration, which is running Linux, your script was interpreted as \/bin\/bash^M\n\nTo mitigate the issue, please convert the scripts to Unix based ending and update life cycle configuration. To achieve this, you could use Notepad++ in Windows. You can go to the Edit menu, select the EOL Conversion submenu, and from the options that come up select UNIX\/OSX Format. The next time you save the file, its line endings will, all going well, be saved with UNIX-style line endings.\n\nAlternatively, you could put the script in a Linux environment, e.g. EC2 instance with Amazon Linux 2 and install dos2unix via sudo yum install dos2unix. After installation, you could convert your files via\n\ndos2unix -n file.sh  output.sh \n\n\nAfter the conversion, please update LCC with the new scripts. You could verify that ^M has been removed via\n\ncat -e your_script.sh\n\n\nThe output will print all special characters directly without hiding.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"directori run backup eb lifecycl linux transit convert unix base end updat lifecycl configur achiev notepad window put linux environ instal dosunix convers updat lcc",
        "Solution_link_count":0.0,
        "Solution_original_content":"extra symbol ctrl stop scrip interpret properli normal prepar msdo window base linux line end unix base line end msdo win base end linux base prepar run cat bin bash normal unix end line symbol window extra symbol recogn unix notebook lifecycl configur run linux interpret bin bash mitig convert unix base end updat life cycl configur achiev notepad window edit menu select eol convers submenu option come select unix osx format time save file line end save unix style line end linux environ instanc linux instal dosunix sudo yum instal dosunix instal convert file dosunix file output convers updat lcc verifi remov cat output print charact directli hide",
        "Solution_preprocessed_content":"extra symbol stop scrip interpret properli normal prepar base linux line end unix base line end base end linux base prepar run cat normal unix symbol window extra symbol recogn unix notebook lifecycl configur run linux interpret mitig convert unix base end updat life cycl configur achiev notepad window edit menu select eol convers submenu option come select format time save file line end save line end linux environ instanc linux instal do unix sudo yum instal do unix instal convert file do unix convers updat lcc verifi remov cat output print charact directli hide",
        "Solution_readability":7.8,
        "Solution_reading_time":18.94,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":250.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":12.2413888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI am a little confused about whether S3 Shard key would work when using PIPE mode, here is a example:\n\nAssume I have:\n\n2 instance, each instance have 4 worker;\n\ndata: total 8 files with total size 8GB, each file is 1GB. Put them into 4 different S3 path, that means, each path has 2 files (2GB in total)\n\nIf I use PIPE mode, and s3_input using distribution='ShardedByS3Key', and create 4 channel (each channel mapping a s3 path, 2 files)\n\ntrain_s3_input_1 = sagemaker.inputs.s3_input(channel_1, distribution='ShardedByS3Key')\n\nQuestion:\n\nHow much data of each worker get to train, 1 file or 2 files? thanks",
        "Challenge_closed_time":1589407880000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589363811000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU31DdUqtuQziixKTkPasZKw\/confusion-about-pipe-mode-when-using-s-3-shard-key",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":8.01,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":12.2413888889,
        "Challenge_title":"confusion about PIPE mode when using S3 shard key",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":25.0,
        "Challenge_word_count":108,
        "Platform":"Tool-specific",
        "Solution_body":"Hi, SageMaker will replicate a subset of data (1\/n ML compute instances) on each ML compute instance that is launched for model training when you specify ShardedByS3Key. If there are n ML compute instances launched for a training job, each instance gets approximately 1\/n of the number of S3 objects. This applies in both File and Pipe modes. Keep this in mind when developing algorithms.\n\nTo answer your question: How much data of each worker get to train, 1 file or 2 files? 1 file each from the training channel.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"shard kei pipe mode distribut shardedbyskei file pipe mode replic subset data comput instanc comput instanc launch model train comput instanc launch train job instanc approxim object worker train file train channel",
        "Solution_link_count":0.0,
        "Solution_original_content":"replic subset data comput instanc comput instanc launch model train specifi shardedbyskei comput instanc launch train job instanc approxim object appli file pipe mode algorithm data worker train file file file train channel",
        "Solution_preprocessed_content":"replic subset data comput instanc launch model train specifi shardedbi kei comput instanc launch train job instanc approxim object appli file pipe mode algorithm data worker train file file file train channel",
        "Solution_readability":6.8,
        "Solution_reading_time":6.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":90.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":19.6539680556,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I am new to Azure ML, and I have been trying to replicate the same structure presented in the MNIST tutorial, but I don't understand how to adapt it to my case.\n\nI am running a python file from the experiment, but I don't understand how I can access data that is currently in a folder in the cloud file system from the script running in the experiment.\nI have found many examples about accessing one single .csv file, but my data is made of many images.\n\nFrom my understanding I should first load the folder to a datastore, then use Dataset.File.upload_directory to create a dataset containing my folder, and here is how I tried to do it:\n\n # Create dataset from data directory\n datastore = Datastore.get(ws, 'workspaceblobstore')\n dataset = Dataset.File.upload_directory(path_data, target, pattern=None, overwrite=False, show_progress=True)\n    \n file_dataset = dataset.register(workspace=ws, name='reduced_classification_dataset',\n                                                  description='reduced_classification_dataset',\n                                                  create_new_version=True)\n\n\n\nBut then I don't understand if and how I can access this data like a normal file system from my python script, or I need further steps to be able to do that.",
        "Challenge_closed_time":1614833313088,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614762558803,
        "Challenge_favorite_count":7.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/296661\/azureml-notebooks-how-to-access-data-from-an-exper.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":15.1,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":19.6539680556,
        "Challenge_title":"AzureML Notebooks: how to access data from an experiment",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":179,
        "Platform":"Tool-specific",
        "Solution_body":"@Matzof Thanks for the question. Please follow the below code for writing.\n\n    datastore = ## get your defined in Workspace as Datastore \n datastore.upload(src_dir='.\/files\/to\/copy\/...',\n                  target_path='target\/directory',\n                  overwrite=True)\n\n\n\nDatastore.upload only support blob and fileshare. For adlsgen2 upload, you can try our new dataset upload API:\n\n\n\n from azureml.core import Dataset, Datastore\n datastore = Datastore.get(workspace, 'mayadlsgen2')\n Dataset.File.upload_directory(src_dir='.\/data', target=(datastore,'data'))\n\n\n\n\nPandas is integrated with fsspec which provides Pythonic implementation for filesystems including s3, gcs, and Azure. You can check the source for Azure here: dask\/adlfs: fsspec-compatible Azure Datake and Azure Blob Storage access (github.com). With this you can use normal filesystem operations like ls, glob, info, etc.\n\nYou can find an example (for reading data) here: azureml-examples\/1.intro-to-dask.ipynb at main \u00b7 Azure\/azureml-examples (github.com)\n\nWriting is essentially the same as reading, you need to switch the protocol to abfs (or az), slightly modify how you're accessing the data, and provide credentials unless your blob has public write access.\n\nYou can use the Azure ML Datastore to retrieve credentials like this (taken from example):",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"datastor upload data directori access normal file dataset upload api adlsgen upload panda integr fsspec normal filesystem oper glob datastor retriev credenti",
        "Solution_link_count":0.0,
        "Solution_original_content":"matzof write datastor defin workspac datastor datastor upload src dir file copi target path target directori overwrit datastor upload blob fileshar adlsgen upload dataset upload api core import dataset datastor datastor datastor workspac mayadlsgen dataset file upload directori src dir data target datastor data panda integr fsspec implement filesystem gc sourc dask adlf fsspec compat datak blob storag access github com normal filesystem oper glob read data intro dask ipynb github com write essenti read switch protocol abf slightli modifi access data credenti blob public write access datastor retriev credenti taken",
        "Solution_preprocessed_content":"write datastor defin workspac datastor overwrit blob fileshar adlsgen upload dataset upload api core import dataset datastor datastor mayadlsgen target datastor data panda integr fsspec implement filesystem gc sourc datak blob storag access normal filesystem oper glob write essenti read switch protocol abf slightli modifi access data credenti blob public write access datastor retriev credenti",
        "Solution_readability":9.8,
        "Solution_reading_time":16.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":160.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":30.8515688889,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to train and build the model in Sagemaker studio and then be able to export the model as a container image to ECR, so I can use the model in external platform by sharing the ECR image to another account where I Can create container with the image from ECR",
        "Challenge_closed_time":1663369533112,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663258467464,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUZHWz5-hpSc-80dEIkuxwQw\/how-to-export-tresained-models-to-ecr-as-container-image",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":3.8,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":30.8515688889,
        "Challenge_title":"How to export tresained models to ECR as container image",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":29.0,
        "Challenge_word_count":61,
        "Platform":"Tool-specific",
        "Solution_body":"The models you train in SageMaker are stored in S3 as .tar.gz files that you can use to deploy to an endpoint, or even test locally (extracting the model file from the tar file). If you are using a built-in algorithm, you can share the .tar.gz file to the second account and deploy the model in the second account, since built-in algorithm containers can be accessed from any AWS account.\n\nIf you are using a custom training image (docs here), you can push this image to ECR and allow a second account to pull the image and then use the image with the model that you have trained. However, note that Studio at this time does not support building Docker images out of the box. You can use SageMaker Notebook Instances instead.\n\nI would recommend keeping the model (.tar.gz) and the image (Docker) separate, since you can easily retrain and deploy the newer versions of models without updating the image every single time.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"share tar file train model account deploi model account built algorithm push train imag ecr allow account pull imag imag train model train imag notebook instanc studio build docker imag model tar imag docker separ easili retrain deploi newer version model updat imag time bias summari",
        "Solution_link_count":0.0,
        "Solution_original_content":"model train store tar file deploi endpoint test local extract model file tar file built algorithm share tar file account deploi model account built algorithm access account train imag doc push imag ecr allow account pull imag imag model train note studio time build docker imag box notebook instanc keep model tar imag docker separ easili retrain deploi newer version model updat imag singl time",
        "Solution_preprocessed_content":"model train store file deploi endpoint test local algorithm share file account deploi model account algorithm access account train imag push imag ecr allow account pull imag imag model train note studio time build docker imag box notebook instanc keep model imag separ easili retrain deploi newer version model updat imag singl time",
        "Solution_readability":8.0,
        "Solution_reading_time":11.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":163.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":6.9001127778,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello.\nYou can only select up to one maximum node when Create an Azure Machine Learning compute cluster. How do I select multiple nodes?",
        "Challenge_closed_time":1635456257703,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635431417297,
        "Challenge_favorite_count":9.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/607903\/about-creating-a-computing-cluster-with-azure-mach.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":2.47,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":6.9001127778,
        "Challenge_title":"About creating a computing cluster with Azure Machine Learning",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":32,
        "Platform":"Tool-specific",
        "Solution_body":"Hi, you can only select min and max number of nodes that you want to provision. The compute will autoscale to a maximum of this node count when a job is submitted. For more details, review Create an AML Compute Cluster.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"select minimum maximum node provis comput cluster comput autoscal maximum node count job submit review creat aml comput cluster document",
        "Solution_link_count":0.0,
        "Solution_original_content":"select node provis comput autoscal maximum node count job submit review creat aml comput cluster kindli accept",
        "Solution_preprocessed_content":"select node provis comput autoscal maximum node count job submit review creat aml comput cluster kindli accept",
        "Solution_readability":5.7,
        "Solution_reading_time":3.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":49.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":0.3261111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nLet's consider an ML edge inference use-case on Greengrass-managed device. The model is unique to each device, however its architecture and invocation logic are the same for all devices. In other words, the same invocation Lambda could be the same for all devices, only the model parameters would need to change across devices. We'd like to deploy a unique inference Lambda to all devices, and load device-specific artifact to each device.\n\nCan this be achieved with Greengrass ML Inference? It seems that GG MLI requires each model to be associated with a specific Lambda.\n\nOtherwise, is the recommended pattern to self-manage the inference in Lambda? E.g. by loading a specific model from S3 unique a local config file or some env variable?",
        "Challenge_closed_time":1605020138000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605018964000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUlVJHC1NaTTOquvDqs444oQ\/how-to-deploy-n-models-on-n-greengrass-devices-with-a-unique-lambda-for-inference-logic",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":10.22,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.3261111111,
        "Challenge_title":"How to deploy N models on N Greengrass devices with a unique Lambda for inference logic?",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":25.0,
        "Challenge_word_count":138,
        "Platform":"Tool-specific",
        "Solution_body":"In IoT Greengrass 1.x, the configuration is unique to each Greengrass Group. This includes Connectors, Lambdas and ML Resources.\n\nThe same Lambda can be referenced by multiple groups as a Greengrass function, which is likely what you want. This is similar to using one of the GG ML connectors (Object Detection or Image Classification).\n\nIn addition to your inference code, you'll also need to configure an ML Resource, which has a local name and a remote model. The local name would be the same for all Greengrass Groups, but in each group you will refer to a different remote object (the model) - either S3 or SageMaker job.\n\nEvery time a model changes, you will need to redeploy the corresponding Greengrass group for the changes to be deployed locally.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"lambda multipl group greengrass function connector object detect imag classif addit infer configur resourc local remot model local greengrass group group remot object model job time model greengrass group redeploi deploi local",
        "Solution_link_count":0.0,
        "Solution_original_content":"iot greengrass configur uniqu greengrass group connector lambda resourc lambda referenc multipl group greengrass function connector object detect imag classif addit infer configur resourc local remot model local greengrass group group remot object model job time model redeploi greengrass group deploi local",
        "Solution_preprocessed_content":"iot greengrass configur uniqu greengrass group connector lambda resourc lambda referenc multipl group greengrass function connector addit infer configur resourc local remot model local greengrass group group remot object job time model redeploi greengrass group deploi local",
        "Solution_readability":8.4,
        "Solution_reading_time":9.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":128.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":36.4428847222,
        "Challenge_answer_count":1,
        "Challenge_body":"I follow the official tutotial from microsoft: https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\n\nBut when I execute:\n\n #Bind model within Spark session\n model = pcontext.bind_model(\n     return_types=RETURN_TYPES, \n     runtime=RUNTIME, \n     model_alias=\"Sales\", #This alias will be used in PREDICT call to refer  this   model\n     model_uri=AML_MODEL_URI, #In case of AML, it will be AML_MODEL_URI\n     aml_workspace=ws #This is only for AML. In case of ADLS, this parameter can be removed\n ).register()\n\n\n\nI\u00b4ve got:\n\n\n\n\nNotADirectoryError: [Errno 20] Not a directory: '\/mnt\/var\/hadoop\/tmp\/nm-local-dir\/usercache\/trusted-service-user\/appcache\/application_1648328086462_0002\/spark-3d802a7e-15b7-4eb6-88c5-f0e01f8cdb35\/userFiles-fbe23a43-67d3-4e65-a879-4a497e804b40\/68603955220f5f8646700d809b71be9949011a2476a34965a3d5c0f3d14de79b.pkl\/MLmodel'\nTraceback (most recent call last):\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_context.py\", line 47, in bind_model\nudf = _create_udf(\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_udf.py\", line 104, in _create_udf\nmodel_runtime = runtime_gen._create_runtime()\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_runtime.py\", line 103, in _create_runtime\nif self._check_model_runtime_compatibility(model_runtime):\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_runtime.py\", line 166, in _check_model_runtime_compatibility\nmodel_wrapper = self._load()\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_runtime.py\", line 78, in _load\nreturn SynapsePredictModelCache._get_or_load(\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_cache.py\", line 172, in _get_or_load\nmodel = load_model(runtime, model_uri, functions)\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/utils\/_model_loader.py\", line 257, in load_model\nmodel = loader.load(model_uri, functions)\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/utils\/_model_loader.py\", line 122, in load\nmodel = self._load(model_uri)\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/utils\/_model_loader.py\", line 215, in _load\nreturn self._load_mlflow(model_uri)\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/utils\/_model_loader.py\", line 59, in _load_mlflow\nmodel = mlflow.pyfunc.load_model(model_uri)\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/mlflow\/pyfunc\/`init`.py\", line 640, in load_model\nmodel_meta = Model.load(os.path.join(local_path, MLMODEL_FILE_NAME))\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/mlflow\/models\/model.py\", line 124, in load\nwith open(path) as f:\n\nNotADirectoryError: [Errno 20] Not a directory: '\/mnt\/var\/hadoop\/tmp\/nm-local-dir\/usercache\/trusted-service-user\/appcache\/application_1648328086462_0002\/spark-3d802a7e-15b7-4eb6-88c5-f0e01f8cdb35\/userFiles-fbe23a43-67d3-4e65-a879-4a497e804b40\/68603955220f5f8646700d809b71be9949011a2476a34965a3d5c0f3d14de79b.pkl\/MLmodel'\n\nHow can I fix that error ?",
        "Challenge_closed_time":1648466772192,
        "Challenge_comment_count":3,
        "Challenge_created_time":1648335577807,
        "Challenge_favorite_count":16.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/788637\/azure-synapse-ml-predict-errno-20-not-a-directory.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":24.8,
        "Challenge_reading_time":48.74,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":36.4428847222,
        "Challenge_title":"Azure Synapse ML predict [Errno 20] Not a directory",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":196,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @ThiloBarth-2620,\n\nThanks for the question and using MS Q&A platform.\n\n(UPDATE:29\/3\/2022): You will experiencing this error message if you model does not contains all the required files in the ML model.\n\nAs per the repro, I had created two ML models named:\n\nsklearn_regression_model: Which contains only sklearn_regression_model.pkl file.\n\nWhen I predict for MLFLOW packaged model named sklearn_regression_model, getting same error as shown above:\n\nlinear_regression: Which contains the below files:\n\nWhen I predict for MLFLOW packaged model named linear_regression, it works as excepted.\n\n--------------------------------------------------\n\nIt should be AML_MODEL_URI = \"<aml model uri>\" #In URI \":x\" => Rossman_Sales:2\n\nBefore running this script, update it with the URI for ADLS Gen2 data file along with model output return data type and ADLS\/AML URI for the model file.\n\n #Set model URI\n        #Set AML URI, if trained model is registered in AML\n           AML_MODEL_URI = \"<aml model uri>\" #In URI \":x\" signifies model version in AML. You can   choose which model version you want to run. If \":x\" is not provided then by default   latest version will be picked.\n    \n        #Set ADLS URI, if trained model is uploaded in ADLS\n           ADLS_MODEL_URI = \"abfss:\/\/<filesystemname>@<account name>.dfs.core.windows.net\/<model   mlflow folder path>\"\n\nModel URI from AML Workspace:\n\n DATA_FILE = \"abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/AML\/LengthOfStay_cooked_small.csv\"\n AML_MODEL_URI_SKLEARN = \"aml:\/\/mlflow_sklearn:1\" #Here \":1\" signifies model version in AML. We can choose which version we want to run. If \":1\" is not provided then by default latest version will be picked\n RETURN_TYPES = \"INT\"\n RUNTIME = \"mlflow\"\n\nModel URI uploaded to ADLS Gen2:\n\n DATA_FILE = \"abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/AML\/LengthOfStay_cooked_small.csv\"\n AML_MODEL_URI_SKLEARN = \"abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/linear_regression\/linear_regression\" #Here \":1\" signifies model version in AML. We can choose which version we want to run. If \":1\" is not provided then by default latest version will be picked\n RETURN_TYPES = \"INT\"\n RUNTIME = \"mlflow\"\n\n\n\nHope this will help. Please let us know if any further queries.\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"messag model file model file updat uri adl gen data file model output return data type adl aml uri model file set aml uri train model regist aml adl uri train model upload adl choos model version run specifi version uri persist",
        "Solution_link_count":0.0,
        "Solution_original_content":"thilobarth platform updat experienc messag model file model repro creat model sklearn regress model sklearn regress model pkl file predict packag model sklearn regress model shown linear regress file predict packag model linear regress except aml model uri uri rossman sale run updat uri adl gen data file model output return data type adl aml uri model file set model uri set aml uri train model regist aml aml model uri uri signifi model version aml choos model version run default latest version pick set adl uri train model upload adl adl model uri abfss df core window net model uri aml workspac data file abfss data cheprasynaps df core window net aml lengthofstai cook small csv aml model uri sklearn aml sklearn signifi model version aml choos version run default latest version pick return type runtim model uri upload adl gen data file abfss data cheprasynaps df core window net aml lengthofstai cook small csv aml model uri sklearn abfss data cheprasynaps df core window net linear regress linear regress signifi model version aml choos version run default latest version pick return type runtim hope queri forget click upvot button origin poster commun faster identifi remind come respons subscrib notif program shape futur volunt moder",
        "Solution_preprocessed_content":"platform experienc messag model file model repro creat model file predict packag model shown file predict packag model except uri run updat uri adl gen data file model output return data type uri model file set model uri set aml uri train model regist aml uri signifi model version aml choos model version run default latest version pick set adl uri train model upload adl model uri aml workspac signifi model version aml choos version run default latest version pick runtim model uri upload adl gen signifi model version aml choos version run default latest version pick runtim hope queri forget click upvot button origin poster commun faster identifi remind come respons subscrib notif program shape futur volunt moder",
        "Solution_readability":10.8,
        "Solution_reading_time":32.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":358.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":2.6611111111,
        "Challenge_answer_count":1,
        "Challenge_body":"When I try to import Hugging Face BERT models to the conda_pytorch_p36 kernal of my Amazon SageMaker Notebook instance using the following pip command, the kernal always dies:\n\n! pip install transformers\n\n\nThe result is the same for Hugging Face BERT, RoBERTa, and GPT2 models on ml.c5.2xlarge and ml.c5d.4xlarge Amazon SageMaker instances.\n\nWhy is this happening, and how do I resolve the issue?",
        "Challenge_closed_time":1604527535000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604517955000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUsO3sfUGpTKeHiU8W9k1Kwg\/why-does-my-kernal-keep-dying-when-i-try-to-import-hugging-face-bert-models-to-amazon-sage-maker",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":6.02,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2.6611111111,
        "Challenge_title":"Why does my kernal keep dying when I try to import Hugging Face BERT models to Amazon SageMaker?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":458.0,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Solution_body":"This issue occurs when the latest sentence piece breaks. The workaround is to force install sentencepiece==0.1.91.\n\npip install sentencepiece==0.1.91",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"kernel dy import hug bert model conda pytorch kernel notebook instanc forc instal sentencepiec pip",
        "Solution_link_count":0.0,
        "Solution_original_content":"latest sentenc piec break workaround forc instal sentencepiec pip instal sentencepiec",
        "Solution_preprocessed_content":"latest sentenc piec break workaround forc instal pip instal",
        "Solution_readability":6.9,
        "Solution_reading_time":1.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":0.9152777778,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to build a camera that automatically recognizes a specific pet with image or video recognition. What AWS service can I use to identify an individual pet (not just the pet type). I've tried to use AWS Rekognition, but it can only differentiate between animal types, race, or color. Amazon SageMaker could be another option to create a completely new mode, but is very costly. What other AWS services can I use to identify specific pets?",
        "Challenge_closed_time":1601648944000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601645649000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU2S-z85hhSr-XVecDxE3ihw\/using-aws-services-to-perform-pet-face-recognition",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":6.02,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.9152777778,
        "Challenge_title":"Using AWS services to perform pet face recognition",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":132.0,
        "Challenge_word_count":84,
        "Platform":"Tool-specific",
        "Solution_body":"You can use Amazon Rekognition Custom Labels to use single class object detection to identify or classify a specific animal. However, note that Amazon Rekognition Custom Labels do not perform animal face recognition. It only classifies the image or object.\n\nFor example, you can train your detection model to identify an animal based on the images you provide for that label. For more information about using Amazon Rekognition Custom Labels, see this blog: https:\/\/aws.amazon.com\/blogs\/machine-learning\/training-a-custom-single-class-object-detection-model-with-amazon-rekognition-custom-labels\/.\n\nTo use frames from a video with Custom Labels, see Video analysis .",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"rekognit label identifi classifi anim train detect model identifi anim base imag label note rekognit label perform anim recognit classifi imag object frame video label high cost",
        "Solution_link_count":1.0,
        "Solution_original_content":"rekognit label singl class object detect identifi classifi anim note rekognit label perform anim recognit classifi imag object train detect model identifi anim base imag label rekognit label blog http com blog train singl class object detect model rekognit label frame video label video analysi",
        "Solution_preprocessed_content":"rekognit label singl class object detect identifi classifi anim note rekognit label perform anim recognit classifi imag object train detect model identifi anim base imag label rekognit label blog frame video label video analysi",
        "Solution_readability":13.6,
        "Solution_reading_time":8.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":86.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":91.5109113889,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>During the preparation for a training (in <code>prepare_data<\/code> in pytorch lightning) I either create or update local data (download, prepare different encodings). I then create a W&amp;B artifact and wait for the upload to be complete. Later in the code (in <code>setup()<\/code> in pytorch lightning) I use the data. Strictly speaking, this is not necessary, because I have the files locally, but I want to track the usage of the data (and the IDs of the data used for training, validation, \u2026). I added the <code>wait()<\/code> statement, because wandb would download the previous version (v=n-1) of the data \/without the enoding just added). In mode <code>ONLINE<\/code> this works nicely. However, in mode <code>DISABLED<\/code> I get this error: <code>ValueError: Cannot call wait on an artifact before it has been logged or in offline mode<\/code>. How am I supposed to handle <code>wait()<\/code>in order to have it work in all modes? (it would be nice if <code>wait()<\/code> would do it).<\/p>\n<p>This is the sample code:<\/p>\n<pre><code class=\"lang-python\"># Upload the data\nartifact = wandb.Artifact(name=..., type=...)\nartifact.description = ...\nartifact.metadata = ...\nartifact.add_file(local_path=...)\nwandb.run.log_artifact(artifact)\nartifact.save()  # I think I don't need this, playing around because of this issue\nartifact.wait()\n<\/code><\/pre>\n<pre><code class=\"lang-python\"># Use (Download) the data\nartifact = wandb.run.use_artifact(artifact_or_name=... + \":latest\")\nartifact_entry = artifact.get_path(...)\nartifact_entry.download(root=...)\n<\/code><\/pre>",
        "Challenge_closed_time":1655462923780,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655133484499,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-deal-with-artifact-wait-when-running-in-mode-disabled\/2607",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":11.1,
        "Challenge_reading_time":20.82,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":91.5109113889,
        "Challenge_title":"How to deal with artifact.wait() when running in mode \"DISABLED\"",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":84.0,
        "Challenge_word_count":210,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/hogru\">@hogru<\/a>, sorry about the late response. Runs have a <code>disabled<\/code> attribute. Here is a code snippet you can use:<\/p>\n<pre><code class=\"lang-auto\">run = wandb.init(mode=\"disabled\")\nif run.disabled:\n    \/\/ your code\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"run disabl mode run disabl mode execut call wait artifact",
        "Solution_link_count":0.0,
        "Solution_original_content":"hogru sorri late respons run disabl attribut run init mode disabl run disabl",
        "Solution_preprocessed_content":"sorri late respons run attribut",
        "Solution_readability":5.6,
        "Solution_reading_time":3.6,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":52.4565155556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created 100 runs, and would like to make two scatter plots. The first scatter plot involves the first 50 simulations, with axis limits [0,10] in both directions. The second scatter plot uses simulations 51 to 100, with different axis limits, say [10,20]. So far, I created a new panel, for both these plots. But wandb does not like that. Whatever I set the axis limits will be the same for both subsets (1-50, and 51-100). What is the recommended approach to have a plot for each of the data subsets? Must I create two different panels? If so, that means that one panel 2 might have to be turned off for the first batch of data experiments, and panel 1 would be turned off for the second batch of experiments. Is this the recommended approach? Thanks.<\/p>",
        "Challenge_closed_time":1660342284524,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660153441068,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/axis-scales\/2892",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":9.39,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":52.4565155556,
        "Challenge_title":"Axis scales",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":139,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/erlebacher\">@erlebacher<\/a>,<\/p>\n<p>Have you tried creating reports with different panel plots? They should work here.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"creat report panel plot creat separ plot subset data",
        "Solution_link_count":0.0,
        "Solution_original_content":"erlebach tri creat report panel plot ramit",
        "Solution_preprocessed_content":"tri creat report panel plot ramit",
        "Solution_readability":6.9,
        "Solution_reading_time":2.39,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":29.6317019444,
        "Challenge_answer_count":2,
        "Challenge_body":"I trained a model with Designer, created a real-time inference pipeline which was succesfully submitted. When deploying to either ACI or AKS it fails and I get the error \"ModuleNotFoundError: No module named 'azureml.api'\". I've had no problems deploying this model many times in the past and haven't changed anything. Even if I use one of the sample pipelines (automobiles basic), I get the same error when deploying to real-time.",
        "Challenge_closed_time":1632968850120,
        "Challenge_comment_count":2,
        "Challenge_created_time":1632862175993,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/569925\/deployment-from-designer-fails-in-every-possible-w.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":9.3,
        "Challenge_reading_time":5.99,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":29.6317019444,
        "Challenge_title":"Deployment from Designer fails in every possible way",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":77,
        "Platform":"Tool-specific",
        "Solution_body":"It's an known issue caused by unexpected module version upgrade. It's been resolved by applying hotfix to all regions. For users, please rerun training pipeline by check on \"Regenerate Output\", and run corresponding inference pipeline and try deployment again.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"messag identifi modul version upgrad appli hotfix region rerun train pipelin regener output run infer pipelin deploy",
        "Solution_link_count":0.0,
        "Solution_original_content":"modul version upgrad appli hotfix region rerun train pipelin regener output run infer pipelin deploy",
        "Solution_preprocessed_content":"modul version upgrad appli hotfix region rerun train pipelin regener output run infer pipelin deploy",
        "Solution_readability":9.5,
        "Solution_reading_time":3.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":90.4033952778,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi Team,\n\nI tried connecting to Azure table storage in Azure ML Studio. It shows connection successful after updating all credentials but after hitting run, import is landing to internal system error.\nBelow is the message :\n[Critical] Error: Sorry, it seems that you have encountered an internal system error. Please contact amlforum@microsoft.com with the full URL in the browser and the time you experienced the failure. We can locate this error with your help and investigate further. Thank you.\n\nRequesting you to please assist in this case.\n\nRegards,\nSachin",
        "Challenge_closed_time":1616395982956,
        "Challenge_comment_count":6,
        "Challenge_created_time":1616070530733,
        "Challenge_favorite_count":9.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/320696\/data-import-error-for-azure-table-storage-to-azure.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":8.2,
        "Challenge_reading_time":7.67,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":90.4033952778,
        "Challenge_title":"Data Import error for Azure table storage to Azure ML studio ?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":100,
        "Platform":"Tool-specific",
        "Solution_body":"Hello,\n\nThere is a known issue that Azure ML Studio only supports \u201chttp\u201d protocol when connecting with Azure Storage Account. You might hit this issue when using the Import Data module.\n\n\n\n\nHere is a quick work around:\nPlease check the \u201cConfiguration\u201d of your Storage Account, and make sure the \u201cSecure transfer required\u201d is disabled (see the figure below).\n\nIf still encountering error after taking these steps, please double check and make sure the account key is correct.\n\n@SachinGaikwad-5400 Please accept the answer if you feel the work around works. Thank you!\n\nRegards,\nYutong",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"workaround configur storag account secur transfer option disabl persist doubl account kei",
        "Solution_link_count":0.0,
        "Solution_original_content":"studio http protocol connect storag account hit import data modul quick configur storag account secur transfer disabl figur step doubl account kei sachingaikwad accept yutong",
        "Solution_preprocessed_content":"studio http protocol connect storag account hit import data modul quick configur storag account secur transfer disabl step doubl account kei accept yutong",
        "Solution_readability":9.4,
        "Solution_reading_time":7.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":93.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":6.8756194445,
        "Challenge_answer_count":1,
        "Challenge_body":"I have created an ML model and created a real time endpoint with the model and also published the pipeline. I retrained it with a different parameter and ran the experiment. Also I have published the endpoint. Now how do I deploy it or replace it with the already created endpoint?",
        "Challenge_closed_time":1612984587887,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612959835657,
        "Challenge_favorite_count":6.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/267341\/how-do-i-deploy-the-run-after-retraining-a-publish.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":4.35,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":6.8756194445,
        "Challenge_title":"How do I deploy the run after retraining a published endpoint and consume it?",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":64,
        "Platform":"Tool-specific",
        "Solution_body":"Hi, this document provides information on how to update a web service that was deployed with Azure Machine Learning. Hope this helps.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"link document explain updat web servic deploi",
        "Solution_link_count":0.0,
        "Solution_original_content":"document updat web servic deploi hope",
        "Solution_preprocessed_content":"document updat web servic deploi hope",
        "Solution_readability":6.4,
        "Solution_reading_time":1.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":55.8666666667,
        "Challenge_answer_count":1,
        "Challenge_body":"When locally testing my custom-job through \"gcloud ai custom-jobs local-run\" command, I would like to have access to a bucket mounted though gcsFuse as it happens when I launch the same containerized job from GCloud console. Is there the option to have the same access locally?Thank you for helping",
        "Challenge_closed_time":1664527380000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664326260000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Mount-gcsfuse-in-gcloud-ai-custom-jobs-local-run\/td-p\/471834\/jump-to\/first-unread-message",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":4.3,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":55.8666666667,
        "Challenge_title":"Mount gcsfuse in gcloud ai custom-jobs local-run",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":96.0,
        "Challenge_word_count":55,
        "Platform":"Tool-specific",
        "Solution_body":"What you could do is use cloud storage as a file system within ai training, since while using fuse your training jobs on both of the platforms can access your data that is stored on Cloud Storage as files on your local file system, also the documentation I shared provides you useful information as the problems you might encounter, permissions, a brief description of cloud storage fuse, performance related information, the restrictions this method has and also how you can make use of the logs.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"cloud storag file train achiev fuse allow train job platform access data store cloud storag file local file document share potenti permiss perform restrict log",
        "Solution_link_count":0.0,
        "Solution_original_content":"cloud storag file train fuse train job platform access data store cloud storag file local file document share permiss brief descript cloud storag fuse perform relat restrict log origin",
        "Solution_preprocessed_content":"cloud storag file train fuse train job platform access data store cloud storag file local file document share permiss brief descript cloud storag fuse perform relat restrict log origin",
        "Solution_readability":19.7,
        "Solution_reading_time":6.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":90.0,
        "Tool":"Vertex AI"
    },
    {
        "Challenge_adjusted_solved_time":0.0708333333,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a customer who wants to install LightGBM on SageMaker notebooks, as they are currently using it outside of SageMaker.\n\nRight now, they are interested in the ability to SSH into the instance, but it would be great if we could provide them a way to install LightGBM right now.\n\nCheers",
        "Challenge_closed_time":1516633097000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1516632842000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUPwkZcylKQR6-u0pghgrseA\/light-gbm-on-sage-maker",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":3.76,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0708333333,
        "Challenge_title":"LightGBM on SageMaker",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":388.0,
        "Challenge_word_count":54,
        "Platform":"Tool-specific",
        "Solution_body":"It's possible to do, I have used it myself for gradient boosting, from within Jupyter you can simply run:\n\n!conda install -y -c conda-forge lightgbm\n\n\nWithin a selected conda environment. No terminal access is needed, however it must be done, On the top right of the Jupyter notebook you can choose a terminal environment which will give you a shell to the backend instance and you can install there.\n\nHowever if you want the notebook to be immutable\/transferable you can do the install within the notebook .\n\nThanks",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"instal lightgbm notebook run conda instal conda forg lightgbm select conda environ jupyt ssh instanc choos termin environ jupyt notebook shell backend instanc instal notebook immut transfer instal notebook",
        "Solution_link_count":0.0,
        "Solution_original_content":"gradient boost jupyt simpli run conda instal conda forg lightgbm select conda environ termin access jupyt notebook choos termin environ shell backend instanc instal notebook immut transfer instal notebook",
        "Solution_preprocessed_content":"gradient boost jupyt simpli run conda instal lightgbm select conda environ termin access jupyt notebook choos termin environ shell backend instanc instal notebook instal notebook",
        "Solution_readability":10.6,
        "Solution_reading_time":6.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":116.0143763889,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I am trying to start a sweep using this yaml file.<\/p>\n<p>sweep.yaml<\/p>\n<pre><code class=\"lang-auto\">method: bayes\nmetric:\n  goal: maximize\n  name: val_f1_score\nparameters:\n  notes:\n    value: \"\"\n  seed:\n    value: 42\n  lr:\n    values: [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n  epochs:\n    value: 30\n  augmentation:\n    value: True\n  class_weights:\n    value: True\n  optimizer:\n    value: adam\n  loss:\n    value: categorical_crossentropy\n  metrics:\n    value: [\"accuracy\"]\n  batch_size:\n    value: 64\n  num_classes:\n    value: 7\n  paths:\n    - \n      data:\n        value: ${hydra:runtime.cwd}\/data\/4_tfds_dataset\/\n\nwandb:\n  -\n    use:\n      value: True\n    project:\n      value: Whats-this-rock\n\ndataset:\n  -\n    id:\n      value: [1, 2, 3, 4]\n    dir:\n      value: data\/3_consume\/\n    image:\n      size:\n        value: 124\n      channels:\n        value: 3\n    classes:\n      value: 10\n    sampling:\n      value: None\n\nmodel:\n  -\n    backbone:\n      value: efficientnetv2m\n    use_pretrained_weights:\n      value: True\n    trainable:\n      value: True\n    preprocess:\n      value: True\n    dropout_rate:\n      value: 0.3\n\ncallback:\n  -\n    monitor:\n      value: \"val_f1_score\"\n    earlystopping:\n      patience:\n        value: 10\n    reduce_lr:\n      factor:\n        values: [.9, .7, .5]\n      min_lr: 0.00001\n      patience:\n        values: [1, 2, 3, 4]\n    save_model:\n      status:\n        value: True\n      best_only:\n        value: True\n\nprogram: src\/models\/train.py\n\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">Error: Invalid sweep config: invalid hyperparameter configuration: paths\n<\/code><\/pre>\n<p>Here\u2019s the full traceback of the error:-<\/p>\n<pre><code class=\"lang-auto\">During handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/cli\/cli.py\", line 97, in wrapper\n    return func(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/cli\/cli.py\", line 942, in sweep\n    launch_scheduler=_launch_scheduler_spec,\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/apis\/internal.py\", line 102, in upsert_sweep\n    return self.api.upsert_sweep(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/apis\/normalize.py\", line 62, in wrapper\n    raise CommError(message, err).with_traceback(sys.exc_info()[2])\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/apis\/normalize.py\", line 26, in wrapper\n    return func(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2178, in upsert_sweep\n    raise e\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2175, in upsert_sweep\n    check_retry_fn=no_retry_4xx,\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/lib\/retry.py\", line 129, in __call__\n    retry_timedelta_triggered = check_retry_fn(e)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2153, in no_retry_4xx\n    raise UsageError(body[\"errors\"][0][\"message\"])\nwandb.errors.CommError: Invalid sweep config: invalid hyperparameter configuration: paths\n<\/code><\/pre>\n<p>I am using hydra and trying to replicate a config.yaml for wandb sweeps<\/p>\n<p>config.yaml<\/p>\n<pre><code class=\"lang-auto\">notes: \"\"\nseed: 42\nlr: 0.001\nepochs: 30\naugmentation: True\nclass_weights: True\noptimizer: adam\nloss: categorical_crossentropy\nmetrics: [\"accuracy\"]\nbatch_size: 64\nnum_classes: 7\n\npaths:\n  data: ${hydra:runtime.cwd}\/data\/4_tfds_dataset\/\n\nwandb:\n  use: True\n  project: Whats-this-rock\n\ndataset:\n  id: [1, 2, 3, 4]\n  dir: data\/3_consume\/\n  image:\n    size: 124\n    channels: 3\n  classes: 10\n  sampling: None\n\nmodel:\n  backbone: efficientnetv2m\n  use_pretrained_weights: True\n  trainable: True\n  preprocess: True\n  dropout_rate: 0.3\n\ncallback:\n  monitor: \"val_f1_score\"\n  earlystopping:\n    patience: 10\n  reduce_lr:\n    factor: 0.4\n    min_lr: 0.00001\n    patience: 2\n  save_model:\n    status: True\n    best_only: True\n\n<\/code><\/pre>",
        "Challenge_closed_time":1663515880848,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663098229093,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/multi-level-nesting-in-yaml-for-sweeps\/3108",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":13.1,
        "Challenge_reading_time":47.1,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":116.0143763889,
        "Challenge_title":"Multi-level nesting in yaml for sweeps",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":762.0,
        "Challenge_word_count":350,
        "Platform":"Tool-specific",
        "Solution_body":"<p>The solution is to use dot notation instead of nested parameters as wandb (v0.13.3) sweeps doesn\u2019t support nested parameters.<\/p>\n<pre><code class=\"lang-auto\">sweep.yaml\n\nmethod: bayes\nmetric:\n  goal: maximize\n  name: val_accuracy\nparameters:\n  notes:\n    value: \"\"\n  seed:\n    values: [1, 42, 100]\n  lr:\n    values: [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n  epochs:\n    value: 100\n  augmentation:\n    value: True\n  class_weights:\n    value: True\n  optimizer:\n    values: [adam, adamax]\n  loss:\n    value: categorical_crossentropy\n  metrics:\n    value: [\"accuracy\"]\n  batch_size:\n    value: 64\n  num_classes:\n    value: 7\n  train_split:\n    values:\n      - 0.70\n      - 0.75\n      - 0.80\n  data_path:\n    value: data\/4_tfds_dataset\/\n  wandb.use:\n    value: True\n  wandb.mode:\n    value: online\n  wandb.project:\n    value: Whats-this-rockv3\n  dataset_id:\n    values:\n      - [1]\n  image_size:\n    value: 224\n  image_channels:\n    value: 3\n  sampling:\n    values: [None, oversampling, undersampling]\n  backbone:\n    values:\n      [\n        efficientnetv2m,\n        efficientnetv2,\n        resnet,\n        mobilenetv2,\n        inceptionresnetv2,\n        xception,\n      ]\n  use_pretrained_weights:\n    values: [True]\n  trainable:\n    values: [True, False]\n  preprocess:\n    value: True\n  dropout_rate:\n    values: [0.3]\n  monitor:\n    value: \"val_accuracy\"\n  earlystopping.use:\n    value: True\n  earlystopping.patience:\n    values: [10]\n  reduce_lr.use:\n    values: [True]\n  reduce_lr.factor:\n    values: [.9, .7, .5, .3]\n  reduce_lr.patience:\n    values: [1, 3, 5, 7, 13]\n  reduce_lr.min_lr:\n    value: 1e-5\n  save_model:\n    value: False\n\nprogram: src\/models\/train.py\ncommand:\n  - ${env}\n  - python\n  - ${program}\n  - ${args_no_hyphens}\n\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"dot notat nest paramet sweep doesnt nest paramet",
        "Solution_link_count":0.0,
        "Solution_original_content":"dot notat nest paramet sweep doesnt nest paramet sweep yaml bay metric goal maxim val accuraci paramet note valu seed valu valu epoch valu augment valu class weight valu optim valu adam adamax loss valu categor crossentropi metric valu accuraci batch size valu num class valu train split valu data path valu data tfd dataset valu mode valu onlin valu what rockv dataset valu imag size valu imag channel valu sampl valu oversampl undersampl backbon valu efficientnetvm efficientnetv resnet mobilenetv inceptionresnetv xception pretrain weight valu trainabl valu preprocess valu dropout rate valu monitor valu val accuraci earlystop valu earlystop patienc valu reduc valu reduc factor valu reduc patienc valu reduc valu save model valu program src model train env program arg hyphen",
        "Solution_preprocessed_content":"dot notat nest paramet sweep doesnt nest paramet",
        "Solution_readability":12.1,
        "Solution_reading_time":18.77,
        "Solution_score_count":null,
        "Solution_sentence_count":15.0,
        "Solution_word_count":159.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":11.1486111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI am new to SageMaker and I am trying to deploy my model to an endpoint but am getting the following error:\n\nFailure reason\nUnable to locate at least 2 availability zone(s) with the requested instance type ml.t2.medium that overlap with SageMaker subnets\n\nI have tried using different instance types but always the same error\n\nI was under the impression that SageMaker will create the required instances for me and I do not need to create the instances first? I am using the EU-WEST-1 zone and using the console to setup the endpoint",
        "Challenge_closed_time":1553556696000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1553516561000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUySs_fgNpSE6wuY-6W7MwqQ\/unable-to-create-endpoint",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":6.82,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":11.1486111111,
        "Challenge_title":"Unable to create endpoint",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":301.0,
        "Challenge_word_count":97,
        "Platform":"Tool-specific",
        "Solution_body":"Hello,\n\nSagemaker engineer here. I looked at the VpcConfig of your model and found only one subnet configured.\n\nThe error message \"Unable to locate at least 2 availability zone(s) with the requested instance type XYZ that overlap with SageMaker subnets\" usually indicates misconfigured VPCs. Sagemaker imposes mandatory requirement for at least 2 availability zones in your VPC subnets even if you only request one instance, to account for the potential use of auto-scaling in the future.\n\nIn order to create the endpoint, the number of subnets in your model needs to be at least 2 in distinct availability zones, and ideally as close to the total number of availability zones as possible in the region.\n\nHope it helps,\nWenzhao",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"messag misconfigur vpc configur subnet distinct zone vpc creat endpoint subnet ideal close zone region",
        "Solution_link_count":0.0,
        "Solution_original_content":"engin vpcconfig model subnet configur messag locat zone request instanc type xyz overlap subnet misconfigur vpc impos mandatori zone vpc subnet request instanc account potenti auto scale futur order creat endpoint subnet model distinct zone ideal close zone region hope wenzhao",
        "Solution_preprocessed_content":"engin vpcconfig model subnet configur messag locat zone request instanc type xyz overlap subnet misconfigur vpc impos mandatori zone vpc subnet request instanc account potenti futur order creat endpoint subnet model distinct zone ideal close zone region hope wenzhao",
        "Solution_readability":11.0,
        "Solution_reading_time":8.89,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":119.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":11.8290191667,
        "Challenge_answer_count":1,
        "Challenge_body":"I see one document mentioned time series training can be done with AutoML: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-forecast is that any sample which from basic build of model?",
        "Challenge_closed_time":1659343661096,
        "Challenge_comment_count":1,
        "Challenge_created_time":1659301076627,
        "Challenge_favorite_count":11.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/949086\/time-series-training.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.2,
        "Challenge_reading_time":3.01,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":11.8290191667,
        "Challenge_title":"time series training",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @matsuoka-4412\n\nThanks for using Microsoft Q&A platform, we don't have any samples for basic build of a model in AutoML, but we do have quick start for how to use time series in AutoML - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-automated-ml-forecast\n\nThis is a low code sample for beginning user. Please take a look.\n\n\n\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"quick start guid time seri automl low sampl begin",
        "Solution_link_count":1.0,
        "Solution_original_content":"matsuoka platform sampl build model automl quick start time seri automl http doc com tutori autom forecast low sampl begin yutong kindli accept commun",
        "Solution_preprocessed_content":"platform sampl build model automl quick start time seri automl low sampl begin yutong kindli accept commun",
        "Solution_readability":9.8,
        "Solution_reading_time":5.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":1.103265,
        "Challenge_answer_count":1,
        "Challenge_body":"I have tried to read the dataset from datastore. Also tried to create the dataset also.\n\nThe code for reading the dataset is below\n\n from azureml.core import Workspace\n ws = Workspace.from_config()\n datastore = Datastore.get(ws, 'qdataset')\n\n\n\nIt works fine still now.\n\n from azureml.core.dataset import Dataset\n six_dataset = Dataset.get_by_name(workspace=ws, name='combined_classifier')\n\n\n\nAlso i have tried from azureml.core import Dataset\n\nIt shows the following error:\n\n2021-04-29 11:56:47.284077 | ActivityCompleted: Activity=_dataflow, HowEnded=Failure, Duration=0.0 [ms], Info = {'activity_id': 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'activity_name': '_dataflow', 'activity_type': 'InternalCall', 'app_name': 'dataset', 'source': 'azureml.dataset', 'version': '1.27.0', 'dataprepVersion': '2.14.2', 'subscription': '', 'run_id': '', 'resource_group': '', 'workspace_name': '', 'experiment_id': '', 'location': '', 'completionStatus': 'Failure', 'durationMs': 962.01}, Exception=AttributeError; module 'azureml.dataprep' has no attribute 'api'\n\n\n\n\nAttributeError Traceback (most recent call last)\n<ipython-input-34-ac7a8d35da4d> in <module>\n1 from azureml.core.dataset import Dataset\n----> 2 six_dataset = Dataset.get_by_name(workspace=ws, name='combined_classifier')\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(args, *kwargs)\n127 with LoggerFactory.track_activity(logger, func.name_, activity_type, custom_dimensions) as al:\n128 try:\n--> 129 return func(args, *kwargs)\n130 except Exception as e:\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in get_by_name(workspace, name, version)\n87 :rtype: typing.Union[azureml.data.TabularDataset, azureml.data.FileDataset]\n88 \"\"\"\n---> 89 dataset = AbstractDataset._get_by_name(workspace, name, version)\n90 AbstractDataset._track_lineage([dataset])\n91 return dataset\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _get_by_name(workspace, name, version)\n652 if not success:\n653 raise result\n--> 654 dataset = _dto_to_dataset(workspace, result)\n655 warn_deprecated_blocks(dataset)\n656 return dataset\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_dataset_rest_helper.py in _dto_to_dataset(workspace, dto)\n93 registration=registration)\n94 if dto.dataset_type == _DATASET_TYPE_FILE:\n---> 95 return FileDataset._create(\n96 definition=dataflow_json,\n97 properties=dto.latest.properties,\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(args, *kwargs)\n127 with LoggerFactory.track_activity(logger, func.name_, activity_type, custom_dimensions) as al:\n128 try:\n--> 129 return func(args, *kwargs)\n130 except Exception as e:\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _create(cls, definition, properties, registration, telemetry_info)\n555 from azureml.data._partition_format import parse_partition_format\n556\n--> 557 steps = dataset._dataflow._get_steps()\n558 partition_keys = []\n559 for step in steps:\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(args, *kwargs)\n127 with LoggerFactory.track_activity(logger, func.name_, activity_type, custom_dimensions) as al:\n128 try:\n--> 129 return func(args, *kwargs)\n130 except Exception as e:\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _dataflow(self)\n215 raise UserErrorException('Dataset definition is missing. Please check how the dataset is created.')\n216 if self._registration and self._registration.workspace:\n--> 217 dataprep().api._datastore_helper._set_auth_type(self._registration.workspace)\n218 if not isinstance(self._definition, dataprep().Dataflow):\n219 try:\n\nAttributeError: module 'azureml.dataprep' has no attribute 'api'\n\n\n\n\n\nPlease give a solution to solve this",
        "Challenge_closed_time":1619702571567,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619698599813,
        "Challenge_favorite_count":7.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/377203\/error-while-accessing-the-dataset-from-a-datastore.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":15.7,
        "Challenge_reading_time":54.54,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":1.103265,
        "Challenge_title":"Error while accessing the dataset from a datastore",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":337,
        "Platform":"Tool-specific",
        "Solution_body":"It now worked..\nWe need to install azure-ml-api-sdk using this command\n\npip install azure-ml-api-sdk",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"instal api sdk packag pip instal api sdk instal packag access dataset attributeerror",
        "Solution_link_count":0.0,
        "Solution_original_content":"instal api sdk pip instal api sdk",
        "Solution_preprocessed_content":null,
        "Solution_readability":6.0,
        "Solution_reading_time":1.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":14.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":74.4278163889,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi, getting this error when i run an azureml experiment with custom_docker_image (basegpu image of mcr) - can anybody help me understand this? Have tested this in local compute and it works, not sure why this does not work on a training cluster vm?\n\n\n\n\n    azureml._restclient.exceptions.ServiceException: ServiceException:\n         Code: 400\n         Message: (UserError) Error when parsing request; unable to deserialize request body\n         Details:\n        \n         Headers: {\n             \"Date\": \"Mon, 08 Jun 2020 11:03:52 GMT\",\n             \"Content-Type\": \"application\/json; charset=utf-8\",\n             \"Transfer-Encoding\": \"chunked\",\n             \"Connection\": \"keep-alive\",\n             \"Request-Context\": \"appId=cid-v1:6a27ce65-5555-41a3-85f7-b7a1ce31fd6b\",\n             \"x-ms-response-type\": \"error\",\n             \"Strict-Transport-Security\": \"max-age=15724800; includeSubDomains; preload\"\n         }\n         InnerException: {\n         \"additional_properties\": {},\n         \"error\": {\n             \"additional_properties\": {},\n             \"code\": \"UserError\",\n             \"message\": \"Error when parsing request; unable to deserialize request body\",\n             \"details_uri\": null,\n             \"target\": null,\n             \"details\": [],\n             \"inner_error\": null,\n             \"debug_info\": null,\n             \"message_format\": null,\n             \"message_parameters\": null,\n             \"reference_code\": null\n         },\n         \"correlation\": {\n             \"operation\": \"e96d6285280f5849a4a5e3f172d65d36\",\n             \"request\": \"1beee8ecb7180147\"\n         },\n         \"environment\": \"westeurope\",\n         \"location\": \"westeurope\",\n         \"time\": {}\n     }",
        "Challenge_closed_time":1591884305636,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591616365497,
        "Challenge_favorite_count":3.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/33313\/usererror-error-when-parsing-request-unable-to-des.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":23.2,
        "Challenge_reading_time":17.86,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":74.4278163889,
        "Challenge_title":"(UserError) Error when parsing request; unable to deserialize request body",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":132,
        "Platform":"Tool-specific",
        "Solution_body":"My bad, after giving it some days and looking at the code, I noticed i had forgotten to add the parameters for the estimator configuration. Here is the estimator configuration that works for me:\n\n\n\n estimator = Estimator(source_directory=experiment_folder,\n                       compute_target=compute_target,\n                       script_params=script_params,\n                       entry_script='rps_efn_b0.py',\n                       node_count=1,        \n                       conda_packages=['ipykernel'],\n                       pip_packages = ['azureml-sdk',\n                                       'pyarrow',\n                                       'pyspark',\n                                       'azureml-mlflow',\n                                       'joblib',\n                                       'matplotlib',\n                                       'Pillow',\n                                       'tensorflow==2.2',\n                                       'tensorflow-datasets',\n                                       'tensorflow-hub',\n                                       'azureml-defaults',\n                                       'azureml-dataprep[fuse,pandas]'],\n                       custom_docker_image='mcr.microsoft.com\/azureml\/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04')",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"paramet estim configur estim configur paramet docker imag run successfulli train cluster",
        "Solution_link_count":0.0,
        "Solution_original_content":"dai notic forgotten add paramet estim configur estim configur estim estim sourc directori folder comput target comput target param param entri rp efn node count conda packag ipykernel pip packag sdk pyarrow pyspark joblib matplotlib pillow tensorflow tensorflow dataset tensorflow hub default dataprep fuse panda docker imag mcr com base gpu openmpi cuda cudnn ubuntu",
        "Solution_preprocessed_content":"dai notic forgotten add paramet estim configur estim configur estim",
        "Solution_readability":22.8,
        "Solution_reading_time":9.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":27.1724516667,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI am trying to export data from Azure ML to an Azure SQL Database using the 'Export Data' module but the log file contains the following messages and no data is exported to the database.\n\n\"Not exporting to run RunHistory as the exporter is either stopped or there is no data\"\n\n\"Process exiting with code: 0\n\nThere is definitely data flowing to the 'Export Data' module from an 'Execute R Script' module as I have checked the Result dataset.\n\nWould appreciate some assistance.\n\nThank you.",
        "Challenge_closed_time":1629106747876,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629008927050,
        "Challenge_favorite_count":9.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/514067\/no-data-being-exported-from-39export-data39-module.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":6.64,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":27.1724516667,
        "Challenge_title":"No Data being exported from 'Export Data' module in Azure ML",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":96,
        "Platform":"Tool-specific",
        "Solution_body":"Hi,\n\nI have resolved this issue. I had set the export table to be dbo.TestTable rather than just TestTable. As the table dbo.TestTable did not exist the 'Export module' created it in the dbo schema so the table name effectively became dbo.dbo.TestTable.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"set export tabl testtabl dbo testtabl tabl dbo testtabl export modul creat dbo schema tabl dbo dbo testtabl",
        "Solution_link_count":0.0,
        "Solution_original_content":"set export tabl dbo testtabl testtabl tabl dbo testtabl export modul creat dbo schema tabl dbo dbo testtabl",
        "Solution_preprocessed_content":"set export tabl testtabl tabl export modul creat dbo schema tabl",
        "Solution_readability":5.4,
        "Solution_reading_time":3.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":2.01856,
        "Challenge_answer_count":1,
        "Challenge_body":"Hey,\nIs there any way to export the ML Pipeline as Template\/PNG\/Code ?",
        "Challenge_closed_time":1668156834876,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668149568060,
        "Challenge_favorite_count":12.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1085047\/azure-ml-pipeline-designer-export.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":1.29,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":2.01856,
        "Challenge_title":"Azure ML pipeline designer export",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":16,
        "Platform":"Tool-specific",
        "Solution_body":"@its-kumar The designer pipelines cannot be exported to code or a template currently.\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"kumar design pipelin export templat click upvot commun member read thread",
        "Solution_preprocessed_content":"design pipelin export templat click upvot commun member read thread",
        "Solution_readability":9.5,
        "Solution_reading_time":2.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":32.6386941667,
        "Challenge_answer_count":1,
        "Challenge_body":"When using speech-to-text to transfer audio file to text, I found that the function would stop working if human voices haven't occurred for about 5 seconds. In my case, what I want to transfer is audios of interviews, which would often contain some advertisements or music in the middle of it, and when this happens, the speech-to-text would only transfer the first half of the whole audio, and report an error that \"No speech could be recognized\".\nIn this case, how can I extend the waiting time of that in order to transfer the whole file in Python codes?",
        "Challenge_closed_time":1649205144236,
        "Challenge_comment_count":3,
        "Challenge_created_time":1649087644937,
        "Challenge_favorite_count":12.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/799565\/how-do-i-extend-the-waiting-time-of-azure-speech-t.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":11.8,
        "Challenge_reading_time":7.62,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":32.6386941667,
        "Challenge_title":"How do I extend the waiting time of Azure speech-to-text API in Python?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":111,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @MuyaoHu-4139\n\nI think there are two solutions you can have a try in Python SDK:\n\nThere is a 'set_property' method on the config to allow you to set parameters to your request, which can change the default silence time:: https:\/\/docs.microsoft.com\/en-us\/python\/api\/azure-cognitiveservices-speech\/azure.cognitiveservices.speech.propertycollection?view=azure-python#azure-cognitiveservices-speech-propertycollection-set-property\n\nThis way you can set the EndSilenceTimeout (PropertyIDs in Pyhton: https:\/\/docs.microsoft.com\/en-us\/python\/api\/azure-cognitiveservices-speech\/azure.cognitiveservices.speech.propertyid?view=azure-python#fields)\n\n\n\n\nPlease notice, the time is as \"ms\". Hope above helps!\n\n\n\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful, thanks!",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"set properti config default silenc time set endsilencetimeout paramet millisecond",
        "Solution_link_count":2.0,
        "Solution_original_content":"muyaohu sdk set properti config allow set paramet request default silenc time http doc com api cognitiveservic speech cognitiveservic speech propertycollect cognitiveservic speech propertycollect set properti set endsilencetimeout propertyid pyhton http doc com api cognitiveservic speech cognitiveservic speech propertyid field notic time hope yutong kindli accept",
        "Solution_preprocessed_content":"sdk config allow set paramet request default silenc time set endsilencetimeout notic time hope yutong kindli accept",
        "Solution_readability":15.0,
        "Solution_reading_time":10.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":74.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":1181.1,
        "Challenge_answer_count":2,
        "Challenge_body":"Hello,I am trying to run a Custom Training Job in the Vertex AI Training service.The job is based on a tutorial for that fine-tuning a pre-trained BERT model (from HuggingFace).When I use the `gcloud` CLI tool to auto-package my training code into a Docker image and deploy it to the Vertex AI Training service like so:$BASE_GPU_IMAGE=\"us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-7:latest\"\n$BUCKET_NAME = \"my-bucket\"gcloud ai custom-jobs create `\n--region=us-central1 `\n--display-name=fine_tune_bert `\n--args=\"--job_dir=$BUCKET_NAME,--num-epochs=2,--model-name=finetuned-bert-classifier\" `\n--worker-pool-spec=\"machine-type=n1-standard-4,replica-count=1,accelerator-type=NVIDIA_TESLA_V100,executor-image-uri=$BASE_GPU_IMAGE,local-package-path=.,python-module=trainer.task\"... I end up with a Docker image that is roughly 18GB (!) and takes a very long time to upload to the GCP registry.Granted the base image is around 6.5GB but where do the additional >10GB come from? Is there a way for me to avoid incurring the added size increase?Please note that my job loads the training data using the `datasets` Python package at run time and AFAIK does not include it in the auto-packaged docker image. Thanks,\nurig",
        "Challenge_closed_time":1650182580000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645930620000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Training-Auto-packaged-Custom-Training-Job-Yields-Very\/td-p\/397685\/jump-to\/first-unread-message",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.3,
        "Challenge_reading_time":16.75,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1181.1,
        "Challenge_title":"Vertex AI Training: Auto-packaged Custom Training Job Yields Very Large Docker Image",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":441.0,
        "Challenge_word_count":155,
        "Platform":"Tool-specific",
        "Solution_body":"Hello Ismail,\n\n\u00a0\n\nThank you for your help.\n\nI've checked and to the best of my knowledge there are no data or log files being picked up into my custom docker image.\n\nAccording to an answer that I've received on stackoverflow.com, it's likely that the 18GB size that I'm seeing is the size of my image after extraction. Apparently the ~6.8GB size is for the image compressed.\n\n\u00a0\n\nCheers,\n\n@urig\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"receiv stackoverflow com size size imag extract size compress imag",
        "Solution_link_count":0.0,
        "Solution_original_content":"ismail knowledg data log file pick docker imag accord receiv stackoverflow com size see size imag extract appar size imag compress cheer urig origin",
        "Solution_preprocessed_content":"ismail knowledg data log file pick docker imag accord receiv size see size imag extract appar size imag compress cheer origin",
        "Solution_readability":5.0,
        "Solution_reading_time":5.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":73.0,
        "Tool":"Vertex AI"
    },
    {
        "Challenge_adjusted_solved_time":113.09082,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi Dears,\n\nI am building ML model using DeepAR Algorithm.\n\nI faced this error while i reached to this point : Error :\n\nClientError: An error occurred (UnknownOperationException) when calling the CreateHyperParameterTuningJob operation: The requested operation is not supported in the called region.\n\nCode: from sagemaker.tuner import ( IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner, ) from sagemaker import image_uris\n\ncontainer = image_uris.retrieve(region= 'af-south-1', framework=\"forecasting-deepar\")\n\ndeepar = sagemaker.estimator.Estimator( container, role, instance_count=1, instance_type=\"ml.m5.2xlarge\", use_spot_instances=True, # use spot instances max_run=1800, # max training time in seconds max_wait=1800, # seconds to wait for spot instance output_path=\"s3:\/\/{}\/{}\".format(bucket, output_path), sagemaker_session=sess, ) freq = \"D\" context_length = 300\n\ndeepar.set_hyperparameters( time_freq=freq, context_length=str(context_length), prediction_length=str(prediction_length) )\n\nCan you please help in solving the error? I have to do that in af-south-1 region.\n\nThanks Basem\n\nhyperparameter_ranges = { \"mini_batch_size\": IntegerParameter(100, 400), \"epochs\": IntegerParameter(200, 400), \"num_cells\": IntegerParameter(30, 100), \"likelihood\": CategoricalParameter([\"negative-binomial\", \"student-T\"]), \"learning_rate\": ContinuousParameter(0.0001, 0.1), }\n\nobjective_metric_name = \"test:RMSE\"\n\ntuner = HyperparameterTuner( deepar, objective_metric_name, hyperparameter_ranges, max_jobs=10, strategy=\"Bayesian\", objective_type=\"Minimize\", max_parallel_jobs=10, early_stopping_type=\"Auto\", )\n\ns3_input_train = sagemaker.inputs.TrainingInput( s3_data=\"s3:\/\/{}\/{}\/train\/\".format(bucket, prefix), content_type=\"json\" ) s3_input_test = sagemaker.inputs.TrainingInput( s3_data=\"s3:\/\/{}\/{}\/test\/\".format(bucket, prefix), content_type=\"json\" )\n\ntuner.fit({\"train\": s3_input_train, \"test\": s3_input_test}, include_cls_metadata=False) tuner.wait()",
        "Challenge_closed_time":1653062956983,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652655830031,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUkAwy2tG8QreIWmLTGIUAqg\/client-error-an-error-occurred-unknown-operation-exception-when-calling-the-create-hyper-parameter-tuning-job-operation-the-requested-operation-is-not-supported-in-the-called-region",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":21.7,
        "Challenge_reading_time":28.66,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":113.09082,
        "Challenge_title":"ClientError: An error occurred (UnknownOperationException) when calling the CreateHyperParameterTuningJob operation: The requested operation is not supported in the called region.",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":135.0,
        "Challenge_word_count":172,
        "Platform":"Tool-specific",
        "Solution_body":"The error message indicates that CreateHyperParameterTuningJob operation is not supported in the region you're currently using. If possible, try the notebook in a region that supports HPO jobs.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"notebook region hyperparamet optim hpo job createhyperparametertuningjob oper region",
        "Solution_link_count":0.0,
        "Solution_original_content":"messag createhyperparametertuningjob oper region notebook region hpo job",
        "Solution_preprocessed_content":"messag createhyperparametertuningjob oper region notebook region hpo job",
        "Solution_readability":13.5,
        "Solution_reading_time":2.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":31.5388561111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am finetuning multiple models using for loop as follows.<\/p>\n<pre><code class=\"lang-auto\">for file in os.listdir(args.data_dir):\n    finetune(args, file)\n<\/code><\/pre>\n<p>BUT <code>wandb<\/code> shows logs only for the first file in <code>data_dir<\/code> although it is training and saving models for other files. It feels very strange behavior.<\/p>\n<pre><code class=\"lang-auto\">wandb: Synced bertweet-base-finetuned-file1: https:\/\/wandb.ai\/***\/huggingface\/runs\/***\n<\/code><\/pre>\n<p>This is a small snippet of <strong>finetuning<\/strong> code with Huggingface:<\/p>\n<pre><code class=\"lang-auto\">def finetune(args, file):\n    training_args = TrainingArguments(\n        output_dir=f'{model_name}-finetuned-{file}',\n        overwrite_output_dir=True,\n        evaluation_strategy='no',\n        num_train_epochs=args.epochs,\n        learning_rate=args.lr,\n        weight_decay=args.decay,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        fp16=True, # mixed-precision training to boost speed\n        save_strategy='no',\n        seed=args.seed,\n        dataloader_num_workers=4,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset['train'],\n        eval_dataset=None,\n        data_collator=data_collator,\n    )\n    trainer.train()\n    trainer.save_model()\n<\/code><\/pre>",
        "Challenge_closed_time":1650552651672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650439111790,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-for-huggingface-trainer-saves-only-first-model\/2270",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":17.1,
        "Challenge_reading_time":17.33,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":31.5388561111,
        "Challenge_title":"Wandb for Huggingface Trainer saves only first model",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":166.0,
        "Challenge_word_count":100,
        "Platform":"Tool-specific",
        "Solution_body":"<p><code>wandb.init(reinit=True)<\/code> and <code>run.finish()<\/code> helped me to log the models <strong>separately<\/strong> on wandb website.<\/p>\n<p>The working code looks like below:<\/p>\n<pre><code class=\"lang-auto\">\nfor file in os.listdir(args.data_dir):\n    finetune(args, file)\n\nimport wandb\ndef finetune(args, file):\n    run = wandb.init(reinit=True)\n    ...\n    run.finish()\n<\/code><\/pre>\n<p>Reference: <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script\" class=\"inline-onebox\">Launch Experiments with wandb.init - Documentation<\/a><\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"log model separ websit init reinit run finish function modifi function finetun function loop file data directori train save model file separ",
        "Solution_link_count":1.0,
        "Solution_original_content":"init reinit run finish log model separ websit file listdir arg data dir finetun arg file import finetun arg file run init reinit run finish launch init document",
        "Solution_preprocessed_content":"log model separ websit launch init document",
        "Solution_readability":14.4,
        "Solution_reading_time":7.73,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":0.2697222222,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer wants to use SageMaker, but doesn't know how to get started with instance sizes or how to forecast the cost for it. I've looked at the SageMaker TCO PDF we have online, but that appears more marketing than helpful, i.e. more price comparison than guidance.\n\nI know that the SageMaker cost is really the underlying EC2 and storage pieces, not SageMaker itself. However, I feel it is incorrect to say that they start with (say) t3.medium and see if that fits and scale up if they need more power behind it. As well, that doesn't help them to forecast either.\n\nAny thoughts here?",
        "Challenge_closed_time":1603286522000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603285551000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUq-Kaj1bLStK6Bs2gCUZ1Iw\/where-can-i-find-guidance-for-getting-a-customer-started-with-sage-maker-sizing-and-cost",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":8.17,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.2697222222,
        "Challenge_title":"Where can I find guidance for getting a customer started with SageMaker sizing and cost?",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":33.0,
        "Challenge_word_count":119,
        "Platform":"Tool-specific",
        "Solution_body":"See the performance efficiency and cost optimization pillars in Machine Learning Lens. Additionally this is an EC2 based right sizing best practices guide.\nOverall, it's better to start small, then increase instance size as needed (as those that start large, never bother reduce the size), or apply auto scaling for SageMaker hosting.\nAssuming a CPU ML predictions: When choosing ml.t2.medium instances the customer will need to keep an eye on the instance CPU credits. If they lack the knowledge, just start with M5.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"perform effici cost optim pillar len base size practic guid start small instanc size scale appli auto scale host choos instanc size base type predict cpu predict start instanc ey cpu credit medium instanc",
        "Solution_link_count":0.0,
        "Solution_original_content":"perform effici cost optim pillar len base size practic guid overal start small increas instanc size start larg bother reduc size appli auto scale host cpu predict choos medium instanc ey instanc cpu credit lack knowledg start",
        "Solution_preprocessed_content":"perform effici cost optim pillar len base size practic guid overal start small increas instanc size appli auto scale host cpu predict choos instanc ey instanc cpu credit lack knowledg start",
        "Solution_readability":7.5,
        "Solution_reading_time":6.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":83.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":3.7363888889,
        "Challenge_answer_count":1,
        "Challenge_body":"What value should I set for the directory_path parameter in FileSystemInput for the Amazon SageMaker SDK?\n\nHere is some information about my Amazon FSx for Lustre file system:\n\nMy FSx ID is fs-0684xxxxxxxxxxx.\nMy FSx has the mount name lhskdbmv.\nThe FSx maps to an Amazon S3 bucket with files (without extra prefixes in their keys)\n\nMy attempts to describe the job and the results are the following:\n\nAttempt 1:\n\nfs = FileSystemInput(\n    file_system_id='fs-0684xxxxxxxxxxx',\n    file_system_type='FSxLustre',\n    directory_path='lhskdbmv',\n    file_system_access_mode='ro')\n\n\nResult:\n\nestimator.fit(fs) returns ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: FileSystem DirectoryPath 'lhskdbmv' for channel 'training' is not absolute or normalized. Please ensure you don't have a trailing \"\/\", and\/or \"..\", \".\", \"\/\/\" in the path.\n\nAttempt 2:\n\nfs = FileSystemInput(\n    file_system_id='fs-0684xxxxxxxxxxx',\n    file_system_type='FSxLustre',\n    directory_path='\/',\n    file_system_access_mode='ro')\n\n\nResult:\n\nClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: The directory path for FSx Lustre file system fs-068406952bf758bac is invalid. The directory path must begin with mount name of the file system.\n\nAttempt 3:\n\nfs = FileSystemInput(\n    file_system_id='fs-0684xxxxxxxxxxx',\n    file_system_type='FSxLustre',\n    directory_path='fsx',\n    file_system_access_mode='ro')\n\n\nResult:\n\nClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: FileSystem DirectoryPath 'fsx' for channel 'training' is not absolute or normalized. Please ensure you don't have a trailing \"\/\", and\/or \"..\", \".\", \"\/\/\" in the path.",
        "Challenge_closed_time":1605296508000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605283057000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUHaScKqcfRu-aZ1Cwza63NQ\/what-value-should-i-set-for-directory-path-for-the-amazon-sage-maker-sdk-with-f-sx-as-data-source",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.1,
        "Challenge_reading_time":22.93,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":3.7363888889,
        "Challenge_title":"What value should I set for directory_path for the Amazon SageMaker SDK with FSx as data source?",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":120.0,
        "Challenge_word_count":210,
        "Platform":"Tool-specific",
        "Solution_body":"The directory_path parameter must point to \/mountname\/path\/to\/specific\/folder\/in-file-system. The value of mountname is returned in the CreateFileSystem API operation response. It is also returned in the response of the describe-file-systems AWS Command Line Interface (AWS CLI) command and the DescribeFileSystems API operation.\n\nFor your use case, the response might look similar to the following: mountName = lhskdbmv",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"directori path paramet set mountnam path folder file mountnam valu return createfilesystem api oper respons respons file system cli describefilesystem api oper",
        "Solution_link_count":0.0,
        "Solution_original_content":"directori path paramet mountnam path folder file valu mountnam return createfilesystem api oper respons return respons file system line interfac cli describefilesystem api oper respons mountnam lhskdbmv",
        "Solution_preprocessed_content":"paramet valu mountnam return createfilesystem api oper respons return respons line interfac describefilesystem api oper respons mountnam lhskdbmv",
        "Solution_readability":11.0,
        "Solution_reading_time":5.35,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":11.6668433334,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi :\n\nI am planing to use k-means to form algorithm to do project. However, I am aware that there are certain shortcomings to find the optimal groups using k-means.\n\nCould you please tell the limitation and provide me with a detailed example?\n\nThanks",
        "Challenge_closed_time":1647898847903,
        "Challenge_comment_count":1,
        "Challenge_created_time":1647856847267,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/780362\/machine-learning-algorithms-questions.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":6.6,
        "Challenge_reading_time":3.51,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":11.6668433334,
        "Challenge_title":"machine learning algorithms questions",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":46,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @hideonbush again,\n\nGenerally to think about k-means, please refer to below cons and pros. If you can provide more details and how you want to develop your project, I can share more:\n\nPros:\n\nK-means is very simple, highly flexible, and efficient.\n\n\nEasy to adjust and interpret the clustering results. Easy to explain the results in contrast to Neural Networks.\n\n\nThe efficiency of k-means implies that the algorithm is good at segmenting a dataset.\n\n\nAn instance can change cluster (move to another cluster) when the centroids are recomputed\n\nCons\n\nIt does not allow to develop the most optimal set of clusters and the number of clusters must be decided before the analysis. How many clusters to include is left at the discretion of the researcher. This involves a combination of common sense, domain knowledge, and statistical tools. Too many clusters tell you nothing because of the groups becoming very small and there are too many of them.\n\n\nWhen doing the analysis, the k-means algorithm will randomly select several different places from which to develop clusters. This can be good or bad depending on where the algorithm chooses to begin at. From there, the center of the clusters is recalculated until an adequate \"center'' is found for the number of clusters requested.\n\n\nThe order of the data input has an impact on the final results.\n\nHope this helps!\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful, thanks.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"adjust interpret cluster easili algorithm segment dataset instanc cluster centroid recomput limit allow optim set cluster cluster decid analysi order data input impact final",
        "Solution_link_count":0.0,
        "Solution_original_content":"hideonbush gener con pro share pro highli flexibl effici adjust interpret cluster explain contrast neural network effici algorithm segment dataset instanc cluster cluster centroid recomput con allow optim set cluster cluster decid analysi cluster left discret research involv combin common sens domain knowledg statist cluster group small analysi algorithm randomli select cluster depend algorithm choos begin center cluster recalcul adequ center cluster request order data input impact final hope yutong kindli accept",
        "Solution_preprocessed_content":"gener con pro share pro highli flexibl effici adjust interpret cluster explain contrast neural network effici algorithm segment dataset instanc cluster centroid recomput con allow optim set cluster cluster decid analysi cluster left discret research involv combin common sens domain knowledg statist cluster group small analysi algorithm randomli select cluster depend algorithm choos begin center cluster recalcul adequ center cluster request order data input impact final hope yutong kindli accept",
        "Solution_readability":8.3,
        "Solution_reading_time":17.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":239.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":1.0150825,
        "Challenge_answer_count":1,
        "Challenge_body":"This error message is super confusing, what does it mean?",
        "Challenge_closed_time":1661980770760,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661977116463,
        "Challenge_favorite_count":11.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/989409\/descriptors-cannot-not-be-created.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":1.15,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":1.0150825,
        "Challenge_title":"Descriptors cannot not be created",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":14,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @matsuoka-4412\n\nThanks for using Microsoft Q&A platform. This problem is caused by breaking changes introduced in protobuf 4.0.0. For more information, see https:\/\/developers.google.com\/protocol-buffers\/docs\/news\/2022-05-06#python-updates.\n\nPlease refer to this troubleshooting guidance - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-protobuf-descriptor-error\n\nI hope it helps.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"break introduc protobuf troubleshoot guidanc link http doc com troubleshoot protobuf descriptor",
        "Solution_link_count":2.0,
        "Solution_original_content":"matsuoka platform break introduc protobuf http com protocol buffer doc updat troubleshoot guidanc http doc com troubleshoot protobuf descriptor hope yutong kindli accept commun",
        "Solution_preprocessed_content":"platform break introduc protobuf troubleshoot guidanc hope yutong kindli accept commun",
        "Solution_readability":12.6,
        "Solution_reading_time":6.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":53.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":673.4043516667,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to train and deploy multiple comprehend custom classifiers (for example 50 models). I want to be able to classify my documents in near real-time (a couple of seconds are fine) 24\/7. The problem is that deploying one end-point for each classifier is very expensive, especially that one or two IU would be enough for all my models combined (I am expecting to process around 10 document a minute total\/length of one document is around 1000 characters ). Is there a way where I can deploy multiple models behind the same endpoint (similar to the multi-model endpoint in SageMaker)? Or maybe do an asynchronous approach and somewho make sure I get the response within seconds?",
        "Challenge_closed_time":1666640051311,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664215795645,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUEQiFVzOhR5q1XYhjlltO7w\/deploying-multiple-comprehend-custom-classifiers-multi-label-mode",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":9.18,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":673.4043516667,
        "Challenge_title":"Deploying multiple Comprehend Custom Classifiers (multi-label mode)",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":36.0,
        "Challenge_word_count":121,
        "Platform":"Tool-specific",
        "Solution_body":"No , Comprehend don't support hosting multiple models with the same endpoint right now. Thanks for your suggestions . We will take them into consideration .",
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"comprehend host multipl model endpoint consider",
        "Solution_preprocessed_content":"comprehend host multipl model endpoint consider",
        "Solution_readability":7.5,
        "Solution_reading_time":1.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":39.0455230556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello,<\/p>\n<p>As I cannot simply upload infinitely many weights using artifacts, I also want to store some locally.<br>\nFor naming, I would like to use the sweep id and\/or the run id.<\/p>\n<p>Can I access that somehow in the train function I hand over to the agent?<\/p>\n<p>Thanks<\/p>\n<p>Markus<\/p>",
        "Challenge_closed_time":1660860269032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660719705149,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/access-sweep-id-and-run-id-within-train-function-for-local-weight-storage\/2948",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":5.5,
        "Challenge_reading_time":4.66,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":39.0455230556,
        "Challenge_title":"Access sweep_id and run_id within train() function for local weight storage",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":113.0,
        "Challenge_word_count":59,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a>!<\/p>\n<p>The <code>wandb.Run<\/code> object that is returned from <code>wandb.init<\/code> contains this information as properties. You should be able to access <code>run.id<\/code> and <code>run.sweep_id<\/code> in the train function after calling <code>run = wandb.init(...)<\/code>.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"access sweep run properti run object return init access run run sweep train function call run init",
        "Solution_link_count":0.0,
        "Solution_original_content":"markuskarn run object return init properti access run run sweep train function call run init ramit",
        "Solution_preprocessed_content":"object return properti access train function call ramit",
        "Solution_readability":8.6,
        "Solution_reading_time":4.98,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":36.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":2.5933255556,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I was wondering if anybody from the W&amp;B team can confirm that there is an outage at the moment.<\/p>\n<p>I\u2019ve been having issues starting runs and it seems like other folks are having issues syncing runs with a network time out error (<a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/4424\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">[CLI]: canno't sync my runs \u00b7 Issue #4424 \u00b7 wandb\/wandb \u00b7 GitHub<\/a>). It\u2019s been ongoing for about 2 hours now.<\/p>\n<p>The status page is saying everything is fine - <a href=\"https:\/\/status.wandb.com\" rel=\"noopener nofollow ugc\">https:\/\/status.wandb.com<\/a><\/p>\n<p>All the best,<br>\nAlexey<\/p>",
        "Challenge_closed_time":1667343090214,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667333754242,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/w-b-outage-11-1-2022\/3360",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":8.0,
        "Challenge_reading_time":8.59,
        "Challenge_score_count":3.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2.5933255556,
        "Challenge_title":"W&B Outage? 11\/1\/2022",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":90.0,
        "Challenge_word_count":84,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Thank you for your patience! Our engineers were able to push a fix for this. There\u2019s still currently an issue regarding batch moving runs, but for the most part this issue has been resolved.<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"team push network time experienc start sync run minor batch move run",
        "Solution_link_count":0.0,
        "Solution_original_content":"patienc engin push there batch move run",
        "Solution_preprocessed_content":"patienc engin push there batch move run",
        "Solution_readability":4.2,
        "Solution_reading_time":2.41,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":41.9026855556,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>Some of our teams projects were erroneously deleted by a colleague since he wanted to clean up his account and probably thought it were his own projects. Is there any chance to get the data back (I assume not, but worth a try\u2026)?<\/p>",
        "Challenge_closed_time":1642591492148,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642440642480,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/a-colleague-accidentally-deleted-some-projects-any-chance-to-get-them-back\/1774",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":6.8,
        "Challenge_reading_time":3.8,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":41.9026855556,
        "Challenge_title":"A colleague accidentally deleted some projects, any chance to get them back?",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":138.0,
        "Challenge_word_count":54,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Please check the names of the project. By username I meant the entity name where the projects were logged. If these were team projects then I need the team name.<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"usernam meant entiti log team team",
        "Solution_preprocessed_content":"usernam meant entiti log team team",
        "Solution_readability":1.3,
        "Solution_reading_time":2.04,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":17.7779194445,
        "Challenge_answer_count":3,
        "Challenge_body":"Bonjour\nJe suis le cours en ligne concernant l'impl\u00e9mentation d'algorithmes de machine learning\nA l'\u00e9tape Create compute resources\nhttps:\/\/docs.microsoft.com\/en-us\/learn\/modules\/use-automated-machine-learning\/create-compute\n\nOn me demande Search for and select Standard_DS11_v2\n\nHors, l'interface me dit que je n'ai pas les quotas disponibles.\nJ'utilise l'offre d'essai \u00e0 200 USD.\nComment faire pour que cela fonctionne ?\nCordialement\nThibaut",
        "Challenge_closed_time":1638432598510,
        "Challenge_comment_count":1,
        "Challenge_created_time":1638368598000,
        "Challenge_favorite_count":12.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/647767\/comment-selectionner-standard-ds11-v2.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":12.2,
        "Challenge_reading_time":6.23,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":17.7779194445,
        "Challenge_title":"Comment s\u00e9lectionner Standard_DS11_v2",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":54,
        "Platform":"Tool-specific",
        "Solution_body":"@ThibautJacquin-3972 For a free account only 200$ credit is available and not all compute can be created or selected because of this limitation. You can choose a lower priced VM and proceed with the creation of compute or upgrade to a pay-as-you-go account for your subscription and select the required compute type. I hope this helps.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"choos lower price proce creation comput upgrad pai account subscript select comput type bias summari",
        "Solution_link_count":0.0,
        "Solution_original_content":"thibautjacquin free account credit comput creat select limit choos lower price proce creation comput upgrad pai account subscript select comput type hope click upvot commun member read thread",
        "Solution_preprocessed_content":"free account credit comput creat select limit choos lower price proce creation comput upgrad account subscript select comput type hope click upvot commun member read thread",
        "Solution_readability":9.4,
        "Solution_reading_time":5.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":214.3601405556,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\n\nI am running an AutoML experiment for a regression task, and looking at the YAML file which is generated it seems that TensorFlowLinearRegressor and TensorFlowDNN models are listed as both 'supported_models' and 'blacklist_algos'.\n\nI tried to deactivate the automatic blacklisting of models by specifying the parameter 'auto_blacklist' to False, and 'blacklist_models' and 'blacklist_algos' parameters to Null, but it doesn't change anything.\n\n automl_settings = {\n     \"primary_metric\": 'normalized_mean_absolute_error',\n     \"featurization\": 'auto',\n     \"verbosity\": logging.INFO,\n     \"n_cross_validations\": 5,\n     \"auto_blacklist\": False,\n     \"blacklist_models\": None,\n     \"blacklist_algos\": None\n }\n run = experiment.submit(automl_config, show_output=True)\n\n\n\nThe generated YAML file (excerpt):\n\n \"whitelist_models\":null,\n \"blacklist_algos\":[\"TensorFlowDNN\",\"TensorFlowLinearRegressor\"],\n \"supported_models\":[\"ElasticNet\",\"GradientBoosting\",\"LightGBM\",\"TensorFlowLinearRegressor\",\"TensorFlowDNN\",\"LassoLars\",\"DecisionTree\",\"RandomForest\",\"FastLinearRegressor\",\"OnlineGradientDescentRegressor\",\"ExtremeRandomTrees\",\"TabnetRegressor\",\"XGBoostRegressor\",\"KNN\",\"SGD\"],\n \"private_models\":[],\n \"auto_blacklist\":false\n\n\n\nMaybe the problem comes from the fact that Deep learning is set to 'Disabled' in the configuration settings, as shown on the following picture:\n\nAre deep learning models not supported anymore by AutoML?",
        "Challenge_closed_time":1654236226443,
        "Challenge_comment_count":3,
        "Challenge_created_time":1653464529937,
        "Challenge_favorite_count":11.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/863297\/automl-tensorflowdnn-and-tensorflowlinearregressor.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":23.6,
        "Challenge_reading_time":19.46,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":214.3601405556,
        "Challenge_title":"AutoML : TensorFlowDNN and TensorFlowLinearRegressor are blacklisted by default",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":130,
        "Platform":"Tool-specific",
        "Solution_body":"@ThierryL-3166 Thanks for the question.\n\nAs mentioned in the below document The following support models in AutoML TensorFlowDNN, TensorFlowLinearRegressor are deprecated.\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-automl-core\/azureml.automl.core.shared.constants.supportedmodels.regression?view=azure-ml-py",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"tensorflowdnn tensorflowlinearregressor model deprec automl longer model regress task",
        "Solution_link_count":1.0,
        "Solution_original_content":"thierryl document model automl tensorflowdnn tensorflowlinearregressor deprec http doc com api automl core automl core share constant supportedmodel regress",
        "Solution_preprocessed_content":"document model automl tensorflowdnn tensorflowlinearregressor deprec",
        "Solution_readability":25.3,
        "Solution_reading_time":4.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":12.2930863889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\n\nWe are sending data from IoT Central to Event Hubs and then to Data Explorer, with the hopes of then sending the data to Azure Machine Learning.\n\nIn order to send data from Event Hubs to Data Explorer it needs a data ingestion into a table on data explorer.\n\nFor this data ingestion, it needs a json mapping.\n\nWe could ingest the data, but the message from the iot central data goes to event hubs that goes to data explorer carries the telemetry data as a dynamic type (a json inside a json).\n\n (\"telemetry\":{\"Temp:\"37\",\"Vol\":\"97\"})\n\n\n\n\nWe want to separate the telemetry data in different columns.\n\nSo Temp will have one column and Vol another.\n\nI am wondering how that can be done?\n\nAnd additionally, since we would like to send the data to ML, can data explorer be used as a datastore in ML?\n\nThanks!!",
        "Challenge_closed_time":1619080604288,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619036349177,
        "Challenge_favorite_count":12.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/366532\/separate-data-in-data-explorer-and-use-as-datastor.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":10.24,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":12.2930863889,
        "Challenge_title":"Separate data in data explorer and use as datastore",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":153,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @yjay-4307,\n\nYou can use parse operator - Evaluates a string expression and parses its value into one or more calculated columns. The calculated columns will have nulls, for unsuccessfully parsed strings.\n\nFor more details, refer SO thread addressing similar issue.\n\nUnfortuantely, Azure Data Explorer is not a supported storage solution with Azure Machine Learning.\n\nDatastores currently support storing connection information to the storage services listed in the following matrix.\n\nFor unsupported storage solutions, and to save data egress cost during ML experiments, move your data to a supported Azure storage solution.\n\nReference: Connect to storage services on Azure - Azure Machine Learning.\n\nHope this helps. Do let us know if you any further queries.\n\nPlease don\u2019t forget to Accept Answer and Up-Vote wherever the information provided helps you, this can be beneficial to other community members.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"pars oper separ telemetri data column data explor data explor storag data storag",
        "Solution_link_count":0.0,
        "Solution_original_content":"yjai pars oper evalu express pars valu calcul column calcul column null unsuccessfulli pars thread address unfortuant data explor storag datastor store connect storag servic list matrix unsupport storag save data egress cost data storag connect storag servic hope queri forget accept vote benefici commun member",
        "Solution_preprocessed_content":"pars oper evalu express pars valu calcul column calcul column null unsuccessfulli pars thread address unfortuant data explor storag datastor store connect storag servic list matrix unsupport storag save data egress cost data storag connect storag servic hope queri forget accept benefici commun member",
        "Solution_readability":10.9,
        "Solution_reading_time":11.3,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":135.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":75.2,
        "Challenge_answer_count":4,
        "Challenge_body":"Hi, I'm using Google Colab +pro and unfortunately I`m getting several Ram calls and have not been able to move forward or train some modelsWhich is the next tool that I should get in order to be able to run the Google Colab models without the Ram calls?Should I get a Google Compute Engine and try to connect the google colab files to it?Should I up load the model to vertex AI?What characteristics should I need to take into consideration before I select any of the different tools?",
        "Challenge_closed_time":1652442120000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652171400000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Next-Step-from-Google-Colab-Pro\/td-p\/421797\/jump-to\/first-unread-message",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":8.2,
        "Challenge_reading_time":6.23,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":75.2,
        "Challenge_title":"Next Step from Google Colab +Pro",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":231.0,
        "Challenge_word_count":93,
        "Platform":"Tool-specific",
        "Solution_body":"Hello,\n\nI have provided a few links to help you through configuring your Google Colab Model.\n\nThis link below contains all Google Colab related questions on Stack Overflow:\n\nhttps:\/\/stackoverflow.com\/search?q=colab&s=7e8e7982-76a3-4765-8bad-63af4a9415fb\n\nThe following link explains how to double the Ram in Google Colab:\n\nhttps:\/\/towardsdatascience.com\/double-your-google-colab-ram-in-10-seconds-using-these-10-characters-...\n\nThe last link is a HOW-TO guide:\n\nhttps:\/\/neptune.ai\/blog\/how-to-use-google-colab-for-deep-learning-complete-tutorial#:~:text=Open%20a....\n\nRegards\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"configur colab model doubl ram stack overflow colab relat guid comput engin",
        "Solution_link_count":3.0,
        "Solution_original_content":"link configur colab model link colab relat stack overflow http stackoverflow com search colab afafb link explain doubl ram colab http towardsdatasci com doubl colab ram charact link guid http blog colab complet tutori text open origin",
        "Solution_preprocessed_content":"link configur colab model link colab relat stack overflow link explain doubl ram colab link guid origin",
        "Solution_readability":14.7,
        "Solution_reading_time":7.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":56.0,
        "Tool":"Vertex AI"
    },
    {
        "Challenge_adjusted_solved_time":249.1975413889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\n\nI would like to know if there is the possibility to use a custom environment (created from the AML portal) for the execution of a Python Script Step in the Azure Machine Learning Designer (only using the designer, not using azureml sdk to publish the pipeline from the code).\n\nThanks,\nG",
        "Challenge_closed_time":1648494342676,
        "Challenge_comment_count":2,
        "Challenge_created_time":1647597231527,
        "Challenge_favorite_count":9.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/777745\/use-custom-environment-in-azure-machine-learning-d.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":14.8,
        "Challenge_reading_time":4.29,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":249.1975413889,
        "Challenge_title":"Use custom environment in Azure Machine Learning Designer",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":59,
        "Platform":"Tool-specific",
        "Solution_body":"Thanks for your feedback. Based on your comments above, it seems you want to configure a custom environment in AML designer and install unsupported python libraries. These are the supported Custom Environments. However, in AML designer, the execute python script component enables you to write custom python scripts and install python libraries. This particular document shows how to configure execute python script. You can install packages that aren't in the preinstalled list by using the following command:\n\n import os\n os.system(f\"pip install scikit-misc\")\n\n\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"environ creat portal execut step design sdk publish pipelin execut compon aml design allow write instal librari instal packag preinstal list",
        "Solution_link_count":0.0,
        "Solution_original_content":"feedback base comment configur environ aml design instal unsupport librari environ aml design execut compon enabl write instal librari document configur execut instal packag aren preinstal list import pip instal scikit misc kindli accept",
        "Solution_preprocessed_content":"feedback base comment configur environ aml design instal unsupport librari environ aml design execut compon enabl write instal librari document configur execut instal packag aren preinstal list import instal kindli accept",
        "Solution_readability":9.5,
        "Solution_reading_time":7.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":2.9180447222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I\u2019m soon going to start implementing W&amp;B for my neural network\u2019s hyperparameter tuning. This is in preparation for an academic paper I\u2019m writing on the subject. The software seems very pragmatic and well-polished, so I\u2019m quite excited to get started.<\/p>\n<p>Its visualizations in particular seem to be of a very high quality. Some present sophisticated functionality that other experiment trackers can\u2019t touch. With proper citation, can these be included for publication?<\/p>",
        "Challenge_closed_time":1638466847308,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638456342347,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/publishing-graphs-visualizations\/1457",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":11.5,
        "Challenge_reading_time":6.52,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2.9180447222,
        "Challenge_title":"Publishing Graphs\/Visualizations",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":196.0,
        "Challenge_word_count":72,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hi Logan,<\/p>\n<p>I\u2019m so happy you\u2019re excited to use our product! Our engineers have worked very hard in order to get it to where it is today. We would love for you to use our graphs in your paper. We have a few examples of how to do so here (<a href=\"https:\/\/docs.wandb.ai\/company\/academics#cite-weights-and-biases\" class=\"inline-onebox-loading\">https:\/\/docs.wandb.ai\/company\/academics#cite-weights-and-biases<\/a>).<\/p>\n<p>Warmly,<br>\nLeslie<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"visual academ paper respons lesli graph paper link properli cite",
        "Solution_link_count":2.0,
        "Solution_original_content":"logan happi your engin order todai love graph paper http doc compani academ cite weight bias warmli lesli",
        "Solution_preprocessed_content":"logan happi your engin order todai love graph paper warmli lesli",
        "Solution_readability":9.9,
        "Solution_reading_time":5.79,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":136.9105555556,
        "Challenge_answer_count":1,
        "Challenge_body":"A data scientist is looking to host a Tensorflow model in SageMaker and process low volume streaming event data (~2-3 per second) to collect inferences about each event. Data scientist is looking at having the SageMaker inference model plugged in as a Kinesis Data Analytics Application but Kinesis Data Analytics currently only supports SQL or Flink.\n\nOne option to set up an ECS or Lambda service to consume data from Kinesis or SNS and invoke the SageMaker inference endpoint per message, but if there is a more automated and optimal solution available for these kind of workflows.\n\nIt is not possible to pass multiple requests currently to a SageMaker endpoint, yet Tensorflow models tend to perform much better on batches of data rather than multiple single invocations so some windowing would be beneficial. Ideally the client would want to react to an inference within 10-15 seconds of the event being processed so an S3 based batch approach is probably too slow.\n\nIs there anything you can recommend for handling this sort of workload?",
        "Challenge_closed_time":1600149573000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599656695000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU6E1eYARES123bRYAe2B0Ag\/automated-streaming-integration-and-multiple-requests-for-sage-maker-endpoint",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":13.76,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":136.9105555556,
        "Challenge_title":"Automated streaming integration and multiple requests for SageMaker endpoint",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":88.0,
        "Challenge_word_count":181,
        "Platform":"Tool-specific",
        "Solution_body":"To build integration between SageMaker endpoints and Kinesis Data Application use this blog - https:\/\/aws.amazon.com\/blogs\/architecture\/realtime-in-stream-inference-kinesis-sagemaker-flink\/. It help to setup serverless service to invoke the SageMaker inference endpoint.\n\nTo use batching. The Tensorflow documentation mentions the following:\n\nThis link mentions that you can include multiple instances in your predict request (or multiple examples in classify\/regress requests) to get multiple prediction results in one request to your Endpoint.\nThis link mentions that you can configure SageMaker TensorFlow Serving Container to batch multiple records together before performing an inference\n\nYou would still have to handle the logic internally in ECS\/Lambda to control how many records you consume from your stream in one batch, but at least you will be able to infer on the whole batch on the SageMaker endpoint end based on the above.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"build integr endpoint kinesi data blog set serverless servic invok infer endpoint batch multipl instanc predict request configur tensorflow serv batch multipl record perform infer logic intern ec lambda control record consum stream batch",
        "Solution_link_count":1.0,
        "Solution_original_content":"build integr endpoint kinesi data blog http com blog architectur realtim stream infer kinesi flink setup serverless servic invok infer endpoint batch tensorflow document link multipl instanc predict request multipl classifi regress request multipl predict request endpoint link configur tensorflow serv batch multipl record perform infer logic intern ec lambda control record consum stream batch infer batch endpoint end base",
        "Solution_preprocessed_content":"build integr endpoint kinesi data blog setup serverless servic invok infer endpoint batch tensorflow document link multipl instanc predict request multipl predict request endpoint link configur tensorflow serv batch multipl record perform infer logic intern control record consum stream batch infer batch endpoint end base",
        "Solution_readability":15.9,
        "Solution_reading_time":11.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":131.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":8.0685027778,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi;\n\nFirst off, where can I find the costs for all the different things I can run in Azure ML? Not just a compute, but editing a notebook, connecting to a datastore, splitting a datastore, etc. Basically where is the price list?\n\nSecond, where can I find what I will be charged for things I ran in the last hour? I want to see what I'm spending before a month is up and the charge is then 100x what I expected (and can afford).\n\nthanks - dave",
        "Challenge_closed_time":1634347958867,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634318912257,
        "Challenge_favorite_count":8.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/592299\/cost-of-running-a-compute-other-tasks.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":5.7,
        "Challenge_reading_time":5.67,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":8.0685027778,
        "Challenge_title":"Cost of running a compute, other tasks",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":92,
        "Platform":"Tool-specific",
        "Solution_body":"Hi, you can use Azure Cost Management to manage Azure costs, please review the quickstart document. Also, the following document provides detailed information on how to plan and manage cost for AML.\n\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"cost cost quickstart document document share plan cost comprehens price list task",
        "Solution_link_count":0.0,
        "Solution_original_content":"cost cost review quickstart document document plan cost aml kindli accept",
        "Solution_preprocessed_content":"cost cost review quickstart document document plan cost aml kindli accept",
        "Solution_readability":8.5,
        "Solution_reading_time":3.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":7.1856155556,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\n\nI created a notebook in the workspace and when I sent the experiment for training I received error message undefined symbol: XGDMatrixSetDenseInfo for algorithm Xgboost. Do you know how to fix the problem?\n\n\n\n\nAzure ML Version: 1.22.0\nCompute Instance: Standard_DS3_v2\n\nCode:\n\nimport logging\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.core.experiment import Experiment\n\nautoml_settings = {\n\"iteration_timeout_minutes\": 10,\n\"experiment_timeout_hours\": 0.3,\n\"enable_early_stopping\": True,\n\"primary_metric\": 'normalized_root_mean_squared_error',\n\"featurization\": 'auto',\n\"verbosity\": logging.INFO,\n\"n_cross_validations\": 5\n}\n\nautoml_config = AutoMLConfig(task='regression',\ndebug_log='automated_ml_errors.log',\ntraining_data=x_train,\nlabel_column_name=\"production_time\",\n**automl_settings)\n\nexperiment = Experiment(ws, \"train-model\")\nlocal_run = experiment.submit(automl_config, show_output=True)\n\nFull Error Message:\n\n\n\n\nERROR: FitException:\nMessage: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\nInnerException: AttributeError: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\nErrorResponse\n{\n\"error\": {\n\"code\": \"SystemError\",\n\"message\": \"Encountered an internal AutoML error. Error Message\/Code: FitException. Additional Info: FitException:\\n\\tMessage: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\\n\\tInnerException: None\\n\\tErrorResponse \\n{\\n \\\"error\\\": {\\n \\\"message\\\": \\\"\/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\\\",\\n \\\"target\\\": \\\"Xgboost\\\",\\n \\\"reference_code\\\": \\\"Xgboost\\\"\\n }\\n}\",\n\"details_uri\": \"https:\/\/docs.microsoft.com\/azure\/machine-learning\/resource-known-issues#automated-machine-learning\",\n\"target\": \"Xgboost\",\n\"inner_error\": {\n\"code\": \"ClientError\",\n\"inner_error\": {\n\"code\": \"AutoMLInternal\"\n}\n},\n\"reference_code\": \"Xgboost\"\n}\n}\n\n\n\n\n\nBest regards,\nCristina\n\n\n\n\n\n\n\n\n79658-packages.txt",
        "Challenge_closed_time":1616196599996,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616170731780,
        "Challenge_favorite_count":7.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/322886\/azure-machine-learning-services-automl-error-runni.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":24.2,
        "Challenge_reading_time":28.75,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":7.1856155556,
        "Challenge_title":"Azure Machine Learning Services - AutoMl - Error running experiment.submit: \"\/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\"",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":156,
        "Platform":"Tool-specific",
        "Solution_body":"Hi, can you try uninstalling and reinstalling Xgboost (try versions <= 0.90 if you continue to get errors).",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"uninstal reinstal persist version equal",
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":5.4,
        "Solution_reading_time":1.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":110.2481841667,
        "Challenge_answer_count":1,
        "Challenge_body":"I\u2019m having an issue importing the data into my Machine Learning Studio. It shows me a Red Cross with the error 0030 - which means that there\u2019s an issue in downloading the data. For background, I\u2019m importing data from the Web URL via HTTP option. I looked up the issue on the troubleshooting page, followed the advice, which shows I\u2019ve done everything correctly. My data link works perfectly fine in my browser. When I enter the http link into my browser, it immediately downloads the csv file. However, my studio is not downloading the data. Importing the data is the first step in my experiment, and I can\u2019t move forward without it. Immediate help would be greatly appreciated! I\u2019ve attached pictures for reference. [1]: \/answers\/storage\/attachments\/72499-0ebb78a4-4805-46e8-a7f1-fbf99682af5f.png",
        "Challenge_closed_time":1614759574360,
        "Challenge_comment_count":2,
        "Challenge_created_time":1614362680897,
        "Challenge_favorite_count":6.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/291213\/importing-data-in-an-experiment-issue.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":10.46,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":110.2481841667,
        "Challenge_title":"Importing Data in Azure ML Studio Experiment",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":130,
        "Platform":"Tool-specific",
        "Solution_body":"Hello,\n\nThis exception in Azure Machine Learning occurs when it is not possible to download a file. You will receive this exception when an attempted read from an HTTP source has failed after three (3) retry attempts.\n\nResolution: Verify that the URI to the HTTP source is correct and that the site is currently accessible via the Internet.\n\nIs this file on any place need authentication?\n\nRegards,\nYutong",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"verifi uri http sourc site access internet file authent access bias summari",
        "Solution_link_count":0.0,
        "Solution_original_content":"except download file receiv except read http sourc retri resolut verifi uri http sourc site access internet file authent yutong",
        "Solution_preprocessed_content":"except download file receiv except read http sourc retri resolut verifi uri http sourc site access internet file authent yutong",
        "Solution_readability":8.7,
        "Solution_reading_time":4.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":68.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":9.7155897222,
        "Challenge_answer_count":2,
        "Challenge_body":"I'm trying to follow the steps given here - https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/explore-analyze-data-with-python\/2-exercise-explore-data\n\nI've tried regions east us2 and east us for creating the instance but it fails after taking more than half an hour. I tried virtual machine sizes - Standard_DS11_v2 & Standard_DS3_v2.\n\nAny help would be appreciated.\n\nEdit - I don't have any other instances running in my subscription, so it should not be a quota issue. The error message says \"An internal server error occurred.\".",
        "Challenge_closed_time":1616282059876,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616247083753,
        "Challenge_favorite_count":6.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/323742\/unable-to-creata-a-compute-instance.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":9.9,
        "Challenge_reading_time":7.1,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":9.7155897222,
        "Challenge_title":"Unable to creata a compute instance",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":76,
        "Platform":"Tool-specific",
        "Solution_body":"Good day @AatishSuman-7641\n\nDid you read the comment in the compute page?\n\nPlease confirm that you are using an account which fit the limitations\n\nFor more information please check this post:\n\nhttps:\/\/azure.microsoft.com\/en-us\/blog\/update-2-on-microsoft-cloud-services-continuity\/\n\nNote: I followed the tutorial which you provided the link to and it is working well for me. Therefore, I assume the issue is related to the above comment.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"account fit limit comment comput page",
        "Solution_link_count":1.0,
        "Solution_original_content":"dai aatishsuman read comment comput page account fit limit http com blog updat cloud servic note tutori link relat comment",
        "Solution_preprocessed_content":"dai read comment comput page account fit limit note tutori link relat comment",
        "Solution_readability":9.3,
        "Solution_reading_time":5.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":5.6422222222,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a customer asking me about the Rendezvous architecture. What I'm thinking is, we could implement this in a number of ways, all using endpoint variants:\n\nLambda (and probably SQS) around the endpoint;\nA custom monitoring job;\nStep Functions\n\nWithout going into details of the above options or of how the evaluation and SLA check will be done, it looks like the several models would fit very well as variants of an endpoint. The thing is, the architecture expects to call them all. Is there a way to directly call all variants of a model, or will a wrapper to identify the variants, call them all and process the results be needed?",
        "Challenge_closed_time":1604506964000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604486652000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU6bm-EMtOQV6robgbTXClLQ\/running-a-request-against-all-variants-in-an-endpoint",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":8.34,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":5.6422222222,
        "Challenge_title":"Running a request against all variants in an endpoint",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":14.0,
        "Challenge_word_count":121,
        "Platform":"Tool-specific",
        "Solution_body":"When I last looked into it, it was not possible to query all versions\/variants of the model automatically. You can specify what variant to use when using the invoke_endpoint method. I would therefore write a lambda function to invoke each of the endpoints one-by-one (see here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html). To be especially rigorous about it, you can add a function in your lambda code that first retrieves all the endpoint variants (see here: https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.describe_endpoint) then queries them one-by-one, and returns all the results.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"write lambda function invok endpoint invok endpoint add function lambda retriev endpoint variant queri return",
        "Solution_link_count":2.0,
        "Solution_original_content":"queri version variant model automat specifi variant invok endpoint write lambda function invok endpoint http doc com latest apirefer api runtim invokeendpoint html rigor add function lambda retriev endpoint variant http boto amazonaw com document api latest servic html client endpoint queri return",
        "Solution_preprocessed_content":"queri model automat specifi variant write lambda function invok endpoint rigor add function lambda retriev endpoint variant queri return",
        "Solution_readability":14.3,
        "Solution_reading_time":8.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":81.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":8.3581641667,
        "Challenge_answer_count":2,
        "Challenge_body":"I have created and published a Azure ML pipeline. I want to trigger the ML pipeline from Azure Data Factory.\n\nIn ADF, i have chosen Machine learning execute pipeline and created the linked service to azure machine learning and able to choose the published pipeline endpoint. However while running, i am getting the below error. I couldn't find much information how to resolve the error.\n\n\"Convert Failed. The value type 'System.String', in key 'azureCloudType' is not expected type 'Microsoft.DataTransfer.Common.Models.AzureCloudType\"",
        "Challenge_closed_time":1645111953528,
        "Challenge_comment_count":10,
        "Challenge_created_time":1645081864137,
        "Challenge_favorite_count":30.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/739240\/trigger-azure-ml-pipeline-from-azure-data-factory.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":12,
        "Challenge_readability":8.7,
        "Challenge_reading_time":7.32,
        "Challenge_score_count":5.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":8.3581641667,
        "Challenge_title":"Trigger Azure ML Pipeline from Azure Data Factory",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":86,
        "Platform":"Tool-specific",
        "Solution_body":"Hi @VinothKumarK-8698 ,\nWelcome to Microsoft Q&A platform and thankyou for posting your query.\nAs per the details you have shared in the query, it looks like a product bug. I have raised this issue with the internal Product team. Once I hear back from them, I will keep everyone posted on this. Thanks for your patience!",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"queri rais intern team updat",
        "Solution_link_count":0.0,
        "Solution_original_content":"vinothkumark welcom platform thankyou queri share queri rais intern team patienc",
        "Solution_preprocessed_content":"welcom platform thankyou queri share queri rais intern team patienc",
        "Solution_readability":4.1,
        "Solution_reading_time":3.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":56.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":59.6941902778,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>Why does these code snippets produce different results?<\/p>\n<pre><code class=\"lang-auto\">for i in range(100):\n    wandb.log({\"train\/loss\": i}, step=i)\n    \nfor i in range(100):\n    wandb.log({\"val\/loss\": i**2}, step=i)\n<\/code><\/pre>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/ioofli05?workspace=user-dminn\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/ioofli05?workspace=user-dminn\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/626b9c1d1ceb6cb5d3bb90cf6ab8d2894a6b8b14.png\" class=\"thumbnail onebox-avatar\" width=\"128\" height=\"128\">\n\n<h3><a href=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/ioofli05?workspace=user-dminn\" target=\"_blank\" rel=\"noopener\">dminn<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<pre><code class=\"lang-auto\">for i in range(100):\n    wandb.log({\"train\/loss\": i}, step=i)\n    wandb.log({\"val\/loss\": i**2}, step=i)\n<\/code><\/pre>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/146hdnar?workspace=user-dminn\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/146hdnar?workspace=user-dminn\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/626b9c1d1ceb6cb5d3bb90cf6ab8d2894a6b8b14.png\" class=\"thumbnail onebox-avatar\" width=\"128\" height=\"128\">\n\n<h3><a href=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/146hdnar?workspace=user-dminn\" target=\"_blank\" rel=\"noopener\">dminn<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
        "Challenge_closed_time":1658411733624,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658196834539,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-log-inconsistent-behavior-with-step-parameter\/2771",
        "Challenge_link_count":10,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":21.6,
        "Challenge_reading_time":31.98,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":59.6941902778,
        "Challenge_title":"Wandb.log inconsistent behavior with step parameter",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":132.0,
        "Challenge_word_count":129,
        "Platform":"Tool-specific",
        "Solution_body":"<p>As far as I understand the step variable, once the step is incremented, the value is stored immutably. So, if chronologically, you execute:<\/p>\n<pre><code class=\"lang-python\">\nwandb.log({'potato': 1}, step=0}\nwandb.log({'tomato': 1}, step=0}\nwandb.log({'potato': 2}, step=1}\nwandb.log({'tomato': 2}, step=1}\n\n<\/code><\/pre>\n<p>It\u2019ll be fine but instead if you execute:<\/p>\n<pre><code class=\"lang-python\">\nwandb.log({'potato': 1}, step=0}\nwandb.log({'potato': 2}, step=1}\nwandb.log({'tomato': 1}, step=0}\nwandb.log({'tomato': 2}, step=1}\n\n<\/code><\/pre>\n<p>the third command (tomato = 1, step 0) will not be executed since the logger has already moved past step 0.<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"explan inconsist step paramet log function step variabl store immut increment logger move past step log lower step valu execut",
        "Solution_link_count":0.0,
        "Solution_original_content":"step variabl step increment valu store immut chronolog execut log potato step log tomato step log potato step log tomato step itll execut log potato step log potato step log tomato step log tomato step tomato step execut logger move past step",
        "Solution_preprocessed_content":"step variabl step increment valu store immut chronolog execut itll execut execut logger move past step",
        "Solution_readability":7.6,
        "Solution_reading_time":8.59,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":81.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":339.8152741667,
        "Challenge_answer_count":8,
        "Challenge_body":"Hi all,\n\nI'm trying to create an inference pipeline with the AML designer.\nI clicked on the \"Create inference pipeline\" button:\n\n\n\n\nand now I want to do some changes in the pipeline. I added at the end two more steps and linked the Webservice output component to the last step:\n\n\n\n\nI clicked on save and submit it.\nThe result is the following:\n\n\n\n\nThe two new steps are present and executed, but the webservice output step is disappeared! I've tried multiple time with the same result.\nThe webservice input step is correctly present at the beginning of the pipeline.\n\nAlso, after making the change and saving correctly, if I exit and reopen the pipeline the step \"Web Service Output\" is no longer there\n\nCan you help me?\n\nThanks!\n\nG",
        "Challenge_closed_time":1654609016260,
        "Challenge_comment_count":1,
        "Challenge_created_time":1653385681273,
        "Challenge_favorite_count":14.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/861882\/azure-machine-learning-designer-webservice-inoputo.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":9,
        "Challenge_readability":8.7,
        "Challenge_reading_time":9.53,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":339.8152741667,
        "Challenge_title":"Azure Machine Learning Designer - Webservice input\/output disappear",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":134,
        "Platform":"Tool-specific",
        "Solution_body":"Hi @Antonio-9417,\n\nSorry for the inconvenience caused.\nThis is a known bug and we've fixed. Could you please retry to see if you can still repro? I tried from my side either manually build an inference pipeline or modify the auto-gen inference pipeline, the web service input\/output components are still there.\n\nIf you can still repro, could you please provide following info for us to investigate?\n- your inference pipeline draft URL\n- inference pipeline job URL of which the webservice input\/output components disappear\n- Is your workspace in Vnet?\n\nWe're also happy to set up a call to investigate, could you please send me an email so that I can send the meeting request?\nWe're based in Beijing (UTC+8).\n\nThanks!",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"retri reproduc persist request set",
        "Solution_link_count":0.0,
        "Solution_original_content":"antonio sorri inconveni retri repro tri manual build infer pipelin modifi auto gen infer pipelin web servic input output compon repro infer pipelin draft url infer pipelin job url webservic input output compon disappear workspac vnet happi set send email send meet request base beij utc",
        "Solution_preprocessed_content":"sorri inconveni retri repro tri manual build infer pipelin modifi infer pipelin web servic compon repro infer pipelin draft url infer pipelin job url webservic compon disappear workspac vnet happi set send email send meet request base beij",
        "Solution_readability":7.9,
        "Solution_reading_time":8.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":118.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":1.3894444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nCustomer who loads the e-bike data to S3 wants to get AI\/ML insight from sensor data. The e-bike sensor data are size about 4KB files each and posted in S3 buckets. The sensor data is put into format like this\n\ntimestamp1, sensorA, sensorB, sensorC, ..., sensorZ timestamp2, sensorA, sensorB, sensorC, ..., sensorZ timestamp3, sensorA, sensorB, sensorC, ..., sensorZ ...\n\nThen these sensor data are put into one file about 4KB size.\n\nThe plan I have is to\n\nRead S3 objects\nParse S3 object with Lambda. I thought about Glue but wanted to put data in DynamoDB where Glue does not seem to support. Also, Glue seems to be more expensive.\nPut the data in DynamoDB with bike ID as primary key and timestamp as sort key.\nUse SageMaker to learn with the DynamoDB data. There will be separate discussion on choosing which model and making time-series inferencing.\nIf we need to re-learn, it will use the DynamoDB data, not from S3. I think it will be faster to get data from DynamoDB instead from the raw S3 data.\nAlso, I think we can filter out some bad input or apply little modification to DynamoDB data (shifting time stamps to the correct time, etc.)\nMake inferencing output based on the model.\n\nWhat do you think? Would you agree? Would you approach the problem differently? Would you rather learn from S3 directly via Athena or direct S3 access? Or would you rather use Glue and Redshift? But the data about 100MB would be sufficient to train the model we have in mind. Glue and Redshift maybe overkill. Currently, Korea region does not support Timestream database. So, time series database closest in Korea could be DynamoDB.\n\nPlease share your thoughts.\n\nThanks!",
        "Challenge_closed_time":1607362919000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607357917000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUebPx1UeWSGOb_3i0TXlBWA\/ai-ml-data-acquisition-and-preprocessing",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":20.71,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":1.3894444444,
        "Challenge_title":"[AI\/ML] Data acquisition and preprocessing",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":38.0,
        "Challenge_word_count":289,
        "Platform":"Tool-specific",
        "Solution_body":"Thoughts about DynamoDB\n\nPer GB, DynamoDB is around 5X more cost per GB of data stored. On top of that, you have RCU\/WCU cost.\n\nI would recommend keeping data in S3. Not only is it more cost effective, but with S3, you do not have to worry about RCU\/WCU cost or throughput of DynamoDB.\n\nSageMaker notebooks and training instances can read directly from S3, and S3 has high-throughput. I don't think you will have a problem with 100 MB datasets.\n\nIf you need to prep\/transform your data, you can do the transformations \"in place\" in S3 using Glue, Athena, Glue DataBrew, GlueStudio, etc.\n\nGlue and DynamoDB\n\nI thought about Glue but wanted to put data in DynamoDB where Glue does not seem to support.\n\nGlue supports both Python and Spark jobs. If you use a Glue Python job, you can import the boto3 (AWS SDK) library and write to DynamoDB.\n\nOther strategies\n\nHow is your customer ingesting the sensor data \/ how is it being written to S3? Are they using AWS IoT Core?\n\nRegardless, the pattern you've described thus far is:\n\nDevice -> Sensor data in S3 -> Transform with Lambda -> store data in DynamoDB\n\nAn alternative approach you could consider is using Kinesis Firehose with Lambda transformations. This will allow you to do \"in-line\" parsing \/ transformation of your data before it is ever written to S3, this removing the need to re-read the data from S3 and apply transformations after the fact. Firehose also allows you to write the stored data in formats such as Parquet, which can help with cost and subsequent query performance.\n\nIf you want to store both raw data and transformed data, you can use a \"fanout\" pattern with Kinesis Streams\/Firehose, where one output is raw data to S3 and the other is a transformed stream.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"acquir preprocess bike sensor data data notebook train instanc read directli glue job import boto librari write dynamodb kinesi firehos lambda transform line pars transform data written remov read data appli transform fanout pattern kinesi stream firehos store raw data transform data",
        "Solution_link_count":0.0,
        "Solution_original_content":"dynamodb dynamodb cost data store rcu wcu cost keep data cost worri rcu wcu cost throughput dynamodb notebook train instanc read directli high throughput dataset prep transform data transform glue athena glue databrew gluestudio glue dynamodb glue data dynamodb glue glue spark job glue job import boto sdk librari write dynamodb strategi ingest sensor data written iot core regardless pattern devic sensor data transform lambda store data dynamodb kinesi firehos lambda transform allow line pars transform data written remov read data appli transform firehos allow write store data format parquet cost subsequ queri perform store raw data transform data fanout pattern kinesi stream firehos output raw data transform stream",
        "Solution_preprocessed_content":"dynamodb dynamodb cost data store cost keep data cost worri cost throughput dynamodb notebook train instanc read directli dataset data transform glue athena glue databrew gluestudio glue dynamodb glue data dynamodb glue glue spark job glue job import boto librari write dynamodb strategi ingest sensor data written iot core regardless pattern devic sensor data transform lambda store data dynamodb kinesi firehos lambda transform allow pars transform data written remov data appli transform firehos allow write store data format parquet cost subsequ queri perform store raw data transform data fanout pattern kinesi output raw data transform stream",
        "Solution_readability":8.2,
        "Solution_reading_time":20.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":299.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":13.5233052778,
        "Challenge_answer_count":1,
        "Challenge_body":"I am running experiments in Azure Machine Learning using ParallelRunStep, and I cannot get the user folder with logs as defined in readme.txt file with the log folder structure.\nI cannot find log\/user folder with \"Logs generated when loading and running user's scripts.\"\n\nreadme.txt file states:\nParallelRunStep has two major parts:\n1. Scheduling, progress tracking and file concatenation for append_row.\n2. Processing mini batch by calling the entry script.\nThe agent manager on each node start agents.\nAn agent gets mini batch and calls the entry script against the mini batch.\n\n The \"logs\" folder has user, sys and perf sub folders.\n The user folder includes messages from the entry script in processing mini batches.\n The sys folder includes messages from #1 and non-entry script log from #2.\n The perf folder includes periodical checking result of resource usage.\n\n\n\nIn majority case, users can find the processing messages from the user folder.\nUsers need to check sys folder for messages beyond processing mini batches.\nlogs\/\nazureml\/: Logs from azureml dependencies. e.g. azureml.dataprep\nuser\/ : Logs generated when loading and running user's scripts.\nerror\/ : Logs of errors encountered while loading and running entry script.\nstderr\/ : stderr output of user's scripts.\nstdout\/ : stdout output of user's scripts.\nentry_script_log\/ : Logs generated by loggers of EntryScript()\n<node seq> :\nprocessNNN.log.txt : Logs generated by loggers of EntryScript() from each process.",
        "Challenge_closed_time":1645670223416,
        "Challenge_comment_count":1,
        "Challenge_created_time":1645621539517,
        "Challenge_favorite_count":8.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/747549\/azure-machine-learning-i-cannot-find-experiment39s.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.4,
        "Challenge_reading_time":19.42,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":13.5233052778,
        "Challenge_title":"Azure Machine Learning: I cannot find experiment's user logs located in logs\/user folder",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":233,
        "Platform":"Tool-specific",
        "Solution_body":"@CalabriaMonteroSalvadorSGRESEDFPDC-5704 Thanks for the question. Please follow the doc to view and log files for a run. Interactive logging sessions are typically used in notebook environments. The method Experiment.start_logging() starts an interactive logging session. Any metrics logged during the session are added to the run record in the experiment. The method run.complete() ends the sessions and marks the run as completed.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics#view-and-download-log-files-for-a-run",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"document log file run document interact log session start start log metric log session run record run complet end session mark run complet log folder locat log run parallelrunstep",
        "Solution_link_count":1.0,
        "Solution_original_content":"calabriamonterosalvadorsgresedfpdc doc log file run interact log session typic notebook environ start log start interact log session metric log session run record run complet end session mark run complet http doc com log metric download log file run",
        "Solution_preprocessed_content":"doc log file run interact log session typic notebook environ start interact log session metric log session run record end session mark run complet",
        "Solution_readability":9.9,
        "Solution_reading_time":7.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":62.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":165.5280555556,
        "Challenge_answer_count":1,
        "Challenge_body":"Is there a way to configure Amazon ECR containers so that they can't be changed once they're created? Here are our requirements:\n\nContainers can't be changed after their built.\nContainers can't receive updates.\nChanges in the containerized application must require the building and deployment of a new container image.\nRuntime data and configurations must be stored outside of the container environment.",
        "Challenge_closed_time":1608306412000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607710511000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUUPiBylRCSe6_ax_u_4g-oA\/can-you-configure-amazon-ecr-containers-to-be-immutable",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":5.74,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":165.5280555556,
        "Challenge_title":"Can you configure Amazon ECR containers to be immutable?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":27.0,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Solution_body":"Yes, you can configure Amazon ECR containers to be immutable. Amazon ECR uses resource-based permissions to control access to repositories. The resource-based permissions let you specify which IAM users or roles have access to a repository and what actions they can perform on it. By default, only the repository owner has access to a repository.\n\nFor more information, see Repository policies and Image tag mutability in the Amazon ECR user guide.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"configur ecr immut resourc base permiss control access repositori resourc base permiss specifi iam role access repositori action perform ecr guid repositori polici imag tag mutabl",
        "Solution_link_count":0.0,
        "Solution_original_content":"configur ecr immut ecr resourc base permiss control access repositori resourc base permiss specifi iam role access repositori action perform default repositori owner access repositori repositori polici imag tag mutabl ecr guid",
        "Solution_preprocessed_content":"configur ecr immut ecr permiss control access repositori permiss specifi iam role access repositori action perform default repositori owner access repositori repositori polici imag tag mutabl ecr guid",
        "Solution_readability":10.0,
        "Solution_reading_time":5.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":5.1008241667,
        "Challenge_answer_count":1,
        "Challenge_body":"I don't understand the model format trained by pipeline create by Azure Machine Learning Designer, Like default Linear Regression. Can I download the onnx format model? and How.",
        "Challenge_closed_time":1658147834667,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658129471700,
        "Challenge_favorite_count":11.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/930701\/can-i-download-onnx-model-from-pipeline-create-by.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":3.23,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":5.1008241667,
        "Challenge_title":"Can I download onnx model from pipeline create by Azure Machine Learning Designer",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":40,
        "Platform":"Tool-specific",
        "Solution_body":"@QingShuang-7885 If the pipeline is created using the designer, then the train model module output would be .ilearner file which is a binary format that encapsulates the statistical patterns learned from the data. You cannot directly modify or read this format; however, other components can use this trained model in your pipelines and you can register the model and deploy it as a endpoint. Please refer this document for more details about the training process.\n\nHowever, if you use the SDK and register a different model format you can download the trained model and convert it to ONNX if it is supported from the listed formats on this page.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"download train model onnx format sdk regist model format pipelin creat design train model modul output ilearn file format directli modifi read regist model deploi endpoint",
        "Solution_link_count":0.0,
        "Solution_original_content":"qingshuang pipelin creat design train model modul output ilearn file binari format encapsul statist pattern data directli modifi read format compon train model pipelin regist model deploi endpoint document train process sdk regist model format download train model convert onnx list format page click upvot commun member read thread",
        "Solution_preprocessed_content":"pipelin creat design train model modul output ilearn file binari format encapsul statist pattern data directli modifi read format compon train model pipelin regist model deploi endpoint document train process sdk regist model format download train model convert onnx list format page click upvot commun member read thread",
        "Solution_readability":10.4,
        "Solution_reading_time":9.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":128.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":0.1808333333,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer wants to connect a Sagemaker notebook to Glue Catalog, but is not allowed to use developer endpoints because of security constraints.\n\nI can't seem to find documentation on the Glue Catalog API that would allow this, or examples of how this might be done. Any links or pointers would be greatly appreciated.",
        "Challenge_closed_time":1594232347000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594231696000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUoiI3L85FT6OmPewooCH4lQ\/how-to-connect-a-sagemaker-notebook-to-glue-catalog",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":4.52,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1808333333,
        "Challenge_title":"How to connect a Sagemaker Notebook to Glue Catalog",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":519.0,
        "Challenge_word_count":62,
        "Platform":"Tool-specific",
        "Solution_body":"So there is the catalog API which allows you to describe databases, tables, etc. Documentation regarding the calls and data structures can be found here:\n\nhttps:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/aws-glue-api-catalog-tables.html\n\nBoto3 for get_table\n\nhttps:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/glue.html#Glue.Client.get_table\n\nIf they have a restrictive security posture (as suggested by the avoidance of Dev Endpoints) you may also suggest a Glue VPC-E: https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/vpce-interface.html\n\nI would ask what are they accessing the catalog for, as the Dev Endpoint isn't entirely about the Glue Catalog, but about the compute resources andSparkMagic.\n\nAlso, think about steering them towards AWS Data Wrangler for interacting with Glue Catalog if they are using Pandas. Helpful snippets can be found here:\n\nhttps:\/\/github.com\/awslabs\/aws-data-wrangler\/blob\/master\/tutorials\/005%20-%20Glue%20Catalog.ipynb",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"glue catalog api databas tabl boto tabl glue vpc secur constraint data wrangler interact glue catalog panda connect notebook glue catalog endpoint",
        "Solution_link_count":4.0,
        "Solution_original_content":"catalog api allow databas tabl document call data structur http doc com glue latest glue api catalog tabl html boto tabl http boto amazonaw com document api latest servic glue html glue client tabl restrict secur postur avoid dev endpoint glue vpc http doc com vpc latest userguid vpce interfac html access catalog dev endpoint isn entir glue catalog comput resourc andsparkmag steer data wrangler interact glue catalog panda http github com awslab data wrangler blob master tutori glue catalog ipynb",
        "Solution_preprocessed_content":"catalog api allow databas tabl document call data structur boto restrict secur postur glue access catalog dev endpoint isn entir glue catalog comput resourc andsparkmag steer data wrangler interact glue catalog panda",
        "Solution_readability":15.0,
        "Solution_reading_time":12.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":105.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":19.7252202778,
        "Challenge_answer_count":1,
        "Challenge_body":"I have two ML models A and B registered in my workspace and I want to deploy them to Azure Container Instance and perform A\/B testing or continuous rollout or Canary deployment. Can it be done without using AKS and only on ACI.",
        "Challenge_closed_time":1648171136616,
        "Challenge_comment_count":3,
        "Challenge_created_time":1648100125823,
        "Challenge_favorite_count":19.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/785262\/ab-testing-using-azure-container-instance.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":8.4,
        "Challenge_reading_time":3.29,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":19.7252202778,
        "Challenge_title":"A\/B testing using Azure Container Instance.",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":48,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @HARISHKUMAR-4152\n\nThanks for reaching out to us. I can understand you want to deploy your ML models to ACI to do the A\/B test. For question can models to be deployed only ACI, the answer is yes, but please be aware that ACI has some limitation compared to AKS, please check if ACI is a good choice for your model:\n\nAbout models:\nACI is suitable only for small models that are under 1 GB in size.\nWe recommend using single-node AKS to dev-test larger models.\nThe number of models to be deployed is limited to 1,000 models per deployment (per container).\n\nAbout Vnet:\nWhen using Azure Container Instances in a virtual network, the virtual network must be in the same resource group as your Azure Machine Learning workspace.\nWhen using Azure Container Instances inside the virtual network, the Azure Container Registry (ACR) for your workspace cannot also be in the virtual network.\n\nOther points:\nprefer not to manage your own Kubernetes cluster\nAre OK with having only a single replica of your service, which may impact uptime\nYou are advised to debug locally before deploying to the web service,\n\nAKS Advantages for your reference as well:\nFast response time\nAutoscaling of the deployed service\nLogging\nModel data collection\nAuthentication\nTLS termination\nHardware acceleration options such as GPU and field-programmable gate arrays (FPGA)\n\nIf you feel like ACI fulfill your need, then you can follow below guidance to do the deployment:\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-container-instance#deploy-to-aci\n\nHope this helps, please let me know if you have more questions.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful, thanks.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"perform test rollout canari deploy model regist workspac instanc aci ak aci suitabl small model size model deploi limit model deploy instanc virtual network virtual network resourc group workspac ak prefer advantag fast respons time autosc deploi servic log model data collect authent tl termin hardwar acceler option gpu",
        "Solution_link_count":1.0,
        "Solution_original_content":"harishkumar reach deploi model aci test model deploi aci awar aci limit compar ak aci choic model model aci suitabl small model size singl node ak dev test larger model model deploi limit model deploy vnet instanc virtual network virtual network resourc group workspac instanc insid virtual network registri acr workspac virtual network prefer kubernet cluster singl replica servic impact uptim debug local deploi web servic ak advantag fast respons time autosc deploi servic log model data collect authent tl termin hardwar acceler option gpu field programm gate arrai fpga aci fulfil guidanc deploy http doc com deploi instanc deploi aci hope yutong kindli accept",
        "Solution_preprocessed_content":"reach deploi model aci test model deploi aci awar aci limit compar ak aci choic model model aci suitabl small model size ak larger model model deploi limit model deploy vnet instanc virtual network virtual network resourc group workspac instanc insid virtual network registri workspac virtual network prefer kubernet cluster singl replica servic impact uptim debug local deploi web servic ak advantag fast respons time autosc deploi servic log model data collect authent tl termin hardwar acceler option gpu gate arrai aci fulfil guidanc deploy hope yutong kindli accept",
        "Solution_readability":13.7,
        "Solution_reading_time":20.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":266.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":20.9247616667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hiya!<\/p>\n<p>Last night I ran an experiment and didn\u2019t bother to check the inputs to <code>wandb.init<\/code>. It turned out later that I mixed some things up and there was a mistake in <code>job_type<\/code> kwarg.  Now I am wondering is there a way to change this parameter (I really need it for grouping) through API or UI?<\/p>\n<p>Thanx<\/p>",
        "Challenge_closed_time":1650267343235,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650192014093,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/is-there-a-way-to-change-job-type\/2253",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":7.1,
        "Challenge_reading_time":4.64,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":20.9247616667,
        "Challenge_title":"Is there a way to change job_type?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":161.0,
        "Challenge_word_count":63,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hey Ilya,<\/p>\n<p>At the moment, changing the job type is not possible. I\u2019ll file a ticket for this. As a workaround I\u2019d suggest tagging the runs using our Public API.<\/p>\n<p>Let me know if you have any questions.<\/p>\n<p>Best,<br>\nArman<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"job type api tag run public api workaround",
        "Solution_link_count":0.0,
        "Solution_original_content":"ilya moment job type ill file ticket workaround tag run public api arman",
        "Solution_preprocessed_content":"ilya moment job type ill file ticket workaround tag run public api arman",
        "Solution_readability":4.1,
        "Solution_reading_time":2.98,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":40.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":17.3883747222,
        "Challenge_answer_count":1,
        "Challenge_body":"Does Data Capture feature used for model monitor and analytics work with the multi model endpoint (one container).. we ran into an error. See error \" An error occurred (ValidationException) when calling the CreateEndPointConfig operation: Data Capture Feature is not supported with MultiModel mode\" Theoretically, it should work because it is calling the DataCaptureConfig:\n\nfrom sagemaker.model_monitor import DataCaptureConfig\n\nendpoint_name = 'your-pred-model-monitor-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()) print(\"EndpointName={}\".format(endpoint_name))\n\ndata_capture_config=DataCaptureConfig( enable_capture = True, sampling_percentage=100, destination_s3_uri=s3_capture_upload_path)",
        "Challenge_closed_time":1649857157628,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649794559479,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUlAvpGSsISyqu0MyebgRJDA\/sage-maker-multi-model-end-point-and-inference-data-capture-feature",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":20.0,
        "Challenge_reading_time":9.95,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":17.3883747222,
        "Challenge_title":"SageMaker Multi Model EndPoint and Inference Data Capture feature",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":175.0,
        "Challenge_word_count":75,
        "Platform":"Tool-specific",
        "Solution_body":"SageMaker multi-model endpoints do not have support for SageMaker Model monitor as of writing this answer. So the error is pointing to exactly that.\n\nHowever, if you are looking to implement data drift using sagemaker model monitor then you can do that my mimicking data capture config functionality by capturing inference input and prediction output and storing it in the format supported by Model Monitor. And then setup a customer monitoring container using the instructions listed https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-byoc-containers.html",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"data captur featur multimodel mode implement data drift model monitor mimic data captur config function captur infer input predict output store format model monitor set monitor instruct document",
        "Solution_link_count":1.0,
        "Solution_original_content":"multi model endpoint model monitor write exactli implement data drift model monitor mimick data captur config function captur infer input predict output store format model monitor setup monitor instruct list http doc com latest model monitor byoc html",
        "Solution_preprocessed_content":"endpoint model monitor write exactli implement data drift model monitor mimick data captur config function captur infer input predict output store format model monitor setup monitor instruct list",
        "Solution_readability":16.7,
        "Solution_reading_time":7.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":77.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2.2959119444,
        "Challenge_answer_count":1,
        "Challenge_body":"Is it possible to convert a web service output as a dataset or a csv file ? I want to consume this in another experiment.",
        "Challenge_closed_time":1592416585020,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592408319737,
        "Challenge_favorite_count":3.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/37214\/convert-web-service-output-to-a-dataset-azure-mls.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":2.14,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2.2959119444,
        "Challenge_title":"Convert web service output to a dataset Azure MLS classic",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":33,
        "Platform":"Tool-specific",
        "Solution_body":"You can delete or export in-product data stored by Azure Machine Learning Studio (classic) by using the Azure portal, the Studio (classic) interface, PowerShell, and authenticated REST APIs. This article tells you how.\n\nTelemetry data can be accessed through the Azure Privacy portal.\n\nMore details please refer to: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/export-delete-personal-data-dsr\n\nAnd also you can use one of the Azure Machine Learning Studio Module - \"Export Data\" to do it : https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-data?redirectedfrom=MSDN\n\nLet me know if you have more questions.\n\nRegards,\nYutong",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"delet export data store studio classic portal studio classic interfac powershel authent rest api export data modul studio convert web servic output dataset csv file link",
        "Solution_link_count":2.0,
        "Solution_original_content":"delet export data store studio classic portal studio classic interfac powershel authent rest api articl telemetri data access privaci portal http doc com studio export delet data dsr studio modul export data http doc com studio modul export data redirectedfrom msdn yutong",
        "Solution_preprocessed_content":"delet export data store studio portal studio interfac powershel authent rest api articl telemetri data access privaci portal studio modul export data yutong",
        "Solution_readability":11.9,
        "Solution_reading_time":8.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":78.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":555.5452777778,
        "Challenge_answer_count":4,
        "Challenge_body":"Hello,\n\nMy aim is to create a model for garden birds. I have 293 photos of birds that I have put through 2 custom labelling jobs in ground truth for training and validation. The issue I encountered was being able to have multiple labels on the bounding box which I managed to do via creating a custom labelling job with the following labels:\n\n<crowd-bounding-box\r\n    name=\"annotatedResult\"\r\n    labels=\"['Blackbird', 'Blue tit', 'Coal tit', 'Dunnock', 'Great tit', 'Long-tailed tit', 'Nuthatch', 'Pigeon', 'Robin']\" .....\n\n\nI now have 2 output manifest files with many lines of this:\n\n{\"source-ref\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\",\"BirdLabel\":{\"workerId\":\"privateXXXXX\",\"imageSource\":{\"s3Uri\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\"},\"boxesInfo\":{\"annotatedResult\":{\"boundingBoxes\":[{\"width\":1619,\"top\":840,\"label\":\"Blackbird\",\"left\":1287,\"height\":753}],\"inputImageProperties\":{\"width\":3872,\"height\":2592}}}},\"BirdLabel-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"birdlabel\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-10T15:41:52+0000\"}}\n\n\nAfter this job was successful, I made an ml.p3.2xlarge instance, using the object_detection_augmented_manifest_training template.\n\nI have filled in the necessary sections, I then run it and received this error when I have the Content Type to 'application\/x-image' with Record wrapper type:RecordIO : 'ClientError: train channel is not specified.'\n\nI then changed the channel to train_annotation instead of train and I receive this error message: \"ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)\\n\\nCaused by: u'train' is a required property\n\nAdditional information can be provided if neccessary.\nAny help would be much apreciated! Thank you.\n\nEdited by: LuciA on Jan 16, 2019 1:12 PM\n\nEdited by: LuciA on Jan 16, 2019 1:18 PM\n\nEdited by: LuciA on Jan 16, 2019 1:19 PM",
        "Challenge_closed_time":1549563627000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547563664000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUjk2-AZ6VQl68Zdm1Owq-_A\/client-error-object-detection-augmented-manifest-training-template",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":13.4,
        "Challenge_reading_time":25.05,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":555.5452777778,
        "Challenge_title":"ClientError: object_detection_augmented_manifest_training template",
        "Challenge_topic":"Database Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":52.0,
        "Challenge_word_count":219,
        "Platform":"Tool-specific",
        "Solution_body":"Hi LuciA - I'm an engineer at AWS. Thanks for continuing to try the service in the face of some difficulties. Can you please cross-reference your augmented manifest against the samples shown in https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-ground-truth-build-highly-accurate-datasets-and-reduce-labeling-costs-by-up-to-70\/, https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/object_detection_augmented_manifest_training\/object_detection_augmented_manifest_training.ipynb, and https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb?\n\nIt looks like your format is a little different, e.g., the algorithm expects to see keys called \"annotations\" and \"image_size\". Can you please check the syntax and let us know if your results change?",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"engin cross referenc augment manifest sampl blog github repositori engin format augment manifest algorithm syntax kei annot imag size present",
        "Solution_link_count":3.0,
        "Solution_original_content":"lucia engin servic cross augment manifest sampl shown http com blog ground build highli accur dataset reduc label cost http github com awslab blob master ground label job object detect augment manifest train object detect augment manifest train ipynb http github com awslab blob master ground label job ground object detect tutori object detect tutori ipynb format littl algorithm kei call annot imag size syntax",
        "Solution_preprocessed_content":"lucia engin servic augment manifest sampl shown format littl algorithm kei call annot syntax",
        "Solution_readability":21.3,
        "Solution_reading_time":11.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":93.1594822222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello everyone, I was trying to execute the example mentioned in the docs - https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_neo_compilation_jobs\/pytorch_torchvision\/pytorch_torchvision_neo.html. I was able to successfully run this example but as soon as I changed the target_device to jetson_tx2, after which I ran the entire script again, keeping the rest of the code as it is, the model stopped working. I was not getting any inferences from the deployed model and it always errors out with the message:\n\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from <users-sagemaker-endpoint> with message \"Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\"                \n\n\nAccording to the troubleshoot docs https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-troubleshooting-inference.html, this seems to be an issue of model_fn() function. The inference script used by this example is mentioned here https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_neo_compilation_jobs\/pytorch_torchvision\/code\/resnet18.py , which itself doesn't contain any model_fn() definition but it still worked for target device ml_c5. So could anyone please help me with the following questions:\n\nWhat changes does SageMaker Neo do to the model depending on target_device type? Since it seems the same model is loaded in a different way for different target device.\nIs there any way to determine how the model is expected to load for a certain target_device type so that I could define the model_fn() function myself in the same inference script mentioned above?\nAt-last, can anyone please help with the inference script for this very same model as mentioned in docs above which works for jetson_tx2 device as well.\n\nAny suggestions or links on how to resolve this issue would be really helpful.",
        "Challenge_closed_time":1668766092280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668430718144,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUJvbkzp91TGSZO1GwW-r90w\/help-with-inference-script-for-amazon-sagemaker-neo-compiled-models",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":25.71,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":93.1594822222,
        "Challenge_title":"Help with Inference Script for Amazon Sagemaker Neo Compiled Models",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":32.0,
        "Challenge_word_count":279,
        "Platform":"Tool-specific",
        "Solution_body":"As you mentioned, you changed the Neo compiling target from ml_c5 to jetson_tx2, the compiled model will require runtime from jetson_tx2. If you kept other code unchanged, the model will be deployed to a ml.c5.9xlarge EC2 instance, which doesn't provide Nvida Jeston.\n\nThe model can't be loaded and will error out since Jestion is a device Nvidia GPU structure while c5 is only equipped with CPU. No CUDA environment.\n\nIf you compile the model with jeston_tx2 as target, you should download the model and run the compiled model in a real Nvidia Jeston device.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"compil model target devic set jetson download compil model run nvidia jetson devic model compil target devic deploy instanc model",
        "Solution_link_count":0.0,
        "Solution_original_content":"neo compil target jetson compil model runtim jetson kept unchang model deploi xlarg instanc nvida jeston model load jestion devic nvidia gpu structur equip cpu cuda environ compil model jeston target download model run compil model nvidia jeston devic",
        "Solution_preprocessed_content":"neo compil target compil model runtim kept unchang model deploi instanc nvida jeston model load jestion devic nvidia gpu structur equip cpu cuda environ compil model target download model run compil model nvidia jeston devic",
        "Solution_readability":7.1,
        "Solution_reading_time":6.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":94.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":21.2240113889,
        "Challenge_answer_count":2,
        "Challenge_body":"Greetings! I'm a data scientist working in SageMaker notebooks. I'd appreciate an explanation about when should I use Glue Interactive and not SageMaker Processing jobs. To my understanding, they are very similar and I can't differentiate them.\n\nThank you!",
        "Challenge_closed_time":1663248095278,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663171688837,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU7J5WaZe3Qzi2giJaOmBFDQ\/glue-interactive-vs-sage-maker-processing",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":11.0,
        "Challenge_reading_time":3.75,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":21.2240113889,
        "Challenge_title":"Glue Interactive vs SageMaker Processing?",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":63.0,
        "Challenge_word_count":43,
        "Platform":"Tool-specific",
        "Solution_body":"I would suggest that you use Sagemaker processing for the data cleansing and preparation. I have led projects where all the data cleansing, preparation and model build and testing have been done in Sagemaker and the data scientists love the tool.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"process data cleans prepar prefer data scientist abil data cleans prepar model build test glue interact",
        "Solution_link_count":0.0,
        "Solution_original_content":"process data cleans prepar led data cleans prepar model build test data scientist love",
        "Solution_preprocessed_content":"process data cleans prepar led data cleans prepar model build test data scientist love",
        "Solution_readability":10.1,
        "Solution_reading_time":3.03,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":41.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":7.3733694445,
        "Challenge_answer_count":1,
        "Challenge_body":"I have pip install pyenchant, but It doesn't seem to be working.\n\n\n\n\n\n\n\n\n\nIs there any other way?\nhttps:\/\/stackoverflow.com\/questions\/21083059\/enchant-c-library-not-found-while-installing-pyenchant-using-pip-on-osx\nI looked it up but do not know where to put it\nThanks!",
        "Challenge_closed_time":1643357543967,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643330999837,
        "Challenge_favorite_count":13.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/713217\/machine-learning-conda-env-packagepyenchant.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":4.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":7.3733694445,
        "Challenge_title":"machine learning conda env package(pyenchant)",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":35,
        "Platform":"Tool-specific",
        "Solution_body":"@YongchaoLiuNeusoftAmericaInc-6769 Based on the error it looks like you also need to ensure the enchant C library is available to use for the package. Based on the pip install page of pyenchant, the package will not work directly out of the box using pip.\n\nIn general, PyEnchant will not work out of the box after having been installed with pip. See the Installation section for more details.\n\nSince you are using Linux, this is the guidance on the installation page.\n\nThe quickest way is to install libenchant using the package manager of your current distribution. PyEnchant tries to be compatible with a large number of libenchant versions. If you find an incompatibility with your libenchant installation, feel free to open a bug report.\n\n\nTo detect the libenchant binaries, PyEnchant uses ctypes.util.find_library(), which requires ldconfig, gcc, objdump or ld to be installed. This is the case on most major distributions, however statically linked distributions (like Alpine Linux) might not bring along binutils by default.\n\n\n\n\nI believe you are using the ubuntu flavor of the azureml base image, In this case I think adding libenchant-2-dev as dependency in your YAML should work.\n\n -libenchant-2-dev=2.2.8\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"enchant librari pyenchant packag instal libench packag distribut libench dev depend yaml file",
        "Solution_link_count":0.0,
        "Solution_original_content":"yongchaoliuneusoftamericainc base enchant librari packag base pip instal page pyenchant packag directli box pip gener pyenchant box instal pip instal section linux guidanc instal page quickest instal libench packag distribut pyenchant tri compat larg libench version incompat libench instal free open report detect libench binari pyenchant ctype util librari ldconfig gcc objdump instal distribut static link distribut alpin linux binutil default believ ubuntu flavor base imag libench dev depend yaml libench dev click upvot commun member read thread",
        "Solution_preprocessed_content":"base enchant librari packag base pip instal page pyenchant packag directli box pip gener pyenchant box instal pip instal section linux guidanc instal page quickest instal libench packag distribut pyenchant tri compat larg libench version incompat libench instal free open report detect libench binari pyenchant ldconfig gcc objdump instal distribut static link distribut binutil default believ ubuntu flavor base imag depend yaml click upvot commun member read thread",
        "Solution_readability":9.6,
        "Solution_reading_time":16.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":210.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":7.3909094444,
        "Challenge_answer_count":2,
        "Challenge_body":"I have an issue while getting Catboost image URI. It is a function for generating ECR image URIs for pre-built SageMaker Docker images. Here is my code catboost_container = sagemaker.image_uris.retrieve(\"catboost\", my_region, \"latest\")",
        "Challenge_closed_time":1658369922828,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658343315554,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU3HFACW88SuKcGZ2izeOsuA\/file-not-found-error-errno-2-no-such-file-or-directory-home-ec-2-user-anaconda-3-envs-python-3-lib-python-3-8-site-packages-sagemaker-image-uri-config-catboost-json",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":14.6,
        "Challenge_reading_time":5.3,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":7.3909094444,
        "Challenge_title":"FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.8\/site-packages\/sagemaker\/image_uri_config\/catboost.json'",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":87.0,
        "Challenge_word_count":39,
        "Platform":"Tool-specific",
        "Solution_body":"As illustrated here in the docs for the algorithm, the parameters for retrieving this URI are a bit different: It's more like using the new JumpStart models (if you're familiar with that) than the old-style pre-built algorithms.\n\ntrain_model_id, train_model_version, train_scope = \"catboost-classification-model\", \"*\", \"training\"\ntraining_instance_type = \"ml.m5.xlarge\"\n\n# Retrieve the docker image\ntrain_image_uri = image_uris.retrieve(\n    region=None,\n    framework=None,\n    model_id=train_model_id,\n    model_version=train_model_version,\n    image_scope=train_scope,\n    instance_type=training_instance_type\n)\n\n\nI tested the above snippet from the doc page on SageMaker Studio and it worked OK. If you still see errors, it's likely your SageMaker Python SDK version is outdated (which can happen if for example you don't restart SM Studio apps or SM Notebook Instances regularly). Can check with sagemaker.__version__ and upgrade with !pip install --upgrade sagemaker if needed.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"paramet document retriev catboost imag uri persist version sdk upgrad pip instal upgrad",
        "Solution_link_count":0.0,
        "Solution_original_content":"illustr doc algorithm paramet retriev uri bit jumpstart model familiar style pre built algorithm train model train model version train scope catboost classif model train train instanc type xlarg retriev docker imag train imag uri imag uri retriev region framework model train model model version train model version imag scope train scope instanc type train instanc type test doc page studio sdk version outdat restart studio app notebook instanc regularli version upgrad pip instal upgrad",
        "Solution_preprocessed_content":"illustr doc algorithm paramet retriev uri bit jumpstart model algorithm train retriev docker imag region framework test doc page studio sdk version outdat upgrad pip instal",
        "Solution_readability":12.5,
        "Solution_reading_time":12.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":116.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":17.5549152778,
        "Challenge_answer_count":1,
        "Challenge_body":"Sorry in advance for the lengthy question as I wanted to explain it as detailed as possible. I used the Azure AutoML to train a model and deployed it as a web service. Now I can access (call) it over the REST endpoint.\n\nI have the following data types for attributes: date (timestamp), number, number, number, number, integer. I trained the model with the following parametres:\n\nTimestaps interval: 15 min\nForecast Horizon: 4 (I need the forecast every hour for the next hour)\nTarget rolling window size: 96 (the forecast must ba based on the last 24 hours of data)\n\nI have two questions.\n\nAs I understand, based on the above, I have to provide last 4 entries to the model for a correct prediction. Otherwise, it will consider a time gap. Am I right? In this case, how I could input 4 instances at a time for a single prediction? The following example is wrong as it asks for 4 predictions for each instance:\n\nimport requests\nimport json\n\nURL for the web service\n\nscoring_uri = 'http:\/\/xxxxx-xxxxxxx-xxxxxx-xxxxxxx.xxxxx.azurecontainer.io\/score'\n\ndata = {\"data\":\n[\n[\n2020-10-04 19:30:00,1.29281,1.29334,1.29334,1.29334,1\n],\n[\n2020-10-04 19:45:00,1.29334,1.29294,1.29294,1.29294,1\n],\n[\n2020-10-04 21:00:00,1.29294,1.29217,1.29334,1.29163,34\n],\n[\n2020-10-04 21:15:00,1.29217,1.29257,1.29301,1.29115,195]\n]\n}\n# Convert to JSON string\ninput_data = json.dumps(data)\n\nSet the content type\n\nheaders = {'Content-Type': 'application\/json'}\n\nMake the request and display the response\n\nresp = requests.post(scoring_uri, input_data, headers=headers)\nprint(resp.text)\n\nThe above code is based on the provided Microsoft example https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service?tabs=python#call-the-service-python.\n\nI am unable to replicate the provided example with my data. I have an error \"SyntaxError: leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\" pointing to the date. I assume, I need to specify the data type but could not find how.\nI tried to load a line from a csv file but I have an error (SyntaxError: invalid syntax) pointing to \"with\" with the following:\n\ndata = {\"data\":\n[with open('file', \"r\") as f:\nfor line in f: pass\nprint(line)]\n}\n\nI tested getting the last line from a csv file intependetly and it works but not inside the full script.\n\nI very appreciate any help or direction. Thank you.",
        "Challenge_closed_time":1609796508692,
        "Challenge_comment_count":1,
        "Challenge_created_time":1609733310997,
        "Challenge_favorite_count":6.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":9.1,
        "Challenge_reading_time":30.86,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":17.5549152778,
        "Challenge_title":"Data input format (call the service) for Azure ML time series forecast model deployed as a web service (Python)",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":356,
        "Platform":"Tool-specific",
        "Solution_body":"@AlexeyPisakov-8757\nPlease try the solution mentioned below.\n\nThe service takes data in form of deserialized pandas data frame. In the example below, it will look like:\nimport json\n\nX_test = pd.DataFrame([\n\n ['2020-10-04 19:30:00', 1.29281, 1.29334, 1.29334, 1.29334, 1],\n ['2020-10-04 19:45:00', 1.29334, 1.29294, 1.29294, 1.29294, 1],\n ['2020-10-04 21:00:00', 1.29294, 1.29217, 1.29334, 1.29163, 34],\n ['2020-10-04 21:15:00', 1.29217, 1.29257, 1.29301, 1.29115, 195]],\n columns=['date', 'number_1', 'number_2', 'number_3', 'number_4', 'integer']\n\n\n\n)\n\ntest_sample = json.dumps({'data': X_test.to_dict(orient='records')})\n\ntest_sample\n\n\n\n\nWhich will result in JSON string as:\n\n{\"data\": [{\"date\": \"2020-10-04 19:30:00\", \"number_1\": 1.29281, \"number_2\": 1.29334, \"number_3\": 1.29334, \"number_4\": 1.29334, \"integer\": 1}, {\"date\": \"2020-10-04 19:45:00\", \"number_1\": 1.29334, \"number_2\": 1.29294, \"number_3\": 1.29294, \"number_4\": 1.29294, \"integer\": 1}, {\"date\": \"2020-10-04 21:00:00\", \"number_1\": 1.29294, \"number_2\": 1.29217, \"number_3\": 1.29334, \"number_4\": 1.29163, \"integer\": 34}, {\"date\": \"2020-10-04 21:15:00\", \"number_1\": 1.29217, \"number_2\": 1.29257, \"number_3\": 1.29301, \"number_4\": 1.29115, \"integer\": 195}]}\n\n\n\n\nPlease rename the columns to the corresponding columns from the training data set.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"input instanc time singl predict data deseri panda data frame convert data json renam column column train data set",
        "Solution_link_count":0.0,
        "Solution_original_content":"alexeypisakov servic data deseri panda data frame import json test datafram column date integ test sampl json dump data test dict orient record test sampl json data date integ date integ date integ date integ renam column column train data set",
        "Solution_preprocessed_content":"servic data deseri panda data frame import json column date integ json data renam column column train data set",
        "Solution_readability":4.3,
        "Solution_reading_time":16.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":25.0,
        "Solution_word_count":141.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":39.8832972222,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to create a tabular dataset in a notebook with R kernel. The following code works with python kernel but how to do the same thing with R kernel ? Can anyone please help me ? Any help would be appreciated.\n\n from azureml.core import Workspace, Dataset\n  from azureml.core.dataset import Dataset\n        \n  subscription_id = 'abc'\n  resource_group = 'abcd'\n  workspace_name = 'xyz'\n        \n  workspace = Workspace(subscription_id, resource_group, workspace_name)\n        \n  dataset = Dataset.get_by_name(workspace, name='test')\n        \n        \n  # create tabular dataset from all parquet files in the directory\n  tabular_dataset_3 = Dataset.Tabular.from_parquet_files(path=(datastore,'\/UI\/09-17-2022_125003_UTC\/userdata1.parquet'))",
        "Challenge_closed_time":1663571695407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663428115537,
        "Challenge_favorite_count":11.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1012184\/how-to-create-tabular-dataset-in-notebook-with-r-k.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.42,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":39.8832972222,
        "Challenge_title":"How to create tabular dataset in notebook with R kernel",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":83,
        "Platform":"Tool-specific",
        "Solution_body":"@Ankit19Gupta-9721 The Azure Machine Learning SDK for R was deprecated at the end of 2021 to make way for an improved R training and deployment experience using Azure Machine Learning CLI 2.0\nPlease refer the azureml SDK repo for more details which was deprecated at the end of last year. You can use CLI to register the dataset using specification file.\n\n\n\n az ml dataset register [--file]\n                        [--output-metadata-file]\n                        [--path]\n                        [--resource-group]\n                        [--show-template]\n                        [--skip-validation]\n                        [--subscription-id]\n                        [--workspace-name]\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"cli regist dataset file regist dataset dataset regist file output metadata file path resourc group templat skip subscript workspac bias summari",
        "Solution_link_count":0.0,
        "Solution_original_content":"ankitgupta sdk deprec end improv train deploy cli sdk repo deprec end year cli regist dataset file dataset regist file output metadata file path resourc group templat skip subscript workspac click upvot commun member read thread",
        "Solution_preprocessed_content":"sdk deprec end improv train deploy cli sdk repo deprec end year cli regist dataset file dataset regist click upvot commun member read thread",
        "Solution_readability":12.3,
        "Solution_reading_time":7.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":92.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":5.7569444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\n\nI've just trained a churn prediction model with XGBoost algorithm, based on the SageMaker example notebooks. I've created SageMaker batch transformation jobs using this model using input from CSV file with multiple records, however the output file is a single record CSV containing all the inferences in a single comma separated row. The result is that I'm not able to use the \"Join source\" feature with \"Input - Merge input data with job output\" since the input and output files must match the number of records. I've tried with different batch job configurations but I always get the same single line output file.\n\nDo you know if is there any configuration that allows me to merge input and output in order to have a direct association between an input column with its inference result? Is this a restriction from the XGBoost algorithm built-in implementation?",
        "Challenge_closed_time":1599791910000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599771185000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUYz7Bz_5sTmG0uBaqlt7J_g\/xgboost-sagemaker-batch-transform-job-output-in-multiple-lines",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":11.44,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":5.7569444444,
        "Challenge_title":"xgboost sagemaker batch transform job output in multiple lines",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":199.0,
        "Challenge_word_count":152,
        "Platform":"Tool-specific",
        "Solution_body":"Sounds like a configuration issue, this algorithm should be able to output proper output CSVs.\n\nAre you using accept=\"text\/csv\" and assemble_with=\"Line\" on your Transformer? Is your strategy set to SingleRecord or MultiRecord?\n\nAnd split_type=\"Line\", content_type=\"text\/csv\" on the .transform() call?\n\nI have had custom algorithms accidentally output row vectors instead of column vectors for multi-record batches in the past (because they gave a 1D output which the default serializer interpreted as a row), but not built-in algorithms.\n\nDropping to SingleRecord could be a last resort (forcing Batch Transform itself to handle the serialization), but would decrease efficiency\/speed.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"configur set batch transform job accept text csv assembl line transform set split type line type text csv transform strategi set multirecord algorithm built implement restrict output format drop singlerecord resort decreas effici speed",
        "Solution_link_count":0.0,
        "Solution_original_content":"configur algorithm output output csv accept text csv assembl line transform strategi set singlerecord multirecord split type line type text csv transform algorithm accident output row vector column vector multi record batch past gave output default serial interpret row built algorithm drop singlerecord resort forc batch transform serial decreas effici speed",
        "Solution_preprocessed_content":"configur algorithm output output csv transform strategi set singlerecord multirecord transform algorithm accident output row vector column vector batch past algorithm drop singlerecord resort decreas",
        "Solution_readability":11.9,
        "Solution_reading_time":8.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":96.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":6.1972222222,
        "Challenge_answer_count":1,
        "Challenge_body":"How does Amazon SageMaker batch transform handle failures? Is there a way to automate failure handling and retries built into the service?",
        "Challenge_closed_time":1593617691000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593595381000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUE10OtSwDRCiB-0pP6wflYQ\/is-there-a-way-to-automate-failure-handling-and-retries-when-using-amazon-sage-maker-batch-transform",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":2.98,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":6.1972222222,
        "Challenge_title":"Is there a way to automate failure handling and retries when using Amazon SageMaker batch transform?",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":120.0,
        "Challenge_word_count":37,
        "Platform":"Tool-specific",
        "Solution_body":"You can use the ModelClientConfig API to configure the timeout and maximum number of retries for processing a transform job invocation. The maximum number of automated retries is three.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"modelclientconfig api configur timeout maximum retri process transform job invoc batch transform maximum autom retri",
        "Solution_link_count":0.0,
        "Solution_original_content":"modelclientconfig api configur timeout maximum retri process transform job invoc maximum autom retri",
        "Solution_preprocessed_content":"modelclientconfig api configur timeout maximum retri process transform job invoc maximum autom retri",
        "Solution_readability":11.3,
        "Solution_reading_time":2.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":69.1720713889,
        "Challenge_answer_count":1,
        "Challenge_body":"I am trying to deploy an R inference script to Azure ML Service Endpoint as an Azure Container Instance. I have made the following steps:\n\ncreated a custom Docker image from scratch and pushed it to the Azure Container Registry (associated with AML Workspace)\n\n\nregistered a custom environment in AML Workspace, based on the image in ACR\n\n\ndeployed R entry script (just a simple hello world script with init() and run() functions defined)\n\n\nthe inference configuration uses the custom AML environment\n\n\ndeployment is made with Azure ML R SDK\n\nThe container instance is created, but the endpoint startup runs into error. Here is the output from the container instance:\n\n 2020-10-16T12:56:21,639812796+00:00 - gunicorn\/run \n 2020-10-16T12:56:21,639290594+00:00 - iot-server\/run \n 2020-10-16T12:56:21,640405198+00:00 - rsyslog\/run \n 2020-10-16T12:56:21,735291424+00:00 - nginx\/run \n EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n 2020-10-16T12:56:23,736657191+00:00 - iot-server\/finish 1 0\n 2020-10-16T12:56:23,834747728+00:00 - Exit code 1 is normal. Not restarting iot-server.\n Starting gunicorn 20.0.4\n Listening at: http:\/\/127.0.0.1:31311 (11)\n Using worker: sync\n worker timeout is set to 300\n Booting worker with pid: 38\n \/bin\/bash: \/root\/miniconda3\/lib\/libtinfo.so.6: no version information available (required by \/bin\/bash)\n SPARK_HOME not set. Skipping PySpark Initialization.\n Exception in worker process\n Traceback (most recent call last):\n   File \"\/var\/azureml-server\/app.py\", line 43, in <module>\n     from azureml.api.exceptions.ClientSideException import ClientSideException\n ModuleNotFoundError: No module named 'azureml.api'\n    \n During handling of the above exception, another exception occurred:\n    \n Traceback (most recent call last):\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/arbiter.py\", line 583, in spawn_worker\n     worker.init_process()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/workers\/base.py\", line 119, in init_process\n     self.load_wsgi()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/workers\/base.py\", line 144, in load_wsgi\n     self.wsgi = self.app.wsgi()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/base.py\", line 67, in wsgi\n     self.callable = self.load()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/wsgiapp.py\", line 49, in load\n     return self.load_wsgiapp()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/wsgiapp.py\", line 39, in load_wsgiapp\n     return util.import_app(self.app_uri)\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/util.py\", line 383, in import_app\n     mod = importlib.import_module(module)\n   File \"\/usr\/lib\/python3.8\/importlib\/__init__.py\", line 127, in import_module\n     return _bootstrap._gcd_import(name[level:], package, level)\n   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\n   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n   File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n   File \"\/var\/azureml-server\/wsgi.py\", line 1, in <module>\n     import create_app\n   File \"\/var\/azureml-server\/create_app.py\", line 3, in <module>\n     from app import main\n   File \"\/var\/azureml-server\/app.py\", line 45, in <module>\n     from azure.ml.api.exceptions.ClientSideException import ClientSideException\n ModuleNotFoundError: No module named 'azure.ml'\n Worker exiting (pid: 38)\n Shutting down: Master\n Reason: Worker failed to boot.\n 2020-10-16T12:56:39,434787859+00:00 - gunicorn\/finish 3 0\n 2020-10-16T12:56:39,435715063+00:00 - Exit code 3 is not normal. Killing image.\n\n\n\nHow do I install the azureml.api dependency, which can not be found? It doesn't seem to be part of the Azure ML SDK. I have installed the following dependencies in my Dockerfile:\n\n RUN apt-get -y install python3-flask python3-rpy2 python3-azure python3-applicationinsights\n RUN pip install azureml-core\n\n\n\nI also have Miniconda installed. Pip refers to Miniconda's pip.\n\nOr, is this dependency available to install at all? Should I use some pre-defined AML environment as the base Docker image? (Note: I am currently using bare FROM: ubuntu). Suggestions how to find and use the base images are also welcome, since this is not documented very well.",
        "Challenge_closed_time":1603104240927,
        "Challenge_comment_count":1,
        "Challenge_created_time":1602855221470,
        "Challenge_favorite_count":6.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/129038\/r-model-deployment-with-custom-docker-image-34modu.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":56.78,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":52,
        "Challenge_solved_time":69.1720713889,
        "Challenge_title":"R model deployment with custom Docker image: \"ModuleNotFoundError: No module named 'azureml.api'\"",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":488,
        "Platform":"Tool-specific",
        "Solution_body":"@LauriLehman-8626 Thanks for the question. Here is the document to Create Custom Docker Base Images for Azure Machine Learning Environments for R people.\nWe have used the AzureML RScriptStep pipeline feature which allows you to point to CRAN or Github or custom URLS, but this requires authoring the pipeline in python or YAML.. In R You can also these arguments in the Azuremlsdk R estimator function: https:\/\/azure.github.io\/azureml-sdk-for-r\/reference\/estimator.html\nAnother option that are not available through conda install as part of the R script with install.packages(\u201cpath\/*.tar.gz\u201d, repos=NULL))\n\nOne of the challenges is that the build at runtime can take a while to prepare the environment. R likes to compile packages on Linux environments and a large package could have lots of dependencies which would take a while. This is an R on Linux\/PaaS thing, rather than specific to AzureML\n\nTo make start up fast we created a custom docker image where you can tightly control the image ahead of runtime. If you want to go in this direction you can find an example Dockerfile to get you started here..\nhttps:\/\/github.com\/Azure\/azureml-sdk-for-r\/blob\/master\/.azure-pipelines\/docker\/Dockerfile",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"rscriptstep pipelin featur sdk estim function instal packag instal packag creat docker imag control environ ahead runtim note build runtim compil packag linux environ depend bias summari",
        "Solution_link_count":2.0,
        "Solution_original_content":"laurilehman document creat docker base imag environ peopl rscriptstep pipelin featur allow cran github url author pipelin yaml argument sdk estim function http github sdk estim html option conda instal instal packag path tar repo null build runtim prepar environ compil packag linux environ larg packag depend linux paa start fast creat docker imag tightli control imag ahead runtim direct dockerfil start http github com sdk blob master pipelin docker dockerfil",
        "Solution_preprocessed_content":"document creat docker base imag environ peopl rscriptstep pipelin featur allow cran github url author pipelin argument sdk estim function option conda instal repo null build runtim prepar environ compil packag linux environ larg packag depend start fast creat docker imag tightli control imag ahead runtim direct dockerfil start",
        "Solution_readability":11.0,
        "Solution_reading_time":14.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":178.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":81.0187747222,
        "Challenge_answer_count":2,
        "Challenge_body":"I have a pipeline defined in Azure Machine Learning. It was launched every day with Azure Data Factory with Machine Learning Execute Pipeline activity. This solution worked without any issues for a few weeks, but since 12\/09\/2021 all pipeline runs have failed with error: User starting the run is not an owner or assigned user to the Compute Instance.\nI did not change anything in ADF or AML.\n\nShould I assign compute to ADF? How to do this?",
        "Challenge_closed_time":1639690537336,
        "Challenge_comment_count":1,
        "Challenge_created_time":1639398869747,
        "Challenge_favorite_count":16.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/661588\/how-to-run-a-pipeline-in-aml-from-adf.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":6.9,
        "Challenge_reading_time":6.1,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":81.0187747222,
        "Challenge_title":"Executing pipeline in AML from ADF suddenly stopped working",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":86,
        "Platform":"Tool-specific",
        "Solution_body":"I ran into this same issue in a slightly different context. I didn't manage to figure out the root cause but managed to resolve it in practice by standing up a Compute Cluster instead of a Compute Instance (see https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-attach-compute-cluster?tabs=python)",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"creat attach comput cluster comput instanc comput cluster root unknown assign comput data factori",
        "Solution_link_count":1.0,
        "Solution_original_content":"ran slightli context figur root practic stand comput cluster comput instanc http doc com creat attach comput cluster tab",
        "Solution_preprocessed_content":"ran slightli context figur root practic stand comput cluster comput instanc",
        "Solution_readability":13.5,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":302.6666666667,
        "Challenge_answer_count":1,
        "Challenge_body":"is there anyplace i can see the list of brands\/logos that are currently supported by the google vision api's logo recognition service?",
        "Challenge_closed_time":1637667660000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636578060000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/is-there-any-list-of-brands-logos-supported-by-google-vision-api\/td-p\/175425\/jump-to\/first-unread-message",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":2.48,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":302.6666666667,
        "Challenge_title":"is there any list of brands\/logos supported by google vision api?",
        "Challenge_topic":"Database Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":233.0,
        "Challenge_word_count":32,
        "Platform":"Tool-specific",
        "Solution_body":"The product team does not currently publish such a list. I recommend for you to submit a Feature Request to the Vision API product team.\u00a0\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"submit featur request vision api team",
        "Solution_link_count":0.0,
        "Solution_original_content":"team publish list submit featur request vision api team origin",
        "Solution_preprocessed_content":"team publish list submit featur request vision api team origin",
        "Solution_readability":6.0,
        "Solution_reading_time":2.03,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":30.0,
        "Tool":"Vertex AI"
    },
    {
        "Challenge_adjusted_solved_time":455.8612047222,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi,\n\nI am doing the Challenge. https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/intro-to-azure-machine-learning-service\/\n\n\n\n\nPlease see what I have installed:\n\npip install azureml-sdk\n\nI am getting the following messages at the end:\n\n\n\n\nERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n\n\nWe recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n\n\njupyterlab 2.2.9 requires jupyterlab-server<2.0,>=1.1.5, which is not installed.\nSuccessfully installed applicationinsights-0.11.9 azure-identity-1.4.1 azureml-automl-core-1.19.0 azureml-dataprep-2.6.3 azureml-dataprep-native-26.0.0 azureml-dataprep-rslex-1.4.0 azureml-dataset-runtime-1.19.0.post1 azureml-pipeline-1.19.0 azureml-pipeline-core-1.19.0 azureml-pipeline-steps-1.19.0 azureml-sdk-1.19.0 azureml-telemetry-1.19.0 azureml-train-1.19.0 azureml-train-automl-client-1.19.0 azureml-train-core-1.19.0 azureml-train-restclients-hyperdrive-1.19.0 distro-1.5.0 dotnetcore2-2.1.20 fusepy-3.0.1 msal-1.8.0 msal-extensions-0.2.2 numpy-1.19.3 portalocker-1.7.1 pyarrow-1.0.1 pywin32-227\n\n\n\n\nNow I am trying to start up and type the following in .py file in Visual Studio Code\n\nfrom azureml.core import Workspace\n\nThis is the error message I am getting:\n\nFile \"c:\/Users\/User\/OneDrive\/Desktop\/New folder\/Build AI Solution\/automl_python.py\", line 1, in <module>\nfrom azureml.core import Workspace\nModuleNotFoundError: No module named 'azureml'\n\n\n\n\nPlease could you help me?\n\nthanks,\n\nNaveen",
        "Challenge_closed_time":1610753174427,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609112074090,
        "Challenge_favorite_count":5.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/211503\/modulenotfounderror-no-module-named-39azureml39.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":21.68,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":455.8612047222,
        "Challenge_title":"ModuleNotFoundError: No module named 'azureml'",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":163,
        "Platform":"Tool-specific",
        "Solution_body":"This is now solved. Thanks!",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-1.9,
        "Solution_reading_time":0.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":5.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":1.4612436111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\nI&#39;m trying to setup an integration between a GITHub repository and my Jupyter Lab but I&#39;m struggling to find the GIT options in my Jupyter Lab application.\n\nI was expecting to see a Git clone button, a Git option on the toolbar and also the same option on the left pane but there is nothing GIT related.\n\nI&#39;ve already installed successfully the following:\n\npip install jupyterlab-git\npip install --upgrade python-gitlab\n\nBut nothing happens.\nWhen I try to clone a GIT repository, I get the folders\/files but then I can&#39;t interact with it. It&#39;s just copying it into my space but then I can&#39;t push\/pull anything.\n\nCan you help me on this?\n\nThank you,\nCarla",
        "Challenge_closed_time":1594378601087,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594373340610,
        "Challenge_favorite_count":30.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/45187\/azure-machine-learning-jupyter-lab-git-options.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":8.86,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.4612436111,
        "Challenge_title":"Azure Machine Learning Jupyter Lab Git options",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":121,
        "Platform":"Tool-specific",
        "Solution_body":"I found the answer to my question by following the steps here:\n\nhttps:\/\/www.oreilly.com\/library\/view\/jupyterlab-quick-start\/9781789805543\/94288841-0158-4a98-8151-4a90ea9bf2da.xhtml",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"step http oreilli com librari jupyterlab quick start aeabfda xhtml",
        "Solution_preprocessed_content":null,
        "Solution_readability":14.3,
        "Solution_reading_time":2.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":13.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":14.2856144445,
        "Challenge_answer_count":1,
        "Challenge_body":"Error message: \"FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/processing\/output\/profile_case.html'\"\n\nBackground: I am working in Sagemaker using python trying to profile a dataframe that is saved in a S3 bucket with pandas profiling. The data is very large so instead of spinning up a large EC2 instance, I am using a SKLearn processor.\n\nEverything runs fine but when the job finishes it does not save the pandas profile (a .html file) in a S3 bucket or back in the instance Sagemaker is running in.\n\nWhen I try to export the .html file that is created from the pandas profile, I keep getting errors saying that the file cannot be found.\n\nDoes anyone know of a way to export the .html file out of the temporary 24xl instance that the SKLearn processor is running in to S3? Below is the exact code I am using:\n\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore==1.19.4')\ninstall('ruamel.yaml')\ninstall('pandas-profiling==2.13.0')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n\n%%writefile casetableprofile.py\n\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore')\ninstall('ruamel.yaml')\ninstall('pandas-profiling')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n\n\n\n\ndef run_profile():\n\n\n\n    query = \"\"\"\n    SELECT  * FROM \"healthcloud-refined\".\"case\"\n    ;\n    \"\"\"\n    tableforprofile = wr.athena.read_sql_query(query,\n                                            database=\"healthcloud-refined\",\n                                            boto3_session=session,\n                                            ctas_approach=False,\n                                            workgroup='DataScientists')\n    print(\"read in the table queried above\")\n\n    print(\"got rid of missing and added a new index\")\n\n    profile_tblforprofile = ProfileReport(tableforprofile, \n                                  title=\"Pandas Profiling Report\", \n                                  minimal=True)\n\n    print(\"Generated carerequest profile\")\n                                      \n    return profile_tblforprofile\n\n\nif __name__ == '__main__':\n\n    profile_tblforprofile = run_profile()\n    \n    print(\"Generated outputs\")\n\n    output_path_tblforprofile = ('profile_case.html')\n    print(output_path_tblforprofile)\n    \n    profile_tblforprofile.to_file(output_path_tblforprofile)\n\n    \n    #Below is the only part where I am getting errors\nimport boto3\nimport os   \ns3 = boto3.resource('s3')\ns3.meta.client.upload_file('\/opt\/ml\/processing\/output\/profile_case.html', 'intl-euro-uk-datascientist-prod','Mark\/healthclouddataprofiles\/{}'.format(output_path_tblforprofile))  \n\nimport sagemaker\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsession = boto3.Session(region_name=\"eu-west-2\")\n\nbucket = 'intl-euro-uk-datascientist-prod'\n\nprefix = 'Mark'\n\nsm_session = sagemaker.Session(boto_session=session, default_bucket=bucket)\nsm_session.upload_data(path='.\/casetableprofile.py',\n                                bucket=bucket,\n                                key_prefix=f'{prefix}\/source')\n\nimport boto3\n#import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nregion = boto3.session.Session().region_name\n\n\nS3_ROOT_PATH = \"s3:\/\/{}\/{}\".format(bucket, prefix)\n\nrole = get_execution_role()\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     sagemaker_session=sm_session,\n                                     instance_type='ml.m5.24xlarge',\n                                     instance_count=1)\n\nsklearn_processor.run(code='s3:\/\/{}\/{}\/source\/casetableprofile.py'.format(bucket, prefix),\n                      inputs=[],\n                      outputs=[ProcessingOutput(output_name='output',\n                                                source='\/opt\/ml\/processing\/output',\n                                                destination='s3:\/\/intl-euro-uk-datascientist-prod\/Mark\/')])\n\n\nThank you in advance!!!",
        "Challenge_closed_time":1660704602950,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660653174738,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU5abOieUyQZSFvyRwfApRVA\/how-to-save-a-html-file-to-s-3-that-is-created-in-a-sagemaker-processing-container",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":15.6,
        "Challenge_reading_time":54.87,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":14.2856144445,
        "Challenge_title":"How to save a .html file to S3 that is created in a Sagemaker processing container",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":115.0,
        "Challenge_word_count":395,
        "Platform":"Tool-specific",
        "Solution_body":"Hi,\n\nFirstly, you should not (usually) need to directly interact with S3 from your processing script: The fact that you've configured your ProcessingOutput means that any files your script saves in \/opt\/ml\/processing\/output should automatically get uploaded to your s3:\/\/... destination URL. Of course there might be particular special cases where you want to directly access S3 from your script, but in general the processing job inputs and outputs should do it for you, to keep your code nice and simple.\n\nI'm no Pandas Profiler expert, but I think the error might be coming from here:\n\n    output_path_tblforprofile = ('profile_case.html')\n    print(output_path_tblforprofile)\n    \n    profile_tblforprofile.to_file(output_path_tblforprofile)\n\nDoesn't this just save the report to profile_case.html in your current working directory? That's not the \/opt\/ml\/processing\/output directory: It's usually the folder where the script is downloaded to the container I believe. The FileNotFound error is telling you that the HTML file is not getting created in the folder you expect, I think.\n\nSo I would suggest to make your output path explicit e.g. \/opt\/ml\/processing\/output\/profile_case.html, and also remove the boto3\/s3 section at the end - hope that helps!",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"directli interact process processingoutput configur automat upload file save opt process output destin url come output path html file explicit save directori opt process output directori output path explicit remov boto section end",
        "Solution_link_count":0.0,
        "Solution_original_content":"directli interact process configur processingoutput file save opt process output automat upload destin url cours directli access gener process job input output nice panda profil expert come output path tblforprofil profil html print output path tblforprofil profil tblforprofil file output path tblforprofil save report profil html directori opt process output directori folder download believ filenotfound html file creat folder output path explicit opt process output profil html remov boto section end hope",
        "Solution_preprocessed_content":"directli interact process configur processingoutput file save automat upload destin url cours directli access gener process job input output nice panda profil expert come save report directori directori folder download believ filenotfound html file creat folder output path explicit remov section end hope",
        "Solution_readability":11.3,
        "Solution_reading_time":15.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":176.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":8.8139933333,
        "Challenge_answer_count":1,
        "Challenge_body":"I have approximately 100k rows of text data (initially PDF documents that have been OCR). Most are rows of less than 5000 characters. Each of the source documents are addressed to some department. These are typically in the form of the below examples where the target department would 'Urology' (there are several departments).\n\nUrologly Department\n\n\nUrologly Clinic\n\n\nUrology Out Patients\n\n\nUrology\n\n\nDear urology team\n\nI have read a bit on ML Text Analysis and it seems I should be able to make a pretty good model by reviewing several hundred documents for each department (I have built an App to help me do this) and manually Classifying those documents. Some documents may mention urology but are actually addressed to another department. Typically the addressed department text is at the top third (first 3-7 lines) of the text body.\n\nI cannot use any online tools, i.e. I can't upload any of the Document text to servers to process I need a client side library. I have read and completed several tutorials using the ML.net but these are pretty basic (sentiment, entity detection without any initial training), and read an excellent blog at MonkeyLearn: which seems to acknowledge that can do what I imagine I should be able to do.\n\nSo can anybody point me in the right direction, can I use some offline Microsoft client library to complete my task? Is there some other Open Source client library i should look at. Will I have to learn Go, or python to complete the task (currently a C# dev).\n\nNote: I could get fairly good matches simply using SQL Text search and a bit of C# with plenty of hard coded rules, but I thought I'd try ML -- however its a nest of complications at the moment and i am going around in circles.\n\nMany Thanks\nMike.",
        "Challenge_closed_time":1637144398316,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637112667940,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/629917\/best-approach-to-clientside-machine-learning-for-t.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":21.86,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":8.8139933333,
        "Challenge_title":"Best Approach to Clientside Machine Learning for Text Classification",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":313,
        "Platform":"Tool-specific",
        "Solution_body":"@MikeShapleski-3383 I see two possible solutions for your scenario.\n\nExtracting text from your documents using the computer vision API and passing the required text as input to Azure Text Analytics for Health API\n\n\nUsing Azure cognitive search to upload the documents and creating a search service and enabling specific skills on the service to extract PII data or entities\n\nThe first solution can help you achieve this and ensure everything is offline or using docker containers without uploading any of your data to any storage externally. For billing purposes the containers need to connect to a metering endpoint on Azure to bill your usage of both these services(Computer Vision API & Azure text analytics containers). Also, you can use C# client library to call the local endpoint of these containers. The setup could take time to configure docker containers and passing the PDF documents to the computer vision read API to extract text. The extracted text can then be directly used or stored, to call the text analytics for health API.\n\nThe second solution can be used to index all the documents by using the search service by having your data in the cloud or behind a firewall to index the documents and make them searchable. There are some skills that can be enabled on the search service to extract entities and other PII information but this may not extract the same data as text analytics for health. This solution can be faster to setup because you can directly query your data after uploading the documents.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"extract text document vision api pass text input text analyt health api cognit search upload document creat search servic enabl skill servic extract pii data entiti advantag disadvantag choic depend bias summari",
        "Solution_link_count":0.0,
        "Solution_original_content":"mikeshapleski extract text document vision api pass text input text analyt health api cognit search upload document creat search servic enabl skill servic extract pii data entiti achiev offlin docker upload data storag extern bill purpos connect meter endpoint usag servic vision api text analyt client librari local endpoint setup time configur docker pass pdf document vision read api extract text extract text directli store text analyt health api index document search servic data cloud firewal index document searchabl skill enabl search servic extract entiti pii extract data text analyt health faster setup directli queri data upload document click upvot commun member read thread",
        "Solution_preprocessed_content":"extract text document vision api pass text input text analyt health api cognit search upload document creat search servic enabl skill servic extract pii data entiti achiev offlin docker upload data storag extern bill purpos connect meter endpoint usag servic client librari local endpoint setup time configur docker pass pdf document vision read api extract text extract text directli store text analyt health api index document search servic data cloud firewal index document searchabl skill enabl search servic extract entiti pii extract data text analyt health faster setup directli queri data upload document click upvot commun member read thread",
        "Solution_readability":14.0,
        "Solution_reading_time":19.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":274.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":0.3588888889,
        "Challenge_answer_count":2,
        "Challenge_body":"A customer has a question about data sources\n\n\u201cmost of our data is stored in SQL databases, while the SageMaker docs say that I have to put it all in S3. It\u2019s not obvious what the best way to do this is. I can think for example of splitting my analysis code in two; one pre-processing step to go from SQL queries to tabular data, and e.g. store that as Parquet files. For high-dimensional tensor data it\u2019s even less obvious.\u201d\n\nCan someone comment on that?",
        "Challenge_closed_time":1533318766000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533317474000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUh_P30-iXTKmzZv0D4vtLOA\/sagemaker-and-data-on-databases",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":5.9,
        "Challenge_reading_time":5.83,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3588888889,
        "Challenge_title":"Sagemaker and Data on Databases",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":208.0,
        "Challenge_word_count":89,
        "Platform":"Tool-specific",
        "Solution_body":"We have an example notebook for interacting from Redshift data from a SageMaker managed notebook, which I believe is suitable for an Exploratory Data Analysis (EDA) use-case: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/working_with_redshift_data\/working_with_redshift_data.ipynb\n\nFor production purposes, the customer should consider separating the job of first extracting data from relational databases to S3 (to build out a data lake), and then using that for downstream processing\/machine learning (including SageMaker, EMR, Athena, Spectrum, etc.). Customers can build extraction pipelines from popular relational databases using AWS Glue, EMR, or their preferred ETL engines like those on the AWS Marketplace.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"glue emr prefer etl engin build extract pipelin sql databas data store downstream process exploratori data analysi notebook interact redshift data notebook",
        "Solution_link_count":1.0,
        "Solution_original_content":"notebook interact redshift data notebook believ suitabl exploratori data analysi eda http github com awslab blob master advanc function redshift data redshift data ipynb purpos separ job extract data relat databas build data lake downstream process emr athena spectrum build extract pipelin popular relat databas glue emr prefer etl engin marketplac",
        "Solution_preprocessed_content":"notebook interact redshift data notebook believ suitabl exploratori data analysi purpos separ job extract data relat databas downstream build extract pipelin popular relat databas glue emr prefer etl engin marketplac",
        "Solution_readability":19.8,
        "Solution_reading_time":9.8,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":91.5201041667,
        "Challenge_answer_count":1,
        "Challenge_body":"I try to download image(.jpg, .png.) from S3 to Endpoint(made by Sagemaker) with s3url(s3:\/\/~~)\n\nBecause At the endpoint made by sagemaker, To send s3url is faster than to send image.\n\nI can download image at sagemaker notebook, from s3 to sagemaker local.\n\nbut I can't download image from s3 to sagemaker endpoint.\n\nThat local download code can not work.",
        "Challenge_closed_time":1661216110040,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660886637665,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QULB64ZDsXSPuHBjqAWik3hQ\/solved-download-image-from-s-3-to-endpoint-made-by-sagemaker-with-s-3-url-s-3",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.5,
        "Challenge_reading_time":5.38,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":91.5201041667,
        "Challenge_title":"[Solved]download image from S3 to Endpoint(made by Sagemaker) with s3url(s3:\/\/~~)",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":102.0,
        "Challenge_word_count":68,
        "Platform":"Tool-specific",
        "Solution_body":"I solve this! I try to download image at endpoint. but endpoint can not connect outside network except Lambda.\n\nI make request with s3url\nDownload image from s3 to lambda\nTransmit image from lambda to endpoint",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"lambda download imag transmit endpoint request surl download imag lambda transmit imag lambda endpoint",
        "Solution_link_count":0.0,
        "Solution_original_content":"download imag endpoint endpoint connect outsid network lambda request surl download imag lambda transmit imag lambda endpoint",
        "Solution_preprocessed_content":"download imag endpoint endpoint connect outsid network lambda request url download imag lambda transmit imag lambda endpoint",
        "Solution_readability":5.6,
        "Solution_reading_time":2.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":16.2021597222,
        "Challenge_answer_count":13,
        "Challenge_body":"<p>Dear Sir or Madam,<\/p>\n<p>Sorry for bothering you, I think there is an error in one of my wandb projects and the records of all runs were lost. The account is nbower0707, email 1155156871@link.cuhk.edu.hk, and the project name is ocp22.<\/p>\n<p>Everything worked fine before today, and I did a lot of experiments on this project. I\u2019m uploading records of my metric around every 5000 steps, and the result validation metric plot should be something like  figure 1 shows(continuous lines of records, with multiple data points) I\u2019m uploading the corresponding metrics every 2500 steps, and wandb displayed all results fine yesterday (either undergoing or finished runs)<\/p>\n<p>However, when I check the plot today, the record of metric in all runs were (completely or partly) lost, except for some small isolated data points left (as figure 2 and 3 shows).<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d.jpeg\" data-download-href=\"\/uploads\/short-url\/n0TMrYL9SyvpaH1YKsBmceDhhRb.jpeg?dl=1\" title=\"Picture 1\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_414x500.jpeg\" alt=\"Picture 1\" data-base62-sha1=\"n0TMrYL9SyvpaH1YKsBmceDhhRb\" width=\"414\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_414x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_621x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_828x1000.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Picture 1<\/span><span class=\"informations\">2337\u00d72818 348 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>I tried to use <strong>wandb sync<\/strong> from the local file, and upload the runs to a new project, the result is still the same.<\/p>\n<p>I didn\u2019t do any specific operations regarding wandb logging process or on the website. The project consist of runs uploaded from different machines, therefore it wouldn\u2019t be mistakenly deletion\/ false operation offline. And the phenomenon of lost of data also occurs on old runs that finished weeks ago.<\/p>\n<p>Please let me know if you have any suggestions on this error, and if the records could be recovered.<\/p>\n<p>Your time and patience are sincerely appreciated.<\/p>\n<p>Bowen Wang<\/p>",
        "Challenge_closed_time":1661461646292,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661403318517,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/all-records-are-lost-in-a-project-without-any-action\/2993",
        "Challenge_link_count":6,
        "Challenge_open_time":null,
        "Challenge_participation_count":13,
        "Challenge_readability":13.5,
        "Challenge_reading_time":39.24,
        "Challenge_score_count":5.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":16.2021597222,
        "Challenge_title":"All records are lost in a project without any action",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":162.0,
        "Challenge_word_count":289,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hey all,<\/p>\n<p>Our engineering team looked into this and rolled back some changes, everything should be working fine now.<\/p>\n<p>Please let us know if this issue persists.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"engin team roll persist report",
        "Solution_link_count":0.0,
        "Solution_original_content":"engin team roll persist ramit",
        "Solution_preprocessed_content":"engin team roll persist ramit",
        "Solution_readability":4.7,
        "Solution_reading_time":2.59,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":8.3539691667,
        "Challenge_answer_count":1,
        "Challenge_body":"I am currently utilizing an ml.c4.2xlarge instance type for a DeepAR use case to run an Automated Model Tuning job. The data consists of 7157 time series with 152 timesteps in the training set and 52 timesteps in the test set respectively. I estimate the run time for the tuning job on this specific instance type to take about 4-5 days. Looking to find out if DeepAR is engineered to take advantage of GPU computing for training and if it would be advisable to use a 'p' or 'g' compute instance instead for faster results. Also would be great for recommendations as to which Accelerated Computing instance would be optimal for this scenario.",
        "Challenge_closed_time":1644892187263,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644862112974,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUnYV-WoO2R3KY4sNEq-Dshw\/optimal-notebook-instance-type-for-deep-ar-in-aws-sagemaker",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":8.53,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":8.3539691667,
        "Challenge_title":"Optimal notebook instance type for DeepAR in AWS Sagemaker",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":96.0,
        "Challenge_word_count":121,
        "Platform":"Tool-specific",
        "Solution_body":"(As detailed further on the algorithm details page), yes, the SageMaker DeepAR algorithm implementation is able to train on GPU-accelerated instances to speed up more challenging jobs. There's also a handy reference table here listing all the SageMaker built-in algorithms and whether they're likely to be accelerated with GPU.\n\nHowever, to be clear, it shouldn't be the notebook instance type that affects this... Typically when training models on SageMaker, the notebook would provide your interactive compute environment but you'd run training in training jobs - for example using the SageMaker Python SDK Estimator class as shown in the sample notebooks for DeepAR electricity and synthetic. The instance type you select for training is independent of the instance type you use for your notebook - for example in the electricity notebook it's set as follows:\n\nestimator = sagemaker.estimator.Estimator(\n    image_uri=image_name,\n    sagemaker_session=sagemaker_session,\n    role=role,\n    train_instance_count=1,  # <-- Setting training instance count\n    train_instance_type=\"ml.c4.2xlarge\",  # <-- Setting training instance type\n    base_job_name=\"deepar-electricity-demo\",\n    output_path=s3_output_path,\n)\n\nSo normally I wouldn't expect you to need to change your notebook instance type to speed up training - just edit the configuration of your training job from within the notebook.\n\nSuggesting a particular type is tricky because DeepAR hyperparameters like context_length, embedding_dimension, and mini_batch_size will affect how much GPU capacity is needed for a particular run. Since you're coming from CPU-only baseline, I'd maybe suggest to start small with trying out single-GPU g4dn.xlarge, g5.xlarge or p3.2xlarge instances, perhaps starting with the lowest cost-per-hour? You can keep an eye on your jobs' GPUUtilization and GPUMemoryUtilization metrics to check whether utilization is low on instances like p3 with \"bigger\" GPUs. Increasing mini_batch_size should help fill extra capacity on these and complete your job faster, but it will probably affect model convergence - so may need to tune other parameters like learning_rate to try and compensate. So considering all of this, you may find trade-offs between speed and total cost, or speed and accuracy, for good hyperparameter combinations on your dataset. Of course you could also scale up to multi-GPU instance types if you'd like to accelerate further.\n\nIf I understood right you're also using SageMaker Automatic Hyperparameter Tuning to search these parameters, something like this XGBoost notebook with the HyperparameterTuner class?\n\nIn that case would also mention:\n\nIncreasing the max_parallel_jobs parameter may accelerate the overall run time (by running more of the individual training jobs in parallel) - with a trade-off on how much information is available when each training job in the budget is kicked off.\nIf you're planning to run this training regularly on a dataset which evolves over time, you probably don't need to run HPO each time: Will likely see good results using your previously-optimized hyperparameters, unless something materially changes in the nature of the data and patterns.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"deepar algorithm implement train gpu acceler instanc speed job instanc type select train independ instanc type notebook start small singl gpu gdn xlarg xlarg xlarg instanc start lowest cost hour increas mini batch size extra capac complet job faster probabl affect model converg tune paramet rate compens trade off speed cost speed accuraci hyper",
        "Solution_link_count":0.0,
        "Solution_original_content":"algorithm page deepar algorithm implement train gpu acceler instanc speed job handi tabl list built algorithm acceler gpu clear notebook instanc type affect typic train model notebook interact comput environ run train train job sdk estim class shown sampl notebook deepar electr synthet instanc type select train independ instanc type notebook electr notebook set estim estim estim imag uri imag session session role role train instanc count set train instanc count train instanc type xlarg set train instanc type base job deepar electr output path output path normal notebook instanc type speed train edit configur train job notebook type tricki deepar hyperparamet context length embed dimens mini batch size affect gpu capac run come cpu baselin mayb start small singl gpu gdn xlarg xlarg xlarg instanc start lowest cost hour ey job gpuutil gpumemoryutil metric util low instanc bigger gpu increas mini batch size extra capac complet job faster probabl affect model converg tune paramet rate compens trade off speed cost speed accuraci hyperparamet combin dataset cours scale multi gpu instanc type acceler understood automat hyperparamet tune search paramet notebook hyperparametertun class increas parallel job paramet acceler overal run time run individu train job parallel trade train job budget kick plan run train regularli dataset evolv time probabl run hpo time previous optim hyperparamet materi natur data pattern",
        "Solution_preprocessed_content":"deepar algorithm implement train instanc speed job handi tabl list algorithm acceler gpu clear notebook instanc type affect typic train model notebook interact comput environ run train train job sdk estim class shown sampl notebook deepar electr synthet instanc type select train independ instanc type notebook electr notebook set estim role role set train instanc count set train instanc type normal notebook instanc type speed train edit configur train job notebook type tricki deepar hyperparamet affect gpu capac run come baselin mayb start small instanc start lowest ey job gpuutil gpumemoryutil metric util low instanc bigger gpu increas extra capac complet job faster probabl affect model converg tune paramet compens speed cost speed accuraci hyperparamet combin dataset cours scale instanc type acceler understood automat hyperparamet tune search paramet notebook hyperparametertun class increas paramet acceler overal run time train job budget kick plan run train regularli dataset evolv time probabl run hpo time hyperparamet materi natur data pattern",
        "Solution_readability":15.3,
        "Solution_reading_time":39.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":447.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":51.3298138889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Is it possible to log tables to WANDB from a list \/sequence of dicts where the keys are the column names and the values wandb.Images (for example)?<\/p>\n<p>Once the my tables are logged to WANDB, if they share a column, can I join them in the WANDB web GUI?<\/p>\n<p>Thanks  beforehand!<\/p>",
        "Challenge_closed_time":1657222685552,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657037898222,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/logging-a-table-from-a-list-of-python-dicts\/2701",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":4.4,
        "Challenge_reading_time":4.04,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":51.3298138889,
        "Challenge_title":"Logging a table from a list of python dicts",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":116.0,
        "Challenge_word_count":59,
        "Platform":"Tool-specific",
        "Solution_body":"<p><a class=\"mention\" href=\"\/u\/fisikillo\">@fisikillo<\/a> ,<\/p>\n<p>Thanks for writing in. We support logging tables using list\/nested listed  with multiple images, see <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#create-tables\">here.<\/a> You can also join tables directly from the web GUI.<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"log tabl list nest list multipl imag tabl directli web gui share column",
        "Solution_link_count":1.0,
        "Solution_original_content":"fisikillo write log tabl list nest list multipl imag tabl directli web gui",
        "Solution_preprocessed_content":"write log tabl list multipl imag tabl directli web gui",
        "Solution_readability":12.0,
        "Solution_reading_time":4.03,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":16.3358102778,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nis it possible (or will be in the future) using Python SDK v2 to create pipeline endpoint (or endpoint + deployment)?\nIm looking for a way to submit a job for a created pipeline with a REST request.\n\nFor SDK v1 pipeline i was able to acquire satisfying result using Pipeline.publish method.\n\n\n\n\nThanks for any advice!",
        "Challenge_closed_time":1664861262760,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664802453843,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1033206\/publishing-aml-pipelines-with-sdk-v2.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.9,
        "Challenge_reading_time":4.29,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":16.3358102778,
        "Challenge_title":"Publishing AML Pipelines with SDK v2",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":61,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @MaciejStefaniak-6173\n\nThanks for using Microsoft Q&A platform. For how to publish pipeline, I don't find anything currently. But for deploy endpoint, please check on this sample repo for SDK v2, there are several samples for you to refer about how to deploy endpoint - https:\/\/github.com\/Azure\/azureml-examples\/tree\/v2samplesreorg\/sdk\/python\n\nAlso, there is an example about using Azure Machine Learning (Azure ML) to create a production ready machine learning (ML) project, using AzureML Python SDK v2 (preview). - https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-python-sdk\n\nI hope this helps, please let me know if you need more information or have any questiion regarding to above examples.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"sampl repositori sdk deploi endpoint creat readi sdk creat pipelin endpoint endpoint deploy sdk submit job creat pipelin rest request",
        "Solution_link_count":2.0,
        "Solution_original_content":"maciejstefaniak platform publish pipelin deploi endpoint sampl repo sdk sampl deploi endpoint http github com tree vsamplesreorg sdk creat readi sdk preview http com tutori pipelin sdk hope questiion yutong kindli accept commun",
        "Solution_preprocessed_content":"platform publish pipelin deploi endpoint sampl repo sdk sampl deploi endpoint creat readi sdk hope questiion yutong kindli accept commun",
        "Solution_readability":11.8,
        "Solution_reading_time":10.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":112.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":76.9123686111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi.\n\nIs it possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance every time which is somewhat time consuming? I'm familiar with local mode, but I understand this is not supported when using AWS SageMaker machine learning estimators.\n\nAppreciate any suggestions for how to make the model training process in SageMaker go faster when using AWS SageMaker machine learning estimators.\n\nThanks, Stefan",
        "Challenge_closed_time":1642148033228,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641871148701,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUsy3vkTMkSA2ojA1bmafDSA\/train-machine-learning-model-using-reserved-instance",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":6.8,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":76.9123686111,
        "Challenge_title":"Train machine learning model using reserved instance",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":151.0,
        "Challenge_word_count":84,
        "Platform":"Tool-specific",
        "Solution_body":"As of today, it's not possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance. The service team is currently working on it, unfortunately I don't have an ETA as to when the feature will be released.\n\nLocal Mode is supported for frameworks images (TensorFlow, MXNet, Chainer, PyTorch, and Scikit-Learn) and images you supply yourself.\n\nUsing the SageMaker Python SDK \u2014 sagemaker 2.72.3 documentation\n\nIf you want to train Built-in algorithm models simply faster, you should check the recommendation in the SageMaker document.\n\nExample Blazingtext-instances, Deepar-instances\n\nIf the algorithm supports it, one can also try using Pipe mode or FastFile mode. These offer some fast training job startup time. Accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"reserv instanc run train model local mode framework imag imag suppli document faster train built algorithm model blazingtext instanc deepar instanc algorithm pipe mode fastfil mode faster train job startup time bias summari",
        "Solution_link_count":0.0,
        "Solution_original_content":"todai train model reserv instanc run provis instanc servic team unfortun eta featur releas local mode framework imag tensorflow mxnet chainer pytorch scikit imag suppli sdk document train built algorithm model simpli faster document blazingtext instanc deepar instanc algorithm pipe mode fastfil mode fast train job startup time acceler model train faster pipe mode",
        "Solution_preprocessed_content":"todai train model reserv instanc run provis instanc servic team unfortun eta featur releas local mode framework imag imag suppli sdk document train algorithm model simpli faster document algorithm pipe mode fastfil mode fast train job startup time",
        "Solution_readability":12.6,
        "Solution_reading_time":10.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":124.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":0.1483333333,
        "Challenge_answer_count":1,
        "Challenge_body":"I read somewhere that some Amazon SageMaker's built-in algorithms can only be trained using GPU, whereas some can use either GPU or CPU, and some can only be used on CPU.\n\nIs there any official documentation explicitly stating which algorithms can only use GPU or both?",
        "Challenge_closed_time":1597251737000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597251203000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUdTLbPM2STGelSj1g3TIjpA\/which-amazon-sage-maker-algorithms-can-only-use-gpu-for-training",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":4.1,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.1483333333,
        "Challenge_title":"Which Amazon SageMaker algorithms can only use GPU for training?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":55,
        "Platform":"Tool-specific",
        "Solution_body":"Documentation for Amazon SageMaker built-in algorithms provides recommendations around choice of Amazon EC2 instances and whether given algorithm supports GPU or CPU devices.\n\nLet's take Image Classification as an example. Here is a excerpt from online documentation:\n\nFor image classification, we support the following GPU instances for training: ml.p2.xlarge, ml.p2.8xlarge, ml.p2.16xlarge, ml.p3.2xlarge, ml.p3.8xlargeand ml.p3.16xlarge. We recommend using GPU instances with more memory for training with large batch sizes. However, both CPU (such as C4) and GPU (such as P2 and P3) instances can be used for the inference. You can also run the algorithm on multi-GPU and multi-machine settings for distributed training.\n\nFor more complex scenarios, such as Script or BYO Container modes, customers have flexibility to choose which device (GPU or CPU) to utilize for which operation. This is configured as part of their training scripts.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"summari document instanc train gpu cpu devic built algorithm imag classif complex flexibl choos devic util oper",
        "Solution_link_count":0.0,
        "Solution_original_content":"document built algorithm choic instanc algorithm gpu cpu devic imag classif excerpt onlin document imag classif gpu instanc train xlarg xlarg xlarg xlarg xlargeand xlarg gpu instanc memori train larg batch size cpu gpu instanc infer run algorithm multi gpu multi set distribut train complex byo mode flexibl choos devic gpu cpu util oper configur train",
        "Solution_preprocessed_content":"document algorithm choic instanc algorithm gpu cpu devic imag classif excerpt onlin document imag classif gpu instanc train gpu instanc memori train larg batch size cpu gpu instanc infer run algorithm set distribut train complex byo mode flexibl choos devic util oper configur train",
        "Solution_readability":10.0,
        "Solution_reading_time":11.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":137.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":11.4773258334,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi, I've made a model and it's predicting prices of cars. hooray! I cannot find the the equation for Azure's Regression Linear model anywhere. I made this model using Designer GUI. For example, in R, the coefficients are returned by running summary(mymodel)\n= y-intercept + (slope miles) + (slope year)\n= 21022.96 + (-0.0249*98500) + (-6.5668*2016)\nsomething like this equation for a line is what I'm looking for in Azure.\n\nwhat I've tried:\n1. If it was only 1 feature, I could solve for an equation using (y2-y1) \/ (miles2-miles1) to find slope and the solve to y intercept. But this model uses miles and year as variables.",
        "Challenge_closed_time":1626277455030,
        "Challenge_comment_count":1,
        "Challenge_created_time":1626236136657,
        "Challenge_favorite_count":8.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/474924\/where-to-find-the-equation-for-the-line-after-maki.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":6.2,
        "Challenge_reading_time":8.96,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":11.4773258334,
        "Challenge_title":"where to find the equation for the line after making Azure ML linear regression model, 2 slopes and 1 y intercept",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":121,
        "Platform":"Tool-specific",
        "Solution_body":"@@MikeRichardson-3493 Thanks, We currently do not have coefficients for regression models, but we will forward this with our data science team to check on this. We are working on an interface to surface models that compose ensembles, model weights and more. While not as involved of an interface, some of this information is available today within the model details tags sections:.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"mikerichardson coeffici regress model forward data scienc team interfac surfac model compos ensembl model weight involv interfac todai model tag section",
        "Solution_preprocessed_content":"coeffici regress model forward data scienc team interfac surfac model compos ensembl model weight involv interfac todai model tag section",
        "Solution_readability":11.2,
        "Solution_reading_time":4.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":4.6135575,
        "Challenge_answer_count":1,
        "Challenge_body":"Currently! I'm experimenting with the azure data labelling tool in the machine learning workspace for image classification, what I found was azure shows only the unlabelled data to each user i.e if a user has already labelled an image, other users won't be shown the same image again.\nIs there any setting that exists, which can be enabled or disabled so that we can let more than one labeller label the same data?",
        "Challenge_closed_time":1625073229547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625056620740,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/458004\/azure-machine-learning-data-labelling-is-it-possib.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":6.8,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.6135575,
        "Challenge_title":"Azure machine learning data labelling- Is it possible to assign different labelers to label same data in a single project to reach a consensus?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":96,
        "Platform":"Tool-specific",
        "Solution_body":"Thanks for reaching to us. This capability is currently in development, and expected to release soon.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"capabl allow label label data releas soon",
        "Solution_link_count":0.0,
        "Solution_original_content":"reach capabl releas soon",
        "Solution_preprocessed_content":"reach capabl releas soon",
        "Solution_readability":8.8,
        "Solution_reading_time":1.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":18.6708936111,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm a novice to Azure and Azure ML, I'm trying to create a batch endpoint and in most of the documentation I found it was mentioned that the input to the batch endpoint would be a file path. In my case I was to connect the endpoint to two blobstorage and get a tuple as input to the batch endpoint. Is it possible? If not is there any other work around as my model takes as input path to two files.",
        "Challenge_closed_time":1661915045907,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661847830690,
        "Challenge_favorite_count":11.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/986878\/azureml-in-batch-endpoints-can-we-use-a-tupletwo-f.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":5.86,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":18.6708936111,
        "Challenge_title":"AzureML: In Batch Endpoints can we use a tuple(two file paths from different folders) as input?",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":96,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @SamarjeetSinghPatil-6739\n\nThanks for using Microsoft Q&A platform! I am sorry you can not use multiple resource as one input for one job, but you can do it in separate jobs.\n\nAn alternative way\/ workaround for you is a FileDataset, you can put your resource into one dataset and then input it.\n\nFileDataset -\nRepresents a collection of file references in datastores or public URLs to use in Azure Machine Learning.\nA FileDataset defines a series of lazily-evaluated, immutable operations to load data from the data source into file streams. Data is not loaded from the source until FileDataset is asked to deliver data.\nA FileDataset is created using the from_files method of the FileDatasetFactory class.\nFor more information, see the article Add & register datasets. To get started working with a file dataset, see https:\/\/aka.ms\/filedataset-samplenotebook.\n\nReference - https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.filedataset?view=azure-ml-py\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-batch-endpoints-studio#start-a-batch-scoring-job-with-different-input-options\n\nI hope it helps!\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"filedataset workaround connect endpoint blobstorag tupl input filedataset repres collect file datastor public url resourc dataset input multipl resourc input job separ job",
        "Solution_link_count":3.0,
        "Solution_original_content":"samarjeetsinghpatil platform sorri multipl resourc input job separ job workaround filedataset resourc dataset input filedataset repres collect file datastor public url filedataset defin seri lazili evalu immut oper load data data sourc file stream data load sourc filedataset deliv data filedataset creat file filedatasetfactori class articl add regist dataset start file dataset http aka filedataset samplenotebook http doc com api core data filedataset http doc com batch endpoint studio start batch score job input option hope yutong kindli accept commun",
        "Solution_preprocessed_content":"platform sorri multipl resourc input job separ job workaround filedataset resourc dataset input filedataset repres collect file datastor public url filedataset defin seri immut oper load data data sourc file stream data load sourc filedataset deliv data filedataset creat filedatasetfactori class articl add regist dataset start file dataset hope yutong kindli accept commun",
        "Solution_readability":11.3,
        "Solution_reading_time":15.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":159.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":4.2104608333,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\nI am using the example provided in the Machine Learning Studio Docs for extracting Health Entities from a given string.\nThe code is shown below.\n\nMy question is: what is the easiest way to convert the output result into JSON format?\n\n from azure.core.credentials import AzureKeyCredential\n from azure.ai.textanalytics import TextAnalyticsClient\n import json\n    \n credential = AzureKeyCredential(\"**********************************\")\n endpoint=\"https:\/\/eastus.api.cognitive.microsoft.com\/\"\n    \n text_analytics_client = TextAnalyticsClient(endpoint, credential)\n    \n documents = [\"Subject is taking 100mg of ibuprofen twice daily\"]\n    \n poller = text_analytics_client.begin_analyze_healthcare_entities(documents)\n result = poller.result()\n    \n docs = [doc for doc in result if not doc.is_error]\n    \n print(\"Results of Healthcare Entities Analysis:\")\n for idx, doc in enumerate(docs):\n     for entity in doc.entities:\n         print(\"Entity: {}\".format(entity.text))\n         print(\"...Normalized Text: {}\".format(entity.normalized_text))\n         print(\"...Category: {}\".format(entity.category))\n         print(\"...Subcategory: {}\".format(entity.subcategory))\n         print(\"...Offset: {}\".format(entity.offset))\n         print(\"...Confidence score: {}\".format(entity.confidence_score))\n         if entity.data_sources is not None:\n             print(\"...Data Sources:\")\n             for data_source in entity.data_sources:\n                 print(\"......Entity ID: {}\".format(data_source.entity_id))\n                 print(\"......Name: {}\".format(data_source.name))\n         if entity.assertion is not None:\n             print(\"...Assertion:\")\n             print(\"......Conditionality: {}\".format(entity.assertion.conditionality))\n             print(\"......Certainty: {}\".format(entity.assertion.certainty))\n             print(\"......Association: {}\".format(entity.assertion.association))\n         for relation in doc.entity_relations:\n             print(\"Relation of type: {} has the following roles\".format(relation.relation_type))\n         for role in relation.roles:\n             print(\"...Role '{}' with entity '{}'\".format(role.name, role.entity.text))\n     print(\"------------------------------------------\")",
        "Challenge_closed_time":1650982690652,
        "Challenge_comment_count":2,
        "Challenge_created_time":1650967532993,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/826603\/converting-textanalytics-result-to-python.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":20.8,
        "Challenge_reading_time":26.78,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":4.2104608333,
        "Challenge_title":"Converting textanalytics result to JSON Format",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":162,
        "Platform":"Tool-specific",
        "Solution_body":"@KamranAli-0346 The result does not seem to be directly serializable to JSON. I found a library JSONS that can do the heavy lifting if you are using python 3.5 or higher.\n\nInstall jsons\n\n pip install jsons\n\n\n\nImport JSONS and using jsons.dump() on docs object.\n\n import jsons #import in the import section\n print(jsons.dump(docs)) #Printing the json after docs is created\n\n\n\nThis should give a file of this format in this case. Uploaded the file in .txt format since JSON files cannot be uploaded on Q&A, download the file and rename it to .json\nI hope this helps!!\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.\n\n\n\n\n196624-health.txt",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"json librari convert output json format instal librari pip import json dump function doc object convert json format file json format download renam json",
        "Solution_link_count":0.0,
        "Solution_original_content":"kamranali directli serializ json librari json heavi lift higher instal json pip instal json import json json dump doc object import json import import section print json dump doc print json doc creat file format upload file txt format json file upload download file renam json hope click upvot commun member read thread health txt",
        "Solution_preprocessed_content":"directli serializ json librari json heavi lift higher instal json pip instal json import json doc object import json import import section print json doc creat file format upload file txt format json file upload download file renam json hope click upvot commun member read thread",
        "Solution_readability":5.0,
        "Solution_reading_time":8.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":116.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":64.1534905556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I think I may have got confused with this one. I had to code up a custom model using TF. It is training and running but I want to do some hyper parameter tuning so been working on getting HParms integrated.<\/p>\n<p>But I\u2019m trying to link up Wandb to keep track of things.<\/p>\n<p>Currently, since I\u2019m using hparms, when I initialize wandb with wandb.init(), it seems to initialize it for the whole process and it doesn\u2019t change when it is a new parameter set.<\/p>\n<p>I am calling the wandb.init() and logging after each parameter run, but still it doesn\u2019t create a unique job.<\/p>\n<p>This the function I call,<\/p>\n<pre><code class=\"lang-auto\">def write_to_wandb(ldl_model_params, KLi, f1_macro):\n    wandb.init(project=\"newjob1\", entity=\"demou\")\n    wandb.config = ldl_model_params\n\n    wandb_log = {\n        \"train KL\": KLi,\n        \"train F1\": f1_macro,\n        }\n\n    # logging accuracy\n    wandb.log(wandb_log)   \n<\/code><\/pre>\n<p>This is called from this train function (a high-level version of it). This <code>train_model<\/code> function is repeated again through another hyperparamter function with different hyper-parameter.<\/p>\n<pre><code class=\"lang-auto\">\ndef train_model(ldl_model_params,X,Y):\n    model = new_model(ldl_model_params)\n    model.fit(X,Y)\n    predict = model.transform(X)\n    KLi,F1 = model.evaluate(predict,Y)\n    write_to_wandb(ldl_model_params,KLi,F1)\n<\/code><\/pre>\n<p>So how do I fix this? I want each call to train_model to be recorded in a new run.<\/p>\n<p>I\u2019m new to wandb so I have a feeling that I am not using it as it should be. Thanks.<\/p>",
        "Challenge_closed_time":1636391832512,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636160879946,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/using-wandb-with-hparams-on-tf\/1233",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":19.42,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":64.1534905556,
        "Challenge_title":"Using Wandb with HParams on TF",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":533.0,
        "Challenge_word_count":210,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Just had a chat with the support and figured out how to fix the problem with over-writing.<\/p>\n<p>Issue was with the init function and there is a flag for reinitializing (<code>reinit=True<\/code>)<\/p>\n<p><code>wandb.init(project=\"newjob1\", entity=\"demou\",reinit=True)<\/code>  this fixed this issue.<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"add flag reinit init function reiniti creat run train model",
        "Solution_link_count":0.0,
        "Solution_original_content":"chat figur write init function flag reiniti reinit init newjob entiti demou reinit",
        "Solution_preprocessed_content":"chat figur init function flag reiniti",
        "Solution_readability":9.3,
        "Solution_reading_time":3.94,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":0.4021608333,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\n\nI have built a predictive model within Azure that works fine. I am trying to improve this by grouping values for some columns.\nI have 2 columns: format1, format2. I need to roundup them (ceil function). I did it in the Azure model but I need now to round these values when the user enters imput via the webservice.\nExample :\nformat1 => 21 => should be 25\nformat2 => 31 => should be 35\nThen these values (25,35) will be used in the Azure model.\nI tried to add a \"apply math operation\" in the predictive experiment but it seems not adapted for this....\n\nThank you for your help.\n\nRegards,\n\nMohamed.",
        "Challenge_closed_time":1611932433296,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611930985517,
        "Challenge_favorite_count":5.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/251134\/data-entry-webservice-azure-consume-transform-valu.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":8.06,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.4021608333,
        "Challenge_title":"Data entry webservice Azure consume: Transform values of data entered",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Platform":"Tool-specific",
        "Solution_body":"Hello,\n\nI got it, it works fine. Adding the module \"apply math operation in the predictive experiment and just after the data entry module is efficient.\n\nThx :)\n\nMohamed.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"appli math oper modul predict data entri modul moham",
        "Solution_link_count":0.0,
        "Solution_original_content":"modul appli math oper predict data entri modul effici thx moham",
        "Solution_preprocessed_content":"modul appli math oper predict data entri modul effici thx moham",
        "Solution_readability":8.8,
        "Solution_reading_time":2.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":28.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":347.8353516667,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello All,\n\nHow to pass a Datapath as a parameter in Azure ML Pipeline activity?\n\nMore details here : Have opened an issue here : https:\/\/github.com\/Azure\/Azure-DataFactory\/issues\/216\n\n\n\n\n\nThanks.",
        "Challenge_closed_time":1601023399256,
        "Challenge_comment_count":4,
        "Challenge_created_time":1599771191990,
        "Challenge_favorite_count":8.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/91785\/azure-data-factory-how-to-pass-datapath-as-a-param.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":10.7,
        "Challenge_reading_time":3.47,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":347.8353516667,
        "Challenge_title":"Azure Data Factory : How to pass DataPath as a parameter to Azure ML Pipeline activity?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Platform":"Tool-specific",
        "Solution_body":"Thanks @SriramNarayanan-6939 for your patience!\n\nI discussed with the Product team and they confirmed that there is no datatype supported for \"DataPath\" parameter today in Azure Data Factory(ADF). However, there is a feature already raised for the same and work is in progress for it.\n\nI would recommend you also to submit an idea in feedback forum. The ideas in this forum are closely monitored by data factory product team and will prioritize implementing them in future releases.\n\nSorry for the inconvenience!",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"team datatyp datapath paramet data factori featur rais progress submit idea feedback forum",
        "Solution_link_count":0.0,
        "Solution_original_content":"sriramnarayanan patienc team datatyp datapath paramet todai data factori adf featur rais progress submit idea feedback forum idea forum close monitor data factori team priorit implement futur releas sorri inconveni",
        "Solution_preprocessed_content":"patienc team datatyp datapath paramet todai data factori featur rais progress submit idea feedback forum idea forum close monitor data factori team priorit implement futur releas sorri inconveni",
        "Solution_readability":8.6,
        "Solution_reading_time":6.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":58.1621369444,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a CDM folder with data coming from Dynamics 365 Business Central.\nI need to do some data cleaning\/preprocessing and then apply my models on that data, but I didn't find a proper way to read CDM folders.\nI found some code on the Microsoft github repository, but is marked as obsolete.\n\nAzure-Samples\/cdm-azure-data-services-integration\n\nI'm searching for something like the Apache Spark CDM connector but to use within Azure Machine Learning service.\n\nps: I know that is possible to copy\/transform files with Azure Data Factory and that is supports CDM folders too, but is not what I want. I want to read CDM folder from python, do my stuff (data cleaning, preprocessing, applying models, ecc) then save the results.\n\nIs there any way?\nAny advice is welcome.\n\nThanks.",
        "Challenge_closed_time":1620687553120,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620478169427,
        "Challenge_favorite_count":7.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/387697\/read-data-from-cdm-folder.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":9.74,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":58.1621369444,
        "Challenge_title":"Read data from CDM folder",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":132,
        "Platform":"Tool-specific",
        "Solution_body":"Hi, thanks for reaching out. The data source you specified isn't a supported storage type in AML. If you're using unsupported storage, we recommend that you move your data to supported Azure storage solutions. Currently, we don't have a python connector for connecting to CRMs. A workaround would be to load your data to a database and connect to the database using python. Hope this helps, sorry for any inconvenience.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"data sourc servic data storag load data databas connect databas read cdm folder servic",
        "Solution_link_count":0.0,
        "Solution_original_content":"reach data sourc specifi isn storag type aml unsupport storag data storag connector connect crm workaround load data databas connect databas hope sorri inconveni",
        "Solution_preprocessed_content":"reach data sourc specifi isn storag type aml unsupport storag data storag connector connect crm workaround load data databas connect databas hope sorri inconveni",
        "Solution_readability":6.7,
        "Solution_reading_time":5.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":70.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":17.8672141667,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hi, I created a sweep from existing runs, but the panel Parallel Coordinates are empty, is this an intended behaviour or a bug?<\/p>\n<p>Here is what I did:<\/p>\n<ul>\n<li>populate projects with many runs (using ray\u2019s wandb_mixin)<\/li>\n<li>create a sweep following <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs\">https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs<\/a>\n<\/li>\n<li>the panel at \u201cSweeps &gt; [2]\u201d contains only 1 run, should contains all 42 runs.<\/li>\n<\/ul>\n<p>The sweep is at <a href=\"https:\/\/wandb.ai\/inc\/try_ray_tune\/sweeps\/smh3d0wg\" class=\"inline-onebox\">Weights &amp; Biases<\/a>, if any one is interested.<\/p>",
        "Challenge_closed_time":1640309961152,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640245639181,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/sweep-from-existing-runs-not-showing-up-in-parallel-coordinates-is-this-intended-or-a-bug\/1601",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":13.8,
        "Challenge_reading_time":10.55,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":17.8672141667,
        "Challenge_title":"Sweep from existing runs not showing up in parallel coordinates, is this intended or a bug?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":201.0,
        "Challenge_word_count":89,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/inc\">@inc<\/a>,<\/p>\n<p>You should be able to see all 42 runs on your parallel coordinates plot by ungrouping the runs. Grouping runs groups them for charts on your workspace as well.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"ungroup run run parallel coordin plot group run group chart workspac",
        "Solution_link_count":0.0,
        "Solution_original_content":"run parallel coordin plot ungroup run group run group chart workspac ramit",
        "Solution_preprocessed_content":"run parallel coordin plot ungroup run group run group chart workspac ramit",
        "Solution_readability":5.5,
        "Solution_reading_time":3.08,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":35.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":5.5346147222,
        "Challenge_answer_count":1,
        "Challenge_body":"I am using Azure Machine Learning Studio to design pipelines to analyze data.\nIs there any possibility to export data to sharepoint?",
        "Challenge_closed_time":1631084842440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631064917827,
        "Challenge_favorite_count":13.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/543361\/azure-machine-learning-automl-export-data-to-share.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":2.38,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":5.5346147222,
        "Challenge_title":"Azure Machine Learning (AutoML) export data to SharePoint",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":29,
        "Platform":"Tool-specific",
        "Solution_body":"Hi @MiaZhangWHQWistron-2092\nPer my research, there is no way to export data from Azure Machine Learning Studio to SharePoint directly.\n\nAs an alternative, you could export data to Azure SQL database first:\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-to-azure-sql-database\n\nThen export data from Azure SQL database to SharePoint list:\nhttps:\/\/social.technet.microsoft.com\/wiki\/contents\/articles\/39170.azure-sql-db-with-sharepoint-online-as-external-list-using-business-connectivity-services.aspx\n\n\nIf an Answer is helpful, please click \"Accept Answer\" and upvote it.\n\nNote: Please follow the steps in our documentation to enable e-mail notifications if you want to receive the related email notification for this thread.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"direct export data studio sharepoint export data sql databas export data sql databas sharepoint list link document step",
        "Solution_link_count":2.0,
        "Solution_original_content":"miazhangwhqwistron research export data studio sharepoint directli export data sql databas http doc com studio modul export sql databas export data sql databas sharepoint list http social technet com wiki articl sql sharepoint onlin extern list busi connect servic aspx click accept upvot note step document enabl mail notif receiv relat email notif thread",
        "Solution_preprocessed_content":"research export data studio sharepoint directli export data sql databas export data sql databas sharepoint list click accept upvot note step document enabl notif receiv relat email notif thread",
        "Solution_readability":15.4,
        "Solution_reading_time":9.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":14.603815,
        "Challenge_answer_count":1,
        "Challenge_body":"I posted a similar question last week and didn't get a response to that yet so I'm posting another one now.\n\nThe code below is what I use to pull data into the compute instance from the Datastore. I transfer data from a Datastore to the compute instance and then save the data to my directory as a csv. The data originates from a SCOPE script and is transferred from Cosmos to the Datastore via Azure Data Factory.\n\nOnce the data is in the directory as a csv, I then utilize R to pull in the data into an RStudio session and then I run various tasks that create new data sets. I also save these new data sets to the compute instance directory as csv's. These new data sets are the ones I'd like to push back to the Datastore so they can be transferred elsewhere via Azure Data Factory and later consumed by a PowerBI app we're looking to create.\n\nI tried using Designer and it ran for 4 days without completing before I cancelled the job and started looking for an alternative route. I don't know if it would have completed or if it ran into memory issues and simply didn't fail. When I pull data into the compute instance from the datastore it takes less than a few minutes to complete so I'm not sure why it would take Designer multiple days to attempt to do the reverse operation.\n\nI've looked through a bunch of documentation and I am not able to find anything that tells us how we can transfer data from the compute instance back to the Datastore aside from Designer which is too slow or unable to handle.\n\nThis task seems like one that should be obvious for use and a major selling point of Azure Machine Learning so I'm a bit dumbfounded to see that this is a challenge figuring out how to do and that the documentation doesn't clearly show users how to achieve this task, assuming it's even possible. If it's not possible then I need to figure out a whole new system to use to get my work done. If it's not possible, the Azure Machine Learning team should enable this functionality as soon as possible.\n\n# Azure management\nfrom azureml.core import Workspace, Dataset\n# MetaData\nsubscription_id = '09b5fdb3-165d-4e2b-8ca0-34f998d176d5'\nresource_group = 'xCloudData'\nworkspace_name = 'xCloudML'\n# Create workspace \nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n# 1. Retention_Engagement_CombinedData\ndataset = Dataset.get_by_name(workspace, name='retention-engagement-combineddata')\n# Save data to file\ndf = dataset.to_pandas_dataframe()\ndf.to_csv('\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/v-aantico1\/code\/RetentionEngagement_CombinedData.csv')\n# 2. TitleNameJoin\ndataset = Dataset.get_by_name(workspace, name='TitleForJoiningInR')\n# Save data to file\ndf = dataset.to_pandas_dataframe()\ndf.to_csv('\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/v-aantico1\/code\/TitleNameJoin.csv')",
        "Challenge_closed_time":1632211214827,
        "Challenge_comment_count":0,
        "Challenge_created_time":1632158641093,
        "Challenge_favorite_count":9.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/559227\/how-can-i-transfer-a-csv-file-on-an-azure-machine.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":36.25,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":14.603815,
        "Challenge_title":"How can I transfer a csv file on an Azure Machine Learning compute instance directory back to the Datastore?",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":439,
        "Platform":"Tool-specific",
        "Solution_body":"@AdrianAnticoTEKsystemsInc-1526 Have you tried the following to upload data to your datastore?\n\n from azureml.core import Workspace\n ws = Workspace.from_config()\n datastore = ws.get_default_datastore()\n    \n datastore.upload(src_dir='.\/data',\n                  target_path='datasets\/',\n                  overwrite=True)\n\n\n\nI think datastore.upload() should work for you to upload the required datafiles from your compute instance to datastore.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"datastor upload transfer csv file comput instanc directori datastor upload data file comput instanc datastor",
        "Solution_link_count":0.0,
        "Solution_original_content":"adriananticoteksystemsinc tri upload data datastor core import workspac workspac config datastor default datastor datastor upload src dir data target path dataset overwrit datastor upload upload datafil comput instanc datastor",
        "Solution_preprocessed_content":"tri upload data datastor core import workspac datastor overwrit upload datafil comput instanc datastor",
        "Solution_readability":12.4,
        "Solution_reading_time":5.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":41.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":132.9492333334,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi all,\n\nI am following the steps on this tutorial:\nTutorial: Score machine learning models with PREDICT in serverless Apache Spark pools tutorial-score-model-predict-spark-pool\nI tried to used a model created with AutoML and another from designer and I am getting this error: RuntimeError: Load model failed\n\n\n\n\nI am using the model according to this: https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/631200\/what-is-aml-model-uri-predict-in-serverless-apache.html?childToView=637754#comment-637754\n\nThank you for your help.",
        "Challenge_closed_time":1639460160303,
        "Challenge_comment_count":1,
        "Challenge_created_time":1638981543063,
        "Challenge_favorite_count":14.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/656548\/what-is-aml-model-uri-predict-in-serverless-apache-1.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":15.9,
        "Challenge_reading_time":8.65,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":132.9492333334,
        "Challenge_title":"RuntimeError: Load model failed - Score machine learning models with PREDICT in serverless Apache Spark pools (Synapse & Azure Machine learning AML)",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @Anaid-6816,\n\nBefore running this script, update it with the URI for ADLS Gen2 data file along with model output return data type and ADLS\/AML URI for the model file.\n\n #Set model URI\n        #Set AML URI, if trained model is registered in AML\n           AML_MODEL_URI = \"<aml model uri>\" #In URI \":x\" signifies model version in AML. You can   choose which model version you want to run. If \":x\" is not provided then by default   latest version will be picked.\n    \n        #Set ADLS URI, if trained model is uploaded in ADLS\n           ADLS_MODEL_URI = \"abfss:\/\/<filesystemname>@<account name>.dfs.core.windows.net\/<model   mlflow folder path>\"\n\nModel URI from AML Workspace:\n\n DATA_FILE = \"abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/AML\/LengthOfStay_cooked_small.csv\"\n AML_MODEL_URI_SKLEARN = \"aml:\/\/mlflow_sklearn:1\" #Here \":1\" signifies model version in AML. We can choose which version we want to run. If \":1\" is not provided then by default latest version will be picked\n RETURN_TYPES = \"INT\"\n RUNTIME = \"mlflow\"\n\nModel URI uploaded to ADLS Gen2:\n\n DATA_FILE = \"abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/AML\/LengthOfStay_cooked_small.csv\"\n AML_MODEL_URI_SKLEARN = \"abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/linear_regression\/linear_regression\" #Here \":1\" signifies model version in AML. We can choose which version we want to run. If \":1\" is not provided then by default latest version will be picked\n RETURN_TYPES = \"INT\"\n RUNTIME = \"mlflow\"\n\n\n\nHope this will help. Please let us know if any further queries.\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"messag runtimeerror load model involv updat uri adl gen data file model output return data type adl aml uri model file set model uri aml workspac upload adl gen instruct",
        "Solution_link_count":0.0,
        "Solution_original_content":"anaid run updat uri adl gen data file model output return data type adl aml uri model file set model uri set aml uri train model regist aml aml model uri uri signifi model version aml choos model version run default latest version pick set adl uri train model upload adl adl model uri abfss df core window net model uri aml workspac data file abfss data cheprasynaps df core window net aml lengthofstai cook small csv aml model uri sklearn aml sklearn signifi model version aml choos version run default latest version pick return type runtim model uri upload adl gen data file abfss data cheprasynaps df core window net aml lengthofstai cook small csv aml model uri sklearn abfss data cheprasynaps df core window net linear regress linear regress signifi model version aml choos version run default latest version pick return type runtim hope queri forget click upvot button origin poster commun faster identifi remind come respons subscrib notif program shape futur volunt moder",
        "Solution_preprocessed_content":"run updat uri adl gen data file model output return data type uri model file set model uri set aml uri train model regist aml uri signifi model version aml choos model version run default latest version pick set adl uri train model upload adl model uri aml workspac signifi model version aml choos version run default latest version pick runtim model uri upload adl gen signifi model version aml choos version run default latest version pick runtim hope queri forget click upvot button origin poster commun faster identifi remind come respons subscrib notif program shape futur volunt moder",
        "Solution_readability":10.2,
        "Solution_reading_time":23.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":266.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":219.4091302778,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi.\n\nSo I would like to create a model that 'listens' to audio from movies\/podcasts (with subtitles) then returns the text transcript from it. Problem is, it's in a language not supported by Azure (or most of the big cloud providers). How would I go about and, from scratch, build a model that is trained on the audio from a new language? The input audio all will have subtitles or captions.\n\nI tried Azure ML studio but I couldn't create datasets with audio files. Not sure if I missed something there. Also tried Speech studio but it only supports a select number of languages. Would that be possible at all?\n\nAny suggestions would be appreciated. Thanks.",
        "Challenge_closed_time":1628120380252,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627330507383,
        "Challenge_favorite_count":9.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/490113\/voicespeech-to-text-train-model.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":5.6,
        "Challenge_reading_time":8.34,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":219.4091302778,
        "Challenge_title":"Voice\/Speech to Text Train Model",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":120,
        "Platform":"Tool-specific",
        "Solution_body":"@NathanCarns-0092 Yes, you are correct, to develop a model for speech to text we need a deep learning model here. This is out of the scope of Azure Machine Learning Studio(classic). But I think Azure Machine Learning service should support it, please refer to this: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-deep-learning-vs-machine-learning#machine-translation\n\nI have found one post which may help: https:\/\/towardsdatascience.com\/audio-deep-learning-made-simple-automatic-speech-recognition-asr-how-it-works-716cfce4c706\n\nMoreover, I have forwarded your feedback to see any plan here for Nigerian in Azure.\n\nThanks.\nYutong",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"model speech text model explor servic audio automat speech recognit forward feedback potenti languag",
        "Solution_link_count":2.0,
        "Solution_original_content":"nathancarn model speech text model scope studio classic servic http doc com concept translat http towardsdatasci com audio automat speech recognit asr cfcec forward feedback plan nigerian yutong",
        "Solution_preprocessed_content":"model speech text model scope studio servic forward feedback plan nigerian yutong",
        "Solution_readability":12.4,
        "Solution_reading_time":8.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":162.8144655556,
        "Challenge_answer_count":1,
        "Challenge_body":"I am trying to run a autoML model as follows:\n\nautoml_settings = {\n\"n_cross_validations\": 5\n}\n\nautoml_config = AutoMLConfig(task = 'regression',\ncompute_target = compute_target,\ntraining_data = train_data.filter(train_data['location']==l),\nlabel_column_name = label,\n**automl_settings)\n\nremote_run = experiment.submit(automl_config, show_output=True)\n\nAnd I get: ValidationException: The data points should have at least 50 rows for a valid regression or classification task with cv 5.\n\nThe data has more than 250 rows. Moreover, when I changed the cv number, nothing changed. Does anybody have any idea what might be happening?",
        "Challenge_closed_time":1630572887363,
        "Challenge_comment_count":1,
        "Challenge_created_time":1629986755287,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/529189\/validationexception-the-data-points-should-have-at.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.0,
        "Challenge_reading_time":9.48,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":162.8144655556,
        "Challenge_title":"ValidationException: The data points should have at least 50 rows for a valid regression or classification task with cv 5.",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":93,
        "Platform":"Tool-specific",
        "Solution_body":"Hello,\n\nHope you have solved this issue. If you are still blocked by this, please feel free to let us know. We can either investigate deeper if we can have more details, or we can help you to enable a support ticket if you do not have a support plan. Thanks.\n\n\n\n\n\nRegards,\nYutong",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"hope block free deeper enabl ticket plan yutong",
        "Solution_preprocessed_content":"hope block free deeper enabl ticket plan yutong",
        "Solution_readability":6.7,
        "Solution_reading_time":3.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":53.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":146.8910691667,
        "Challenge_answer_count":9,
        "Challenge_body":"<p>I have a compute environment where I was running wandb offline for quite a while. I am now hoping to use it online (to get automatic syncing), however I seem to be unable to set this up now. The following is a minimal reproducible example:<\/p>\n<pre><code class=\"lang-auto\">&gt;&gt; import wandb\n&gt;&gt; test = wandb.init(mode='online')\nTraceback (most recent call last):\n  File \"[path]\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_init.py\", line 867, in init\n    wi.setup(kwargs)\n  File \"[path]\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_init.py\", line 182, in setup\n    user_settings = self._wl._load_user_settings()\n  File \"[path]\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_setup.py\", line 183, in _load_user_settings\n    flags = self._server._flags\nAttributeError: 'NoneType' object has no attribute '_flags'\nwandb: ERROR Abnormal program exit\n<\/code><\/pre>\n<p>I have tried<\/p>\n<ul>\n<li>running wandb online in the terminal<\/li>\n<li>setting the wandb mode environment variable to be online<\/li>\n<li>uninstalling and reinstalling wandb<\/li>\n<\/ul>\n<p>Is there any way I can run this online?<\/p>",
        "Challenge_closed_time":1637034448659,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636505640810,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/unable-to-run-wandb-online-after-running-offline\/1252",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":9,
        "Challenge_readability":10.4,
        "Challenge_reading_time":14.63,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":146.8910691667,
        "Challenge_title":"Unable to run wandb online after running offline",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":289.0,
        "Challenge_word_count":133,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/dimaduev\">@dimaduev<\/a> , thanks so much for the fixes! I think I was able to resolve this through looking at the different WANDB_DIR locations\u2026 I had several in different bashrc\/zshrc files and I suspect this was causing an issue. It seems to be resolved now!<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"dir locat remov conflict on",
        "Solution_link_count":0.0,
        "Solution_original_content":"dimaduev dir locat bashrc zshrc file",
        "Solution_preprocessed_content":null,
        "Solution_readability":8.1,
        "Solution_reading_time":3.72,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":46.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":625.9599527778,
        "Challenge_answer_count":1,
        "Challenge_body":"How to I properly cancel all child runs in an Azure ML experiment? When I use the code below as expected from documentation, I get an error. \"RunConfigurationException:\nMessage: Error in deserialization. dict fields don't have list element type information. field=output_data, list_element_type=<class 'azureml.core.runconfig.OutputData'>...} with exception init() missing 2 required positional arguments: 'datastore_name' and 'relative_path'\"\n\nrun = Run.get(ws, 'run-id-123456789')\n\nfor child in run.get_children():\nprint(child.get_details())\ntry:\nchild.cancel()\nexcept Exception as e:\nprint(e)\ncontinue\n\nThe datasets and runs were configured properly because they run just fine.",
        "Challenge_closed_time":1651506755547,
        "Challenge_comment_count":1,
        "Challenge_created_time":1649253299717,
        "Challenge_favorite_count":11.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/802549\/cancel-all-child-runs-in-azure-ml.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.3,
        "Challenge_reading_time":9.15,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":625.9599527778,
        "Challenge_title":"Cancel all child runs in Azure ML",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":88,
        "Platform":"Tool-specific",
        "Solution_body":"You should cancel all the children run by canceling the parent.\n\nAny benefit to cancel child once a time? Just curious",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"cancel child run cancel parent run",
        "Solution_link_count":0.0,
        "Solution_original_content":"cancel children run cancel parent benefit cancel child time",
        "Solution_preprocessed_content":"cancel children run cancel parent benefit cancel child time",
        "Solution_readability":6.2,
        "Solution_reading_time":1.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":0.4425733334,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have some variables in my sweeps that i want to be able to group by at the same time in my charts.<\/p>\n<p>In this specific case it\u2019s 3 hyperparameters of the architecture: \u201clevels\u201d, \u201cconvolutions per level\u201d and \u201cstarting features\u201d.<\/p>\n<p>I can have multiple charts, grouping by one at a time, and see how each individual variable affects the runs, but it would be much more beneficial to see the effects of all three together.<\/p>\n<p>The \u201ccustom chart\u201d seemed the way to go, but i couldn\u2019t make it work so far. Any help would be really appreciated!<\/p>",
        "Challenge_closed_time":1668699023534,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668697430270,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/group-by-multiple-variables-in-charts\/3435",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":8.7,
        "Challenge_reading_time":7.27,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.4425733334,
        "Challenge_title":"Group by multiple variables in charts",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":96.0,
        "Challenge_word_count":102,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mateoballa\">@mateoballa<\/a> thank you for writing in! In the Project level, you can group your Runs by all these three hyperparameters from the Group button as in the attachment.<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/4\/40fde472a1d51962372411528e99e65a7c90656d.png\" alt=\"Screenshot 2022-11-17 at 15.26.35\" data-base62-sha1=\"9gWweVkvwN7ym0GpqkZ8aunFcq9\" width=\"596\" height=\"303\"><\/p>\n<p>Then the Charts will adjust to this grouping. Would this help, or you wanted something different to achieve? Could you share a screenshot of your current custom chart?<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"group run hyperparamet group button level adjust chart group share screenshot chart",
        "Solution_link_count":1.0,
        "Solution_original_content":"mateoballa write level group run hyperparamet group button attach chart adjust group achiev share screenshot chart",
        "Solution_preprocessed_content":"write level group run hyperparamet group button attach chart adjust group achiev share screenshot chart",
        "Solution_readability":8.3,
        "Solution_reading_time":8.34,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":68.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":0.1111111111,
        "Challenge_answer_count":1,
        "Challenge_body":"A wants to manage Sagemaker resources (such as models and endpoints) via CloudFormation. As part of their model deployment pipeline, they'd like to be able to create or update existing Sagemaker Endpoint with new model data. Customers wants to re-use the same endpoint name for a given workload.\n\nQuestion:\n\nHow to express in CF a following logic:\n\nIf Sagemaker endpoint with name \"XYZ\" doesn't exist in customer account, then create a new endpoint;\nIf Sagemaker endpoint with name \"XYZ\" already exist, then update existing endpoint with new model data.",
        "Challenge_closed_time":1607357193000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607356793000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUXiLSnlxkQHKzQVFj6GKT7w\/create-or-update-sagemaker-endpoint-via-cloud-formation",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":7.51,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1111111111,
        "Challenge_title":"Create or update Sagemaker Endpoint via CloudFormation",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":242.0,
        "Challenge_word_count":95,
        "Platform":"Tool-specific",
        "Solution_body":"This functionality of \"UPSERT\" type does not exist in CFn natively. You would need to use a Custom Resource to handle this logic. One alternative that is not exactly what you asked for but might be a decent compromise is to use a Parameter to supply the endpoint if it does exist. Then use a condition to check the value. If the paramter is blank then create an endpoint if not use the value supplied. I know this is not what you asked for but it allows you to avoid the custom resource solution.\n\nSample of similiar UPSERT example for a VPC:\n\nParameters :\n\n  Vpc:\n    Type: AWS::EC2::VPC::Id\n\nConditions:\n\n  VpcNotSupplied: !Equals [!Ref Vpc, '']\n\nResources:\n\n  NewVpc:\n    Type: AWS::EC2::VPC\n    Condition: VpcNotSupplied\n    Properties:\n      CidrBlock: 10.0.0.0\/16\n\n  SecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupDescription: Sample\n      GroupName: Sample\n      VpcId: !If [VpcNotSupplied, !Ref NewVpc, !Ref Vpc ]\n\n\nHere the Vpc input parameter can be supplied if the VPC you wish to use already exists, left blank if you want to create a new one. The NewVPC resource uses the Condition to only create if the supplied Vpc parameter value is blank. The Security group then uses the same condition to decide whetehr to use and existing Vpc or the newly created one.\n\nHope this makes sense.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"function upsert type cloudform nativ paramet suppli endpoint condit valu paramet blank creat endpoint valu suppli allow avoid resourc sampl upsert vpc",
        "Solution_link_count":0.0,
        "Solution_original_content":"function upsert type cfn nativ resourc logic exactli decent compromis paramet suppli endpoint condit valu paramt blank creat endpoint valu suppli allow avoid resourc sampl similiar upsert vpc paramet vpc type vpc condit vpcnotsuppli equal ref vpc resourc newvpc type vpc condit vpcnotsuppli properti cidrblock securitygroup type securitygroup properti groupdescript sampl groupnam sampl vpcid vpcnotsuppli ref newvpc ref vpc vpc input paramet suppli vpc wish left blank creat newvpc resourc condit creat suppli vpc paramet valu blank secur group condit decid whetehr vpc newli creat hope sens",
        "Solution_preprocessed_content":"function upsert type cfn nativ resourc logic exactli decent compromis paramet suppli endpoint condit valu paramt blank creat endpoint valu suppli allow avoid resourc sampl similiar upsert vpc paramet vpc type vpc condit vpcnotsuppli equal resourc newvpc type vpc condit vpcnotsuppli properti cidrblock securitygroup type securitygroup properti groupdescript sampl groupnam sampl vpcid vpc input paramet suppli vpc wish left blank creat newvpc resourc condit creat suppli vpc paramet valu blank secur group condit decid whetehr vpc newli creat hope sens",
        "Solution_readability":9.4,
        "Solution_reading_time":15.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":204.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":27.8660444444,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>I made a change to my script and now I have to manually synchronise my runs, my script contains<\/p>\n<pre><code>if args.dry_run:\n    os.environ['WANDB_MODE'] = 'dryrun'\n\nwandb.init(project=args.project_name, notes=args.notes)\n\n# log all experimental args to wandb\nwandb.config.update(args)\n<\/code><\/pre>\n<p>The change I made was the first line, setting <code>WANDB_MODE=dryrun<\/code>. From that point on I cannot re-enable automatic synchronisation.<\/p>\n<p>I\u2019ve run <code>wandb online<\/code> and run my script with <code>dryrun=False<\/code>. I also realised that this doesn\u2019t unset WANDB_MODE so I tried setting it to \u2018online\u2019 when <code>dryrun==False<\/code>. But it always ends up logging to <code>wandb\/offline-run-*<\/code> and I have to manually sync it.<\/p>\n<p>Is there another step to re-enable sync\u2019ing?<\/p>",
        "Challenge_closed_time":1662355289728,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662254971968,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-reenable-automatic-synchronisation\/3061",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":9.7,
        "Challenge_reading_time":10.94,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":27.8660444444,
        "Challenge_title":"How to reenable automatic synchronisation",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":130.0,
        "Challenge_word_count":105,
        "Platform":"Tool-specific",
        "Solution_body":"<p>I\u2019ve found a way around this - I\u2019m not really sure why it\u2019s happening but I noticed that the huggingface trainer logs the metrics at the end of training as follows:<\/p>\n<pre><code>                if not args.load_best_model_at_end\n                else {\n                    f\"eval\/{args.metric_for_best_model}\": state.best_metric,\n                    \"train\/total_floss\": state.total_flos,\n                }\n<\/code><\/pre>\n<p>Meaning it logs the validation loss, but only if you train with <code>load_best_model_at_end=True<\/code> and set <code>save_strategy==evaluation_strategy<\/code> (epoch or steps) and <code>save_steps=eval_steps<\/code>.<\/p>\n<p>Doing this means I didn\u2019t need to perform the separate eval step since it\u2019s already logged the evaluation loss from the best model during training.<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"iv notic huggingfac trainer log metric end train arg load model end eval arg metric model state metric train floss state flo log loss train load model end set save strategi evalu strategi epoch step save step eval step didnt perform separ eval step log evalu loss model train",
        "Solution_preprocessed_content":"iv notic huggingfac trainer log metric end train log loss train set didnt perform separ eval step log evalu loss model train",
        "Solution_readability":12.4,
        "Solution_reading_time":9.24,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":86.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":3.9460580556,
        "Challenge_answer_count":5,
        "Challenge_body":"I am trying to connect to my Azure ML workspace using SDK for python, using Virtual Studio Code to do so. After pip installing the needed SDK packages:\npip install azureml-sdk\npip install azureml-sdk[notebooks,automl,explain]\n\nI downloaded the .json configuration file for my workspace, made sure it was in the correct location for the file path and tried the following code (with my subscription id, resource group and workspace name in place of the fillers in this chunk of code):\n\n {\n     \"subscription_id\": \"1234567-abcde-890-fgh...\",\n     \"resource_group\": \"aml-resources\",\n     \"workspace_name\": \"aml-workspace\"\n }\n\n\n\nUpon executing this in my ipy kernel in Virutal Studio Code I got a UserErrorException (see image below, I have blocked out subscription id's and other sensitive information):\n\n\n\n\n\n\nI then tried this alternative way to connect to my workspace using the following code (again with my info filled in instead of the fillers in the code):\nfrom azureml.core import Workspace\n\n from azureml.core import Workspace\n    \n ws = Workspace.get(name='aml-workspace',\n                    subscription_id='1234567-abcde-890-fgh...',\n                    resource_group='aml-resources')\n    \n ws = Workspace.from_config()\n\n\n\nThis produced the same error upon execution. I have tried using different subscriptions with different workspace names and resource groups and it gives me the same error every time. It appears to be telling me I do not have access to the subscription that I am logged in to? I am unsure how to fix this. I am trying to do this as part of the lessons in the Microsoft Azure Data Scientist certification if anyone is familiar with that or has run into the same problem while trying to complete the modules for that certification provided through Microsoft.",
        "Challenge_closed_time":1606097262372,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606083056563,
        "Challenge_favorite_count":13.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/171465\/problems-connecting-to-workspace-using-azure-machi.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":10.0,
        "Challenge_reading_time":22.2,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":3.9460580556,
        "Challenge_title":"Problems connecting to workspace using Azure Machine Learning SDK for Python",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":264,
        "Platform":"Tool-specific",
        "Solution_body":"can you try using InteractiveLoginAuthentication?\n\nbelow code might help you\n\n from azureml.core.authentication import InteractiveLoginAuthentication\n ia = InteractiveLoginAuthentication(tenant_id='YourTenant id')\n # You can find tenant id under azure active directory->properties\n ws = Workspace.get(name='aml-workspace',\n                     subscription_id='1234567-abcde-890-fgh...',\n                     resource_group='aml-resources',auth=ia)",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"interactiveloginauthent authent login access workspac",
        "Solution_link_count":0.0,
        "Solution_original_content":"interactiveloginauthent core authent import interactiveloginauthent interactiveloginauthent tenant yourten tenant activ directori properti workspac aml workspac subscript abcd fgh resourc group aml resourc auth",
        "Solution_preprocessed_content":"interactiveloginauthent import interactiveloginauthent tenant activ",
        "Solution_readability":22.5,
        "Solution_reading_time":5.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":38.2833333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Google cloud translation have added new languages. About 24 new languages has been added to Google Translate. Very good job, well done. But they are not listed on this link.\nhttps:\/\/cloud.google.com\/translate\/docs\/languages\n\nI tried to access it using basic v2 API code, but no response came to my translation request. When will this new languages be available to be accessed by v2 APIs? ",
        "Challenge_closed_time":1652889600000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652751780000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/The-new-languages-are-missing\/td-p\/423648\/jump-to\/first-unread-message",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":4.6,
        "Challenge_reading_time":5.16,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":38.2833333333,
        "Challenge_title":"The new languages are missing",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":89.0,
        "Challenge_word_count":66,
        "Platform":"Tool-specific",
        "Solution_body":"These are the new 24 languages[1].\n\nIn that post there is a research paper[2] where you can see the codes it begins on page 57.\n\nThe document that you shared it is in an internal Work in Progress with no launch date yet.\n\n[1]https:\/\/blog.google\/products\/translate\/24-new-languages\/\u00a0\n\n[2]https:\/\/arxiv.org\/pdf\/2205.03983.pdf\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"languag research paper link languag access api document share progress launch date",
        "Solution_link_count":2.0,
        "Solution_original_content":"languag research paper begin page document share intern progress launch date http blog translat languag http arxiv org pdf pdf origin",
        "Solution_preprocessed_content":"languag research paper begin page document share intern progress launch date origin",
        "Solution_readability":5.8,
        "Solution_reading_time":4.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":50.0,
        "Tool":"Vertex AI"
    },
    {
        "Challenge_adjusted_solved_time":4.4182138889,
        "Challenge_answer_count":1,
        "Challenge_body":"Is it possible to retrieve notebooks that were hosted on notebooks.azure.com? If so, how? The service is now discontinued but I would like to retrieve files that were hosted on the service.",
        "Challenge_closed_time":1615975649020,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615959743450,
        "Challenge_favorite_count":4.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/317875\/retrieve-notebooks-azure-files.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":2.73,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.4182138889,
        "Challenge_title":"Retrieve Notebooks Azure Files",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":35,
        "Platform":"Tool-specific",
        "Solution_body":"@sean-9375 I am afraid that the option to retrieve this data is not possible. Please refer this thread for information and the options that were available before the last day to migrate them. Thanks!!",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"sean afraid option retriev data thread option dai migrat",
        "Solution_preprocessed_content":"afraid option retriev data thread option dai migrat",
        "Solution_readability":7.6,
        "Solution_reading_time":2.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":0.0858333333,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi Team,\nI'm trying to package sagemaker dependencies as external dependcies to upload to lambda.\nBut I'm getting the max size limit error. Package size is more than allowed size limit i.e.. deployment package size is 50 MB.\nAnd the reason I'm trying to do this is, 'get_image_uri' api is not accessible with boto3.\nsample code for this api :\n#Import the get_image_url utility function Amazon SageMaker Python SDK and get the location of the XGBoost container.\n\nimport sagemaker\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\ncontainer = get_image_uri(boto3.Session().region_name, 'xgboost')\n\nAny reference would be of great help. Thank you.",
        "Challenge_closed_time":1568642184000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568641875000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUg5l3Jjl4SISDvnjIVYcqaA\/not-able-to-add-sagemaker-dependencies-as-external-dependencies-to-lambda",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.12,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.0858333333,
        "Challenge_title":"not able to add sagemaker dependencies as external dependencies to lambda",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":134.0,
        "Challenge_word_count":104,
        "Platform":"Tool-specific",
        "Solution_body":"Could you explain in more detail why do you want to have sagemaker inside of a lambda please?",
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":6.8,
        "Solution_reading_time":1.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":0.9897075,
        "Challenge_answer_count":1,
        "Challenge_body":"I was trying to deploy and score a machine learning model by using an online endpoint.\n\nWhen I was trying to run code this on Azure Machine Learning Wordspace,\n\n !az ml online-deployment create --name fraud-ga --endpoint endpoint-name -f ..\/deployment\/deployment.yml --all-traffic\n\n\n\nI got this error:\n\n {\"errors\":{\"VmSize\":[\"Not enough quota available for Standard_F16s_v2 in SubscriptionId 671ef6e1-2ded-466b-8fd1-91363cf12275. Current usage\/limit: 4\/6. Additional needed: 32 Please see troubleshooting guide, available here: https:\/\/aka.ms\/oe-tsg#error-outofquota\"]},\"type\":\"https:\/\/tools.ietf.org\/html\/rfc7231#section-6.5.1\",\"title\":\"One or more validation errors occurred.\",\"status\":400,\"traceId\":\"00-a308e99ddee5fc8714e34fd0808b7e93-2031400dbc3e84d1-01\"}\n\n\n\n\nWhat I understood is that I need more cores.\n\nSo, in this case how many cores I needed and how to solve this error?",
        "Challenge_closed_time":1666152123510,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666148560563,
        "Challenge_favorite_count":11.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1053752\/not-enough-quota-available-when-deploying-a-machin.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":12.31,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.9897075,
        "Challenge_title":"Not enough quota available when deploying a machine learning model on Azure",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":106,
        "Platform":"Tool-specific",
        "Solution_body":"It sounds like you don't have enough quota available in the region where you are trying to deploy the ML model.\nIn Azure portal, you can check the allocated quota for each region under the subscription blade:\n\nThe error message says: Current usage\/limit: 4\/6. Additional needed: 32\n\nRequest a quota increase and Azure support team can help on that!\n\n\n\n\n\n\n--please don't forget to upvote and Accept as answer if the reply is helpful--",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"alloc quota region subscript blade portal request quota increas team core",
        "Solution_link_count":0.0,
        "Solution_original_content":"quota region deploi model portal alloc quota region subscript blade messag sai usag limit addit request quota increas team forget upvot accept repli",
        "Solution_preprocessed_content":"quota region deploi model portal alloc quota region subscript blade messag sai addit request quota increas team forget upvot accept repli",
        "Solution_readability":9.2,
        "Solution_reading_time":5.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":43.8341666667,
        "Challenge_answer_count":1,
        "Challenge_body":"I'd like to set up Amazon SageMaker XGBoost to train datasets on multiple machines. Is that possible? If so, how?",
        "Challenge_closed_time":1583654787000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583496984000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUOKZq2V_RQaaFzQkapcWpsA\/does-amazon-sage-maker-xg-boost-support-parallel-training-across-multiple-machines",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":2.45,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":43.8341666667,
        "Challenge_title":"Does Amazon SageMaker XGBoost support parallel training across multiple machines?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":51.0,
        "Challenge_word_count":29,
        "Platform":"Tool-specific",
        "Solution_body":"Yes, using Amazon SageMaker hosting with XGBoost allows you to train datasets on multiple machines.\n\nFor more information, see Docker registry paths and example code in the Amazon SageMaker developer guide.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"host parallel train multipl guid docker registri path set",
        "Solution_link_count":0.0,
        "Solution_original_content":"host allow train dataset multipl docker registri path guid",
        "Solution_preprocessed_content":"host allow train dataset multipl docker registri path guid",
        "Solution_readability":10.5,
        "Solution_reading_time":2.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2.4724386111,
        "Challenge_answer_count":1,
        "Challenge_body":"I would like to know if there is any problem in terms of license if enterprise companies use Anaconda that is preinstalled in Azure Data Science Virtual Machine. In another inquiry, I saw an answer that Anaconda included in Azure Machine Learning service has no problem in terms of the license but I would like to confirm whether DSVM also has a problem or not. https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/165312\/anaconda-commercial-use-on-azure-machine-learning.html",
        "Challenge_closed_time":1614766234916,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614757334137,
        "Challenge_favorite_count":8.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/296502\/anaconda-commercial-use-on-azure-data-science-virt.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":15.3,
        "Challenge_reading_time":6.8,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2.4724386111,
        "Challenge_title":"Anaconda commercial use on Azure Data Science Virtual Machine",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":74,
        "Platform":"Tool-specific",
        "Solution_body":"@kenta-takahashi The thread referenced by a user was in a different context who wanted to check if they had to subscribe to commercial license to use Azure ML. In the case of DSVM where anaconda packages are installed they are still configured to use open source packages irrespective of the subscription that spins them up. So, you can definitely use the DSVM for your purposes and configure any license's that were acquired to enhance your usage experience with the tools that have been pre-installed. Thanks!!",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"licens enterpris compani anaconda preinstal data scienc virtual anaconda packag configur open sourc packag licens acquir enhanc usag pre instal",
        "Solution_link_count":0.0,
        "Solution_original_content":"kenta takahashi thread referenc context subscrib commerci licens dsvm anaconda packag instal configur open sourc packag irrespect subscript spin definit dsvm purpos configur licens acquir enhanc usag pre instal",
        "Solution_preprocessed_content":"thread referenc context subscrib commerci licens dsvm anaconda packag instal configur open sourc packag irrespect subscript spin definit dsvm purpos configur licens acquir enhanc usag",
        "Solution_readability":13.1,
        "Solution_reading_time":6.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":13.2357005556,
        "Challenge_answer_count":1,
        "Challenge_body":"I have created a machine learning workspace in West Europe region. But the storage account, key vault and application insights got created in East US region. All these got created by default with creation on ML workspace.\nSo I want to know the reason for different region and also want to move the storage account to West Europe region.",
        "Challenge_closed_time":1613167744632,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613120096110,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/270693\/why-the-storage-account-assosiated-to-azure-machin.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":5.45,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":13.2357005556,
        "Challenge_title":"Why the storage account assosiated to azure machine learning has differenent region compared ML workspace?",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":73,
        "Platform":"Tool-specific",
        "Solution_body":"Hi, this is unusual. There's no way to move your default AML storage account to a different region. I recommend creating a new workspace or contacting Azure Support to investigate further.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"creat workspac region storag account kei vault contact note default aml storag account region",
        "Solution_link_count":0.0,
        "Solution_original_content":"unusu default aml storag account region creat workspac contact",
        "Solution_preprocessed_content":"unusu default aml storag account region creat workspac contact",
        "Solution_readability":7.3,
        "Solution_reading_time":2.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":141.0333333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi to allIm trying to run a procedure looking to reduce the number of features for a model.The first try was with google Colab pro+ but it keep crashing and nver run the entire process, then I got a VM n1-highmem-8 that has: GPUs1 x NVIDIA Tesla V100  +  n1-highmem-8 (vCPUs: 8, RAM: 52GB)and still not getting the process done.The question is how to determin which type of machine should I use? Can I get any metric from the cell that is runing in colab and be able to determin the Type of VM that I need?",
        "Challenge_closed_time":1659349860000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658842140000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-determin-which-GCP-VM-do-I-need-for-ML\/td-p\/447075\/jump-to\/first-unread-message",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.0,
        "Challenge_reading_time":6.52,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":141.0333333333,
        "Challenge_title":"How to determin which GCP VM do I need for ML",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":73.0,
        "Challenge_word_count":106,
        "Platform":"Tool-specific",
        "Solution_body":"There are a few things to take in consideration:\n\nHave you installed all the necessary drivers for the GPU? Here is a complete guide that you can follow.\nI do not see any Python wrapper for CUDA in your code. The way you specify when to use the GPU for specific tasks is through this wrapper, it seems to me that you are using the CPU instead and that is why the task keeps crashing. Now, converting your code to a CUDA version is not a trivial task, and it involves a deeper knowledge on how a GPU works. If you are in a hurry, you could try the Py2CUDA github project, but I would strongly recommend taking a look at the Getting Started Blogs.\u00a0\u00a0\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"driver gpu instal wrapper cuda specifi gpu task convert cuda version involv deeper knowledg gpu pycuda github hurri start blog determin type metric cell run colab decis",
        "Solution_link_count":0.0,
        "Solution_original_content":"consider instal driver gpu complet guid wrapper cuda specifi gpu task wrapper cpu task keep crash convert cuda version trivial task involv deeper knowledg gpu hurri pycuda github strongli start blog origin",
        "Solution_preprocessed_content":"consider instal driver gpu complet guid wrapper cuda specifi gpu task wrapper cpu task keep crash convert cuda version trivial task involv deeper knowledg gpu hurri cuda github strongli start blog origin",
        "Solution_readability":7.0,
        "Solution_reading_time":8.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":130.0,
        "Tool":"Vertex AI"
    },
    {
        "Challenge_adjusted_solved_time":56.8854483334,
        "Challenge_answer_count":1,
        "Challenge_body":"My question centers around working with AML models that have been published as web services, as described here:\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service?tabs=azure-portal\n\nAre there any endpoints or ways of obtaining more detailed information about a published model? For example, the documentation states that inputs to the model are passed in via a \"data\" property, and obviously, this will vary my the model:\n\n{\n\"data\":\n[\n<model-specific-data-structure>\n]\n}\n\nIs there a way to programatically find out what the model expects as input?\n\nThe full 'wish-list' of metadata info we'd like is listed here:\n\nWhat models are available for serving\n\n\nWhat is the model prediction endpoint\n\n\nWhat are the required inputs and their data types\n\n\nWhat are the model outputs and data types\n\nAre there any endpoints or any way at getting to this information?",
        "Challenge_closed_time":1649630016347,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649425228733,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/805976\/endpoints-for-getting-metadata-about-published-mod.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.8,
        "Challenge_reading_time":11.66,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":56.8854483334,
        "Challenge_title":"Endpoints for getting metadata about published models?",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":133,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @MKRP-6344\n\nThanks for reaching out to us, I will answer your question below, at the meantime, if you feel like I am not getting your point well, please point it out and correct me.\n\nI think you are mentioning how to monitor published model and collect data, there are several choice depends on the data you want to collect:\n\nCollect data from models in production - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-data-collection\n\nThe following data can be collected:\n\nModel input data from web services deployed in an AKS cluster. Voice audio, images, and video are not collected.\nModel predictions using production input data.\n\nOnce collection is enabled, the data you collect helps you:\n\nMonitor data drifts on the production data you collect.\nAnalyze collected data using Power BI or Azure Databricks\nMake better decisions about when to retrain or optimize your model.\nRetrain your model with the collected data.\n\n2 . Monitor and collect data from ML web service endpoints - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-app-insights\n\nYou can use Azure Application Insights to collect the following data from an endpoint:\n\nOutput data\nResponses\nRequest rates, response times, and failure rates\nDependency rates, response times, and failure rates\nExceptions\n\n3 . More details from Data Drift - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-monitor-datasets?tabs=python\n\nWith Azure Machine Learning dataset monitors (preview), you can:\n\nAnalyze drift in your data to understand how it changes over time.\nMonitor model data for differences between training and serving datasets. Start by collecting model data from deployed models.\nMonitor new data for differences between any baseline and target dataset.\nProfile features in data to track how statistical properties change over time.\nSet up alerts on data drift for early warnings to potential issues.\nCreate a new dataset version when you determine the data has drifted too much.\n\nHope above information helps, please let us know if you need further helps!\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful, thanks a lot.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"metadata publish aml model collect data model model input data predict monitor collect data web servic endpoint output data respons request rate respons time rate dataset monitor analyz drift data monitor model data train serv dataset profil featur data set alert data drift creat dataset version",
        "Solution_link_count":3.0,
        "Solution_original_content":"mkrp reach monitor publish model collect data choic depend data collect collect data model http doc com enabl data collect data collect model input data web servic deploi ak cluster voic audio imag video collect model predict input data collect enabl data collect monitor data drift data collect analyz collect data power decis retrain optim model retrain model collect data monitor collect data web servic endpoint http doc com enabl app collect data endpoint output data respons request rate respons time rate depend rate respons time rate except data drift http doc com monitor dataset tab dataset monitor preview analyz drift data time monitor model data train serv dataset start collect model data deploi model monitor data baselin target dataset profil featur data track statist properti time set alert data drift earli warn potenti creat dataset version determin data drift hope yutong kindli accept",
        "Solution_preprocessed_content":"reach monitor publish model collect data choic depend data collect collect data model data collect model input data web servic deploi ak cluster voic audio imag video collect model predict input data collect enabl data collect monitor data drift data collect analyz collect data power decis retrain optim model retrain model collect data monitor collect data web servic endpoint collect data endpoint output data respons request rate respons time rate depend rate respons time rate except data drift dataset monitor analyz drift data time monitor model data train serv dataset start collect model data deploi model monitor data baselin target dataset profil featur data track statist properti time set alert data drift earli warn potenti creat dataset version determin data drift hope yutong kindli accept",
        "Solution_readability":10.5,
        "Solution_reading_time":26.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":309.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":61.8899861111,
        "Challenge_answer_count":2,
        "Challenge_body":"I'm trying to implement a self-service solution in Azure so users can run a Jupyter or PySpark notebook on-Demand\/automatically with the dataset they found a search in the Azure Data Catalog. I visualize, once the user finds the data in a search, there will be a link that will take him\/her to a Notebook and the dataset can be used for analysis. Any suggestion would be very much appreciated!",
        "Challenge_closed_time":1619432530247,
        "Challenge_comment_count":1,
        "Challenge_created_time":1619209726297,
        "Challenge_favorite_count":9.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/369952\/azure-on-demand-ml-cluster-from-a-search-in-the-da.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":9.8,
        "Challenge_reading_time":5.52,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":61.8899861111,
        "Challenge_title":"Azure On-Demand ML cluster from a search in the data catalog",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Solution_body":"@JairoMelo-1657 Thanks for the question.Azure Purview can find, understand, and consume data sources. Please follow the Azure Purview documentation: https:\/\/docs.microsoft.com\/en-us\/azure\/purview\/\n\nand We have Azure Open Datasets where you can download a Notebook for AML, Databricks or Synapse that explores the data: Azure Open Datasets Catalog | Microsoft Azure. What are open datasets? Curated public datasets - Azure Open Datasets | Microsoft Docs.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"purview consum data sourc purview document guidanc explor data open dataset download notebook aml synaps open dataset curat public dataset",
        "Solution_link_count":1.0,
        "Solution_original_content":"jairomelo purview consum data sourc purview document http doc com purview open dataset download notebook aml synaps explor data open dataset catalog open dataset curat public dataset open dataset doc",
        "Solution_preprocessed_content":"purview consum data sourc purview document open dataset download notebook aml synaps explor data open dataset catalog open dataset curat public dataset open dataset doc",
        "Solution_readability":5.9,
        "Solution_reading_time":5.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":59.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":16.0833333333,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi, I am following this tutorial on model deployment (https:\/\/codelabs.developers.google.com\/vertex-image-deploy#6), but I ran into a issue when importing the aiplatform library.When running \"from google.cloud import aiplatform\", I get the following error message:The versions of the concerned libraries are shown below.I have tried grpcio versions 1.26, 1.27.2, and even the latest 1.50, but all of them had import errors (concerning importing of aio module for 1.26 and 127.2 and AbortError module for 1.50). Are there any additional steps or libraries that I need to take to avoid these import errors?Thank you!",
        "Challenge_closed_time":1668446100000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668388200000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issues-with-importing-aiplatform\/td-p\/489087\/jump-to\/first-unread-message",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":7.8,
        "Challenge_reading_time":8.15,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":16.0833333333,
        "Challenge_title":"Issues with importing aiplatform",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":93.0,
        "Challenge_word_count":93,
        "Platform":"Tool-specific",
        "Solution_body":"Hi, thank you for your reply. I am running the code on Vertex AI.\n\nI realised I had to restart the kernel to refresh the package after updating grpcio, and I could then import aiplatform without any issues as shown below:\n\nfrom google.cloud import aiplatform\nprint(\"aiplatform version: \", aiplatform.__version__)\n\naiplatform version:  1.17.0\n\nThanks again for your help!\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"restart kernel updat grpcio import aiplatform",
        "Solution_link_count":0.0,
        "Solution_original_content":"repli run realis restart kernel refresh packag updat grpcio import aiplatform shown cloud import aiplatform print aiplatform version aiplatform version aiplatform version origin",
        "Solution_preprocessed_content":"repli run realis restart kernel refresh packag updat grpcio import aiplatform shown import aiplatform print aiplatform version origin",
        "Solution_readability":5.5,
        "Solution_reading_time":4.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":61.0,
        "Tool":"Vertex AI"
    },
    {
        "Challenge_adjusted_solved_time":70.4262202778,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>I am inspecting and analysing my best runs. I expected that <code>group<\/code> and <code>job_type<\/code> would be populated with the resumed run\u2019s values after running the code below.<\/p>\n<pre><code class=\"lang-python\">run_id = input(\"id=\")\nwith wandb.init(entity=wandb_entity, project=wandb_project, id=run_id, resume=\"must\") as wandb_r:\n    config = wandb_r.config\n    group = wandb_r.group\n    job_type = wandb_r.job_type\n<\/code><\/pre>\n<p>Even though <code>config<\/code> is successfully recovered, <code>group<\/code> and <code>job_type<\/code> are just empty strings. How do I retrieve group and job_type values from WandB? Thanks.<\/p>",
        "Challenge_closed_time":1662128318611,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661874784218,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-retrieve-the-group-and-job-type-of-a-resumed-run\/3031",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":10.2,
        "Challenge_reading_time":8.93,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":70.4262202778,
        "Challenge_title":"How to retrieve the `group` and `job_type` of a resumed run?",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":84.0,
        "Challenge_word_count":78,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/avm21\">@avm21<\/a>, it looks like we don\u2019t download these on resumed runs but rather we don\u2019t update them unless you explicitly change them on a resumed run. If you need to get group\/job_type you can use the public API like this to access anything you may need:<\/p>\n<pre><code class=\"lang-auto\">import wandb\nfrom wandb import Api\n\napi = Api()\n\nwith wandb.init(entity=wandb_entity, project=wandb_project, id=run_id, resume=\"must\") as wandb_r:\n    config = wandb_r.config\n\n    # A resumed run will still have the path attribute which can be used to access the run via the API\n    api_run = api.run(wandb_r.path)\n\n    # This will correctly print the group of the run\n    print(api_run.group)\n<\/code><\/pre>\n<p>Let me know if you have any questions around this.<\/p>\n<p>Thank you,<br>\nNate<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"group job type valu updat resum run explicitli retriev group job type valu public api access run api path attribut resum run",
        "Solution_link_count":0.0,
        "Solution_original_content":"avm download resum run updat explicitli resum run group job type public api access import import api api api init entiti entiti run resum config config resum run path attribut access run api api run api run path print group run print api run group nate",
        "Solution_preprocessed_content":"download resum run updat explicitli resum run public api access nate",
        "Solution_readability":8.4,
        "Solution_reading_time":9.97,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":113.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":0.8666666667,
        "Challenge_answer_count":1,
        "Challenge_body":"So I'm pulling my hair out over this and reaching out here for help. I'm trying to set up a service account with Cloud Translation, and Text-to-speech enabled, but we keep getting this response:I have confirmed that the service account has the \"cloudtranslate.generalModels.predict\" permission, and showing the \"Cloud Translation API User\" role. We've also confirmed that it works with a different Service account that my colleague set up in his personal Google console profile. But, we need this setup with an account through our org. I did verify that the service account has the permission from the https:\/\/console.cloud.google.com\/iam-admin\/troubleshooter so and that my organization's admin sees that the service account is granted access through ancestor policies.  So what else can we check? ",
        "Challenge_closed_time":1668501000000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668497880000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Translation-Permission\/td-p\/489632\/jump-to\/first-unread-message",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":10.34,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.8666666667,
        "Challenge_title":"Cloud Translation Permission",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":53.0,
        "Challenge_word_count":123,
        "Platform":"Tool-specific",
        "Solution_body":"Ok, turned out we had a hard-coded value for resource location, which was set to the wrong project. So of course it was coming back as permission denied.\u00a0\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"identifi valu resourc locat set permiss updat resourc locat",
        "Solution_link_count":0.0,
        "Solution_original_content":"turn valu resourc locat set cours come permiss origin",
        "Solution_preprocessed_content":"turn valu resourc locat set cours come permiss origin",
        "Solution_readability":5.2,
        "Solution_reading_time":2.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Vertex AI"
    },
    {
        "Challenge_adjusted_solved_time":188.1755805556,
        "Challenge_answer_count":2,
        "Challenge_body":"I have big data that I need to pass it to already trained Machine Learning model on Azure and has been deployed as online endpoint, I realize that batch endpoints supports adding a reference to blob file as input, my question is: how to do the same for online enpdoints ?\n\nSo far all examples I see are passing the payload as json (Even in the test tab of the online enpoints) but i don't know how to simply pass a blob storage file's uri as the payload.",
        "Challenge_closed_time":1656051612080,
        "Challenge_comment_count":1,
        "Challenge_created_time":1655374179990,
        "Challenge_favorite_count":21.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/891865\/how-to-use-blob-storage-file-as-input-to-azure-ml.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":12.5,
        "Challenge_reading_time":6.08,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":188.1755805556,
        "Challenge_title":"How to use blob storage file as input to azure ml endpoint",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":98,
        "Platform":"Tool-specific",
        "Solution_body":"@MostafaMansour-4203 Thanks for the question. I have checked internally with the product team, Currently Online endpoints are for Realtime synchronous requests.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"mostafamansour intern team onlin endpoint realtim synchron request click upvot commun member read thread",
        "Solution_preprocessed_content":"intern team onlin endpoint realtim synchron request click upvot commun member read thread",
        "Solution_readability":8.5,
        "Solution_reading_time":3.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":27.2427933334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I think it would be beneficial to select and delete several experiments at the same time.<br>\nNow I have to delete one by one and it is very time consuming.<\/p>\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1652874850582,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652776776526,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/remove-multiple-runs-at-the-same-time\/2435",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":4.8,
        "Challenge_reading_time":2.86,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":27.2427933334,
        "Challenge_title":"Remove multiple runs at the same time",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":142.0,
        "Challenge_word_count":39,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/lucasventura\">@lucasventura<\/a>, you can do it like <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/runs-table#filter-and-delete-unwanted-runs\">this<\/a>.<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"link filter delet unwant run allow select delet multipl time",
        "Solution_link_count":1.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":30.0,
        "Solution_reading_time":2.67,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":28.6481963889,
        "Challenge_answer_count":2,
        "Challenge_body":"I make pytorch model with sagemaker, MMS. This is my mms code.\n\n%%time\ninstance_type = 'c5.large'\n# accelerator_type = 'eia2.medium'\npredictor = mme.deploy(\n    initial_instance_count=1,\n    instance_type=f\"ml.{instance_type}\"\n)\n\nmme.add_model(model_data_source=model_path, model_data_path=\"model.tar.gz\")\nlist(mme.list_models())\n#> [ 'model.tar.gz']\n\nI try to predict with this code.\n\nstart_time = time.time()\npredicted_value = predictor.predict(requests, target_model=\"LV1\")\nduration = time.time() - start_time\nprint(\"${:,.2f}, took {:,d} ms\\n\".format(predicted_value[0], int(duration * 1000)))\n\nAnd, return error message.\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"{\n  \"code\": 500,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Failed to start workers\"\n}\n\n\nMMS with pytorch is 'little' difficult. X)\n\nhelp me, please.",
        "Challenge_closed_time":1660312923532,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660209790025,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUBCxtcfyrTymZ7isHG3X5Qg\/problem-at-mms-predict-at-mms-sagemaker-error-code-500-type-internal-server-exception",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.8,
        "Challenge_reading_time":12.96,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":28.6481963889,
        "Challenge_title":"[problem at MMS predict] At MMS(sagemaker), error code(500), type(InternalServerException)",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":80.0,
        "Challenge_word_count":95,
        "Platform":"Tool-specific",
        "Solution_body":"Hi , I think your target model on the prediction needs to have the name of the model you have deployed - for example , when you are adding the model with mme.add_model(model_data_source=model_path, model_data_path=\"model.tar.gz\") the model_data_path contains the name of the model . From the sagemaker-examples: (https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb) **model_data_path is the relative path to the S3 prefix we specified above (i.e. model_data_prefix) where our endpoint will source models for inference requests.Since this is a relative path, we can simply pass the name of what we wish to call the model artifact at inference time (i.e. Chicago_IL.tar.gz). In your case \"model.tar.gz\". However, when predicting you call the model ,target_model=\"LV1\"?",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"target model predict model deploi model data path model predict model call target model",
        "Solution_link_count":1.0,
        "Solution_original_content":"target model predict model deploi model mme add model model data sourc model path model data path model tar model data path model http github com blob advanc function multi model home valu multi model endpoint home valu ipynb model data path rel path prefix specifi model data prefix endpoint sourc model infer request rel path simpli pass wish model artifact infer time chicago tar model tar predict model target model",
        "Solution_preprocessed_content":"target model predict model deploi model model rel path prefix specifi endpoint sourc model infer rel path simpli pass wish model artifact infer time predict model",
        "Solution_readability":13.0,
        "Solution_reading_time":11.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":103.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":59.0207758333,
        "Challenge_answer_count":1,
        "Challenge_body":"My dataset now has enough samples to start pre-labeling a set of labels (bounding boxes for image identification).\n\nHowever, rather worryingly this seems fundamentally broken?\nWe appear to have lost the ability to zoom the image (zoom just appears to zoom the bounding boxes, and not the underlying image) which basically makes this entire functionality useless.\n\nAm I missing something or is this feature completely broken?\nI hope the former, as the pre-labeling was a significant factor in choosing this platform.\n\nWe have tried multiple browsers in case this was a browser issue but to no success, they all present the same issue.\n\nIs anyone able to advise??",
        "Challenge_closed_time":1616398055640,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616185580847,
        "Challenge_favorite_count":6.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/323305\/azure-machine-learning-data-labelling-zoom-broken.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":9.03,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":59.0207758333,
        "Challenge_title":"Azure Machine Learning Data Labelling - Zoom broken on prelabelled tasks??",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Platform":"Tool-specific",
        "Solution_body":"@ChrisH-5786 Thanks for the question. Can you please share image and snapshot for the same. We are able to zoom the underlying image using the data labeling.\n\nPlease follow the doc to Tag images and specify bounding boxes for object detection.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"document tag imag specifi bound box object detect share imag snapshot broken zoom featur",
        "Solution_link_count":0.0,
        "Solution_original_content":"chrish share imag snapshot zoom underli imag data label doc tag imag specifi bound box object detect",
        "Solution_preprocessed_content":"share imag snapshot zoom underli imag data label doc tag imag specifi bound box object detect",
        "Solution_readability":6.1,
        "Solution_reading_time":2.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":41.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":456.5955555556,
        "Challenge_answer_count":1,
        "Challenge_body":"Do we have guidelines on requirements gathering\/designing the provisioning of SageMaker Studio domains across large global enterprises with many business units?\n\nI've seen discussions where topics like number of users\/domain, org\/team structure, collaboration patterns, resource needs, classes of ML problems, framework\/library usage, security and others were raised when defining requirements and boundaries. Customer is starting their first Studio deployment and they are asking for guidance on how to scope and design that so that they can have a scalable process.",
        "Challenge_closed_time":1614090886000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612447142000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU0QiBS-GdSIKZdXxSf1NUYA\/sage-maker-studio-enterprise-deployment-guidelines",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":7.83,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":456.5955555556,
        "Challenge_title":"SageMaker Studio Enterprise Deployment guidelines",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":32.0,
        "Challenge_word_count":84,
        "Platform":"Tool-specific",
        "Solution_body":"You should guide your customer based on the general principles of multi account best practices that we provide for other services.\n\nHere are some high level boundaries.\n\nOne studio domain per account and region. No cross region AWS SSO configuration provided.\n\nMaximum numbers of users allowed in studio vary between 60 - 200 users. Although AWS SSO can support many more users, there are some considerations around other dependencies such as EFS among others.\n\nIf you need to isolate any model artifacts produced by SageMaker, you may want to have them use a separate account. Even if you use tag based access control, you can still technically list those artifacts.\n\nSageMaker feature store should follow the data lake pattern closely. As a general rule, you want to write in one account and can read from many other accounts perhaps using Lake formation to expose datasets into other accounts. Teams can create their own offline \/ online feature store for non production use cases.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"provis studio domain account region limit maximum allow studio domain isol model artifact produc separ account data lake pattern close featur store allow team creat offlin onlin featur store bias summari",
        "Solution_link_count":0.0,
        "Solution_original_content":"guid base gener principl multi account practic servic high level boundari studio domain account region cross region sso configur maximum allow studio vari sso consider depend ef isol model artifact produc separ account tag base access control technic list artifact featur store data lake pattern close gener rule write account read account lake format expos dataset account team creat offlin onlin featur store",
        "Solution_preprocessed_content":"guid base gener principl multi account practic servic high level boundari studio domain account region cross region sso configur maximum allow studio vari sso consider depend ef isol model artifact produc separ account tag base access control technic list artifact featur store data lake pattern close gener rule write account read account lake format expos dataset account team creat offlin onlin featur store",
        "Solution_readability":7.8,
        "Solution_reading_time":12.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":161.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":12.4729783333,
        "Challenge_answer_count":2,
        "Challenge_body":"I am able to find the pricing the page for SDK or designer, are they pricing the same?",
        "Challenge_closed_time":1654080178612,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654035275890,
        "Challenge_favorite_count":12.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/872161\/is-machine-learning-sdk-and-designer-pricing-the-s.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":4.8,
        "Challenge_reading_time":1.69,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":12.4729783333,
        "Challenge_title":"Is machine learning SDK and designer pricing the same",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Tool-specific",
        "Solution_body":"@benwu-8989 Adding to @DSPatrick response, Since you have used the tag azure-machine-learning tag I think you are using the latest version of Azure Machine Learning rather than classic studio. In the case of the new Azure Machine Learning studio and the SDK there will be no charge for using the service. You will only be charged for the compute used for your experiments and other Azure services consumed, including but not limited to Azure Blob Storage, Azure Key Vault, Azure Container Registry and Azure Application Insights. Please check the details of pricing for compute for Azure ML on this page.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"latest version studio sdk charg servic charg comput servic consum price comput page",
        "Solution_link_count":0.0,
        "Solution_original_content":"benwu dspatrick respons tag tag latest version classic studio studio sdk charg servic charg comput servic consum limit blob storag kei vault registri price comput page",
        "Solution_preprocessed_content":"respons tag tag latest version classic studio studio sdk charg servic charg comput servic consum limit blob storag kei vault registri price comput page",
        "Solution_readability":10.7,
        "Solution_reading_time":7.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":100.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":11.3998158333,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI created a working pipeline in azure machine learning studio but I am stuck how i can use it with a live dataset. Could anybody help to me in this issue? I dont have such option to deploy it.\n\nthank you in advance",
        "Challenge_closed_time":1640726658680,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640685619343,
        "Challenge_favorite_count":8.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/677175\/how-can-i-use-a-working-pipeline.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":3.7,
        "Challenge_reading_time":2.95,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":11.3998158333,
        "Challenge_title":"How can I use a working pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":50,
        "Platform":"Tool-specific",
        "Solution_body":"Hi, please review Test the real-time endpoint for more details on how to test your model. You can consume your model using a Client or PowerBI.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"test model time endpoint consum client powerbi deploi pipelin live dataset",
        "Solution_link_count":0.0,
        "Solution_original_content":"review test time endpoint test model consum model client powerbi kindli accept",
        "Solution_preprocessed_content":"review test endpoint test model consum model client powerbi kindli accept",
        "Solution_readability":6.5,
        "Solution_reading_time":2.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":52.2182897222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello everyone!\n\nI am taking the Create a Regression Model with Azure Machine Learning designer course in Microsoft Learn. When I perform the steps in the Explore Data section, after selecting the \"Edit column\" button of the \"Select Columns in Dataset\" module in Designer, it will be stuck in the \"loading\" state. Therefore, I cannot proceed to the next step.\n\n\n\n\n\nThank you very much!\n\nBest regards,\nLing",
        "Challenge_closed_time":1617711172380,
        "Challenge_comment_count":7,
        "Challenge_created_time":1617523186537,
        "Challenge_favorite_count":7.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/343427\/after-selecting-the-34edit-column34-button-of-the.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":8,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.57,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":52.2182897222,
        "Challenge_title":"After selecting the \"Edit column\" button of the \"Select Columns in Dataset\" module in Designer, it will be stuck in the \"loading\" state.",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":89,
        "Platform":"Tool-specific",
        "Solution_body":"@KaiXiuGao This issue is now fixed in all regions and it does not require an additional parameter to be added to the URL. Please try and let us know if it works fine.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"edit column button stuck load state region addit paramet url",
        "Solution_link_count":0.0,
        "Solution_original_content":"kaixiugao region addit paramet url",
        "Solution_preprocessed_content":"region addit paramet url",
        "Solution_readability":7.4,
        "Solution_reading_time":1.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":79.8691547222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I\u2019m trying to setup a self hosted wandb on k8s using helm charts. Unfortunately, I am not able to connect to my Amazon S3.<\/p>\n<p>I tried two ways:<\/p>\n<ol>\n<li>\n<p>Based on the example here <a href=\"https:\/\/docs.wandb.ai\/guides\/self-hosted\/setup\/on-premise-baremetal\" class=\"inline-onebox\">On Prem \/ Baremetal - Documentation<\/a>, I used the format:<br>\ns3:\/\/myaccess:myseceret@s3.amazonaws.com\/ofer-bucket-1<br>\nHowever when the wandb pod starts, it says that the URL is not valid, as \u201c:mysecret\u201d is not a valid port.<br>\nFor some reason it considers the secret to indicate URL port and not secret<\/p>\n<\/li>\n<li>\n<p>I also tried changing my bucket to public,  but wandb pod failed to initialize again, this time with error 403 access denied.<\/p>\n<\/li>\n<\/ol>\n<p>Anyone has an example for the correct format of the BUCKET value or can explain how it should be structured? I prefer to have it with access\/secret key. But I\u2019m ok with public as well.<\/p>",
        "Challenge_closed_time":1668315898264,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668028369307,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/cannot-connect-to-amazon-s3-from-self-hosted-wandb\/3399",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":9.1,
        "Challenge_reading_time":12.53,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":79.8691547222,
        "Challenge_title":"Cannot connect to Amazon S3 from self hosted wandb",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":72.0,
        "Challenge_word_count":150,
        "Platform":"Tool-specific",
        "Solution_body":"<p>I\u2019ll post the reply of Chris Van Pelt from WandB support, to assist anyone who encounters this:<br>\nThe correct format is indeed s3:\/\/access:secret@host\/bucket<br>\nHowever, each component (access, secret, etc) needs to be url encoded as special characters within them can interfere with the parsing.<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"chri van pelt format bucket valu access secret host bucket compon access secret url encod charact interfer pars",
        "Solution_link_count":0.0,
        "Solution_original_content":"ill repli chri van pelt assist format access secret host bucket compon access secret url encod charact interfer pars",
        "Solution_preprocessed_content":"ill repli chri van pelt assist format compon url encod charact interfer pars",
        "Solution_readability":21.6,
        "Solution_reading_time":3.86,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":44.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":506.6507333334,
        "Challenge_answer_count":1,
        "Challenge_body":"I need to move my Machine Learning Studio workspace to a new region. I am aware that the move function doesn't allow automatically moving to a new region, so I'll have to create a new workspace. That's not a big problem, but I still want to keep my job\/experiment history (in my new workspace). How can I do that?",
        "Challenge_closed_time":1664291528087,
        "Challenge_comment_count":1,
        "Challenge_created_time":1662467585447,
        "Challenge_favorite_count":11.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/995833\/moving-azure-machine-learning-studio-jobs-to-a-new.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":6.2,
        "Challenge_reading_time":4.47,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":506.6507333334,
        "Challenge_title":"Moving Azure Machine Learning Studio jobs to a new region",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":68,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @David-3633\n\nSorry, I just got confimation from product team, this is currently impossible. I am sorry for the inconvenience.\n\nA near future workaround which could let users at least share some experiment outputs\/inputs like environments, models, datasets cross region, but not the jobs\/metrics\/logs themselves. This feature is in private preview now and will be in public preview soon.\n\nI hope this information helps.\n\n\n\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"unfortun automat studio workspac region keep job histori futur workaround privat preview allow share output input environ model dataset cross region job metric log featur public preview soon",
        "Solution_link_count":0.0,
        "Solution_original_content":"david sorri confim team imposs sorri inconveni futur workaround share output input environ model dataset cross region job metric log featur privat preview public preview soon hope yutong kindli accept commun",
        "Solution_preprocessed_content":"sorri confim team imposs sorri inconveni futur workaround share environ model dataset cross region featur privat preview public preview soon hope yutong kindli accept commun",
        "Solution_readability":8.6,
        "Solution_reading_time":6.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":83.5716872222,
        "Challenge_answer_count":1,
        "Challenge_body":"The objective is to replicate \"MLOps template for model building, training, and deployment with third-party Git repositories using Jenkins\" builtin Sagemaker Project template. I want to feed custom seed code to the Github repository each time a project is created using my organization custom template instead of the default seed code that the builtin template feeds.\n\nI am able to create the custom template using service catalog but I could not find a solution for feeding the seed code to github repo. So, I decided to see how the built in project template is doing this and it is using resources from this bucket \"s3:\/\/sagemaker-servicecatalog-seedcode-us-east-1\/bootstrap\/GitRepositorySeedCodeCheckinCodeBuildProject-v1.0.zip\" but I could not access it. I am not sure how to achieve the objective?",
        "Challenge_closed_time":1657560195771,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657259337697,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU_Y4T-A3aQySFeRr3feBscA\/how-to-feed-seed-code-to-git-hub-repository-from-sagemaker-projects-organization-template-created-with-service-catalog",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":11.53,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":83.5716872222,
        "Challenge_title":"How to feed seed code to GitHub Repository from Sagemaker Projects Organization Template created with Service Catalog?",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":169.0,
        "Challenge_word_count":136,
        "Platform":"Tool-specific",
        "Solution_body":"You can download the seed package using awscli s3 cp <s3_uri> <target_path> or by using this URL: https:\/\/sagemaker-servicecatalog-seedcode-us-east-1.s3.amazonaws.com\/bootstrap\/GitRepositorySeedCodeCheckinCodeBuildProject-v1.0.zip\n\nThis .zip is used by CodeBuild that is called when the template is deployed (by a lambda mapped to a CFN custom component). If you take a look in the template you'll find a component named \"SageMakerModelBuildSeedCodeCheckinProjectTriggerLambdaInvoker\". You can find some env vars defined for this component like: SEEDCODE_BUCKET_NAME and SEEDCODE_BUCKET_KEY. These vars point to an S3 uri that has another .zip file with the content of the seed for the git repo. If you get the default values defined there you can re-create the URL and download the .zip file as well: https:\/\/sagemaker-servicecatalog-seedcode-us-east-1.s3.amazonaws.com\/toolchain\/model-building-workflow-jenkins-v1.0.zip\n\nSo, in the end, if you want to change the content that is pushed to the git repo, you can redefine these 2 vars and point to an S3 path that contains a .zip file you created.\n\nBonus: If you're a curious person, I recommend you to take a look at the .java file (src\/main\/java\/GitRepositorySeedCodeBootStrapper.java) inside the .zip of the CodeBuild .zip for you to understand what it does to prepare the git repo like: download a .zip, unpack it, commit\/push to the git repo.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"redefin seedcod bucket seedcod bucket kei environ variabl path zip file creat achiev download seed packag awscli url codebuild zip file prepar git repo",
        "Solution_link_count":2.0,
        "Solution_original_content":"download seed packag awscli url http servicecatalog seedcod amazonaw com bootstrap gitrepositoryseedcodecheckincodebuildproject zip zip codebuild call templat deploi lambda map cfn compon templat compon modelbuildseedcodecheckinprojecttriggerlambdainvok env var defin compon seedcod bucket seedcod bucket kei var uri zip file seed git repo default valu defin creat url download zip file http servicecatalog seedcod amazonaw com toolchain model build workflow jenkin zip end push git repo redefin var path zip file creat bonu java file src java gitrepositoryseedcodebootstrapp java insid zip codebuild zip prepar git repo download zip unpack commit push git repo",
        "Solution_preprocessed_content":"download seed packag awscli url zip codebuild call templat deploi templat compon modelbuildseedcodecheckinprojecttriggerlambdainvok env var defin compon var uri zip file seed git repo default valu defin url download zip file end push git repo redefin var path zip file creat bonu java file insid zip codebuild zip prepar git repo download zip unpack git repo",
        "Solution_readability":9.3,
        "Solution_reading_time":17.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":197.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":135.3537413889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi all,\n\nI am following the steps on this tutorial:\nTutorial: Score machine learning models with PREDICT in serverless Apache Spark pools https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\n\nI don't know what is the AML_MODEL_URI. I thought it was the REST endpoint or the Swagger URI from the endpoint.\n\n\nBut it is not working. I am getting this error on Synapse: \"RuntimeError: Load model failed\nTraceback (most recent call last):\"\n\n\nI appreciate you help.\n\nKind regards,\nAnaid",
        "Challenge_closed_time":1637667545272,
        "Challenge_comment_count":2,
        "Challenge_created_time":1637180271803,
        "Challenge_favorite_count":14.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/631200\/what-is-aml-model-uri-predict-in-serverless-apache.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":9.2,
        "Challenge_reading_time":8.11,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":135.3537413889,
        "Challenge_title":"What is AML_MODEL_URI - PREDICT in serverless Apache Spark pools (Synapse & Azure Machine learning AML)",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":85,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @Anaid-6816,\n\nThanks for the question and using MS Q&A platform.\n\nAML_MODEL_URL is the same name of the model in the ML workspace with (follow the format of aml:\/\/ + Name of the Model).\n\nExample: aml:\/\/sklearn_regression_model:1 (follow the format of aml:\/\/ + Name of the Model).\n\nHope this will help. Please let us know if any further queries.\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"format aml model uri aml model aml sklearn regress model",
        "Solution_link_count":0.0,
        "Solution_original_content":"anaid platform aml model url model workspac format aml model aml sklearn regress model format aml model hope queri forget click upvot button origin poster commun faster identifi remind come respons subscrib notif program shape futur volunt moder",
        "Solution_preprocessed_content":"platform model workspac hope queri forget click upvot button origin poster commun faster identifi remind come respons subscrib notif program shape futur volunt moder",
        "Solution_readability":7.8,
        "Solution_reading_time":9.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":131.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":1.3832386111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello, Are there any drawbacks I should be aware of if we restrict user access to only a single region?\n\nWe use a variety of AWS services but mainly S3 and Sagemaker Studio. Our team is located in various locations so their default regions are different. It has been a challenge to keep track of studio instances when they are created in different regions so we are now considering restricting access to a single region. Are there issues that we may face in that case? Any services we may miss?",
        "Challenge_closed_time":1642705784219,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642700804560,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUxt7fqO9HQrKWDfi4V4Lagg\/pros-and-cons-of-restricting-user-access-to-certain-regions",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":6.68,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.3832386111,
        "Challenge_title":"Pros and cons of restricting user access to certain regions",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":43.0,
        "Challenge_word_count":99,
        "Platform":"Tool-specific",
        "Solution_body":"I would take a look at this for some potential edge cases. In summary, you may need to allow us-east-1 and us-west-2 in addition to whatever regions your team is in since they host some of the global service endpoints (like IAM, Route 53, Global Accelerator, and a few others). For STS, I would use the regional endpoints if you aren't already.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"allow access addit region team host global servic endpoint st region endpoint",
        "Solution_link_count":0.0,
        "Solution_original_content":"potenti edg summari allow addit region team host global servic endpoint iam rout global acceler st region endpoint aren",
        "Solution_preprocessed_content":"potenti edg summari allow addit region team host global servic endpoint st region endpoint aren",
        "Solution_readability":9.0,
        "Solution_reading_time":4.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":11.7354919444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I'm using Azure ML Designer to run a pipeline. The pipeline performs a few steps and then it cancels the work throwing an error message with no further details.\n\nIf I re-submit the pipeline it completes the previously failed step but fails on the next step. If I re-submit the same thing happens (completes previously failed step to then fail the next step)... until it gets stuck in a specific sql transform step (see log below)\n\n\n\n\nHere is a sequence of run ids related with the issue:\nd33d23a2-2e60-4198-a6b6-f47e6e27ef4e\n57e04c1e-73e8-4ddf-91a8-c407cd1ad5ef\nad7dc826-6549-4eb3-9536-9a801d8e8c0b\ne6623f6f-b7b9-4f19-9501-c8c28f53ab23\n\nIt may be due to the way my pipeline is built but seems like JOIN, SQL Transform and SELECT Column operations tend to fail the most.\n\nWould much appreciate any help on this.\n\n 2021\/05\/11 01:57:24 Starting App Insight Logger for task:  runTaskLet\n 2021\/05\/11 01:57:24 Attempt 1 of http call to http:\/\/10.0.0.6:16384\/sendlogstoartifacts\/info\n 2021\/05\/11 01:57:24 Attempt 1 of http call to http:\/\/10.0.0.6:16384\/sendlogstoartifacts\/status\n [2021-05-11T01:57:24.912444] Entering context manager injector.\n [context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['urldecode_invoker.py', 'python', '-m', 'azureml.designer.modules.datatransform.invoker', 'ApplySqlTransModule', '--dataset', 'DatasetOutputConfig:Result_dataset', '--t1=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr', '--t2=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy', '--t3=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji', '--sqlquery=%22select+b.*%2cc.*%0d%0afrom+(%0d%0a++++select+a.customer_id%2c+a.sku_id%0d%0a++++from+(%0d%0a++++++++select+*+from+t1+cross+join+t2%0d%0a++++)+a%0d%0a++++where+exists+(%0d%0a++++++++select+t3.top_skus%0d%0a++++++++from+t3%0d%0a++++++++where+t3.sku_id+%3d+a.sku_id%0d%0a++++)%0d%0a)+b%0d%0ainner+join+(%0d%0a++++select+distinct+sku_id%2c+top_skus%0d%0a++++from+t3%0d%0a)+c%0d%0aon+c.sku_id+%3d+b.sku_id%22'])\n Script type = None\n [2021-05-11T01:57:26.142183] Entering Run History Context Manager.\n [2021-05-11T01:57:26.734197] Current directory: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/mounts\/workspaceblobstore\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\n [2021-05-11T01:57:26.734493] Preparing to call script [urldecode_invoker.py] with arguments:['python', '-m', 'azureml.designer.modules.datatransform.invoker', 'ApplySqlTransModule', '--dataset', '$Result_dataset', '--t1=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr', '--t2=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy', '--t3=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji', '--sqlquery=%22select+b.*%2cc.*%0d%0afrom+(%0d%0a++++select+a.customer_id%2c+a.sku_id%0d%0a++++from+(%0d%0a++++++++select+*+from+t1+cross+join+t2%0d%0a++++)+a%0d%0a++++where+exists+(%0d%0a++++++++select+t3.top_skus%0d%0a++++++++from+t3%0d%0a++++++++where+t3.sku_id+%3d+a.sku_id%0d%0a++++)%0d%0a)+b%0d%0ainner+join+(%0d%0a++++select+distinct+sku_id%2c+top_skus%0d%0a++++from+t3%0d%0a)+c%0d%0aon+c.sku_id+%3d+b.sku_id%22']\n [2021-05-11T01:57:26.734551] After variable expansion, calling script [urldecode_invoker.py] with arguments:['python', '-m', 'azureml.designer.modules.datatransform.invoker', 'ApplySqlTransModule', '--dataset', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpnbybe4mu', '--t1=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr', '--t2=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy', '--t3=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji', '--sqlquery=%22select+b.*%2cc.*%0d%0afrom+(%0d%0a++++select+a.customer_id%2c+a.sku_id%0d%0a++++from+(%0d%0a++++++++select+*+from+t1+cross+join+t2%0d%0a++++)+a%0d%0a++++where+exists+(%0d%0a++++++++select+t3.top_skus%0d%0a++++++++from+t3%0d%0a++++++++where+t3.sku_id+%3d+a.sku_id%0d%0a++++)%0d%0a)+b%0d%0ainner+join+(%0d%0a++++select+distinct+sku_id%2c+top_skus%0d%0a++++from+t3%0d%0a)+c%0d%0aon+c.sku_id+%3d+b.sku_id%22']\n    \n Session_id = 4b5b4c29-cfda-4ab6-a715-47fee287c468\n Invoking module by urldecode_invoker 0.0.8.\n    \n Module type: custom module.\n    \n Using runpy to invoke module 'azureml.designer.modules.datatransform.invoker'.\n    \n \/azureml-envs\/azureml_7c975cabc8bb1dc19c3de94457d707fd\/lib\/python3.6\/site-packages\/azureml\/designer\/modules\/datatransform\/tools\/dataframe_utils.py:2: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n   from pandas.util.testing import assert_frame_equal\n 2021-05-11 01:57:27,324 [             invoker] [    INFO] .[main] Start custom modules\n 2021-05-11 01:57:27,337 [             invoker] [    INFO] .[main] Module version: 0.0.74\n 2021-05-11 01:57:27,344 [             invoker] [    INFO] .[main] args: azureml.designer.modules.datatransform.invoker, ApplySqlTransModule, --dataset, \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpnbybe4mu, --t1=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr, --t2=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy, --t3=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji, --sqlquery=select b.*,c.*\n from (\n     select a.customer_id, a.sku_id\n     from (\n         select * from t1 cross join t2\n     ) a\n     where exists (\n         select t3.top_skus\n         from t3\n         where t3.sku_id = a.sku_id\n     )\n ) b\n inner join (\n     select distinct sku_id, top_skus\n     from t3\n ) c\n on c.sku_id = b.sku_id\n 2021-05-11 01:57:27,352 [             invoker] [    INFO] .[main] \"transform_module_class_name\": ApplySqlTransModule\n 2021-05-11 01:57:27,444 [         module_base] [    INFO] ...[get_arg_parser] Construct arg parser\n 2021-05-11 01:57:27,460 [         module_base] [    INFO] ...[get_arg_parser] arg: t1\n 2021-05-11 01:57:27,468 [         module_base] [    INFO] ...[get_arg_parser] arg: t2\n 2021-05-11 01:57:27,476 [         module_base] [    INFO] ...[get_arg_parser] arg: t3\n 2021-05-11 01:57:27,484 [         module_base] [    INFO] ...[get_arg_parser] arg: dataset\n 2021-05-11 01:57:27,492 [         module_base] [    INFO] ...[get_arg_parser] arg: sqlquery\n 2021-05-11 01:57:27,500 [         module_base] [    INFO] ..[parse_and_insert_args] invoker args:\n  module_classname = ApplySqlTransModule\n  t1 = \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr\n  t2 = \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy\n  t3 = \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji\n  dataset = \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpnbybe4mu\n  sqlquery = select b.*,c.*\n from (\n     select a.customer_id, a.sku_id\n     from (\n         select * from t1 cross join t2\n     ) a\n     where exists (\n         select t3.top_skus\n         from t3\n         where t3.sku_id = a.sku_id\n     )\n ) b\n inner join (\n     select distinct sku_id, top_skus\n     from t3\n ) c\n on c.sku_id = b.sku_id\n    \n 2021-05-11 01:57:27,508 [             invoker] [    INFO] .[main] start to run custom module: ApplySqlTransModule\n 2021-05-11 01:57:27,516 [apply_sql_trans_module] [    INFO] ...[run] Construct SQLite Server\n 2021-05-11 01:57:27,530 [    module_parameter] [    INFO] ......[data] Read data from \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr\n 2021-05-11 01:57:29,215 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t1 with only column names\n 2021-05-11 01:57:29,227 [    module_parameter] [    INFO] ......[data] Read data from \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy\n 2021\/05\/11 01:57:29 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n Stopped: false\n OriginalData: 1\n FilteredData: 0.\n 2021-05-11 01:57:30,093 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t2 with only column names\n 2021-05-11 01:57:30,106 [    module_parameter] [    INFO] ......[data] Read data from \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji\n 2021-05-11 01:57:30,876 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t3 with only column names\n 2021-05-11 01:57:30,888 [apply_sql_trans_module] [    INFO] ...[run] Read SQL script query\n 2021-05-11 01:57:30,895 [apply_sql_trans_module] [    INFO] ...[run] Validate SQL script query\n 2021-05-11 01:57:30,912 [apply_sql_trans_module] [    INFO] ...[run] Insert data to SQLite Server\n 2021-05-11 01:57:30,919 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t1\n 2021-05-11 01:57:30,930 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t2\n 2021-05-11 01:57:30,970 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t3\n 2021-05-11 01:57:31,053 [apply_sql_trans_module] [    INFO] ...[run] Generate SQL query result from SQLite Server",
        "Challenge_closed_time":1620739797128,
        "Challenge_comment_count":2,
        "Challenge_created_time":1620697549357,
        "Challenge_favorite_count":8.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/390003\/azure-ml-pipeline-fails-giving-no-error.html",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":17.3,
        "Challenge_reading_time":129.3,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":69,
        "Challenge_solved_time":11.7354919444,
        "Challenge_title":"azure ml pipeline fails at sql transform task",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":599,
        "Platform":"Tool-specific",
        "Solution_body":"Found the problem.\n\nThere was a task failing but due to the size of the canvas I wasn't able to spot it at first (working late hours didn't help also).\n\nHowever it certainly didn't help the fact that the error message didn't provide any info regarding which task failed, so maybe the AML team would like to add more descriptive messages in cases like this one.\n\nthanks",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"aml team descript messag task",
        "Solution_link_count":0.0,
        "Solution_original_content":"task size canva wasn spot late hour certainli messag task mayb aml team add descript messag",
        "Solution_preprocessed_content":"task size canva wasn spot certainli messag task mayb aml team add descript messag",
        "Solution_readability":8.4,
        "Solution_reading_time":4.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":28.0968075,
        "Challenge_answer_count":1,
        "Challenge_body":"ML studio is, by default picking up Python 3.6 kernel, even when I'm specifying use Python 3.8 AzureML kernel. In UI, it's changed but not actually.",
        "Challenge_closed_time":1647451118727,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647349970220,
        "Challenge_favorite_count":9.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/772790\/azure-ml-studio-is-bugged-out-and-can-not-create-a.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":5.0,
        "Challenge_reading_time":3.06,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":28.0968075,
        "Challenge_title":"Azure ML Studio is bugged out and can not create a Microsoft ticket under MSDN. Need a few suggestions",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":44,
        "Platform":"Tool-specific",
        "Solution_body":"Hi, thanks for reaching out. It looks like the command you ran isn't supported. A better command to test kernel changes is shown below:\n\n from platform import python_version\n print(python_version())\n\nHope this helps!",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"test kernel studio default kernel inabl creat ticket msdn",
        "Solution_link_count":0.0,
        "Solution_original_content":"reach ran isn test kernel shown platform import version print version hope",
        "Solution_preprocessed_content":"reach ran isn test kernel shown platform import hope",
        "Solution_readability":5.1,
        "Solution_reading_time":2.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":11.1764397222,
        "Challenge_answer_count":1,
        "Challenge_body":"How is the output of Fisher Linear Discriminant Analysis experiment interpreted now that the column labels in the output are replaced with Col1, Col2, Col3.......etc? How can the model be used to predict clusters of other input data as deployed web service requires even the dependent valuable(the same same ones we wish to predict)?",
        "Challenge_closed_time":1621895240423,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621855005240,
        "Challenge_favorite_count":5.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/407053\/fisher-linear-discriminant-analysis-azure.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":4.67,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":11.1764397222,
        "Challenge_title":"Fisher Linear Discriminant Analysis Azure",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":58,
        "Platform":"Tool-specific",
        "Solution_body":"Are you referring to the categories generated from LDA module? If so, then that's expected. LDA is an unsupervised technique, it groups words into categories\/topics and it's up to the analyst to interpret it by observing the results and transforming the output dataset accordingly. Here's are some examples of LDA approach in Azure AI Gallery. Hope this helps.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"observ transform output dataset accordingli lda unsupervis techniqu lda galleri",
        "Solution_link_count":0.0,
        "Solution_original_content":"gener lda modul lda unsupervis techniqu group analyst interpret observ transform output dataset accordingli lda galleri hope",
        "Solution_preprocessed_content":"gener lda modul lda unsupervis techniqu group analyst interpret observ transform output dataset accordingli lda galleri hope",
        "Solution_readability":7.8,
        "Solution_reading_time":4.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":58.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":137.3216258333,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Can you please help with content for hf-fastai2<\/p>",
        "Challenge_closed_time":1644114659104,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643620301251,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/access-to-study-group\/1850",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":4.3,
        "Challenge_reading_time":0.97,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":137.3216258333,
        "Challenge_title":"Access to study group",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":145.0,
        "Challenge_word_count":11,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Slides can be found in fastai <a href=\"https:\/\/discord.com\/channels\/689892369998676007\/859175939368026162\/937472311836176425\" rel=\"noopener nofollow ugc\">discord.<\/a><\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"slide fastai studi group fastai discord channel",
        "Solution_link_count":1.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":12.5,
        "Solution_reading_time":2.39,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":1.0001155556,
        "Challenge_answer_count":1,
        "Challenge_body":"As the document\nA composed model is created by taking a collection of custom models and assigning them to a single model ID. You can assign up to 100 trained custom models to a single composed model ID. When a document is submitted to a composed model, the service performs a classification step to decide which custom model accurately represents the form presented for analysis.\n\nWhat\u2019s the price for the classification step?",
        "Challenge_closed_time":1669044705523,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669041105107,
        "Challenge_favorite_count":13.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1098169\/compose-model.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":5.39,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.0001155556,
        "Challenge_title":"Compose model",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":73,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @KenSmith-3969\n\nThanks for reaching out to us and sorry for the confusion of the document.\n\nThere is no extra fee for the classification you mentioned in the document. You only pay for the custom model you finally run for your document.\n\nI will raise a ticket to fix the document, thanks a lot for pointing out it.\n\nI hope this helps!\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks!",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"address clarif price model",
        "Solution_link_count":0.0,
        "Solution_original_content":"kensmith reach sorri document extra fee classif document pai model final run document rais ticket document hope yutong kindli accept commun",
        "Solution_preprocessed_content":"reach sorri document extra fee classif document pai model final run document rais ticket document hope yutong kindli accept commun",
        "Solution_readability":7.2,
        "Solution_reading_time":5.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":78.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":10.5141666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Quick questions on ML metrics persistence from sagemaker training tasks. The SageMaker regexp-over-CloudWatch is an attractive option, yet the metric retention in Cloudwatch seems to be restricted to 15 days.\n\nHow to persist those metrics longer? Is it common to extract them out of Cloudwatch regularly to persist them somewhere else, eg S3 or an RDS? what is the best practice for long-term persistence of those metrics?\nWould SageMaker Experiments allow a collection of similar data (customer-defined training metrics) over a longer retention?",
        "Challenge_closed_time":1578681130000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578643279000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU197giXXuRn-4Hz56HZZpSw\/sage-maker-metrics-persistence",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":7.21,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":10.5141666667,
        "Challenge_title":"SageMaker metrics persistence",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":103.0,
        "Challenge_word_count":85,
        "Platform":"Tool-specific",
        "Solution_body":"You can now persist algorithm metrics from SageMaker Training Jobs (the ones you can collect with regexes or the ones available from built-in algorithms by default) by setting EnableSageMakerTimeSeriesMetrics through the AWS SDK or enable_sagemaker_metrics=true in the SageMaker Python SDK. These metrics are persisted long term, and available through Amazon SageMaker Studio. (Go to \"Metrics\" -> \"Add Chart\" from the detail page of a training job). These are available at no additional cost.\n\nYes, SageMaker Experiments allow collection of similar data\n\nNote that system metrics (CPU\/GPU\/Memory\/Disk) are still available only through CloudWatch.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"persist algorithm metric train job set enabletimeseriesmetr sdk enabl metric sdk metric persist term studio collect data note metric cpu gpu memori disk cloudwatch",
        "Solution_link_count":0.0,
        "Solution_original_content":"persist algorithm metric train job on collect regex on built algorithm default set enabletimeseriesmetr sdk enabl metric sdk metric persist term studio metric add chart page train job addit cost allow collect data note metric cpu gpu memori disk cloudwatch",
        "Solution_preprocessed_content":"persist algorithm metric train job set enabletimeseriesmetr sdk sdk metric persist term studio addit cost allow collect data note metric cloudwatch",
        "Solution_readability":11.6,
        "Solution_reading_time":8.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":11.3083616667,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm new to Azure ML so I have very little knowledge of this service..\nI've built a dummy regression model using automl package and now I'm trying to deploy it.\nI looked up some docs and followed a tutorial I found to deploy the model and I'm getting some errors..\n <- this is the error I'm currently getting\nI think there is a problem with my score.py so I'm attaching the photo here as well.\n\n\nand this is the output i need to print out through the model..\n\n\nI'd appreciate it much if somebody could give me some help\n\nthank you",
        "Challenge_closed_time":1639459821472,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639419111370,
        "Challenge_favorite_count":12.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/662007\/getting-an-error-when-trying-to-deploy-azure-ml-mo.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.87,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":11.3083616667,
        "Challenge_title":"getting an error when trying to deploy azure ml model",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":109,
        "Platform":"Tool-specific",
        "Solution_body":"Hello,\n\nThanks for reaching out to us. From the above error it looks like the package did not install successfully. A more detailed procedure to install the SDK is available directly in the documentation: https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py\n\nHow to set up the environment: https:\/\/github.com\/Azure\/azureml-examples\/tree\/main\/python-sdk\/tutorials\/automl-with-azureml#3-setup-a-new-conda-environment\n\nYou can test if you have set the env correct by below code:\n\n import azureml.core\n    \n print(\"This notebook was created using version 1.35.0 of the Azure ML SDK.\")\n print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK.\")\n assert (\n     azureml.core.VERSION >= \"1.35\"\n ), \"Please upgrade the Azure ML SDK by running '!pip install --upgrade azureml-sdk' then restart the kernel.\"\n\n\n\nThere are some prerequisites to deploy models:\n\nAn Azure Machine Learning workspace. For more information, see Create an Azure Machine Learning workspace. https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace\n\n\nA model. The examples in this article use a pre-trained model.\n\n\nThe Azure Machine Learning software development kit (SDK) for Python. https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/intro\n\n\nA machine that can run Docker, such as a compute instance. https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance\n\nMore information please refer to https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#prerequisites\n\nHope this will help. Please let us know if any further queries.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"procedur instal sdk document test set environ prerequisit deploi model workspac pre train model sdk run docker direct deploi model",
        "Solution_link_count":6.0,
        "Solution_original_content":"reach packag instal successfulli procedur instal sdk directli document http doc com api overview instal set environ http github com tree sdk tutori automl setup conda environ test set env import core print notebook creat version sdk print version core version sdk assert core version upgrad sdk run pip instal upgrad sdk restart kernel prerequisit deploi model workspac creat workspac http doc com workspac model articl pre train model softwar kit sdk http doc com api overview intro run docker comput instanc http doc com creat comput instanc http doc com deploi tab prerequisit hope queri forget click upvot button origin poster commun faster identifi remind come respons subscrib notif program shape futur volunt moder",
        "Solution_preprocessed_content":"reach packag instal successfulli procedur instal sdk directli document set environ test set env import core print print assert upgrad sdk run pip instal sdk restart prerequisit deploi model workspac creat workspac model articl model softwar kit run docker comput instanc hope queri forget click upvot button origin poster commun faster identifi remind come respons subscrib notif program shape futur volunt moder",
        "Solution_readability":9.7,
        "Solution_reading_time":26.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":249.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":24.1505972222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I am getting the error from the subject line when i try to inner join a dataset of 850K rows and 3 columns (parquet data file of around 4mb) with another with 300K rows and 10 columns (parquet data file is about 1mb). I'm using Azure ML Studio Designer\n\nMy compute is Standard Dv2 Family vCPUs (20% of utilization).\n\nI was surprised by this hitting a limit. Any idea on how i should proceed?",
        "Challenge_closed_time":1619444233763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619357291613,
        "Challenge_favorite_count":7.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/370636\/moduleexceptionmessagemoduleoutofmemory-memory-has.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":6.11,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":24.1505972222,
        "Challenge_title":"ModuleExceptionMessage:ModuleOutOfMemory: Memory has been exhausted, unable to complete running of module.",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":85,
        "Platform":"Tool-specific",
        "Solution_body":"i manage to do this by trainning the model in a subset of records (using the Sample model).\n\nAlso noted that the documentation implies that an out of memory error is dependant on the RAM of the client \/ Designer user machine not the compute selected (or at least that is my understanding of the note at the beginning of the doc)",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"train model subset record sampl model note messag depend ram client design comput select",
        "Solution_link_count":0.0,
        "Solution_original_content":"train model subset record sampl model note document memori depend ram client design comput select note begin doc",
        "Solution_preprocessed_content":"train model subset record note document memori depend ram client design comput select",
        "Solution_readability":14.0,
        "Solution_reading_time":3.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":10.7709561111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi all,\n\nI am asking if it's possible to use framework processor inside a sagemaker pipeline.\n\nI am asking because the to submit the source_dir for the framework processor, we have to do so when calling the .run() method, when wrapping the processor inside a sagemaker.workflow.steps.ProcessingStep, there isn't an available argument to specify the source_dir.\n\nThank you! Best, Ruoy",
        "Challenge_closed_time":1652383066859,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652344291417,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUbY_u2lSORnmomHzZsGOZAA\/sage-maker-framework-processor-compatibility-with-sagemaker-pipelines",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":5.66,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":10.7709561111,
        "Challenge_title":"SageMaker framework processor compatibility with sagemaker pipelines",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":201.0,
        "Challenge_word_count":65,
        "Platform":"Tool-specific",
        "Solution_body":"You can do this with the latest version of the sagemaker sdk 2.89.0\n\nfrom sagemaker.workflow.pipeline_context import PipelineSession\n\nsession = PipelineSession()\n\ninputs = [\n    ProcessingInput(\n    source=\"s3:\/\/my-bucket\/sourcefile\", \n    destination=\"\/opt\/ml\/processing\/inputs\/\",),\n]\n\nprocessor = FrameworkProcessor(...)\n\nstep_args = processor.run(inputs=inputs, source_dir=\"...\")\n\nstep_sklearn = ProcessingStep(\n    name=\"MyProcessingStep\",\n    step_args=step_args,\n)",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"latest version sdk version specifi sourc dir wrap processor insid workflow step processingstep",
        "Solution_link_count":0.0,
        "Solution_original_content":"latest version sdk workflow pipelin context import pipelinesess session pipelinesess input processinginput sourc bucket sourcefil destin opt process input processor frameworkprocessor step arg processor run input input sourc dir step sklearn processingstep myprocessingstep step arg step arg",
        "Solution_preprocessed_content":"latest version sdk import pipelinesess session pipelinesess input processor processingstep",
        "Solution_readability":18.2,
        "Solution_reading_time":5.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":32.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":3.2563888889,
        "Challenge_answer_count":1,
        "Challenge_body":"I am trying to control a Spark cluster (using SparkR) from a Sagemaker notebook. I followed these instructions closely: https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/ and got it to work.\n\nToday when I try to run the SageMaker notebook (using the exact same code as before) I inexplicably get this error:\n\nAn error was encountered:\n[1] \"Error in callJMethod(sparkSession, \\\"read\\\"): Invalid jobj 1. If SparkR was restarted, Spark operations need to be re-executed.\"\n\n\nDoes anyone know why this is? I terminated the SparkR kernel and am still getting this error.",
        "Challenge_closed_time":1591041375000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591029652000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUqyiKb_XvRhGxm1RxwjXhJQ\/spark-r-not-working",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":8.11,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":3.2563888889,
        "Challenge_title":"SparkR not working",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":44.0,
        "Challenge_word_count":87,
        "Platform":"Tool-specific",
        "Solution_body":"You cannot have multiple SparkContexts in one JVM. The issue is resolved as WON'T FIX. You have to stop the spark session which spawned the sparkcontext (which you have already done).\n\nsparkR.session.stop()\n\nhttps:\/\/issues.apache.org\/jira\/browse\/SPARK-2243",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"stop spark session spawn sparkcontext sparkr session stop",
        "Solution_link_count":1.0,
        "Solution_original_content":"multipl sparkcontext jvm stop spark session spawn sparkcontext sparkr session stop http apach org jira brows spark",
        "Solution_preprocessed_content":"multipl sparkcontext jvm stop spark session spawn sparkcontext",
        "Solution_readability":7.6,
        "Solution_reading_time":3.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":1.4038472222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hallo, i would like make an appointment for Exam AI-900: Microsoft Azure AI Fundamentals.\nHowever this exam is currently not available at Pearson vue or Certiport. When can i expect this again? Is there an alternative ?",
        "Challenge_closed_time":1662210933980,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662205880130,
        "Challenge_favorite_count":11.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/992629\/exam-ai-900-microsoft-azure-ai-fundamentals-not-av.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":3.69,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.4038472222,
        "Challenge_title":"certification test for AI-900: Microsoft Azure AI Fundamentals not available",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":45,
        "Platform":"Tool-specific",
        "Solution_body":"Hi Jurian,\n\nThis is available in PearsonVue check this. ai-900\n\nAny specific region you are trying from?\n\n\n\n\n\n==\nPlease \"Accept the answer\" if the information helped you. This will help us and others in the community as well.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"pearsonvu fundament certif test specifi region schedul test option",
        "Solution_link_count":0.0,
        "Solution_original_content":"jurian pearsonvu region accept commun",
        "Solution_preprocessed_content":"jurian pearsonvu region accept commun",
        "Solution_readability":5.7,
        "Solution_reading_time":2.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":0.5301305556,
        "Challenge_answer_count":1,
        "Challenge_body":"I wonder how could I convert the result of boundingbox of form recognizer into image coordinate to visualize the overlay image and recognized data.\n\nI could not have that accomplished because it is not similar to normal coordinates.",
        "Challenge_closed_time":1648418537720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648416629250,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/789141\/result-with-coordinator-convertion.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":3.32,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.5301305556,
        "Challenge_title":"Result with coordinator convertion",
        "Challenge_topic":"Database Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":41,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @masterhunter-9726\n\nThank you for reaching out to us, I think you have questions about the value of boundingBoxes, below I will give an example to explain it so that you can convert it to the coordinate you want to use:\n\nExample:\n\n\n\n\n    'boundingBox': [\n                 57.1,\n                 683.3,\n                 100.2,\n                 683.3,\n                 100.2,\n                 673.3,\n                 57.1,\n                 673.3\n               ]\n\n\n\nThose values represent the vertices of the bounding box as below:\n\n   (57.1,683.3) X1,Y1---->x2,y2(100.2,683.3)\n                   |                |\n                   |                |\n   (57.1,673.3) X4,Y4<----x3,y3(100.2,673.3)\n\n\n\nThe (0,0) is on the bottom left as you can see.\n\n \/\/ Azure Bounding box is like this                     \n \/\/                                                     0---->1\n \/\/                                                    |     |\n \/\/                                         Y          |     |\n \/\/                                         \u2191         3<----2\n \/\/                                  Origin . \u2192 X\n\n\n\nIf you want to measure the boundingBoxes, you can use above vertices to do the calculation.\n\nHope this helps!\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful, thanks!",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"explain convert bound box coordin desir imag coordin visual purpos involv vertic bound box calcul coordin",
        "Solution_link_count":0.0,
        "Solution_original_content":"masterhunt reach valu boundingbox explain convert coordin boundingbox valu repres vertic bound box origin measur boundingbox vertic calcul hope yutong kindli accept",
        "Solution_preprocessed_content":"reach valu boundingbox explain convert coordin boundingbox valu repres vertic bound box left bound box origin measur boundingbox vertic calcul hope yutong kindli accept",
        "Solution_readability":7.6,
        "Solution_reading_time":9.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":119.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":21.8707033333,
        "Challenge_answer_count":1,
        "Challenge_body":"I have trained a machine learning model in Databricks workspace using mlflow. Model is getting registered in databricks model registry and saved in databricks file share. Now I want to download model artifacts from workspace. Currently I am transferring model to azure machine learning workspace. There I am able to download all the artifacts. How to do it from databricks workspace?",
        "Challenge_closed_time":1665051150292,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664972415760,
        "Challenge_favorite_count":18.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1036179\/how-to-download-mlflow-model-from-azure-databricks.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":5.0,
        "Challenge_reading_time":5.91,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":21.8707033333,
        "Challenge_title":"How to download mlflow model artifacts from Azure Databricks workspace to local directory?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":73,
        "Platform":"Tool-specific",
        "Solution_body":"@SriramReddy-9682 Thanks for the question. To download a model from Databricks workspace you need to do two things:\n\nSet MLFlow tracking URI to databricks using python API\n\nSetup databricks authentication. I prefer authenticating by setting the following environment variables, you can also use databricks CLI to authenticate:\n\nDATABRICKS_HOST\n\nDATABRICKS_TOKEN\nHere's a basic code snippet to download a model from Databricks workspace model registry:\n\n import os\n import mlflow\n from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n    \n model_name = \"example-model-name\"\n model_stage = \"Staging\"  # Should be either 'Staging' or 'Production'\n    \n mlflow.set_tracking_uri(\"databricks\")\n    \n os.makedirs(\"model\", exist_ok=True)\n local_path = ModelsArtifactRepository(\n     f'models:\/{model_name}\/{model_stage}').download_artifacts(\"\", dst_path=\"model\")\n    \n print(f'{model_stage} Model {model_name} is downloaded at {local_path}')\n\n\n\nRunning above python script will download an ML model in the model directory.\n\nContainerizing MLFlow model serving with Docker\n\nFor more information you can follow this article from Akshay Milmile",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"download model workspac involv set track uri api set authent authent set environ variabl cli authent download model workspac model registri container model serv docker",
        "Solution_link_count":0.0,
        "Solution_original_content":"sriramreddi download model workspac set track uri api setup authent prefer authent set environ variabl cli authent host token download model workspac model registri import import store artifact model artifact repo import modelsartifactrepositori model model model stage stage stage set track uri makedir model local path modelsartifactrepositori model model model stage download artifact dst path model print model stage model model download local path run download model model directori container model serv docker articl akshai milmil",
        "Solution_preprocessed_content":"download model workspac set track uri api setup authent prefer authent set environ variabl cli authent download model workspac model registri import import import modelsartifactrepositori stage stage modelsartifactrepositori model download run download model model directori container model serv docker articl akshai milmil",
        "Solution_readability":16.2,
        "Solution_reading_time":14.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":125.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":17.8105555556,
        "Challenge_answer_count":1,
        "Challenge_body":"Where can I find the actual references to API definitions and descriptions for ModelBiasMonitor and ModelExplainabilityMonitor Classes?\n\nI can a find a few mentions in the Amazon SageMaker documentation in the following links. https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model_monitor.html https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_model_monitor\/fairness_and_explainability\/SageMaker-Model-Monitor-Fairness-and-Explainability.html\n\nWhere can I find the actual reference and the code implementation for these Classes?",
        "Challenge_closed_time":1611571671000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611507553000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU9SPQKzelSUu3dr-D4zaXHQ\/api-definition-for-model-bias-monitor-and-model-explainability-monitor",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":27.4,
        "Challenge_reading_time":8.24,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":17.8105555556,
        "Challenge_title":"API definition for ModelBiasMonitor and ModelExplainabilityMonitor",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":25.0,
        "Challenge_word_count":54,
        "Platform":"Tool-specific",
        "Solution_body":"The actual reference to the classes can be found here: https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/model_monitor\/clarify_model_monitoring.py\nIt encapsulates the definitions and descriptions for all of SageMaker Clarify related monitoring classes.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"modelbiasmonitor modelexplainabilitymonitor class link http github com sdk blob master src model monitor clarifi model monitor link definit descript clarifi relat monitor class",
        "Solution_link_count":1.0,
        "Solution_original_content":"class http github com sdk blob master src model monitor clarifi model monitor encapsul definit descript clarifi relat monitor class",
        "Solution_preprocessed_content":"class encapsul definit descript clarifi relat monitor class",
        "Solution_readability":24.7,
        "Solution_reading_time":3.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":0.2574272222,
        "Challenge_answer_count":2,
        "Challenge_body":"I repeat the example from https:\/\/docs.microsoft.com\/en-us\/azure\/azure-sql-edge\/deploy-onnx \"Deploy and make predictions with an ONNX model and SQL machine learning\" In this quickstart, you'll learn how to train a model, convert it to ONNX, deploy it to Azure SQL Edge, and then run native PREDICT on data using the uploaded ONNX model.\n\nSuccessfully create a model using Python, convert to onnx format, I test the model using Python, save the model to the database, load the necessary data and try to execute the SQL query\nUSE onnx\nDECLARE @model VARBINARY(max) = (\nSELECT DATA\nFROM dbo.models\nWHERE id = 1\n);\nWITH predict_input\nAS (\nSELECT TOP (1000) [id]\n, CRIM\n, ZN\n, INDUS\n, CHAS\n, NOX\n, RM\n, AGE\n, DIS\n, RAD\n, TAX\n, PTRATIO\n, B\n, LSTAT\nFROM [dbo].[features]\n)\nSELECT predict_input.id\n, p.variable1 AS MEDV\nFROM PREDICT(MODEL = @model, DATA = predict_input, RUNTIME=ONNX) WITH (variable1 FLOAT) AS p;\n\nAs a result I get an error Msg 102, Level 16, State 5, Line 27 Incorrect syntax near 'RUNTIME'.\n\nI can't figure out what's wrong. The documentation clearly says \"The RUNTIME = ONNX argument is only available in Azure SQL Edge, Azure Synapse Analytics, and is in Preview in Azure SQL Managed Instance.\"",
        "Challenge_closed_time":1650461393248,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650460466510,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/819485\/azure-sql-managed-instance-predict-with-an-onnx-mo.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":11.6,
        "Challenge_reading_time":15.34,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.2574272222,
        "Challenge_title":"Azure SQL Managed Instance PREDICT with an ONNX model",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":193,
        "Platform":"Tool-specific",
        "Solution_body":"I think there is a misunderstanding here. The quickstart article named \"Deploy and make predictions with an ONNX model and SQL machine learning\" can be successfully implemented only with Azure SQL Edge and cannot be implemented with Azure SQL Managed Instance.\n\nYou cannot have an ONNX model and make predictions with it on Azure SQL Managed Instance. Please deploy Azure SQL Edge on an IoT device using this documentation.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"quickstart articl implement sql edg sql instanc deploi sql edg iot devic document",
        "Solution_link_count":0.0,
        "Solution_original_content":"misunderstand quickstart articl deploi predict onnx model sql successfulli implement sql edg implement sql instanc onnx model predict sql instanc deploi sql edg iot devic document",
        "Solution_preprocessed_content":"misunderstand quickstart articl deploi predict onnx model sql successfulli implement sql edg implement sql instanc onnx model predict sql instanc deploi sql edg iot devic document",
        "Solution_readability":10.0,
        "Solution_reading_time":5.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":29.7170177778,
        "Challenge_answer_count":1,
        "Challenge_body":"I have been using Sagemaker Studio Notebook and suddenly it started hanging. When this happens, the notebook freezes completely. Than I have to wait some seconds (the delay duration is not constant and is common to reach about 30 seconds) and then it just freezes again, making its usage impossible. I was using a temporary account provided by Udacity and after trying different approaches to find and solve the problem, I switched to a personal account but the problem persists. Approaches I have tried so far:\n\nShutdow and start kernel\nRestart kernel\nRestart kernel and clear outputs\nLog out and Login (from Sagemaker)\nLog out and Login (from AWS)\nChange region\nTrying a different browser (I tried Chrome and Firefox)\nTrying using other account (personal)\n\nI also checked CloudWatch logs but didn't find anything that seemed unusual.",
        "Challenge_closed_time":1657857148680,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657750167416,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUbUkR0L2-Q1CcAHtTbLYJmg\/sagemaker-notebook-keeps-hanging-freezing",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.4,
        "Challenge_reading_time":10.81,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":29.7170177778,
        "Challenge_title":"Sagemaker Notebook keeps hanging\/freezing",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":50.0,
        "Challenge_word_count":140,
        "Platform":"Tool-specific",
        "Solution_body":"The most likely cause of this from my experience is a (very) large number of active git changes.\n\nGiven your \"current\" working folder (the one you're navigated to in the folder sidebar menu), the jupyterlab-git integration regularly checks if you're inside a git repository and polls for changes in that repository if so.\n\nWhen this list is very large, I've sometimes seen it cause significant slowdowns in the overall UI because of the way the underlying (open-source) extension works. This has been discussed before for example in this GitHub issue - which is now marked closed but I've still seen it happening.\n\nFor example, maybe you (like me \ud83d\ude05) forgot to gitignore a data folder or node_modules and generated thousands of untracked files there: You might see a significant slowdown whenever you're navigated to a folder within the scope of that git repo.\n\nSuggested solution would be:\n\nUse the folder sidebar to navigate anywhere other than the affected git repository (e.g. to your root folder?), and you should see the slowdown resolve pretty much immediately if this is the underlying cause\nNow the tricky task of finding and clearing up the problemmatic folder(s) without navigating to them in the folder GUI:\nYou could use a System Terminal, cd to the affected folder and run git status to see where the many changes are hiding, if you're not sure already\nAdd a .gitignore file (or modify your existing one) to make git ignore those changes. Because it starts with a dot, .gitignore is hidden by default in the JupyterLab file browser anyway. I usually use a system terminal to e.g. cp myrepo\/.gitignore gitignore.txt to create a visible copy (somewhere other than the repository folder which you're trying to avoid navigating to!) and then mv gitignore.txt myrepo\/.gitignore to overwrite with my edited version\n\nAlternatively (if e.g. it's a folder full of new files that you no longer care about like node_modules) you could just slog through the slowness to delete the problemmatic folder in the UI - but of course the problem would return if you re-created them later without .gitignore.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"larg activ git folder sidebar navig affect git repositori add gitignor file git ignor hide termin affect folder run git statu slog slow delet problemat folder return creat later gitignor",
        "Solution_link_count":0.0,
        "Solution_original_content":"larg activ git folder navig folder sidebar menu jupyterlab git integr regularli insid git repositori poll repositori list larg signific slowdown overal underli open sourc extens github mark close mayb forgot gitignor data folder node modul gener thousand untrack file signific slowdown navig folder scope git repo folder sidebar navig affect git repositori root folder slowdown pretti immedi underli tricki task clear problemmat folder navig folder gui termin affect folder run git statu hide add gitignor file modifi git ignor start dot gitignor hidden default jupyterlab file browser termin myrepo gitignor gitignor txt creat visibl copi repositori folder avoid navig gitignor txt myrepo gitignor overwrit edit version folder file longer care node modul slog slow delet problemmat folder cours return creat later gitignor",
        "Solution_preprocessed_content":"larg activ git folder integr regularli insid git repositori poll repositori list larg signific slowdown overal underli extens github mark close mayb forgot gitignor data folder gener thousand untrack file signific slowdown navig folder scope git repo folder sidebar navig affect git repositori slowdown pretti immedi underli tricki task clear problemmat folder navig folder gui termin affect folder run git statu hide add gitignor file git ignor start dot gitignor hidden default jupyterlab file browser termin creat visibl copi overwrit edit version slog slow delet problemmat folder cours return later gitignor",
        "Solution_readability":10.5,
        "Solution_reading_time":25.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":346.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":0.5526797223,
        "Challenge_answer_count":1,
        "Challenge_body":"According to the doc ( https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/smd_model_parallel_general.html ), there are different parameters depending on the version of smdistributed-modelparallel module \/ package. However, I am unable to find a way to check the version (e.g. via sagemaker python SDK) or just from the training container documentation (e.g. https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md#huggingface-training-containers ).\n\nAny idea?\n\nThanks!",
        "Challenge_closed_time":1660807962126,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660805972479,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUsfpWY8CuRsiyHg_x7qyJzw\/how-to-check-smdistributed-modelparallel-version",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":20.2,
        "Challenge_reading_time":7.23,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.5526797223,
        "Challenge_title":"How to check smdistributed-modelparallel version?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":31.0,
        "Challenge_word_count":50,
        "Platform":"Tool-specific",
        "Solution_body":"Have not yet found a programmatic way to check the version.\n\nHowever, for each DLC (Deep Learning Container) available at https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md , we can look at the corresponding docker build files.\n\nE.g. for PyTorch 1.10.2 with HuggingFace transformers DLC, the corresponding dockerfile is here: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/huggingface\/pytorch\/training\/docker\/1.10\/py3\/cu113\/Dockerfile.gpu\n\nAnd we can see that the version: smdistributed_modelparallel-1.8.1-cp38-cp38-linux_x86_64.whl.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"programmat version smdistribut modelparallel modul packag docker build file dlc http github com blob master imag pytorch huggingfac transform dlc dockerfil http github com blob master huggingfac pytorch train docker dockerfil gpu version smdistribut modelparallel linux whl",
        "Solution_link_count":2.0,
        "Solution_original_content":"programmat version dlc http github com blob master imag docker build file pytorch huggingfac transform dlc dockerfil http github com blob master huggingfac pytorch train docker dockerfil gpu version smdistribut modelparallel linux whl",
        "Solution_preprocessed_content":"programmat version dlc docker build file pytorch huggingfac transform dlc dockerfil version",
        "Solution_readability":16.1,
        "Solution_reading_time":7.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":52.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":30.1195555556,
        "Challenge_answer_count":3,
        "Challenge_body":"I am working on a project to calculate surface area from digital floor maps. I am currently experimenting with azure cognitive services - Custom computer vision. However I don't know if this is the right track.\n\nIf possible I would like to use a existent tool instead of reinventing the wheel. Has anyone experience with this and can provide me with some guidance?",
        "Challenge_closed_time":1631217055323,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631108624923,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/544199\/custom-computer-vision-for-surface-calculations-on.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":5.32,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":30.1195555556,
        "Challenge_title":"Custom computer vision for surface calculations on digital floor maps",
        "Challenge_topic":"Database Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":71,
        "Platform":"Tool-specific",
        "Solution_body":"Sure, thanks for clarifying. I agree, an out of box approach would be to use computer vision or custom vision to detect objects and then use the metadata to calculate the surface area. I haven't seen an existing solution using Azure Cognitive services at the moment, so you'd most likely have to use a heuristic approach.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"vision vision detect object metadata calcul surfac area cognit servic heurist",
        "Solution_link_count":0.0,
        "Solution_original_content":"clarifi agre box vision vision detect object metadata calcul surfac area haven cognit servic moment heurist",
        "Solution_preprocessed_content":"clarifi agre box vision vision detect object metadata calcul surfac area haven cognit servic moment heurist",
        "Solution_readability":9.4,
        "Solution_reading_time":3.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":56.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":9.1561111111,
        "Challenge_answer_count":2,
        "Challenge_body":"Customer wants to know if AWS AI\/ML services integrate with Power BI. The customer currently uses Power BI that integrates with Azure ML for sentiment analysis, opinion mining, etc. Customer is looking for a push button solution where the business analyst can do text analytics on the response from the model. Is there a way to do this on AWS or a marketplace solution?",
        "Challenge_closed_time":1607528438000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607495476000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU4VexAnfiSFi4Jf5i9RyO_A\/aws-ai-ml-integration-with-power-bi",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":4.95,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":9.1561111111,
        "Challenge_title":"AWS AI\/ML integration with Power BI",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":148.0,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Solution_body":"PowerBI can connect to Amazon Redshift and leverage the new SQL based ML capability in Redshift that uses Sagemaker under the hood.\n\nAs an alternative thought the customer can integrate Amazon Sagemaker Model with Amazon Quicksight to achieve functionality very similar to PowerBI with Azure ML. Quicksight does have some embedded ML capability like forecasting and anomaly detection but Opinion mining is not one of them yet.\n\nYou should be able to leverage Blazing Text Algorithm in Sagemaker or some market place solution like Twinword sentiment model in sagemaker for sentiment analysis for Text mining after the integration.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"powerbi connect redshift sql base capabl redshift hood integr model quicksight achiev function powerbi quicksight mine capabl leverag blaze text algorithm marketplac twinword sentiment model sentiment analysi text mine integr bias summari",
        "Solution_link_count":0.0,
        "Solution_original_content":"powerbi connect redshift leverag sql base capabl redshift hood integr model quicksight achiev function powerbi quicksight embed capabl forecast anomali detect mine leverag blaze text algorithm market twinword sentiment model sentiment analysi text mine integr",
        "Solution_preprocessed_content":"powerbi connect redshift leverag sql base capabl redshift hood integr model quicksight achiev function powerbi quicksight embed capabl forecast anomali detect mine leverag blaze text algorithm market twinword sentiment model sentiment analysi text mine integr",
        "Solution_readability":14.0,
        "Solution_reading_time":7.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":98.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":0.2554944445,
        "Challenge_answer_count":6,
        "Challenge_body":"I tried to deploy a VM to Azure Machine Learning, but I get the error message \"You do not have enough quota for the following VM sizes. Click here to view and request quota.\" And the VM cannot be deployed.\n\nBut I have enough quota (24 CPUs).\n\nWhat is causing the problem?\n\nI'm using Azure's Free trial plan.",
        "Challenge_closed_time":1623772934643,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623772014863,
        "Challenge_favorite_count":19.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/437136\/can39t-deploy-a-vm-on-the-azure-machine-learning.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":3.4,
        "Challenge_reading_time":4.23,
        "Challenge_score_count":4.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.2554944445,
        "Challenge_title":"Can't deploy a VM on the Azure Machine Learning.",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":66,
        "Platform":"Tool-specific",
        "Solution_body":"Hi @Sss-1842 ,\n\nthere are different quotas in Azure:\n\nThere are quotas for vCPUs per Azure Region\n\n\nIn addition there are quotas for vCPUs per VM Series\n\nBoth quotas (for Azure Region and VM Series) must fit the requirements.\n\nIt seems like the quota for vCPUs per region is ok but you haven't enough vCPUs per VM series.\nYou can check your quotas by the link you marked with the red line in your screenshot.\n\n(If the reply was helpful please don't forget to upvote and\/or accept as answer, thank you)\n\nRegards\nAndreas Baumgarten",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"quota vcpu region vcpu seri quota vcpu region vcpu seri messag quota link",
        "Solution_link_count":0.0,
        "Solution_original_content":"quota quota vcpu region addit quota vcpu seri quota region seri fit quota vcpu region haven vcpu seri quota link mark red line screenshot repli forget upvot accept andrea baumgarten",
        "Solution_preprocessed_content":"quota quota vcpu region addit quota vcpu seri quota fit quota vcpu region haven vcpu seri quota link mark red line screenshot repli forget upvot accept andrea baumgarten",
        "Solution_readability":10.0,
        "Solution_reading_time":6.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":93.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":44.4506683334,
        "Challenge_answer_count":2,
        "Challenge_body":"I am new to docker and environments. This could be basics but i have been trying to install packages in my pyproject.toml file in Dockerfile without success.\n\nI have tried using poetry to export requirements.txt file and using it with the\nEnvironment.from_pip_requirements('requirements.txt') function and a Dockerfile.\n\nBut could there be any elegant solution to use toml file directly for creating a custom environment ?",
        "Challenge_closed_time":1638981076963,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638821054557,
        "Challenge_favorite_count":8.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/653688\/custom-dockerfile-on-azure-environment-with-python.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":5.99,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":44.4506683334,
        "Challenge_title":"Custom Dockerfile on Azure Environment with python poetry",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Solution_body":"Thanks for the response, @Ram-msft\nUsing the Dockerfile :\n\n FROM python:3.8-slim-buster\n ENV PYTHONUNBUFFERED=1 \\\n     PYTHONDONTWRITEBYTECODE=1 \\\n     PIP_NO_CACHE_DIR=1 \\\n     PIP_DISABLE_PIP_VERSION_CHECK=1 \\\n     POETRY_VERSION=1.1.7 \\\n     PYLINT_VERSION=2.9.4\n    \n RUN pip install pylint==$PYLINT_VERSION \\\n     && pip install \"poetry==$POETRY_VERSION\" \n    \n COPY pyproject.toml .\/\n RUN poetry config virtualenvs.create false",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"dockerfil instal packag file instal poetri pylint pip copi pyproject toml file docker imag final run poetri instal instal packag list toml file",
        "Solution_link_count":0.0,
        "Solution_original_content":"respons ram msft dockerfil slim buster env pythonunbuff pythondontwritebytecod pip cach dir pip disabl pip version poetri version pylint version run pip instal pylint pylint version pip instal poetri poetri version copi pyproject toml run poetri config virtualenv creat",
        "Solution_preprocessed_content":"respons dockerfil env pythonunbuff pythondontwritebytecod run pip instal pip instal copi run poetri config",
        "Solution_readability":14.6,
        "Solution_reading_time":5.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":9.843735,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\n\nI am trying to create an Azure ML Environment using a Dockerfile but it contains the 'COPY' instruction.\n\nFrom the documentation of Environment.from_dockerfile ( https:\/\/docs.microsoft.com\/fr-fr\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#from-dockerfile-name--dockerfile--conda-specification-none--pip-requirements-none- ), I can not find a way to give it some files along with the Dockerfile itself.\n\nSo, how to pass context to enable using COPY in the Dockerfile ?\n\nThank you for your time !",
        "Challenge_closed_time":1634774742763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634739305317,
        "Challenge_favorite_count":9.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/597612\/azuremlpython-sdkenvironmentdocker-docker-copy-mis.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":7.84,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":9.843735,
        "Challenge_title":"[Azure][ML][Python SDK][Environment][Docker] Docker copy missing context",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":63,
        "Platform":"Tool-specific",
        "Solution_body":"Docker context is not supported with AzureML Python SDK at the moment. Context support will added later this year",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"context docker later year sdk",
        "Solution_link_count":0.0,
        "Solution_original_content":"docker context sdk moment context later year",
        "Solution_preprocessed_content":"docker context sdk moment context later year",
        "Solution_readability":4.6,
        "Solution_reading_time":1.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":50.3174552778,
        "Challenge_answer_count":1,
        "Challenge_body":"Can you please share any code examples for training random forests with GPU on Azure using libraries.\nI want to run on the multiple nodes.",
        "Challenge_closed_time":1595231268032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595050125193,
        "Challenge_favorite_count":34.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/49008\/random-forests-on-azure-gpu-vm-using-the-sdk.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":4.0,
        "Challenge_reading_time":2.22,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":50.3174552778,
        "Challenge_title":"Random forests on Azure GPU VM using the SDK",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":33,
        "Platform":"Tool-specific",
        "Solution_body":"@vautoml-0887 Thanks for the question. You can run LightGBM with boosting=random_forest, Please follow the below documentation:\nhttps:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst#boosting\n\n\n\n\nHere is a general tutorial on how to run LightGBM on GPU, You can run it on any Azure GPU VM:\nhttps:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/GPU-Tutorial.rst\n\n\n\n\nIf you need to run it on multiple nodes, there is also a distributed spark implementation available at https:\/\/github.com\/Azure\/mmlspark.\n\n\n\n\nRandom Forests for the GPU using PyCUDA: https:\/\/pypi.org\/project\/cudatree\/",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"lightgbm boost random forest train random forest gpu document http github com lightgbm blob master doc paramet rst boost gener tutori run lightgbm gpu http github com lightgbm blob master doc gpu tutori rst run multipl node distribut spark implement http github com mmlspark option pycuda train random forest gpu packag cudatre purpos",
        "Solution_link_count":4.0,
        "Solution_original_content":"vautoml run lightgbm boost random forest document http github com lightgbm blob master doc paramet rst boost gener tutori run lightgbm gpu run gpu http github com lightgbm blob master doc gpu tutori rst run multipl node distribut spark implement http github com mmlspark random forest gpu pycuda http pypi org cudatre",
        "Solution_preprocessed_content":"run lightgbm document gener tutori run lightgbm gpu run gpu run multipl node distribut spark implement random forest gpu pycuda",
        "Solution_readability":15.5,
        "Solution_reading_time":7.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":4.1436111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Is there official documentation showing the regions in which SageMaker AutoPilot is supported? From my understanding, it should work with the SDK wherever SageMaker is supported, while in the no-code mode only where SageMaker Studio is available. Is this true?\n\nThanks!",
        "Challenge_closed_time":1596635055000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596620138000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU_7jk19ozQSeyjTAR_D_hEA\/sage-maker-auto-pilot-regions",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":3.73,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.1436111111,
        "Challenge_title":"SageMaker AutoPilot Regions",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":34.0,
        "Challenge_word_count":43,
        "Platform":"Tool-specific",
        "Solution_body":"SageMaker Autopilot works in all the regions where Amazon SageMaker is available today as noted in this blog post \"Amazon SageMaker Autopilot \u2013 Automatically Create High-Quality Machine Learning Models With Full Control And Visibility\". In addition, Autopilot is also integrated with Amazon SageMaker Studio, which is available in us-east-1, us-east-2, us-west-2 and eu-west-1. For a current list of available regions, please check the AWS Regional Services List.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"autopilot region sdk mode studio region servic list list region",
        "Solution_link_count":0.0,
        "Solution_original_content":"autopilot region todai note blog autopilot automat creat high qualiti model control visibl addit autopilot integr studio list region region servic list",
        "Solution_preprocessed_content":"autopilot region todai note blog autopilot automat creat model control visibl addit autopilot integr studio list region region servic list",
        "Solution_readability":14.2,
        "Solution_reading_time":5.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":5.1083627778,
        "Challenge_answer_count":2,
        "Challenge_body":"When trying to use the Azure Pricing estimate in the Azure Pricing Calculator, the \"Estimated monthly costs\" seems to include but also far exceeds the compute cost. Does this Estimated Monthly cost include the other resources that get created?\neg. Azure Container Registry Basic account, Azure Block Blob Storage (general purpose v1), Key Vault\n\nThank you\nPeter",
        "Challenge_closed_time":1609285909056,
        "Challenge_comment_count":1,
        "Challenge_created_time":1609267518950,
        "Challenge_favorite_count":6.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/213635\/does-the-34estimated-monthly-costs34-for-azure-mac.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":12.2,
        "Challenge_reading_time":6.55,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":5.1083627778,
        "Challenge_title":"Does the \"Estimated monthly costs\" for Azure Machine Learning in the Price Calculator include all other non-compute \"additional resources\" created in the workspace",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Solution_body":"Hi Peter.\n\nThanks for reaching out. I tried your selections but I don't have the same service as you. Have you selected other services in you calculator?\n\nFor your question, the estimated price is only for Azure Machine Learning Service. You need to select all services you need in the calculator like below:\n\n\nPlease note I only use random number for the example.\n\nFrom the number I guess you have selected 2 Machine Learning Services and also other services since they added to your basket when you clicked them, you can click the button to see what you have all as below.\n\n\nAlso you are selecting Reservation service, detail: https:\/\/docs.microsoft.com\/en-us\/azure\/cost-management-billing\/reservations\/save-compute-costs-reservations\n\nRegards,\nYutong",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"estim monthli cost price calcul cost servic accur estim select servic calcul registri account block blob storag gener purpos kei vault click button select select reserv servic save comput cost",
        "Solution_link_count":1.0,
        "Solution_original_content":"peter reach tri select servic select servic calcul estim price servic select servic calcul note random guess select servic servic basket click click button select reserv servic http doc com cost bill reserv save comput cost reserv yutong",
        "Solution_preprocessed_content":"peter reach tri select servic select servic calcul estim price servic select servic calcul note random guess select servic servic basket click click button select reserv servic yutong",
        "Solution_readability":8.8,
        "Solution_reading_time":9.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":112.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":55.4034008333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In the table view of a project, is it possible to show only the columns that have different values among runs? This would be very useful to quickly explore how changing parameters affect the model.<\/p>",
        "Challenge_closed_time":1661383337880,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661183885637,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/show-only-columns-with-different-values-in-experiments-table\/2972",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":3.28,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":55.4034008333,
        "Challenge_title":"Show only columns with different values in experiments table",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":69.0,
        "Challenge_word_count":43,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/enajx\">@enajx<\/a> , would the run comparer table work for your use case, see <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/run-comparer\">here<\/a>.<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"run compar tabl featur compar run column valu",
        "Solution_link_count":1.0,
        "Solution_original_content":"enajx run compar tabl",
        "Solution_preprocessed_content":null,
        "Solution_readability":19.4,
        "Solution_reading_time":2.59,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":17.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":26.0827755556,
        "Challenge_answer_count":1,
        "Challenge_body":"We are in a process to move all of our IAM users to aws SSO we used to have this policy for sagemaker :\n\n\"\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListTags\",\n                \"sagemaker:DeleteNotebookInstance\",\n                \"sagemaker:StopNotebookInstance\",\n                \"sagemaker:CreatePresignedNotebookInstanceUrl\",\n                \"sagemaker:DescribeNotebookInstance\",\n                \"sagemaker:StartNotebookInstance\",\n                \"sagemaker:UpdateNotebookInstance\"\n            ],\n            \"Resource\": \"arn:aws:sagemaker:::notebook-instance\/${aws:username}*\"\n        },\n        {\n            \"Sid\": \"VisualEditor1\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListNotebookInstanceLifecycleConfigs\",\n                \"sagemaker:ListNotebookInstances\",\n                \"sagemaker:ListCodeRepositories\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n\n\"\n\nthis would give access to each user to use his\\hers own notebook now on the new SSO permission set i gave this\n\n\"\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"glue:CreateScript\",\n                \"secretsmanager:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListTags\",\n                \"sagemaker:DeleteNotebookInstance\",\n                \"sagemaker:StopNotebookInstance\",\n                \"sagemaker:CreatePresignedNotebookInstanceUrl\",\n                \"sagemaker:Describe*\",\n                \"sagemaker:StartNotebookInstance\",\n                \"sagemaker:UpdateNotebookInstance\",\n                \"sagemaker:CreatePresignedDomainUrl\",\n                \"sagemaker:*\"\n            ],\n            \"Resource\": \"arn:aws:sagemaker:::notebook-instance\/*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"aws:ResourceTag\/Owner\": \"${identitystore:UserId}\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListTags\",\n                \"sagemaker:Describe*\",\n                \"sagemaker:StartNotebookInstance\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\"\n\n\nthis is what i tried but i cant make it work please assist?",
        "Challenge_closed_time":1658148377804,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658054479812,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUruheXJHaQVu_S9LIzUyDAw\/policy-that-allows-only-one-sso-user-to-access-a-resource",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":38.9,
        "Challenge_reading_time":22.58,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":26.0827755556,
        "Challenge_title":"Policy that allows only one SSO user to access a resource",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":90.0,
        "Challenge_word_count":128,
        "Platform":"Tool-specific",
        "Solution_body":"Hello,\n\nI understand that you are currently trying to restrict access to Sagemaker notebook using SSO identity's UserID.\n\nCurrently, I leveraged your provided SSO Permission set and tweaked it out as you can see below, and finally tested it out on AWS SageMaker Console by logging in as an AWS SSO User, and was able to see successful start\/stop\/describing of the SageMaker notebook (with Tags - Owner:UserId) corresponding to the SSO UserId.\n\n{\n\t\"Version\": \"2012-10-17\",\n\t\"Statement\": [\n\t\t{\n\t\t\t\"Effect\": \"Allow\",\n\t\t\t\"Action\": [\n\t\t\t\t\"glue:CreateScript\",\n\t\t\t\t\"secretsmanager:*\"\n\t\t\t],\n\t\t\t\"Resource\": \"*\"\n\t\t},\n\t\t{\n\t\t\t\"Effect\": \"Allow\",\n\t\t\t\"Action\": [\n\t\t\t\t\"sagemaker:ListTags\",\n\t\t\t\t\"sagemaker:DeleteNotebookInstance\",\n\t\t\t\t\"sagemaker:StopNotebookInstance\",\n\t\t\t\t\"sagemaker:CreatePresignedNotebookInstanceUrl\",\n\t\t\t\t\"sagemaker:Describe*\",\n\t\t\t\t\"sagemaker:StartNotebookInstance\",\n\t\t\t\t\"sagemaker:UpdateNotebookInstance\",\n\t\t\t\t\"sagemaker:CreatePresignedDomainUrl\"\n\t\t\t],\n\t\t\t\"Resource\": \"arn:aws:sagemaker:us-east-1:7XXXXXXXXX:notebook-instance\/*\",\n\t\t\t\"Condition\": {\n\t\t\t\t\"StringEquals\": {\n\t\t\t\t\t\"sagemaker:ResourceTag\/Owner\": \"${identitystore:UserId}\"\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t{\n\t\t\t\"Sid\": \"VisualEditor1\",\n\t\t\t\"Effect\": \"Allow\",\n\t\t\t\"Action\": [\n\t\t\t\t\"sagemaker:ListNotebookInstanceLifecycleConfigs\",\n\t\t\t\t\"sagemaker:ListNotebookInstances\",\n\t\t\t\t\"sagemaker:ListCodeRepositories\"\n\t\t\t],\n\t\t\t\"Resource\": \"*\"\n\t\t}\n\t]\n}\n\n\nHowever, in case if this SSO User tried to stop any other Sagemaker notebooks, which didn't have the tags corresponding to their UserId, then the following errors were observed as expected behavior -\n\nUser: arn:aws:sts::7XXXXXXXXX:assumed-role\/AWSReservedSSO_SageMXXXXXXXXXbe\/test1 is not authorized to perform: sagemaker:StopNotebookInstance on resource: arn:aws:sagemaker:us-east-1:7XXXXXXXXX:notebook-instance\/userachecking because no identity-based policy allows the sagemaker:StopNotebookInstance action\n\nor \n\nUser: arn:aws:sts::7XXXXXXXXX:assumed-role\/AWSReservedSSO_SageMXXXXXXXXXbe\/test1 is not authorized to perform: sagemaker:DescribeNotebookInstance on resource: arn:aws:sagemaker:us-east-1:7XXXXXXXXX:notebook-instance\/Test1Check because no identity-based policy allows the sagemaker:DescribeNotebookInstance action\n\n\nAlso, please note that unlike your provided IAM policy, your SSO permission set policy was missing the action - sagemaker:ListNotebookInstances which also raised an error for not being able to list out the notebook instances on AWS SageMaker Console in my testing. Hence, I had added the appropriate Sagemaker list actions to your permission set as well.\n\nAdditional Information -\n\na. ${identitystore:UserId} -> Each user in the AWS SSO identity store is assigned a unique UserId. You can view the UserId for your users by using the AWS SSO console and navigating to each user or by using the DescribeUser API action. [1]\n\nb. ListNotebookInstances -> Returns a list of the SageMaker notebook instances in the requester's account in an AWS Region. [2]\n\nc. ResourceTag -> You can use the ResourceTag\/key-name condition key to determine whether to allow access to the resource based on the tags that are attached to the resource. [3][4]\n\nd. sagemaker:ResourceTag\/ -> Filters access by the preface string for a tag key and value pair attached to a resource [5]\n\ne. sagemaker:ResourceTag\/${TagKey} -> Filters access by a tag key and value pair [5]\n\nI hope the shared information is insightful to your query. In case, if you have any other queries or concerns regarding AWS SSO or Sagemaker services or any account specific configuration that you would like to discuss, then please feel free to reach out to our team directly by creating a support case with our premium support team.\n\nHave a wonderful day ahead and stay safe.\n\nReferences:\n\n[1] https:\/\/docs.aws.amazon.com\/singlesignon\/latest\/userguide\/using-predefined-attributes.html\n\n[2] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ListNotebookInstances.html\n\n[3] https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_tags.html\n\n[4] https:\/\/aws.amazon.com\/blogs\/security\/simplify-granting-access-to-your-aws-resources-by-using-tags-on-aws-iam-users-and-roles\/\n\n[5] https:\/\/docs.aws.amazon.com\/service-authorization\/latest\/reference\/list_amazonsagemaker.html#amazonsagemaker-policy-keys",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"sso permiss set tweak restrict access notebook sso ident userid add list action permiss set resourcetag kei condit kei determin allow access resourc base tag attach resourc resourcetag filter access prefac tag kei valu pair attach resourc",
        "Solution_link_count":5.0,
        "Solution_original_content":"restrict access notebook sso ident userid leverag sso permiss set tweak final test consol log sso start stop notebook tag owner userid sso userid version statement allow action glue createscript secretsmanag resourc allow action listtag deletenotebookinst stopnotebookinst createpresignednotebookinstanceurl startnotebookinst updatenotebookinst createpresigneddomainurl resourc arn notebook instanc condit stringequ resourcetag owner identitystor userid sid visualeditor allow action listnotebookinstancelifecycleconfig listnotebookinst listcoderepositori resourc sso tri stop notebook tag userid observ arn st role awsreservedsso sagemb test author perform stopnotebookinst resourc arn notebook instanc useracheck ident base polici allow stopnotebookinst action arn st role awsreservedsso sagemb test author perform describenotebookinst resourc arn notebook instanc testcheck ident base polici allow describenotebookinst action note unlik iam polici sso permiss set polici miss action listnotebookinst rais list notebook instanc consol test list action permiss set addit identitystor userid sso ident store assign uniqu userid userid sso consol navig describeus api action listnotebookinst return list notebook instanc request account region resourcetag resourcetag kei condit kei determin allow access resourc base tag attach resourc resourcetag filter access prefac tag kei valu pair attach resourc resourcetag tagkei filter access tag kei valu pair hope share queri queri sso servic account configur free reach team directli creat premium team dai ahead stai safe http doc com singlesignon latest userguid predefin attribut html http doc com latest apirefer api listnotebookinst html http doc com iam latest userguid access tag html http com blog secur simplifi grant access resourc tag iam role http doc com servic author latest list html polici kei",
        "Solution_preprocessed_content":"restrict access notebook sso ident userid leverag sso permiss set tweak final test consol log sso notebook sso userid version statement resourc allow action resourc condit sid visualeditor allow action resourc sso tri stop notebook tag userid observ author perform stopnotebookinst resourc polici allow stopnotebookinst action author perform describenotebookinst resourc polici allow describenotebookinst action note unlik iam polici sso permiss set polici miss action listnotebookinst rais list notebook instanc consol test list action permiss set addit sso ident store assign uniqu userid userid sso consol navig describeus api action listnotebookinst return list notebook instanc request account region resourcetag condit kei determin allow access resourc base tag attach resourc resourcetag filter access prefac tag kei valu pair attach resourc filter access tag kei valu pair hope share queri queri sso servic account configur free reach team directli creat premium team dai ahead stai safe",
        "Solution_readability":18.6,
        "Solution_reading_time":53.68,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":434.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":198.4418677778,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello!\n\nI created an Azure ML pipeline in Python and used multiple PythonScriptSteps for each of my tasks. For example, I have three training steps running in parallel, so I create three PythonScriptSteps in a for loop with my train.py script and different data. Later, I came across the ModuleStep, which seems to do exactly this, but with an extra layer of (seemingly pointless) abstraction. What does the ModuleStep add to a PythonScriptStep?\n\nAlso, I imagined the ModuleStep might make it possible to use a custom PythonScriptStep in the pipeline designer (by creating a new drag and drop module), however this doesn't seem to be the case. Is there any way of doing this?",
        "Challenge_closed_time":1628020406187,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627306015463,
        "Challenge_favorite_count":9.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/489671\/what-is-the-point-of-azureml-modules.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":8.0,
        "Challenge_reading_time":8.7,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":198.4418677778,
        "Challenge_title":"What is the point of AzureML modules?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":120,
        "Platform":"Tool-specific",
        "Solution_body":"Just to close this question, I have since discovered that the ModuleStep does create a custom drag-and-drop module in the designer. I don't know if I'd missed this (I imagine so) or if this is a new feature. Either way, that's the answer. @YutongTie-MSFT can you confirm if this was recently added?",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"modulestep allow creation drag drop modul pipelin design unclear featur miss previous",
        "Solution_link_count":0.0,
        "Solution_original_content":"close modulestep creat drag drop modul design miss imagin featur yutongti msft",
        "Solution_preprocessed_content":"close modulestep creat modul design miss featur",
        "Solution_readability":6.0,
        "Solution_reading_time":3.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":13.2693258333,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI couldn\u2019t find a specific github repo for azureml-dataprep so I decided to also write you here. Can you forward it to the devs?\n\n\n\n\nazureml-dataprep (which is a depedency for azureml-dataset-runtime) has requirement cloudpickle<2.0.0 and >=1.1.0. However there is to my knowledage no breaking features going from cloudpickle==1.6.0 to cloudpickle==2.0.0. cloudpickle==2.0.0 introduces some very effective tools for serializing helper scripts which is very helful when working with azureml. So azureml-dataprep should allow cloudpickle<=2.0.0\n\nIntro to new cloudpickle:\nhttps:\/\/github.com\/cloudpipe\/cloudpickle#overriding-pickles-serialization-mechanism-for-importable-constructs\nPR:\nhttps:\/\/github.com\/cloudpipe\/cloudpickle\/pull\/417\nGithub issue:\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1637",
        "Challenge_closed_time":1637290125060,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637242355487,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/632441\/loosen-azureml-dataprep-requirements-to-cloudpickl.html",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":11.46,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":13.2693258333,
        "Challenge_title":"Loosen azureml-dataprep requirements to cloudpickle<=2.0.0",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":88,
        "Platform":"Tool-specific",
        "Solution_body":"@ThomasH-1455\n\nThank you so much for the contribute, I have sent an email to the author for the PR review and merge.\n\nHope this will help. Please let us know if any further queries.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"thomash contribut sent email author review merg hope queri forget click upvot button origin poster commun faster identifi remind come respons subscrib notif program shape futur volunt moder",
        "Solution_preprocessed_content":"contribut sent email author review merg hope queri forget click upvot button origin poster commun faster identifi remind come respons subscrib notif program shape futur volunt moder",
        "Solution_readability":7.0,
        "Solution_reading_time":7.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":51.0787569444,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hey everyone,<br>\nLet\u2019s say that I have a dataset with 50000 samples and I am training my model for 10 epochs. Now, in each epoch, I am recording the <em>per sample loss<\/em> (i.e. loss of each sample - Not the average loss of all samples). This means that there are 50000 loss values per epoch. I want to log these values for <em>each epoch<\/em>, so that I can later perform some analysis on how the loss values for the samples change as training progresses (And, if possible, observe the loss values of a particular sample across epochs). For reference, <a href=\"https:\/\/proceedings.neurips.cc\/paper\/2020\/file\/62000dee5a05a6a71de3a6127a68778a-Paper.pdf\" rel=\"noopener nofollow ugc\">this<\/a> paper tracks such  statistics. Here are two ways I can think of doing this -<\/p>\n<ul>\n<li>A simple way to do this is to update the values in a 50000x10 array, then log the array as a table at the end of training (I would obviously need to track which indices belong to which samples). However, I need to wait for the training to end in this scenario.<\/li>\n<li>I can also log each sample\u2019s statistic with wandb.log (Maybe put them under \u201csample_statistics\/\u201d to pull them more easily). This ensures that the metrics are logged as and when they are observed, however, I am not sure if this is the most optimal solution.<\/li>\n<\/ul>\n<p>Is there any other way in which I can do this so that I can analyse the resulting data effectively? Open to all suggestions!<br>\nThank you!<\/p>",
        "Challenge_closed_time":1656718364004,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656534480479,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/logging-metrics-for-each-sample-per-epoch\/2678",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":7.0,
        "Challenge_reading_time":18.55,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":51.0787569444,
        "Challenge_title":"Logging Metrics for each sample per epoch",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":89.0,
        "Challenge_word_count":248,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tataganesh\">@tataganesh<\/a> ,<\/p>\n<p>Thank you for writing in with your question.   Ideally what would be best in your case here is to create an Empty table and log per sample loss values per epoch and be able to see your data live in the UI. However, we currently,  don\u2019t support adding new rows to existing tables that you\u2019ve already logged. We are working on adding this functionality.<\/p>\n<p>In the meantime here are two approaches<\/p>\n<ol>\n<li>Keep the wandb.Table locally holding all the data in memory and logging it once.<\/li>\n<li>Keep logging the same table at each step, and just add new rows to it. The final table you log will have all the rows you want, and you\u2019ll be able to see the latest table logged in the UI. This would be risky if you have large table sizes.<\/li>\n<\/ol>\n<p>Please Note: If you were to look through our docs and come across the<a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#add-data\"> Add Data Incrementally<\/a> to  Tables doc, this functionality is currently broken and we are working on an active fix.  There is github issues thread <a href=\"https:\/\/github.com\/wandb\/client\/issues\/2981\" rel=\"noopener nofollow ugc\">here<\/a> where community members have posted workarounds for this, you may find it helpful.<\/p>\n<p>Please let me know if you have any questions.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"log sampl loss epoch train model dataset sampl epoch tabl local hold data memori log log tabl step add row note riski tabl size larg function row tabl team featur",
        "Solution_link_count":2.0,
        "Solution_original_content":"tataganesh write ideal creat tabl log sampl loss valu epoch data live row tabl youv log function tabl local hold data memori log log tabl step add row final tabl log row youll latest tabl log riski larg tabl size note doc come add data increment tabl doc function broken activ github thread commun member workaround mohammad",
        "Solution_preprocessed_content":"write ideal creat tabl log sampl loss valu epoch data live row tabl youv log function tabl local hold data memori log log tabl step add row final tabl log row youll latest tabl log riski larg tabl size note doc come add data increment tabl doc function broken activ github thread commun member workaround mohammad",
        "Solution_readability":7.6,
        "Solution_reading_time":17.08,
        "Solution_score_count":null,
        "Solution_sentence_count":15.0,
        "Solution_word_count":210.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":2.8375316667,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to use an Amazon Sagemaker endpoint for a custom classification model. The endpoint should only handle sporadic input (say a few times a week). For this purpose I want to employ autoscaling that scales the number of instances down to 0 when the endpoint is not used.\n\nAre there any costs associated with having an endpoint with 0 instances?\n\nThanks!",
        "Challenge_closed_time":1666371029704,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666360814590,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU0VGYdZe8TRivmtGHoiDDHw\/cost-of-autoscaling-endpoint-amazon-sage-maker-endpoint-to-zero",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":5.11,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.8375316667,
        "Challenge_title":"Cost of autoscaling endpoint Amazon SageMaker endpoint to zero",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":57.0,
        "Challenge_word_count":70,
        "Platform":"Tool-specific",
        "Solution_body":"You dont pay any compute costs for the duration when the endpoint size scales down to 0. But i think you can design it better. There are few other options for you to use in SageMaker Endpoint(assuming you are using realtime endpoint)\n\nTry using SageMaker Serverless Inference instead. Its purely serverless in nature so you pay only when the endpoint is serving inference. i think that would fit your requirement better.\nYou can think of using Lambda as well which will reduce your hosting costs. but you have to do more work in setting up the inference stack all by yourself.\nThere is also an option of SageMaker asynchronous inference but its mostly useful for inference which require longer time to process each request. The reason i mention this is it also support scale to 0 when no traffic is coming.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"serverless infer lambda reduc host cost pai endpoint serv infer asynchron infer option scale traffic suitabl infer longer process time cost associ endpoint instanc",
        "Solution_link_count":0.0,
        "Solution_original_content":"pai comput cost durat endpoint size scale design option endpoint realtim endpoint serverless infer pure serverless natur pai endpoint serv infer fit lambda reduc host cost set infer stack option asynchron infer infer longer time process request reason scale traffic come",
        "Solution_preprocessed_content":"pai comput cost durat endpoint size scale design option endpoint serverless infer pure serverless natur pai endpoint serv infer fit lambda reduc host cost set infer stack option asynchron infer infer longer time process request reason scale traffic come",
        "Solution_readability":7.1,
        "Solution_reading_time":9.77,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":141.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":1.2302777778,
        "Challenge_answer_count":1,
        "Challenge_body":"Does SageMaker pipe mode serve as a cost saving measure? Or is is just faster than file mode but generally not much cheaper? The cost savings of it might be 1. no need to copy data to training instances and 2. training instances need less space. Are these savings generally significant for customers?",
        "Challenge_closed_time":1590165887000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590161458000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QURbsBp9m5TsqKWWDdP8VJyw\/sage-maker-pipe-mode",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":5.2,
        "Challenge_reading_time":3.91,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.2302777778,
        "Challenge_title":"SageMaker Pipe Mode",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":33.0,
        "Challenge_word_count":55,
        "Platform":"Tool-specific",
        "Solution_body":"To the best of my understanding, pipe mode decreases startup times, but frequently increases the bill.\n\nThe SageMaker billing starts after the data has been copied onto the container in File mode and control is transferred to the user script.\n\nReading the data in pipe mode starts after control is transferred, so the data transfer happens during the billable time.\n\nFurther the data is, to the best of my knowledge, not hitting the disk (EBS). This is fast, but also means that if you pass over your data multiple times, you have to re-read it again, on your dime (S3 requests and container wait times).\n\nPipe mode is still a good idea. For example if you have only few passes over the data and the data is rather large, so that it would not fit on an EBS volume.\n\nAlso, in PyTorch for example, data loading can happen in parallel. So while the GPU is chucking away on one batch, the CPUs load and prepare the data for the next batch.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"pipe mode decreas startup time frequent increas read data pipe mode start control transfer data transfer billabl time pipe mode idea pass data data larg fit eb volum pytorch data load parallel gpu chuck awai batch cpu load prepar data batch cost save copi data train instanc space train instanc",
        "Solution_link_count":0.0,
        "Solution_original_content":"pipe mode decreas startup time frequent increas bill start data copi file mode control transfer read data pipe mode start control transfer data transfer billabl time data knowledg hit disk eb fast pass data multipl time read dime request wait time pipe mode idea pass data data larg fit eb volum pytorch data load parallel gpu chuck awai batch cpu load prepar data batch",
        "Solution_preprocessed_content":"pipe mode decreas startup time frequent increas bill start data copi file mode control transfer read data pipe mode start control transfer data transfer billabl time data knowledg hit disk fast pass data multipl time dime pipe mode idea pass data data larg fit eb volum pytorch data load parallel gpu chuck awai batch cpu load prepar data batch",
        "Solution_readability":8.3,
        "Solution_reading_time":11.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":171.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":21.4334622222,
        "Challenge_answer_count":3,
        "Challenge_body":"The SageMaker Data Wrangler UI in SageMaker Studio doesn't seem to support all the features that the API does. When will the UI support:\n\nLoading all s3 objects under a prefix? https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_csv.html#awswrangler.s3.read_csv\nLoading JSON objects in addition to CSV and Parquet files? https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_json.html#awswrangler.s3.read_json",
        "Challenge_closed_time":1642021146280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641943985816,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUcsIt78jnSTW8Ta9__kUm-w\/sage-maker-data-wrangler-ui-features",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":16.2,
        "Challenge_reading_time":6.55,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":21.4334622222,
        "Challenge_title":"SageMaker Data Wrangler UI Features",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":47,
        "Platform":"Tool-specific",
        "Solution_body":"As mentioned by Tulio Alberto in comments, Amazon SageMaker Data Wrangler (the graphical data preparation feature inside Amazon SageMaker) is separate from AWS Data Wrangler (an open-source data prep utility published by AWS Labs): The two tools are based on different technologies and don't necessarily aim for full feature parity - they just happen to share similar names.\n\nTo my knowledge there's no committed timeline we can share at the moment for when these particular features will make it to SageMaker Data Wrangler, but I think as feature requests they make sense and the reasoning for both is pretty clear: I'm aware that both have been discussed to some extent internally already, and I'd personally like to see them launch too!\n\nThanks for sharing the feedback, and apologies for the naming confusion!",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"commit timelin featur load object prefix load json object data wrangler featur intern reason featur request data wrangler data wrangler separ base technolog necessarili aim featur pariti",
        "Solution_link_count":0.0,
        "Solution_original_content":"tulio alberto comment data wrangler graphic data prepar featur insid separ data wrangler open sourc data prep util publish lab base technolog necessarili aim featur pariti share knowledg commit timelin share moment featur data wrangler featur request sens reason pretti clear awar extent intern launch share feedback apolog",
        "Solution_preprocessed_content":"tulio alberto comment data wrangler separ data wrangler base technolog necessarili aim featur pariti share knowledg commit timelin share moment featur data wrangler featur request sens reason pretti clear awar extent intern launch share feedback apolog",
        "Solution_readability":20.3,
        "Solution_reading_time":9.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":131.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":20.4316775,
        "Challenge_answer_count":4,
        "Challenge_body":"Hello,\n\nI understand there was a process how to connect to on-prem sql db from Azure ML studio, but with the transition to the new UI, I don't see the option to connect to the gateway. I have it successfully installed and registered in MS Azure, but from Studio it simply does not offer it as a dataset type when using the Import Data module.\nI can't find any documentation regarding the new UI nor any useful guides for this.\n\nWould anybody know whether this function is still available in the new studio and if so how can an on-prem gateway be connected?\n\nThank you,\nVS",
        "Challenge_closed_time":1638351388392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638277834353,
        "Challenge_favorite_count":22.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/646058\/new-azure-ml-vs-on-prem-sql.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":8.1,
        "Challenge_reading_time":7.14,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":20.4316775,
        "Challenge_title":"NEW Azure ML vs On-Prem SQL",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":111,
        "Platform":"Tool-specific",
        "Solution_body":"@sorcrow-1800\n\nThanks for reaching out to us. I just got confirmation from the pm of AML, on-prem SQL is not supported in AML yet, but it's now on our plan.\n\nI will forward your feedback to product team as well.\n\nHope this will help. Please let us know if any further queries.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"prem sql databas studio team plan feedback forward",
        "Solution_link_count":0.0,
        "Solution_original_content":"sorcrow reach aml prem sql aml plan forward feedback team hope queri forget click upvot button origin poster commun faster identifi remind come respons subscrib notif program shape futur volunt moder",
        "Solution_preprocessed_content":"reach aml sql aml plan forward feedback team hope queri forget click upvot button origin poster commun faster identifi remind come respons subscrib notif program shape futur volunt moder",
        "Solution_readability":6.4,
        "Solution_reading_time":8.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":127.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":5.6364072222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hey there,\n\nI was wondering, whether it is possible to connect your local computer as a compute target to the workspace and then access it as a compute target for AutoML and the Designer in the ML Studio (instead of a compute cluster)?\nI have read through the documentation and I feel like if this is possible, it is not very well-documented.\n\nThanks in advance!",
        "Challenge_closed_time":1624652423443,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624632132377,
        "Challenge_favorite_count":8.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/452250\/attaching-local-computer-to-ml-studio-and-use-it-w.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":5.41,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":5.6364072222,
        "Challenge_title":"Attaching local computer to ML Studio and use it with Azure AutoML and Azure Designer",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Solution_body":"Hi, thanks for reaching out. You can use local compute for model training\/deployment including automl. However, you cannot attach it directly in Designer or ML Studio interface. You can only attach it from your local environment. Hope this helps!",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"local comput model train deploy automl attach directli design studio interfac attach local environ",
        "Solution_link_count":0.0,
        "Solution_original_content":"reach local comput model train deploy automl attach directli design studio interfac attach local environ hope",
        "Solution_preprocessed_content":"reach local comput model automl attach directli design studio interfac attach local environ hope",
        "Solution_readability":7.5,
        "Solution_reading_time":3.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":153.0623633333,
        "Challenge_answer_count":1,
        "Challenge_body":"Can we make a new code through the sagemaker studio?\nIn my computer, GPU is GTX2080ti model, so if I use AWS sagemaker for paid service, can I get better performance?\nHow much GPU performance can you improve compared to before?\nI want to proceed with object segmentation through AWS sagemaker, can I use the code I used through sagemaker studio?",
        "Challenge_closed_time":1661427331608,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660876307100,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCKKplP7ES22DuZf8QJ38JA\/ask-aws-sage-maker",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":4.42,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":153.0623633333,
        "Challenge_title":"Ask AWS SageMaker",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":35.0,
        "Challenge_word_count":63,
        "Platform":"Tool-specific",
        "Solution_body":"My apologies, I am not fully sure on all the questions. But let me still make an attempt to respond to see if it helps.\n\nYes, you can write your own custom code through SageMaker studio.\n\nThis may not be an apple to apple comparison. The main advantage in this context, is your able to scale out your training to multiple nodes and cores (if your underlying model supports that). Likewise you can scale out the deployment as well. Typically the studio notebook is backed by a lightweight EC2 instance, but there are a large range of EC2 instances for training on SageMaker. Please refer to the following links for further assistance. 1. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-available-instance-types.html 2. https:\/\/aws.amazon.com\/ec2\/instance-types\/\n\nPlease refer to the response above for question # 2.\n\nDid you mean semantic segmentation? If yes, the answer is yes too.\n\nHope that helps!\n\nRegards, Punya",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"written studio train deploy scale multipl node core rang instanc train object segment semant segment",
        "Solution_link_count":2.0,
        "Solution_original_content":"apolog fulli write studio appl appl comparison advantag context scale train multipl node core underli model likewis scale deploy typic studio notebook back lightweight instanc larg rang instanc train link assist http doc com latest notebook instanc type html http com instanc type respons semant segment hope punya",
        "Solution_preprocessed_content":"apolog fulli write studio appl appl comparison advantag context scale train multipl node core likewis scale deploy typic studio notebook back lightweight instanc larg rang instanc train link assist respons semant segment hope punya",
        "Solution_readability":7.9,
        "Solution_reading_time":11.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":142.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":17.0936141667,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi there,\n\nI am interested in using model custom metadata. Looks like it got released recently.\nhttps:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/12\/sagemaker-model-registry-endpoint-visibility-custom-metadata-model-metrics\/\n\nMetadata can be set and read successfully via cli aws sagemaker describe-model-package --model-package-name \"arn:aws:sagemaker:us-east-1:ACCOUNT:model-package\/MODEL_PACKAGE_NAME\/1\"\n\naws --profile dev sagemaker describe-model-package --model-package-name \"arn:aws:sagemaker:us-east-1:ACCOUNT:model-package\/MODEL_PACKAGE_NAME\/1\" | jq .CustomerMetadataProperties { \"KeyName1\": \"string2\", \"KeyName2\": \"string2\" }\n\nHowever, it is not clear how custom metadata can be set in Sagemaker ML pipeline when model is train and registered using RegisterModel\n\nThanks in advance.",
        "Challenge_closed_time":1641417888071,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641356351060,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUFtq-bSeiRDisaQaEzHr7sQ\/how-to-set-model-custom-metadata-in-sagemaker-ml-pipeline",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":21.3,
        "Challenge_reading_time":11.18,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":17.0936141667,
        "Challenge_title":"How to set model custom metadata in Sagemaker ML pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":491.0,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Solution_body":"Have you tried using this parameter on the RegisterModel step?",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"paramet registermodel step set metadata model pipelin",
        "Solution_link_count":0.0,
        "Solution_original_content":"tri paramet registermodel step",
        "Solution_preprocessed_content":"tri paramet registermodel step",
        "Solution_readability":9.6,
        "Solution_reading_time":0.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2.3117755556,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello Folks,\n\nI'm currently using a free trial version of Azure.\n\nWhen creating a compute instance in machine learning studio, I cannot select a virtual machine.\nAll virtual machine names in the selection field are inactive.\n\n\nHow are these virtual machines activated?\n\n\n\n\n\nI am creating a virtual machine \"Standard_D2s_v3\".\nAnd I interpret this capture as showing that there are two available quarters for \"Standard DSv3 Family vCPUs\".If so, why is the virtual machine \"Standard_D2s_v3\" inactive?\n\n\n\n\n\nI'm sorry for my poor English.\nI very appreciate any help or direction. Thank you.\n\nRegards,\nKoki",
        "Challenge_closed_time":1610008313752,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609999991360,
        "Challenge_favorite_count":6.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/221759\/how-to-create-a-compute-instance.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":7.67,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":2.3117755556,
        "Challenge_title":"How to create a Compute Instance",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":97,
        "Platform":"Tool-specific",
        "Solution_body":"@Koki-2318 I think there could be two reasons here for this behaviour.\n\nThe second screen shot could be showing the availability in a different region than your Azure ML workspace region. Please verify if this is the case.\n\n\nThe Azure ML resource under the free tier has a special exception with regards to the compute that can be used. This is limited based on the available cores and the subscription offering. This is documented here with more details. If this is the case you can raise a quota increase and our team will evaluate the feasibility based on your subscription offering.\n\nIf the above two cases do not resolve you can convert to a pay as you go subscription and use the required compute size with your Azure ML experiments. Thanks!!",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"verifi virtual region workspac region resourc free tier except comput rais quota increas team evalu feasibl base subscript convert pai subscript comput size bias summari",
        "Solution_link_count":0.0,
        "Solution_original_content":"koki reason screen shot region workspac region verifi resourc free tier except comput limit base core subscript document rais quota increas team evalu feasibl base subscript convert pai subscript comput size",
        "Solution_preprocessed_content":"reason screen shot region workspac region verifi resourc free tier except comput limit base core subscript document rais quota increas team evalu feasibl base subscript convert pai subscript comput size",
        "Solution_readability":7.3,
        "Solution_reading_time":9.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":131.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":8.2510238889,
        "Challenge_answer_count":1,
        "Challenge_body":"I was working on a sagemaker studio for ML work, I attached Lifecycle Configuration with it, which was creating problem. Then I deleted the lifecycle configuration without detaching it, and this problem is happening. Can't start sagemaker studio notebook and this is shown.\n\nAny suggestion to fix this ?",
        "Challenge_closed_time":1667785787444,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667756083758,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU91ywEwTsRRqmHKZJ1yVrrA\/sagemaker-studio-is-not-opening-after-deleting-lifecycle-configuration",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":4.66,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":8.2510238889,
        "Challenge_title":"Sagemaker Studio is not opening after deleting lifecycle configuration",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":23.0,
        "Challenge_word_count":56,
        "Platform":"Tool-specific",
        "Solution_body":"You can try detaching the LCC script using the CLI. You can use the CloudShell from console, since your console role is able to perform updates on the domain.\n\nUse the update-domain CLI call, and provide an empty configuration for the default user settings, something like-\n\naws sagemaker update-domain --domain-id d-abc123 \\\n--default-user-settings '{\n\"JupyterServerAppSettings\": {\n  \"DefaultResourceSpec\": {\n    \"InstanceType\": \"system\"\n   },\n}}'",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"detach lcc cli cloudshel consol perform updat domain updat domain cli configur default set",
        "Solution_link_count":0.0,
        "Solution_original_content":"detach lcc cli cloudshel consol consol role perform updat domain updat domain cli configur default set updat domain domain abc default set jupyterserverappset defaultresourcespec instancetyp",
        "Solution_preprocessed_content":"detach lcc cli cloudshel consol consol role perform updat domain cli configur default set",
        "Solution_readability":12.9,
        "Solution_reading_time":5.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":12.2183122222,
        "Challenge_answer_count":3,
        "Challenge_body":"When I'm submitting my experiment fom notebook, experiment is queing for a long time then I get as error:\n\nAzureMLCompute job failed.\nClusterIdentityNotFound: Identity of the specified\nmanaged compute <hidden cluster location> is not found\n\n\n\n\nI've updated all azure ml packages and restarted cluster, deleted, recreating, ... Nothing seems to be working.\n\nWhat Should I do?",
        "Challenge_closed_time":1633987970467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633943984543,
        "Challenge_favorite_count":9.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/585373\/clusteridentitynotfound-when-submitting-experiment.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":10.2,
        "Challenge_reading_time":5.32,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":12.2183122222,
        "Challenge_title":"ClusterIdentityNotFound when submitting experiment.",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":58,
        "Platform":"Tool-specific",
        "Solution_body":"Hi, are you by any chance using a low priority VM? If so, can you try selecting 'dedicated' as priority to verify? Also, ensure that you are following the steps outlined in this document for creating a compute cluster. In the advanced settings, ensure to assign a managed identity and specify a system-assigned identity or user-assigned identity.\n\n\n\n\n--- Kindly Accept Answer if the information provided helps. Thanks.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"low prioriti select dedic prioriti verifi step outlin document creat comput cluster advanc set assign ident specifi assign ident assign ident bias respons",
        "Solution_link_count":0.0,
        "Solution_original_content":"chanc low prioriti select dedic prioriti verifi step outlin document creat comput cluster advanc set assign ident specifi assign ident assign ident kindli accept",
        "Solution_preprocessed_content":"chanc low prioriti select dedic prioriti verifi step outlin document creat comput cluster advanc set assign ident specifi ident ident kindli accept",
        "Solution_readability":9.6,
        "Solution_reading_time":5.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":3.4035702778,
        "Challenge_answer_count":1,
        "Challenge_body":"Working on deployment of 170 ml models using ML studio and azure Kubernetes service which is referred on the below doc link \"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/how-to-deploy-azure-kubernetes-service.md\".\n\nWe are training the model using python script with the custom environment and we are registering the ml model on the Azure ML services. Once we register the mode we are deploying it on the AKS by using the container images.\n\nWhile deploying the ML model we are able to deploy up 10 to 11 models per pods for each Node in AKS. When we try to deploy the model on the same node we are getting deployment timeout error and we are getting the below error message.",
        "Challenge_closed_time":1630758725400,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630746472547,
        "Challenge_favorite_count":14.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/540001\/how-many-models-can-be-deployed-in-single-node-in.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":9.72,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":3.4035702778,
        "Challenge_title":"how many models can be deployed in single node in azure kubernetes service?",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":null,
        "Challenge_word_count":123,
        "Platform":"Tool-specific",
        "Solution_body":"Hi @suvedharan-5910\n\nThe number of models to be deployed is limited to 1,000 models per deployment (per container).\n\nAutoscaling for Azure ML model deployments is azureml-fe, which is a smart request router. Since all inference requests go through it, it has the necessary data to automatically scale the deployed model(s).\nmore details\n\n\n\n\n\nIf the Answer is helpful, please click Accept Answer and up-vote, so that it can help others in the community looking for help on similar topics.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"model deploi limit model deploy autosc model deploy smart request router automat scale deploi model base infer request",
        "Solution_link_count":0.0,
        "Solution_original_content":"suvedharan model deploi limit model deploy autosc model deploy smart request router infer request data automat scale deploi model click accept vote commun",
        "Solution_preprocessed_content":"model deploi limit model deploy autosc model deploy smart request router infer request data automat scale deploi model click accept commun",
        "Solution_readability":10.9,
        "Solution_reading_time":5.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":78.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":0.2627777778,
        "Challenge_answer_count":1,
        "Challenge_body":"Is it possible to deploy an existing model artifact from SageMaker to Redshift ML?\n\nFor example, with an Aurora ML you can reference a SageMaker endpoint and then use it as a UDF in a SELECT statement. Redshift ML works a bit differently - when you call CREATE MODEL - the model is trained with SageMaker Autopilot and then deployed to the Redshift Cluster.\n\nWhat if I already have a trained model, can i deploy it to a Redshift Cluster and then use a UDF for Inference?",
        "Challenge_closed_time":1609955532000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609954586000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCMYCx28qRe-MOCIfj91Y2g\/redshift-ml-sage-maker-deploy-an-existing-model-artifact-to-a-redshift-cluster",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":6.63,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2627777778,
        "Challenge_title":"Redshift ML \/ SageMaker - Deploy an existing model artifact to a Redshift Cluster",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":96,
        "Platform":"Tool-specific",
        "Solution_body":"As of January 30 2021, you can't deploy an existing model artifact from SageMaker to Redshift ML directly with currently announced Redshift ML preview features. But you can reference sagemaker endpoint through a lambda function and use that lambda function as an user defined function in Redshift.\n\nBelow would be the steps:\n\nTrain and deploy your SageMaker model in a SageMaker Endpoint.\nUse Lambda function to reference sagemaker endpoint.\nCreate a Redshift Lambda UDF referring above lambda function to run predictions.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"januari deploi model artifact redshift directli endpoint lambda function lambda function defin function redshift step involv train deploi model endpoint lambda function endpoint creat redshift lambda udf lambda function run predict",
        "Solution_link_count":0.0,
        "Solution_original_content":"januari deploi model artifact redshift directli announc redshift preview featur endpoint lambda function lambda function defin function redshift step train deploi model endpoint lambda function endpoint creat redshift lambda udf lambda function run predict",
        "Solution_preprocessed_content":"januari deploi model artifact redshift directli announc redshift preview featur endpoint lambda function lambda function defin function redshift step train deploi model endpoint lambda function endpoint creat redshift lambda udf lambda function run predict",
        "Solution_readability":10.8,
        "Solution_reading_time":6.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":81.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":132.1636936111,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>I log values which have names in the form of <code>test\/temp_top-k.---1<\/code> (I want the dashes for sorting reasons). I can create graph panels with these values, but they do not show up in the column view. When I <code>Manage Columns<\/code> they are not listed in the <code>Hidden Columns<\/code>. When I search for them, it gives me no (an empty) result. Even when I select <code>Show All<\/code> they don\u2019t show up in the column view. A bug?<\/p>",
        "Challenge_closed_time":1647985099859,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647509310562,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/logged-value-available-in-graph-panel-but-not-in-columns\/2100",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":5.6,
        "Challenge_reading_time":6.24,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":132.1636936111,
        "Challenge_title":"Logged value available in graph panel, but not in columns",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":478.0,
        "Challenge_word_count":85,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hi Leslie,<\/p>\n<p>I apologize for not responding earlier. I assumed to be notified by e-mail when this thread is updated. Probably I need to check my settings, or \u201cwatch\u201d this thread.<\/p>\n<p>I log via Pytorch Lightning:<\/p>\n<pre><code class=\"lang-auto\">wandb_logger = WandbLogger(project=settings.project_name, log_model=True)\nwandb_logger.watch(model, log='gradients', log_freq=50, log_graph=True)\n<\/code><\/pre>\n<p>The actual code for the logging is this:<\/p>\n<pre><code class=\"lang-auto\">temp_accs_top_k = {f'{k:-&gt;4d}': v for k, v in zip(settings.ks, temp_accs)}\nlightning_module.log(f'{split}\/temp_top-k', temp_accs_top_k, batch_size=lightning_module.batch_size)\n<\/code><\/pre>\n<p>That looks a bit odd I suppose. The code is in a function that I call from several different <code>pl.LightningModule<\/code>s. The variable <code>lightning_module<\/code> refers to that module. The parameter <code>temp_accs_top_k<\/code> evaluates to (straight from the debugger):<\/p>\n<p><code>{'---1': 0.00019996000628452748, '---2': 0.00019996000628452748, '---3': 0.00039992001256905496, '---5': 0.0005998800043016672, '--10': 0.0005998800043016672, '--20': 0.001399720087647438, '--50': 0.004199160262942314, '-100': 0.007598480209708214, '1000': 0.08318336308002472}<\/code><\/p>\n<p>Which is wrong. But I am seeing the values in the graph panels (see attached screenshot).<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3ae06dc0b1399ce16eb917dfb94b60a5e0f77acd.png\" alt=\"Screen Shot 2022-03-22 at 21.52.37\" data-base62-sha1=\"8oQwPNwBrsRhQBp0p6SuZ6ht6NL\" width=\"412\" height=\"275\"><\/p>\n<p>I changed the code so that <code>temp_accs_top_k<\/code>now contains <code>{'test\/temp_top-k.---1': 0.2963850498199463, 'test\/temp_top-k.---2': 0.3962452709674835, 'test\/temp_top-k.---3': 0.44557619094848633, 'test\/temp_top-k.---5': 0.5052925944328308, 'test\/temp_top-k.--10': 0.5733972191810608, 'test\/temp_top-k.--20': 0.6277211904525757, 'test\/temp_top-k.--50': 0.6810465455055237, 'test\/temp_top-k.-100': 0.716596782207489, 'test\/temp_top-k.1000': 0.802676260471344}<\/code>.<\/p>\n<p>I log in a loop since Pytorch Lightning can\u2019t log a dict (I believe). I know that wandb does it, but I need the batch_size parameter (I have two dataloaders with different sizes\/lenghts and need to make sure that Pytorch Lightning does not get confused with steps\/epochs).<\/p>\n<pre><code class=\"lang-auto\">for k, v in temp_accs_top_k.items():\n    lightning_module.log(k, v, batch_size=lightning_module.batch_size)\n<\/code><\/pre>\n<p>Update: just realized that Pytorch Lightning has a <code>log_dict<\/code> function which lets me get rid of the awkward for loop.<\/p>\n<p>So the \u201cbug\u201d is more like \u201cwhy did it work in the first place (in the graph panels)?\u201d<\/p>\n<p>Hope that\u2019s not too much to digest and it is traceable.<\/p>\n<p>Best,<br>\nStephan<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"log valu prefix specifi split pytorch lightn log dict function log dictionari loop realiz misunderstand log valu graph panel log prefix specifi split",
        "Solution_link_count":1.0,
        "Solution_original_content":"lesli apolog earlier notifi mail thread updat probabl set watch thread log pytorch lightn logger logger set log model logger watch model log gradient log freq log graph log temp acc zip set temp acc lightn modul log split temp temp acc batch size lightn modul batch size bit odd suppos function lightningmodul variabl lightn modul modul paramet temp acc evalu straight debugg see valu graph panel attach screenshot temp acc test temp test temp test temp test temp test temp test temp test temp test temp test temp log loop pytorch lightn log dict believ batch size paramet dataload size lenght pytorch lightn step epoch temp acc item lightn modul log batch size lightn modul batch size updat realiz pytorch lightn log dict function rid awkward loop graph panel hope that digest traceabl stephan",
        "Solution_preprocessed_content":"lesli apolog earlier notifi thread updat probabl set watch thread log pytorch lightn log bit odd suppos function variabl modul paramet evalu see valu graph panel log loop pytorch lightn log dict paramet updat realiz pytorch lightn function rid awkward loop hope that digest traceabl stephan",
        "Solution_readability":9.0,
        "Solution_reading_time":37.55,
        "Solution_score_count":null,
        "Solution_sentence_count":33.0,
        "Solution_word_count":281.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":0.5344444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi everyone,\n\nhow can i install custom OS libraries on Sagemaker studio? When I open a terminal it states:\n\nroot@0f04278e59cf:~\/# yum install unzip\n\nbash: yum: command not found\n\nThanks!",
        "Challenge_closed_time":1592471900000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592469976000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUZZFjMw_gS5Cz8sh-TK4J3w\/custom-packages-in-sagemaker-studio",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":2.73,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.5344444444,
        "Challenge_title":"Custom packages in Sagemaker studio",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":266.0,
        "Challenge_word_count":33,
        "Platform":"Tool-specific",
        "Solution_body":"Short answer: [Studio UI] > File > New > Terminal > sudo yum install unzip\nThen unzip away...\n\nLong answer:\nYou can open a terminal in two different types of compute environment:\n\nOn a specific kernel you're running: [Studio UI] > kernal tab > Terminial icon.\nOn the compute studio (jupyter) itself: [Studio UI] > File > New > Terminal\n\nIn both options your personal files folder is accessible. In a kernel terminal: \/root. In a Jupyter terminal: \/home\/sagemaker-user.\nWhen opening a kernel terminal you'll have access to the software that is part of the kernel's container (say tensorflow container). Which in your case is missing yum. You can of course try apt-get, and such to install more tools.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"open termin studio navig studio file termin run sudo yum instal unzip instal librari type comput environ studio file folder access option kernel termin softwar kernel access jupyt termin softwar jupyt access",
        "Solution_link_count":0.0,
        "Solution_original_content":"short studio file termin sudo yum instal unzip unzip awai open termin type comput environ kernel run studio kernal tab termini icon comput studio jupyt studio file termin option file folder access kernel termin root jupyt termin home open kernel termin access softwar kernel tensorflow miss yum cours apt instal",
        "Solution_preprocessed_content":"short file termin sudo yum instal unzip unzip open termin type comput environ kernel run kernal tab termini icon comput studio file termin option file folder access kernel termin root jupyt termin open kernel termin access softwar kernel miss yum cours instal",
        "Solution_readability":8.7,
        "Solution_reading_time":8.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":110.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":5.5969425,
        "Challenge_answer_count":2,
        "Challenge_body":"Do I need to add the .ipynb extension manually to my notebook file ?\nI can't find the run button.",
        "Challenge_closed_time":1617872811416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617852662423,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/348777\/can39t-find-the-run-button.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":3.2,
        "Challenge_reading_time":1.47,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":5.5969425,
        "Challenge_title":"Can't find the run button",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":23,
        "Platform":"Tool-specific",
        "Solution_body":"@paulgureghian-9874 You can re-name your file with .ipynb extension which should help to display the cells and the available options like run button for cell. Usually while creating new files in your workspace the UI prompts to select the extension of the file. If you can create a new file with .ipynb extension and copy these cells individually that should also work. Thanks.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"manual add ipynb extens notebook file renam displai cell option run button cell creat file ipynb extens copi cell individu",
        "Solution_link_count":0.0,
        "Solution_original_content":"paulgureghian file ipynb extens displai cell option run button cell creat file workspac prompt select extens file creat file ipynb extens copi cell individu",
        "Solution_preprocessed_content":"file ipynb extens displai cell option run button cell creat file workspac prompt select extens file creat file ipynb extens copi cell individu",
        "Solution_readability":5.8,
        "Solution_reading_time":4.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":66.2517636111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nIs there a way to specify the disk storage type for Compute instances?\nBoth the Azure portal and ARM templates do not have an option to define the disk storage type, which defaults to the P10 disks (Premium SSD).\n\nThanks",
        "Challenge_closed_time":1636952401092,
        "Challenge_comment_count":1,
        "Challenge_created_time":1636713894743,
        "Challenge_favorite_count":7.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/625035\/azure-machine-learning-specify-disk-storage-type.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.1,
        "Challenge_reading_time":3.33,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":66.2517636111,
        "Challenge_title":"Azure Machine Learning - Specify disk storage type",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":47,
        "Platform":"Tool-specific",
        "Solution_body":"@simonmagrin Thanks, Currently There's no way to change the disk storage type for CIs or compute clusters. We have added this to our product backlog item to support in the near future.\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"disk storag type comput instanc comput cluster backlog item futur",
        "Solution_link_count":0.0,
        "Solution_original_content":"simonmagrin disk storag type ci comput cluster backlog item futur forget click upvot button origin poster commun faster identifi remind come respons subscrib notif program shape futur volunt moder",
        "Solution_preprocessed_content":"disk storag type ci comput cluster backlog item futur forget click upvot button origin poster commun faster identifi remind come respons subscrib notif program shape futur volunt moder",
        "Solution_readability":9.1,
        "Solution_reading_time":7.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":107.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":0.5700419445,
        "Challenge_answer_count":1,
        "Challenge_body":"I am exploring the Sagemaker Built-in algorithms, and I am curious to learn more about the details of the algorithms. However, I am surprised that it is hard to find any references for the research background and implementation details in the numerous documents and tutorials for particular algorithms. If such information exists somewhere, I would highly appreciate a pointer. Thanks a lot in advance!",
        "Challenge_closed_time":1652688680111,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652686627960,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUDkYruiibS9S05bzFSkLaxg\/sagemaker-built-in-algorithms",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":5.39,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.5700419445,
        "Challenge_title":"Sagemaker Built-in Algorithms",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":76.0,
        "Challenge_word_count":66,
        "Platform":"Tool-specific",
        "Solution_body":"thanks for your interest in the built-in algorithms! You can find research papers in the documentation of many of them. And documentation page has a section \"how it works\" explaining the science of every algorithm. For example:\n\nBlazingText: BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs, Gupta et Khare\nDeepAR DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks, Salinas et al.\nFactorization Machines\nIP Insights\nKMeans\nKNN\nLDA\nLinear Learner\nNTM\nObject2Vec\nObject Detection (it's an SSD model)\nPCA\nRandom Cut Forest: Robust Random Cut Forest Based Anomaly Detection On Streams, Guha et al\nSemantic Segmentation\nSeq2seq\nXGBoost",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"research paper document built algorithm document page section explain scienc algorithm list research paper algorithm blazingtext deepar factor kmean knn lda linear learner ntm objectvec object detect pca random cut forest semant segment seqseq",
        "Solution_link_count":0.0,
        "Solution_original_content":"built algorithm research paper document document page section explain scienc algorithm blazingtext blazingtext scale acceler wordvec multipl gpu gupta khare deepar deepar probabilist forecast autoregress recurr network salina factor kmean knn lda linear learner ntm objectvec object detect ssd model pca random cut forest robust random cut forest base anomali detect stream guha semant segment seqseq",
        "Solution_preprocessed_content":"algorithm research paper document document page section explain scienc algorithm blazingtext blazingtext scale acceler vec multipl gpu gupta khare deepar deepar probabilist forecast autoregress recurr network salina factor kmean knn lda linear learner ntm object vec object detect pca random cut forest robust random cut forest base anomali detect stream guha semant segment seq seq",
        "Solution_readability":13.2,
        "Solution_reading_time":8.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":97.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":15.64844,
        "Challenge_answer_count":1,
        "Challenge_body":"I came across this page which describes how to work with AML datasets. I'm specifically interested in mounting. It states that \"Mounting is supported for Linux-based computes\". Is there no way to do this on Windows?\n\nThanks,\nYordan",
        "Challenge_closed_time":1653382797127,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653326462743,
        "Challenge_favorite_count":11.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/860774\/mount-aml-dataset-on-windows.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":4.2,
        "Challenge_reading_time":3.2,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":15.64844,
        "Challenge_title":"Mount AML dataset on Windows",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":42,
        "Platform":"Tool-specific",
        "Solution_body":"@YordanZaykov-7763 Yes, currently this is only supported for linux based computes for Azure ML. Windows only supports download option.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"yordanzaykov linux base comput window download option click upvot commun member read thread",
        "Solution_preprocessed_content":"linux base comput window download option click upvot commun member read thread",
        "Solution_readability":8.2,
        "Solution_reading_time":3.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":38.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":29.7666666667,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi there,In Serbia we are using 2 scripts side by side - Cyrillic and Latin script.I am heaving an issue with translation to Serbian Latin.\nBy default Google translate offer translation to Serbian Cyrillic , but bellow that default translation there is a translation to Serbian Latin.\nTake a look at this example:\nhttps:\/\/translate.google.com\/?sl=en&tl=sr&text=Hello%20world!&op=translateI have found this post from early 2019.\nhttps:\/\/support.google.com\/translate\/thread\/1836538?hl=enLike in that post my question is the same:\nI need it to support Serbian Latin, for some projects I don`t use the Cyrillic script. Also there is a problem with translating pages or similar plugins, e.g.: Google Language Translator for WordPress and some others CMS system like Kopage you can translate only to Serbian Cyrillic script.As I found this post on stackoverflow:\nhttps:\/\/stackoverflow.com\/questions\/73699065\/google-cloud-translate-serbian-latin-not-workingIt seems, according to the poster of that article, that there was a workaround.\nInstead of \"sr\" ISO-639 code you can put \"sr_Latn\" - and you will get translation into Serbian Latin script.\nBut that workaround stop working several weeks ago - according to the poster.Is there a workaround to translate into Serbian Latin characters rather into Serbian Cyrillic characters?Regards,\nBranislav",
        "Challenge_closed_time":1665562320000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1665455160000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translate-API-and-Serbian-Latin-script\/td-p\/476723\/jump-to\/first-unread-message",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":17.55,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":29.7666666667,
        "Challenge_title":"Google Translate API and Serbian Latin script",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":128.0,
        "Challenge_word_count":187,
        "Platform":"Tool-specific",
        "Solution_body":"It appears that translation to the Serbian Latin Alphabet is not officially supported by the Cloud Translate API, as discussed in this recent issue. Therefore it\u2019s not assured that any possible workaround will be functional or reliable. You can see the list of supported language codes for translation here.\n\nYou can, however, submit a Feature Request to the public Google issue tracker for Cloud Translation API. The higher the number of users who bring attention to this request, the more likely it is for it to be eventually built into the API.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"submit featur request public tracker cloud translat api add serbian latin workaround function reliabl translat serbian latin alphabet offici cloud translat api",
        "Solution_link_count":0.0,
        "Solution_original_content":"translat serbian latin alphabet offici cloud translat api assur workaround function reliabl list languag translat submit featur request public tracker cloud translat api higher attent request built api origin",
        "Solution_preprocessed_content":"translat serbian latin alphabet offici cloud translat api assur workaround function reliabl list languag translat submit featur request public tracker cloud translat api higher attent request built api origin",
        "Solution_readability":8.4,
        "Solution_reading_time":7.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":97.0,
        "Tool":"Vertex AI"
    },
    {
        "Challenge_adjusted_solved_time":1.2025963889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi all,\n\nI have raised a ticket for multiple issues we've been having with SageMaker lately, the ticket was created more than 36 hours ago, and I have not had any response, in fact the ticket hasn't even been assigned yet.\n\nThe case ID is 10300240931.\n\nI thought AWS guarantee a response under 12 hours for \"system impaired\" issues, does anyone know what I can do to accelerate this?\n\nthank you! Ruoy",
        "Challenge_closed_time":1656585271688,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656580942341,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUFgnjt9J3T0iXhE0axG10vQ\/how-long-does-it-take-for-aws-tech-support-team-to-respond-to-a-system-impaired-issue",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":5.85,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.2025963889,
        "Challenge_title":"How long does it take for AWS tech support team to respond to a \"system impaired\" issue?",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":76.0,
        "Challenge_word_count":88,
        "Platform":"Tool-specific",
        "Solution_body":"Hi Ruoy! My advice here is to scale this issue via your account team, they will have the mechanisms to scale this concern. If you are on basic or developer support, you could look into upgrading to business support for a day and open a live chat with support! Hope this helps",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"scale account team upgrad busi dai open live chat bias respons",
        "Solution_link_count":0.0,
        "Solution_original_content":"ruoi advic scale account team mechan scale upgrad busi dai open live chat hope",
        "Solution_preprocessed_content":"ruoi advic scale account team mechan scale upgrad busi dai open live chat hope",
        "Solution_readability":6.5,
        "Solution_reading_time":3.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":52.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2.4680805556,
        "Challenge_answer_count":2,
        "Challenge_body":"I am trying to deploy the SageMaker Inference Endpoint by extending the Pre-built image. However, it failed with \"FileNotFoundError: [Errno 2] No such file or directory: 'serve'\"\n\nMy Dockerfile\n\nARG REGION=us-west-2\n\n# SageMaker PyTorch image\nFROM 763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2\n\nRUN apt-get update\n\nENV PATH=\"\/opt\/ml\/code:${PATH}\"\n\n# this environment variable is used by the SageMaker PyTorch container to determine our user code directory.\nENV SAGEMAKER_SUBMIT_DIRECTORY \/opt\/ml\/code\n\n# \/opt\/ml and all subdirectories are utilized by SageMaker, use the \/code subdirectory to store your user code.\nCOPY inference.py \/opt\/ml\/code\/inference.py\n\n# Defines inference.py as script entrypoint \nENV SAGEMAKER_PROGRAM inference.py\n\n\nCloudWatch Log From \/aws\/sagemaker\/Endpoints\/mytestEndpoint\n\n2022-09-30T04:47:09.178-07:00\nTraceback (most recent call last):\n  File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module>\n    subprocess.check_call(shlex.split(' '.join(sys.argv[1:])))\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call\n    retcode = call(*popenargs, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call\n    with Popen(*popenargs, **kwargs) as p:\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child\n    raise child_exception_type(errno_num, err_msg, err_filename)\nTraceback (most recent call last): File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module> subprocess.check_call(shlex.split(' '.join(sys.argv[1:]))) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call retcode = call(*popenargs, **kwargs) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call with Popen(*popenargs, **kwargs) as p: File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child raise child_exception_type(errno_num, err_msg, err_filename)\n\n2022-09-30T04:47:13.409-07:00\nFileNotFoundError: [Errno 2] No such file or directory: 'serve'",
        "Challenge_closed_time":1664550898846,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664542013756,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUR-uTDaDsQBGjMoAUcsi2sQ\/aws-sage-maker-extending-pre-built-container-deploy-endpoint-failed-no-such-file-or-directory-serve",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":13.8,
        "Challenge_reading_time":31.19,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":2.4680805556,
        "Challenge_title":"AWS SageMaker - Extending Pre-built Container, Deploy Endpoint Failed. No such file or directory: 'serve'\"",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":97.0,
        "Challenge_word_count":222,
        "Platform":"Tool-specific",
        "Solution_body":"Should use the Sagemaker image\n\n763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-sagemaker\n\n\ninstead of ec2\n\n763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"imag imag imag file depend deploi infer endpoint",
        "Solution_link_count":0.0,
        "Solution_original_content":"imag dkr ecr region amazonaw com pytorch infer gpu ubuntu dkr ecr region amazonaw com pytorch infer gpu ubuntu",
        "Solution_preprocessed_content":null,
        "Solution_readability":35.9,
        "Solution_reading_time":3.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":10.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":1.1990072222,
        "Challenge_answer_count":1,
        "Challenge_body":"We are planing for next gen of product. Will V2 provide way more changes than V1?",
        "Challenge_closed_time":1661981560816,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661977244390,
        "Challenge_favorite_count":11.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/989368\/sdk-v1-or-v2.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":1.0,
        "Challenge_reading_time":1.12,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.1990072222,
        "Challenge_title":"SDK v1 or V2",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":19,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @nam-4027\n\nThanks for using Microsoft Q&A. I will recommend you keeping in V1 at this moment.\n\nSDK v2 is currently in public preview. The preview version is provided without a service level agreement, and it's not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see Supplemental Terms of Use for Microsoft Azure Previews.\n\nhttps:\/\/azure.microsoft.com\/support\/legal\/preview-supplemental-terms\/\n\nI hope this helps.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"stick sdk gener sdk public preview workload",
        "Solution_link_count":1.0,
        "Solution_original_content":"nam keep moment sdk public preview preview version servic level agreement workload featur constrain capabl supplement term preview http com legal preview supplement term hope yutong kindli accept commun",
        "Solution_preprocessed_content":"keep moment sdk public preview preview version servic level agreement workload featur constrain capabl supplement term preview hope yutong kindli accept commun",
        "Solution_readability":9.9,
        "Solution_reading_time":7.77,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":10.4716361111,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hy, I\u2019m in love with wandb, but I have a problem\u2026<\/p>\n<p>I have a simple question\u2026<\/p>\n<p>How can I analyze hyperparameters\u2026As seen in this picture, without actually creating a sweep.<\/p>\n<p>In my own code\u2026<\/p>\n<p><img src=\"https:\/\/mail.google.com\/mail\/u\/0?ui=2&amp;ik=8824e8d63e&amp;attid=0.1&amp;permmsgid=msg-a:r-1242756300606160728&amp;th=181d7b1a169f2ed0&amp;view=fimg&amp;fur=ip&amp;sz=s0-l75-ft&amp;attbid=ANGjdJ9LbpPclu5VUg_KiYT_9MyY2AbgyxXn6tmqz8qoKH2kUghMnyxeJstBhkIK4wCOgqfFHueuZ6ul6juIl6zvWD3lcsPXIvZAnZatibVLxPjneVvO-xSUoWLyCpM&amp;disp=emb&amp;realattid=ii_l5aqmkag2\" alt=\"68747470733a2f2f692e696d6775722e636f6d2f5455333451465a2e706e67.png\" width=\"339\" height=\"205\"><\/p>\n<p>I\u2019m preforming learning and for every model i\u2019m sending config with hyperparams\u2026<\/p>\n<p>wandb.finish(quiet=True)<br>\nwandb.init(<br>\nentity=var.WANDB_ENTITY,<br>\nproject=f\u2019{var.version} | {var.INPUT_DATASET}',<br>\ndir=str(var.working_dir),<br>\nconfig=utils.keras.hyper_params(hp))<\/p>\n<p>But in dashboard I dont see hyperparameters dashboard\u2026 And this makes me really sad !<\/p>",
        "Challenge_closed_time":1657219039908,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657181342018,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/analyzing-hyperparameters-without-actualy-performing-a-sweep\/2719",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":21.3,
        "Challenge_reading_time":15.42,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":10.4716361111,
        "Challenge_title":"Analyzing hyperparameters without actualy performing a sweep",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":95.0,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Solution_body":"<p>I can\u2019t see the images above, but if you would like to create a <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/parallel-coordinates\">parallel coordinates plot<\/a>, you can do so using the UI by clicking \u201cadd panel\u201d in your workspace and choosing Parallel Coordinates.<\/p>\n<p>If you need to do this programmatically, one <em>very<\/em> recent feature would be to create a W&amp;B Report using our Api. You can programatically define what plots show up. It is a very new feature so it\u2019ll become better documented and more stable over time.<\/p>\n<p>Here\u2019s how you would create a Parallel Coordinates plot programmatically and save it in a report using Python.<\/p>\n<pre><code class=\"lang-auto\">import wandb\nimport wandb.apis.reports as wb\napi = wandb.Api()\nproject = 'pytorch-sweeps-demo'\nwandb.require('report-editing') # this is needed as of version 0.12.21 but will likely not be needed in future.\nreport = wb.Report(\n    project=project,\n    title='Sweep Results',\n    blocks=[\n            wb.PanelGrid(panels=[\n                 wb.ParallelCoordinatesPlot(\n                     columns=[wb.reports.PCColumn('batch_size'), wb.reports.PCColumn('epoch'), wb.reports.PCColumn('loss')])\n            ], runsets=[wb.RunSet(project=project)]),\n    ]\n)\nreport.save()\n<\/code><\/pre>\n<p>This will then show up in the Reports tab on your project.<br>\nAs this is a very fresh API, there may be issues or features that are not supported yet. I do apologise if that happens to you, I\u2019ll be happy to follow up and provide help.<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"creat parallel coordin plot click add panel workspac choos parallel coordin programmat creat parallel coordin plot save report api defin plot report report tab",
        "Solution_link_count":1.0,
        "Solution_original_content":"imag creat parallel coordin plot click add panel workspac choos parallel coordin programmat featur creat report api programat defin plot featur itll document stabl time here creat parallel coordin plot programmat save report import import api report api api pytorch sweep report edit version futur report report titl sweep block panelgrid panel parallelcoordinatesplot column report pccolumn batch size report pccolumn epoch report pccolumn loss runset runset report save report tab fresh api featur apologis ill happi",
        "Solution_preprocessed_content":"imag creat parallel coordin plot click add panel workspac choos parallel coordin programmat featur creat report api programat defin plot featur itll document stabl time here creat parallel coordin plot programmat save report report tab fresh api featur apologis ill happi",
        "Solution_readability":10.5,
        "Solution_reading_time":18.39,
        "Solution_score_count":null,
        "Solution_sentence_count":15.0,
        "Solution_word_count":187.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":2.5986111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI want to use awswrangler package in my Jupyter Notebook instance of SageMaker.\n\nI understand that we have to use Lifecycle configuration. I tried to do it using the following script:\n\n#!\/bin\/bash\n\npip install awswrangler==0.2.2\n\n\nBut when I import that package into my Notebook:\n\nimport boto3                                      # For executing native S3 APIs\nimport pandas as pd                               # For munging tabulara data\nimport numpy as np                                # For doing some calculation\nimport awswrangler as wr\nimport io\nfrom io import StringIO\n\n\nI still get the following error:\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n<ipython-input-1-f3d85c7dd0f6> in <module>()\n      2 import pandas as pd                               # For munging tabulara data\n      3 import numpy as np                                # For doing some calculation\n----> 4 import awswrangler as wr\n      5 import io\n      6 from io import StringIO\n\nModuleNotFoundError: No module named 'awswrangler'\n\n\nAny documentation or reference on how to install certain package for Jupyter Notebook in SageMaker?",
        "Challenge_closed_time":1592832724000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592823369000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU4pvReJNZS6eDLxhd4pK-tQ\/how-to-install-phyton-package-in-jupyter-notebook-instance-in-sage-maker",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":13.51,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2.5986111111,
        "Challenge_title":"How to install Phyton package in Jupyter Notebook instance in SageMaker?",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":636.0,
        "Challenge_word_count":154,
        "Platform":"Tool-specific",
        "Solution_body":"Hi,\n\nexample how to use lifecycle config to install python package in one environment : https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-pip-package-single-environment\/on-start.sh\n\nand to all conda env - https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-pip-package-all-environments\/on-start.sh",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"link github repositori instal packag jupyt notebook instanc lifecycl configur link instal packag singl environ link instal packag conda environ",
        "Solution_link_count":2.0,
        "Solution_original_content":"lifecycl config instal packag environ http github com sampl notebook instanc lifecycl config sampl blob master instal pip packag singl environ start conda env http github com sampl notebook instanc lifecycl config sampl blob master instal pip packag environ start",
        "Solution_preprocessed_content":"lifecycl config instal packag environ conda env",
        "Solution_readability":40.4,
        "Solution_reading_time":6.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2.8071302778,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a state-machine workflow with 3 following states:\n\nscreenshot-of-my-workflow\n\nA 'Pass' block that adds a list of strings(SageMaker endpoint names) to the original input. (this 'Pass' will be replaced by a call to DynamoDB to fetch list in future.)\nUse map to call SageMaker endpoints dictated by the array(or list) from above result.\nSend the result of above 'Map' to a Lambda function and exit the workflow.\n\nHere's the entire workflow in .asl.json, inspired from this aws blog.\n\n{\n  \"Comment\": \"A description of my state machine\",\n  \"StartAt\": \"Pass\",\n  \"States\": {\n    \"Pass\": {\n      \"Type\": \"Pass\",\n      \"Next\": \"InvokeEndpoints\",\n      \"Result\": {\n        \"Endpoints\": [\n          \"sagemaker-endpoint-1\",\n          \"sagemaker-endpoint-2\",\n          \"sagemaker-endpoint-3\"\n        ]\n      },\n      \"ResultPath\": \"$.EndpointList\"\n    },\n    \"InvokeEndpoints\": {\n      \"Type\": \"Map\",\n      \"Next\": \"Post-Processor Lambda\",\n      \"Iterator\": {\n        \"StartAt\": \"InvokeEndpoint\",\n        \"States\": {\n          \"InvokeEndpoint\": {\n            \"Type\": \"Task\",\n            \"End\": true,\n            \"Parameters\": {\n              \"Body\": \"$.InvocationBody\",\n              \"EndpointName\": \"$.EndpointName\"\n            },\n            \"Resource\": \"arn:aws:states:::aws-sdk:sagemakerruntime:invokeEndpoint\",\n            \"ResultPath\": \"$.InvocationResult\"\n          }\n        }\n      },\n      \"ItemsPath\": \"$.EndpointList.Endpoints\",\n      \"MaxConcurrency\": 300,\n      \"Parameters\": {\n        \"InvocationBody.$\": \"$.body.InputData\",\n        \"EndpointName.$\": \"$$.Map.Item.Value\"\n      },\n      \"ResultPath\": \"$.InvocationResults\"\n    },\n    \"Post-Processor Lambda\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::lambda:invoke\",\n      \"Parameters\": {\n        \"Payload.$\": \"$\",\n        \"FunctionName\": \"arn:aws:lambda:<my-region>:<my-account-id>:function:<my-lambda-function-name>:$LATEST\"\n      },\n      \"Retry\": [\n        {\n          \"ErrorEquals\": [\n            \"Lambda.ServiceException\",\n            \"Lambda.AWSLambdaException\",\n            \"Lambda.SdkClientException\"\n          ],\n          \"IntervalSeconds\": 2,\n          \"MaxAttempts\": 6,\n          \"BackoffRate\": 2\n        }\n      ],\n      \"End\": true\n    }\n  }\n}\n\n\nAs can be seen in the workflow, I am iterating over the list from the previous 'Pass' block and mapping those to iterate inside 'Map' block and trying to access the Parameters of 'Map' block inside each iteration. Iteration works fine with number of iterators, but I can't access the Parameters inside the iteration. I get this error:\n\n{\n  \"resourceType\": \"aws-sdk:sagemakerruntime\",\n  \"resource\": \"invokeEndpoint\",\n  \"error\": \"SageMakerRuntime.ValidationErrorException\",\n  \"cause\": \"1 validation error detected: Value '$.EndpointName' at 'endpointName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9])* (Service: SageMakerRuntime, Status Code: 400, Request ID: ed5cad0c-28d9-4913-853b-e5f9ac924444)\"\n}\n\n\nSo, I presume the error is because \"$.EndpointName\" is not being filled with the relevant value. How do I avoid this.\n\nBut, when I open the failed execution and check the InvokeEndpoint block from graph-inspector, input to that is what I expected and above JSON-Paths to fetch the parameters should work, but they don't.\nscreenshot-of-graph-inspector\n\nWhat's causing the error and How do I fix this?",
        "Challenge_closed_time":1647513967263,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647503861594,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUDc1foN9TQhe3OYkkGzCKhQ\/aws-step-functions-sage-makers-invoke-endpoint-block-throws-validation-error-when-fetching-parameters-for-itself-inside-iterator-of-map-block",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":39.27,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":2.8071302778,
        "Challenge_title":"AWS StepFunctions - SageMaker's InvokeEndpoint block throws \"validation error\" when fetching parameters for itself inside iterator of Map block",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":110.0,
        "Challenge_word_count":333,
        "Platform":"Tool-specific",
        "Solution_body":"In general (as mentioned here in the parameters doc), you also need to end the parameter name with .$ when using a JSON Path.\n\nIt looks like you're doing that some places in your sample JSON (e.g. \"InvocationBody.$\": \"$.body.InputData\"), but not in others (\"EndpointName\": \"$.EndpointName\"), so I think the reason you're seeing the validation error here is that Step Functions is trying to interpret $.EndpointName as literally the name of the endpoint (which doesn't satisfy ^[a-zA-Z0-9](-*[a-zA-Z0-9])*!)\n\nSo suggest you change to EndpointName.$ and Body.$ in your InvokeEndpoint parameters",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"end paramet json path invokeendpoint block insid iter map block stepfunct workflow paramet endpointnam bodi invokeendpoint paramet",
        "Solution_link_count":0.0,
        "Solution_original_content":"gener paramet doc end paramet json path sampl json invocationbodi bodi inputdata endpointnam endpointnam reason see step function interpret endpointnam liter endpoint endpointnam bodi invokeendpoint paramet",
        "Solution_preprocessed_content":"gener end paramet json path sampl json reason see step function interpret liter endpoint invokeendpoint paramet",
        "Solution_readability":7.5,
        "Solution_reading_time":7.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":14.4512016667,
        "Challenge_answer_count":2,
        "Challenge_body":"Hello Microsoft Q&A Team,\n\nI get the error\n\nAssetException: Error with code: Can't connect to HTTPS URL because the SSL module is not available\n\nwhen executing the following command:\n\npipeline_job = ml_client.jobs.create_or_update(\npipeline_job, experiment_name=\"data_preparation\"\n)\npipeline_job\n\nYesterday the command worked without an error. I did not make any changes. So I have no idea, what the problem is.\n\nThanks for helping me out.\n\nCheers\n\nLukas",
        "Challenge_closed_time":1667605270036,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667553245710,
        "Challenge_favorite_count":11.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1075753\/aml-assetexception-error-with-code-can39t-connect.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":6.93,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":14.4512016667,
        "Challenge_title":"AML - AssetException: Error with code: Can't connect to HTTPS URL because the SSL module is not available.",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Solution_body":"@Lukas-6968 Thanks for your question. Can you please add more details about the document\/sample that you are trying.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"luka add document sampl",
        "Solution_preprocessed_content":null,
        "Solution_readability":3.3,
        "Solution_reading_time":1.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":2.9963816667,
        "Challenge_answer_count":9,
        "Challenge_body":"<p>This morning we were looking at the visualizations and charts on an active training run and everything was fine. After about 11am PDT, all of the visualizations started randomly disappearing. Sometimes only the loss charts would be visible, other times the losses and metrics and statistics would all be visible.<\/p>\n<p>The best I can tell is that the site is only showing charts for whatever things were in the most recent step. If you send some things less frequently, then their charts\/visualizations disappear until they\u2019re in the step data again.<\/p>",
        "Challenge_closed_time":1650494038572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650483251598,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/visualizations-metrics-etc-keep-randomly-appearing-and-disappearing\/2283",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":9,
        "Challenge_readability":8.7,
        "Challenge_reading_time":7.83,
        "Challenge_score_count":4.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2.9963816667,
        "Challenge_title":"Visualizations, metrics, etc. keep randomly appearing and disappearing",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":147.0,
        "Challenge_word_count":97,
        "Platform":"Tool-specific",
        "Solution_body":"<p><a class=\"mention\" href=\"\/u\/cogwheel\">@cogwheel<\/a> Could you possibly send me a link to your workspace?  If you don\u2019t want to share here you can also email <a href=\"mailto:support@wandb.com\">support@wandb.com<\/a> and explain the issue and I can respond via email.<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"cogwheel send link workspac share email com explain email",
        "Solution_preprocessed_content":"send link workspac share email explain email",
        "Solution_readability":9.2,
        "Solution_reading_time":3.45,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"Weights & Biases"
    },
    {
        "Challenge_adjusted_solved_time":0.6372222222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nWhat parquet data loading logic is known to work well to train with SageMaker on parquet? ml-io? pyarrow? any examples? That would be to train a classifier, either logistic regression, XGBoost or custom TF.",
        "Challenge_closed_time":1588843302000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588841008000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCqvDUq4hSQqRT97tBUvE8Q\/training-a-classifier-on-parquet-with-sage-maker",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":3.22,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.6372222222,
        "Challenge_title":"Training a classifier on parquet with SageMaker ?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":188.0,
        "Challenge_word_count":42,
        "Platform":"Tool-specific",
        "Solution_body":"XGBoost as a framework container (v0.90+) can read parquet for training (see example notebook).\nThe full list of valid content types are CSV, LIBSVM, PARQUET, RECORDIO_PROTOBUF (see source)\n\nAdditionally:\nUber Petastorm for reading parquet into Tensorflow, Pytorch, and PySpark inputs.\nAs XGBoost accepts numpy, you can convert from PySpark to numpy\/pandas using the mentioned PyArrow.",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"framework read parquet train uber petastorm read parquet tensorflow pytorch pyspark input pyarrow convert pyspark numpi panda accept input bias summari",
        "Solution_link_count":0.0,
        "Solution_original_content":"framework read parquet train notebook list type csv libsvm parquet recordio protobuf sourc uber petastorm read parquet tensorflow pytorch pyspark input accept numpi convert pyspark numpi panda pyarrow",
        "Solution_preprocessed_content":"framework read parquet train list type csv libsvm parquet uber petastorm read parquet tensorflow pytorch pyspark input accept numpi convert pyspark pyarrow",
        "Solution_readability":9.9,
        "Solution_reading_time":4.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":2.8511111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Customer who has created a AWS glue dev endpoint and want to run two Sagemaker notebooks in parallel on same single Dev endpoint but its not working .\n\nThe one which is invoked first is only able to run the job, while another one fails. what could be possible reasons and fix for it?",
        "Challenge_closed_time":1591030326000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591020062000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUIDitlJMgTlGai61w_Zvqdg\/running-concurrent-sessions-from-sage-maker-notebooks-on-glue-dev-endpoints",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":4.35,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2.8511111111,
        "Challenge_title":"Running concurrent sessions from SageMaker notebooks on Glue Dev Endpoints.",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":157.0,
        "Challenge_word_count":62,
        "Platform":"Tool-specific",
        "Solution_body":"SageMaker notebooks are Jupyter notebooks that uses the SparkMagic module to connect to a local Livy setup. The local Livy does an SSH tunnel to Livy service on the Glue Spark server. Apache Livy binds to post 8998 and is a RESTful service that can relay multiple Spark session commands at the same time so multiple port binding conflicts cannot happen. So yes, you can have multiple sessions as long as the backend cluster has resources to serve that many sessions.\n\nYou can run the following command in a notebook to check the defaults for Spark sessions:\n\nspark.sparkContext.getConf().getAll()\n\n\nI see the following defaults in my Spark session. You can easily override them from the config file at ~\/.sparkmagic\/config.json or by using the %%configure magic from within the notebook.\n\nspark.executor.cores 4\nspark.executor.memory 5g\nspark.driver.memory 5g\n\n\nNote that spark.executor.instances is not set and spark.dynamicAllocation.enabled is not overridden which means that it is true, so if you have a demanding Spark job in one notebook, it can take over all resources in the cluster and prevent other Spark sessions from starting. The recommendation when sharing a single Glue Dev endpoint is to limit each session to a few executors so that multiple sessions can acquire resources from the cluster e.g.:\n\n%%configure -f\n{\"executorMemory\": \"5G\", \"executorCores\":4,\"numExecutors\":2}\n\n\n(Note: Tested on multiple SageMaker PySpark notebooks in single SageMaker notebook instances as well as multiple SageMaker notebook instances.)",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"default spark session spark sparkcontext getconf getal notebook overrid default config file sparkmag config json configur magic notebook limit session executor multipl session acquir resourc cluster notebook run",
        "Solution_link_count":0.0,
        "Solution_original_content":"notebook jupyt notebook sparkmag modul connect local livi setup local livi ssh tunnel livi servic glue spark server apach livi bind rest servic relai multipl spark session time multipl port bind conflict multipl session backend cluster resourc serv session run notebook default spark session spark sparkcontext getconf getal default spark session easili overrid config file sparkmag config json configur magic notebook spark executor core spark executor memori spark driver memori note spark executor instanc set spark dynamicalloc enabl overridden spark job notebook resourc cluster prevent spark session start share singl glue dev endpoint limit session executor multipl session acquir resourc cluster configur executormemori executorcor numexecutor note test multipl pyspark notebook singl notebook instanc multipl notebook instanc",
        "Solution_preprocessed_content":"notebook jupyt notebook sparkmag modul connect local livi setup local livi ssh tunnel livi servic glue spark server apach livi bind rest servic relai multipl spark session time multipl port bind conflict multipl session backend cluster resourc serv session run notebook default spark session default spark session easili overrid config file configur magic notebook note set overridden spark job notebook resourc cluster prevent spark session start share singl glue dev endpoint limit session executor multipl session acquir resourc cluster configur executormemori executorcor numexecutor note test multipl pyspark notebook singl notebook instanc multipl notebook",
        "Solution_readability":9.3,
        "Solution_reading_time":19.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":231.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":93.5734752778,
        "Challenge_answer_count":1,
        "Challenge_body":"In Azure Machine learning Studio, I have imported a dataset from a locally stored spreadsheet. In the designer, I drag the dataset into the workspace, right click, and select 'Visualize. I get the following error:\n\n\"Unable to visualize this dataset. This might be because your data is stored behind a virtual network or your data does not support profile\". I've searched for hours for a remedy, but find nothing.\n\nWhat do I do to fix this error?",
        "Challenge_closed_time":1603128837008,
        "Challenge_comment_count":5,
        "Challenge_created_time":1602791972497,
        "Challenge_favorite_count":4.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/127980\/error-when-visualizing-dataset-in-microsoft-azure.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":6.9,
        "Challenge_reading_time":6.35,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":93.5734752778,
        "Challenge_title":"Error when Visualizing Dataset in Microsoft Azure Machine Learning Studio",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":86,
        "Platform":"Tool-specific",
        "Solution_body":"@DanaShields-7459 I have tried this scenario with my workspace and i was able to replicate the message you have seen. It looks like you are using the Dataset type as File while creating the dataset which is causing the issue. Please register the dataset as Tabular type and then use the dataset in designer. This should show you the preview of the data. Here is a screen shot from my workspace of the designer.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"regist dataset tabular type file type design visual data replic messag tabular type",
        "Solution_link_count":0.0,
        "Solution_original_content":"danashield tri workspac replic messag dataset type file creat dataset regist dataset tabular type dataset design preview data screen shot workspac design",
        "Solution_preprocessed_content":"tri workspac replic messag dataset type file creat dataset regist dataset tabular type dataset design preview data screen shot workspac design",
        "Solution_readability":5.5,
        "Solution_reading_time":4.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":74.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":158.8487369445,
        "Challenge_answer_count":3,
        "Challenge_body":"do I still have the access to it?",
        "Challenge_closed_time":1655315539550,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654743684097,
        "Challenge_favorite_count":12.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/882424\/will-experiments-disappear-if-i-dont-migrate-to-de.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":6.4,
        "Challenge_reading_time":1.13,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":158.8487369445,
        "Challenge_title":"will experiments disappear if I don\u2019t migrate to Designer?",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":null,
        "Challenge_word_count":16,
        "Platform":"Tool-specific",
        "Solution_body":"Hello @Alexandre-2525\n\nI hope Rohit's reponse is helpful, please let us know if you have more question. All the data of studio will be avaiable till August 2024, you still have time to decide if you want to keep them, but Designer will provide the same experience and supporting the same function, you may want to try.\n\nPlease kindly accept the answer if you feel helpful to support the community, thanks a lot.\n\n\n\n\nRegards,\nYutong",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"clarif access migrat design",
        "Solution_link_count":0.0,
        "Solution_original_content":"alexandr hope rohit repons data studio avaiabl till august time decid design function kindli accept commun yutong",
        "Solution_preprocessed_content":"hope rohit repons data studio avaiabl till august time decid design function kindli accept commun yutong",
        "Solution_readability":10.7,
        "Solution_reading_time":5.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":11.6161658333,
        "Challenge_answer_count":2,
        "Challenge_body":"I found that I can register the model using Mlflow, but I don't know how to register it in ONNX format.\nI found out that the model is registered using Mlflow.\nBut I don't know how to convert AutoML models to ONNX format and register them with Mlflow.\n\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.core import Workspace, Dataset\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.model import Model\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nfrom azureml.automl.runtime.onnx_convert import OnnxConverter\nfrom random import random\nfrom mlflow.tracking import MlflowClient\nimport mlflow\nimport mlflow.onnx\nimport os\nimport azureml.mlflow\n\nauth = ServicePrincipalAuthentication(\ntenant_id=\"\",\nservice_principal_id=\"\",\nservice_principal_password=\"\")\n\nsubscription_id = ''\nresource_group = ''\nworkspace_name = ''\n\nml_client = MLClient(credential=auth,\nsubscription_id=subscription_id,\nresource_group_name=resource_group)\n\nazure_mlflow_uri = ml_client.workspaces.get(workspace_name).mlflow_tracking_uri\nmlflow.set_tracking_uri(azure_mlflow_uri)\n\nws = Workspace(subscription_id, resource_group, workspace_name, auth=auth)\n\ntrain_data = Dataset.get_by_name(ws, name='iris')\n\nlabel = \"class\"\n\nautoml_settings = {\n\"primary_metric\": 'AUC_weighted',\n\"n_cross_validations\": 2\n}\n\nautoml_classifier = AutoMLConfig(\ntask='classification',\nblocked_models=['XGBoostClassifier'],\nenable_onnx_compatible_models=True,\nexperiment_timeout_minutes=30,\ntraining_data=train_data,\nlabel_column_name=label,\n**automl_settings\n)\n\nexperiment_name = 'experimetn_with_mlflow'\nmlflow.set_experiment(experiment_name)\nexperiment = Experiment(ws, experiment_name)\n\nwith mlflow.start_run() as mlflow_run:\nmlflow.log_metric(\"iris_metric\", random())\n\n mlflow_run = experiment.submit(automl_classifier, show_output=True)\n description = 'iris_Description'\n model = mlflow_run.register_model(description=description,\n                                model_name='iris_Model')\n best_run, onnx_mdl = mlflow_run.get_output(return_onnx_model=True)\n onnx_fl_path = \".\/best_model.onnx\"\n OnnxConverter.save_onnx_model(onnx_mdl, onnx_fl_path)\n model = Model.register(workspace=ws,\n                     description=description,\n                     model_name='iris_onnx_model',\n                     model_path=onnx_fl_path)\n client = MlflowClient()\n finished_mlflow_run = MlflowClient().get_run(mlflow_run.run_id)\n metrics = finished_mlflow_run.data.metrics\n tags = finished_mlflow_run.data.tags\n params = finished_mlflow_run.data.params\n model_path  = \"best_model\"\n model_uri = 'runs:\/{}\/{}'.format(mlflow_run.run_id, model_path)\n mlflow.register_model(model_uri, 'iris_onnx_mlflow_model')",
        "Challenge_closed_time":1664453127300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664411309103,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1027830\/i-want-to-register-the-model-learned-by-automl-in.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":20.2,
        "Challenge_reading_time":37.33,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":11.6161658333,
        "Challenge_title":"I want to register the model learned by AutoML in Azure Machine learning in ONNX format and call it in Azure Synapse Analitics.",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":null,
        "Challenge_word_count":207,
        "Platform":"Tool-specific",
        "Solution_body":"@10433767 Thanks for the question. Can you please share document\/sample that you are trying. In order to save trained model download (and score) as the ONNX model you have here a few code examples.\nMLflow model registry will enable Synapse to run ONNX models is in preview.\n\nHere is the ONNX prediction section in the sample notebook.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"model registri regist automl model onnx format onnx model download score onnx predict section sampl notebook",
        "Solution_link_count":0.0,
        "Solution_original_content":"share document sampl order save train model download score onnx model model registri enabl synaps run onnx model preview onnx predict section sampl notebook",
        "Solution_preprocessed_content":"share order save train model download onnx model model registri enabl synaps run onnx model preview onnx predict section sampl notebook",
        "Solution_readability":5.4,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":54.7114913889,
        "Challenge_answer_count":1,
        "Challenge_body":"I used Custom Vision to create a small proof of concept project, and it works super nice; however, now that we are ready for the next steps to see if it is doable to use it, I have got some questions that might be roadblocks if I were to go ahead and work on the business implementation:\n\nThe service cannot be deployed in Canada regions. Is this something that will be considered in the future? This is a huge block because the items classified may contain data that should not leave the Canadian space\n\n\nWhat's the privacy terms of using custom vision or where can I find them to read? As the previous item describes, the items classified could contain compromised data, so it would be unfeasible to use the custom vision service if the data is going to be \"shared\" or \"used\" by Microsoft or other parties for other purposes.\n\nAs a side question, is this service capable of classifying PDF documents as images? And if not, is there a known Azure\/Microsoft service that does so?\n\nThanks a bunch! :)\n\nKenny Perroni",
        "Challenge_closed_time":1606709384576,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606512423207,
        "Challenge_favorite_count":4.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/178721\/custom-vision-for-canada-region.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":12.55,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":54.7114913889,
        "Challenge_title":"Custom Vision for Canada region",
        "Challenge_topic":"Database Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":186,
        "Platform":"Tool-specific",
        "Solution_body":"@KennyPerroni-1017 Thanks for the feedback. We have forwarded this feedback to our product team, You can also raise a user voice request here so the community can vote and provide their feedback, the product team then checks this feedback and implements the same for Canada region. For Region availability please check the following link\n\nFor custom vision service you control over the storage and deletion of any customer data that store as part of the service. Please follow the below for privacy and compliance. As with all of the Cognitive Services, developers using the Custom Vision service should be aware of Microsoft's policies on customer data. See the below Cognitive Services page on the Microsoft Trust Center to learn more.\nhttps:\/\/azure.microsoft.com\/en-us\/support\/legal\/cognitive-services-compliance-and-privacy\/\n\n\n\n\nThe Computer vision Read service support PDF document as images. Here is the link for vision best practice and samples. Also the Form Recognizer supports OCR and PDF documents with AI builder.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"rais voic request vision canada region team implement base commun feedback trust center privaci complianc polici relat vision cognit servic vision read servic classifi pdf document imag link practic sampl recogn servic ocr pdf document builder bias summari",
        "Solution_link_count":1.0,
        "Solution_original_content":"kennyperroni feedback forward feedback team rais voic request commun vote feedback team feedback implement canada region region link vision servic control storag delet data store servic privaci complianc cognit servic vision servic awar polici data cognit servic page trust center http com legal cognit servic complianc privaci vision read servic pdf document imag link vision practic sampl recogn ocr pdf document builder",
        "Solution_preprocessed_content":"feedback forward feedback team rais voic request commun vote feedback team feedback implement canada region region link vision servic control storag delet data store servic privaci complianc cognit servic vision servic awar polici data cognit servic page trust center vision read servic pdf document imag link vision practic sampl recogn ocr pdf document builder",
        "Solution_readability":9.9,
        "Solution_reading_time":12.75,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":153.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":4.8680555556,
        "Challenge_answer_count":1,
        "Challenge_body":"Can I train models in parallel? Is is possible to train model in parallel on like hyperdrive?",
        "Challenge_closed_time":1653922130807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653904605807,
        "Challenge_favorite_count":10.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/869619\/parallel-training.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":1.38,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4.8680555556,
        "Challenge_title":"Parallel training",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":18,
        "Platform":"Tool-specific",
        "Solution_body":"@Chungsun-1776 Thanks for the question. The max number of parallel tasks is limited by number of cores in the cluster (excluding master node).\nThe demand for parallelism comes from two sources: 1. The cross validation which address multiple combination of train-val datasets & parameters 2. The training algorithm itself which can be parallelized.\n\n\u2022 You can run multiple runs in a distributed fashion across AML clusters, meaning that each cluster node can be running a run in parallel to other nodes running other runs. For instance, that\u2019s what we also do with Pipeline steps, HyperParameter Tunning child runs and for Azure AutoML child runs.\n\nhttps:\/\/aka.ms\/many-models is a solution accelerator that will help you walk through to run many models.\nIn the HyperDriveConfig there is AMLcompute max_concurrent_runs map to maximum number of nodes that will be used to run a hyperparameter tuning run. So there would be 1 execution per node.\n\n\n\n\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"run multipl run distribut fashion aml cluster cluster node run run parallel node run run hyperdriveconfig amlcomput concurr run map maximum node run hyperparamet tune run execut node direct acceler call http aka model walk run model",
        "Solution_link_count":2.0,
        "Solution_original_content":"chungsun parallel task limit core cluster exclud master node parallel come sourc cross address multipl combin train val dataset paramet train algorithm parallel run multipl run distribut fashion aml cluster cluster node run run parallel node run run instanc that pipelin step hyperparamet tun child run automl child run http aka model acceler walk run model hyperdriveconfig amlcomput concurr run map maximum node run hyperparamet tune run execut node http doc com api train core train hyperdr hyperdriveconfig",
        "Solution_preprocessed_content":"parallel task limit core cluster parallel come sourc cross address multipl combin dataset paramet train algorithm parallel run multipl run distribut fashion aml cluster cluster node run run parallel node run run instanc that pipelin step hyperparamet tun child run automl child run acceler walk run model hyperdriveconfig amlcomput map maximum node run hyperparamet tune run execut node",
        "Solution_readability":10.3,
        "Solution_reading_time":13.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":149.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":6.0041666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I see in this page of documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-interface-endpoint.html that:\n\n\"You can connect to your notebook instance from your VPC through an interface endpoint in your Virtual Private Cloud (VPC) instead of connecting over the internet. When you use a VPC interface endpoint, communication between your VPC and the notebook instance is conducted entirely and securely within the AWS network.\"\n\nHow would customer interact on their laptop with the UI of a notebook instance sitting in a VPC?",
        "Challenge_closed_time":1541516577000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541494962000,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU5z-7bQ9zQOi_NrVlHy_5oA\/which-connection-method-when-using-sage-maker-notebook-through-vpc-interface-endpoint",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":7.96,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":6.0041666667,
        "Challenge_title":"Which connection method when using SageMaker Notebook through VPC Interface Endpoint?",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":280.0,
        "Challenge_word_count":88,
        "Platform":"Tool-specific",
        "Solution_body":"If you are trying to access from within VPC, you'll have a direct connection. Otherwise, you'll need a configuration in place, such as Amazon VPN or AWS Direct Connect, to connect to your notebooks. Here is the blog post where we tried to explain how to set up AWS PrivateLink for Amazon SageMaker notebooks: https:\/\/aws.amazon.com\/blogs\/machine-learning\/direct-access-to-amazon-sagemaker-notebooks-from-amazon-vpc-by-using-an-aws-privatelink-endpoint\/",
        "Solution_comment_count":null,
        "Solution_gpt_summary":"set privatelink notebook connect notebook instanc vpc interfac endpoint vpn direct connect connect notebook vpc",
        "Solution_link_count":1.0,
        "Solution_original_content":"access vpc direct connect configur vpn direct connect connect notebook blog tri explain set privatelink notebook http com blog direct access notebook vpc privatelink endpoint",
        "Solution_preprocessed_content":"access vpc direct connect configur vpn direct connect connect notebook blog tri explain set privatelink notebook",
        "Solution_readability":14.0,
        "Solution_reading_time":5.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Challenge_adjusted_solved_time":7.7066591667,
        "Challenge_answer_count":1,
        "Challenge_body":"We are evaluating the Azure ML Assisted Object detection labeling and I have some questions:\n1. How do I mark an image as a negative?\n2. How do I rename a label?\n3. How do I go back to a skipped image?\n4. When labeling if I discover that an image should not be in the dataset, how do I delete it from the dataset? The id of the image is no where to be found.\n5. For an autolabeled image, if I accidentally delete the bounding box, how do I undo this operation?\n6. Sometime the autolabeler creates small bounding boxes without any labels. Is this a bug?\n\nThank you",
        "Challenge_closed_time":1659001066756,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658973322783,
        "Challenge_favorite_count":11.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/945298\/negative-samples-in-ml-assisted-image-labeling.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":7.24,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":7.7066591667,
        "Challenge_title":"Negative Samples in ML Assisted Image Labeling",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Platform":"Tool-specific",
        "Solution_body":"@PrashantSaraswat-9512 I think I can answer some of your questions from some of the projects I used for labeling.\n\nHow do I mark an image as a negative?\nUnlike the Azure custom vision labeling experience, there isn't a feature to mark a label as negative. I believe you can add another label and use it as a negative label and tag images.\n\nHow do I go back to a skipped image?\nGo to the Data tab of your project and select Review Labels tab from the side. Using the filters option on right hand side, set the Asset Type as \"Skipped\". This should pull any skipped images and you should be able to assign the required label and a button should be enabled to update label. The same applies for updating any labeled image or bounding box.\n\n\nHow do I rename a label?\nI think a label cannot be renamed after it is created. You can delete all labels and create a new set though. Just stop your project and select the Details-> Label Classes tab and click Add label option to see this screen.\n\n\nWhen labeling if I discover that an image should not be in the dataset, how do I delete it from the dataset? The id of the image is no where to be found.\nI think you can skip the image since the dataset is registered while creating a project there is no option to delete certain images after this action.\n\nFor an autolabeled image, if I accidentally delete the bounding box, how do I undo this operation?\nI have not used auto labeling before but the same step to update the skipped image or label should help you with this step.\n\nSometime the autolabeler creates small bounding boxes without any labels. Is this a bug?\nNot sure about this issue since I haven't come across it. You could report through support or through portal using the smiley image on the top right hand corner.\n\nI hope this helps!!\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"mark imag neg add label neg label tag imag skip imag data tab select review label tab set asset type skip assign label updat label label renam creat delet label creat set imag dataset skip imag option delet imag dataset regist creat undo accident delet bound box step updat skip imag label",
        "Solution_link_count":0.0,
        "Solution_original_content":"prashantsaraswat label mark imag neg unlik vision label isn featur mark label neg believ add label neg label tag imag skip imag data tab select review label tab filter option hand set asset type skip pull skip imag assign label button enabl updat label appli updat label imag bound box renam label label renam creat delet label creat set stop select label class tab click add label option screen label imag dataset delet dataset imag skip imag dataset regist creat option delet imag action autolabel imag accident delet bound box undo oper auto label step updat skip imag label step autolabel creat small bound box label haven come report portal smilei imag hand corner hope click upvot commun member read thread",
        "Solution_preprocessed_content":"label mark imag neg unlik vision label isn featur mark label neg believ add label neg label tag imag skip imag data tab select review label tab filter option hand set asset type skip pull skip imag assign label button enabl updat label appli updat label imag bound box renam label label renam creat delet label creat set stop select label class tab click add label option screen label imag dataset delet dataset imag skip imag dataset regist creat option delet imag action autolabel imag accident delet bound box undo oper auto label step updat skip imag label step autolabel creat small bound box label haven come report portal smilei imag hand corner hope click upvot commun member read thread",
        "Solution_readability":6.7,
        "Solution_reading_time":22.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":352.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":1.5253416667,
        "Challenge_answer_count":1,
        "Challenge_body":"Anaconda announced that commercial users should purchase the licenses on APR, 20, 2020.\nHowever, Azure Machine Learning heavily depends on this anaconda packages; developing models on computing instance and deploy container environment.\n\nDo commercial developers have to pay to anaconda to continue usage of Azure Machine Learning with anaconda?",
        "Challenge_closed_time":1605602646227,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605597154997,
        "Challenge_favorite_count":5.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/165312\/anaconda-commercial-use-on-azure-machine-learning.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":4.99,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.5253416667,
        "Challenge_title":"Anaconda commercial use on Azure Machine Learning",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Platform":"Tool-specific",
        "Solution_body":"@EisukeYonezawa-9200 If you are have the commercial version of Anaconda you can configure the same for your experiments to deploy packages that are available under license but in most cases you can simply use conda to install available python packages without paying for the commercial license. There is no restriction to have a commercial license to continue using Azure Machine Learning with anaconda. I hope this helps!!",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"anaconda purchas commerci licens conda instal packag pai commerci licens commerci version anaconda configur deploi packag licens",
        "Solution_link_count":0.0,
        "Solution_original_content":"eisukeyonezawa commerci version anaconda configur deploi packag licens simpli conda instal packag pai commerci licens restrict commerci licens anaconda hope",
        "Solution_preprocessed_content":"commerci version anaconda configur deploi packag licens simpli conda instal packag pai commerci licens restrict commerci licens anaconda hope",
        "Solution_readability":12.0,
        "Solution_reading_time":5.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":9.2260158334,
        "Challenge_answer_count":1,
        "Challenge_body":"I'd like to deploy a machine learning service using AzureML on AKS. I also need to add some OpenAPI specification for it.\n\nFeatures in https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python are neat, but that of having API docs\/swagger for the webservice seems missing.\n\nHaving some documentation is useful especially if the model takes in input several features of different type.\n\nTo overcome this, I currently get models trained in AzureML and include them in Docker containers that use the python FastAPI library to build the API and OpenAPI\/Swagger specs, and those are deployed on some host.\n\nCan I do something equivalent to this with AKS in AzureML instead? If so, how?",
        "Challenge_closed_time":1600930445547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600897231890,
        "Challenge_favorite_count":7.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/105437\/can-i-add-openapi-specification-to-a-webservice-de.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":10.08,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":9.2260158334,
        "Challenge_title":"Can I add OpenAPI specification to a webservice deployed with AzureML in AKS?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":118,
        "Platform":"Tool-specific",
        "Solution_body":"@DavideFiocco-7346 The deployments of Azure ML provide a swagger specification URI that can be used directly. The documentation of this is available here. You can print your swagger_uri of the web service and check if it confirms with the specifications you are creating currently.\n\nIf the above response helps, please accept the response as answer. Thanks!!",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"swagger uri deploy add openapi webservic deploi ak document websit print swagger uri web servic creat",
        "Solution_link_count":0.0,
        "Solution_original_content":"davidefiocco deploy swagger uri directli document print swagger uri web servic creat respons accept respons",
        "Solution_preprocessed_content":"deploy swagger uri directli document print web servic creat respons accept respons",
        "Solution_readability":8.8,
        "Solution_reading_time":4.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":15.3321294445,
        "Challenge_answer_count":1,
        "Challenge_body":"The \"top predictors by influence\" in the training reports of AutoML regression models is very useful (see reference image), but I'm looking for a way to display all of the predictors, not just the top 10. Any way I can visualise this either in the training report or using the data tables themselves would be very useful.",
        "Challenge_closed_time":1635215186563,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635159990897,
        "Challenge_favorite_count":8.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/602784\/how-do-you-see-all-predictors-by-influence-not-jus.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":5.11,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":15.3321294445,
        "Challenge_title":"How do you see ALL predictors by influence not just the top predictors of AutoML training reports?",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":null,
        "Challenge_word_count":73,
        "Platform":"Tool-specific",
        "Solution_body":"Hi, PowerBi is not currently supported here on Q&A. Please post your question on the PowerBI community forum for faster response. Thanks.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"powerbi powerbi commun forum faster respons kindli accept",
        "Solution_preprocessed_content":"powerbi powerbi commun forum faster respons kindli accept",
        "Solution_readability":7.2,
        "Solution_reading_time":2.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":0.4168277778,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi, I am getting this error when scoring a model.\n\nSeems like it is an out-of-memory issue or a segfault issue (no idea what that means).\n\nI'm using Designer while my compute is Standard Dv2 Family vCPUs. Have made no changes to my storage account key.\n\nAny advice on how to debug this one? Many thanks in advance\n\nAzureMLCompute job failed.\nUserProcessKilledBySystemSignal: Job failed since the user script received system termination signal usually due to out-of-memory or segfault.\nReason: Process Killed with either 6:aborted or 9:killed or 11:segment fault. exit code here is from wrapping bash hence 128 + n\nCause: killed\nTaskIndex:\nNodeIp: 10.0.0.5\nNodeId: tvmps_ee452edcf7395836bdf60c0e0cd5f3a6308fafbb41c860c50a47be1367393df6_d\nReason: Job failed with non-zero exit Code",
        "Challenge_closed_time":1620235517220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620234016640,
        "Challenge_favorite_count":8.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/384152\/userprocesskilledbysystemsignal-job-failed-since-t.html",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":11.59,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.4168277778,
        "Challenge_title":"UserProcessKilledBySystemSignal: Job failed since the user script received system termination signal usually due to out-of-memory or segfault.",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":129,
        "Platform":"Tool-specific",
        "Solution_body":"Seems like it was an out-of-memory problem. If I reduce the trainning set, I get no error.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"reduc train set avoid memori",
        "Solution_link_count":0.0,
        "Solution_original_content":"memori reduc train set",
        "Solution_preprocessed_content":null,
        "Solution_readability":4.2,
        "Solution_reading_time":1.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":8.2721991667,
        "Challenge_answer_count":1,
        "Challenge_body":"I am considering using Personalizer for project and have found limited third party metrics for this service. This one article indicates needing TENS of thousands of hits to get good results.\n\nhttps:\/\/medium.com\/@EnefitIT\/we-tested-azure-personalizer-heres-what-you-can-expect-8c5ec074a28e\n\nCan any one provide any other data?\n\nObviously, over time it will get better, but does it have to get to 10K+ to get good?",
        "Challenge_closed_time":1606893531207,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606863751290,
        "Challenge_favorite_count":6.0,
        "Challenge_link":"https:\/\/learn.microsoft.com\/answers\/questions\/182318\/training-personalizer.html",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":5.49,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":8.2721991667,
        "Challenge_title":"Training Personalizer",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":null,
        "Challenge_word_count":58,
        "Platform":"Tool-specific",
        "Solution_body":"@GregorioRojas-6472 The minimum requirements to have an effective recommendation is to have a minimum of ~1k\/day content-related events. Higher rate of events do help you to provide faster and better recommendations. All the requirements are documented in the official documentation page of the service. The samples repo provides some data along with the quickstart's from the documentation to get started. The service now provides an E0 tier or apprentice mode that helps you test the service and gain confidence to move to a higher tier with production level recommendations.",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"offici document page minimum dai relat event higher rate event faster servic tier apprentic mode test servic gain confid move higher tier level addit",
        "Solution_link_count":0.0,
        "Solution_original_content":"gregorioroja minimum minimum dai relat event higher rate event faster document offici document page servic sampl repo data quickstart document start servic tier apprentic mode test servic gain confid higher tier level",
        "Solution_preprocessed_content":"minimum minimum event higher rate event faster document offici document page servic sampl repo data quickstart document start servic tier apprentic mode test servic gain confid higher tier level",
        "Solution_readability":11.4,
        "Solution_reading_time":7.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":89.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Challenge_adjusted_solved_time":85.1185105556,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi! During training, my script crashed unexpectedly and did not save the latest epoch information.  I restarted training without being aware of it, and now my epochs are offset by a large number.<\/p>\n<p>Is it possible to edit the epoch number (index) and add a certain value to each entry? I have tried opening the \u201crun_name.wandb\u201d file and I can already see the \u2018_step\u2019 variable for each entry, but I was wondering if there is a cleaner way to perform such an update.<\/p>\n<p>Thank you in advance for your help!<\/p>",
        "Challenge_closed_time":1659047787071,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658741360433,
        "Challenge_favorite_count":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/update-offline-run-before-syncing\/2794",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.73,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":85.1185105556,
        "Challenge_title":"Update offline run before syncing",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":122.0,
        "Challenge_word_count":94,
        "Platform":"Tool-specific",
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/vandrew\">@vandrew<\/a> , I understand what you are attempting to achieve now. At this time our API doesn\u2019t support offline mode to access local log files. We do have this planned as a future feature but I can\u2019t speak to a specific timeline. At this time you will have to sync your runs first in online mode, then update metrics using the API.<\/p>",
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"vandrew achiev time api doesnt offlin mode access local log file plan futur featur speak timelin time sync run onlin mode updat metric api",
        "Solution_preprocessed_content":"achiev time api doesnt offlin mode access local log file plan futur featur speak timelin time sync run onlin mode updat metric api",
        "Solution_readability":7.2,
        "Solution_reading_time":4.63,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":64.0,
        "Tool":"Weights & Biases"
    }
]