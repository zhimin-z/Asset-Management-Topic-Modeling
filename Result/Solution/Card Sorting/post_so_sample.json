[
    {
        "Answerer_created_time":1263294862568,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":0.3139213889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>following the answers to this question <a href=\"https:\/\/stackoverflow.com\/questions\/48264656\/load-s3-data-into-aws-sagemaker-notebook\">Load S3 Data into AWS SageMaker Notebook<\/a> I tried to load data from S3 bucket to SageMaker Jupyter Notebook.<\/p>\n<p>I used this code:<\/p>\n<pre><code>import pandas as pd\n\nbucket='my-bucket'\ndata_key = 'train.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n\npd.read_csv(data_location)\n<\/code><\/pre>\n<p>I replaced <code>'my-bucket'<\/code> by the ARN (Amazon Ressource name) of my S3 bucket (e.g. &quot;<code>arn:aws:s3:::name-of-bucket<\/code>&quot;) and replaced <code>'train.csv'<\/code> by the csv-filename which is stored in the S3 bucket. Regarding the rest I did not change anything at all. What I got was this <code>ValueError<\/code>:<\/p>\n<pre><code>ValueError: Failed to head path 'arn:aws:s3:::name-of-bucket\/name_of_file_V1.csv': Parameter validation failed:\nInvalid bucket name &quot;arn:aws:s3:::name-of-bucket&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.\\-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).*:s3:[a-z\\-0-9]+:[0-9]{12}:accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$&quot;\n<\/code><\/pre>\n<p>What did I do wrong? Do I have to modify the name of my S3 bucket?<\/p>",
        "Challenge_closed_time":1613558457267,
        "Challenge_comment_count":1,
        "Challenge_created_time":1613557327150,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66239966",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":19.3,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":0.3139213889,
        "Challenge_title":"Loading data from S3 bucket to SageMaker Jupyter Notebook - ValueError - Invalid bucket name",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":345,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559131080072,
        "Poster_location":null,
        "Poster_reputation_count":1166.0,
        "Poster_view_count":248.0,
        "Solution_body":"<p>The path should be:<\/p>\n<pre><code>data_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n<\/code><\/pre>\n<p>where <code>bucket<\/code> is <code>&lt;bucket-name&gt;<\/code> <strong>not ARN<\/strong>. For example <code>bucket=my-bucket-333222<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.1,
        "Solution_reading_time":3.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1572449042430,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":2082.0,
        "Answerer_view_count":238.0,
        "Challenge_adjusted_solved_time":2.8597922222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The entire error message after executing <code>terraform apply<\/code> within the terraform-folder of <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\" rel=\"nofollow noreferrer\">this source code in my GitHub-repo<\/a> (inspired by <a href=\"https:\/\/www.linkedin.com\/pulse\/terraform-sagemaker-part-2a-creating-custom-notebook-instance-david\" rel=\"nofollow noreferrer\">this tutorial<\/a> and <a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\" rel=\"nofollow noreferrer\">its related GitHub-repo<\/a>):<\/p>\n<pre><code>aws_sagemaker_notebook_instance.notebook_instance: Creating...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [10s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [20s elapsed]\n...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [15m21s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [15m31s elapsed]\n\u2577\n\u2502 Error: error waiting for sagemaker notebook instance (aws-sm-notebook-instance) to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(&lt;nil&gt;)\n\u2502\n\u2502   with aws_sagemaker_notebook_instance.notebook_instance,\n\u2502   on notebook_instance.tf line 2, in resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot;:\n\u2502    2: resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n\u2502\n<\/code><\/pre>\n<p>Internet research seemed to provide the solution in <a href=\"https:\/\/yuyasugano.medium.com\/machine-learning-infrastructure-terraforming-sagemaker-part-2-f2460a9a4663\" rel=\"nofollow noreferrer\">this article<\/a>, which inspired be to increase the allowed <code>IDLE_TIME<\/code> in the <code>on-start.sh<\/code> - script to <code>IDLE_TIME=1800<\/code> (in seconds, which equals 30 minutes). This should've been sufficient for the deployment time of around 15 minutes; yet, it threw the same error again.<\/p>\n<p>Next, I found <a href=\"https:\/\/stackoverflow.com\/questions\/65884743\/resolving-broken-deleted-state-in-terraform\">this post on StackOverFlow<\/a> suggesting to<\/p>\n<blockquote>\n<p>run <code>terraform refresh<\/code>, which will cause Terraform to refresh its state\nfile against what actually exists with the cloud provider.<\/p>\n<\/blockquote>\n<p>Unfortunately, running <code>terraform apply<\/code> right after refreshing didn't resolve the issue either.\nI'm wondering why the aforementioned <code>IDLE_TIME=1800<\/code> - setting does not have any effect. This should be more than sufficient for a 15-minute apply-time.<\/p>\n<hr \/>\n<p><strong>EDIT: adding code specifics for enhanced understanding<\/strong><\/p>\n<p><strong>1. Creating the SageMaker notebook instance<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n  name                    = &quot;aws-sm-notebook-instance&quot;\n  role_arn                = aws_iam_role.notebook_iam_role.arn\n  instance_type           = &quot;ml.t2.medium&quot;\n  lifecycle_config_name   = aws_sagemaker_notebook_instance_lifecycle_configuration.notebook_config.name\n  default_code_repository = aws_sagemaker_code_repository.git_repo.code_repository_name\n}\n<\/code><\/pre>\n<p><strong>2. Defining the SageMaker notebook lifecycle configuration<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance_lifecycle_configuration&quot; &quot;notebook_config&quot; {\n  name      = &quot;dev-platform-al-sm-lifecycle-config&quot;\n  on_create = filebase64(&quot;..\/scripts\/on-create.sh&quot;)\n  on_start  = filebase64(&quot;..\/scripts\/on-start.sh&quot;)\n}\n<\/code><\/pre>\n<p><strong>3. Defining the Git repo to instantiate on the SageMaker notebook instance<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_code_repository&quot; &quot;git_repo&quot; {\n  code_repository_name = &quot;aws-sm-notebook-instance-repo&quot;\n\n  git_config {\n    repository_url = &quot;https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance.git&quot;\n  }\n}\n<\/code><\/pre>\n<p><strong>Contents of <code>on-start.sh<\/code> (including IDLE_TIME - parameter)<\/strong>\nNote that this script will be invoked by the <code>scripts\/autostop.py<\/code> - script, which you can find <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\/blob\/main\/scripts\/autostop.py\" rel=\"nofollow noreferrer\">here<\/a> in the associated <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\" rel=\"nofollow noreferrer\">public repo containing the source code<\/a>.<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\n\n## IDLE AUTOSTOP STEPS\n## ----------------------------------------------------------------\n\n## Setting the timeout (in seconds) for how long the SageMaker notebook can run idly before being auto-stopped\n# -&gt; e.g. 1800 s = 30 min since first deployment can take between 15 and 20 minutes which could then fail like so:\n# &quot;Error: error waiting for sagemaker notebook instance (aws-sm-notebook-instance) to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(&lt;nil&gt;)&quot;\n# Hint for solution under following link: https:\/\/yuyasugano.medium.com\/machine-learning-infrastructure-terraforming-sagemaker-part-2-f2460a9a4663\nIDLE_TIME=1800\n\n# Getting the autostop.py script from GitHub\necho &quot;Fetching the autostop script...&quot;\nwget https:\/\/raw.githubusercontent.com\/andreasluckert\/aws-sm-notebook-instance\/main\/scripts\/autostop.py\n\n# Using crontab to autostop the notebook when idle time is breached\necho &quot;Starting the SageMaker autostop script in cron.&quot;\n(crontab -l 2&gt;\/dev\/null; echo &quot;*\/5 * * * * \/usr\/bin\/python $PWD\/autostop.py --time $IDLE_TIME --ignore-connections&quot;) | crontab -\n\n\n\n## CUSTOM CONDA KERNEL USAGE STEPS\n## ----------------------------------------------------------------\n\n# Setting the proper user credentials\nsudo -u ec2-user -i &lt;&lt;'EOF'\nunset SUDO_UID\n\n# Setting the source for the custom conda kernel\nWORKING_DIR=\/home\/ec2-user\/SageMaker\/custom-miniconda\nsource &quot;$WORKING_DIR\/miniconda\/bin\/activate&quot;\n\n# Loading all the custom kernels\nfor env in $WORKING_DIR\/miniconda\/envs\/*; do\n    BASENAME=$(basename &quot;$env&quot;)\n    source activate &quot;$BASENAME&quot;\n    python -m ipykernel install --user --name &quot;$BASENAME&quot; --display-name &quot;Custom ($BASENAME)&quot;\ndone\n<\/code><\/pre>",
        "Challenge_closed_time":1631187921612,
        "Challenge_comment_count":6,
        "Challenge_created_time":1631108848430,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1631177626360,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69104302",
        "Challenge_link_count":10,
        "Challenge_participation_count":7,
        "Challenge_readability":16.9,
        "Challenge_reading_time":85.0,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":21.9647727778,
        "Challenge_title":"Terraform Error: error waiting for sagemaker notebook instance to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(<nil>)",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":768,
        "Challenge_word_count":500,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572449042430,
        "Poster_location":"Germany",
        "Poster_reputation_count":2082.0,
        "Poster_view_count":238.0,
        "Solution_body":"<p>The solution to the problem was to check the CloudWatch Log events under <code>CloudWatch -&gt; Log groups -&gt; \/aws\/sagemaker\/NotebookInstances -&gt; aws-sm-notebook-instance\/LifecycleConfigOnCreate<\/code> to find the following error-message:<\/p>\n<pre><code>\/bin\/bash: \/tmp\/OnCreate_2021-09-08-12-24rw5al34g: \/bin\/bash^M: bad interpreter: No such file or directory\n<\/code><\/pre>\n<p>A bit of internet research brought me to <a href=\"https:\/\/askubuntu.com\/questions\/304999\/not-able-to-execute-a-sh-file-bin-bashm-bad-interpreter\/305001#305001\">this solution related to newline characters in shell-scripts<\/a>, which depend on whether you are on <code>Windows<\/code> or a <code>UNIX<\/code>-system.\nAs I'm working on Windows, the shell-scripts created in VS-Code comprised dos-specific <code>CRLF<\/code> newline-handling, which could be resolved via the button on the bottom-right in <code>VS-Code<\/code> to switch the <em>carriage return<\/em> (CRLF) character to the <em>line feed<\/em> (LF) character used by UNIX.<\/p>\n<p>As the compute instance employed by AWS Sagemaker is a Linux-system, it cannot handle the dos-style CRLF newline-characters in the shell-scripts and this &quot;adds&quot; a <code>^M<\/code> after <code>\/bin\/bash<\/code> which obviously leads to an error as such an interpreter does not exist.<\/p>\n<p>So, finally <code>terraform apply<\/code> worked out well:<\/p>\n<pre><code>$ terraform apply\n...\n...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [7m30s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [7m40s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Creation complete after 7m43s [id=aws-sm-notebook-instance]\n\nApply complete! Resources: 1 added, 1 changed, 1 destroyed.\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.3,
        "Solution_reading_time":23.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":184.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":1.7733930556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm playing around with Azure ML Studio. Now I would like to add a new column in my dataset to calculate and in a further step to cluster my data. What's the best way to do it? I tried to add a column with sql (alter table) but it didn't work.<\/p>\n\n<p>btw. the \"add columns\" function only adds columns from another dataset...<\/p>\n\n<p>Thanks in advance!<\/p>",
        "Challenge_closed_time":1525265262092,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525258877877,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50133056",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.8,
        "Challenge_reading_time":4.82,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.7733930556,
        "Challenge_title":"Add a column in Microsoft Azure ML Studio",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":517,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1463242510132,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>The \"Apply SQL Transformation\" module should be able to do it. For example, I have a dataset with an <em>age<\/em> column and here's the SQL to create another column called <em>double_age<\/em>:<\/p>\n\n<pre><code>select age, age * 2 as double_age from t1;\n<\/code><\/pre>\n\n<p>Which produces a dataset with just the <em>age<\/em> and <em>double_age<\/em> columns:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/uZblo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uZblo.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.3,
        "Solution_reading_time":6.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1343828614448,
        "Answerer_location":"Seville, Spain",
        "Answerer_reputation_count":359.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":85.6601075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an inference pipeline with some PythonScriptStep with a ParallelRunStep in the middle. Everything works fine except for the fact that all mini batches are run on one node during the ParallelRunStep, no matter how many nodes I put in the <code>node_count<\/code> config argument.<\/p>\n<p>All the nodes seem to be up and running in the cluster, and according to the logs the <code>init()<\/code> function has been run on them multiple times. Diving into the logs I can see in <strong>sys\/error\/10.0.0.*<\/strong> that all the workers except the one that is working are saying:<\/p>\n<p><code>FileNotFoundError: [Errno 2] No such file or directory: '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/virtualstage\/azureml\/c36eb050-adc9-4c34-8a33-5f6d42dcb19c\/wd\/tmp8_txakpm\/bg.png'<\/code><\/p>\n<p><strong>bg.png<\/strong> happens to be a side argument created in a previous PythonScriptStep that I'm passing to the ParallelRunStep:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>bg_file = PipelineData('bg',  datastore=data_store)\nbg_file_ds = bg_file.as_dataset()\nbg_file_named = bg_file_ds.as_named_input(&quot;bg&quot;)\nbg_file_dw = bg_file_named.as_download()\n\n...\n\nparallelrun_step = ParallelRunStep(\n    name='batch-inference',\n    parallel_run_config=parallel_run_config,\n    inputs=[frames_data_named.as_download()],\n    arguments=[&quot;--bg_folder&quot;, bg_file_dw],\n    side_inputs=[bg_file_dw],\n    output=inference_frames_ds,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>What's happening here? Why the side argument seems to be available only in one worker while it fails in the others?<\/p>\n<p>BTW I found <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/957\" rel=\"nofollow noreferrer\">this<\/a> similar but unresolved question.<\/p>\n<p>Any help is much appreciated, thanks!<\/p>",
        "Challenge_closed_time":1619694485790,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619386109403,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67258465",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":23.72,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":85.6601075,
        "Challenge_title":"AzureML ParallelRunStep runs only on one node",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":244,
        "Challenge_word_count":186,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343828614448,
        "Poster_location":"Seville, Spain",
        "Poster_reputation_count":359.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>Apparently you need to specify a local mount path to use side_inputs in more than one node:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>bg_file_named = bg_file_ds.as_named_input(f&quot;bg&quot;)\nbg_file_mnt = bg_file_named.as_mount(f&quot;\/tmp\/{str(uuid.uuid4())}&quot;)\n\n...\n\nparallelrun_step = ParallelRunStep(\n    name='batch-inference',\n    parallel_run_config=parallel_run_config,\n    inputs=[frames_data_named.as_download()],\n    arguments=[&quot;--bg_folder&quot;, bg_file_mnt],\n    side_inputs=[bg_file_mnt],\n    output=inference_frames_ds,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>Sources:<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-parallel-run-step\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-parallel-run-step<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/18355\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/18355<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":31.7,
        "Solution_reading_time":13.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1522870754323,
        "Answerer_location":"Redmond, WA, USA",
        "Answerer_reputation_count":366.0,
        "Answerer_view_count":173.0,
        "Challenge_adjusted_solved_time":405.5357452778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using azureml sdk in Azure Databricks.<\/p>\n<p>When I write the script for inference model (%%writefile script.py) in a databricks cell,\nI try to load a .bin file that I loaded in Azure Machine Learning Datasets.<\/p>\n<p>I would like to do this in the script.py:<\/p>\n<pre><code>fasttext.load_model(azuremldatasetpath)\n<\/code><\/pre>\n<p>How can I do to give good dataset path of my .bin file in azuremldatasetpath variable ? (Without calling workspace in the script).<\/p>\n<p>Something like:<\/p>\n<pre><code>dataset_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'file.bin')\n<\/code><\/pre>",
        "Challenge_closed_time":1645723579036,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644263650353,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71024584",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":8.48,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":405.5357452778,
        "Challenge_title":"How give azure machine learning dataset path in an inference script?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":172,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1638189721320,
        "Poster_location":null,
        "Poster_reputation_count":151.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>You can use your model name with the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#azureml-core-model-model-get-model-path\" rel=\"nofollow noreferrer\">Model.get_model_path()<\/a> method to retrieve the path of the model file or files on the local file system. If you register a folder or a collection of files, this API returns the path of the directory that contains those files.<\/p>\n<p>More info you may want to refer: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#azureml_model_dir\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#azureml_model_dir<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":17.9,
        "Solution_reading_time":10.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1444758849803,
        "Answerer_location":null,
        "Answerer_reputation_count":11962.0,
        "Answerer_view_count":960.0,
        "Challenge_adjusted_solved_time":3.0605805556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there a way to view the schema of a graph in a Neptune cluster using Jupyter Notebook? <\/p>\n\n<p>Like you would do a \"select * from tablename limit 10\" in an RDS using SQL, similarly is there a way to get a sense of the graph data through Jupyter Notebook?<\/p>",
        "Challenge_closed_time":1584902412783,
        "Challenge_comment_count":1,
        "Challenge_created_time":1584891394693,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":1637708419888,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60801292",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.0,
        "Challenge_reading_time":3.78,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.0605805556,
        "Challenge_title":"View Neptune Graph Schema using Jupyter notebook",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":831,
        "Challenge_word_count":56,
        "Platform":"Stack Overflow",
        "Poster_created_time":1584891066787,
        "Poster_location":null,
        "Poster_reputation_count":99.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>It depends on how large your graph is as to how well this will perform but you can get a sense of the type of nodes and edges you have using something like the example below. From the tags you used I assume you are using Gremlin:<\/p>\n\n<pre><code>g.V().groupCount().by(label)\ng.E().groupCount().by(label)\n<\/code><\/pre>\n\n<p>If you have a very large graph try putting something like <code>limit(100000)<\/code> before the <code>groupCount<\/code> step.<\/p>\n\n<p>If you are using a programming language like Python (with gremlin python installed) then you will need to add a <code>next()<\/code> terminal step to the queries as in:<\/p>\n\n<pre><code>g.V().groupCount().by(label).next()\ng.E().groupCount().by(label).next()\n<\/code><\/pre>\n\n<p>Having found the labels and distribution of the labels you could use one of them to explore some properties. Let's imagine there is a label called \"person\".<\/p>\n\n<pre><code>g.V().hasLabel('person').limit(10).valueMap().toList()\n<\/code><\/pre>\n\n<p>Remember with Gremlin property graphs vertices with the same label may not necessarily have all the same properties so it's good to look at more than one vertex to get a sense for that as well.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":14.81,
        "Solution_score_count":4.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":162.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":30.0886975,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Im trying to learn Azure, with little luck (yet). All the tutorials show using PipelineData just as a file, when configured in &quot;upload&quot; mode. However, im getting &quot;FileNotFoundError: [Errno 2] No such file or directory: ''&quot; error. I would love to ask a more specific question, but i just can't see what im doing wrong.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore,Dataset,Environment\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.pipeline.steps import PythonScriptStep\nfrom azureml.pipeline.core import Pipeline, PipelineData\nimport os\n\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n\ncompute_name = &quot;cpucluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=compute_name)\naml_run_config = RunConfiguration()\naml_run_config.target = compute_target\naml_run_config.environment.python.user_managed_dependencies = False\naml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['pandas','scikit-learn'], \n    pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]'], \n    pin_sdk_version=False)\n\noutput1 = PipelineData(&quot;processed_data1&quot;,datastore=datastore, output_mode=&quot;upload&quot;)\nprep_step = PythonScriptStep(\n    name=&quot;dataprep&quot;,\n    script_name=&quot;dataprep.py&quot;,\n    source_directory=os.path.join(os.getcwd(),'dataprep'),\n    arguments=[&quot;--output&quot;, output1],\n    outputs = [output1],\n    compute_target=compute_target,\n    runconfig=aml_run_config,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>In the dataprep.py i hve the following:<\/p>\n<pre><code>import numpy, argparse, pandas\nfrom azureml.core import Run\nrun = Run.get_context()\nparser = argparse.ArgumentParser()\nparser.add_argument('--output', dest='output', required=True)\nargs = parser.parse_args()\ndf = pandas.DataFrame(numpy.random.rand(100,3))\ndf.iloc[:, 2] = df.iloc[:,0] + df.iloc[:,1]\nprint(df.iloc[:5,:])\ndf.to_csv(args.output)\n\n<\/code><\/pre>\n<p>So, yeah. pd is supposed to write to the output, but my compute cluster says the following:<\/p>\n<pre><code>&quot;User program failed with FileNotFoundError: [Errno 2] No such file or directory: ''\\&quot;.\n<\/code><\/pre>\n<p>When i dont include the to_csv() function, the cluster does not complain<\/p>",
        "Challenge_closed_time":1626667519372,
        "Challenge_comment_count":2,
        "Challenge_created_time":1626541888290,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1626559748576,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68422680",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":15.6,
        "Challenge_reading_time":32.96,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":34.8975227778,
        "Challenge_title":"how to write to Azure PipelineData properly?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":404,
        "Challenge_word_count":207,
        "Platform":"Stack Overflow",
        "Poster_created_time":1588424911652,
        "Poster_location":null,
        "Poster_reputation_count":59.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Here is an <a href=\"https:\/\/github.com\/james-tn\/highperformance_python_in_azure\/blob\/master\/parallel_python_processing\/pipeline_definition.ipynb\" rel=\"nofollow noreferrer\">example<\/a> for PRS.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipelinedata?view=azure-ml-py\" rel=\"nofollow noreferrer\">PipelineData<\/a> was intended to represent &quot;transient&quot; data from one step to the next one, while OutputDatasetConfig was intended for capturing the final state of a dataset (and hence why you see features like lineage, ADLS support, etc). PipelineData always outputs data in a folder structure like {run_id}{output_name}. OutputDatasetConfig allows to decouple the data from the run and hence it allows you to control where to land the data (although by default it will produce similar folder structure). The OutputDatasetConfig allows even to register the output as a Dataset, where getting rid of such folder structure makes sense. From the docs itself: &quot;Represent how to copy the output of a run and be promoted as a FileDataset. The OutputFileDatasetConfig allows you to specify how you want a particular local path on the compute target to be uploaded to the specified destination&quot;.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-batch-scoring-classification#create-dataset-objects\" rel=\"nofollow noreferrer\">OutFileDatasetConfig<\/a> is a control plane concept to pass data between pipeline steps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1626668067887,
        "Solution_link_count":3.0,
        "Solution_readability":14.5,
        "Solution_reading_time":19.88,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":167.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":498.5483013889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Azure ML provides client libraries (e.g. azureml for Python) for dataset management and model deploying. From what I understand, the custom algorithm would be serialized as a Pickle file, but I'm not sure what happens after that. If I have a custom model with a deep NN architecture and set up a web service for training and another for scoring, do I still need the machine that the model was developed on for the web services to run? I found this on the azureml documentation that was helpful:<\/p>\n<blockquote>\n<p>If a function has no source file associated with it (for example, you're developing inside of a REPL environment) then the functions byte code is serialized. If the function refers to any global variables those will also be serialized using Pickle. In this mode all of the state which you're referring to needs to be already defined (e.g. your published function should come after any other functions you are calling).<\/p>\n<p>If a function is saved on disk then the entire module the function is defined in will be serialized and re-executed on the server to get the function back. In this mode the entire contents of the file is serialized and the order of the function definitions don't matter.<\/p>\n<\/blockquote>\n<p>What if the function uses a library like TensorFlow or Keras? Can someone explain what happens after the Pickle model is created?<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1532614432912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530819659027,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51198775",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":17.6,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":498.5483013889,
        "Challenge_title":"How does Azure web service deployment work locally?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":126,
        "Challenge_word_count":239,
        "Platform":"Stack Overflow",
        "Poster_created_time":1338127253383,
        "Poster_location":"Minneapolis, MN, United States",
        "Poster_reputation_count":115.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>You need to take the model.pkl file, zip it, and upload it into Azure Machine Learning Studio as a new dataset. Then add the python module and connect it to your newly generated zip.<\/p>\n\n<p>You can now use it inside the AML Studio experiment. To use the model add the following code in your python module:<\/p>\n\n<pre><code>import pandas as pd\nimport sys\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    sys.path.insert(0,\".\\Script Bundle\")\n    model = pickle.load(open(\".\\Script Bundle\\model.pkl\", 'rb'))\n    pred = model.predict(dataframe1)\n    return pd.DataFrame([pred[0]]),\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/blogs.technet.microsoft.com\/uktechnet\/2018\/04\/25\/deploying-externally-generated-pythonr-models-as-web-services-using-azure-machine-learning-studio\/\" rel=\"nofollow noreferrer\">You may find this post useful<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.5,
        "Solution_reading_time":10.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":89.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1577353307072,
        "Answerer_location":null,
        "Answerer_reputation_count":491.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":1.6874425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I\u2019m having some issues trying to access a FileDataset created from two http URIs in an Azure ML Pipeline PythonScriptStep.<\/p>\n<p>In the step, I\u2019m only getting a single file named <code>['https%3A\u2019]<\/code> when doing an <code>os.listdir()<\/code> on my mount point. I would have expected two files, with their actual names instead. This happens both when sending the dataset <code>as_upload<\/code> and <code>as_mount<\/code>. Even happens when I send the dataset reference to the pipeline step and mount it directly from the step.<\/p>\n<p>The dataset is registered in a notebook, the same notebook that creates and invokes the pipeline, as seen below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>tempFileData = Dataset.File.from_files(\n        ['https:\/\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg',\n        'https:\/\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg'])\ntempFileData.register(ws, name='FileData', create_new_version=True)\n\n#...\n\nread_datasets_step = PythonScriptStep(\n    name='The Dataset Reader',\n    script_name='read-datasets.py',\n    inputs=[fileData.as_named_input('Files'), fileData.as_named_input('Files_mount').as_mount(), fileData.as_named_input('Files_download').as_download()],\n    compute_target=compute_target,\n    source_directory='.\/dataset-reader',\n    allow_reuse=False,\n)\n\n<\/code><\/pre>\n<p>The <code>FileDataset<\/code> seems to be registered properly, if I examine it within the notebook I get the following result:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;source&quot;: [\n    &quot;https:\/\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg&quot;,\n    &quot;https:\/\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg&quot;\n  ],\n  &quot;definition&quot;: [\n    &quot;GetFiles&quot;\n  ],\n  &quot;registration&quot;: {\n    &quot;id&quot;: &quot;...&quot;,\n    &quot;name&quot;: &quot;FileData&quot;,\n    &quot;version&quot;: 4,\n    &quot;workspace&quot;: &quot;Workspace.create(...)&quot;\n  }\n}\n<\/code><\/pre>\n<p>For reference, the machine running the notebook is using AML SDK v1.24, whereas the node running the pipeline steps is running v1.25.<\/p>\n<p>Has anybody encountered anything like this? Is there a way to make it work?<\/p>\n<p>Note that I'm specifically looking at file datasets created from web uris, and not necessarily interested in getting a <code>FileDataset<\/code> to work with blob storage or similar.<\/p>",
        "Challenge_closed_time":1618855169223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618832324140,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1618849094430,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67161293",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":32.29,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":6.3458563889,
        "Challenge_title":"Issues accessing a FileDataset created from HTTP URIs in a PythonScriptStep",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":91,
        "Challenge_word_count":230,
        "Platform":"Stack Overflow",
        "Poster_created_time":1250158552416,
        "Poster_location":"Romania",
        "Poster_reputation_count":7916.0,
        "Poster_view_count":801.0,
        "Solution_body":"<p>The files should've been mounted at path &quot;https%3A\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg&quot; and &quot;https%3A\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg&quot;.<\/p>\n<p>We retain the directory structure following the url structure to avoid potential conflicts.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.3,
        "Solution_reading_time":4.39,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":2.79163,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am following a guide to get a Vertex AI pipeline working:<\/p>\n<p><a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5\" rel=\"nofollow noreferrer\">https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5<\/a><\/p>\n<p>I have implemented the following custom component:<\/p>\n<pre><code>from google.cloud import aiplatform as aip\nfrom google.oauth2 import service_account\n\nproject = &quot;project-id&quot;\nregion = &quot;us-central1&quot;\ndisplay_name = &quot;lookalike_model_pipeline_1646929843&quot;\n\nmodel_name = f&quot;projects\/{project}\/locations\/{region}\/models\/{display_name}&quot;\napi_endpoint = &quot;us-central1-aiplatform.googleapis.com&quot; #europe-west2\nmodel_resource_path = model_name\nclient_options = {&quot;api_endpoint&quot;: api_endpoint}\n\n# Initialize client that will be used to create and send requests.\nclient = aip.gapic.ModelServiceClient(credentials=service_account.Credentials.from_service_account_file('..\\\\service_accounts\\\\aiplatform_sa.json'), \nclient_options=client_options)\n#get model evaluation\nresponse = client.list_model_evaluations(parent=model_name)\n<\/code><\/pre>\n<p>And I get following error:<\/p>\n<pre><code>(&lt;class 'google.api_core.exceptions.PermissionDenied'&gt;, PermissionDenied(&quot;Permission 'aiplatform.modelEvaluations.list' denied on resource '\/\/aiplatform.googleapis.com\/projects\/project-id\/locations\/us-central1\/models\/lookalike_model_pipeline_1646929843' (or it may not exist).&quot;), &lt;traceback object at 0x000002414D06B9C0&gt;)\n<\/code><\/pre>\n<p>The model definitely exists and has finished training. I have given myself admin rights in the aiplatform service account. In the guide, they do not use a service account, but uses only client_options instead. The client_option has the wrong type since it is a dict(str, str) when it should be: Optional['ClientOptions']. But this doesn't cause an error.<\/p>\n<p>My main question is: how do I get around this permission issue?<\/p>\n<p>My subquestions are:<\/p>\n<ol>\n<li>How can I use my model_name variable in a URL to get to the model?<\/li>\n<li>How can I create an Optional['ClientOptions'] object to pass as client_option<\/li>\n<li>Is there another way I can list_model_evaluations from a model that is in VertexAI, trained using automl?<\/li>\n<\/ol>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1646971559036,
        "Challenge_comment_count":7,
        "Challenge_created_time":1646943487337,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1646987974512,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71430286",
        "Challenge_link_count":2,
        "Challenge_participation_count":9,
        "Challenge_readability":16.7,
        "Challenge_reading_time":31.25,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":7.7976941667,
        "Challenge_title":"Permission Denied using Google AiPlatform ModelServiceClient",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":936,
        "Challenge_word_count":215,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562706291280,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>I tried using your code and it did not also work for me and got a different error. As @DazWilkin mentioned it is recommended to use the Cloud Client.<\/p>\n<p>I used <code>aiplatform_v1<\/code> and it worked fine. One thing I noticed is that you should always define a value for <code>client_options<\/code> so it will point to the correct endpoint. Checking the code for <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/google\/cloud\/aiplatform_v1\/services\/model_service\/client.py#L122\" rel=\"nofollow noreferrer\">ModelServiceClient<\/a>, if I'm not mistaken the endpoint defaults to <strong>&quot;aiplatform.googleapis.com&quot;<\/strong> which don't have a location prepended. AFAIK the endpoint should prepend a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/locations\" rel=\"nofollow noreferrer\">location<\/a>.<\/p>\n<p>See code below. I used AutoML models and it returns their model evaluations.<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\nfrom typing import Optional\n\ndef get_model_eval(\n        project_id: str,\n        model_id: str,\n        client_options: dict,\n        location: str = 'us-central1',\n        ):\n\n    client_model = aiplatform.services.model_service.ModelServiceClient(client_options=client_options)\n\n    model_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\n    list_eval_request = aiplatform.types.ListModelEvaluationsRequest(parent=model_name)\n    list_eval = client_model.list_model_evaluations(request=list_eval_request)\n    print(list_eval)\n\n\n\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint} # api_endpoint is required for client_options\nproject_id = 'project-id'\nlocation = 'us-central1'\nmodel_id = '99999999999' # aiplatform_v1 uses the model_id\n\nget_model_eval(\n        client_options = client_options,\n        project_id = project_id,\n        location = location,\n        model_id = model_id,\n        )\n<\/code><\/pre>\n<p>This is an output snippet from my AutoML Text Classification:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/RXrxh.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RXrxh.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1646998024380,
        "Solution_link_count":4.0,
        "Solution_readability":16.5,
        "Solution_reading_time":28.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":184.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1604093818187,
        "Answerer_location":"Krakow, Poland",
        "Answerer_reputation_count":1200.0,
        "Answerer_view_count":263.0,
        "Challenge_adjusted_solved_time":4.8079869444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm struggling to correctly set Vertex AI pipeline which does the following:<\/p>\n<ol>\n<li>read data from API and store to GCS and as as input for batch prediction.<\/li>\n<li>get an existing model (Video classification on Vertex AI)<\/li>\n<li>create Batch prediction job with input from point 1.<br \/>\nAs it will be seen, I don't have much experience with Vertex Pipelines\/Kubeflow thus I'm asking for help\/advice, hope it's just some beginner mistake.\nthis is the gist of the code I'm using as pipeline<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import dsl\n\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Output,\n    Artifact,\n    Model,\n)\n\nPROJECT_ID = 'my-gcp-project'\nBUCKET_NAME = &quot;mybucket&quot;\nPIPELINE_ROOT = &quot;{}\/pipeline_root&quot;.format(BUCKET_NAME)\n\n\n@component\ndef get_input_data() -&gt; str:\n    # getting data from API, save to Cloud Storage\n    # return GS URI\n    gcs_batch_input_path = 'gs:\/\/somebucket\/file'\n    return gcs_batch_input_path\n\n\n@component(\n    base_image=&quot;python:3.9&quot;,\n    packages_to_install=['google-cloud-aiplatform==1.8.0']\n)\ndef load_ml_model(project_id: str, model: Output[Artifact]):\n    &quot;&quot;&quot;Load existing Vertex model&quot;&quot;&quot;\n    import google.cloud.aiplatform as aip\n\n    model_id = '1234'\n    model = aip.Model(model_name=model_id, project=project_id, location='us-central1')\n\n\n\n@dsl.pipeline(\n    name=&quot;batch-pipeline&quot;, pipeline_root=PIPELINE_ROOT,\n)\ndef pipeline(gcp_project: str):\n    input_data = get_input_data()\n    ml_model = load_ml_model(gcp_project)\n\n    gcc_aip.ModelBatchPredictOp(\n        project=PROJECT_ID,\n        job_display_name=f'test-prediction',\n        model=ml_model.output,\n        gcs_source_uris=[input_data.output],  # this doesn't work\n        # gcs_source_uris=['gs:\/\/mybucket\/output\/'],  # hardcoded gs uri works\n        gcs_destination_output_uri_prefix=f'gs:\/\/{PIPELINE_ROOT}\/prediction_output\/'\n    )\n\n\nif __name__ == '__main__':\n    from kfp.v2 import compiler\n    import google.cloud.aiplatform as aip\n    pipeline_export_filepath = 'test-pipeline.json'\n    compiler.Compiler().compile(pipeline_func=pipeline,\n                                package_path=pipeline_export_filepath)\n    # pipeline_params = {\n    #     'gcp_project': PROJECT_ID,\n    # }\n    # job = aip.PipelineJob(\n    #     display_name='test-pipeline',\n    #     template_path=pipeline_export_filepath,\n    #     pipeline_root=f'gs:\/\/{PIPELINE_ROOT}',\n    #     project=PROJECT_ID,\n    #     parameter_values=pipeline_params,\n    # )\n\n    # job.run()\n<\/code><\/pre>\n<p>When running the pipeline it throws this exception when running Batch prediction:<br \/>\n<code>details = &quot;List of found errors: 1.Field: batch_prediction_job.model; Message: Invalid Model resource name. <\/code>\nso I'm not sure what could be wrong. I tried to load model in the notebook (outside of component) and it correctly returns.<\/p>\n<p>Second issue I'm having is referencing GCS URI as output from component to batch job input.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>   input_data = get_input_data2()\n   gcc_aip.ModelBatchPredictOp(\n        project=PROJECT_ID,\n        job_display_name=f'test-prediction',\n        model=ml_model.output,\n        gcs_source_uris=[input_data.output],  # this doesn't work\n        # gcs_source_uris=['gs:\/\/mybucket\/output\/'],  # hardcoded gs uri works\n        gcs_destination_output_uri_prefix=f'gs:\/\/{PIPELINE_ROOT}\/prediction_output\/'\n    )\n<\/code><\/pre>\n<p>During compilation, I get following exception <code>TypeError: Object of type PipelineParam is not JSON serializable<\/code>, though I think this could be issue of ModelBatchPredictOp component.<\/p>\n<p>Again any help\/advice appreciated, I'm dealing with this from yesterday, so maybe I missed something obvious.<\/p>\n<p>libraries I'm using:<\/p>\n<pre><code>google-cloud-aiplatform==1.8.0  \ngoogle-cloud-pipeline-components==0.2.0  \nkfp==1.8.10  \nkfp-pipeline-spec==0.1.13  \nkfp-server-api==1.7.1\n<\/code><\/pre>\n<p><strong>UPDATE<\/strong>\nAfter comments, some research and tuning, for referencing model this works:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component\ndef load_ml_model(project_id: str, model: Output[Artifact]):\n    region = 'us-central1'\n    model_id = '1234'\n    model_uid = f'projects\/{project_id}\/locations\/{region}\/models\/{model_id}'\n    model.uri = model_uid\n    model.metadata['resourceName'] = model_uid\n<\/code><\/pre>\n<p>and then I can use it as intended:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>batch_predict_op = gcc_aip.ModelBatchPredictOp(\n        project=gcp_project,\n        job_display_name=f'batch-prediction-test',\n        model=ml_model.outputs['model'],\n        gcs_source_uris=[input_batch_gcs_path],\ngcs_destination_output_uri_prefix=f'gs:\/\/{BUCKET_NAME}\/prediction_output\/test'\n    )\n<\/code><\/pre>\n<p><strong>UPDATE 2<\/strong>\nregarding GCS path, a workaround is to define path outside of the component and pass it as an input parameter, for example (abbreviated):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@dsl.pipeline(\n    name=&quot;my-pipeline&quot;,\n    pipeline_root=PIPELINE_ROOT,\n)\ndef pipeline(\n        gcp_project: str,\n        region: str,\n        bucket: str\n):\n    ts = datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)\n    \n    gcs_prediction_input_path = f'gs:\/\/{BUCKET_NAME}\/prediction_input\/video_batch_prediction_input_{ts}.jsonl'\n    batch_input_data_op = get_input_data(gcs_prediction_input_path)  # this loads input data to GCS path\n\n    batch_predict_op = gcc_aip.ModelBatchPredictOp(\n        project=gcp_project,\n        model=training_job_run_op.outputs[&quot;model&quot;],\n        job_display_name='batch-prediction',\n        # gcs_source_uris=[batch_input_data_op.output],\n        gcs_source_uris=[gcs_prediction_input_path],\n        gcs_destination_output_uri_prefix=f'gs:\/\/{BUCKET_NAME}\/prediction_output\/',\n    ).after(batch_input_data_op)  # we need to add 'after' so it runs after input data is prepared since get_input_data doesn't returns anything\n\n<\/code><\/pre>\n<p>still not sure, why it doesn't work\/compile when I return GCS path from <code>get_input_data<\/code> component<\/p>",
        "Challenge_closed_time":1640097300176,
        "Challenge_comment_count":7,
        "Challenge_created_time":1639525350357,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1640079991423,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70356856",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":17.4,
        "Challenge_reading_time":78.55,
        "Challenge_score_count":5,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":158.8749497222,
        "Challenge_title":"Vertex AI Model Batch prediction, issue with referencing existing model and input file on Cloud Storage",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1202,
        "Challenge_word_count":495,
        "Platform":"Stack Overflow",
        "Poster_created_time":1372196724860,
        "Poster_location":"Prague, Czech Republic",
        "Poster_reputation_count":276.0,
        "Poster_view_count":47.0,
        "Solution_body":"<p>I'm glad you solved most of your main issues and found a workaround for model declaration.<\/p>\n<p>For your <code>input.output<\/code> observation on <code>gcs_source_uris<\/code>, the reason behind it is because the way the function\/class returns the value. If you dig inside the class\/methods of <code>google_cloud_pipeline_components<\/code>  you will find that it implements a structure that will allow you to use <code>.outputs<\/code> from the returned value of the function called.<\/p>\n<p>If you go to the implementation of one of the components of the pipeline you will find that it returns an output array from <code>convert_method_to_component<\/code> function. So, in order to have that implemented in your custom class\/function your function should return a value which can be called as an attribute. Below is a basic implementation of it.<\/p>\n<pre><code>class CustomClass():\n     def __init__(self):\n       self.return_val = {'path':'custompath','desc':'a desc'}\n      \n     @property\n     def output(self):\n       return self.return_val \n\nhello = CustomClass()\nprint(hello.output['path'])\n<\/code><\/pre>\n<p>If you want to dig more about it you can go to the following pages:<\/p>\n<ul>\n<li><p><a href=\"https:\/\/github.com\/bharathdsce\/kubeflow\/blob\/fcd627714664956b2c280b0109b64633bc99fa05\/components\/google-cloud\/google_cloud_pipeline_components\/aiplatform\/utils.py#L383\" rel=\"nofollow noreferrer\">convert_method_to_component<\/a>, which is the implementation of <code>convert_method_to_component<\/code><\/p>\n<\/li>\n<li><p><a href=\"https:\/\/www.programiz.com\/python-programming\/property\" rel=\"nofollow noreferrer\">Properties<\/a>, basics of property in python.<\/p>\n<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.6,
        "Solution_reading_time":21.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":177.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1430233500800,
        "Answerer_location":null,
        "Answerer_reputation_count":212.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":21511.1452602778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I suspect this has to more to do with IAM roles than Sagemaker.<\/p>\n\n<p>I'm following the example <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"noreferrer\">here<\/a><\/p>\n\n<p>Specifically, when it makes this call<\/p>\n\n<pre><code>tf_estimator.fit('s3:\/\/bucket\/path\/to\/training\/data')\n<\/code><\/pre>\n\n<p>I get this error<\/p>\n\n<pre><code>ClientError: An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:sts::013772784144:assumed-role\/AmazonSageMaker-ExecutionRole-20181022T195630\/SageMaker is not authorized to perform: iam:GetRole on resource: role SageMakerRole\n<\/code><\/pre>\n\n<p>My notebook instance has an IAM role attached to it.\nThat role has the <code>AmazonSageMakerFullAccess<\/code> policy. It also has a custom policy that looks like this<\/p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:GetObject\",\n            \"s3:PutObject\",\n            \"s3:DeleteObject\",\n            \"s3:ListBucket\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::*\"\n        ]\n    }\n]\n<\/code><\/pre>\n\n<p>}<\/p>\n\n<p>My input files and .py script is in an s3 bucket with the phrase <code>sagemaker<\/code> in it.<\/p>\n\n<p>What else am I missing?<\/p>",
        "Challenge_closed_time":1543010904776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1542853625410,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53423061",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.6,
        "Challenge_reading_time":16.26,
        "Challenge_score_count":6,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":43.6887127778,
        "Challenge_title":"How do I make this IAM role error in aws sagemaker go away?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":8160,
        "Challenge_word_count":130,
        "Platform":"Stack Overflow",
        "Poster_created_time":1319234288808,
        "Poster_location":null,
        "Poster_reputation_count":4966.0,
        "Poster_view_count":304.0,
        "Solution_body":"<p>If you're running the example code on a SageMaker notebook instance, you can use the execution_role which has the <code>AmazonSageMakerFullAccess<\/code> attached.<\/p>\n<pre><code>from sagemaker import get_execution_role\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n<\/code><\/pre>\n<p>And you can pass this role when initializing <code>tf_estimator<\/code>.\nYou can check out the example <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html\" rel=\"nofollow noreferrer\">here<\/a> for using <code>execution_role<\/code> with S3 on notebook instance.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1620293748347,
        "Solution_link_count":1.0,
        "Solution_readability":18.4,
        "Solution_reading_time":8.09,
        "Solution_score_count":8.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":57.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1456411465888,
        "Answerer_location":null,
        "Answerer_reputation_count":105.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":22032.4912397222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have built my own Docker container that provides inference code to be deployed as endpoint on Amazon Sagemaker. However, this container needs to have access to some files from s3. The used IAM role has access to all s3 buckets that I am trying to reach.<\/p>\n\n<p>Code to download files using a boto3 client:<\/p>\n\n<pre><code>import boto3\n\nmodel_bucket = 'my-bucket'\n\ndef download_file_from_s3(s3_path, local_path):\n    client = boto3.client('s3')\n    client.download_file(model_bucket, s3_path, local_path)\n<\/code><\/pre>\n\n<p>The IAM role's policies:<\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::my-bucket\/*\"\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>Starting the docker container locally allows me to download files from s3 just like expected. <\/p>\n\n<p>Deploying as an endpoint on Sagemaker, however, the request times out:<\/p>\n\n<pre><code>botocore.vendored.requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='my-bucket.s3.eu-central-1.amazonaws.com', port=443): Max retries exceeded with url: \/path\/to\/my-file (Caused by ConnectTimeoutError(&lt;botocore.awsrequest.AWSHTTPSConnection object at 0x7f66244e69b0&gt;, 'Connection to my-bucket.s3.eu-central-1.amazonaws.com timed out. (connect timeout=60)'))\n<\/code><\/pre>\n\n<p>Any help is appreciated!<\/p>",
        "Challenge_closed_time":1562142120396,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561982365937,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56835306",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.3,
        "Challenge_reading_time":18.79,
        "Challenge_score_count":4,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":44.3762386111,
        "Challenge_title":"Download file using boto3 within Docker container deployed on Sagemaker Endpoint",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1171,
        "Challenge_word_count":149,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456411465888,
        "Poster_location":null,
        "Poster_reputation_count":105.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>For anyone coming across this question, when creating a model, the 'Enable Network Isolation' property defaults to True.\nFrom AWS docs:<\/p>\n<blockquote>\n<p>If you enable network isolation, the containers are not able to make any outbound network calls, even to other AWS services such as Amazon S3. Additionally, no AWS credentials are made available to the container runtime environment.<\/p>\n<\/blockquote>\n<p>So this property needs to be set to False in order to connect to any other AWS service.<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/5qERm.jpg\" alt=\"AWS Sagemaker UI Network Isolation set to False\" \/><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1641299334400,
        "Solution_link_count":1.0,
        "Solution_readability":9.1,
        "Solution_reading_time":7.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442430064503,
        "Answerer_location":null,
        "Answerer_reputation_count":6147.0,
        "Answerer_view_count":1230.0,
        "Challenge_adjusted_solved_time":970.3788427778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I create a model in Azure ML studio. \nI deployed the web service.<\/p>\n\n<p>Now, I know how to check one record at a time, but how can I load a csv file and made the algorithm go through all records ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/1tHuM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1tHuM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If I click on Batch Execution - it will ask me to create an account for Azure storage. <\/p>\n\n<p>Is any way to execute multiple records from csv file without creating any other accounts?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/90zP7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/90zP7.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1518941429767,
        "Challenge_comment_count":0,
        "Challenge_created_time":1515448065933,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48158545",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":10.25,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":970.3788427778,
        "Challenge_title":"How to execute multiple rows in web service Azure Machine Learning Studio",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":204,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457596845392,
        "Poster_location":"San Diego, CA, United States",
        "Poster_reputation_count":4046.0,
        "Poster_view_count":825.0,
        "Solution_body":"<p>Yes, there is a way and it is simple. What you need is an excel add-in. You need not create any other account.<\/p>\n\n<p>You can either read <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/excel-add-in-for-web-services\" rel=\"nofollow noreferrer\">Excel Add-in for Azure Machine Learning web services doc<\/a> or you can watch <a href=\"https:\/\/www.youtube.com\/watch?v=ju1CzDjiOMQ\" rel=\"nofollow noreferrer\">Azure ML Excel Add-in video<\/a>. <\/p>\n\n<p>If you search for <a href=\"https:\/\/www.google.co.in\/search?q=excel%20add%20in%20for%20azure%20ml&amp;client=firefox-b-ab&amp;dcr=0&amp;source=lnms&amp;tbm=vid&amp;sa=X&amp;ved=0ahUKEwinqP3a_67ZAhXBr48KHdiYAXUQ_AUICigB&amp;biw=1280&amp;bih=616\" rel=\"nofollow noreferrer\">videos on excel add in for azure ml<\/a>, you get other useful videos too. <\/p>\n\n<p>I hope this is the solution you are looking for.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":11.6,
        "Solution_reading_time":11.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":84.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1280505139752,
        "Answerer_location":"Bangalore, India",
        "Answerer_reputation_count":4265.0,
        "Answerer_view_count":403.0,
        "Challenge_adjusted_solved_time":24.7636894444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was trying Azure Machine Learning Services following this tutorial (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/quickstart-installation\" rel=\"nofollow noreferrer\">Link<\/a>). After successfully creating the Azure Machine Learning services accounts, I successfully installed the Workbench on my Windows 10 Laptop (Behind Proxy; Proxy has been configured at the WorkBench). Next, I was trying to create project following this section (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/quickstart-installation#create-a-project-in-workbench\" rel=\"nofollow noreferrer\">Link<\/a>). Once I click on the Create button, it goes to \"Creating\" state and stays there for ever. The errors displayed at Errors.log is the following. Any suggestion will be appreciated. <\/p>\n\n<pre><code>[2018-07-09 09:47:08.437] [ERROR] HttpService - {\"event\":\"HttpService\",\"task\":\"Failed\",\"data\":{\"url\":\"http:\/\/localhost:54240\/projects\/v1.0\/create\/template\",\"status\":500,\"statusText\":\"INKApi Error\",\"jsonError\":null,\"requestId\":null,\"sessionType\":\"Workbench\"},\"sid\":\"365395c0-832b-11e8-b4ce-e5d7046c6143\"}\n\n[2018-07-09 09:47:08.960] [ERROR] CreateProjectForm - {\"event\":\"CreateProject\",\"task\":\"Error\",\"data\":{\"_body\":null,\"status\":500,\"ok\":false,\"statusText\":\"INKApi Error\",\"headers\":{\"Date\":[\"Mon\",\" 09 Jul 2018 04:17:06 GMT\"],\"Via\":[\"1.1 localhost.localdomain\"],\"Proxy-Connection\":[\"close\"],\"Content-Length\":[\"0\"],\"Content-Type\":[\"text\/html\"]},\"type\":2,\"url\":\"http:\/\/localhost:54240\/projects\/v1.0\/create\/template\"},\"sid\":\"365395c0-832b-11e8-b4ce-e5d7046c6143\"}\n\n[2018-07-09 09:47:08.963] [FATAL] ExceptionLogger - {\"event\":\"exception\",\"task\":\"\",\"data\":{\"message\":\"Cannot read property 'error' of null\",\"name\":\"TypeError\",\"stack\":\"TypeError: Cannot read property 'error' of null\\n    at SafeSubscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:61476:58)\\n    at SafeSubscriber.__tryOrUnsub (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212279:20)\\n    at SafeSubscriber.error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212241:30)\\n    at Subscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212172:30)\\n    at Subscriber.error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212146:22)\\n    at MergeMapSubscriber.OuterSubscriber.notifyError (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:210968:30)\\n    at InnerSubscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:211072:25)\\n    at InnerSubscriber.Subscriber.error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212146:22)\\n    at DeferSubscriber.OuterSubscriber.notifyError (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:210968:30)\\n    at InnerSubscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:211072:25)\"},\"sid\":\"365395c0-832b-11e8-b4ce-e5d7046c6143\"}\n<\/code><\/pre>",
        "Challenge_closed_time":1531201276972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531112127690,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51238413",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":29.4,
        "Challenge_reading_time":46.14,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":24.7636894444,
        "Challenge_title":"Azure Machine Learning Workbench hangs while creating new project",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":67,
        "Challenge_word_count":160,
        "Platform":"Stack Overflow",
        "Poster_created_time":1280505139752,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":4265.0,
        "Poster_view_count":403.0,
        "Solution_body":"<p>It was happening because of the Proxy (although I have configured the Proxy on the Workbench). When I am connected to internet directly, everything works fine (Able to create project, train, compare models etc). However the Workbench should return meaningful error instead of hanging or simply waiting while creating the project.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":4.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1479159384132,
        "Answerer_location":"Illinois, United States",
        "Answerer_reputation_count":513.0,
        "Answerer_view_count":113.0,
        "Challenge_adjusted_solved_time":42.2916322222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I want to run a pipeline for different files, but some of them don't need all of the defined nodes. How can I pass them?<\/p>",
        "Challenge_closed_time":1573135218943,
        "Challenge_comment_count":2,
        "Challenge_created_time":1572974635783,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1572982969067,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58716474",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":3.0,
        "Challenge_reading_time":2.06,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":44.6064333334,
        "Challenge_title":"How to run a pipeline except for a few nodes?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":859,
        "Challenge_word_count":34,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572973807452,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>To filter out a few lines of a pipeline you can simply filter the pipeline list from inside of python, my favorite way is to use a list comprehension.<\/p>\n\n<p><strong>by name<\/strong><\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>nodes_to_run = [node for node in pipeline.nodes if 'dont_run_me' not in node.name]\nrun(nodes_to_run, io)\n<\/code><\/pre>\n\n<p><strong>by tag<\/strong><\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>nodes_to_run = [node for node in pipeline.nodes if 'dont_run_tag' not in node.tags]\nrun(nodes_to_run, io)\n<\/code><\/pre>\n\n<p>It's possible to filter by any attribute tied to the pipeline node, (name, inputs, outputs, short_name, tags)<\/p>\n\n<p>If you need to run your pipeline this way in production or from the command line, you can either tag your pipeline to run with tags, or add a custom <code>click.option<\/code> to your <code>run<\/code> function inside of <code>kedro_cli.py<\/code> then run this filter when the flag is <code>True<\/code>.<\/p>\n\n<p><strong>Note<\/strong>\nThis assumes that you have your pipeline loaded into memory as <code>pipeline<\/code> and catalog loaded in as <code>io<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":14.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":148.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1464811778510,
        "Answerer_location":null,
        "Answerer_reputation_count":196.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":11.9435663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have successfully initialized a ModelQualityMonitor object.\nThen I created a monitoring schedule using the CreateMonitoringSchedule API! In the background sagemaker runs two processing jobs which merges the ground truth data with the collected endpoint data and then analyzes and creates the predefined regression metrics:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html<\/a><\/p>\n<p>Unfortunately, I am missing the MAPE (Mean Absolute Percentage Error) in the metrics, and would like to create this with in the future (also in CloudWatch).<\/p>\n<p>Sagemaker provides the following functionalities:<\/p>\n<ul>\n<li>Preprocessing and Postprocessing:\nIn addition to using the built-in mechanisms, you can extend the code with the preprocessing and postprocessing scripts.<\/li>\n<li>Bring Your Own Containers:\nAmazon SageMaker Model Monitor provides a prebuilt container with ability to analyze the data captured from endpoints for tabular datasets. If you would like to bring your own container, Model Monitor provides extension points which you can leverage.<\/li>\n<li>CloudWatch Metrics for Bring Your Own Containers<\/li>\n<\/ul>\n<p>Those points are documented on this site: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-custom-monitoring-schedules.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-custom-monitoring-schedules.html<\/a><\/p>\n<p>How exactly can I achieve my target of including MAPE with the above points?<\/p>\n<p>Here is a code snippet of my current implementation:<\/p>\n<pre><code>from sagemaker.model_monitor.model_monitoring import ModelQualityMonitor\nfrom sagemaker.model_monitor import EndpointInput\nfrom sagemaker.model_monitor.dataset_format import DatasetFormat\n\n# Create the model quality monitoring object\nMQM = ModelQualityMonitor(\n    role=role,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    volume_size_in_gb=20,\n    max_runtime_in_seconds=1800,\n    sagemaker_session=sagemaker_session,\n)\n\n# suggest a baseline\njob = MQM.suggest_baseline(\n    job_name=baseline_job_name,\n    baseline_dataset=&quot;.\/baseline.csv&quot;,\n    dataset_format=DatasetFormat.csv(header=True),\n    output_s3_uri=baseline_results_uri,\n    problem_type=&quot;Regression&quot;,\n    inference_attribute=&quot;predicted_price&quot;,\n    ground_truth_attribute=&quot;price&quot;,\n)\njob.wait(logs=False)\nbaseline_job = MQM.latest_baselining_job\n\n# create a monitoring schedule\nendpointInput = EndpointInput(\n    endpoint_name=&quot;dev-TestEndpoint&quot;,\n    destination=&quot;\/opt\/ml\/processing\/input_data&quot;,\n    inference_attribute=&quot;$.data.predicted_price&quot;\n)\nMQM.create_monitoring_schedule(\n    monitor_schedule_name=&quot;DS-Schedule&quot;,\n    endpoint_input=endpointInput,\n    output_s3_uri=baseline_results_uri,\n    constraints=baseline_job.suggested_constraints(),\n    problem_type=&quot;Regression&quot;,\n    ground_truth_input=ground_truth_upload_path,\n    schedule_cron_expression=&quot;cron(0 * ? * * *)&quot;, # hourly\n    enable_cloudwatch_metrics=True\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1651171215176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651128218337,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72039147",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":21.7,
        "Challenge_reading_time":43.53,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":11.9435663889,
        "Challenge_title":"Is there a way to include custom Regression Metrics in ModelQualityMonitor in AWS sagemaker?",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":165,
        "Challenge_word_count":261,
        "Platform":"Stack Overflow",
        "Poster_created_time":1613661928947,
        "Poster_location":null,
        "Poster_reputation_count":160.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>Amazon SageMaker model monitor only supports metrics that are defined <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html\" rel=\"nofollow noreferrer\">here<\/a> out of the box.\nIf you need to include another metric such as MAPE (Mean Absolute Percentage Error) in your case, you will have to rely on BYOC approach, note that with this approach you cannot &quot;add&quot; a metric to the available list, unfortunately you will have to implement the entire suite of metrics yourself. I understand this is not ideal for customers, I'd encourage you to reach out to your AWS account manager to create a request to add MAPE (Mean Absolute Percentage Error) as a supported metric in the long run. I've made a note of it as well and will rely it back to the team.<\/p>\n<p>In the meantime, you can find examples on how to BYOC <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/detect-nlp-data-drift-using-custom-amazon-sagemaker-model-monitor\/\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>I work for AWS but my opinions are my own.<\/p>\n<p>Thanks,\nRaghu<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.2,
        "Solution_reading_time":13.93,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":150.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":123.1885911111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am really confused about organizing google Vertex Ai dataset and train the autoML model in GCP. Could any one please help me to understand?<\/p>\n<p>Let me explain scenarios in which I have confusion.<\/p>\n<p>Let\u2019s suppose if I have <em>Text entity extraction<\/em> dataset in vertex Ai \u201c<strong>contract_delivery_02<\/strong>\u201d with 25 files. I have 3 labels created (<em>DelIncoTerms, DelLocation and DelWindow<\/em>) and I have trained model. This is working great.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KvWom.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KvWom.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Now, I have 10 more files to upload, where I have introduced 2 additional labels (<em>DelPrice &amp; DelDelivery<\/em>).<\/p>\n<p>My questions<\/p>\n<ol>\n<li>Do I require to do upload all the files (25 + 10) again ?<\/li>\n<li>Do I require to retrain my whole autoML model again ? or is there any other approach for this scenario?<\/li>\n<\/ol>",
        "Challenge_closed_time":1657060259888,
        "Challenge_comment_count":2,
        "Challenge_created_time":1656616780960,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72821008",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":7.4,
        "Challenge_reading_time":12.93,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":123.1885911111,
        "Challenge_title":"Vertex AI updating dataset and train model",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":149,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462469556836,
        "Poster_location":null,
        "Poster_reputation_count":509.0,
        "Poster_view_count":65.0,
        "Solution_body":"<p>For question #1, you don't have to upload all files again. In your <strong>Dataset<\/strong>, you just have to add your <strong>2 new labels<\/strong> and then upload your additional 10 files.\n<a href=\"https:\/\/i.stack.imgur.com\/rLep2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rLep2.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Once uploaded, you may now proceed to put labels on your newly added files (in your example, total of 10 files) and then assign the new labels on <strong>ALL files<\/strong> (25 + 10). You can do this by double-clicking the newly added text from the UI and then assign necessary labels.\n<a href=\"https:\/\/i.stack.imgur.com\/jqIaj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jqIaj.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For question #2, since there are newly added labels and training texts, it is necessary for you to retrain the whole autoML for more accurate Model and better quality of results.<\/p>\n<p>You may refer to this <a href=\"https:\/\/cloud.google.com\/natural-language\/automl\/docs\/prepare#expandable-2\" rel=\"nofollow noreferrer\">Text Entity Extraction preparation of data<\/a> and <a href=\"https:\/\/cloud.google.com\/natural-language\/automl\/docs\/models\" rel=\"nofollow noreferrer\">Training Models<\/a> documentation for more details.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":12.9,
        "Solution_reading_time":17.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":155.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1506516283190,
        "Answerer_location":"Torino, TO, Italia",
        "Answerer_reputation_count":875.0,
        "Answerer_view_count":52.0,
        "Challenge_adjusted_solved_time":433.2236119444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello Stackoverflowers,<\/p>\n\n<p>I'm using azureml and I'm wondering if it is possible to log a confusion matrix of the xgboost model I'm training, together with the other metrics I'm already logging. Here's a sample of the code I'm using:<\/p>\n\n<pre><code>from azureml.core.model import Model\nfrom azureml.core import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nimport json\n\nwith open('.\/azureml.config', 'r') as f:\n    config = json.load(f)\n\nsvc_pr = ServicePrincipalAuthentication(\n   tenant_id=config['tenant_id'],\n   service_principal_id=config['svc_pr_id'],\n   service_principal_password=config['svc_pr_password'])\n\n\nws = Workspace(workspace_name=config['workspace_name'],\n                        subscription_id=config['subscription_id'],\n                        resource_group=config['resource_group'],\n                        auth=svc_pr)\n\ny_pred = model.predict(dtest)\n\nacc = metrics.accuracy_score(y_test, (y_pred&gt;.5).astype(int))\nrun.log(\"accuracy\",  acc)\nf1 = metrics.f1_score(y_test, (y_pred&gt;.5).astype(int), average='binary')\nrun.log(\"f1 score\",  f1)\n\n\ncmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\nrun.log_confusion_matrix('Confusion matrix', cmtx)\n<\/code><\/pre>\n\n<p>The above code raises this kind of error:<\/p>\n\n<pre><code>TypeError: Object of type ndarray is not JSON serializable\n<\/code><\/pre>\n\n<p>I already tried to transform the matrix in a simpler one, but another error occurred as before I logged a \"manual\" version of it (<code>cmtx = [[30000, 50],[40, 2000]]<\/code>).<\/p>\n\n<pre><code>run.log_confusion_matrix('Confusion matrix', [list([int(y) for y in x]) for x in cmtx])\n\nAzureMLException: AzureMLException:\n    Message: UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-    c5103b205379\/Confusion matrix already exists.\n    InnerException None\n    ErrorResponse \n{\n    \"error\": {\n        \"message\": \"UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-c5103b205379\/Confusion matrix already exists.\"\n    }\n}\n<\/code><\/pre>\n\n<p>This makes me think that I'm not properly handling the command <code>run.log_confusion_matrix()<\/code>. So, again, which is the best way I can log a confusion matrix to my azureml experiments?<\/p>",
        "Challenge_closed_time":1593519812260,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591960207257,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62343056",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":29.81,
        "Challenge_score_count":3,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":433.2236119444,
        "Challenge_title":"How to log a confusion matrix to azureml platform using python",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1418,
        "Challenge_word_count":216,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506516283190,
        "Poster_location":"Torino, TO, Italia",
        "Poster_reputation_count":875.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>I eventually found a solution thanks to colleague of mine. I'm hence answering myself, in order to close the question and, maybe, help somebody else.<\/p>\n<p>You can find the proper function in this link: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----<\/a>.<\/p>\n<p>Anyway, you also have to consider that, apparently, Azure doesn't work with the standard confusion matrix format returned by sklearn. It accepts indeed ONLY list of list, instead of numpy array, populated with numpy.int64 elements. So you also have to transform the matrix in a simpler format (for the sake of simplicity I used the nested list comprehension in the command below:<\/p>\n<pre><code>cmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\ncmtx = {\n\n&quot;schema_type&quot;: &quot;confusion_matrix&quot;,\n&quot;parameters&quot;: params,\n &quot;data&quot;: {&quot;class_labels&quot;: [&quot;0&quot;, &quot;1&quot;],\n          &quot;matrix&quot;: [[int(y) for y in x] for x in cmtx]}\n}\nrun.log_confusion_matrix('Confusion matrix - error rate', cmtx)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.4,
        "Solution_reading_time":17.29,
        "Solution_score_count":6.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":126.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":4.4489933333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I started a batch prediction job in AutoML (now VertexAI) for a small csv in one of my buckets, using a classification model, then I noticed the csv had an error but was unable to find a way to cancel the job using the web GUI, it just says &quot;running&quot; but I see no &quot;stop&quot; or &quot;cancel&quot; button.<\/p>\n<p>Fortunately, it was done after 20 minutes, but I need to know how to stop a job since I will require predictions for way bigger files and can't risk having to wait until the job ends by itself. It was kind of desperating being able to watch the log throwing error after error and not being able to stop the job. I tried to delete the job but it said it can't be deleted while its running.<\/p>\n<p>I found a related question, but it was not answered, the job just finished itself after a couple of days. I can't risk that.\n<a href=\"https:\/\/stackoverflow.com\/questions\/68077606\/how-do-i-force-batch-prediction-to-stop\">How do I force &quot;batch prediction&quot; to stop?<\/a><\/p>\n<p>I will greatly appreciate any help.<\/p>",
        "Challenge_closed_time":1625628028808,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625608160277,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1625612012432,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68277691",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":14.01,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":5.5190363889,
        "Challenge_title":"How do I stop a Google Cloud's AutoML (now VertexAI) batch prediction job using the web GUI?",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":619,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Poster_created_time":1431573886067,
        "Poster_location":null,
        "Poster_reputation_count":38.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>Unfortunately the cancel\/stop feature is not yet available in the Vertex AI UI. As per <a href=\"https:\/\/stackoverflow.com\/questions\/68077606\/how-do-i-force-batch-prediction-to-stop\">How do I force &quot;batch prediction&quot; to stop?<\/a>, the OP sent a feedback. You can ask if there was a public issue tracker created for this so you can monitor the progress of the feature request there.<\/p>\n<p>But there is a workaround for this, just send a request <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.batchPredictionJobs\/cancel\" rel=\"nofollow noreferrer\">projects.locations.batchPredictionJobs.cancel<\/a> via REST.<\/p>\n<p>To do this you can send a request via curl. In this example the model and endpoint are located in <code>us-central1<\/code> thus the location defined in the request.<\/p>\n<p>Just supply your <code>project-id<\/code> and the <code>batch-prediction-id<\/code> on the request. To get the <code>batch-prediction-id<\/code> you can get it via UI:<\/p>\n<p>Get <code>batch-prediction-id<\/code> via UI:<\/p>\n<ul>\n<li>Open &quot;Batch Predictions&quot; tab in the Vertex AI UI<\/li>\n<li>Click on the job you want to cancel<\/li>\n<li>Job information will be displayed and the 1st entry will contain the Job ID<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/lKJdT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lKJdT.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To cancel the job send a cancel request via curl. If requests is successful, the response body is empty.<\/p>\n<pre><code>curl -X POST -H &quot;Content-Type: application\/json&quot; \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\nhttps:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/your-project-id\/locations\/us-central1\/batchPredictionJobs\/batch-prediction-job-id:cancel\n<\/code><\/pre>\n<p>Check in Vertex AI UI if the job was canceled.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/atSqt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/atSqt.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":13.5,
        "Solution_reading_time":27.51,
        "Solution_score_count":2.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":219.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":24.7979425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When reading the examples from Microsoft on azure ML CLI v2, they use the symbols:\n&quot;|&quot;, &quot;&gt;&quot;, etc., in their yml files.<\/p>\n<p>What do they mean, and where can I find explanations of possible syntax for the Azure CLI v2 engine?<\/p>",
        "Challenge_closed_time":1649231701843,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649142429250,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71747545",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":3.6,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":24.7979425,
        "Challenge_title":"Commands in the Azure ML yml files",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":100,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Poster_created_time":1620049475608,
        "Poster_location":"Denmark",
        "Poster_reputation_count":3.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>| - This pipe symbol in YAML document is used for <em><strong>&quot;Multiple line statements&quot;<\/strong><\/em><\/p>\n<pre><code>description: |\n  # Azure Machine Learning &quot;hello world&quot; job\n\n  This is a &quot;hello world&quot; job running in the cloud via Azure Machine Learning!\n\n  ## Description\n\n  Markdown is supported in the studio for job descriptions! You can edit the description there or via CLI.\n<\/code><\/pre>\n<p>in the above example, we need to write some multiple line description. So, we need to use &quot;|&quot; symbol<\/p>\n<p>&quot;&gt;&quot; - This symbol is used to save some content directly to a specific location document.<\/p>\n<pre><code>command: echo &quot;hello world&quot; &gt; .\/outputs\/helloworld.txt\n<\/code><\/pre>\n<p>In this above command, we need to post <strong>&quot;hello world&quot;<\/strong> to <em><strong>&quot;helloworld.txt&quot;<\/strong><\/em><\/p>\n<p>Check the below link for complete documentation regarding YAML files.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-yaml-job-command\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-yaml-job-command<\/a><\/p>\n<p>All these symbols are the YAML job commands which are used to accomplish a specific task through CLI.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.6,
        "Solution_reading_time":16.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":139.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1428951492492,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":1261.0,
        "Answerer_view_count":111.0,
        "Challenge_adjusted_solved_time":20.6618036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Despite prominent how-to posts on how to add datasets to Azure Machine Learning that say Excel is supported, when I actually go to add a dataset and select a local Excel file, there's no option for \"Excel\" in the required datatype property dropdown. I'm surprised that Azure wouldn't support Excel (right?) - am I missing something?<\/p>",
        "Challenge_closed_time":1476307260447,
        "Challenge_comment_count":0,
        "Challenge_created_time":1476303145067,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40007515",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":5.11,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.1431611111,
        "Challenge_title":"Azure Machine Learning Studio: how to add a dataset from a local Excel file?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1250,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1357592818807,
        "Poster_location":"Ann Arbor, MI",
        "Poster_reputation_count":99.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>The dropdown list indicates the \"Destination\" datatype for the new DATASET file you are creating, not the source type.<\/p>\n\n<p>I just uploaded a <code>.xlsx<\/code> file successfully into a <code>.CSV<\/code> file in AML.<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1476377527560,
        "Solution_link_count":0.0,
        "Solution_readability":5.2,
        "Solution_reading_time":2.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1327302732867,
        "Answerer_location":"USA",
        "Answerer_reputation_count":19711.0,
        "Answerer_view_count":1030.0,
        "Challenge_adjusted_solved_time":0.1674261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Running SageMaker within a local Jupyter notebook (using VS Code) works without issue, except that attempting to train an XGBoost model using the AWS hosted container results in errors (container name: <code>246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1-cpu-py3<\/code>).<\/p>\n<h2>Jupyter Notebook<\/h2>\n<pre class=\"lang-py prettyprint-override\"><code>import sagemaker\n\nsession = sagemaker.LocalSession()\n\n# Load and prepare the training and validation data\n...\n\n# Upload the training and validation data to S3\ntest_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\nval_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\ntrain_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n\nregion = session.boto_region_name\ninstance_type = 'ml.m4.xlarge'\ncontainer = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1', 'py3', instance_type=instance_type)\n\nrole = 'arn:aws:iam::&lt;USER ID #&gt;:role\/service-role\/AmazonSageMaker-ExecutionRole-&lt;ROLE ID #&gt;'\n\nxgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n\nxgb_estimator.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6,\n                                  subsample=0.8, objective='reg:squarederror', early_stopping_rounds=10,\n                                  num_round=200)\n\ns3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\ns3_input_validation = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='csv')\n\nxgb_estimator.fit({'train': s3_input_train, 'validation': s3_input_validation})\n<\/code><\/pre>\n<h2>Docker Container KeyError<\/h2>\n<pre><code>algo-1-tfcvc_1  | ERROR:sagemaker-containers:Reporting training FAILURE\nalgo-1-tfcvc_1  | ERROR:sagemaker-containers:framework error: \nalgo-1-tfcvc_1  | Traceback (most recent call last):\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_containers\/_trainer.py&quot;, line 84, in train\nalgo-1-tfcvc_1  |     entrypoint()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 94, in main\nalgo-1-tfcvc_1  |     train(framework.training_env())\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 90, in train\nalgo-1-tfcvc_1  |     run_algorithm_mode()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 68, in run_algorithm_mode\nalgo-1-tfcvc_1  |     checkpoint_config=checkpoint_config\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/train.py&quot;, line 115, in sagemaker_train\nalgo-1-tfcvc_1  |     validated_data_config = channels.validate(data_config)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 106, in validate\nalgo-1-tfcvc_1  |     channel_obj.validate(value)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 52, in validate\nalgo-1-tfcvc_1  |     if (value[CONTENT_TYPE], value[TRAINING_INPUT_MODE], value[S3_DIST_TYPE]) not in self.supported:\nalgo-1-tfcvc_1  | KeyError: 'S3DistributionType'\n\n<\/code><\/pre>\n<h2>Local PC Runtime Error<\/h2>\n<pre><code>RuntimeError: Failed to run: ['docker-compose', '-f', '\/tmp\/tmp71tx0fop\/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1\n<\/code><\/pre>\n<p>If the Jupyter notebook is run using the Amazon cloud SageMaker environment (rather than on the local PC), there are no errors. Note that when running on the cloud notebook, the session is initialized as:<\/p>\n<pre><code>session = sagemaker.Session()\n<\/code><\/pre>\n<p>It appears that there is an issue with how the <code>LocalSession()<\/code> works with the hosted docker container.<\/p>",
        "Challenge_closed_time":1597366468132,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597366468133,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63405080",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":23.3,
        "Challenge_reading_time":56.88,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker in local Jupyter notebook: cannot use AWS hosted XGBoost container (\"KeyError: 'S3DistributionType'\" and \"Failed to run: ['docker-compose'\")",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1174,
        "Challenge_word_count":293,
        "Platform":"Stack Overflow",
        "Poster_created_time":1327302732867,
        "Poster_location":"USA",
        "Poster_reputation_count":19711.0,
        "Poster_view_count":1030.0,
        "Solution_body":"<p>When running SageMaker in a local Jupyter notebook, it expects the Docker container to be running on the local machine as well.<\/p>\n<p>The key to ensuring that SageMaker (running in a local notebook) uses the AWS hosted docker container, is to omit the <code>LocalSession<\/code> object when initializing the <code>Estimator<\/code>.<\/p>\n<h2>Wrong<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n<\/code><\/pre>\n<h2>Correct<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output')\n<\/code><\/pre>\n<p>\u00a0\u00a0<\/p>\n<h2>Additional info<\/h2>\n<p>The SageMaker Python SDK source code provides the following helpful hints:<\/p>\n<h1>File: <em>sagemaker\/local\/local_session.py<\/em><\/h1>\n<pre><code>class LocalSagemakerClient(object):\n    &quot;&quot;&quot;A SageMakerClient that implements the API calls locally.\n\n    Used for doing local training and hosting local endpoints. It still needs access to\n    a boto client to interact with S3 but it won't perform any SageMaker call.\n    ...\n<\/code><\/pre>\n<h1>File: <em>sagemaker\/estimator.py<\/em><\/h1>\n<pre><code>class EstimatorBase(with_metaclass(ABCMeta, object)):\n    &quot;&quot;&quot;Handle end-to-end Amazon SageMaker training and deployment tasks.\n\n    For introduction to model training and deployment, see\n    http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\n\n    Subclasses must define a way to determine what image to use for training,\n    what hyperparameters to use, and how to create an appropriate predictor instance.\n    &quot;&quot;&quot;\n\n    def __init__(self, role, train_instance_count, train_instance_type,\n                 train_volume_size=30, train_max_run=24 * 60 * 60, input_mode='File',\n                 output_path=None, output_kms_key=None, base_job_name=None, sagemaker_session=None, tags=None):\n        &quot;&quot;&quot;Initialize an ``EstimatorBase`` instance.\n\n        Args:\n            role (str): An AWS IAM role (either name or full ARN). ...\n            \n        ...\n\n            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with\n                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one\n                using the default AWS configuration chain.\n        &quot;&quot;&quot;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1597367070867,
        "Solution_link_count":1.0,
        "Solution_readability":16.7,
        "Solution_reading_time":32.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":235.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1334649856036,
        "Answerer_location":null,
        "Answerer_reputation_count":2091.0,
        "Answerer_view_count":501.0,
        "Challenge_adjusted_solved_time":315.0776647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Azure ML, I want to enter data to a model through a published Web Service. \nThe way to tell this to the Web Service, as far as I can tell, it to have an 'Enter Data' box coming into the same input as the Web service. <\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/m1x5x.png\" alt=\"enter image description here\"><\/p>\n\n<p>You can then set you data format in the 'Enter Data' properties:<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/VQJ9V.png\" alt=\"enter image description here\"><\/p>\n\n<p>I want that list to be an arbitrary-length array of samples. This works if your input is:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"samples\"\n      ],\n      \"Values\": [\n        [\n          1\n        ],\n        [\n          2\n        ],\n        [\n          3\n        ],\n        [\n          4\n        ],\n        [\n          5\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>This is ok, but ideally it would be easier, and (more importantly) more network-efficient, if I could send them as:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"samples\"\n      ],\n      \"Values\": [\n        [\n          1,2,3,4,5\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>Is there a correct syntax to implement this? <\/p>",
        "Challenge_closed_time":1438871325836,
        "Challenge_comment_count":2,
        "Challenge_created_time":1437737046243,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/31609319",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":13.74,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":315.0776647222,
        "Challenge_title":"'Enter Data' as list instead of list of lists in Azure ML Web Service",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":174,
        "Challenge_word_count":153,
        "Platform":"Stack Overflow",
        "Poster_created_time":1266595927520,
        "Poster_location":null,
        "Poster_reputation_count":7681.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>I have worked internally to request a confirmation of your concern - <\/p>\n\n<blockquote>\n  <p>'Enter Data' as list instead of list of lists in Azure ML Web Service<\/p>\n<\/blockquote>\n\n<p>but you expected feature is not available today in Azure ML Studio (The reason behind is Azure ML has to be able to read the input data as a tabular format, rows and columns). Such being the case, I would like to suggest you to submit a new feature request via below option:<\/p>\n\n<p>On Azure ML Studio -> the upper right corner, there is a smiley face, please click that and send the feedback.<\/p>\n\n<p>Should you have any further concerns, please feel free to let me know.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.8,
        "Solution_reading_time":7.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":114.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1431258605807,
        "Answerer_location":"Melbourne, Victoria, Australia",
        "Answerer_reputation_count":331.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":617.4599341667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a dataset in azure machine learning (.csv), on the same dataset I have multiple models build, I want to subset data for each of the model based on a different column<\/p>\n\n<p>Input:<\/p>\n\n<pre><code>ID col1 col2 col3\n1  0    13   0\n2  5    45   0\n3  10   0    34\n4  12   1    3\n<\/code><\/pre>\n\n<p>For the 1st model I want to retain all records where col1 not equal to None<\/p>\n\n<pre><code>ID col1 col2 col3\n2  5    45   0\n3  10   0    34\n4  12   1    3\n<\/code><\/pre>\n\n<p>Similarly for model 2<\/p>\n\n<pre><code>ID col1 col2 col3\n1  0    13   0\n2  5    45   0\n4  12   1    3\n<\/code><\/pre>\n\n<p>Hope it was clear<\/p>\n\n<p>The equivalent in R would be <\/p>\n\n<pre><code>df[!df$col1 == \"None\",] \n<\/code><\/pre>",
        "Challenge_closed_time":1461479422230,
        "Challenge_comment_count":0,
        "Challenge_created_time":1459161991533,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1459256566467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36260727",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.4,
        "Challenge_reading_time":8.24,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":643.7307491667,
        "Challenge_title":"Equivalent of Subset in Azure machine learning studio",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":243,
        "Challenge_word_count":130,
        "Platform":"Stack Overflow",
        "Poster_created_time":1406266059940,
        "Poster_location":"Link\u00f6ping, Sweden",
        "Poster_reputation_count":1677.0,
        "Poster_view_count":221.0,
        "Solution_body":"<p>You can use the \"Execute R Script\" module and just plug in your R code there.<\/p>\n\n<pre><code>df &lt;- maml.mapInputPort(1)\ndf &lt;- df[!df$col1 == \"None\",] \nmaml.mapOutputPort(\"df\");\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":2.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1327570314367,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation_count":2854.0,
        "Answerer_view_count":324.0,
        "Challenge_adjusted_solved_time":30.6574666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are deploying a data consortium between more than 10 companies. Wi will deploy several machine learning models (in general advanced analytics models) for all the companies and we will administrate all the models. We are looking for a solution that administrates several servers, clusters and data science pipelines. I love kedro, but not sure what is the best option to administrate all while using kedro.<\/p>\n<p>In summary, we are looking for the best solution to administrate several models, tasks and pipelines in different servers and possibly Spark clusters. Our current options are:<\/p>\n<ul>\n<li><p>AWS as our data warehouse and Databricks for administrating servers, clusters and tasks. I don't feel that the notebooks of databricks are a good solution for building pipelines and to work collaboratively, so I would like to connect kedro to databricks (is it good? is it easy to schedule the run of the kedro pipelines using databricks?)<\/p>\n<\/li>\n<li><p>Using GCP for data warehouse and use kubeflow (iin GCP) for deploying models and the administration and the schedule of the pipelines and the needed resources<\/p>\n<\/li>\n<li><p>Setting up servers from ASW or GCP, install kedro and schedule the pipelines with airflow (I see a big problem administrating 20 servers and 40 pipelines)<\/p>\n<\/li>\n<\/ul>\n<p>I would like to know if someone knows what is the best option between these alternatives, their  downsides and advantages, or if there are more possibilities.<\/p>",
        "Challenge_closed_time":1605891844630,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605830422657,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1605836750283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64921833",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":19.1,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":17.0616591667,
        "Challenge_title":"DataBricks + Kedro Vs GCP + Kubeflow Vs Server + Kedro + Airflow",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":967,
        "Challenge_word_count":241,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605828724552,
        "Poster_location":null,
        "Poster_reputation_count":59.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I'll try and summarise what I know, but be aware that I've not been part of a KubeFlow project.<\/p>\n<h2>Kedro on Databricks<\/h2>\n<p>Our approach was to build our project with CI and then execute the pipeline from a notebook. We <em>did not<\/em> use the <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/11_tools_integration\/03_databricks.html\" rel=\"nofollow noreferrer\">kedro recommended approach<\/a> of using databricks-connect due to the <a href=\"https:\/\/databricks.com\/product\/aws-pricing\" rel=\"nofollow noreferrer\">large price difference<\/a> between Jobs and Interactive Clusters (which are needed for DB-connect). If you're working on several TB's of data, this quickly becomes relevant.<\/p>\n<p>As a DS, this approach may feel natural, as a SWE though it does not. Running pipelines in notebooks feels hacky. It works but it feels non-industrialised. Databricks performs well in automatically spinning up and down clusters &amp; taking care of the runtime for you. So their value add is abstracting IaaS away from you (more on that later).<\/p>\n<h2>GCP &amp; &quot;Cloud Native&quot;<\/h2>\n<p><strong>Pro<\/strong>: GCP's main selling point is BigQuery. It is an incredibly powerful platform, simply because you can be productive from day 0. I've seen people build entire web API's on top of it. KubeFlow isn't tied to GCP so you could port this somewhere else later on. Kubernetes will also allow you to run anything else you wish on the cluster, API's, streaming, web services, websites, you name it.<\/p>\n<p><strong>Con<\/strong>: Kubernetes is complex. If you have 10+ engineers to run this project long-term, you should be OK. But don't underestimate the complexity of Kubernetes. It is to the cloud what Linux is to the OS world. Think log management, noisy neighbours (one cluster for web APIs + batch spark jobs), multi-cluster management (one cluster per department\/project), security, resource access etc.<\/p>\n<h2>IaaS server approach<\/h2>\n<p>Your last alternative, the manual installation of servers is one I would recommend only if you have a large team, extremely large data and are building a long-term product who's revenue can sustain the large maintenance costs.<\/p>\n<h2>The people behind it<\/h2>\n<p>How does the talent market look like in your region? If you can hire experienced engineers with GCP knowledge, I'd go for the 2nd solution. GCP is a mature, &quot;native&quot; platform in the sense that it abstracts a lot away for customers. If your market has mainly AWS engineers, that may be a better road to take. If you have a number of kedro engineers, that also has relevance. Note that kedro is agnostic enough to run anywhere. It's really just python code.<\/p>\n<p><strong>Subjective advise<\/strong>:<\/p>\n<p>Having worked mostly on AWS projects and a few GCP projects, I'd go for GCP. I'd use the platform's components (BigQuery, Cloud Run, PubSub, Functions, K8S) as a toolbox to choose from and build an organisation around that. Kedro can run in any of these contexts, as a triggered job by the Scheduler, as a container on Kubernetes or as a ETL pipeline bringing data into (or out of) BigQuery.<\/p>\n<p>While Databricks is &quot;less management&quot; than raw AWS, it's still servers to think about and VPC networking charges to worry over. BigQuery is simply GB queried. Functions are simply invocation count. These high level components will allow you to quickly show value to customers and you only need to go deeper (RaaS -&gt; PaaS -&gt; IaaS) as you scale.<\/p>\n<p>AWS also has these higher level abstractions over IaaS but in general, it appears (to me) that Google's offering is the most mature. Mainly because they have published tools they've been using internally for almost a decade whereas AWS has built new tools for the market. AWS is the king of IaaS though.<\/p>\n<p>Finally, a bit of content, <a href=\"https:\/\/youtu.be\/kjhXMTOLtac?t=618\" rel=\"nofollow noreferrer\">two former colleagues have discussed ML industrialisation frameworks earlier this fall<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1605947117163,
        "Solution_link_count":3.0,
        "Solution_readability":7.9,
        "Solution_reading_time":49.9,
        "Solution_score_count":4.0,
        "Solution_sentence_count":41.0,
        "Solution_word_count":606.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1614873430827,
        "Answerer_location":null,
        "Answerer_reputation_count":169.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":14.7412641667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to setup DVC with Google Drive storage as shown <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#url-format\" rel=\"nofollow noreferrer\">here<\/a>. So far, I've been unsuccessful in pushing data to the remote. I tried both with and without the Google App setup.<\/p>\n<p>After running a <code>dvc push -v<\/code>, the following exception is shown:<\/p>\n<pre><code>  File &quot;(...)\/anaconda3\/lib\/python3.8\/site-packages\/googleapiclient\/discovery.py&quot;, line 387, in _retrieve_discovery_doc\n    raise UnknownApiNameOrVersion(&quot;name: %s  version: %s&quot; % (serviceName, version))\ngoogleapiclient.errors.UnknownApiNameOrVersion: name: drive  version: v2\n<\/code><\/pre>\n<p>DVC was installed via <code>pip install dvc[gdrive]<\/code>. The <code>pip freeze<\/code> of the concerning packages is:<\/p>\n<pre><code>oauth2client==4.1.3\ngoogle-api-python-client==2.0.1\ndvc==2.0.1\n<\/code><\/pre>\n<p>Any help is thoroughly appreciated.<\/p>",
        "Challenge_closed_time":1614873489452,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614869465907,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1614874338736,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66477468",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":14.21,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.1176513889,
        "Challenge_title":"Data Version Control with Google Drive Remote: \"googleapiclient.errors.UnknownApiNameOrVersion: name: drive version: v2\"",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":979,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1593006899208,
        "Poster_location":null,
        "Poster_reputation_count":704.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>Can you try to install <code>google-api-python-client==1.12.8<\/code> and test in that way?<\/p>\n<p>Edit:<\/p>\n<p>It appears to be that, this was a bug in the 2.0.0-2.0.1 of google-api-client and resolved in 2.0.2. So this should also work <code>google-api-python-client&gt;=2.0.2<\/code><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1614927407287,
        "Solution_link_count":0.0,
        "Solution_readability":3.7,
        "Solution_reading_time":3.76,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1550902509267,
        "Answerer_location":null,
        "Answerer_reputation_count":2669.0,
        "Answerer_view_count":3292.0,
        "Challenge_adjusted_solved_time":9.0819222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>Issue<\/strong> : Unable to get best model from AutoML run.<\/p>\n<p><strong>Code:<\/strong><\/p>\n<pre><code>best_run, fitted_model = automl_run.get_output()\nprint(best_run.properties[&quot;run_algorithm&quot;])\n<\/code><\/pre>\n<p><strong>Error Message :<\/strong><\/p>\n<pre><code>ErrorResponse \n[stderr]{\n[stderr]    &quot;error&quot;: {\n[stderr]        &quot;code&quot;: &quot;UserError&quot;,\n[stderr]        &quot;message&quot;: &quot;The model you attempted to retrieve requires 'xgboost' to be installed at '==1.3.3'. You have 'xgboost==1.3.3', please reinstall 'xgboost==1.3.3' (e.g. `pip install xgboost==1.3.3`) and rerun the previous command.&quot;,\n[stderr]        &quot;target&quot;: &quot;get_output&quot;,\n[stderr]        &quot;inner_error&quot;: {\n[stderr]            &quot;code&quot;: &quot;NotSupported&quot;,\n[stderr]            &quot;inner_error&quot;: {\n[stderr]                &quot;code&quot;: &quot;IncompatibleOrMissingDependency&quot;\n[stderr]            }\n[stderr]        },\n[stderr]        &quot;reference_code&quot;: &quot;910310e6-2433-40cd-b597-9ec2950bc1d8&quot;\n[stderr]    }\n<\/code><\/pre>\n<p><strong>Conda Dependency<\/strong><\/p>\n<pre><code># Conda environment specification. The dependencies defined in this file will\n# be automatically provisioned for runs with userManagedDependencies=False.\n\n# Details about the Conda environment file format:\n# https:\/\/conda.io\/docs\/user-guide\/tasks\/manage-environments.html#create-env-file-manually\n\nname: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.12\n\n- pip:\n  - azureml-train-automl-runtime==1.38.0\n  - azureml-train-automl-client==1.38.0\n  - inference-schema\n  - azureml-interpret==1.38.0\n  - azureml-defaults==1.38.0\n- numpy&gt;=1.16.0,&lt;1.19.0\n- pandas==0.25.1\n- scikit-learn==0.22.1\n- py-xgboost&lt;=1.3.3\n- fbprophet==0.5\n- holidays==0.9.11\n- psutil&gt;=5.2.2,&lt;6.0.0\n- matplotlib=3.3.2\n- seaborn=0.9.0\n- joblib=0.13.2\n- joblib\nchannels:\n- anaconda\n- conda-forge\n<\/code><\/pre>\n<p><strong>Question:<\/strong><\/p>\n<ul>\n<li>What should be in my conda dependency that can fix this error<\/li>\n<li>I've tried making <code>py-xgboost==1.3.3<\/code> , but it didn't work.<\/li>\n<li>Any luck - how to fix this ?<\/li>\n<\/ul>",
        "Challenge_closed_time":1648186385707,
        "Challenge_comment_count":2,
        "Challenge_created_time":1648153690787,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71609028",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.9,
        "Challenge_reading_time":29.38,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":9.0819222222,
        "Challenge_title":"Azure AutoML dependency failure",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":142,
        "Challenge_word_count":173,
        "Platform":"Stack Overflow",
        "Poster_created_time":1642621384680,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":11.0,
        "Solution_body":"<blockquote>\n<p>Error - &quot;The model you attempted to retrieve requires 'xgboost' to be\ninstalled at '==1.3.3'. You have 'xgboost==1.3.3', please reinstall\n'xgboost==1.3.3' (e.g. <code>pip install xgboost==1.3.3<\/code>) and rerun the\nprevious command.&quot;<\/p>\n<\/blockquote>\n<p>As given in above error message, it should be <code>pip install xgboost==1.3.3<\/code> not <code>py-xgboost&lt;=1.3.3<\/code><\/p>\n<p>If it does not work, try downgraded version of <code>xgboost<\/code><\/p>\n<pre><code>pip install xgboost==0.90\n<\/code><\/pre>\n<p>Refer this github <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1421\" rel=\"nofollow noreferrer\">link<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.6,
        "Solution_reading_time":8.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426093220648,
        "Answerer_location":"Bengaluru, India",
        "Answerer_reputation_count":1861.0,
        "Answerer_view_count":294.0,
        "Challenge_adjusted_solved_time":11.1307344444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any way to access the kedro pipeline environment name? Actually below is my problem.<\/p>\n<p>I am loading the config paths as below<\/p>\n<pre><code>conf_paths = [&quot;conf\/base&quot;, &quot;conf\/local&quot;]  \nconf_loader = ConfigLoader(conf_paths)\nparameters = conf_loader.get(&quot;parameters*&quot;, &quot;parameters*\/**&quot;)\ncatalog = conf_loader.get(&quot;catalog*&quot;)\n\n<\/code><\/pre>\n<p>But  I have few environments like  <code>&quot;conf\/server&quot; <\/code>, <code>&quot;conf\/test&quot;<\/code> etc, So if I have env name available I can add it to conf_paths as <code>&quot;conf\/&lt;env_name&gt;&quot;<\/code>  so that kedro will read the files from the respective env folder.\nBut now if the env path is not added to conf_paths, the files are not being read by kedro even if i specify the env name while I  run kedro like    <code>kedro run --env=server <\/code>\nI searched for all the docs but was not able to find any solution.<\/p>\n<p>EDIT:\nElaborating more on the problem.\nI am using the above-given parameters and catalog dicts in the nodes. I only have keys that are common for all runs in <code>conf\/base\/parameters.yml<\/code> and the environment specific keys in <code>conf\/server\/parameters.yml<\/code> but when i do <code>kedro run --env=server<\/code> I am getting <code>keyerror<\/code> which means the keys in <code>conf\/server\/parameters.yml<\/code> is not available in the parameters dict. If I add  <code>conf\/server<\/code> to config_paths kedro is running well without keyerror.<\/p>",
        "Challenge_closed_time":1639600021140,
        "Challenge_comment_count":8,
        "Challenge_created_time":1639518159733,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1639559950496,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70355869",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":11.3,
        "Challenge_reading_time":19.8,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":22.7392797222,
        "Challenge_title":"How to access environment name in kedro pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":712,
        "Challenge_word_count":201,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495105930728,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>You don't need to define config paths, config loader etc unless you are trying to override something.<\/p>\n<p>If you are using kedro 0.17.x, the hooks.py will look something like this.<\/p>\n<p>Kedro will pass, base, local and the env you specified during runtime in <code>conf_paths<\/code> into <code>ConfigLoader<\/code>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class ProjectHooks:\n    @hook_impl\n    def register_config_loader(\n        self, conf_paths: Iterable[str], env: str, extra_params: Dict[str, Any]\n    ) -&gt; ConfigLoader:\n        return ConfigLoader(conf_paths)\n\n    @hook_impl\n    def register_catalog(\n        self,\n        catalog: Optional[Dict[str, Dict[str, Any]]],\n        credentials: Dict[str, Dict[str, Any]],\n        load_versions: Dict[str, str],\n        save_version: str,\n        journal: Journal,\n    ) -&gt; DataCatalog:\n        return DataCatalog.from_config(\n            catalog, credentials, load_versions, save_version, journal\n        )\n<\/code><\/pre>\n<p>In question, I can see you have defined <code>conf_paths<\/code> and <code>conf_loader<\/code> and the env path is not present. So kedro will ignore the env passed during runtime.<\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.7,
        "Solution_reading_time":13.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":121.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":6.7810055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am submitting the training through a script file. Following is the content of the <code>train.py<\/code> script. Azure ML is treating all these as one run (instead of run per alpha value as coded below) as <code>Run.get_context()<\/code> is returning the same Run id.<\/p>\n<p><strong>train.py<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.opendatasets import Diabetes\nfrom azureml.core import Run\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.externals import joblib\n\nimport math\nimport os\nimport logging\n\n# Load dataset\ndataset = Diabetes.get_tabular_dataset()\nprint(dataset.take(1))\n\ndf = dataset.to_pandas_dataframe()\ndf.describe()\n\n# Split X (independent variables) &amp; Y (target variable)\nx_df = df.dropna()      # Remove rows that have missing values\ny_df = x_df.pop(&quot;Y&quot;)    # Y is the label\/target variable\n\nx_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=66)\nprint('Original dataset size:', df.size)\nprint(&quot;Size after dropping 'na':&quot;, x_df.size)\nprint(&quot;Training split size: &quot;, x_train.size)\nprint(&quot;Test split size: &quot;, x_test.size)\n\n# Training\nalphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters\n\n# Create and log interactive runs\n\noutput_dir = os.path.join(os.getcwd(), 'outputs')\n\nfor hyperparam_alpha in alphas:\n    # Get the experiment run context\n    run = Run.get_context()\n    print(&quot;Started run: &quot;, run.id)\n    run.log(&quot;train_split_size&quot;, x_train.size)\n    run.log(&quot;test_split_size&quot;, x_train.size)\n    run.log(&quot;alpha_value&quot;, hyperparam_alpha)\n\n    # Train\n    print(&quot;Train ...&quot;)\n    model = Ridge(hyperparam_alpha)\n    model.fit(X = x_train, y = y_train)\n    \n    # Predict\n    print(&quot;Predict ...&quot;)\n    y_pred = model.predict(X = x_test)\n\n    # Calculate &amp; log error\n    rmse = math.sqrt(mean_squared_error(y_true = y_test, y_pred = y_pred))\n    run.log(&quot;rmse&quot;, rmse)\n    print(&quot;rmse&quot;, rmse)\n\n    # Serialize the model to local directory\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir, exist_ok=True) \n\n    print(&quot;Save model ...&quot;)\n    model_name = &quot;model_alpha_&quot; + str(hyperparam_alpha) + &quot;.pkl&quot; # Pickle file\n    file_path = os.path.join(output_dir, model_name)\n    joblib.dump(value = model, filename = file_path)\n\n    # Upload the model\n    run.upload_file(name = model_name, path_or_stream = file_path)\n\n    # Complete the run\n    run.complete()\n<\/code><\/pre>\n<p><strong>Experiments view<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/E6xAG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E6xAG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Authoring code (i.e. control plane)<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom azureml.core import Workspace, Experiment, RunConfiguration, ScriptRunConfig, VERSION, Run\n\nws = Workspace.from_config()\nexp = Experiment(workspace = ws, name = &quot;diabetes-local-script-file&quot;)\n\n# Create new run config obj\nrun_local_config = RunConfiguration()\n\n# This means that when we run locally, all dependencies are already provided.\nrun_local_config.environment.python.user_managed_dependencies = True\n\n# Create new script config\nscript_run_cfg = ScriptRunConfig(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    script = 'train.py',\n    run_config = run_local_config) \n\nrun = exp.submit(script_run_cfg)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>",
        "Challenge_closed_time":1599436122230,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599411710610,
        "Challenge_favorite_count":3,
        "Challenge_last_edit_time":1599438192360,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63766714",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":46.74,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":6.7810055556,
        "Challenge_title":"Run.get_context() gives the same run id",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2523,
        "Challenge_word_count":324,
        "Platform":"Stack Overflow",
        "Poster_created_time":1245726715288,
        "Poster_location":"Cumming, GA",
        "Poster_reputation_count":77230.0,
        "Poster_view_count":6359.0,
        "Solution_body":"<h2>Short Answer<\/h2>\n<h3>Option 1: create child runs within run<\/h3>\n<p><code>run = Run.get_context()<\/code> assigns the run object of the run that you're currently in to <code>run<\/code>. So in every iteration of the hyperparameter search, you're logging to the same run. To solve this, you need to create child (or sub-) runs for each hyperparameter value. You can do this with <code>run.child_run()<\/code>. Below is the template for making this happen.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>run = Run.get_context()\n\nfor hyperparam_alpha in alphas:\n    # Get the experiment run context\n    run_child = run.child_run()\n    print(&quot;Started run: &quot;, run_child.id)\n    run_child.log(&quot;train_split_size&quot;, x_train.size)\n<\/code><\/pre>\n<p>On the <code>diabetes-local-script-file<\/code> Experiment page, you can see that Run <code>9<\/code> was the parent run and Runs <code>10-19<\/code> were the child runs if you click &quot;Include child runs&quot; page. There is also a &quot;Child runs&quot; tab on Run 9 details page.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5IMcz.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5IMcz.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Long answer<\/h2>\n<p>I highly recommend abstracting the hyperparameter search away from the data plane (i.e. <code>train.py<\/code>) and into the control plane (i.e. &quot;authoring code&quot;). This becomes especially valuable as training time increases and you can arbitrarily parallelize and also choose Hyperparameters more intelligently by using Azure ML's <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters\" rel=\"noreferrer\"><code>Hyperdrive<\/code><\/a>.<\/p>\n<h3>Option 2 Create runs from control plane<\/h3>\n<p>Remove the loop from your code, add the code like below (<a href=\"https:\/\/gist.github.com\/swanderz\/f5c0dc1aefc796d37f6bc3600f2ae3cd\" rel=\"noreferrer\">full data and control here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import argparse\nfrom pprint import pprint\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--alpha', type=float, default=0.5)\nargs = parser.parse_args()\nprint(&quot;all args:&quot;)\npprint(vars(args))\n\n# use the variable like this\nmodel = Ridge(args.alpha)\n<\/code><\/pre>\n<p>below is how to submit a single run using a script argument. To submit multiple runs, just use a loop in the control plane.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters\n\nlist_rcs = [ScriptRunConfig(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    script = 'train.py',\n    arguments=['--alpha',a],\n    run_config = run_local_config) for a in alphas]\n\nlist_runs = [exp.submit(rc) for rc in list_rcs]\n\n<\/code><\/pre>\n<h3>Option 3 Hyperdrive (IMHO the recommended approach)<\/h3>\n<p>In this way you outsource the hyperparameter source to <code>Hyperdrive<\/code>. The UI will also report results exactly how you want them, and via the API you can easily download the best model.  Note you can't use this locally anymore and must use <code>AMLCompute<\/code>, but to me it is a worthwhile trade-off.<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters#configure-experiment\" rel=\"noreferrer\">This is a great overview<\/a>. Excerpt below (<a href=\"https:\/\/gist.github.com\/swanderz\/f5c0dc1aefc796d37f6bc3600f2ae3cd#file-hyperdrive-ipynb\" rel=\"noreferrer\">full code here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>param_sampling = GridParameterSampling( {\n        &quot;alpha&quot;: choice(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)\n    }\n)\n\nestimator = Estimator(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    entry_script = 'train.py',\n    compute_target=cpu_cluster,\n    environment_definition=Environment.get(workspace=ws, name=&quot;AzureML-Tutorial&quot;)\n)\n\nhyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n                          hyperparameter_sampling=param_sampling, \n                          policy=None,\n                          primary_metric_name=&quot;rmse&quot;, \n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                          max_total_runs=10,\n                          max_concurrent_runs=4)\n\nrun = exp.submit(hyperdrive_run_config)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q1AhJ.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q1AhJ.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":14.3,
        "Solution_reading_time":57.41,
        "Solution_score_count":7.0,
        "Solution_sentence_count":41.0,
        "Solution_word_count":417.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":3.9519652778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Amazon SageMaker has <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">inference pipelines<\/a> that process requests for inferences on data. It sounds as though inferences are similar (or perhaps identical) to predictions. Are there any differences between inferences and predictions? If so, what? If not, why not just call it a prediction pipeline?<\/p>",
        "Challenge_closed_time":1561569885852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561555658777,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56773989",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":6.54,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.9519652778,
        "Challenge_title":"In Amazon SageMaker, what (if any) is the difference between an inference and prediction?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":42,
        "Challenge_word_count":61,
        "Platform":"Stack Overflow",
        "Poster_created_time":1336973807643,
        "Poster_location":"Minneapolis, MN, United States",
        "Poster_reputation_count":1907.0,
        "Poster_view_count":174.0,
        "Solution_body":"<p>Inference usually refers to applying a learned transformation to input data. That learned transformation could be something else than a prediction (eg dim reduction, clustering, entity extraction etc). So calling that process a prediction would be a bit too restrictive in my opinion<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":3.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1621410539876,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":3579.0,
        "Answerer_view_count":1775.0,
        "Challenge_adjusted_solved_time":4.1121516667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>got a folder called data-asset which contains a yaml file with the following<\/p>\n<pre><code>type: uri_folder\nname: &lt;name_of_data&gt;\ndescription: &lt;description goes here&gt;\npath: &lt;path&gt;\n<\/code><\/pre>\n<p>In a pipeline am referencing this using azure cli inline script using the following command az ml data create -f .yml but getting error<\/p>\n<p>full error-D:\\a\\1\\s\\ETL\\data-asset&gt;az ml data create -f data-asset.yml\nERROR: 'ml' is misspelled or not recognized by the system.<\/p>\n<p>Examples from AI knowledge base:\naz extension add --name anextension\nAdd extension by name<\/p>\n<p>trying to implement this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-data-assets?tabs=CLI\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-data-assets?tabs=CLI<\/a><\/p>\n<p>how can a resolve this?<\/p>",
        "Challenge_closed_time":1659361184080,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659348144177,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73192053",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":12.93,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.6221952778,
        "Challenge_title":"azure cli not recognizing the following command az ml data create -f <file-name>.yml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":112,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606756004663,
        "Poster_location":null,
        "Poster_reputation_count":49.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>One of the workaround you can follow to resolve the above issue;<\/p>\n<p>Based on this <a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/21390#issuecomment-1161782243\" rel=\"nofollow noreferrer\"><em><strong>GitHub issue<\/strong><\/em><\/a> as suggested by @<em>adba-msft<\/em> .<\/p>\n<blockquote>\n<p><strong>Please make sure that you have upgraded your azure cli to latest and<\/strong>\n<strong>Azure CLI ML extension v2 is being used.<\/strong><\/p>\n<\/blockquote>\n<p>To check and upgrade the cli we can use the below <code>cmdlts<\/code>:<\/p>\n<pre><code>az version\n\naz upgrade\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Uopde.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Uopde.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For more information please refer this similar <a href=\"https:\/\/stackoverflow.com\/questions\/73110661\/create-is-misspelled-or-not-recognized-by-the-system-on-az-ml-dataset-create\"><em><strong>SO THREAD|'create' is misspelled or not recognized by the system on az ml dataset create<\/strong><\/em><\/a> .<\/p>\n<p>I did observe the same issue after trying the aforementioned suggestion by @<em>Dor Lugasi-Gal<\/em> it works for me with (in my case <code>az ml -h<\/code>) after installed the extension with  <code>az extension add -n ml -y<\/code> can able to get the result of <code>az ml -h<\/code> without any error.<\/p>\n<p><em><strong>SCREENSHOT FOR REFERENCE:-<\/strong><\/em><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/39LHa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/39LHa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1659362947923,
        "Solution_link_count":6.0,
        "Solution_readability":12.5,
        "Solution_reading_time":21.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":161.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1518707555920,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":1664.0,
        "Answerer_view_count":560.0,
        "Challenge_adjusted_solved_time":9.0124052778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using nested parameters in my <code>parameters.yml<\/code> and would like to override these using runtime parameters for the <code>kedro run<\/code> CLI command:<\/p>\n<pre><code>train:\n    batch_size: 32\n    train_ratio: 0.9\n    epochs: 5\n<\/code><\/pre>\n<p>The following doesn't seem to work:<\/p>\n<pre><code>kedro run --params  train.batch_size:64,train.epochs:50 \n<\/code><\/pre>\n<p>the values for epoch and batch_size are those from the <code>parameters.yml<\/code>. How can I override these parameters with the cli command?<\/p>",
        "Challenge_closed_time":1596531353283,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596500312837,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63238607",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.2,
        "Challenge_reading_time":7.39,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":8.6223461111,
        "Challenge_title":"Override nested parameters using kedro run CLI command",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":549,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1362514672823,
        "Poster_location":"Vorarlberg, Austria",
        "Poster_reputation_count":1570.0,
        "Poster_view_count":159.0,
        "Solution_body":"<p>The additional parameters get passed into the <code>KedroContext<\/code> object via <code>load_context(Path.cwd(), env=env, extra_params=params)<\/code> in <code>kedro_cli.py<\/code>. Here you can see that there's a callback (protected) function called <code>_split_params<\/code> which splits the key-value pairs on <code>:<\/code>.<\/p>\n<p>This <code>_split_params<\/code> first splits string on commas (to get multiple params) and then on colons. Actually adding a print\/logging statement of what gets passed into <code>extra_params<\/code> will show you something like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>{'train.batch_size': 64, 'train.epochs': 50}\n<\/code><\/pre>\n<p>I think you have a couple options:<\/p>\n<ol>\n<li>Un-nesting the params. That way you will override them correctly.<\/li>\n<li>Adding custom logic to <code>_split_params<\/code> in <code>kedro_cli.py<\/code> to create a nested dictionary on <code>.<\/code> characters which gets passed into the func mentioned above. I think you can reuse a lot of the existing logic.<\/li>\n<\/ol>\n<p>NB: This was tested on <code>kedro==0.16.2<\/code>.<\/p>\n<p>NB2: The way <code>kedro<\/code> splits out nested params is using the <code>_get_feed_dict<\/code> and <code>_add_param_to_feed_dict<\/code> functions in <code>context.py<\/code>. Specifically, <code>_add_param_to_feed_dict<\/code> is a recursive function that unpacks a dictionary and formats as <code>&quot;{}.{}&quot;.format(key, value)<\/code>. IMO you can use the logic from here.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1596532757496,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":19.54,
        "Solution_score_count":3.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":171.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1473257955532,
        "Answerer_location":null,
        "Answerer_reputation_count":145.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":26.3318861111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have read similar questions regarding azure app service, but I still can't find an answer. I was trying to deploy a model in azure kubernetes service, but I came across an error when importing cv2 (which is essential to me).<\/p>\n<p>Opencv-python is included in my environment .yaml file:<\/p>\n<pre><code>name: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n  # You must list azureml-defaults as a pip dependency\n  - azureml-defaults&gt;=1.0.45\n  - Cython\n  - matplotlib&gt;=3.2.2\n  - numpy&gt;=1.18.5\n  - opencv-python&gt;=4.1.2\n  - pillow\n  - PyYAML&gt;=5.3\n  - scipy&gt;=1.4.1\n  - torch&gt;=1.6.0\n  - torchvision&gt;=0.7.0\n  - tqdm&gt;=4.41.0\nchannels:\n- conda-forge\n<\/code><\/pre>\n<p>I am deploying as follows:<\/p>\n<pre><code>aks_service = Model.deploy(ws,\n                       models=[model],\n                       inference_config=inference_config,\n                       deployment_config=gpu_aks_config,\n                       deployment_target=aks_target,\n                       name=aks_service_name)\n<\/code><\/pre>\n<p>And I get this error:<\/p>\n<pre><code>    Traceback (most recent call last):\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 129, in init_process\n    self.load_wsgi()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 138, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 52, in load\n    return self.load_wsgiapp()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 41, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/util.py&quot;, line 350, in import_app\n    __import__(module)\n  File &quot;\/var\/azureml-server\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import create_app\n  File &quot;\/var\/azureml-server\/create_app.py&quot;, line 3, in &lt;module&gt;\n    from app import main\n  File &quot;\/var\/azureml-server\/app.py&quot;, line 32, in &lt;module&gt;\n    from aml_blueprint import AMLBlueprint\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 23, in &lt;module&gt;\n    main_module_spec.loader.exec_module(main)\n  File &quot;\/var\/azureml-app\/score.py&quot;, line 8, in &lt;module&gt;\n    import cv2\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/cv2\/__init__.py&quot;, line 5, in &lt;module&gt;\n    from .cv2 import *\nImportError: libGL.so.1: cannot open shared object file: No such file or directory\nWorker exiting (pid: 41)\n<\/code><\/pre>",
        "Challenge_closed_time":1603897093470,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603802298680,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64554615",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.1,
        "Challenge_reading_time":41.93,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":26.3318861111,
        "Challenge_title":"Import cv2 error when deploying in Azure Kubernetes Service - python",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":484,
        "Challenge_word_count":243,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473257955532,
        "Poster_location":null,
        "Poster_reputation_count":145.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>This might go wrong at some point, but my workaround was installing opencv-python-headless instead of opencv.<\/p>\n<p>In the environment .yaml file, just replace:<\/p>\n<pre><code>- opencv-python&gt;=4.1.2\n<\/code><\/pre>\n<p>with:<\/p>\n<pre><code>- opencv-python-headless\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.0,
        "Solution_reading_time":3.72,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1327234712912,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":53015.0,
        "Answerer_view_count":3262.0,
        "Challenge_adjusted_solved_time":0.0405816667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Basically I'm receiving an output like this from my azure ws output:<\/p>\n\n<pre><code>{\n    'Results': {\n        'WSOutput': {\n            'type': 'table',\n            'value': {\n                'ColumnNames': ['ID', 'Start', 'Ask', 'Not', 'Passed', 'Suggest'],\n                'ColumnTypes': ['Int32', 'Int32', 'Int32', 'Double', 'Int64', 'Int32'],\n                'Values': [['13256025', '25000', '19000', '0.35', '1', '25000']]\n            }\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>The string, as you can see, has the info to create a datatable object. Now, I can't seem to find an easy way to cast it to an actual datatable POCO. I'm able to manually code a parser with Newtonsoft.Json.Linq but there has to be an easier way. <\/p>\n\n<p>Does anybody know how? I can't seem to find anything on the net.<\/p>",
        "Challenge_closed_time":1519929928907,
        "Challenge_comment_count":7,
        "Challenge_created_time":1519929782813,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1519930038023,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49056593",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":6.5,
        "Challenge_reading_time":9.56,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.0405816667,
        "Challenge_title":"Convert a datatable string from Azure ML WS to an actual Datatable C# Object?",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":83,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324654920387,
        "Poster_location":"Waterloo, ON, Canada",
        "Poster_reputation_count":5211.0,
        "Poster_view_count":449.0,
        "Solution_body":"<p>Yes, there is a open source online gernator on the net (<a href=\"http:\/\/jsonutils.com\/\" rel=\"nofollow noreferrer\">http:\/\/jsonutils.com\/<\/a>). Copy paste your result will give you that:<\/p>\n\n<pre><code> public class Value\n    {\n        public IList&lt;string&gt; ColumnNames { get; set; }\n        public IList&lt;string&gt; ColumnTypes { get; set; }\n        public IList&lt;IList&lt;string&gt;&gt; Values { get; set; }\n    }\n\n    public class WSOutput\n    {\n        public string type { get; set; }\n        public Value value { get; set; }\n    }\n\n    public class Results\n    {\n        public WSOutput WSOutput { get; set; }\n    }\n\n    public class Example\n    {\n        public Results Results { get; set; }\n    }\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":7.82,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1520413126203,
        "Answerer_location":null,
        "Answerer_reputation_count":37123.0,
        "Answerer_view_count":4058.0,
        "Challenge_adjusted_solved_time":47.4339141667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I trained a model in AZURE ML. Now i want to use that model in my ios app to predict the output\u00a0.<\/p>\n\n<p>How to download the model from AZURE and use it my swift code.<\/p>",
        "Challenge_closed_time":1525849618928,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525678856837,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1558224843256,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50209284",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.0,
        "Challenge_reading_time":2.66,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":47.4339141667,
        "Challenge_title":"How to use the trained model developed in AZURE ML",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":516,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1510206999776,
        "Poster_location":null,
        "Poster_reputation_count":81.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>As far as I know, the model could run in <strong>Azure Machine Learning Studio<\/strong>.It seems that you are unable to download it, the model could do nothing outside of Azure ML. <\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/41236871\/how-to-download-the-trained-models-from-azure-machine-studio\">Here<\/a> is a similar post for you to refer, I have also tried @Ahmet's \nmethod, but result is like @mrjrdnthms says.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1525850503192,
        "Solution_link_count":1.0,
        "Solution_readability":8.6,
        "Solution_reading_time":5.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":16.7675258334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently in the process of setting up a custom sagemaker container to run training jobs on Sagemaker and have succeeded in doing so. However, I am a bit confused over this question which is currently bugging me and is definitely something that I need to consider in the future<\/p>\n<ol>\n<li>Is it possible to run custom scripts on a custom container when declaring a sagemaker training job?<\/li>\n<\/ol>\n<p>My current understanding when it comes to creating a custom sagemaker image is that I create a train file that gets executed when running a training job, but I could never find documentation on whether is it possible to overwrite this and run a training script (but using the same custom container), like how we run training jobs using in-built algorithms. Is it the case that for custom algorithms we are restricted by this limitation?<\/p>",
        "Challenge_closed_time":1656147044190,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656086681097,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72746701",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.5,
        "Challenge_reading_time":11.47,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":16.7675258334,
        "Challenge_title":"Running custom scripts in a custom container while running a sagemaker training job",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":26,
        "Challenge_word_count":157,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644981356940,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I don't fully understand your question (can you make an example please?), specially when you say<\/p>\n<blockquote>\n<p>like how we run training jobs using in-built algorithms<\/p>\n<\/blockquote>\n<p>But basically you can do whatever you want in your container, as probably you already did, you have a proper <code>train<\/code> file in your container, which is the one that sagemaker calls as the entrypoint. In that file you can call external script (which are in your container too) and also pass parameters to your container (see how for example hyperparameters are passed). There is a quite clear documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/docker-containers-create.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":9.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":100.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":5.2579222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been exploring Azure ML Pipeline. I am referring to <a href=\"https:\/\/github.com\/MicrosoftLearning\/DP100\/blob\/master\/06A%20-%20Creating%20a%20Pipeline.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a> for the below code:<\/p>\n<p>Here is a small snippet from a MS Repo:<\/p>\n<pre><code>train_step = PythonScriptStep(name = &quot;Prepare Data&quot;,\nsource_directory = experiment_folder,\nscript_name = &quot;prep_diabetes.py&quot;,\narguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n'--prepped-data', prepped_data_folder],\noutputs=[prepped_data_folder],\ncompute_target = pipeline_cluster,\nrunconfig = pipeline_run_config,\nallow_reuse = True)\n<\/code><\/pre>\n<p>This suggests that while defining a pipeline, we must provide it a compute resource(pipeline_cluster). This obviously makes sense, since specific compute might be required for a specific step.<\/p>\n<p>But do we need to have this compute resource up and running always, so that whenever a pipeline is triggered, it can find the compute resource?<\/p>\n<p>Also, i figured we can probably keep a cluster with Zero minimum nodes, in which cases cluster is resized whenever pipeline is triggered. But i think there is a minimal cost incurrent in probably container registry regularly in such a setup. Is this the recommended way to deploy ML pipelines or some more efficient approach is possible?<\/p>",
        "Challenge_closed_time":1620823042460,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620804113940,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67498965",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":18.8,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":5.2579222222,
        "Challenge_title":"Pre-existing Compute Resource necessary for running a scheduled Azure ML pipeline?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":29,
        "Challenge_word_count":171,
        "Platform":"Stack Overflow",
        "Poster_created_time":1315165259620,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":339.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>Yep you're right -- create a <code>ComputeTarget<\/code> with a minimum of zero nodes. The <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/container-registry\/#pricing\" rel=\"nofollow noreferrer\">container registry costs<\/a> are ~$0.16 USD\/day and, IIRC, that cost is bundled in with Azure Machine learning.<\/p>\n<p>This is what our team does for our published pipelines in production.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.2,
        "Solution_reading_time":5.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1635045129020,
        "Answerer_location":null,
        "Answerer_reputation_count":53.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":250.6002630556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm dabbling with ML and was able to take a tutorial and get it to work for my needs.  It's a simple recommender system using TfidfVectorizer and linear_kernel.  I run into a problem with how I go about deploying it through Sagemaker with an end point.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel \nimport json\nimport csv\n\nwith open('data\/big_data.json') as json_file:\n    data = json.load(json_file)\n\nds = pd.DataFrame(data)\n\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(ds['content'])\ncosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n\nresults = {}\n\nfor idx, row in ds.iterrows():\n    similar_indices = cosine_similarities[idx].argsort()[:-100:-1]\n    similar_items = [(cosine_similarities[idx][i], ds['id'][i]) for i in similar_indices]\n\n    results[row['id']] = similar_items[1:]\n\ndef item(id):\n    return ds.loc[ds['id'] == id]['id'].tolist()[0]\n\ndef recommend(item_id, num):\n    print(&quot;Recommending &quot; + str(num) + &quot; products similar to &quot; + item(item_id) + &quot;...&quot;)\n    print(&quot;-------&quot;)\n    recs = results[item_id][:num]\n    for rec in recs:\n        print(&quot;Recommended: &quot; + item(rec[1]) + &quot; (score:&quot; + str(rec[0]) + &quot;)&quot;)\n\nrecommend(item_id='129035', num=5)\n<\/code><\/pre>\n<p>As a starting point I'm not sure if the output from <code>tf.fit_transform(ds['content'])<\/code> is considered the model or the output from <code>linear_kernel(tfidf_matrix, tfidf_matrix)<\/code>.<\/p>",
        "Challenge_closed_time":1636075476147,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635046349497,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1635173315200,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69693666",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.2,
        "Challenge_reading_time":21.77,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":285.8685138889,
        "Challenge_title":"How to Deploy ML Recommender System on AWS",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":63,
        "Challenge_word_count":164,
        "Platform":"Stack Overflow",
        "Poster_created_time":1635045129020,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>I came to the conclusion that I didn't need to deploy this through SageMaker.  Since the final linear_kernel output was a Dictionary I could do quick ID lookups to find correlations.<\/p>\n<p>I have it working on AWS with API Gateway\/Lambda, DynamoDB and an EC2 server to collect, process and plug the data into DynamoDB for fast lookups.  No expensive SageMaker endpoint needed.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":4.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1505653015243,
        "Answerer_location":null,
        "Answerer_reputation_count":1128.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":0.1140905556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to export a machine learning model I created in Azure Machine Learning studio. One of the required input is \"Path to blob beginning with container\"<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/xvSDd.png\" alt=\"Here is the screenshoot from azure export\"><\/p>\n\n<p>How do I find this path? I have already created a blob storage but I have no idea how to find the path to the blob storage. <\/p>",
        "Challenge_closed_time":1538035308943,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538034898217,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1538038508827,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52532078",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":5.24,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1140905556,
        "Challenge_title":"How to find the path to blob?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":15096,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>you should be able to find this from the Azure portal. Open the storage account, drill down into blobs, then your container. Use properties for the context menu, the URL should be the path ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":4.7,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1379362192207,
        "Answerer_location":"Embrach, Schweiz",
        "Answerer_reputation_count":181.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":0.6535952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have to do a sales prediction and I'm evaluation using a ML.NET solution hosted in a virtual machine(in Azure) vs using Azure ML Studio. The data may change once or twice per month. Which solutions should I choose? Also, for my use case, pricing might be a factor.\nThank you. <\/p>",
        "Challenge_closed_time":1575557093136,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575554740193,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1575622440107,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59196919",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.6,
        "Challenge_reading_time":3.76,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.6535952778,
        "Challenge_title":"ML.NET vs Azure ML Studio",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":510,
        "Challenge_word_count":56,
        "Platform":"Stack Overflow",
        "Poster_created_time":1406013299956,
        "Poster_location":null,
        "Poster_reputation_count":450.0,
        "Poster_view_count":105.0,
        "Solution_body":"<p>In short:\nIf you are building a .NET application and want to integrate ML, use ML.NET.\nIf you don't do .NET, use Azure ML.\nDocs are helpful here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/data-guide\/technology-choices\/data-science-and-machine-learning\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/data-guide\/technology-choices\/data-science-and-machine-learning<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.2,
        "Solution_reading_time":5.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1651331397896,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":43.4867702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I need to setup a shared cache in minikube in such a way that different services can use that cache to pull and update DVC models and data needed for training Machine Learning models. The structure of the project is to use 1 pod to periodically update the cache with new models and outputs. Then, multiple pods can read the cache to recreate the updated models and data. So I need to be able to update the local cache directory and pull from it using DVC commands, so that all the services have consistent view on the latest models and data created by a service.<\/p>\n<p>More specifically, I have a docker image called <code>inference-service<\/code> that should only <code>dvc pull<\/code> or some how use the info in the shared dvc cache to get the latest model and data locally in <code>models<\/code> and <code>data<\/code> folders (see dockerfile) in minikube. I have another image called <code>test-service<\/code> that\nruns the ML pipeline using <code>dvc repro<\/code> which creates the models and data that DVC needs (dvc.yaml) to track and store in the shared cache. So <code>test-service<\/code> should push created outputs from the ML pipeline into the shared cache so that <code>inference-service<\/code> can pull it and use it instead of running dvc repro by itself. <code>test-service<\/code> should only re-train and write the updated models and data into the shared cache while <code>inference-service<\/code> should only read and recreate the updated\/latest models and data from the shared cache.<\/p>\n<p><em><strong>Problem: the cache does get mounted on the minikube VM, but the inference service does not pull (using <code>dvc pull -f<\/code>) the data and models after the test service is done with <code>dvc repro<\/code> and results the following warnings and failures:<\/strong><\/em><\/p>\n<p><em>relevant kubernetes pod log of inference-service<\/em><\/p>\n<pre><code>WARNING: Output 'data\/processed\/train_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/train_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/validation_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/validation_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/test_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/test_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/interim\/train_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/train_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/validation_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/validation_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/test_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/test_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'models\/mlb.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/tfidf_vectorizer.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/model.pkl'(stage: 'train') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'reports\/scores.json'(stage: 'evaluate') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: No file hash info found for '\/root\/models\/model.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/reports\/scores.json'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/train_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/validation_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/test_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/train_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/validation_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/test_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/mlb.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/tfidf_vectorizer.pkl'. It won't be created.\n10 files failed\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\n\/root\/models\/model.pkl\n\/root\/reports\/scores.json\n\/root\/data\/processed\/train_preprocessed.pkl\n\/root\/data\/processed\/validation_preprocessed.pkl\n\/root\/data\/processed\/test_preprocessed.pkl\n\/root\/data\/interim\/train_featurized.pkl\n\/root\/data\/interim\/validation_featurized.pkl\n\/root\/data\/interim\/test_featurized.pkl\n\/root\/models\/mlb.pkl\n\/root\/models\/tfidf_vectorizer.pkl\nIs your cache up to date?\n<\/code><\/pre>\n<p><em>relevant kubernetes pod log of test-service<\/em><\/p>\n<pre><code>Stage 'preprocess' is cached - skipping run, checking out outputs\nGenerating lock file 'dvc.lock'\nUpdating lock file 'dvc.lock'\nStage 'featurize' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'train' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'evaluate' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nUse `dvc push` to send your updates to remote storage.\n<\/code><\/pre>\n<p><strong>Project Tree<\/strong><\/p>\n<pre><code>\u251c\u2500 .dvc\n\u2502  \u251c\u2500 .gitignore\n\u2502  \u251c\u2500 config\n\u2502  \u2514\u2500 tmp\n\u251c\u2500 deployment\n\u2502  \u251c\u2500 docker-compose\n\u2502  \u2502  \u251c\u2500 docker-compose.yml\n\u2502  \u251c\u2500 minikube-dep\n\u2502  \u2502  \u251c\u2500 inference-test-services_dep.yaml\n\u2502  \u251c\u2500 startup_minikube_with_mount.sh.sh\n\u251c\u2500 Dockerfile # for inference service\n\u251c\u2500 dvc-cache # services should push and pull from this cache folder and see this as the DVC repo\n\u251c- dvc.yaml\n\u251c- params.yaml\n\u251c\u2500 src\n\u2502  \u251c\u2500 build_features.py\n|  \u251c\u2500 preprocess_data.py\n|  \u251c\u2500 serve_model.py\n|  \u251c\u2500 startup.sh  \n|  \u251c\u2500 requirements.txt\n\u251c\u2500 test_dep\n\u2502  \u251c\u2500 .dvc # same as .dvc in the root folder\n|  |  \u251c\u2500...\n\u2502  \u251c\u2500 Dockerfile # for test service\n\u2502  \u251c\u2500 dvc.yaml\n|  \u251c\u2500 params.yaml\n\u2502  \u2514\u2500 src\n\u2502     \u251c\u2500 build_features.py # same as root src folder\n|     \u251c\u2500 preprocess_data.py # same as root src folder\n|     \u251c\u2500 serve_model.py # same as root src folder\n|     \u251c\u2500 startup_test.sh  \n|     \u251c\u2500 requirements.txt  # same as root src folder\n<\/code><\/pre>\n<p><strong>dvc.yaml<\/strong><\/p>\n<pre><code>stages:\n  preprocess:\n    cmd: python ${preprocess.script}\n    params:\n      - preprocess\n    deps:\n      - ${preprocess.script}\n      - ${preprocess.input_train}\n      - ${preprocess.input_val}\n      - ${preprocess.input_test}\n    outs:\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n  featurize:\n    cmd: python ${featurize.script}\n    params:\n      - preprocess\n      - featurize\n    deps:\n      - ${featurize.script}\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n    outs:\n      - ${featurize.output_train}\n      - ${featurize.output_val}\n      - ${featurize.output_test}\n      - ${featurize.mlb_out}\n      - ${featurize.tfidf_vectorizer_out}\n  train:\n    cmd: python ${train.script}\n    params:\n      - featurize\n      - train\n    deps:\n      - ${train.script}\n      - ${featurize.output_train}\n    outs:\n      - ${train.model_out}\n  evaluate:\n    cmd: python ${evaluate.script}\n    params:\n      - featurize\n      - train\n      - evaluate\n    deps:\n      - ${evaluate.script}\n      - ${train.model_out}\n      - ${featurize.output_val}\n    metrics:\n      - ${evaluate.scores_path}\n<\/code><\/pre>\n<p><strong>params.yaml<\/strong><\/p>\n<pre><code>preprocess:\n  script: src\/preprocess\/preprocess_data.py\n  input_train: data\/raw\/train.tsv\n  input_val: data\/raw\/validation.tsv\n  input_test: data\/raw\/test.tsv\n  output_train: data\/processed\/train_preprocessed.pkl\n  output_val: data\/processed\/validation_preprocessed.pkl\n  output_test: data\/processed\/test_preprocessed.pkl\n\nfeaturize:\n  script: src\/features\/build_features.py\n  output_train: data\/interim\/train_featurized.pkl\n  output_val: data\/interim\/validation_featurized.pkl\n  output_test: data\/interim\/test_featurized.pkl\n  mlb_out: models\/mlb.pkl\n  tfidf_vectorizer_out: models\/tfidf_vectorizer.pkl\n\ntrain:\n  script: src\/models\/train_model.py\n  model_out: models\/model.pkl\n\nevaluate:\n  script: src\/models\/evaluate_model.py\n  scores_path: reports\/scores.json\n  roc_json: reports\/roc_plot.json\n  prc_json: reports\/prc_plot.json\n<\/code><\/pre>",
        "Challenge_closed_time":1654875420463,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654718868090,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1655157056467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72551630",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":119.44,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":135,
        "Challenge_solved_time":43.4867702778,
        "Challenge_title":"How to setup a DVC shared cache without git repository between different services in minikube?",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":196,
        "Challenge_word_count":1051,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562403039336,
        "Poster_location":null,
        "Poster_reputation_count":298.0,
        "Poster_view_count":47.0,
        "Solution_body":"<p>After running <code>dvc repro<\/code> in <code>test-service<\/code>, a new <code>dvc.lock<\/code> will be created, containing the file hashes relative to your pipeline (i.e. the hash for <code>models\/model.pkl<\/code> etc).<\/p>\n<p>If you're running a shared cache, <code>inference-service<\/code> should have access to the updated <code>dvc.lock<\/code>. If that is present, it will be sufficient to run <code>dvc checkout<\/code> to populate the workspace with the files corresponding to the hashes in the shared cache.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":6.67,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":67.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1587281590603,
        "Answerer_location":null,
        "Answerer_reputation_count":473.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":84.1130386111,
        "Challenge_answer_count":1,
        "Challenge_body":"<h2>The problem<\/h2>\n<p>I have a jupyter notebook with an <code>ipydatagrid<\/code> widget that displays a dataframe. This notebook works correctly when run locally, but not when run in AWS SageMaker Studio. When run in SageMaker Studio, instead of showing the widget it simply shows the text <code>Loading widget...<\/code><\/p>\n<p>How does one use a <code>ipydatagrid<\/code> widget in the SageMaker Studio environment?<\/p>\n<h2>Details<\/h2>\n<p>Python version:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ python --version\nPython 3.7.10\n<\/code><\/pre>\n<p>Run at start:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ pip install -r requirements.txt\n$ jupyter nbextension enable --py --sys-prefix widgetsnbextension\n$ jupyter nbextension install --py --symlink --sys-prefix ipydatagrid\n$ jupyter nbextension enable --py --sys-prefix ipydatagrid\n<\/code><\/pre>\n<p>File <code>requirements.txt<\/code>:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>ipydatagrid==1.1.11\npandas==1.0.1\n<\/code><\/pre>\n<p>Notebook contents:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># %%\nimport pandas as pd\nfrom ipydatagrid import DataGrid\nfrom IPython.display import display\nimport ipywidgets\n\n# %%\ndata = [\n    (&quot;potato&quot;, 1.2, True),\n    (&quot;sweet potato&quot;, 0.8, False),\n    (&quot;french fries&quot;, 4.5, True),\n    (&quot;waffle fries&quot;, 4.9, True)\n]\ndf = pd.DataFrame(\n    data,\n    columns=[&quot;food&quot;, &quot;stars&quot;, &quot;is_available&quot;]\n)\n\n# %%\ngrid = DataGrid(df)\n\n# %%\ndisplay(grid)\n\n# %%\n# SANITY CHECK:\nbutton = ipywidgets.Button(\n    description=&quot;Button&quot;,\n    disabled=False\n)\ndef on_click(b):\n    print(&quot;CLICK&quot;)\nbutton.on_click(on_click)\ndisplay(button)\n<\/code><\/pre>\n<h3>Error messages<\/h3>\n<p>If I use the Google Chrome developer tools, I can see more logs in the browser that give some error messages, most of which are repeated:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>manager.js:305 Uncaught (in promise) Error: Module ipydatagrid, semver range ^1.1.11 is not registered as a widget module\n    at C.loadClass (manager.js:305:19)\n    at C.&lt;anonymous&gt; (manager-base.js:263:46)\n    at l (manager-base.js:44:23)\n    at Object.next (manager-base.js:25:53)\n    at manager-base.js:19:71\n    at new Promise (&lt;anonymous&gt;)\n    at Rtm6.k (manager-base.js:15:12)\n    at C.e._make_model (manager-base.js:257:16)\n    at C.&lt;anonymous&gt; (manager-base.js:246:45)\n    at l (manager-base.js:44:23)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>utils.js:119 Error: Could not create a model.\n    at n (utils.js:119:27)\n    at async C._handleCommOpen (manager.js:61:51)\n    at async v._handleCommOpen (default.js:994:100)\n    at async v._handleMessage (default.js:1100:43)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>manager-base.js:273 Could not instantiate widget\n<\/code><\/pre>\n<p>However, there is no overt error message that's immediately obvious to the user, including in the log where <code>print<\/code> statements send their output.<\/p>",
        "Challenge_closed_time":1648549758096,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648246951157,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71623732",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":39.78,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":84.1130386111,
        "Challenge_title":"ipydatagrid widget does not display in SageMaker Studio",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":312,
        "Challenge_word_count":304,
        "Platform":"Stack Overflow",
        "Poster_created_time":1560606498248,
        "Poster_location":"Provo, UT, USA",
        "Poster_reputation_count":528.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>SageMaker Studio currently runs JupyterLab v1.2 (as confirmed by <em>Help &gt; About JupyterLab<\/em>), and per the <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid#Installation\" rel=\"nofollow noreferrer\">ipydatagrid installation instructions<\/a>, current\/recent versions of this widget require v3+... So I think this is most likely your problem - as there were breaking changes in the interfaces for extensions between these major versions.<\/p>\n<p>I had a quick look at the past releases of <code>ipydatagrid<\/code> to see if using an older version would be possible, and it seems like the documented JLv3 requirement gets added between <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/0.2.16\" rel=\"nofollow noreferrer\">v0.2.16<\/a> and <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/1.0.1\" rel=\"nofollow noreferrer\">v1.0.1<\/a> (which are adjacent on GitHub).<\/p>\n<p>However, the old install instructions documented on 0.2 don't seem to work anymore: I get <code>ValueError: &quot;jupyter-datagrid&quot; is not a valid npm package<\/code> and also note that versions &lt;1.0 don't seem to be present <a href=\"https:\/\/libraries.io\/pypi\/ipydatagrid\/versions\" rel=\"nofollow noreferrer\">on PyPI<\/a>.<\/p>\n<p>So unfortunately I think (unless\/until SM Studio gets a JupyterLab version upgrade), this widget's not likely to work unless you dive in to building it from an old source code version.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":10.3,
        "Solution_reading_time":18.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":168.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1373922677212,
        "Answerer_location":null,
        "Answerer_reputation_count":1350.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":24155.5917811111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have created a simple experiment in Azure ML and trigger it with an http client. In Azure ML workspace, everything works ok when executed. However, the experiment times out and fails when I trigger the experiment using an http client. Setting a timeout value for the http client does not seem to work.<\/p>\n\n<p>Is there any way we can set this timeout value so that the experiment does not fail?<\/p>",
        "Challenge_closed_time":1522603988172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1435643857760,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/31130629",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.9,
        "Challenge_reading_time":5.26,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":24155.5917811111,
        "Challenge_title":"Azure ML web service times out",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1681,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1313488868532,
        "Poster_location":null,
        "Poster_reputation_count":209.0,
        "Poster_view_count":42.0,
        "Solution_body":"<p>Looks like it isn't possible to set this timeout based on <a href=\"https:\/\/feedback.azure.com\/forums\/257792-machine-learning\/suggestions\/6472562-configurable-timeout-for-experiments-and-web-servi\" rel=\"nofollow noreferrer\">a feature request that is still marked as \"planned\" as of 4\/1\/2018<\/a>.<\/p>\n\n<p>The recommendation from <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/sqlserver\/en-US\/cb4ee96d-c2ca-4c65-b02f-0ccb26181f7f\/timeout-in-web-service?forum=MachineLearning\" rel=\"nofollow noreferrer\">MSDN forums from 2017<\/a> is to use the Batch Execution Service, which starts the machine learning experiment and then asynchronously asks whether it's done.<\/p>\n\n<p>Here's a code snippet from the Azure ML Web Services Management Sample Code (all comments are from their sample code):<\/p>\n\n<pre><code>        using (HttpClient client = new HttpClient())\n        {\n            var request = new BatchExecutionRequest()\n            {\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt; () {\n                    {\n                        \"output\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = storageConnectionString,\n                            RelativeLocation = string.Format(\"{0}\/outputresults.file_extension\", StorageContainerName) \/*Replace this with the location you would like to use for your output file, and valid file extension (usually .csv for scoring results, or .ilearner for trained models)*\/\n                        }\n                    },\n                },    \n\n                GlobalParameters = new Dictionary&lt;string, string&gt;() {\n                }\n            };\n\n            client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n\n            \/\/ WARNING: The 'await' statement below can result in a deadlock\n            \/\/ if you are calling this code from the UI thread of an ASP.Net application.\n            \/\/ One way to address this would be to call ConfigureAwait(false)\n            \/\/ so that the execution does not attempt to resume on the original context.\n            \/\/ For instance, replace code such as:\n            \/\/      result = await DoSomeTask()\n            \/\/ with the following:\n            \/\/      result = await DoSomeTask().ConfigureAwait(false)\n\n            Console.WriteLine(\"Submitting the job...\");\n\n            \/\/ submit the job\n            var response = await client.PostAsJsonAsync(BaseUrl + \"?api-version=2.0\", request);\n\n            if (!response.IsSuccessStatusCode)\n            {\n                await WriteFailedResponse(response);\n                return;\n            }\n\n            string jobId = await response.Content.ReadAsAsync&lt;string&gt;();\n            Console.WriteLine(string.Format(\"Job ID: {0}\", jobId));\n\n            \/\/ start the job\n            Console.WriteLine(\"Starting the job...\");\n            response = await client.PostAsync(BaseUrl + \"\/\" + jobId + \"\/start?api-version=2.0\", null);\n            if (!response.IsSuccessStatusCode)\n            {\n                await WriteFailedResponse(response);\n                return;\n            }\n\n            string jobLocation = BaseUrl + \"\/\" + jobId + \"?api-version=2.0\";\n            Stopwatch watch = Stopwatch.StartNew();\n            bool done = false;\n            while (!done)\n            {\n                Console.WriteLine(\"Checking the job status...\");\n                response = await client.GetAsync(jobLocation);\n                if (!response.IsSuccessStatusCode)\n                {\n                    await WriteFailedResponse(response);\n                    return;\n                }\n\n                BatchScoreStatus status = await response.Content.ReadAsAsync&lt;BatchScoreStatus&gt;();\n                if (watch.ElapsedMilliseconds &gt; TimeOutInMilliseconds)\n                {\n                    done = true;\n                    Console.WriteLine(string.Format(\"Timed out. Deleting job {0} ...\", jobId));\n                    await client.DeleteAsync(jobLocation);\n                }\n                switch (status.StatusCode) {\n                    case BatchScoreStatusCode.NotStarted:\n                        Console.WriteLine(string.Format(\"Job {0} not yet started...\", jobId));\n                        break;\n                    case BatchScoreStatusCode.Running:\n                        Console.WriteLine(string.Format(\"Job {0} running...\", jobId));\n                        break;\n                    case BatchScoreStatusCode.Failed:\n                        Console.WriteLine(string.Format(\"Job {0} failed!\", jobId));\n                        Console.WriteLine(string.Format(\"Error details: {0}\", status.Details));\n                        done = true;\n                        break;\n                    case BatchScoreStatusCode.Cancelled:\n                        Console.WriteLine(string.Format(\"Job {0} cancelled!\", jobId));\n                        done = true;\n                        break;\n                    case BatchScoreStatusCode.Finished:\n                        done = true;\n                        Console.WriteLine(string.Format(\"Job {0} finished!\", jobId));\n                        ProcessResults(status);\n                        break;\n                }\n\n                if (!done) {\n                    Thread.Sleep(1000); \/\/ Wait one second\n                }\n            }\n        }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.3,
        "Solution_reading_time":50.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":45.0,
        "Solution_word_count":338.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":29.32571,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I read that there is a way to train and host multiple models using a single endpoint for a single dataset in AWS Sagemaker. But I have 2 different datasets in S3 and have to train a model for each dataset. Can these 2 different models be hosted using a single endpoint? <\/p>",
        "Challenge_closed_time":1578755333996,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578649761440,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59679192",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":4.11,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":29.32571,
        "Challenge_title":"Hosting multiple models for multiple datasets in aws sagemaker",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":411,
        "Challenge_word_count":61,
        "Platform":"Stack Overflow",
        "Poster_created_time":1550756471932,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Yes, this is called a multi-model endpoint. You can use a large number of models on the same endpoint. They get loaded and unloaded dynamically as needed, and you simply have to pass the model name in your prediction request.<\/p>\n\n<p>Here are some resources:<\/p>\n\n<ul>\n<li><p>Blog post + example : <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/<\/a><\/p><\/li>\n<li><p>Video explaining model deployment scenarios on SageMaker: <a href=\"https:\/\/youtu.be\/dT8jmdF-ZWw\" rel=\"nofollow noreferrer\">https:\/\/youtu.be\/dT8jmdF-ZWw<\/a><\/p><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":17.5,
        "Solution_reading_time":10.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1467237684900,
        "Answerer_location":null,
        "Answerer_reputation_count":613.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":10.2121516667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to install spacy which is not available as part of the Sagemaker platform. How should can I pip install it?<\/p>",
        "Challenge_closed_time":1522941835116,
        "Challenge_comment_count":2,
        "Challenge_created_time":1522908559597,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49665241",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.2,
        "Challenge_reading_time":2.29,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":9.2431997222,
        "Challenge_title":"How do I load python modules which are not available in Sagemaker?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":2578,
        "Challenge_word_count":33,
        "Platform":"Stack Overflow",
        "Poster_created_time":1410972175307,
        "Poster_location":null,
        "Poster_reputation_count":1124.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>When creating you model, you can specify the requirements.txt as an environment variable. <\/p>\n\n<p>For Eg. <\/p>\n\n<pre><code>env = {\n    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.\n}\nsagemaker_model = TensorFlowModel(model_data = 's3:\/\/mybucket\/modelTarFile,\n                                  role = role,\n                                  entry_point = 'entry.py',\n                                  code_location = 's3:\/\/mybucket\/runtime-code\/',\n                                  source_dir = 'src',\n                                  env = env,\n                                  name = 'model_name',\n                                  sagemaker_session = sagemaker_session,\n                                 )\n<\/code><\/pre>\n\n<p>This would ensure that the requirements file is run after the docker container is created, before running any code on it. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1522945323343,
        "Solution_link_count":0.0,
        "Solution_readability":11.6,
        "Solution_reading_time":7.93,
        "Solution_score_count":10.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":160.3563522222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running custom training jobs in google's Vertex AI. A simple <code>gcloud<\/code> command to execute a custom job would use something like the following syntax (complete documentation for the command can be seen <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/beta\/ai\/custom-jobs\/create#--config\" rel=\"nofollow noreferrer\">here<\/a>):<\/p>\n<pre><code>gcloud beta ai custom-jobs create --region=us-central1 \\\n--display-name=test \\\n--config=config.yaml\n<\/code><\/pre>\n<p>In the <code>config.yaml<\/code> file, it is possible to specify the machine and accelerator (GPU) types, etc., and in my case, point to a custom container living in the Google Artifact Registry that executes the training code (specified in the <code>imageUri<\/code> part of the <code>containerSpec<\/code>). An example config file may look like this:<\/p>\n<pre><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n    acceleratorType: NVIDIA_TESLA_P100\n    acceleratorCount: 2\n  replicaCount: 1\n  containerSpec:\n    imageUri: {URI_FOR_CUSTOM_CONATINER}\n    args:\n    - {ARGS TO PASS TO CONTAINER ENTRYPOINT COMMAND}\n<\/code><\/pre>\n<p>The code we're running needs some runtime environment variables (that need to be secure) passed to the container. In the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">API documentation<\/a> for the <code>containerSpec<\/code>, it says it is possible to set environment variables as follows:<\/p>\n<pre><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n    acceleratorType: NVIDIA_TESLA_P100\n    acceleratorCount: 2\n  replicaCount: 1\n  containerSpec:\n    imageUri: {URI_FOR_CUSTOM_CONATINER}\n    args:\n    - {ARGS TO PASS TO CONTAINER ENTRYPOINT COMMAND}\n    env:\n    - name: SECRET_ONE\n      value: $SECRET_ONE\n    - name: SECRET_TWO\n      value: $SECRET_TWO\n<\/code><\/pre>\n<p>When I try and add the <code>env<\/code> flag to the <code>containerSpec<\/code>, I get an error saying it's not part of the container spec:<\/p>\n<pre><code>ERROR: (gcloud.beta.ai.custom-jobs.create) INVALID_ARGUMENT: Invalid JSON payload received. Unknown name &quot;env&quot; at 'custom_job.job_spec.worker_pool_specs[0].container_spec': Cannot find field.\n- '@type': type.googleapis.com\/google.rpc.BadRequest\n  fieldViolations:\n  - description: &quot;Invalid JSON payload received. Unknown name \\&quot;env\\&quot; at 'custom_job.job_spec.worker_pool_specs[0].container_spec':\\\n      \\ Cannot find field.&quot;\n    field: custom_job.job_spec.worker_pool_specs[0].container_spec\n<\/code><\/pre>\n<p>Any idea how to securely set runtime environment variables in Vertex AI custom jobs using custom containers?<\/p>",
        "Challenge_closed_time":1632987814700,
        "Challenge_comment_count":7,
        "Challenge_created_time":1632409414673,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1632410531832,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69302528",
        "Challenge_link_count":2,
        "Challenge_participation_count":8,
        "Challenge_readability":14.7,
        "Challenge_reading_time":35.84,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":160.6666741667,
        "Challenge_title":"How to pass environment variables to gcloud beta ai custom-jobs create with custom container (Vertex AI)",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1033,
        "Challenge_word_count":283,
        "Platform":"Stack Overflow",
        "Poster_created_time":1464106929608,
        "Poster_location":null,
        "Poster_reputation_count":457.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>There are two versions of the REST API - \u201c<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">v1<\/a>\u201d and \u201c<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1beta1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">v1beta1<\/a>\u201d where &quot;v1beta1&quot; does not have the <code>env<\/code> option in <code>ContainerSpec<\/code> but &quot;v1&quot; does. The <code>gcloud ai custom-jobs create<\/code> command without the <code>beta<\/code> parameter doesn\u2019t throw the error as it uses version \u201cv1\u201d to make the API calls.<\/p>\n<p>The environment variables from the yaml file can be passed to the custom container in the following way:<\/p>\n<p>This is the docker file of the sample custom training application I used to test the requirement. Please refer to this <a href=\"https:\/\/codelabs.developers.google.com\/vertex_custom_training_prediction\" rel=\"nofollow noreferrer\">codelab<\/a> for more information about the training application.<\/p>\n<pre class=\"lang-docker prettyprint-override\"><code>FROM gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3\nWORKDIR \/root\n\nWORKDIR \/\n\n# Copies the trainer code to the docker image.\nCOPY trainer \/trainer\n\n\n# Copies the bash script to the docker image.\nCOPY commands.sh \/scripts\/commands.sh\n\n# Bash command to make the script file an executable\nRUN [&quot;chmod&quot;, &quot;+x&quot;, &quot;\/scripts\/commands.sh&quot;]\n\n\n# Command to execute the file\nENTRYPOINT [&quot;\/scripts\/commands.sh&quot;]\n\n# Sets up the entry point to invoke the trainer.\n# ENTRYPOINT &quot;python&quot; &quot;-m&quot; $SECRET_TWO \u21d2 To use the environment variable  \n# directly in the docker ENTRYPOINT. In case you are not using a bash script, \n# the trainer can be invoked directly from the docker ENTRYPOINT.\n<\/code><\/pre>\n<br \/>\n<p>Below is the <code>commands.sh<\/code> file used in the docker container to test whether the environment variables are passed to the container.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>#!\/bin\/bash\nmkdir \/root\/.ssh\necho $SECRET_ONE\npython -m $SECRET_TWO\n<\/code><\/pre>\n<br \/>\n<p>The example <code>config.yaml<\/code> file<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n  replicaCount: 1\n  containerSpec:\n    imageUri: gcr.io\/infosys-kabilan\/mpg:v1\n    env:\n    - name: SECRET_ONE\n      value: &quot;Passing the environment variables&quot;\n    - name: SECRET_TWO\n      value: &quot;trainer.train&quot;\n<\/code><\/pre>\n<p>As the next step, I built and pushed the container to Google Container Repository. Now, the <code>gcloud ai custom-jobs create --region=us-central1  --display-name=test --config=config.yaml<\/code> can be run to create the custom training job and the output of the <code>commands.sh<\/code> file can be seen in the job logs as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qRHV7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qRHV7.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":12.1,
        "Solution_reading_time":39.15,
        "Solution_score_count":4.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":319.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1442816236550,
        "Answerer_location":null,
        "Answerer_reputation_count":230.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":0.3051033333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I just started with DVC. following are the steps I am doing to push my models on S3<\/p>\n<p>Initialize<\/p>\n<pre><code>dvc init\n<\/code><\/pre>\n<p>Add bucket url<\/p>\n<pre><code>dvc remote add -d storage s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>\n<p>add some files<\/p>\n<pre><code>dvc add somefiles\n<\/code><\/pre>\n<p>Add aws keys<\/p>\n<pre><code>dvc remote modify storage access_key_id AWS_ACCESS_KEY_ID\ndvc remote modify storage secret_access_key AWS_SECRET_ACCESS_KEY\n<\/code><\/pre>\n<p>now when I push<\/p>\n<pre><code>dvc push\n<\/code><\/pre>\n<p>it shows<\/p>\n<pre><code>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n<p>Am i missing something?<\/p>\n<p><strong>update1<\/strong><\/p>\n<p>result of <code>dvc doctor<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc doctor\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n<\/code><\/pre>\n<p>and the <code>dvc push-vv<\/code><\/p>\n<pre><code>C:\\my-server&gt;dvc push -vv  \n2021-09-21 13:21:38,382 TRACE: Namespace(all_branches=False, all_commits=False, all_tags=False, cd='.', cmd='push', cprofile=False, cprofile_dump=None, func=&lt;class 'dvc.command.data_sync.CmdDataPush'&gt;, glob=False, instrument=False, instrument_open=False, jobs=None, pdb=False, quiet=0, recursive=False, remote=None, run_cache=False, targets=[], verbose=2, version=None, with_deps=False)\n2021-09-21 13:21:39,293 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:39,296 TRACE: Assuming 'C:\\my-server\\.dvc\\cache\\02\\5b196462b86d2f10a9f659e2224da8.dir' is unchanged since \nit is read-only\n2021-09-21 13:21:40,114 DEBUG: Preparing to transfer data from '.dvc\\cache' to 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,117 DEBUG: Preparing to collect status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,119 DEBUG: Collecting status from 's3:\/\/my-bucket\/models'\n2021-09-21 13:21:40,121 DEBUG: Querying 1 hashes via object_exists\n2021-09-21 13:21:44,840 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (InvalidAccessKeyId) when calling the ListObjectsV2 operation: The AWS Access Key Id you provided does not exist in our records.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1080, in _info\n    out = await self._simple_info(path)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 993, in _simple_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: The AWS Access Key Id you provided does not exist in our records.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 248, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\aiobotocore\\client.py&quot;, line 155, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\base.py&quot;, line 45, in do_run\n    return self.run()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\command\\data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\__init__.py&quot;, line 50, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\repo\\push.py&quot;, line 48, in push\n    pushed += self.cloud.push(obj_ids, jobs, remote=remote, odb=odb)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\data_cloud.py&quot;, line 85, in push\n    return transfer(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\transfer.py&quot;, line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 160, in compare_status\n    dest_exists, dest_missing = status(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 122, in status\n    exists = hashes.intersection(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\status.py&quot;, line 48, in _indexed_dir_hashes\n    dir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 415, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 611, in result_iterator\n    yield fs.pop().result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 439, in result\n    return self.__get_result()\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\concurrent\\futures\\thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\objects\\db\\base.py&quot;, line 406, in exists_with_progress\n    ret = self.fs.exists(path_info)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\dvc\\fs\\fsspec_wrapper.py&quot;, line 97, in exists\n    return self.fs.exists(self._with_bucket(path_info))\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 88, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 69, in sync\n    raise result[0]\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\fsspec\\asyn.py&quot;, line 25, in _runner\n    result[0] = await coro\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 820, in _exists\n    await self._info(path, bucket, key, version_id=version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1084, in _info\n    out = await self._version_aware_info(path, version_id)\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 1027, in _version_aware_info\n    out = await self._call_s3(\n  File &quot;c:\\users\\sgarg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\s3fs\\core.py&quot;, line 268, in _call_s3\n    raise err\nPermissionError: Forbidden\n------------------------------------------------------------\n2021-09-21 13:21:45,178 DEBUG: Version info for developers:\nDVC version: 2.7.4 (pip)\n---------------------------------\nPlatform: Python 3.8.0 on Windows-10-10.0.19041-SP0\nSupports:\n        http (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        https (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),\n        s3 (s3fs = 2021.8.1, boto3 = 1.17.106)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-09-21 13:21:45,185 DEBUG: Analytics is enabled.\n2021-09-21 13:21:45,446 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n2021-09-21 13:21:45,456 DEBUG: Spawned '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\sgarg\\\\AppData\\\\Local\\\\Temp\\\\tmpm_p9f3eq']'\n<\/code><\/pre>",
        "Challenge_closed_time":1632210565512,
        "Challenge_comment_count":0,
        "Challenge_created_time":1632209467140,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1632236873616,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69265000",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":17.2,
        "Challenge_reading_time":127.49,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":85,
        "Challenge_solved_time":0.3051033333,
        "Challenge_title":"DVC - Forbidden: An error occurred (403) when calling the HeadObject operation",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":816,
        "Challenge_word_count":719,
        "Platform":"Stack Overflow",
        "Poster_created_time":1363322632587,
        "Poster_location":"Chandigarh, India",
        "Poster_reputation_count":13237.0,
        "Poster_view_count":2675.0,
        "Solution_body":"<p>Could you please run <code>dvc doctor<\/code> and rerun <code>dvc push<\/code> and add <code>-vv<\/code> flag. And give the two results?<\/p>\n<pre><code>PermissionError: The AWS Access Key Id you provided does not exist in our records.\n<\/code><\/pre>\n<p>Does the <code>aws cli<\/code> works correctly for you? First setup <code>AWS_ACCESS_KEY_ID<\/code> and <code>AWS_SECRET_ACCESS_KEY<\/code> in envs then<\/p>\n<pre><code>aws s3 ls s3:\/\/mybucket\/dvcstore\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1632211831430,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":6.01,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1569638810883,
        "Answerer_location":null,
        "Answerer_reputation_count":5255.0,
        "Answerer_view_count":306.0,
        "Challenge_adjusted_solved_time":5.2755416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am fairly new to AWS and Sagemaker and have decided to follow some of the tutorials Amazon has to familiarize myself with it. I've been following this one (<a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/semantic-content-recommendation-system-amazon-sagemaker\/5\/\" rel=\"nofollow noreferrer\">tutorial<\/a>) and I've realized that it's an older tutorial using Sagemaker v1. I've been able to look up and change whatever is needed for the tutorial to work in v2 but I became stuck at this part for storing the training data in a S3 bucket to deploy the model.<\/p>\n<pre><code>import io\nimport sagemaker.amazon.common as smac\n\nprint('train_features shape = ', predictions.shape)\nprint('train_labels shape = ', labels.shape)\nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, predictions, labels)\nbuf.seek(0)\n\nbucket = BUCKET\nprefix = PREFIX\nkey = 'knn\/train'\nfname = os.path.join(prefix, key)\nprint(fname)\nboto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\ns3_train_data = 's3:\/\/{}\/{}\/{}'.format(bucket, prefix, key)\nprint('uploaded training data location: {}'.format(s3_train_data))\n<\/code><\/pre>\n<p>It returns this error<\/p>\n<pre><code>NameError Traceback (most recent call \nlast)\n&lt;ipython-input-20-9e52dd949332&gt; in &lt;module&gt;\n 3\n 4\n----&gt; 5 print('train_features shape = ', predictions.shape)\n 6 print('train_labels shape = ', labels.shape)\n 7 buf = io.BytesIO()\nNameError: name 'predictions' is not defined\n<\/code><\/pre>\n<p>I'm curious as to why this would have worked in Sagemaker v1 and not v2 if predictions is not defined and if anyone could point me in the right direction for correcting this.<\/p>\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1623033557140,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623014565190,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1623040885867,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67863816",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":22.36,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":5.2755416667,
        "Challenge_title":"semantic content recommendation system with Amazon SageMaker, storing in S3",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":76,
        "Challenge_word_count":198,
        "Platform":"Stack Overflow",
        "Poster_created_time":1623013767550,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>It looks like they've left some of the code out, or changed the terminology and left in predictions by accident. predictions is an object that is defined on this page <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-test-model.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-test-model.html<\/a><\/p>\n<p>You'll have to work out what predictions is in your case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.4,
        "Solution_reading_time":5.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1589293508567,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":766.0535255556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When invoking Azure ML Batch Endpoints (creating jobs for inferencing), the run() method should return a pandas DataFrame or an array as explained <a href=\"https:\/\/i.stack.imgur.com\/azJDX.png\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<p>However this example shown, doesn't represent an output with headers for a csv, as it is often needed.<\/p>\n<p>The first thing I've tried was to return the data as a <em>pandas DataFrame<\/em> and the result is just a simple csv with a single column and without the headers.<\/p>\n<p>When trying to pass the values with several columns and it's corresponding headers, to be later saved as csv, as a result, I'm getting awkward square brackets (representing the lists in python) and the apostrophes (representing strings)<\/p>\n<p>I haven't been able to find documentation elsewhere, to fix this:\n<a href=\"https:\/\/i.stack.imgur.com\/azJDX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/azJDX.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1635509192960,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635509192960,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69768602",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":13.5,
        "Challenge_score_count":2,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to output data to Azure ML Batch Endpoint correctly using python?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":295,
        "Challenge_word_count":144,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589293508567,
        "Poster_location":null,
        "Poster_reputation_count":833.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>This is the way I found to create a clean output in csv format using python, from a batch endpoint invoke in AzureML:<\/p>\n<pre><code>def run(mini_batch):\n    batch = []\n    for file_path in mini_batch:\n        df = pd.read_csv(file_path)\n        \n        # Do any data quality verification here:\n        if 'id' not in df.columns:\n            logger.error(&quot;ERROR: CSV file uploaded without id column&quot;)\n            return None\n        else:\n            df['id'] = df['id'].astype(str)\n\n        # Now we need to create the predictions, with previously loaded model in init():\n        df['prediction'] = model.predict(df)\n        # or alternative, df[MULTILABEL_LIST] = model.predict(df)\n\n        batch.append(df)\n\n    batch_df = pd.concat(batch)\n\n    # After joining all data, we create the columns headers as a string,\n    # here we remove the square brackets and apostrophes:\n    azureml_columns = str(batch_df.columns.tolist())[1:-1].replace('\\'','')\n    result = []\n    result.append(azureml_columns)\n\n    # Now we have to parse all values as strings, row by row, \n    # adding a comma between each value\n    for row in batch_df.iterrows():\n        azureml_row = str(row[1].values).replace(' ', ',')[1:-1].replace('\\'','').replace('\\n','')\n        result.append(azureml_row)\n\n    logger.info(&quot;Finished Run&quot;)\n    return result\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1638266985652,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":15.13,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":132.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1395422283667,
        "Answerer_location":null,
        "Answerer_reputation_count":1411.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":0.6555202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed an AzureML published experiment with deployed web service. I tried to use the <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-consume-web-services\/\" rel=\"nofollow\">sample code provided in the configuration page<\/a>, but universal apps do not implement Http.Formatting yet, thus I couldn't use <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/hh944521(v=vs.118).aspx\" rel=\"nofollow\">postasjsonasync<\/a>.<\/p>\n\n<p>I tried to follow the sample code as much as possible, but I'm getting statuscode of 415 \"Unsupported Media Type\", What's the mistake I'm doing?<\/p>\n\n<pre><code>var client = new HttpClient();\nclient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n\/\/ client.BaseAddress = uri;\n\nvar scoreRequest = new\n{\n            Inputs = new Dictionary&lt;string, StringTable&gt;() {\n                    {\n                        \"dataInput\",\n                        new StringTable()\n                        {\n                            ColumnNames = new [] {\"Direction\", \"meanX\", \"meanY\", \"meanZ\"},\n                            Values = new [,] {  { \"\", x.ToString(), y.ToString(), z.ToString() },  }\n                        }\n                    },\n                },\n            GlobalParameters = new Dictionary&lt;string, string&gt;() { }\n };\n var stringContent = new StringContent(scoreRequest.ToString());\n HttpResponseMessage response = await client.PostAsync(uri, stringContent);\n<\/code><\/pre>\n\n<p>Many Thanks<\/p>",
        "Challenge_closed_time":1452007973623,
        "Challenge_comment_count":0,
        "Challenge_created_time":1452005613750,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34614582",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":16.98,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.6555202778,
        "Challenge_title":"Send request as Json on UWP",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3194,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1352139399460,
        "Poster_location":"Cyprus",
        "Poster_reputation_count":820.0,
        "Poster_view_count":256.0,
        "Solution_body":"<p>You'll need to serialize the object to a JSON string (I recommend using NewtonSoft.Json to make it easier) and set the content type accordingly. Here's an implementation I'm using in my UWP apps (note that <code>_client<\/code> is an <code>HttpClient<\/code>):<\/p>\n\n<pre><code>    public async Task&lt;HttpResponseMessage&gt; PostAsJsonAsync&lt;T&gt;(Uri uri, T item)\n    {\n        var itemAsJson = JsonConvert.SerializeObject(item);\n        var content = new StringContent(itemAsJson);\n        content.Headers.ContentType = new MediaTypeHeaderValue(\"application\/json\");\n\n        return await _client.PostAsync(uri, content);\n    }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":7.86,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":62.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1357242695640,
        "Answerer_location":null,
        "Answerer_reputation_count":1022.0,
        "Answerer_view_count":97.0,
        "Challenge_adjusted_solved_time":236.6804366667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Conceptual question.  My company is pushing Azure + DataBricks.  I am trying to understand where this can take us.<\/p>\n<p>I am porting some work I've done locally to the Azure + Databricks platform.  I want to run an experiment with a large number of hyperparameter combinations using Azure + Databricks + MLfLow.  I am using PyTorch to implement my models.<\/p>\n<p>I have a cluster with 8 nodes.  I want to kick off the parameter search across all of the nodes in an embarrassingly parallel manner (one run per node, running independently).  Is this as simple as creating a MLflow project and then using the mlflow.projects.run command for each hyperparameter combination and Databricks + MLflow will take care of the rest?<\/p>\n<p>Is this technology capable of this?  I'm looking for some references I could use to make this happen.<\/p>",
        "Challenge_closed_time":1594986774772,
        "Challenge_comment_count":1,
        "Challenge_created_time":1594133526057,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1594134725200,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62778020",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":11.22,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":237.0135319444,
        "Challenge_title":"Embarrassingly parallel hyperparameter search via Azure + DataBricks + MLFlow",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":262,
        "Challenge_word_count":140,
        "Platform":"Stack Overflow",
        "Poster_created_time":1328818302350,
        "Poster_location":"Sioux City, IA",
        "Poster_reputation_count":325.0,
        "Poster_view_count":51.0,
        "Solution_body":"<p>The short answer is yes, it's possible, but won't be exactly as easy as running a single mlflow command. You can paralelize single-node workflows using spark Python UDFs, a good example of this is this <a href=\"https:\/\/pages.databricks.com\/rs\/094-YMS-629\/images\/Fine-Grained-Time-Series-Forecasting.html?_ga=2.64430959.1760852900.1593769579-972789996.1561118598\" rel=\"nofollow noreferrer\">notebook<\/a><\/p>\n<p>I'm not sure if this will work with pytorch, but there is hyperopt library that lets you parallelize search across parameters using Spark - it's integrated with mlflow and available in databricks ML runtime. I've been using it only with scikit-learn, but it may be <a href=\"https:\/\/docs.databricks.com\/applications\/machine-learning\/automl\/hyperopt\/hyperopt-model-selection.html\" rel=\"nofollow noreferrer\">worth checking out<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":11.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":89.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1370505440848,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":38.5735138889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to avoid to use the managed policies <code>AmazonSageMakerReadOnly<\/code> and <code>AmazonSageMakerFullAccess<\/code> because I only want the users to be able to start\/stop their own notebook instance and to open jupyter in their instance.<\/p>\n<p>So far the user role has the following permissions among others<\/p>\n<pre><code>...\n        {\n            &quot;Sid&quot;: &quot;&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;sagemaker:StopNotebookInstance&quot;,\n                &quot;sagemaker:StartNotebookInstance&quot;,\n                &quot;sagemaker:CreatePresignedNotebookInstanceUrl&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEquals&quot;: {\n                    &quot;aws:ResourceTag\/OwnerRole&quot;: &quot;${aws:userid}&quot;\n                }\n            }\n        },\n  \n<\/code><\/pre>\n<p>The policy does <strong>not<\/strong> have <code>sagemaker:CreatePresignedDomainUrl<\/code> but it has <code>sagemaker:CreatePresignedNotebookInstanceUrl<\/code>, when the user with this policy click on <code>Open Jupyter<\/code> in the AWS Sagemaker console , it opens an url <code>https:\/\/xxxxxx.notebook.eu-north-1.sagemaker.aws\/auth?authToken=xxxxx<\/code> but that url will return:<\/p>\n<p><code>403 Forbidden. Access to xxxxxx.notebook.eu-north-1.sagemaker.aws was denied. You don't have authorisation to view this page. HTTP ERROR 403<\/code><\/p>\n<p>As soon as I added <code>sagemaker:CreatePresignedDomainUrl<\/code> for resource <code>*<\/code> then the 403 error <strong>was gone<\/strong> and the user could open the jupyter notebook.<\/p>\n<p>My question is <strong>why is that needed<\/strong>, and <strong>what resource should I put<\/strong> instead of <code>*<\/code>, the documentation mentions <code>arn:aws:sagemaker:regionXXX:account-idXXX:app\/domain-id\/userProfileNameXXXX\/*<\/code> but I do not have any domain or user profile.<\/p>",
        "Challenge_closed_time":1652829020203,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652690155553,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72256288",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.2,
        "Challenge_reading_time":25.16,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":38.5735138889,
        "Challenge_title":"Is sagemaker:CreatePresignedDomainUrl required to open jupyter in SageMaker notebook instance?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":173,
        "Challenge_word_count":176,
        "Platform":"Stack Overflow",
        "Poster_created_time":1237484106592,
        "Poster_location":"Stockholm, Sweden",
        "Poster_reputation_count":19085.0,
        "Poster_view_count":906.0,
        "Solution_body":"<p><code>CreatePresignedDomainUrl<\/code> statement allows the role to launch a SageMaker Studio app (and hence the <code>domain-id\/user-profile<\/code> ARN). Opening SageMaker notebook instance does not need the presigned domain url permission.<\/p>\n<p>You'll need to make sure you're tagging the notebook with an OwnerRole key, with value = userid (not username). In addition, you'll need to use the <code>sagemaker:ResourceTag<\/code> (instead of <code>aws:ResourceTag<\/code>).<\/p>\n<p>See the <a href=\"https:\/\/docs.aws.amazon.com\/service-authorization\/latest\/reference\/list_amazonsagemaker.html\" rel=\"nofollow noreferrer\">service authorization page<\/a> for a complete list of actions and condition keys.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.0,
        "Solution_reading_time":9.28,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":74.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554860971800,
        "Answerer_location":"Tokyo, Japan",
        "Answerer_reputation_count":301.0,
        "Answerer_view_count":65.0,
        "Challenge_adjusted_solved_time":22.4942855556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I would like to get the best model to use later in the notebook to predict using a different test batch.<\/p>\n\n<p>reproducible example (taken from Optuna Github) :<\/p>\n\n<pre><code>import lightgbm as lgb\nimport numpy as np\nimport sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\n\nimport optuna\n\n\n# FYI: Objective functions can take additional arguments\n# (https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#objective-func-additional-args).\ndef objective(trial):\n    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n    dtrain = lgb.Dataset(train_x, label=train_y)\n    dvalid = lgb.Dataset(valid_x, label=valid_y)\n\n    param = {\n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n\n    # Add a callback for pruning.\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"auc\")\n    gbm = lgb.train(\n        param, dtrain, valid_sets=[dvalid], verbose_eval=False, callbacks=[pruning_callback]\n    )\n\n    preds = gbm.predict(valid_x)\n    pred_labels = np.rint(preds)\n    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n    return accuracy\n\n<\/code><\/pre>\n\n<p>my understanding is that the study below will tune for accuracy. I would like to somehow retrieve the best model from the study (not just the parameters) without saving it as a pickle, I just want to use the model somewhere else in my notebook. <\/p>\n\n<pre><code>\nif __name__ == \"__main__\":\n    study = optuna.create_study(\n        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"maximize\"\n    )\n    study.optimize(objective, n_trials=100)\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n\n<\/code><\/pre>\n\n<p>desired output would be <\/p>\n\n<pre><code>best_model = ~model from above~\nnew_target_pred = best_model.predict(new_data_test)\nmetrics.accuracy_score(new_target_test, new__target_pred)\n\n<\/code><\/pre>",
        "Challenge_closed_time":1591153873300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591072505577,
        "Challenge_favorite_count":9,
        "Challenge_last_edit_time":1591072893872,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62144904",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.2,
        "Challenge_reading_time":33.18,
        "Challenge_score_count":14,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":22.6021452778,
        "Challenge_title":"Python: How to retrive the best model from Optuna LightGBM study?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":8921,
        "Challenge_word_count":229,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529932143432,
        "Poster_location":"Melbourne VIC, Australia",
        "Poster_reputation_count":525.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>I think you can use the <code>callback<\/code> argument of <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/reference\/study.html#optuna.study.Study.optimize\" rel=\"noreferrer\"><code>Study.optimize<\/code><\/a> to save the best model. In the following code example, the callback checks if a given trial is corresponding to the best trial and saves the model as a global variable <code>best_booster<\/code>.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>best_booster = None\ngbm = None\n\ndef objective(trial):\n    global gbm\n    # ...\n\ndef callback(study, trial):\n    global best_booster\n    if study.best_trial == trial:\n        best_booster = gbm\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(\n        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"maximize\"\n    )\n    study.optimize(objective, n_trials=100, callbacks=[callback])\n\n<\/code><\/pre>\n\n<p>If you define your objective function as a class, you can remove the global variables. I created a notebook as a code example. Please take a look at it:\n<a href=\"https:\/\/colab.research.google.com\/drive\/1ssjXp74bJ8bCAbvXFOC4EIycBto_ONp_?usp=sharing\" rel=\"noreferrer\">https:\/\/colab.research.google.com\/drive\/1ssjXp74bJ8bCAbvXFOC4EIycBto_ONp_?usp=sharing<\/a><\/p>\n\n<blockquote>\n  <p>I would like to somehow retrieve the best model from the study (not just the parameters) without saving it as a pickle<\/p>\n<\/blockquote>\n\n<p>FYI, if you can pickle the boosters, I think you can make the code simple by following <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/faq.html#how-to-save-machine-learning-models-trained-in-objective-functions\" rel=\"noreferrer\">this FAQ<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.3,
        "Solution_reading_time":21.14,
        "Solution_score_count":8.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":152.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1653396543887,
        "Answerer_location":"Kolkata, India",
        "Answerer_reputation_count":1231.0,
        "Answerer_view_count":290.0,
        "Challenge_adjusted_solved_time":21.0213941667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to follow <a href=\"https:\/\/datatonic.com\/insights\/dbt-vertex-ai-pipelines-google-cloud\/\" rel=\"nofollow noreferrer\">this<\/a> tutorial to run a dbt docker image as a Vertex AI component. When the pipeline runs the component just seems to sit there for ever. Is there any way of debugging the component?<\/p>",
        "Challenge_closed_time":1657175196140,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657099839053,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72881056",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":4.63,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":20.9325241667,
        "Challenge_title":"Why does my dbt container hang in Vertex AI?",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":73,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>You see the pipeline logs to get an idea regarding what is going on in the pipeline.\nFrom the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/logging\" rel=\"nofollow noreferrer\">doc<\/a><\/p>\n<blockquote>\n<p>After you define and build a pipeline, you can use Cloud Logging to create log entries to help you monitor events such as pipeline failures. You can create custom log-based metrics that send notifications when the rate of pipeline failures reaches a given threshold.<\/p>\n<\/blockquote>\n<p>You can also select a component inside Pipeline's runtime graph and then view detailed info and logs of that particular component.\n<a href=\"https:\/\/i.stack.imgur.com\/QXm02.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QXm02.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also you can hover your cursor on the component status area(green check or grey disabled icon) to check the current status of that component.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1657175516072,
        "Solution_link_count":3.0,
        "Solution_readability":11.4,
        "Solution_reading_time":12.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":124.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1550779047856,
        "Answerer_location":null,
        "Answerer_reputation_count":363.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":4538.5667388889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am getting the below error.\nDoes anyone have any idea how to solve it?<\/p>\n<pre><code>Failed to create pipeline job. Error: Vertex AI Service Agent \n'XXXXX@gcp-sa-aiplatform-cc.iam.gserviceaccount.com' should be granted\n access to the image gcr.io\/gcp-project-id\/application:latest\n<\/code><\/pre>",
        "Challenge_closed_time":1659424830192,
        "Challenge_comment_count":3,
        "Challenge_created_time":1643025892210,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1643085989932,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70833594",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":8.5,
        "Challenge_reading_time":4.54,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":4555.2605505556,
        "Challenge_title":"GCP Vertex AI Service Agent access to GCR image Error",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":707,
        "Challenge_word_count":44,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501131989640,
        "Poster_location":null,
        "Poster_reputation_count":329.0,
        "Poster_view_count":88.0,
        "Solution_body":"<p><code>{PROJECT_NUMBER}@gcp-sa-aiplatform-cc.iam.gserviceaccount.com<\/code> is google's <a href=\"https:\/\/cloud.google.com\/iam\/docs\/service-agents\" rel=\"nofollow noreferrer\">AI Platform service agent<\/a>.\nThis Service agent requires access to read\/pull the docker image from your project's gcr to create container for pipeline run.<\/p>\n<p>If You have permission to edit <a href=\"https:\/\/cloud.google.com\/iam\/docs\/understanding-roles\" rel=\"nofollow noreferrer\">IAM roles<\/a>, You can try adding <a href=\"https:\/\/cloud.google.com\/artifact-registry\/docs\/access-control#roles\" rel=\"nofollow noreferrer\">Artifact Registry roles<\/a> to the above service agent.\nYou can start with adding <code>roles\/artifactregistry.reader<\/code>.<\/p>\n<p>Hope this helps :)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.8,
        "Solution_reading_time":10.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":66.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1331657670247,
        "Answerer_location":null,
        "Answerer_reputation_count":3932.0,
        "Answerer_view_count":274.0,
        "Challenge_adjusted_solved_time":484.7536388889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>From a segmentation mask, I am trying to retrieve what labels are being represented in the mask. <\/p>\n\n<p>This is the image I am running through a semantic segmentation model in AWS Sagemaker.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/XbMMP.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XbMMP.png\" alt=\"Motorbike and everything else background\"><\/a><\/p>\n\n<p>Code for making prediction and displaying mask.<\/p>\n\n<pre><code>from sagemaker.predictor import json_serializer, json_deserializer, RealTimePredictor\nfrom sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n\n%%time\nss_predict = sagemaker.RealTimePredictor(endpoint=ss_model.endpoint_name, \n                                     sagemaker_session=sess,\n                                    content_type = 'image\/jpeg',\n                                    accept = 'image\/png')\n\nreturn_img = ss_predict.predict(img)\n\nfrom PIL import Image\nimport numpy as np\nimport io\n\nnum_labels = 21\nmask = np.array(Image.open(io.BytesIO(return_img)))\nplt.imshow(mask, vmin=0, vmax=num_labels-1, cmap='jet')\nplt.show()\n<\/code><\/pre>\n\n<p>This image is the segmentation mask that was created and it represents the motorbike and everything else is the background.<\/p>\n\n<p>[<img src=\"https:\/\/i.stack.imgur.com\/6FbVn.png\" alt=\"Segmented mask[2]\"><\/p>\n\n<p>As you can see from the code there are 21 possible labels and 2 were used in the mask, one for the motorbike and another for the background. What I would like to figure out now is how to print which labels were actually used in this mask out of the 21 possible options?<\/p>\n\n<p>Please let me know if you need any further information and any help is much appreciated. <\/p>",
        "Challenge_closed_time":1592390011563,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590644898463,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62057838",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":21.14,
        "Challenge_score_count":8,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":484.7536388889,
        "Challenge_title":"How to retrieve the labels used in a segmentation mask in AWS Sagemaker",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":489,
        "Challenge_word_count":197,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449513251820,
        "Poster_location":null,
        "Poster_reputation_count":693.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>Somewhere you should have a mapping from label integers to label classes, e.g.<\/p>\n\n<pre><code>label_map = {0: 'background', 1: 'motorbike', 2: 'train', ...}\n<\/code><\/pre>\n\n<p>If you are using the Pascal VOC dataset, that would be (1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle, 6=bus, 7=car , 8=cat, 9=chair, 10=cow, 11=diningtable, 12=dog, 13=horse, 14=motorbike, 15=person, 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv\/monitor) - see here: <a href=\"http:\/\/host.robots.ox.ac.uk\/pascal\/VOC\/voc2012\/segexamples\/index.html\" rel=\"nofollow noreferrer\">http:\/\/host.robots.ox.ac.uk\/pascal\/VOC\/voc2012\/segexamples\/index.html<\/a><\/p>\n\n<p>Then you can simply use that map:<\/p>\n\n<pre><code>used_classes = np.unique(mask)\nfor cls in used_classes:\n    print(\"Found class: {}\".format(label_map[cls]))\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":10.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":76.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1354644345812,
        "Answerer_location":null,
        "Answerer_reputation_count":3969.0,
        "Answerer_view_count":356.0,
        "Challenge_adjusted_solved_time":189.2750694445,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>I just wanted to test how good can neural network approximate multiplication function (regression task). \nI am using Azure Machine Learning Studio. I have 6500 samples, 1 hidden layer\n(I have tested 5 \/30 \/100 neurons per hidden layer), no normalization. And default parameters \n<a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn906030.aspx\" rel=\"nofollow\">Learning rate - 0.005, Number of learning iterations - 200, The initial learning weigh - 0.1,\n The momentum - 0 [description]<\/a>. I got extremely bad accuracy, close to 0.\n<em>At the same time boosted Decision forest regression shows very good approximation.<\/em><\/p>\n\n<p>What am I doing wrong? This task should be very easy for NN.<\/p>",
        "Challenge_closed_time":1465277784230,
        "Challenge_comment_count":0,
        "Challenge_created_time":1464596393980,
        "Challenge_favorite_count":3,
        "Challenge_last_edit_time":1491916922983,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37520849",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":9.1,
        "Challenge_reading_time":9.96,
        "Challenge_score_count":3,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":189.2750694445,
        "Challenge_title":"Can't approximate simple multiplication function in neural network with 1 hidden layer",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":4342,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1354644345812,
        "Poster_location":null,
        "Poster_reputation_count":3969.0,
        "Poster_view_count":356.0,
        "Solution_body":"<p>Big multiplication function gradient forces the net probably almost immediately into some horrifying state where all its hidden nodes have zero gradient.\nWe can use two approaches:<\/p>\n\n<p>1) Devide by constant. We are just deviding everything before the learning and multiply after.<\/p>\n\n<p>2) Make log-normalization. It makes multiplication into addition:<\/p>\n\n<pre><code>m = x*y =&gt; ln(m) = ln(x) + ln(y).\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1465820459196,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":5.36,
        "Solution_score_count":5.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1541802293200,
        "Answerer_location":null,
        "Answerer_reputation_count":163.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":109.1823416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The short story is, when I try to submit an azure ML pipeline run (an <em>azure ML pipeline<\/em>, not an <em>Azure pipeline<\/em>) from a jupyter notebook, I get PermissionError: [Errno 13] Permission denied: '.\\NTUSER.DAT'.  More details:<\/p>\n\n<p>Relevant code:<\/p>\n\n<pre><code>from azureml.train.automl import AutoMLConfig\nfrom azureml.train.automl.runtime import AutoMLStep\nautoml_settings = {\n    \"iteration_timeout_minutes\": 20,\n    \"experiment_timeout_minutes\": 30,\n    \"n_cross_validations\": 3,\n    \"primary_metric\": 'r2_score',\n    \"preprocess\": True,\n    \"max_concurrent_iterations\": 3,\n    \"max_cores_per_iteration\": -1,\n    \"verbosity\": logging.INFO,\n    \"enable_early_stopping\": True,\n    'time_column_name': \"DateTime\"\n}\n\nautoml_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             path = \".\",\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config,                               \n                             training_data = financeforecast_dataset,\n                             label_column_name = 'TotalUSD',\n                             **automl_settings\n                            )\n\nautoml_step = AutoMLStep(\n    name='automl_module',\n    automl_config=automl_config,\n    allow_reuse=False)\n\ntraining_pipeline = Pipeline(\n    description=\"training_pipeline\",\n    workspace=ws,    \n    steps=[automl_step])\n\ntraining_pipeline_run = Experiment(ws, 'test').submit(training_pipeline)\n<\/code><\/pre>\n\n<p>The training_pipeline step runs for apx 20 seconds, and then I get a long trace, ending in:<\/p>\n\n<pre><code>~\\AppData\\Local\\Continuum\\anaconda2\\envs\\forecasting\\lib\\site- \npackages\\azureml\\pipeline\\core\\_module_builder.py in _hash_from_file_paths(hash_src)\n    100             hasher = hashlib.md5()\n    101             for f in hash_src:\n--&gt; 102                 with open(str(f), 'rb') as afile:\n    103                     buf = afile.read()\n    104                     hasher.update(buf)\n\nPermissionError: [Errno 13] Permission denied: '.\\\\NTUSER.DAT'\n<\/code><\/pre>\n\n<p>According to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">Azure's docs on this topic<\/a>, submitting a pipeline uploads a \"snapshot\" of the \"source directory\" you specified.  Initially, I hadn't specified a source directory, so, to test that out, I added: <\/p>\n\n<pre><code>default_source_directory=\"testing\",\n<\/code><\/pre>\n\n<p>as a parameter for the training_pipeline object, but saw the same behavior when I then tried to run it.  Not sure if that is the same source directory the documentation is referring to.  The docs also say that if no source directory is specified, the \"current local directory\" is uploaded.  I used print (os.getcwd()) to get the working directory and gave \"Everyone\" full control permissions on the directory (working in a windows env).<\/p>\n\n<p>All the preceding code works fine, and I can submit an experiment if I use a ScriptRunConfig and run it on attached compute rather than using a pipeline\/training cluster.  <\/p>\n\n<p>Any ideas?  Thanks in advance to anyone who tries to help.  P.S. There is no \"azure-machine-learning-pipelines\" tag, and I can't add one because I don't have enough reputation points.  Someone else could though!  <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-ml-pipelines\" rel=\"nofollow noreferrer\">General<\/a> info on what they are.<\/p>",
        "Challenge_closed_time":1577994637207,
        "Challenge_comment_count":0,
        "Challenge_created_time":1577601580777,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59517355",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":41.38,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":109.1823416667,
        "Challenge_title":"Permission denied: '.\\NTUSER.DAT' when trying to run an Azure ML Pipeline",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":409,
        "Challenge_word_count":336,
        "Platform":"Stack Overflow",
        "Poster_created_time":1541802293200,
        "Poster_location":null,
        "Poster_reputation_count":163.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>I resolved this answer by setting the path and the data_script variables in the AutoMLConfig task object, like this (relevant code indicated by -->):<\/p>\n\n<pre><code>automl_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config,\n                             --&gt;path = \"c:\\\\users\\\\me\",\n                             data_script =\"script.py\",&lt;--\n                             **automl_settings\n                            )\n<\/code><\/pre>\n\n<p>Setting the data_script variable to include the full path, as shown below, did not work.<\/p>\n\n<pre><code>automl_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             path = \".\",\n                             --&gt;data_script = \"c:\\\\users\\\\me\\\\script.py\"&lt;--\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config, \n                             **automl_settings\n                            )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.7,
        "Solution_reading_time":10.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":6.8229247222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to figure out how viable Azure ML in production; I would like to accomplish the following:<\/p>\n\n<ol>\n<li>Specify <em>custom environments<\/em> for my pipelines using a <em>pip file<\/em> and use them in a pipeline<\/li>\n<li><em>Declaratively<\/em> specify my workspace, environments and pipelines in an <em>Azure DevOps repo<\/em><\/li>\n<li><em>Reproducibly<\/em> deploy my Azure ML workspace to my subscription using an <em>Azure DevOps pipeline<\/em><\/li>\n<\/ol>\n\n<p>I found an <a href=\"https:\/\/stackoverflow.com\/questions\/60506398\/how-do-i-use-an-environment-in-an-ml-azure-pipeline\">explanation of how to specify environments using notebooks<\/a> but this seems ill-suited for the second and third requirements I have.<\/p>",
        "Challenge_closed_time":1583340584892,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583316022363,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1583348292107,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60523435",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.4,
        "Challenge_reading_time":10.52,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":6.8229247222,
        "Challenge_title":"How do I version control Azure ML workspaces with custom environments and pipelines?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":659,
        "Challenge_word_count":98,
        "Platform":"Stack Overflow",
        "Poster_created_time":1317398821727,
        "Poster_location":null,
        "Poster_reputation_count":1263.0,
        "Poster_view_count":44.0,
        "Solution_body":"<p>Currently, we have a python script, <code>pipeline.py<\/code> that uses the <code>azureml-sdk<\/code>to create, register and run all of our ML artifacts (envs, pipelines, models). We call this script in our Azure DevOps CI pipeline with a Python Script task after building the right pip env from the requirements file in our repo.<\/p>\n\n<p>However, it is worth noting there is YAML support for ML artifact definition. Though I don't know if the existing support will cover all of your bases (though that is the plan).<\/p>\n\n<p>Here's some great docs from MSFT to get you started:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/microsoft\/MLOpsPython\" rel=\"nofollow noreferrer\">GitHub Template repo of an end-to-end example of ML pipeline + deployment<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">How to define\/create an environment (using Pip or Conda) and use it in a remote compute context<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/targets\/azure-machine-learning?context=azure%2Fmachine-learning%2Fservice%2Fcontext%2Fml-context&amp;view=azure-devops&amp;tabs=yaml\" rel=\"nofollow noreferrer\">Azure Pipelines guidance on CI\/CD for ML Service<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-pipeline-yaml\" rel=\"nofollow noreferrer\">Defining ML pipelines in YAML<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1583340824232,
        "Solution_link_count":4.0,
        "Solution_readability":13.8,
        "Solution_reading_time":18.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":148.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1370085456943,
        "Answerer_location":null,
        "Answerer_reputation_count":344.0,
        "Answerer_view_count":35.0,
        "Challenge_adjusted_solved_time":27.1951413889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have some python code that trains a Neural Network using tensorflow. <\/p>\n\n<p>I've created a docker image based on a tensorflow\/tensorflow:latest-gpu-py3 image that runs my python script.\nWhen I start an EC2 p2.xlarge instance I can run my docker container using the command<\/p>\n\n<pre><code>docker run --runtime=nvidia cnn-userpattern train\n<\/code><\/pre>\n\n<p>and the container with my code runs with no errors and uses the host GPU. <\/p>\n\n<p>The problem is, when I try to run the same container in an AWS Sagemaker training job with instance ml.p2.xlarge (I also tried with ml.p3.2xlarge), the algorithm fails with error code:<\/p>\n\n<blockquote>\n  <p>ImportError: libcuda.so.1: cannot open shared object file: No such file or directory<\/p>\n<\/blockquote>\n\n<p>Now I know what that error code means. It means that the runtime environment of the docker host is not set to \"nvidia\". The AWS documentation says that the command used to run the docker image is always<\/p>\n\n<pre><code>docker run image train\n<\/code><\/pre>\n\n<p>which would work if the default runtime is set to \"nvidia\" in the docker\/deamon.json. Is there any way to edit the host deamon.json or tell docker in the Dockerfile to use \"--runtime=nvidia\"?<\/p>",
        "Challenge_closed_time":1551964655196,
        "Challenge_comment_count":0,
        "Challenge_created_time":1551866752687,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55020390",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":16.04,
        "Challenge_score_count":2,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":27.1951413889,
        "Challenge_title":"How do I start an AWS Sagemaker training job with GPU access in my docker container?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1858,
        "Challenge_word_count":201,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370085456943,
        "Poster_location":null,
        "Poster_reputation_count":344.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>With some help of the AWS support service we were able to find the problem.\nThe docker image I used to run my code on was, as I said tensorflow\/tensorflow:latest-gpu-py3 (available on <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a>)<\/p>\n\n<p>the \"latest\" tag refers to version 1.12.0 at this time. The problem was not my own, but with this version of the docker image. <\/p>\n\n<p>If I base my docker image on tensorflow\/tensorflow:1.10.1-gpu-py3, it runs as it should and uses the GPU fully. <\/p>\n\n<p>Apparently the default runtime is set to \"nvidia\" in the docker\/deamon.json on all GPU instances of AWS sagemaker.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.3,
        "Solution_reading_time":9.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":98.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1455667285907,
        "Answerer_location":null,
        "Answerer_reputation_count":283.0,
        "Answerer_view_count":85.0,
        "Challenge_adjusted_solved_time":97.8512702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a set of pre-processing stages in sklearn <code>Pipeline<\/code> and an estimator which is a <code>KerasClassifier<\/code> (<code>from tensorflow.keras.wrappers.scikit_learn import KerasClassifier<\/code>).<\/p>\n<p>My overall goal is to tune and log the whole sklearn pipeline in <code>mlflow<\/code> (in databricks evn). I get a confusing type error which I can't figure out how to reslove:<\/p>\n<blockquote>\n<p>TypeError: can't pickle _thread.RLock objects<\/p>\n<\/blockquote>\n<p>I have the following code (without tuning stage) which returns the above error:<\/p>\n<pre><code>conda_env = _mlflow_conda_env(\n    additional_conda_deps=None,\n    additional_pip_deps=[\n        &quot;cloudpickle=={}&quot;.format(cloudpickle.__version__),\n        &quot;scikit-learn=={}&quot;.format(sklearn.__version__),\n        &quot;numpy=={}&quot;.format(np.__version__),\n        &quot;tensorflow=={}&quot;.format(tf.__version__),\n    ],\n    additional_conda_channels=None,\n)\n\nsearch_space = {\n    &quot;estimator__dense_l1&quot;: 20,\n    &quot;estimator__dense_l2&quot;: 20,\n    &quot;estimator__learning_rate&quot;: 0.1,\n    &quot;estimator__optimizer&quot;: &quot;Adam&quot;,\n}\n\n\ndef create_model(n):\n\n    model = Sequential()\n    model.add(Dense(int(n[&quot;estimator__dense_l1&quot;]), activation=&quot;relu&quot;))\n    model.add(Dense(int(n[&quot;estimator__dense_l2&quot;]), activation=&quot;relu&quot;))\n    model.add(Dense(1, activation=&quot;sigmoid&quot;))\n    model.compile(\n        loss=&quot;binary_crossentropy&quot;,\n        optimizer=n[&quot;estimator__optimizer&quot;],\n        metrics=[&quot;accuracy&quot;],\n    )\n\n    return model\n\n\nmlflow.sklearn.autolog()\nwith mlflow.start_run(nested=True) as run:\n\n    classfier = KerasClassifier(build_fn=create_model, n=search_space)\n    # fit the pipeline\n    clf = Pipeline(steps=[(&quot;preprocessor&quot;, preprocessor), \n                          (&quot;estimator&quot;, classfier)])\n    h = clf.fit(\n        X_train,\n        y_train.values,\n        estimator__validation_split=0.2,\n        estimator__epochs=10,\n        estimator__verbose=2,\n    )\n\n    # log scores\n    acc_score = clf.score(X=X_test, y=y_test)\n    mlflow.log_metric(&quot;accuracy&quot;, acc_score)\n\n    signature = infer_signature(X_test, clf.predict(X_test))\n    # Log the model with a signature that defines the schema of the model's inputs and outputs.\n    mlflow.sklearn.log_model(\n        sk_model=clf, artifact_path=&quot;model&quot;, \n        signature=signature, \n        conda_env=conda_env\n    )\n<\/code><\/pre>\n<p>I also get this warning before the error:<\/p>\n<pre><code>\n    WARNING mlflow.sklearn.utils: Truncated the value of the key `steps`. Truncated value: `[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n                      transformer_weights=None,\n                      transformers=[('num',\n                                   Pipeline(memory=None,\n<\/code><\/pre>\n<p>note the the whole pipeline runs outside mlflow.\ncan someone help?<\/p>",
        "Challenge_closed_time":1631593268676,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631241004103,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69126555",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":36.8,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":97.8512702778,
        "Challenge_title":"how to log KerasClassifier model in a sklearn pipeline mlflow?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":435,
        "Challenge_word_count":209,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455667285907,
        "Poster_location":null,
        "Poster_reputation_count":283.0,
        "Poster_view_count":85.0,
        "Solution_body":"<p>I think I find sort of a workaround\/solution for this for now, but I think this issue needs to be addressed in MLFloow anyways.<\/p>\n<p>What I did is not the best way probably.\nI used a python package called <a href=\"https:\/\/scikeras.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">scikeras<\/a> that does this wrapping and then could log the model<\/p>\n<p>The code:<\/p>\n<pre><code>import scikeras \nimport tensorflow as tf \nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Flatten, Activation \n \nfrom scikeras.wrappers import KerasClassifier \n  \n \nclass ModelWrapper(mlflow.pyfunc.PythonModel): \n    def __init__(self, model): \n        self.model = model \n \n    def predict(self, context, model_input): \n        return self.model.predict(model_input) \n \nconda_env =  _mlflow_conda_env( \n      additional_conda_deps=None, \n      additional_pip_deps=[ \n        &quot;cloudpickle=={}&quot;.format(cloudpickle.__version__),  \n        &quot;scikit-learn=={}&quot;.format(sklearn.__version__), \n        &quot;numpy=={}&quot;.format(np.__version__), \n        &quot;tensorflow=={}&quot;.format(tf.__version__), \n        &quot;scikeras=={}&quot;.format(scikeras.__version__), \n      ], \n      additional_conda_channels=None, \n  ) \n \nparam = { \n   &quot;dense_l1&quot;: 20, \n   &quot;dense_l2&quot;: 20, \n   &quot;optimizer__learning_rate&quot;: 0.1, \n   &quot;optimizer&quot;: &quot;Adam&quot;, \n   &quot;loss&quot;:&quot;binary_crossentropy&quot;, \n} \n \n  \ndef create_model(dense_l1, dense_l2, meta): \n  \n  n_features_in_ = meta[&quot;n_features_in_&quot;] \n  X_shape_ = meta[&quot;X_shape_&quot;] \n  n_classes_ = meta[&quot;n_classes_&quot;] \n \n  model = Sequential() \n  model.add(Dense(n_features_in_, input_shape=X_shape_[1:], activation=&quot;relu&quot;)) \n  model.add(Dense(dense_l1, activation=&quot;relu&quot;)) \n  model.add(Dense(dense_l2, activation=&quot;relu&quot;)) \n  model.add(Dense(1, activation=&quot;sigmoid&quot;)) \n \n  return model   \n \nmlflow.sklearn.autolog() \nwith mlflow.start_run(run_name=&quot;sample_run&quot;): \n \n  classfier = KerasClassifier( \n    create_model, \n    loss=param[&quot;loss&quot;], \n    dense_l1=param[&quot;dense_l1&quot;], \n    dense_l2=param[&quot;dense_l2&quot;], \n    optimizer__learning_rate = param[&quot;optimizer__learning_rate&quot;], \n    optimizer= param[&quot;optimizer&quot;], \n) \n \n  # fit the pipeline \n  clf = Pipeline(steps=[('preprocessor', preprocessor), \n                      ('estimator', classfier)])   \n \n  h = clf.fit(X_train, y_train.values) \n  # log scores \n  acc_score = clf.score(X=X_test, y=y_test) \n  mlflow.log_metric(&quot;accuracy&quot;, acc_score) \n  signature = infer_signature(X_test, clf.predict(X_test)) \n  model_nn = ModelWrapper(clf,)  \n \n  mlflow.pyfunc.log_model( \n      python_model= model_nn, \n      artifact_path = &quot;model&quot;,  \n      signature = signature,  \n      conda_env = conda_env \n  ) \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.3,
        "Solution_reading_time":35.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":180.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1370505440848,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":29.4236144444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I use visual studio code with jupyter notebook, I have an &quot;outline&quot; tab in the left panels that display the Markdown section of my notebook for quick access.<\/p>\n<p>But in Sagemaker studio I don't have this and I would like to add it.<\/p>",
        "Challenge_closed_time":1652459529952,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652353604940,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72214443",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":3.98,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":29.4236144444,
        "Challenge_title":"Get notebook outline in sagemaker studio like in Visual Studio Code",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":164,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_created_time":1576136255052,
        "Poster_location":null,
        "Poster_reputation_count":795.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>The 'Outline' tab (Table of Contents extension in Jupyter) is not available for Studio yet.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi.html\" rel=\"nofollow noreferrer\">SageMaker notebook instances<\/a> come with the extension prebuilt.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.8,
        "Solution_reading_time":3.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1546882781360,
        "Answerer_location":null,
        "Answerer_reputation_count":106.0,
        "Answerer_view_count":41.0,
        "Challenge_adjusted_solved_time":1144.8089597222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The output folder of an annotation job contains the following file structure: <\/p>\n\n<ul>\n<li><p>active learning<\/p><\/li>\n<li><p>annotation-tools<\/p><\/li>\n<li><p>annotations<\/p><\/li>\n<li><p>intermediate<\/p><\/li>\n<li><p>manifests<\/p><\/li>\n<\/ul>\n\n<p>Each line of the manifests\/output\/output.manifest file is a dictionary, where the key 'jobname' contains information about the annotations, and the key 'jobname-metadata' contains confidence score and other information about each of the bounding box annotations. There is also another folder called annotations which contain json files which contain information about annotations and associated worker ids. How are the two annotation informations related to each other? Is there any blogs\/tutorials which discuss how to interpret the data received from amazon sagemaker ground-truth service? Thanks in advance. <\/p>\n\n<p>Links I referred to: \n1. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-output.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-output.html<\/a>\n2. <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb<\/a> <\/p>\n\n<p>I have displayed the annotations received using the code available in the link 2 <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, which treats consolidated annotations and worker response separately.<\/p>",
        "Challenge_closed_time":1566235546552,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562114234297,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56861525",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":22.5,
        "Challenge_reading_time":25.13,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1144.8089597222,
        "Challenge_title":"Interpretation of output of bounding box annotation job",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":199,
        "Challenge_word_count":155,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562111269163,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Thank you for your question. I\u2019m the product manager for Amazon SageMaker Ground Truth and am happy to answer your question here.<\/p>\n\n<p>We have a feature called annotation consolidation that takes the responses from multiple workers for a single image and then consolidates those responses into a single set of bounding boxes for the image. The bounding boxes referenced in the manifest file are the consolidated responses whereas what you see in the annotations folders are the raw annotations (which is why you have the respective worker IDs). <\/p>\n\n<p>You can find out more about the annotation consolidation feature here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-annotation-consolidation.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-annotation-consolidation.html<\/a><\/p>\n\n<p>Please let us know if you have any further questions.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.4,
        "Solution_reading_time":11.52,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":114.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1526889513900,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":24.1450022222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to write a small program using the AzureML Python SDK (v1.0.85) to register an Environment in AMLS and use that definition to construct a local Conda environment when experiments are being run (for a pre-trained model). The code works fine for simple scenarios where all dependencies are loaded from Conda\/ public PyPI, but when I introduce a private dependency (e.g. a utils library) I am getting a InternalServerError with the message \"Error getting recipe specifications\".<\/p>\n\n<p>The code I am using to register the environment is (after having authenticated to Azure and connected to our workspace):<\/p>\n\n<pre><code>environment_name = config['environment']['name']\npy_version = \"3.7\"\nconda_packages = [\"pip\"]\npip_packages = [\"azureml-defaults\"]\nprivate_packages = [\".\/env-wheels\/utils-0.0.3-py3-none-any.whl\"]\n\nprint(f\"Creating environment with name {environment_name}\")\nenvironment = Environment(name=environment_name)\nconda_deps = CondaDependencies()\n\nprint(f\"Adding Python version: {py_version}\")\nconda_deps.set_python_version(py_version)\n\nfor conda_pkg in conda_packages:\n    print(f\"Adding Conda denpendency: {conda_pkg}\")\n    conda_deps.add_conda_package(conda_pkg)\n\nfor pip_pkg in pip_packages:\n    print(f\"Adding Pip dependency: {pip_pkg}\")\n    conda_deps.add_pip_package(pip_pkg)\n\nfor private_pkg in private_packages:\n    print(f\"Uploading private wheel from {private_pkg}\")\n    private_pkg_url = Environment.add_private_pip_wheel(workspace=ws, file_path=Path(private_pkg).absolute(), exist_ok=True)\n    print(f\"Adding private Pip dependency: {private_pkg_url}\")\n    conda_deps.add_pip_package(private_pkg_url)\n\nenvironment.python.conda_dependencies = conda_deps\nenvironment.register(workspace=ws)\n<\/code><\/pre>\n\n<p>And the code I am using to create the local Conda environment is:<\/p>\n\n<pre><code>amls_environment = Environment.get(ws, name=environment_name, version=environment_version)\n\nprint(f\"Building environment...\")\namls_environment.build_local(workspace=ws)\n<\/code><\/pre>\n\n<p>The exact error message being returned when <code>build_local(...)<\/code> is called is:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"C:\\Anaconda\\envs\\AMLSExperiment\\lib\\site-packages\\azureml\\core\\environment.py\", line 814, in build_local\n    raise error\n  File \"C:\\Anaconda\\envs\\AMLSExperiment\\lib\\site-packages\\azureml\\core\\environment.py\", line 807, in build_local\n    recipe = environment_client._get_recipe_for_build(name=self.name, version=self.version, **payload)\n  File \"C:\\Anaconda\\envs\\AMLSExperiment\\lib\\site-packages\\azureml\\_restclient\\environment_client.py\", line 171, in _get_recipe_for_build\n    raise Exception(message)\nException: Error getting recipe specifications. Code: 500\n: {\n  \"error\": {\n    \"code\": \"ServiceError\",\n    \"message\": \"InternalServerError\",\n    \"detailsUri\": null,\n    \"target\": null,\n    \"details\": [],\n    \"innerError\": null,\n    \"debugInfo\": null\n  },\n  \"correlation\": {\n    \"operation\": \"15043e1469e85a4c96a3c18c45a2af67\",\n    \"request\": \"19231be75a2b8192\"\n  },\n  \"environment\": \"westeurope\",\n  \"location\": \"westeurope\",\n  \"time\": \"2020-02-28T09:38:47.8900715+00:00\"\n}\n\nProcess finished with exit code 1\n<\/code><\/pre>\n\n<p>Has anyone seen this error before or able to provide some guidance around what the issue may be?<\/p>",
        "Challenge_closed_time":1583243758048,
        "Challenge_comment_count":1,
        "Challenge_created_time":1583156836040,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60490195",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":42.95,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":24.1450022222,
        "Challenge_title":"Unable to build local AMLS environment with private wheel",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":239,
        "Challenge_word_count":291,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526889513900,
        "Poster_location":null,
        "Poster_reputation_count":81.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The issue was with out firewall blocking the required requests between AMLS and the storage container (I presume to get the environment definitions\/ private wheels).<\/p>\n\n<p>We resolved this by updating the firewall with appropriate ALLOW rules for the AMLS service to contact and read from the attached storage container.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":4.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":49.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1511812251067,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":98.2814944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I just started with using the Azure Machine Learning Services and ran into this problem. Creating a local environment and deploying my model to localhost works perfectly fine. \nCan anyone identify what could have caused this error, because i do not know where to start..<\/p>\n\n<p>I tried to create a cluster for Location \"eastus2\" aswell, which caused the same error.\nThank you very much in advance!<\/p>\n\n<p>Btw, the ressource group and ressources are being created into my azure account.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wYwJD.png\" rel=\"nofollow noreferrer\">Image of error<\/a><\/p>",
        "Challenge_closed_time":1511812251067,
        "Challenge_comment_count":1,
        "Challenge_created_time":1511458774783,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47460981",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":7.5,
        "Challenge_reading_time":8.59,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":98.1878566667,
        "Challenge_title":"Error when creating cluster environment for azureML: \"Failed to get scoring front-end info\"",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":381,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1511458091352,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Ashvin [MSFT]<\/p>\n\n<p>Sorry to hear that you were facing issues. We checked logs on our side using the info you provided in the screenshot. The cluster setup failed because there weren't enough cores to fit AzureML and system components in the cluster. You specified agent-vm-size of D1v2 which has 1 CPU core. By default we create 2 agents so total cores were 2. To resolve, can you please try creating a new cluster without specifying agent size? Then AzureML will create 2 agents of D3v2 which is 8 cores total. This should fit the AzureML and system components and leave some room for you to deploy your services. <\/p>\n\n<p>If you wish a bigger cluster you could specify agent-count along with agent-vm-size to appropriately size your cluster but please have minimum total of 8 cores with each individual VM >= 2 cores to ensure cluster works smoothly. Hope this helps.<\/p>\n\n<p>We are working on our side to add error handling to ensure request fails with clear error message. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1511812588163,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":11.99,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":168.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1285219808283,
        "Answerer_location":"Perth WA, Australia",
        "Answerer_reputation_count":6770.0,
        "Answerer_view_count":1127.0,
        "Challenge_adjusted_solved_time":0.0213536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm currently trying to integrate an ML model currently deployed as a webservice on AzureML with PowerBI.<\/p>\n<p>I see that it can be <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#invoking-the-azure-ml-model-in-power-bi\" rel=\"nofollow noreferrer\">integrated<\/a> but the model requires the addition of a schema file when it is <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#schema-discovery-for-machine-learning-models\" rel=\"nofollow noreferrer\">being deployed as a webservice<\/a>. Without this, the model can't be viewed in PowerBI.<\/p>\n<p>The problem that I have come up against is that I use MLflow to log ML model performances and subsequently to deploy a selected model onto AzureML as a webservice using MLflow's AzureML integration - mlflow.azureml.deploy(). This unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in PowerBI as it lacks the required schema file.<\/p>\n<p>My options seem to be:<\/p>\n<ol>\n<li>Find a workaround, possibly using the working <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-serving\" rel=\"nofollow noreferrer\">REST api of the model in a power query<\/a>.<\/li>\n<li>Rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.<\/li>\n<\/ol>\n<p>I thought I would ask to see if I am maybe missing something as I can't find a workaround using my current code to define a schema file in MLflow when deploying with mlflow.azureml.deploy().<\/p>",
        "Challenge_closed_time":1600604920243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600261190477,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1600855880503,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63920599",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":21.83,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":95.4804905556,
        "Challenge_title":"PowerBI and MLflow integration (through AzureML)",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":405,
        "Challenge_word_count":204,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600260166047,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.<\/p>\n<p>Scoring code is something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nfrom mlflow.pyfunc import load_model\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef init():\n    global model\n    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))\n\n@input_schema('data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    return model.predict(data)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1600855957376,
        "Solution_link_count":0.0,
        "Solution_readability":16.9,
        "Solution_reading_time":10.68,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1342713532568,
        "Answerer_location":"Seoul, South Korea",
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":3.9274925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In DVC one may define pipelines.  In Unix, one typically does not work at the root level.  Further, DVC expects files to be inside the git repository.<\/p>\n<p>So, this seems like a typical problem.<\/p>\n<p>Suppose I have the following:<\/p>\n<pre><code>\/home\/user\/project\/content-folder\/data\/data-type\/cfg.json\n\/home\/user\/project\/content-folder\/app\/foo.py\n<\/code><\/pre>\n<p>Git starts at <code>\/home\/user\/project\/<\/code><\/p>\n<pre><code>cd ~\/project\/content-folder\/data\/data-type\n..\/..\/app\/foo.py do-this --with cfg.json --dest $(pwd) \n<\/code><\/pre>\n<p>Seems reasonable to me: the script takes a configuration, which is stored in a particular location, runs it against some encapsulated functionality, and outputs it to the destination using an absolute path.<\/p>\n<p>The default behavior of <code>--dest<\/code> is to output to the current working directory.  This seems like another reasonable default.<\/p>\n<hr \/>\n<p>Next, I go to configure the <code>params.yaml<\/code> file for <code>dvc<\/code>, and I am immediately confusing and unsure what is going to happen.  I write:<\/p>\n<pre><code>foodoo:\n  params: do-this --with ????\/cfg.json --dest ????\n<\/code><\/pre>\n<p>What I want to write (and would in a shell script):<\/p>\n<pre><code>#!\/usr\/bin\/env bash\norigin:=$(git rev-parse --show-toplevel)\n\nverb=do-this\nparams=--with $(origin)\/content-folder\/data\/data-type\/cfg.json --dest $(origin)\/content-folder\/data\/data-type\n<\/code><\/pre>\n<hr \/>\n<p>But, in DVC, the pathing seems to be implicit, and I do not know where to start as either:<\/p>\n<ol>\n<li>DVC will calculate the path to my script locally<\/li>\n<li>Not calculate the path to my script locally<\/li>\n<\/ol>\n<p>Which is fine -- I can discover that.  But I am reasonably sure that DVC will absolutely not prefix the directory and file params in my params.yaml with the path to my project.<\/p>\n<hr \/>\n<p>How does one achieve path control that does not assume a fixed project location, like I would in BASH?<\/p>",
        "Challenge_closed_time":1608686869860,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608672730887,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65416056",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":25.9,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":3.9274925,
        "Challenge_title":"Data Version Control: Absolute Paths and Project Paths in the Pipeline Parameters?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":448,
        "Challenge_word_count":262,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405262190020,
        "Poster_location":"Atlanta, GA",
        "Poster_reputation_count":26244.0,
        "Poster_view_count":1383.0,
        "Solution_body":"<p>By default, DVC will run your stage command from the same directory as the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files#dvcyaml-file\" rel=\"nofollow noreferrer\">dvc.yaml<\/a> file. If you need to run the command from a different location, you can specify an alternate working directory via <code>wdir<\/code>, which should be a path relative to <code>dvc.yaml<\/code>'s location.<\/p>\n<p>Paths for everything else in your stage (like <code>params.yaml<\/code>) should be specified as relative to <code>wdir<\/code> (or relative to <code>dvc.yaml<\/code> if <code>wdir<\/code> is not provided).<\/p>\n<p>Looking at your example, there also seems to be a bit of confusion on parameters in DVC. In a DVC stage, <code>params<\/code> is for specifying <a href=\"https:\/\/dvc.org\/doc\/command-reference\/params\" rel=\"nofollow noreferrer\">parameter dependencies<\/a>, not used for specifying command-line flags. The full command including flags\/options should be included  the <code>cmd<\/code> section for your stage. If you wanted to make sure that your stage was rerun every time certain values in <code>cfg.json<\/code> have changed, your stage's <code>params<\/code> section would look something like:<\/p>\n<pre><code>params:\n  &lt;relpath from dvc.yaml&gt;\/cfg.json:\n    - param1\n    - param2\n    ...\n<\/code><\/pre>\n<p>So your example <code>dvc.yaml<\/code> would look something like:<\/p>\n<pre><code>stages:\n  foodoo:\n    cmd: &lt;relpath from dvc.yaml&gt;\/foo.py do-this --with &lt;relpath from dvc.yaml&gt;\/cfg.json --dest &lt;relpath from dvc.yaml&gt;\/...\n    deps:\n      &lt;relpath from dvc.yaml&gt;\/foo.py\n    params:\n      &lt;relpath from dvc.yaml&gt;\/cfg.json:\n        ...\n    ...\n<\/code><\/pre>\n<p>This would make the command <code>dvc repro<\/code> rerun your stage any time that the code in foo.py has changed, or the specified parameters in <code>cfg.json<\/code> have changed.<\/p>\n<p>You may also want to refer to the docs for <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#run\" rel=\"nofollow noreferrer\">dvc run<\/a>, which can be used to generate or update a <code>dvc.yaml<\/code> stage (rather than writing <code>dvc.yaml<\/code> by hand)<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":9.4,
        "Solution_reading_time":26.99,
        "Solution_score_count":2.0,
        "Solution_sentence_count":26.0,
        "Solution_word_count":248.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1348082104976,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":9564.0,
        "Answerer_view_count":894.0,
        "Challenge_adjusted_solved_time":3234.1247702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have hundreds of CSV files that I want to process similarly. For simplicity, we can assume that they are all in <code>.\/data\/01_raw\/<\/code> (like <code>.\/data\/01_raw\/1.csv<\/code>, <code>.\/data\/02_raw\/2.csv<\/code>) etc. I would much rather not give each file a different name and keep track of them individually when building my pipeline. I would like to know if there is any way to read all of them in bulk by specifying something in the <code>catalog.yml<\/code> file?<\/p>",
        "Challenge_closed_time":1588804881827,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588799145203,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":1588803043230,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61645397",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":6.58,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.5935066667,
        "Challenge_title":"How do I add many CSV files to the catalog in Kedro?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":806,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1453233461910,
        "Poster_location":null,
        "Poster_reputation_count":299.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>You are looking for <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html#partitioned-dataset\" rel=\"nofollow noreferrer\">PartitionedDataSet<\/a>. In your example, the <code>catalog.yml<\/code> might look like this:<\/p>\n<pre><code>my_partitioned_dataset:\n  type: &quot;PartitionedDataSet&quot;\n  path: &quot;data\/01_raw&quot;\n  dataset: &quot;pandas.CSVDataSet&quot;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1600445892403,
        "Solution_link_count":1.0,
        "Solution_readability":21.1,
        "Solution_reading_time":5.42,
        "Solution_score_count":8.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":25.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1498252453503,
        "Answerer_location":"USA",
        "Answerer_reputation_count":399.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":298.7422783333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to build a hyperparameter optimization job in Amazon Sagemaker, in python, but something is not working. Here is what I have:<\/p>\n\n<pre><code>sess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n                                    role, \n                                    train_instance_count=1, \n                                    train_instance_type='ml.m4.4xlarge',\n                                    output_path=output_path_1,\n                                    base_job_name='HPO-xgb',\n                                    sagemaker_session=sess)\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter    \n\nhyperparameter_ranges = {'eta': ContinuousParameter(0.01, 0.2),\n                         'num_rounds': ContinuousParameter(100, 500),\n                         'num_class':  4,\n                         'max_depth': IntegerParameter(3, 9),\n                         'gamma': IntegerParameter(0, 5),\n                         'min_child_weight': IntegerParameter(2, 6),\n                         'subsample': ContinuousParameter(0.5, 0.9),\n                         'colsample_bytree': ContinuousParameter(0.5, 0.9)}\n\nobjective_metric_name = 'validation:mlogloss'\nobjective_type='minimize'\nmetric_definitions = [{'Name': 'validation-mlogloss',\n                       'Regex': 'validation-mlogloss=([0-9\\\\.]+)'}]\n\ntuner = HyperparameterTuner(xgb,\n                            objective_metric_name,\n                            objective_type,\n                            hyperparameter_ranges,\n                            metric_definitions,\n                            max_jobs=9,\n                            max_parallel_jobs=3)\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_validation}) \n<\/code><\/pre>\n\n<p>And the error I get is: <\/p>\n\n<pre><code>AttributeError: 'str' object has no attribute 'keys'\n<\/code><\/pre>\n\n<p>The error seems to come from the <code>tuner.py<\/code> file:<\/p>\n\n<pre><code>----&gt; 1 tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in fit(self, inputs, job_name, **kwargs)\n    144             self.estimator._prepare_for_training(job_name)\n    145 \n--&gt; 146         self._prepare_for_training(job_name=job_name)\n    147         self.latest_tuning_job = _TuningJob.start_new(self, inputs)\n    148 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in _prepare_for_training(self, job_name)\n    120 \n    121         self.static_hyperparameters = {to_str(k): to_str(v) for (k, v) in self.estimator.hyperparameters().items()}\n--&gt; 122         for hyperparameter_name in self._hyperparameter_ranges.keys():\n    123             self.static_hyperparameters.pop(hyperparameter_name, None)\n    124 \n\nAttributeError: 'list' object has no attribute 'keys'                           \n<\/code><\/pre>",
        "Challenge_closed_time":1530057890672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529660522477,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1530173488990,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50985138",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":23.8,
        "Challenge_reading_time":31.2,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":110.3800541667,
        "Challenge_title":"Sagemaker Hyperparameter Optimization XGBoost",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1523,
        "Challenge_word_count":168,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432680790120,
        "Poster_location":null,
        "Poster_reputation_count":455.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>Your arguments when initializing the HyperparameterTuner object are in the wrong order. The constructor has the following signature:<\/p>\n\n<pre><code>HyperparameterTuner(estimator, \n                    objective_metric_name, \n                    hyperparameter_ranges, \n                    metric_definitions=None, \n                    strategy='Bayesian', \n                    objective_type='Maximize', \n                    max_jobs=1, \n                    max_parallel_jobs=1, \n                    tags=None, \n                    base_tuning_job_name=None)\n<\/code><\/pre>\n\n<p>so in this case, your <code>objective_type<\/code> is in the wrong position. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/tuner.html#sagemaker.tuner.HyperparameterTuner\" rel=\"nofollow noreferrer\">the docs<\/a> for more details.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1531248961192,
        "Solution_link_count":1.0,
        "Solution_readability":25.8,
        "Solution_reading_time":8.53,
        "Solution_score_count":5.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426093220648,
        "Answerer_location":"Bengaluru, India",
        "Answerer_reputation_count":1861.0,
        "Answerer_view_count":294.0,
        "Challenge_adjusted_solved_time":0.3459813889,
        "Challenge_answer_count":1,
        "Challenge_body":"<ol>\n<li><p>I want to write pytest unit test in <strong>Kedro 0.17.5<\/strong>. They need to perform integrity checks on dataframes created by the pipeline.\nThese dataframes are specified in the <code>catalog.yml<\/code> and already persisted successfully using <code>kedro run<\/code>. The <code>catalog.yml<\/code> is in <code>conf\/base<\/code>.<\/p>\n<\/li>\n<li><p>I have a test module <code>test_my_dataframe.py<\/code> in <code>src\/tests\/pipelines\/my_pipeline\/<\/code>.<\/p>\n<\/li>\n<\/ol>\n<p>How can I load the data catalog based on my <code>catalog.yml<\/code> programmatically from within <code>test_my_dataframe.py<\/code> in order to properly access my specified dataframes?<\/p>\n<p>Or, for that matter, how can I programmatically load the whole project context (including the data catalog) in order to also execute nodes etc.?<\/p>",
        "Challenge_closed_time":1656143487270,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656142241737,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1656265981920,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72752043",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":11.24,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.3459813889,
        "Challenge_title":"Load existing data catalog programmatically",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":107,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1258185382660,
        "Poster_location":null,
        "Poster_reputation_count":333.0,
        "Poster_view_count":33.0,
        "Solution_body":"<ol>\n<li><p>For unit testing, we test just the function which we are testing, and everything external to the function we should mock\/patch. Check if you really need kedro project context while writing the unit test.<\/p>\n<\/li>\n<li><p>If you really need project context in test, you can do something like following<\/p>\n<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from kedro.framework.project import configure_project\nfrom kedro.framework.session import KedroSession\n\nwith KedroSession.create(package_name=&quot;demo&quot;, project_path=Path.cwd()) as session:\n    context = session.load_context()\n    catalog = context.catalog\n<\/code><\/pre>\n<p>or you can also create pytest fixture to use it again and again with scope of your choice.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@pytest.fixture\ndef get_project_context():\n    session = KedroSession.create(\n        package_name=&quot;demo&quot;,\n        project_path=Path.cwd()\n    )\n    _activate_session(session, force=True)\n    context = session.load_context()\n    return context\n<\/code><\/pre>\n<p>Different args supported by KedroSession create you can check it here <a href=\"https:\/\/kedro.readthedocs.io\/en\/0.17.5\/kedro.framework.session.session.KedroSession.html#kedro.framework.session.session.KedroSession.create\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/0.17.5\/kedro.framework.session.session.KedroSession.html#kedro.framework.session.session.KedroSession.create<\/a><\/p>\n<p>To read more about pytest fixture you can refer to <a href=\"https:\/\/docs.pytest.org\/en\/6.2.x\/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session\" rel=\"nofollow noreferrer\">https:\/\/docs.pytest.org\/en\/6.2.x\/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1656143791536,
        "Solution_link_count":4.0,
        "Solution_readability":19.5,
        "Solution_reading_time":23.58,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":135.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1400869861950,
        "Answerer_location":"Rio de Janeiro, Brazil",
        "Answerer_reputation_count":135.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":18.1100302778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a pretrained spacy model on a local folder that I can easily read with <code>m = spacy.load(&quot;path\/model\/&quot;)<\/code><\/p>\n<p>But now I have to upload it as a .tar.gz file to use as a Sagemaker model artifact.\nHow can I read this .tar.gz file?<\/p>\n<p>Ideally I want to read the unzipped folder from memory. Without extracting all to disk and then reading it again<\/p>\n<p>My question is almost a duplicate of this one <a href=\"https:\/\/stackoverflow.com\/questions\/49274650\/directly-load-spacy-model-from-packaged-tar-gz-file\">Directly load spacy model from packaged tar.gz file<\/a>. But the answers don't explain how to untar unzip the folder into memory<\/p>",
        "Challenge_closed_time":1652803358076,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652738161967,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72266041",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":9.2,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":18.1100302778,
        "Challenge_title":"Loading a spacy .tar.gz model artifact from s3 Sagemaker",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":184,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400869861950,
        "Poster_location":"Rio de Janeiro, Brazil",
        "Poster_reputation_count":135.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Turns out Sagemaker already decompress the <code>.tar.gz<\/code> file automatically.\nSo I can just read the folder exactly like before.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":1.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":314.4053583334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a use case wherein I need to refer to the input dataset in the ACI\/AKS which is in a blob (same used for training model). I'm not able to find related resources in the Microsoft official documentation. If anyone suggests to me how to do it, that will be very helpful.<\/p>",
        "Challenge_closed_time":1627304144467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626172285177,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68360738",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":3.83,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":314.4053583334,
        "Challenge_title":"AKS an ACI Deployment with blob mount",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":57,
        "Challenge_word_count":59,
        "Platform":"Stack Overflow",
        "Poster_created_time":1448994884167,
        "Poster_location":null,
        "Poster_reputation_count":265.0,
        "Poster_view_count":156.0,
        "Solution_body":"<p>It will be supported in the near future, Running Python scripts on Azure with Azure Container Instances to connect the blob.\n<a href=\"https:\/\/kohera.be\/tutorials-2\/running-python-scripts-on-azure-with-azure-container-instances\/\" rel=\"nofollow noreferrer\">https:\/\/kohera.be\/tutorials-2\/running-python-scripts-on-azure-with-azure-container-instances\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":25.9,
        "Solution_reading_time":4.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1314313109232,
        "Answerer_location":"Technion, Israel",
        "Answerer_reputation_count":18777.0,
        "Answerer_view_count":2000.0,
        "Challenge_adjusted_solved_time":93.5721875,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>upon running <code>pip install trains<\/code> in my virtual env<\/p>\n<p>I am getting<\/p>\n<pre><code>    ERROR: Command errored out with exit status 1:\n     command: \/home\/epdadmin\/noam\/code\/venv_linux\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-lxz5t8pu\/install-record.txt --single-version-externally-managed --compile --install-headers \/home\/epdadmin\/noam\/code\/venv_linux\/include\/site\/python3.8\/retrying\n         cwd: \/tmp\/pip-install-owzh8lnl\/retrying\/\n    Complete output (10 lines):\n    running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib\n    copying retrying.py -&gt; build\/lib\n    running install_lib\n    copying build\/lib\/retrying.py -&gt; \/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\n    byte-compiling \/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\/retrying.py to retrying.cpython-38.pyc\n    error: [Errno 13] Permission denied: '\/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\/__pycache__\/retrying.cpython-38.pyc.139678407381360'\n    ----------------------------------------\nERROR: Command errored out with exit status 1: \/home\/epdadmin\/noam\/code\/venv_linux\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-lxz5t8pu\/install-record.txt --single-version-externally-managed --compile --install-headers \/home\/epdadmin\/noam\/code\/venv_linux\/include\/site\/python3.8\/retrying Check the logs for full command output.\n<\/code><\/pre>\n<p>I know that <a href=\"https:\/\/stackoverflow.com\/questions\/15028648\/is-it-acceptable-and-safe-to-run-pip-install-under-sudo\">I am not supposed to run under sudo when using a venv<\/a>, so I don't really understand the problem<\/p>\n<p>running for example <code>pip install pandas<\/code> does work.<\/p>\n<p>Python 3.8<\/p>\n<p>How to install trains?<\/p>\n<hr \/>\n<p>EDIT:<\/p>\n<p>running <code>pip install trains --user<\/code> or <code>pip install --user trains<\/code> gives<\/p>\n<pre><code>ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\n<\/code><\/pre>",
        "Challenge_closed_time":1602767930968,
        "Challenge_comment_count":3,
        "Challenge_created_time":1602431071093,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1609353956110,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64305945",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":19.6,
        "Challenge_reading_time":39.57,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":93.5721875,
        "Challenge_title":"pip install trains fails",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1031,
        "Challenge_word_count":187,
        "Platform":"Stack Overflow",
        "Poster_created_time":1314313109232,
        "Poster_location":"Technion, Israel",
        "Poster_reputation_count":18777.0,
        "Poster_view_count":2000.0,
        "Solution_body":"<p>The problem was a permissions problem for the venv.\nAnother problem was trains required some packages that were not yet available with wheels on Python3.8, so I had to downgrade Python to 3.7<\/p>\n<p>That venv was created using Pycharm, and for some reason it was created with low permissions.<\/p>\n<p>There was probably a way to elevate its permissions, but instead I just deleted it and created another one using command line by<\/p>\n<pre><code>python -m virtualenv --python=\/usr\/bin\/python3.7 venv\n<\/code><\/pre>\n<p>And now <code>pip install trains<\/code> worked.<\/p>\n<p>Very annoying.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1604308183096,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":7.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":86.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1499682655627,
        "Answerer_location":"Heidelberg, Germany",
        "Answerer_reputation_count":193.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":0.7683222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm very beginner with wandb , so this is very basic question.\nI have dataframe which has my x features and y values.\nI'm tryin to follow <a href=\"https:\/\/docs.wandb.ai\/examples\" rel=\"nofollow noreferrer\">this tutorial<\/a>  to train model from my pandas dataframe . However, when I try to create wandb table from my pandas dataframe, I get an error:<\/p>\n<pre><code>\nwandb.init(project='my-xgb', config={'lr': 0.01})\n\n#the log didn't work  so I haven't run it at the moment (the log 'loss') \n#wandb.log({'loss': loss, ...})\n\n\n# Create a W&amp;B Table with your pandas dataframe\ntable = wandb.Table(df1)\n<\/code><\/pre>\n<blockquote>\n<p>AssertionError: columns argument expects a <code>list<\/code> object<\/p>\n<\/blockquote>\n<p>I have no idea why is this happen, and why it excpect a list. In the tutorial it doesn't look like the dataframe is list.<\/p>\n<p>My end goal - to be able to create wandb table.<\/p>",
        "Challenge_closed_time":1655983680023,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655983093280,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72729259",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":12.3,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.1629841667,
        "Challenge_title":"wandb.Table raises error: AssertionError: columns argument expects a `list` object",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":63,
        "Challenge_word_count":139,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572256318027,
        "Poster_location":"Israel",
        "Poster_reputation_count":1387.0,
        "Poster_view_count":224.0,
        "Solution_body":"<p><strong>Short answer<\/strong>: <code>table = wandb.Table(dataframe=my_df)<\/code>.<\/p>\n<p>The explanation of your specific case is at the bottom.<\/p>\n<hr \/>\n<p><strong>Minimal example<\/strong> of using <code>wandb.Table<\/code> with a DataFrame:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import wandb\nimport pandas as pd\n\niris_path = 'https:\/\/raw.githubusercontent.com\/mwaskom\/seaborn-data\/master\/iris.csv'\niris = pd.read_csv(iris_path)\ntable = wandb.Table(dataframe=iris)\nwandb.log({'dataframe_in_table': table})\n<\/code><\/pre>\n<p>(Here the dataset is called the Iris dataset that consists of &quot;3 different types of irises\u2019 (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray&quot;)<\/p>\n<p>There are two ways of creating W&amp;B <code>Table<\/code>s according to <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#create-tables\" rel=\"nofollow noreferrer\">the official documentation<\/a>:<\/p>\n<ul>\n<li><strong>List of Rows<\/strong>: Log named columns and rows of data. For example: <code>wandb.Table(columns=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], data=[[&quot;1a&quot;, &quot;1b&quot;, &quot;1c&quot;], [&quot;2a&quot;, &quot;2b&quot;, &quot;2c&quot;]])<\/code> generates a table with two rows and three columns.<\/li>\n<li><strong>Pandas DataFrame<\/strong>: Log a DataFrame using <code>wandb.Table(dataframe=my_df)<\/code>. Column names will be extracted from the DataFrame.<\/li>\n<\/ul>\n<hr \/>\n<p><strong>Explanation<\/strong>: Why <code>table = wandb.Table(my_df)<\/code> gives error &quot;columns argument expects a <code>list<\/code> object&quot;? Because <code>wandb.Table<\/code>'s init function looks like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def __init__(\n        self,\n        columns=None,\n        data=None,\n        rows=None,\n        dataframe=None,\n        dtype=None,\n        optional=True,\n        allow_mixed_types=False,\n    ):\n<\/code><\/pre>\n<p>If one passes a DataFrame without telling it's a DataFrame, <code>wandb.Table<\/code> will assume the argument is <code>columns<\/code>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1655985859240,
        "Solution_link_count":2.0,
        "Solution_readability":11.0,
        "Solution_reading_time":26.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":182.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1298044626600,
        "Answerer_location":null,
        "Answerer_reputation_count":1476.0,
        "Answerer_view_count":105.0,
        "Challenge_adjusted_solved_time":2.4293805556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there an API to receive list of published webservices?<\/p>\n<p>I have workspace id and auth token, I can get list of projects and experiments, but I can't get list of services created from experiments. Specifically I need the URL in order to post requests.\n<a href=\"https:\/\/i.stack.imgur.com\/6YHP1.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6YHP1.png\" alt=\"\" \/><\/a><br \/>\n<sub>(source: <a href=\"https:\/\/msdnshared.blob.core.windows.net\/media\/TNBlogsFS\/prod.evol.blogs.technet.com\/CommunityServer.Blogs.Components.WeblogFiles\/00\/00\/01\/02\/52\/JupRay-2.png\" rel=\"nofollow noreferrer\">windows.net<\/a>)<\/sub><\/p>\n<p>In client api I see if we publish a new service we can get it, but do we have more options?<\/p>",
        "Challenge_closed_time":1462276416390,
        "Challenge_comment_count":0,
        "Challenge_created_time":1462267670620,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1639596690136,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37000397",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":10.12,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":2.4293805556,
        "Challenge_title":"How to get list of services in Azure ML?",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":42,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369681858647,
        "Poster_location":null,
        "Poster_reputation_count":464.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>The R Azure ML API has that. Excerpt from <a href=\"https:\/\/htmlpreview.github.io\/?https:\/\/github.com\/RevolutionAnalytics\/AzureML\/blob\/master\/vignettes\/getting_started.html\" rel=\"nofollow\">https:\/\/htmlpreview.github.io\/?https:\/\/github.com\/RevolutionAnalytics\/AzureML\/blob\/master\/vignettes\/getting_started.html<\/a> : <\/p>\n\n<p><code>\n(webservices &lt;- services(ws, name = \"AzureML-vignette-silly\"))\n<\/code><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":37.8,
        "Solution_reading_time":5.71,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":20.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1490275561927,
        "Answerer_location":"Tel Aviv",
        "Answerer_reputation_count":83.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":5100.4654275,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am building a data transformation and training pipeline on Azure Machine Leaning Service. I'd like to save my fitted transformer (e.g. tf-idf) to the blob, so my prediction pipeline can later access it. <\/p>\n\n<pre><code>transformed_data = PipelineData(\"transformed_data\", \n                               datastore = default_datastore,\n                               output_path_on_compute=\"my_project\/tfidf\")\n\nstep_tfidf = PythonScriptStep(name = \"tfidf_step\",\n                              script_name = \"transform.py\",\n                              arguments = ['--input_data', blob_train_data, \n                                           '--output_folder', transformed_data],\n                              inputs = [blob_train_data],\n                              outputs = [transformed_data],\n                              compute_target = aml_compute,\n                              source_directory = project_folder,\n                              runconfig = run_config,\n                              allow_reuse = False)\n\n<\/code><\/pre>\n\n<p>The above code saves the transformer to a current run's folder, which is dynamically generated during each run. <\/p>\n\n<p>I want to save the transformer to a fixed location on blob, so I can access it later, when calling a prediction pipeline.<\/p>\n\n<p>I tried to use an instance of <code>DataReference<\/code> class as <code>PythonScriptStep<\/code> output, but it results in an error: \n<code>ValueError: Unexpected output type: &lt;class 'azureml.data.data_reference.DataReference'&gt;<\/code> <\/p>\n\n<p>It's because <code>PythonScriptStep<\/code> only accepts <code>PipelineData<\/code> or <code>OutputPortBinding<\/code> objects as outputs.<\/p>\n\n<p>How could I save my fitted transformer so it's later accessible by any aribitraly process (e.g. my prediction pipeline)?<\/p>",
        "Challenge_closed_time":1562586040652,
        "Challenge_comment_count":0,
        "Challenge_created_time":1560330676657,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56558552",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":14.3,
        "Challenge_reading_time":20.23,
        "Challenge_score_count":3,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":626.4899986111,
        "Challenge_title":"How to save your fitted transformer into blob, so your prediction pipeline can use it in AML Service?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1056,
        "Challenge_word_count":177,
        "Platform":"Stack Overflow",
        "Poster_created_time":1490275561927,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":83.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>Another solution is to pass <code>DataReference<\/code> as an input to your <code>PythonScriptStep<\/code>. <\/p>\n\n<p>Then inside <code>transform.py<\/code> you're able to read this <code>DataReference<\/code> as a command line argument. <\/p>\n\n<p>You can parse it and use it just as any regular path to save your vectorizer to.<\/p>\n\n<p>E.g. you can:<\/p>\n\n<pre><code>step_tfidf = PythonScriptStep(name = \"tfidf_step\",\n                              script_name = \"transform.py\",\n                              arguments = ['--input_data', blob_train_data, \n                                           '--output_folder', transformed_data,\n                                           '--transformer_path', trained_transformer_path],\n                              inputs = [blob_train_data, trained_transformer_path],\n                              outputs = [transformed_data],\n                              compute_target = aml_compute,\n                              source_directory = project_folder,\n                              runconfig = run_config,\n                              allow_reuse = False)\n<\/code><\/pre>\n\n<p>Then inside your script (<code>transform.py<\/code> in the example above) you can e.g.:<\/p>\n\n<pre><code>import argparse\nimport joblib as jbl\nimport os\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--transformer_path', dest=\"transformer_path\", required=True)\nargs = parser.parse_args()\n\ntfidf = ### HERE CREATE AND TRAIN YOUR VECTORIZER ###\n\nvect_filename = os.path.join(args.transformer_path, 'my_vectorizer.jbl')\n\n<\/code><\/pre>\n\n<hr>\n\n<p>EXTRA: The third way would be to just register the vectorizer as another model in your workspace. You can then use it exactly as any other registered model. (Though this option does not involve explicit writing to blob - as specified in the question above)<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1578692352196,
        "Solution_link_count":0.0,
        "Solution_readability":12.5,
        "Solution_reading_time":20.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":161.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1452426675696,
        "Answerer_location":null,
        "Answerer_reputation_count":77.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":1.7254188889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am writing code in Jupyter Notebook in Azure ML Studio.<\/p>\n\n<p>At current moment every command causes kernel death. Even in new clear notebook I could not execute even <code>print 'hello'<\/code> - kernel died immediately.<\/p>\n\n<p>Also I could not use bash commands like <code>!ls<\/code> - It crashes kernel too.<\/p>\n\n<p>How could I restart my VM or restart session in Azure ML Studio with killing all running VM?<\/p>",
        "Challenge_closed_time":1458573398360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1458551625417,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1458567186852,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36126897",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.9,
        "Challenge_reading_time":5.67,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6.0480397222,
        "Challenge_title":"How to restart VM in Azure ML Notebook?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":488,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1452426675696,
        "Poster_location":null,
        "Poster_reputation_count":77.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>I have found that if I go from notebook menu to File->Open, then I see all my notebooks, their statuses and I could shutdown them.<\/p>\n\n<p>Also I have found that some of my closed notebooks were still alive and have shut them down.<\/p>\n\n<p>After this my working notebook came back to life.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.3,
        "Solution_reading_time":3.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1294730361590,
        "Answerer_location":"New York, NY",
        "Answerer_reputation_count":57082.0,
        "Answerer_view_count":4597.0,
        "Challenge_adjusted_solved_time":5.2439738889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to list conda dependencies using a .yaml file for an AzureML <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-an-inference-configuration\" rel=\"nofollow noreferrer\">environment<\/a>. I do not want to use a custom docker image just for a few variations. I wonder if there is a way to instruct the build to run python commands using the .yaml file. Here are excerpts of what I have tried as of now:<\/p>\n<pre><code>name: classifer_environment\ndependencies:\n- python=3.6.2\n\n- pip:\n  - azureml-defaults&gt;=1.0.45\n  - nltk==3.4.5\n  - spacy\n\n- command: \n  - bash -c &quot;python -m nltk.downloader stopwords&quot;\n  - bash -c &quot;python -m spacy download en_core_web_sm&quot;\n<\/code><\/pre>\n<p>I also tried this:<\/p>\n<pre><code>name: classifer_environment\ndependencies:\n- python=3.6.2\n\n- pip:\n  - azureml-defaults&gt;=1.0.45\n  - nltk==3.4.5\n  - spacy\n\n- python: \n  - nltk.downloader stopwords\n  - spacy download en_core_web_sm\n<\/code><\/pre>\n<p>I do not have much clarity about yaml specifications. Both the specifications fail with the following messages respectively in the build logs: <br>\n&quot;Unable to install package for command.&quot; <br>\n&quot;Unable to install package for python.&quot;<\/p>",
        "Challenge_closed_time":1599673418303,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599654539997,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63811726",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":16.83,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":5.2439738889,
        "Challenge_title":"How to execute python commands from a conda .yaml specification file?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":408,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Poster_created_time":1555424023328,
        "Poster_location":"Bhubaneswar, Odisha, India",
        "Poster_reputation_count":328.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>This might be a neat feature to have, but for now it's not a thing - at least not directly in the YAML like this.<\/p>\n<p>Instead, the unit of computation in Conda is the <em>package<\/em>. That is, if you need to run additional scripts or commands at environment creation, it can be achieved by building a custom package and including this package in the YAML as a dependency. The package itself could be pretty much empty, but whatever code one needs to run would be included via <a href=\"https:\/\/docs.conda.io\/projects\/conda-build\/en\/latest\/resources\/link-scripts.html\" rel=\"nofollow noreferrer\">some installation scripts<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.5,
        "Solution_reading_time":7.96,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":92.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1395737095150,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":468.8686019445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If I instantiate a SageMaker <code>PyTorchModel<\/code> object like this:<\/p>\n\n<pre><code>from sagemaker.pytorch import PyTorchModel\n\nmodel = PyTorchModel(name=name_from_base('model-name'),\n                     model_data=model_data,\n                     role=role,\n                     framework_version='1.0.0',\n                     entry_point='serve.py',\n                     source_dir='src',\n                     sagemaker_session=sagemaker_session,\n                     predictor_cls=ImagePredictor)\n\n#model.create_without_deploying??\n<\/code><\/pre>\n\n<p>Is there a way that I can create this model using the sagemaker python SDK so that the model shows up in the SageMaker console, but <em>without<\/em> actually deploying it to an endpoint?<\/p>",
        "Challenge_closed_time":1561626837347,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559938343560,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1559938910380,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56500704",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.5,
        "Challenge_reading_time":8.64,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":469.0260519445,
        "Challenge_title":"SageMaker create PyTorchModel without deploying",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":434,
        "Challenge_word_count":61,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>I don't think it is possible to do so using the high-level SageMaker Pyhton SDK. However, you should be able to do it by calling the CreateModel API using the low-level boto3 <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model<\/a>. For your reference, below is an example snippet code on how to do it.<\/p>\n\n<pre><code>%%time\nimport boto3\nimport time\n\nsage = boto3.Session().client(service_name='sagemaker')\n\nimage_uri = '520713654638.dkr.ecr.us-east-1.amazonaws.com\/sagemaker-pytorch:1.0.0-cpu-py3'\nmodel_data ='s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/output\/model.tar.gz'\nsource = 's3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/sourcedir.tar.gz'\nrole = 'arn:aws:iam::xxxxxxxx:role\/service-role\/AmazonSageMaker-ExecutionRole-xxxxxx'\n\ntimestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\nmodel_name = 'my-pytorch-model' + timestamp\n\nresponse = sage.create_model(\n    ModelName=model_name,\n    PrimaryContainer={\n        'Image': image_uri,\n        'ModelDataUrl': model_data,\n        'Environment': { 'SAGEMAKER_CONTAINER_LOG_LEVEL':'20', 'SAGEMAKER_ENABLE_CLOUDWATCH_METRICS': 'False', \n                   'SAGEMAKER_PROGRAM': 'generate.py','SAGEMAKER_REGION': 'us-east-1','SAGEMAKER_SUBMIT_DIRECTORY': source}\n         },\n         ExecutionRoleArn=role\n}\nprint(response)\n<\/code><\/pre>\n\n<p>If you get no error message, then the model will shows up in the SageMaker console<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":23.2,
        "Solution_reading_time":20.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":106.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":2.4924897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using a self-deployed ClearML server with the clearml-data CLI, I would like to manage (or view) my datasets in the WebUI as shown on the ClearML webpage (<a href=\"https:\/\/clear.ml\/mlops\/clearml-feature-store\/\" rel=\"nofollow noreferrer\">https:\/\/clear.ml\/mlops\/clearml-feature-store\/<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/2T8IC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2T8IC.png\" alt=\"ClearML feature store\" \/><\/a><\/p>\n<p>However, this feature does not show up in my Web UI. According to the pricing page, the feature store is not a premium feature. Do I need to configure my server in a special way to use this feature?<\/p>",
        "Challenge_closed_time":1615831186403,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615821875663,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1615822213440,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66640850",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":9.09,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":2.5863166667,
        "Challenge_title":"How to manage datasets in ClearML Web UI?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":310,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1551960780550,
        "Poster_location":null,
        "Poster_reputation_count":354.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>Disclaimer: I'm part of the ClearML (formerly Trains) Team<\/p>\n<p>I think this screenshot is taken from the premium version...\nThe feature itself exists in the open-source version, but I &quot;think&quot; some of the dataset visualization capabilities are not available in the open-source self hosted version.<\/p>\n<p>Nonetheless, you have a fully featured feature-store, with the ability to add your own metrics \/ samples for every dataset\/feature version. The open-source version also includes the advanced versioning &amp; delta based storage for datasets\/features (i.e. only the change set from the parent version is stored)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":7.99,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":90.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1426046529436,
        "Answerer_location":null,
        "Answerer_reputation_count":180.0,
        "Answerer_view_count":20.0,
        "Challenge_adjusted_solved_time":207.9795563889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use XGBoost on Sagemaker notebook.<\/p>\n\n<p>I am using <code>conda_python3<\/code> kernel, and the following packages are installed:<\/p>\n\n<ul>\n<li>py-xgboost-mutex<\/li>\n<li>libxgboost<\/li>\n<li>py-xgboost<\/li>\n<li>py-xgboost-gpu<\/li>\n<\/ul>\n\n<p>But once I am trying to import xgboost it fails on import:<\/p>\n\n<pre><code>ModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-5-5943d1bfe3f1&gt; in &lt;module&gt;()\n----&gt; 1 import xgboost as xgb\n\nModuleNotFoundError: No module named 'xgboost'\n<\/code><\/pre>",
        "Challenge_closed_time":1592571589163,
        "Challenge_comment_count":2,
        "Challenge_created_time":1591825043587,
        "Challenge_favorite_count":3,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62313532",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":15.1,
        "Challenge_reading_time":7.55,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":207.3737711111,
        "Challenge_title":"xgboost on Sagemaker notebook import fails",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1532,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Poster_created_time":1391632571720,
        "Poster_location":"Tel Aviv-Yafo, Israel",
        "Poster_reputation_count":1248.0,
        "Poster_view_count":137.0,
        "Solution_body":"<p>In Sagemaker notebooks  use the below steps <\/p>\n\n<h3>a) If in Notebook<\/h3>\n\n<p>i)  <code>!type python3<\/code><\/p>\n\n<p>ii) Say the above is \/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 for you <\/p>\n\n<p>iii) <code>!\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 -m pip install  xgboost<\/code><\/p>\n\n<p>iv)  <code>import xgboost<\/code><\/p>\n\n<hr>\n\n<h3>b) If using Terminal<\/h3>\n\n<p>i) <code>conda activate conda_python3<\/code><br>\nii) <code>pip install xgboost<\/code><\/p>\n\n<p>Disclaimer :  sometimes the installation would fail with gcc version ,in that case  update pip version before running install<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1592573769990,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":7.84,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":57.0580416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am execute a python script in Azure machine learning studio. I am including other python scripts and python library, Theano. I can see the Theano get loaded and I got the proper result after script executed. But I saw the error message:<\/p>\n\n<blockquote>\n  <p>WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.<\/p>\n<\/blockquote>\n\n<p>Did anyone know how to solve this problem? Thanks!<\/p>",
        "Challenge_closed_time":1539764377230,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539726201673,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52844431",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":9.37,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":10.6043213889,
        "Challenge_title":"Azure Machine Learning Studio execute python script, Theano unable to execute optimized C-implementations (for both CPU and GPU)",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":99,
        "Challenge_word_count":112,
        "Platform":"Stack Overflow",
        "Poster_created_time":1337362023536,
        "Poster_location":null,
        "Poster_reputation_count":581.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>I don't think you can fix that - the Python script environment in Azure ML Studio is rather locked down, you can't really configure it (except for choosing from a small selection of Anaconda\/Python versions). <\/p>\n\n<p>You might be better off using the new Azure ML service, which allows you considerably more configuration options (including using GPUs and the like). <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1539931610623,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":4.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":8.3613402778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Trying to use the python SDK to deploy an AutoML model, but am recieving the error<\/p>\n<pre><code>google.api_core.exceptions.InvalidArgument: 400 'automatic_resources' is not supported for Model\n<\/code><\/pre>\n<p>Here's what I'm running:<\/p>\n<pre><code>from google.cloud import aiplatform\n\naiplatform.init(project=&quot;MY_PROJECT&quot;)\n\n# model is an AutoML tabular model\nmodel = aiplatform.Model(&quot;MY_MODEL_ID&quot;)\n\n# this fails\nmodel.deploy()\n<\/code><\/pre>",
        "Challenge_closed_time":1643942120972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643912020147,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70976273",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.1,
        "Challenge_reading_time":7.37,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":8.3613402778,
        "Challenge_title":"`Model.deploy()` failing for AutoML with `400 'automatic_resources' is not supported for Model`",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":149,
        "Challenge_word_count":57,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519933306780,
        "Poster_location":null,
        "Poster_reputation_count":3576.0,
        "Poster_view_count":203.0,
        "Solution_body":"<p>You encounter the <code>automatic_resources<\/code> because deploying models for AutoML Tables requires the user to define the machine type at model deployment. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#specify\" rel=\"nofollow noreferrer\">configure compute resources for prediction docs<\/a> for more information.<\/p>\n<blockquote>\n<p>If you want to use a custom-trained model or an AutoML tabular model\nto serve online predictions, you must specify a machine type when you\ndeploy the Model resource as a DeployedModel to an Endpoint.<\/p>\n<\/blockquote>\n<p>You should add defining of resources to your code for you to continue the deployment. Adding <code>machine_type<\/code> parameter should suffice. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#machine-types\" rel=\"nofollow noreferrer\">supported machine types docs<\/a> for complete list.<\/p>\n<p>See code below:<\/p>\n<pre><code>from google.cloud import aiplatform\n\nproject = &quot;your-project-id&quot;\nlocation = &quot;us-central1&quot;\nmodel_id = &quot;99999999&quot;\n\naiplatform.init(project=project, location=location)\nmodel_name = (f&quot;projects\/{project}\/locations\/{location}\/models\/{model_id}&quot;)\n\nmodel = aiplatform.Model(model_name=model_name)\n\nmodel.deploy(machine_type=&quot;n1-standard-4&quot;)\n\nmodel.wait()\nprint(model.display_name)\n<\/code><\/pre>\n<p>Output:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NHpJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NHpJF.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":17.7,
        "Solution_reading_time":21.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":136.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1544645236423,
        "Answerer_location":"Chihuahua, Mexico",
        "Answerer_reputation_count":661.0,
        "Answerer_view_count":639.0,
        "Challenge_adjusted_solved_time":3.1209341667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a Google Vertex AI pipeline to query from a BigQuery table. In the pipeline, I am using the right project and the service account(which has bigquery.jobs.create access). But I see when it runs, it is accessing another project e1cd7306fb577e88gq-uq. I am not able to figure out where from this project is coming from. I am running the pipeline from Vertex AI user managed notebook<\/p>\n<pre><code>pandas_gbq.exceptions.GenericGBQException: Reason: 403 POST https:\/\/bigquery.googleapis.com\/bigquery\/v2\/projects\/e1cd7306fb577e88gq-uq\/jobs?prettyPrint=false: Access Denied: Project e1cd7306fb577e88gq-uq: User does not have bigquery.jobs.create permission in project e1cd7306fb577e88gq-uq.\n<\/code><\/pre>",
        "Challenge_closed_time":1650042428203,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650031192840,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71884962",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":10.15,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":3.1209341667,
        "Challenge_title":"Vertex AI Pipeline is failing while trying to get data from BigQuery",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":525,
        "Challenge_word_count":99,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530457174832,
        "Poster_location":null,
        "Poster_reputation_count":1043.0,
        "Poster_view_count":212.0,
        "Solution_body":"<p>The service agent or service account running your code does have the required permission, but your code is trying to access a resource in the wrong project. Due to the way Vertex AI runs your training code, this problem can occur inadvertently if you don't explicitly specify a project ID or project number in your code.<\/p>\n<p>You can explicitly select the project you want this way:<\/p>\n<pre><code>import os\n\nfrom google.cloud import bigquery\n\nproject_number = os.environ[&quot;CLOUD_ML_PROJECT_ID&quot;]\n\nclient = bigquery.Client(project=project_number)\n<\/code><\/pre>\n<p>You can read more about training code requirements <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/code-requirements#other-services\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.6,
        "Solution_reading_time":9.78,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":89.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":66.306625,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I've deployed an endpoint in sagemaker and was trying to invoke it through my python program. I had tested it using postman and it worked perfectly ok. Then I wrote the invocation code as follows<\/p>\n\n<pre><code>import boto3\nimport pandas as pd\nimport io\nimport numpy as np\n\ndef np2csv(arr):\n    csv = io.BytesIO()\n    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n    return csv.getvalue().decode().rstrip()\n\n\nruntime= boto3.client('runtime.sagemaker')\npayload = np2csv(test_X)\n\nruntime.invoke_endpoint(\n    EndpointName='&lt;my-endpoint-name&gt;',\n    Body=payload,\n    ContentType='text\/csv',\n    Accept='Accept'\n)\n<\/code><\/pre>\n\n<p>Now whe I run this I get a validation error<\/p>\n\n<pre><code>ValidationError: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint &lt;my-endpoint-name&gt; of account &lt;some-unknown-account-number&gt; not found.\n<\/code><\/pre>\n\n<p>While using postman i had given my access key and secret key but I'm not sure how to pass it when using sagemaker apis. I'm not able to find it in the documentation also. <\/p>\n\n<p>So my question is, how can I use sagemaker api from my local machine to invoke my endpoint?<\/p>",
        "Challenge_closed_time":1517113913470,
        "Challenge_comment_count":0,
        "Challenge_created_time":1516867519790,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1516875209620,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48438202",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.1,
        "Challenge_reading_time":15.26,
        "Challenge_score_count":5,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":68.4426888889,
        "Challenge_title":"Errors while using sagemaker api to invoke endpoints",
        "Challenge_topic":"Lambda Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":4467,
        "Challenge_word_count":155,
        "Platform":"Stack Overflow",
        "Poster_created_time":1410972175307,
        "Poster_location":null,
        "Poster_reputation_count":1124.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>When you are using any of the AWS SDK (including the one for Amazon SageMaker), you need to configure the credentials of your AWS account on the machine that you are using to run your code. If you are using your local machine, you can use the AWS CLI flow. You can find detailed instructions on the Python SDK page: <a href=\"https:\/\/aws.amazon.com\/developers\/getting-started\/python\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/developers\/getting-started\/python\/<\/a> <\/p>\n\n<p>Please note that when you are deploying the code to a different machine, you will have to make sure that you are giving the EC2, ECS, Lambda or any other target a role that will allow the call to this specific endpoint. While in your local machine it can be OK to give you admin rights or other permissive permissions, when you are deploying to a remote instance, you should restrict the permissions as much as possible. <\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sagemaker:InvokeEndpoint\",\n            \"Resource\": \"arn:aws:sagemaker:*:1234567890:endpoint\/&lt;my-endpoint-name&gt;\"\n        }\n    ]\n}\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.0,
        "Solution_reading_time":14.44,
        "Solution_score_count":4.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":156.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1262067470272,
        "Answerer_location":"Los Angeles, CA, United States",
        "Answerer_reputation_count":11653.0,
        "Answerer_view_count":727.0,
        "Challenge_adjusted_solved_time":162.6529575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I'm trying to use <code>tf.contrib.learn.preprocessing.VocabularyProcessor.restore()<\/code> to restore a vocabulary file from an S3 bucket. First, I tried to get the path name to the bucket to use in <code>.restore()<\/code> and I kept getting 'object doesn't exist' error. Afterwards, upon further research, I found a method people use to load text files and JSON files and applied the same method here:<\/p>\n\n<pre><code>obj = s3.Object(BUCKET_NAME, KEY).get()['Body'].read()\nvocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(obj)\n<\/code><\/pre>\n\n<p>This worked for a while until the contents of file increased and eventually got a 'File name too long' error. Is there a better way to load and restore a file from an S3 bucket? <\/p>\n\n<p>By the way, I tested this out locally on my machine and it works perfectly fine since there it just needs to take the path to the file, not the entire contents of the file. <\/p>",
        "Challenge_closed_time":1521485629848,
        "Challenge_comment_count":0,
        "Challenge_created_time":1521471311783,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49365900",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":12.77,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":3.9772402778,
        "Challenge_title":"What's a better way to load a file using boto? (getting filename too long error)",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":532,
        "Challenge_word_count":153,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521470035783,
        "Poster_location":null,
        "Poster_reputation_count":210.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>It looks like you\u2019re passing in the actual contents of the file as the file name?<\/p>\n\n<p>I think you\u2019ll need to download the object from S3 to a tmp file and pass the path to that file into restore.<\/p>\n\n<p>Try using the method here: <a href=\"http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Object.download_file\" rel=\"nofollow noreferrer\">http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Object.download_file<\/a><\/p>\n\n<p>Update:\nI went through the code here: <a href=\"https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py<\/a> and it looks like this just saves a pickle so you can really easily just import pickle and call the following:<\/p>\n\n<pre><code>import pickle\nobj = s3.Object(BUCKET_NAME, KEY).get()['Body']\nvocab_processor = pickle.loads(obj.read())\n<\/code><\/pre>\n\n<p>Hopefully that works?<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1522056862430,
        "Solution_link_count":4.0,
        "Solution_readability":16.7,
        "Solution_reading_time":13.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1544390307847,
        "Answerer_location":"Palo Alto, CA, USA",
        "Answerer_reputation_count":151.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":167.9029461111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a supervised machine learning problem, and I am setting up a custom labeling task to send out to Amazon Mechanical Turk for human annotation.<\/p>\n\n<p>I have uploaded the data to AWS S3 in the json-lines (<code>.jsonl<\/code>) format as follows, pursuant to the instructions as specified in the AWS documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html<\/a>: <\/p>\n\n<pre><code>{\"source\": \"value0\"}\n{\"source\": \"value1\"}    \n{\"source\": \"value2\"}\n...\n{\"source\": \"value2\"}\n<\/code><\/pre>\n\n<p>When I click on the default text classification template, I can see my data come through and everything appears to work.<\/p>\n\n<p>However, I am getting the following error when I attempt to use the custom annotation task template interface: <code>MissingRequiredParameter: Missing required key 'FunctionName' in params<\/code> <\/p>\n\n<p>The error resembles an AWS Lambda error, except the strange thing is that I am not using AWS Lambda. Suggestions for how to proceed? <\/p>",
        "Challenge_closed_time":1552342307056,
        "Challenge_comment_count":0,
        "Challenge_created_time":1551737856450,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54992434",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.1,
        "Challenge_reading_time":15.03,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":167.9029461111,
        "Challenge_title":"MissingRequiredParameter: Missing required key 'FunctionName' in params",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":7776,
        "Challenge_word_count":144,
        "Platform":"Stack Overflow",
        "Poster_created_time":1336429658227,
        "Poster_location":"San Francisco, CA, USA",
        "Poster_reputation_count":947.0,
        "Poster_view_count":61.0,
        "Solution_body":"<p>I am from the engineering team and happy to help you here. I think the issue is not related to the manifest as it looks correct to me. The error suggests that you may haven't provided a correct lambda ARN for pre or post labeling task. Please see this doc for more details: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step3.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step3.html<\/a><\/p>\n\n<p>I can also help further if you can send me details on how you starting the job and what parameters you are sending.  <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.7,
        "Solution_reading_time":7.58,
        "Solution_score_count":5.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1361240380856,
        "Answerer_location":null,
        "Answerer_reputation_count":3349.0,
        "Answerer_view_count":465.0,
        "Challenge_adjusted_solved_time":46.4306352778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In data exploration phrase, I need some visualization to check the relationship between data columns. I have tried Azure ML built-in visualization and felt it's limited. R ggplot allows me to choose the chart I want and do some customization.<\/p>\n\n<p>I am very new in using Azure ML. When I am trying Azure ML R script and type:<\/p>\n\n<pre><code>library(\"ggplot2\")\n\np &lt;- ggplot(train, aes(x= Item_Visibility, y = Item_Outlet_Sales)) +\ngeom_point(size = 2.5, color = \"navy\") + xlab(\"Item Visibility\") + ylab(\"Item Outlet Sales\")\n<\/code><\/pre>\n\n<p>Cannot find where is my visualization... It is not in visualization result, neither in log ourput\nI have also tried Azure ML built-in plot(), it cannot find the column in my dataset... <\/p>\n\n<p>So, is there anyway, when I am using ggplot in Azure ML R Script, I can find the visualization results?<\/p>",
        "Challenge_closed_time":1461446546867,
        "Challenge_comment_count":2,
        "Challenge_created_time":1461279396580,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36781843",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.0,
        "Challenge_reading_time":11.0,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":46.4306352778,
        "Challenge_title":"how to use ggplot through Azure ML R Script",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":425,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361240380856,
        "Poster_location":null,
        "Poster_reputation_count":3349.0,
        "Poster_view_count":465.0,
        "Solution_body":"<p>Finally I found the visualization. Just put the code in AzureML Studio Execute R Script module will be fine.\nAfter the code has been executed successfully, right click Execute R Script module, and choose R device, the visualization is there<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/1aZ0y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1aZ0y.png\" alt=\"ggplot through AzureML R Script\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.6,
        "Solution_reading_time":5.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1434117836363,
        "Answerer_location":"Toronto, ON, Canada",
        "Answerer_reputation_count":78.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":0.2549266667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have installed sagemaker using <code>sc.install_pypi_package(&quot;sagemaker==2.5.1&quot;)<\/code>. However, I get the following error when I try to import sagemaker and it is pointing to python2.7.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yfln3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yfln3.png\" alt=\"cannot import name git_utils\" \/><\/a><\/p>\n<p>I checked my EMR master node running pyspark and the version there is pyspark 2.4.5 running python 3.7.6.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hyctk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hyctk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So then I tried to upgrade the python version of my spark context but it says<\/p>\n<blockquote>\n<p>&quot;ValueError: Package already installed for current Spark context!&quot;<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/XBg3E.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XBg3E.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So I thought lemme try uninstalling python2.7 from spark context and that does not let me do it, saying<\/p>\n<blockquote>\n<p>&quot;Not uninstalling python at \/usr\/lib64\/python2.7\/lib-dynload, outside\nenvironment \/tmp\/1598628537004-0&quot;<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ktlfO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ktlfO.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What am I doing wrong? I believe the sagemaker import is failing due to spark context referring python2.7. How do I fix this?<\/p>",
        "Challenge_closed_time":1598632128583,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598630122480,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1598631210847,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63637178",
        "Challenge_link_count":8,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":22.17,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":0.5572508334,
        "Challenge_title":"Spark is running python 3.7.6 but spark context is showing python 2.7. How to fix using the spark context?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":855,
        "Challenge_word_count":178,
        "Platform":"Stack Overflow",
        "Poster_created_time":1434117836363,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":78.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Referred to this <a href=\"https:\/\/aws.amazon.com\/blogs\/big-data\/install-python-libraries-on-a-running-cluster-with-emr-notebooks\/\" rel=\"nofollow noreferrer\">link<\/a> and updated the python version of spark context to python3. This fixes the issue:<\/p>\n<pre><code>%%configure -f\n{ &quot;conf&quot;:{\n          &quot;spark.pyspark.python&quot;: &quot;python3&quot;,\n          &quot;spark.pyspark.virtualenv.enabled&quot;: &quot;true&quot;,\n          &quot;spark.pyspark.virtualenv.type&quot;:&quot;native&quot;,\n          &quot;spark.pyspark.virtualenv.bin.path&quot;:&quot;\/usr\/bin\/virtualenv&quot;\n         }\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":22.2,
        "Solution_reading_time":8.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1406731060412,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":139.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":7804.7933702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have tried connecting through Sagemaker notebook to RDS. However, to connect to RDS, my public IP needs to be allowed for security reasons. I can see when I run this command: &quot;curl ifconfig.me&quot; on Sagemaker Notebook instance that public IP keeps changing from time to time.<\/p>\n<p>What is the correct way to connect to RDS with notebook on sagemaker? Do I need to crawl the RDS with AWS Glue and then use Athena on crawled tables and then take the query results from S3 with Sagemaker notebook?<\/p>",
        "Challenge_closed_time":1627578308223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599481052090,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63777462",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.89,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":7804.7933702778,
        "Challenge_title":"Sagemaker Jupyter Notebook Cannot Connect to RDS",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":865,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509559597047,
        "Poster_location":null,
        "Poster_reputation_count":313.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>RDS is just a managed database running on an EC2 instance. You can connect to that database in a very same way as you would connect from an application. For example, you can use a python based DB client library (depending on what DB flavor you're using, e.g. Postgres) and configure with the connection string, as you would connect any other application to your RDS instance.<\/p>\n<p>I would not recommend to connect to the RDS instance through the public interface. You can place your Notebook instance to the same VPC where your RDS instance is, thus you can talk to RDS directly through the VPC.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":7.35,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":105.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":1.7422338889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking to use Azure Machine Learning Services (the one with the new drag and drop feature; still in preview) in a new data science project. <\/p>\n\n<p>I have realised that I can preview the data when I connect a data set; I am able to do this using the option 'Dataset output' which is available as part of the dataset.<\/p>\n\n<p>To be able to see this data, the data needs to be cached some where. <\/p>\n\n<p>Can someone advise where this is cached? <\/p>",
        "Challenge_closed_time":1582256294052,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582250022010,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60331084",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.13,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.7422338889,
        "Challenge_title":"Where does Azure Machine Learning Service cache data?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":397,
        "Challenge_word_count":93,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456485566000,
        "Poster_location":"Canberra, Australian Capital Territory, Australia",
        "Poster_reputation_count":214.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>Data is cached by default in a storage account that is created along with the the ML service workspace. It has the same name as the workspace plus some numbers. Inside the account there is a blobstore called <code>azureml-blobstore-{GUID}<\/code> Inside of that container your data is cached,  organized by runs.<\/p>\n\n<p>This data is made available to ML service as a <code>Datastore<\/code> that you can navigate to in the UI by clicking \"Datastores\" in the blade on the left-hand of the Studio.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YVwPl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YVwPl.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.3,
        "Solution_reading_time":8.46,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1484748258356,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":248.2981122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using a GCP account that started as free, but does have billing enabled, I can't create a managed notebook and get the following popup error:<\/p>\n<p>Quota exceeded for quota metric 'Create Runtime API requests' and limit 'Create Runtime API requests per minute' of service 'notebooks.googleapis.com' for consumer 'project_number:....'<\/p>\n<p>Navigating to Quotas --&gt; Notebook API --&gt; Create Runtime API requests per minute<\/p>\n<p>Edit Quota: Create Runtime API requests per minute\nCurrent limit: 0\nEnter a new quota limit between 0 and 0.<\/p>\n<p>0 doesn't work..<\/p>\n<p>Is there something that I can do, or should have done already to increase this quota?<\/p>\n<p>TIA for any help.<\/p>",
        "Challenge_closed_time":1638931046247,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638037173043,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70137519",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":9.27,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":248.2981122222,
        "Challenge_title":"GCP cannot create Managed Notebook on Vertex AI",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":285,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1285776739110,
        "Poster_location":"Chicago, IL",
        "Poster_reputation_count":8508.0,
        "Poster_view_count":144.0,
        "Solution_body":"<p>Managed notebooks is still pre-GA and is currently unavailable to the projects with insufficient service usage history.<\/p>\n<p>You can wait for the GA of the service or use a project with more service usage.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":2.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":237.2008941667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The Amazon documentation lists several approaches to evaluate a model (e.g. cross validation, etc.) however these methods does not seem to be available in the Sagemaker Java SDK. \nCurrently if we want to do 5-fold cross validation it seems the only option is to create 5 models (and also deploy 5 endpoints) one model for each subset of data and manually compute the performance metric (recall, precision, etc.). <\/p>\n\n<p>This approach is not very efficient and can also be expensive need to deploy k-endpoints, based on the number of folds in the k-fold validation.<\/p>\n\n<p>Is there another way to test the performance of a model?<\/p>",
        "Challenge_closed_time":1526758178327,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526673699553,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50418501",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":8.17,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":23.4663261111,
        "Challenge_title":"Sagemaker model evaluation",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1442,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428454496052,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Amazon SageMaker is a set of multiple components that you can choose which ones to use. <\/p>\n\n<p>The built-in algorithms are designed for (infinite) scale, which means that you can have huge datasets and be able to build a model with them quickly and with low cost. Once you have large datasets you usually don't need to use techniques such as cross-validation, and the recommendation is to have a clear split between training data and validation data. Each of these parts will be defined with an input channel when you are submitting a training job.  <\/p>\n\n<p>If you have a small amount of data and you want to train on all of it and use cross-validation to allow it, you can use a different part of the service (interactive notebook instance). You can bring your own algorithm or even container image to be used in the development, training or hosting. You can have any python code based on any machine learning library or framework, including scikit-learn, R, TensorFlow, MXNet etc. In your code, you can define cross-validation based on the training data that you copy from S3 to the worker instances. <\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1527527622772,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":13.5,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":192.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1557646363768,
        "Answerer_location":null,
        "Answerer_reputation_count":51.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":937.3051341667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to upload training job artifacts to S3 in a non-compressed manner.<\/p>\n<p>I am familiar with the output_dir one can provide to a sagemaker Estimator, then everything saved under \/opt\/ml\/output is uploaded compressed to the S3 output dir.<\/p>\n<p>I want to have the option to access a specific artifact without having to decompress the output every time. Is there a clean way to go about it? if not any workaround in mind?\nThe artifacts of my interest are small meta-data files .txt or .csv, while in my case the rest of the artifacts can be ~1GB so downloading and decompressing is quite excessive.<\/p>\n<p>any help would be appreciated<\/p>",
        "Challenge_closed_time":1612085605603,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608711307120,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65421005",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.5,
        "Challenge_reading_time":9.08,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":937.3051341667,
        "Challenge_title":"how to save uncompressed outputs from a training job in using aws Sagemaker python SDK?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":313,
        "Challenge_word_count":123,
        "Platform":"Stack Overflow",
        "Poster_created_time":1557646363768,
        "Poster_location":null,
        "Poster_reputation_count":51.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>I ended up using the checkpoint path that is by default being synced with the specified S3 path in an uncompressed manner.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":1.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1491898605956,
        "Answerer_location":"Sydney NSW, Australia",
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":12.7567577778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following this example from aws <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-multi-model-endpoint-tensorflow-computer-vision\/blob\/main\/multi-model-endpoint-tensorflow-cv.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-multi-model-endpoint-tensorflow-computer-vision\/blob\/main\/multi-model-endpoint-tensorflow-cv.ipynb<\/a>\nto apply same workflow with two pre trained models (trained outside of sagemaker).<\/p>\n<p>But when I do the following, logs say that models can't be found:<\/p>\n<pre><code>import boto3\nimport datetime\nfrom datetime import datetime\nimport time\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.tensorflow.serving import TensorFlowModel\nfrom sagemaker.multidatamodel import MultiDataModel\n\nmodel_data_prefix = f's3:\/\/{BUCKET}\/{PREFIX}\/mme\/'\noutput = f's3:\/\/{BUCKET}\/{PREFIX}\/mme\/test.tar.gz'\n\nmodele = TensorFlowModel(model_data=output, \n                          role=role, \n                          image_uri=IMAGE_URI)\n\nmme = MultiDataModel(name=f'mme-tensorflow-{current_time}',\n                     model_data_prefix=model_data_prefix,\n                     model=modele,\n                     sagemaker_session=sagemaker_session)\n\npredictor = mme.deploy(initial_instance_count=1,\n                       instance_type='ml.m5.2xlarge',\n                       endpoint_name=f'mme-tensorflow-{current_time}')\n<\/code><\/pre>\n<p>When I give an image as input to predict, I have this message:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message &quot;&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Internal Server Error&lt;\/title&gt;\n  &lt;\/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;&lt;p&gt;Internal Server Error&lt;\/p&gt;&lt;\/h1&gt;\n    \n  &lt;\/body&gt;\n&lt;\/html&gt;\n&quot;.\n<\/code><\/pre>\n<p>Logs give:<\/p>\n<pre><code>Could not find base path \/opt\/ml\/models\/...\/model for servable ...\n<\/code><\/pre>\n<p>What did I missed ?<\/p>",
        "Challenge_closed_time":1651629562456,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651582909370,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1651583638128,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72099790",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":21.3,
        "Challenge_reading_time":25.9,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":12.9591905556,
        "Challenge_title":"Error when deploying pre trained Tensorflow models to one endpoint (multimodel for one endpoint) in sagemaker?",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":72,
        "Challenge_word_count":155,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606642099552,
        "Poster_location":null,
        "Poster_reputation_count":371.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>In the sample notebook, the model is trained within SageMaker. So it is created with certain environment variables like the &quot;SAGEMAKER_PROGRAM&quot;(I think, need to check the documentation) with value set to entry point script.<\/p>\n<p>But while you are creating the model with models trained outside the SageMaker you need to add those environment variables.<\/p>\n<p>Without an entry point script SageMaker is not in a position to know what to do with the request.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":5.93,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":73.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":1.8544463889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I had to stop training in the middle, which set the Trains status to <code>Aborted<\/code>.\nLater I continued it from the last checkpoint, but the status remained <code>Aborted<\/code>.\nFurthermore, automatic training metrics stopped appearing in the dashboard (though custom metrics still do).<\/p>\n<p>Can I reset the status back to <code>Running<\/code> and make Trains log training stats again?<\/p>\n<p><strong>Edit:<\/strong> When continuing training, I retrieved the task using <code>Task.get_task()<\/code> and not <code>Task.init()<\/code>. Maybe that's why training stats are not updated anymore?<\/p>\n<p><strong>Edit2:<\/strong> I also tried <code>Task.init(reuse_last_task_id=original_task_id_string)<\/code>, but it just creates a new task, and doesn't reuse the given task ID.<\/p>",
        "Challenge_closed_time":1593515579540,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593508903533,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1609467668432,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62654203",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":11.03,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.8544463889,
        "Challenge_title":"Trains: Can I reset the status of a task? (from 'Aborted' back to 'Running')",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":228,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>Disclaimer: I'm a member of Allegro Trains team<\/p>\n<blockquote>\n<p>When continuing training, I retrieved the task using Task.get_task() and not Task.init(). Maybe that's why training stats are not updated anymore?<\/p>\n<\/blockquote>\n<p>Yes that's the only way to continue the same exact Task.\nYou can also mark it as started with <code>task.mark_started()<\/code> , that said the automatic logging will not kick in, as <code>Task.get_task<\/code> is usually used for accessing previously executed tasks and not continuing it (if you think the continue use case is important please feel free to open a GitHub issue, I can definitely see the value there)<\/p>\n<p>You can also do something a bit different, and justcreate a new Task continuing from the last iteration the previous run ended. Notice that if you load the weights file (PyTorch\/TF\/Keras\/JobLib) it will automatically connect it with the model that was created in the previous run (assuming the model was stored is the same location, or if you have the model on https\/S3\/Gs\/Azure and you are using <code>trains.StorageManager.get_local_copy()<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>previous_run = Task.get_task()\ntask = Task.init('examples', 'continue training')\ntask.set_initial_iteration(previous_run.get_last_iteration())\ntorch.load('\/tmp\/my_previous_weights')\n<\/code><\/pre>\n<p>BTW:<\/p>\n<blockquote>\n<p>I also tried Task.init(reuse_last_task_id=original_task_id_string), but it just creates a new task, and doesn't reuse the given task ID.<\/p>\n<\/blockquote>\n<p>This is a great idea for an interface to continue a previous run, feel free to add it as GitHub issue.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1593557456892,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":21.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":219.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1538275960603,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation_count":381.0,
        "Answerer_view_count":50.0,
        "Challenge_adjusted_solved_time":9.2060972223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-manage-quotas#azure-machine-learning-compute\" rel=\"nofollow noreferrer\">Manage and request quotas for Azure resources<\/a> documentation page states that the default quota depends \"on your subscription offer type\". The quota doesn't show up in Azure web portal. Is there a way to find out current quota values using SDK, CLI, REST API?<\/p>",
        "Challenge_closed_time":1570669528590,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570636386640,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58307950",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":6.05,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":9.2060972223,
        "Challenge_title":"Azure Machine Learning Compute quota?",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":354,
        "Challenge_word_count":52,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518706063680,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>You probably want to try something like this command : <\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>az vm list-usage --location eastus --out table\n<\/code><\/pre>\n\n<p>It would get you the core usage for the region, which is what is important for deployment of resources.<\/p>\n\n<p>Other choices (az + Powershell) are available <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/networking\/check-usage-against-limits\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Hope this helps!<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":6.3,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1577919980176,
        "Answerer_location":"Hamburg, Germany",
        "Answerer_reputation_count":5588.0,
        "Answerer_view_count":398.0,
        "Challenge_adjusted_solved_time":171.7806,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I developed an ANN tool by using pycharm\/tensorflow on my own computer. I uploaded the h5 and json files to Amazon Sagemaker by creating a Notebook Instance. I was finally able to successfully create an endpoint and make it work. The following code in Notebook Instance -Jupyter works:<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\nimport sagemaker\nfrom sagemaker.tensorflow.model import TensorFlowModel\nclient = boto3.client('runtime.sagemaker')\ndata = np.random.randn(1,6).tolist()\nendpoint_name = 'sagemaker-tensorflow-**********'\nresponse = client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(data))\nresponse_body = response['Body']\nprint(response_body.read())\n<\/code><\/pre>\n<p>However, the problem occurs when I created a lambda function and call the endpoint from there. The input should be a row of 6 features -that is a 1-by-6 vector. I enter the following input into lambda {&quot;data&quot;:&quot;1,1,1,1,1,1&quot;} and it gives me the following error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/var\/task\/lambda_function.py&quot;, line 20, in lambda_handler\n    Body=payload)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 635, in _make_api_call\n    raise error_class(parsed_response, operation_name)\n<\/code><\/pre>\n<p>I think the problem is that the input needs to be 1-by-6 instead of 6-by-1 and I don't know how to do that.<\/p>",
        "Challenge_closed_time":1599633575463,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599015165303,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63698011",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":20.61,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":171.7806,
        "Challenge_title":"AWS Notebook Instance is working but Lambda is not accepting the input",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":51,
        "Challenge_word_count":187,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369539919747,
        "Poster_location":null,
        "Poster_reputation_count":311.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>I assume the content type you specified is <code>text\/csv<\/code>, so try out:<\/p>\n<pre><code>{&quot;data&quot;: [&quot;1,1,1,1,1,1&quot;]}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":2.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":15.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1276294622427,
        "Answerer_location":null,
        "Answerer_reputation_count":4334.0,
        "Answerer_view_count":496.0,
        "Challenge_adjusted_solved_time":135.0612036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have <a href=\"https:\/\/docs.aws.amazon.com\/code-samples\/latest\/catalog\/python-comprehend-TopicModeling.py.html\" rel=\"nofollow noreferrer\">Amazon sample code<\/a> for running <code>comprehend.start_topics_detection_job<\/code>. Here is the code with the variables filled in for my job:<\/p>\n\n<pre><code>import re\nimport csv\nimport pytz\nimport boto3\nimport json\n\n# https:\/\/docs.aws.amazon.com\/code-samples\/latest\/catalog\/python-comprehend-TopicModeling.py.html\n# https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/API_InputDataConfig.html\n\n# Set these values before running the program\ninput_s3_url = \"s3:\/\/comprehend-topic-modelling-bucket\/input_800_cleaned_articles\/\"\ninput_doc_format = \"ONE_DOC_PER_LINE\"\noutput_s3_url = \"s3:\/\/comprehend-topic-modelling-bucket\/output\"\ndata_access_role_arn = \"arn:aws:iam::372656143103:role\/access-aws-services-from-sagemaker\"\nnumber_of_topics = 30\n\n# Set up job configuration\ninput_data_config = {\"S3Uri\": input_s3_url, \"InputFormat\": input_doc_format}\noutput_data_config = {\"S3Uri\": output_s3_url}\n\n# Begin a job to detect the topics in the document collection\ncomprehend = boto3.client('comprehend')\nstart_result = comprehend.start_topics_detection_job(\n    NumberOfTopics=number_of_topics,\n    InputDataConfig=input_data_config,\n    OutputDataConfig=output_data_config,\n    DataAccessRoleArn=data_access_role_arn)\n\n# Output the results\nprint('Start Topic Detection Job: ' + json.dumps(start_result))\njob_id = start_result['JobId']\nprint(f'job_id: {job_id}')\n\n# Retrieve and output information about the job\ndescribe_result = comprehend.describe_topics_detection_job(JobId=job_id)\nprint('Describe Job: ' + json.dumps(describe_result)) . #&lt;===LINE 36\n\n# List and output information about current jobs\nlist_result = comprehend.list_topics_detection_jobs()\nprint('list_topics_detection_jobs_result: ' + json.dumps(list_result))\n<\/code><\/pre>\n\n<p>It's failing with the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-8-840a7ee043d4&gt; in &lt;module&gt;()\n     34 # Retrieve and output information about the job\n     35 describe_result = comprehend.describe_topics_detection_job(JobId=job_id)\n---&gt; 36 print('Describe Job: ' + json.dumps(describe_result))\n     37 \n     38 # List and output information about current jobs\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\n    229         cls is None and indent is None and separators is None and\n    230         default is None and not sort_keys and not kw):\n--&gt; 231         return _default_encoder.encode(obj)\n    232     if cls is None:\n    233         cls = JSONEncoder\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in encode(self, o)\n    197         # exceptions aren't as detailed.  The list call should be roughly\n    198         # equivalent to the PySequence_Fast that ''.join() would do.\n--&gt; 199         chunks = self.iterencode(o, _one_shot=True)\n    200         if not isinstance(chunks, (list, tuple)):\n    201             chunks = list(chunks)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in iterencode(self, o, _one_shot)\n    255                 self.key_separator, self.item_separator, self.sort_keys,\n    256                 self.skipkeys, _one_shot)\n--&gt; 257         return _iterencode(o, 0)\n    258 \n    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in default(self, o)\n    178         \"\"\"\n    179         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n--&gt; 180                         o.__class__.__name__)\n    181 \n    182     def encode(self, o):\n\nTypeError: Object of type 'datetime' is not JSON serializable\n<\/code><\/pre>\n\n<p>It fails instantly, the second I pus \"run\". It seems to me that the call to <code>comprehend.start_topics_detection_job<\/code> may be failing, leading to an error line 36, <code>print('Describe Job: ' + json.dumps(describe_result))<\/code>.<\/p>\n\n<p>What am I missing?<\/p>\n\n<p><strong>UPDATE<\/strong><\/p>\n\n<p>The same IAM role is being used for the notebook, as well as in the above code. Here are the permissions currently assigned to that IAM role:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6ihIr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6ihIr.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1556698045340,
        "Challenge_comment_count":3,
        "Challenge_created_time":1556211825007,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1586235824608,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55854377",
        "Challenge_link_count":5,
        "Challenge_participation_count":4,
        "Challenge_readability":14.9,
        "Challenge_reading_time":57.0,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":135.0612036111,
        "Challenge_title":"comprehend.start_topics_detection_job Fails with Silent Error?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":232,
        "Challenge_word_count":379,
        "Platform":"Stack Overflow",
        "Poster_created_time":1276294622427,
        "Poster_location":null,
        "Poster_reputation_count":4334.0,
        "Poster_view_count":496.0,
        "Solution_body":"<p>It turns out that there was nothing wrong with the call to <code>comprehend.describe_topics_detection_job<\/code> -- it was just returning, in <code>describe_result<\/code>, something that could not be json serialized, so <code>json.dumps(describe_result))<\/code> was throwing an error. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.8,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1528629350990,
        "Answerer_location":"Pakistan",
        "Answerer_reputation_count":439.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":121.4413405556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a pre-trained <code>keras<\/code> model which I have hosted on <code>AWS<\/code> using <code>AWS SageMaker<\/code>. I've got an <code>endpoint<\/code> and I can make successful <code>predictions<\/code> using the <code>Amazon SageMaker Notebook instance<\/code>.<\/p>\n<p>What I do there is that I serve a <code>.PNG image<\/code> like the following and the model gives me correct prediction.<\/p>\n<pre><code>file= s3.Bucket(bucketname).download_file(filename_1, 'normal.png')\nfile_name_1='normal.png'\n\n\nimport sagemaker\nfrom sagemaker.tensorflow.model import TensorFlowModel\n\nendpoint = 'tensorflow-inference-0000-11-22-33-44-55-666' #endpoint\n\npredictor=sagemaker.tensorflow.model.TensorFlowPredictor(endpoint, sagemaker_session)\ndata = np.array([resize(imread(file_name), (137, 310, 3))])\npredictor.predict(data)\n<\/code><\/pre>\n<p>Now I wanted to make predictions using a <code>mobile application<\/code>. For that I have to wrote a <code>Lambda function<\/code> in python and attached an <code>API gateway<\/code> to it. My <code>Lambda function<\/code> is the following.<\/p>\n<pre><code>import os\nimport sys\n\nCWD = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(CWD, &quot;lib&quot;))\n\nimport json\nimport base64\nimport boto3\nimport numpy as np\nfrom scipy import signal\nfrom scipy.signal import butter, lfilter\nfrom scipy.io import wavfile\nimport scipy.signal as sps\nimport io\nfrom io import BytesIO\nimport matplotlib.pylab as plt\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nfrom datetime import datetime\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom PIL import Image\n\nENDPOINT_NAME = 'tensorflow-inference-0000-11-22-33-44-55-666'\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    s3 = boto3.client(&quot;s3&quot;)\n    \n    # retrieving data from event.\n    get_file_content_from_postman = event[&quot;content&quot;]\n    \n    # decoding data.\n    decoded_file_name = base64.b64decode(get_file_content_from_postman)\n    \n    image = Image.open(io.BytesIO(decoded_file_name))\n\n    data = np.array([resize(imread(image), (137, 310, 3))])\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='text\/csv', Body=data)\n        \n    result = json.loads(response['Body'].read().decode())\n    \n    return result\n<\/code><\/pre>\n<p>The third last line is giving me error <code>'PngImageFile' object has no attribute 'read'<\/code>.\nAny idea what I am missing here?<\/p>",
        "Challenge_closed_time":1618052704636,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617615515810,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1618052912968,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66950948",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.3,
        "Challenge_reading_time":32.91,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":121.4413405556,
        "Challenge_title":"How to make inference to a keras model hosted on AWS SageMaker via AWS Lambda function?",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":114,
        "Challenge_word_count":251,
        "Platform":"Stack Overflow",
        "Poster_created_time":1528629350990,
        "Poster_location":"Pakistan",
        "Poster_reputation_count":439.0,
        "Poster_view_count":51.0,
        "Solution_body":"<p>I was missing one thing which was causing this error. After receiving the image data I used python list and then <code>json.dump<\/code> that list (of lists). Below is the code for reference.<\/p>\n<pre><code>import os\nimport sys\n\nCWD = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(CWD, &quot;lib&quot;))\n\nimport json\nimport base64\nimport boto3\nimport numpy as np\nimport io\nfrom io import BytesIO\nfrom skimage.io import imread\nfrom skimage.transform import resize\n\n# grab environment variable of Lambda Function\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    s3 = boto3.client(&quot;s3&quot;)\n    \n    # retrieving data from event.\n    get_file_content_from_postman = event[&quot;content&quot;]\n    \n    # decoding data.\n    decoded_file_name = base64.b64decode(get_file_content_from_postman)\n    \n    data = np.array([resize(imread(io.BytesIO(decoded_file_name)), (137, 310, 3))])\n    \n    payload = json.dumps(data.tolist())\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n        \n    result = json.loads(response['Body'].read().decode())\n    \n    return result\n        \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":15.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":106.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1539125320752,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":5233.5912852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to understand how parameters servers (PS's) work for distributed training in Tensorflow on Amazon SageMaker. <\/p>\n\n<p>To make things more concrete, I am able to run the example from AWS using PS's: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/tf-distributed-training.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/tf-distributed-training.ipynb<\/a><\/p>\n\n<p>Here is the code block that initializes the estimator for Tensorflow:<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ngit_config = {'repo': 'https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode', 'branch': 'master'}\n\nps_instance_type = 'ml.p3.2xlarge'\nps_instance_count = 2\n\nmodel_dir = \"\/opt\/ml\/model\"\n\ndistributions = {'parameter_server': {\n                    'enabled': True}\n                }\nhyperparameters = {'epochs': 60, 'batch-size' : 256}\n\nestimator_ps = TensorFlow(\n                       git_config=git_config,\n                       source_dir='tf-distribution-options\/code',\n                       entry_point='train_ps.py', \n                       base_job_name='ps-cifar10-tf',\n                       role=role,\n                       framework_version='1.13',\n                       py_version='py3',\n                       hyperparameters=hyperparameters,\n                       train_instance_count=ps_instance_count, \n                       train_instance_type=ps_instance_type,\n                       model_dir=model_dir,\n                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'dist'}],\n                       distributions=distributions)\n<\/code><\/pre>\n\n<p>Going through the documentation for Tensorflow, it seems that a device scope can be used for assigning a variable to a particular worker. However, I never see this done when running training jobs on SageMaker. In the example from AWS, the model is defined by:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/code\/model_def.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/code\/model_def.py<\/a><\/p>\n\n<p>Here is a snippet:<\/p>\n\n<pre><code>def get_model(learning_rate, weight_decay, optimizer, momentum, size, mpi=False, hvd=False):\n\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), padding='same', input_shape=(HEIGHT, WIDTH, DEPTH)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Conv2D(32, (3, 3)))\n\n    ...\n\n    model.add(Flatten())\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(NUM_CLASSES))\n    model.add(Activation('softmax'))\n\n    if mpi:\n        size = hvd.size()\n\n    if optimizer.lower() == 'sgd':\n        ...\n\n    if mpi:\n        opt = hvd.DistributedOptimizer(opt)\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=opt,\n                  metrics=['accuracy'])\n\n    return model\n<\/code><\/pre>\n\n<p>Here, there are no references to distribution strategies (except with MPI, but that flag is set to False for PS's). Somehow, Tensorflow or the SageMaker container is able to decide where the variables for each layer should be stored. However, I'm not seeing anything in the container code that does anything with the distribution strategy.<\/p>\n\n<p>I am able to run this code and train the model using 1 and 2 instances. When i do so, I see a decrease of almost 50% in the runtime, suggesting that a distributed training is occurring.<\/p>\n\n<p>My questions are:<\/p>\n\n<ol>\n<li>How does Tensorflow decide the distribution of variables on the PS's? In the example code, there is no explicit reference to devices. Somehow the distribution is done automatically.<\/li>\n<li>Is it possible to see which parameters have been assigned to each PS? Or to see what the communication between PS's looks like? If so, how?<\/li>\n<\/ol>",
        "Challenge_closed_time":1600204151310,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581363222683,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60157184",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":16.1,
        "Challenge_reading_time":47.73,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":5233.5912852778,
        "Challenge_title":"Tensorflow Parameter Servers on SageMaker",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":307,
        "Challenge_word_count":350,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416337478872,
        "Poster_location":null,
        "Poster_reputation_count":101.0,
        "Poster_view_count":13.0,
        "Solution_body":"<blockquote>\n<p>My questions are:<\/p>\n<p>How does Tensorflow decide the distribution of variables on the PS's?\nIn the example code, there is no explicit reference to devices.\nSomehow the distribution is done automatically.<\/p>\n<\/blockquote>\n<p>The TensorFlow image provided by SageMaker has the code to setup TF_CONFIG and launching parameter server for multi work training. See the code [here][1] The setup is for each node in the cluster there is a PS and a worker thread configured.<\/p>\n<p>It's not using any DistributionStrategy so the default strategy is used. See [here][2].<\/p>\n<p>If you would like to use a different DistributionStrategy or different TF_CONFIG you will need to disable <code>parameter_server<\/code> option when launching the SageMaker training job and set everything up in your training script.<\/p>\n<blockquote>\n<p>Is it possible to see which parameters have been assigned to each PS?\nOr to see what the communication between PS's looks like? If so, how?<\/p>\n<\/blockquote>\n<p>You should be able to get some information from the output log which can be found in CloudWatch. The link is available on the Training Job console page.\n[1]: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-training-toolkit\/blob\/master\/src\/sagemaker_tensorflow_container\/training.py#L37\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-training-toolkit\/blob\/master\/src\/sagemaker_tensorflow_container\/training.py#L37<\/a>\n[2]: <a href=\"https:\/\/www.tensorflow.org\/guide\/distributed_training#default_strategy\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/guide\/distributed_training#default_strategy<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.6,
        "Solution_reading_time":21.32,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":187.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1490674180056,
        "Answerer_location":"%Temp%",
        "Answerer_reputation_count":302.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":526.8721119444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How do i schedule Azure ML Experiments which is not deployed as web service?<\/p>\n\n<p>I have developed a Azure Experiment which imports data from on-premise database and exports data to SQL db. How can i schedule that to run weekly?<\/p>",
        "Challenge_closed_time":1502973065180,
        "Challenge_comment_count":0,
        "Challenge_created_time":1501076325577,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45328657",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":3.53,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":526.8721119444,
        "Challenge_title":"Scheduling Azure Machine Learning Experimnets",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":520,
        "Challenge_word_count":44,
        "Platform":"Stack Overflow",
        "Poster_created_time":1359784845430,
        "Poster_location":"India",
        "Poster_reputation_count":400.0,
        "Poster_view_count":147.0,
        "Solution_body":"<p>You can use <strong>Azure PowerShell<\/strong> for automating this task, and use <strong>Windows Task Scheduler<\/strong> to schedule this script to run automatically.<\/p>\n\n<p>For Azure PowerShell,<\/p>\n\n<p>You may visit <a href=\"https:\/\/github.com\/hning86\/azuremlps\" rel=\"nofollow noreferrer\"><strong>this page<\/strong><\/a> to setup an Azure PowerShell script. It's a long journey, but it's worth it. Make sure to <strong><em>follow the prerequisites to be installed on your local PC (Azure-PowerShell v4.0.1)<\/em><\/strong>.<\/p>\n\n<p>For Windows Task Scheduler,<\/p>\n\n<p>Visit <a href=\"https:\/\/www.metalogix.com\/help\/Content%20Matrix%20Console\/SharePoint%20Edition\/002_HowTo\/004_SharePointActions\/012_SchedulingPowerShell.htm\" rel=\"nofollow noreferrer\"><strong>this link<\/strong><\/a> to schedule your created Azure PowerShell script to run at a scheduled\/repeated time.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.7,
        "Solution_reading_time":11.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":84.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":6.8632988889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I understand the azure machine learning studio (classic) version using Anaconda distribution but my question is where would the python modules like pandas\/tensorflow are installed when using <strong>IPython interface of Azure ML<\/strong>. Is this on AML studio itself or in azure blob (AML studio uses blob as backend store)? <\/p>",
        "Challenge_closed_time":1578465543192,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578440246977,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1578440835316,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59637596",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":5.05,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":7.0267263889,
        "Challenge_title":"Where does Python modules installed on Azure Machine Learning Studio",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":136,
        "Challenge_word_count":59,
        "Platform":"Stack Overflow",
        "Poster_created_time":1500744375327,
        "Poster_location":null,
        "Poster_reputation_count":255.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>Here is my screenshots for tabs <code>EXPERIMENTS<\/code> and <code>NOTEBOOKS<\/code> in Azure Machine Learning Studio (classic), as the figures below.<\/p>\n\n<p>Fig 1. I created a <code>Excute Python Script<\/code> module with the code to print the <code>sys.path<\/code> and the real path of <code>pandas<\/code> installed.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/KdkJa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KdkJa.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 2. The <code>View output log<\/code> page of the code in Fig 1 shows <code>EXPERIMENTS<\/code> is a runtime of Anaconda on Windows. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/zo9te.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zo9te.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 3. I created a notebook named <code>demo<\/code> and run the same code as Fig 1, the result shows <code>NOTEBOOKS<\/code> is a runtime of Anaconda on Linux, even the notenooks url is started with <code>notebooks.azure.com<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/uAGRk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uAGRk.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 4. I used different commands like <code>lsb_release -a<\/code>, <code>fdisk -l<\/code>, <code>lsdev<\/code>, <code>ls \/dev<\/code>, <code>df -a<\/code> to try to see the Linux version and its disk or partition information, the result shows it's a Ubuntu Linux container.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/R4wC6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/R4wC6.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Other infomation what you want to know, you can try to check by yourself.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":10.7,
        "Solution_reading_time":22.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":187.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":54.1517388889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Allow me to ask again this question, as answers found on the forum did not help me so far.<\/p>\n\n<p>I am trying to convert a column from 'string' into 'numerical' data type. <\/p>\n\n<p>The column has no missing values and no errors, it comes from a CSV file. For the record, I tried modifing the format type of the column on the CSV file and saving it as a number, but later when importing the CSV file on Azure ML it was coded as string.<\/p>\n\n<p>So far, I have tried the following options:<\/p>\n\n<ul>\n<li><p>'<strong>Execute Python scrip<\/strong>t'. Unfortunately it does not work . It returns an error when I run the experiment. The Code I entered is:<\/p>\n\n<pre><code>import pandas as df\n\ndef azureml_main (df):\n  df.age=pd.to_numeric(df.age,errors=\u2019coerce\u2019)\n\nreturn df\n<\/code><\/pre><\/li>\n<li><p>Use '<strong>Edit Metadata<\/strong>' module. Select as Datatype: 'Integer' or 'Floating point' but I keep on getting an error when running the experiment.<\/p><\/li>\n<\/ul>\n\n<p>Please kindly let me know what your thoughts are.<\/p>\n\n<p>Thanks for your help.<\/p>\n\n<p>Josep Maria<\/p>\n\n<p>P.S: It's the second time I write in this forum. I hope this time it is well formulated.\n<a href=\"https:\/\/i.stack.imgur.com\/glLFo.jpg\" rel=\"nofollow noreferrer\">screenshot of 'Execute Python Script' error<\/a><\/p>",
        "Challenge_closed_time":1533375326832,
        "Challenge_comment_count":7,
        "Challenge_created_time":1533283734597,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1533293784207,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51668053",
        "Challenge_link_count":1,
        "Challenge_participation_count":8,
        "Challenge_readability":6.7,
        "Challenge_reading_time":16.63,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":25.4422875,
        "Challenge_title":"Change data type using Azure Machine Learning Studio",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":636,
        "Challenge_word_count":199,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>It looks like the Python script just needs a little updating. :)<\/p>\n\n<p>This should work since you get <code>dataframe1<\/code> automatically as a <code>pandas<\/code> data frame.<\/p>\n\n<pre><code>import pandas as pd\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n  dataframe1.age = pd.to_numeric(dataframe1.age, errors=\"coerce\")\n\n  return dataframe1\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1533488730467,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":4.77,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1370505440848,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":4.6262580556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a function I can use to get the instance type of my SageMaker instance.<\/p>\n<p>I basically want to do something like this<\/p>\n<pre><code>region = boto3.Session().region_name\n<\/code><\/pre>\n<p>but for the instance type.<\/p>\n<p>I know I can find it manually, but I want to automate it so that my script can work on any instance.<\/p>",
        "Challenge_closed_time":1658271130892,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658254476363,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73041737",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":4.61,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4.6262580556,
        "Challenge_title":"Print SageMaker instance type",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":40,
        "Challenge_word_count":57,
        "Platform":"Stack Overflow",
        "Poster_created_time":1657050754840,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You can use <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeNotebookInstance.html\" rel=\"nofollow noreferrer\">DescribeNotebookInstance<\/a> API to get the instance size.<\/p>\n<pre><code>sm_client = boto3.client(&quot;sagemaker&quot;)\nsm.describe_notebook_instance(\n    NotebookInstanceName=&lt;nb-name&gt;\n)['InstanceType']\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":24.0,
        "Solution_reading_time":5.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":14416.4976566667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to start my way with <a href=\"https:\/\/github.com\/allegroai\/clearml\" rel=\"nofollow noreferrer\">ClearML<\/a> (formerly known as Trains).<\/p>\n<p>I see on the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/getting_started\/index.html\" rel=\"nofollow noreferrer\">documentation<\/a> that I need to have server running, either on the ClearML platform itself, or on a remote machine using AWS etc.<\/p>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<p>According to <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/deploying_clearml\/index.html\" rel=\"nofollow noreferrer\">this<\/a> I can install the <code>trains-server<\/code> on any remote machine, so in theory I should also be able to install it on my local machine, but it still requires me to have Kubernetes or Docker, but I am not using any of them.<\/p>\n<p>Anyone had any luck using ClearML (or Trains, I think it's still quite the same API and all) on a local server?<\/p>\n<ul>\n<li>My OS is Ubuntu 18.04.<\/li>\n<\/ul>",
        "Challenge_closed_time":1609345139523,
        "Challenge_comment_count":2,
        "Challenge_created_time":1609343679217,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1609427009848,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65509754",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":10.3,
        "Challenge_reading_time":14.07,
        "Challenge_score_count":5,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.4056405556,
        "Challenge_title":"Can ClearML (formerly Trains) work a local server?",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":740,
        "Challenge_word_count":150,
        "Platform":"Stack Overflow",
        "Poster_created_time":1383083414230,
        "Poster_location":null,
        "Poster_reputation_count":2801.0,
        "Poster_view_count":131.0,
        "Solution_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<blockquote>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<\/blockquote>\n<p>A few options:<\/p>\n<ol>\n<li>The Clearml Free trier offers free hosting for your experiments, these experiment are only accessible to you, unless you specifically want to share them among your colleagues. This is probably the easiest way to <a href=\"https:\/\/app.community.clear.ml\" rel=\"nofollow noreferrer\">get started<\/a>.<\/li>\n<li>Install the ClearML-Server basically all you need is docker installed and you should be fine. There are full instructions <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/deploying_clearml\/clearml_server_linux_mac\/\" rel=\"nofollow noreferrer\">here<\/a> , this is the summary:<\/li>\n<\/ol>\n<pre class=\"lang-bash prettyprint-override\"><code>echo &quot;vm.max_map_count=262144&quot; &gt; \/tmp\/99-trains.conf\nsudo mv \/tmp\/99-trains.conf \/etc\/sysctl.d\/99-trains.conf\nsudo sysctl -w vm.max_map_count=262144\nsudo service docker restart\n\nsudo curl -L &quot;https:\/\/github.com\/docker\/compose\/releases\/latest\/download\/docker-compose-$(uname -s)-$(uname -m)&quot; -o \/usr\/local\/bin\/docker-compose\nsudo chmod +x \/usr\/local\/bin\/docker-compose\n\nsudo mkdir -p \/opt\/trains\/data\/elastic_7\nsudo mkdir -p \/opt\/trains\/data\/mongo\/db\nsudo mkdir -p \/opt\/trains\/data\/mongo\/configdb\nsudo mkdir -p \/opt\/trains\/data\/redis\nsudo mkdir -p \/opt\/trains\/logs\nsudo mkdir -p \/opt\/trains\/config\nsudo mkdir -p \/opt\/trains\/data\/fileserver\n\nsudo curl https:\/\/raw.githubusercontent.com\/allegroai\/trains-server\/master\/docker-compose.yml -o \/opt\/trains\/docker-compose.yml\ndocker-compose -f \/opt\/trains\/docker-compose.yml up -d\n<\/code><\/pre>\n<ol start=\"3\">\n<li>ClearML also supports full offline mode (i.e. no outside connection is made). Once your experiment completes, you can manually import the run to your server (either self hosted or free tier server)<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.set_offline(True)\ntask = Task.init(project_name='examples', task_name='offline mode experiment')\n<\/code><\/pre>\n<p>When the process ends you will get a link to a zip file containing the output of the entire offline session:<\/p>\n<pre><code>ClearML Task: Offline session stored in \/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip\n<\/code><\/pre>\n<p>Later you can import the session with:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.import_offline_session('\/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip')\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1661326401412,
        "Solution_link_count":4.0,
        "Solution_readability":14.4,
        "Solution_reading_time":35.39,
        "Solution_score_count":6.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":265.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1454844135036,
        "Answerer_location":"T\u00fcrkiye",
        "Answerer_reputation_count":462.0,
        "Answerer_view_count":83.0,
        "Challenge_adjusted_solved_time":1.2255941667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created an endpoint in <code>aws sagemaker<\/code> and it works well, I created a <code>lambda<\/code> function(<code>python3.6<\/code>) that takes files from <code>S3<\/code>, invoke the endpoint and then put the output in a file in <code>S3<\/code>. <\/p>\n\n<p>I wonder if I can create the endpoint at every event(a file uploaded in an <code>s3 bucket)<\/code> and then delete the endpoint <\/p>",
        "Challenge_closed_time":1550836836772,
        "Challenge_comment_count":0,
        "Challenge_created_time":1550832424633,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1550840459832,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54825390",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":5.57,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.2255941667,
        "Challenge_title":"create aws sagemker endpoint with lambda function",
        "Challenge_topic":"Lambda Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":180,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Poster_created_time":1502010899808,
        "Poster_location":"Tunis, Tunisia",
        "Poster_reputation_count":109.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>Yes you can Using <code>S3<\/code> event notification for object-created and call a <code>lambda<\/code> for creating endpoint for <code>sagemaker<\/code>.<\/p>\n\n<p>This example shows how to make <code>object-created event trigger lambda<\/code><\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-s3.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-s3.html<\/a><\/p>\n\n<p>You can use <code>python sdk<\/code> to create endpoint for <code>sagemaker<\/code><\/p>\n\n<p><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_endpoint\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_endpoint<\/a><\/p>\n\n<p>But it might be slow for creating endpoint so you may be need to wait.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1550837083230,
        "Solution_link_count":4.0,
        "Solution_readability":23.8,
        "Solution_reading_time":11.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1577353307072,
        "Answerer_location":null,
        "Answerer_reputation_count":491.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":0.6647269444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to connect to an Azure SQL Database from inside Azure Machine Learning Studio. Based on <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py<\/a>, it seems that the recommended pattern is to create a Datastore using the Datastore.register_azure_sql_database method as follows:<\/p>\n<pre><code>import os\nfrom azureml.core import Workspace, Datastore\n\nws = Workspace.from_config() # asks for interactive authentication the first time\n\nsql_datastore_name  = &quot;datastore_test_01&quot; # any name should be fine\nserver_name         = os.getenv(&quot;SQL_SERVERNAME&quot;    , &quot;{SQL_SERVERNAME}&quot;) # Name of the Azure SQL server\ndatabase_name       = os.getenv(&quot;SQL_DATABASENAME&quot;  , &quot;{SQL_DATABASENAME}&quot;) # Name of the Azure SQL database\nusername            = os.getenv(&quot;SQL_USER_NAME&quot;     , &quot;{SQL_USER_NAME}&quot;) # The username of the database user.\npassword            = os.getenv(&quot;SQL_USER_PASSWORD&quot; , &quot;{SQL_USER_PASSWORD}&quot;) # The password of the database user.\n\nsql_datastore = Datastore.register_azure_sql_database(workspace      = ws,\n                                                      datastore_name = sql_datastore_name,\n                                                      server_name    = server_name,\n                                                      database_name  = database_name,\n                                                      username       = username,\n                                                      password       = password)\n<\/code><\/pre>\n<p>I am pretty sure I have set all parameters right, having copied them from the ADO.NET connection string at my SQL Database resource --&gt; Settings --&gt; Connection strings:<\/p>\n<pre><code>Server=tcp:{SQL_SERVERNAME}.database.windows.net,1433;Initial Catalog={SQL_DATABASENAME};Persist Security Info=False;User ID={SQL_USER_NAME};Password={SQL_USER_PASSWORD};MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;\n<\/code><\/pre>\n<p>However, I get the following error:<\/p>\n<pre><code>Registering datastore failed with a 400 error code and error message 'Azure SQL Database Error -2146232060: Please check the correctness of the datastore information.'\n<\/code><\/pre>\n<p>Am I missing something? E.g., a firewall rule? I have also tried adding the Azure ML compute resource's public IP address to the list of allowed IP addresses in my SQL Database resource, but still no success.<\/p>\n<hr \/>\n<p><strong>UPDATE<\/strong>: adding <code>skip_validation = True<\/code> to <code>Datastore.register_azure_sql_database<\/code> solves the issue. I can then query the data with<\/p>\n<pre><code>from azureml.core import Dataset\nfrom azureml.data.datapath import DataPath\n\nquery   = DataPath(sql_datastore, 'SELECT * FROM my_table')\ntabular = Dataset.Tabular.from_sql_query(query, query_timeout = 10)\ndf = tabular.to_pandas_dataframe()\n<\/code><\/pre>",
        "Challenge_closed_time":1598979186260,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598976793243,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1599032818923,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63691515",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":37.78,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":0.6647269444,
        "Challenge_title":"Azure Machine Learning Studio: cannot create Datastore from Azure SQL Database",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":793,
        "Challenge_word_count":262,
        "Platform":"Stack Overflow",
        "Poster_created_time":1502454917960,
        "Poster_location":"Milano, MI, Italia",
        "Poster_reputation_count":132.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>is the datastore behind vnet? where are you running the registration code above? On a compute instance behind the same vnet?\nhere is the doc that describe what you need to do to connect to data behind vnet:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-virtual-network#use-datastores-and-datasets\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-virtual-network#use-datastores-and-datasets<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.0,
        "Solution_reading_time":6.46,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1361339272692,
        "Answerer_location":"NYC",
        "Answerer_reputation_count":6281.0,
        "Answerer_view_count":958.0,
        "Challenge_adjusted_solved_time":3630.1468475,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>From <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Sagemaker python SDK<\/a> I have seen two API, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.ScriptProcessor\" rel=\"nofollow noreferrer\">ScriptProcessor<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.Processor\" rel=\"nofollow noreferrer\">Processor<\/a>. It seems like we can achieve the same goals using either of them, the only difference I noticed ScriptProcessor support docker <code>command<\/code> parameter on the other hand Processor support docker <code>entrypoint<\/code> parameter. Is there any other difference amongst them? <\/p>",
        "Challenge_closed_time":1604879360048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591810831397,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62309772",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":21.3,
        "Challenge_reading_time":11.06,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3630.1468475,
        "Challenge_title":"Difference between Processor and ScriptProcessor in AWS Sagemaker SDK",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":400,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370231111260,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":5295.0,
        "Poster_view_count":455.0,
        "Solution_body":"<p><code>sagemaker.processing.ScriptProcessor<\/code> subclasses <code>sagemaker.processing.Processor<\/code>. <code>ScriptProcessor<\/code> can be used to write a custom processing script. <code>Processor<\/code> can be subclassed to create a <code>CustomProcessor<\/code> class for a more complex use case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.5,
        "Solution_reading_time":4.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1619402503747,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":669.2438802778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am currently working on creating a Sagemaker Pipeline to train a Tensorflow model. I'm new to this area and I have been following <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-pipelines\/tabular\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint.ipynb\" rel=\"nofollow noreferrer\">this guide<\/a> created by AWS as well as the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html\" rel=\"nofollow noreferrer\">standard pipeline workflow<\/a> listed in the Sagemaker developer guide.<\/p>\n<p>I have a pipeline that runs without error when I only include the preprocessing, training, evaluation, and condition steps. When I add the register step:<\/p>\n<pre><code># Package evaluation metrics into an evaluation report `PropertyFile`\nevaluation_report = PropertyFile(\n        name=&quot;EvaluationReport&quot;, output_name=&quot;evaluation&quot;, path=&quot;evaluation_report.json&quot;\n)\n\n# Create ModelMetrics object using the evaluation report from the evaluation step\n# A ModelMetrics object contains metrics captured from a model.\nmodel_metrics = ModelMetrics(model_statistics=evaluation_report)\n\n# Create a RegisterModel step, which registers the model with Sagemaker Model Registry.\nregister_step = RegisterModel(\n    name=&quot;Foo&quot;,\n    estimator=estimator,\n    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n    content_types=[&quot;text\/csv&quot;],\n    response_types=[&quot;text\/csv&quot;],\n    inference_instances=config[&quot;instance&quot;][&quot;inference&quot;],\n    transform_instances=config[&quot;instance&quot;][&quot;transform&quot;],\n    model_package_group_name=&quot;Bar&quot;,\n    model_metrics=model_metrics,\n    approval_status=&quot;approved&quot;,\n)\n<\/code><\/pre>\n<p>to the condition step's <code>if_steps<\/code>:<\/p>\n<pre><code># Create a Sagemaker Pipelines ConditionStep, using the condition above.\n# Enter the steps to perform if the condition returns True \/ False.\ncond_step = ConditionStep(\n    name=&quot;MSE-Lower-Than-Threshold-Condition&quot;,\n    conditions=[cond_lte],\n    if_steps=[register_step],\n    else_steps=[],\n)\n<\/code><\/pre>\n<p>I get the following trace:<\/p>\n<pre><code>PropertyFile(name='EvaluationReport', output_name='evaluation', path='evaluation_report.json')\nNo finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\nTraceback (most recent call last):\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 474, in &lt;module&gt;\n    main()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 466, in main\n    pipeline = define_pipeline()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 457, in define_pipeline\n    print(json.loads(pipeline.definition()))\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/pipeline.py&quot;, line 257, in definition\n    request_dict = self.to_request()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/pipeline.py&quot;, line 89, in to_request\n    &quot;Steps&quot;: list_to_request(self.steps),\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/utilities.py&quot;, line 37, in list_to_request\n    request_dicts.append(entity.to_request())\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 99, in to_request\n    &quot;Arguments&quot;: self.arguments,\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/condition_step.py&quot;, line 87, in arguments\n    IfSteps=list_to_request(self.if_steps),\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/utilities.py&quot;, line 39, in list_to_request\n    request_dicts.extend(entity.request_dicts())\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/step_collections.py&quot;, line 50, in request_dicts\n    return [step.to_request() for step in self.steps]\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/step_collections.py&quot;, line 50, in &lt;listcomp&gt;\n    return [step.to_request() for step in self.steps]\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 209, in to_request\n    step_dict = super().to_request()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 99, in to_request\n    &quot;Arguments&quot;: self.arguments,\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/_utils.py&quot;, line 423, in arguments\n    model_package_args = get_model_package_args(\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/session.py&quot;, line 4217, in get_model_package_args\n    model_package_args[&quot;model_metrics&quot;] = model_metrics._to_request_dict()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/model_metrics.py&quot;, line 66, in _to_request_dict\n    model_quality[&quot;Statistics&quot;] = self.model_statistics._to_request_dict()\nAttributeError: 'PropertyFile' object has no attribute '_to_request_dict'\n<\/code><\/pre>\n<p>From this trace I see two, potentially related, issues. The immediate issue is the <code>AttributeError: 'PropertyFile' object has no attribute '_to_request_dict'<\/code>. I haven't been able to find any information on why we might be receiving it between forums and Sagemaker documentation.<\/p>\n<p>I also see a sneaky issue towards the top of the trace that has plagued me all day. The line <code>No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config<\/code> tells me that the register step is using our estimator when it should be waiting until after the training step has run. I can't seem to find any reference to this error, besides a somewhat-similar <a href=\"https:\/\/datascience.stackexchange.com\/questions\/100113\/how-to-fix-sagemakers-no-finished-training-job-found-associated-with-this-esti\">stack exchange post<\/a>.<\/p>\n<p>I've compared my code to the AWS-published examples many times and I'm confident that I'm not doing anything taboo. Would anyone be able to shine some light on what these errors are suggesting? Is there any more information or code that would be relevant?<\/p>\n<p>Thanks so much!<\/p>",
        "Challenge_closed_time":1642620406232,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640211128263,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70455676",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":19.9,
        "Challenge_reading_time":93.25,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":57,
        "Challenge_solved_time":669.2438802778,
        "Challenge_title":"AWS Sagemaker Pipelines throws a \"No finished training job found associated with this estimator\" after introducing a register step",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1006,
        "Challenge_word_count":526,
        "Platform":"Stack Overflow",
        "Poster_created_time":1619402503747,
        "Poster_location":null,
        "Poster_reputation_count":61.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I was able to work through the issue! Here is the code for the register step that ended up working with my Tensorflow model:<\/p>\n<pre><code># Package the model\npipeline_model = PipelineModel(models=[model], role=params[&quot;role&quot;].default_value, sagemaker_session=session)\n\n# Create a RegisterModel step, which registers the model with Sagemaker Model Registry.\nregister_step = RegisterModel(\n    name=&quot;Bar&quot;,\n    model=pipeline_model,\n    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n    content_types=[&quot;application\/json&quot;],\n    response_types=[&quot;application\/json&quot;],\n    inference_instances=[config[&quot;instance&quot;][&quot;inference&quot;]],\n    transform_instances=[config[&quot;instance&quot;][&quot;transform&quot;]],\n    model_package_group_name=&quot;Foo&quot;,\n    approval_status=&quot;Approved&quot;,\n)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":26.5,
        "Solution_reading_time":11.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":27.0735711111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have trained a semantic segmentation model using the sagemaker and the out has been saved to a s3 bucket. I want to load this model from the s3 to predict some images in sagemaker. <\/p>\n\n<p>I know how to predict if I leave the notebook instance running after the training as its just an easy deploy but doesn't really help if I want to use an older model.<\/p>\n\n<p>I have looked at these sources and been able to come up with something myself but it doesn't work hence me being here:<\/p>\n\n<p><a href=\"https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker\" rel=\"noreferrer\">https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker<\/a>\n<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a><\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb<\/a><\/p>\n\n<p>My code is this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.pipeline import PipelineModel\nfrom sagemaker.model import Model\n\ns3_model_bucket = 'bucket'\ns3_model_key_prefix = 'prefix'\ndata = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\nmodels = ss_model.create_model() # ss_model is my sagemaker.estimator\n\nmodel = PipelineModel(name=data, role=role, models= [models])\nss_predictor = model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>",
        "Challenge_closed_time":1558621559712,
        "Challenge_comment_count":0,
        "Challenge_created_time":1558522248223,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1558524094856,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56255154",
        "Challenge_link_count":8,
        "Challenge_participation_count":2,
        "Challenge_readability":22.6,
        "Challenge_reading_time":27.69,
        "Challenge_score_count":6,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":27.5865247222,
        "Challenge_title":"How to use a pretrained model from s3 to predict some data?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":7404,
        "Challenge_word_count":152,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558518328152,
        "Poster_location":"London, UK",
        "Poster_reputation_count":79.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>You can actually instantiate a Python SDK <code>model<\/code> object from existing artifacts, and deploy it to an endpoint. This allows you to deploy a model from trained artifacts, without having to retrain in the notebook. For example, for the semantic segmentation model:<\/p>\n\n<pre><code>trainedmodel = sagemaker.model.Model(\n    model_data='s3:\/\/...model path here..\/model.tar.gz',\n    image='685385470294.dkr.ecr.eu-west-1.amazonaws.com\/semantic-segmentation:latest',  # example path for the semantic segmentation in eu-west-1\n    role=role)  # your role here; could be different name\n\ntrainedmodel.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>\n\n<p>And similarly, you can instantiate a predictor object on a deployed endpoint from any authenticated client supporting the SDK, with the following command:<\/p>\n\n<pre><code>predictor = sagemaker.predictor.RealTimePredictor(\n    endpoint='endpoint name here',\n    content_type='image\/jpeg',\n    accept='image\/png')\n<\/code><\/pre>\n\n<p>More on those abstractions:<\/p>\n\n<ul>\n<li><code>Model<\/code>: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html<\/a><\/li>\n<li><code>Predictor<\/code>:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":22.9,
        "Solution_reading_time":18.55,
        "Solution_score_count":13.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":112.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1247721040396,
        "Answerer_location":"Letterkenny, Ireland",
        "Answerer_reputation_count":2017.0,
        "Answerer_view_count":166.0,
        "Challenge_adjusted_solved_time":47.612795,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained an automated machine learning model on an Azure ML compute cluster.<\/p>\n\n<p>I am trying to use that remote model in my Azure hosted Jupyter notebook. <\/p>\n\n<p>This is the code in the workbook that tries to load the remote model:<\/p>\n\n<pre><code>remote_run = AutoMLRun(experiment = experiment, run_id = '... Experiment id ...')\nremote_best_run, remote_fitted_model = remote_run.get_output()\n<\/code><\/pre>\n\n<p>This code fails with the following error:<\/p>\n\n<blockquote>\n  <p>ModuleNotFoundError                       Traceback (most recent call\n  last)  in \n        2 # remote_run.wait_for_completion(show_output = True)\n        3 import pandas as pd\n  ----> 4 remote_best_run, remote_fitted_model = remote_run.get_output()\n        5 #!pip list<\/p>\n  \n  <p>~\/anaconda3_501\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/run.py\n  in get_output(self, iteration, metric)\n      406 \n      407         with open(model_local, \"rb\") as model_file:\n  --> 408             fitted_model = pickle.load(model_file)\n      409         return curr_run, fitted_model\n      410 <\/p>\n  \n  <p>ModuleNotFoundError: No module named 'pandas._libs.tslibs.timestamps'<\/p>\n<\/blockquote>\n\n<p>Presumably there is a version difference between what is installed on the Azure ML compute cluster vs what is installed in the kernel of the Jupyter notebook, or I have a package missing. <\/p>\n\n<p>How can I make this remote model work?<\/p>\n\n<p>For additional reference, I am following this tutorial: <a href=\"https:\/\/notebooks.azure.com\/NileshA\/projects\/GlobalAI\" rel=\"nofollow noreferrer\">https:\/\/notebooks.azure.com\/NileshA\/projects\/GlobalAI<\/a><\/p>\n\n<p><strong>Note 1<\/strong> I can also reproduce this error by running the following code in my jupyter notebook: <\/p>\n\n<pre><code>import pickle\n\nwith open('model.pkl', 'rb') as p_f:\n    data = pickle.load(p_f)\n<\/code><\/pre>",
        "Challenge_closed_time":1545041889310,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544869470737,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1544870483248,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53791461",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":23.43,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":47.8940480556,
        "Challenge_title":"Import error when using remote Azure Automated Machine Learning model in Azure notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":353,
        "Challenge_word_count":204,
        "Platform":"Stack Overflow",
        "Poster_created_time":1247721040396,
        "Poster_location":"Letterkenny, Ireland",
        "Poster_reputation_count":2017.0,
        "Poster_view_count":166.0,
        "Solution_body":"<p>I emailed the Auto ML helpdesk and they solved the problem.<\/p>\n\n<p>Quote from them: <\/p>\n\n<blockquote>\n  <p>We have a bug where the AutoML inferencing fails because the pandas\n  version is 0.22.0 which doesn\u2019t have some API support.<\/p>\n<\/blockquote>\n\n<p>I upgraded pandas on my hosted notebook to version 0.23.4, and after this the model unpickles and works successfully<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":4.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1634692867416,
        "Answerer_location":"Philippines",
        "Answerer_reputation_count":3105.0,
        "Answerer_view_count":290.0,
        "Challenge_adjusted_solved_time":13.8649916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am just curious for more information on what an intermediate step actually is and how to use pruning if you're using a different ml library that isn't in the tutorial section eg) XGB, Pytorch etc.<\/p>\n<p>For example:<\/p>\n<pre><code>X, y = load_iris(return_X_y=True)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y)\nclasses = np.unique(y)\nn_train_iter = 100\n\ndef objective(trial):\n    global num_pruned\n    alpha = trial.suggest_float(&quot;alpha&quot;, 0.0, 1.0)\n    clf = SGDClassifier(alpha=alpha)\n    for step in range(n_train_iter):\n        clf.partial_fit(X_train, y_train, classes=classes)\n\n        intermediate_value = clf.score(X_valid, y_valid)\n        trial.report(intermediate_value, step)\n\n        if trial.should_prune():\n            raise optuna.TrialPruned()\n\n    return clf.score(X_valid, y_valid)\n\n\nstudy = optuna.create_study(\n    direction=&quot;maximize&quot;,\n    pruner=optuna.pruners.HyperbandPruner(\n        min_resource=1, max_resource=n_train_iter, reduction_factor=3\n    ),\n)\nstudy.optimize(objective, n_trials=30)\n<\/code><\/pre>\n<p>What is the point of the <code>for step in range()<\/code> section? Doesn't doing this just make the optimisation take more time and won't you yield the same result for every step in the loop?<\/p>\n<p>I'm really trying to figure out the need for <code>for step in range()<\/code> and is it required every time you wish to use pruning?<\/p>",
        "Challenge_closed_time":1637119128723,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637069214753,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69990009",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":17.82,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":13.8649916667,
        "Challenge_title":"Understanding Intermediate Values and Pruning in Optuna",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":841,
        "Challenge_word_count":154,
        "Platform":"Stack Overflow",
        "Poster_created_time":1482279816096,
        "Poster_location":null,
        "Poster_reputation_count":138.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>The basic model creation can be done by passing a complete training datasets once. But there are models that can still be improved (an increase in accuracy) by re-training again on the same training datasets.<\/p>\n<p>To see to it that we are not wasting resources here, we would check the accuracy after every step using the validation datasets via <code>intermediate_score<\/code> if accuracy improves, if not we prune the whole trial skipping other steps. Then we go for next trial asking another value of alpha - the hyperparameter that we are trying to determine to have the greatest accuracy on the validation datasets.<\/p>\n<p>For other libraries, it is just a matter of asking ourselves what do we want with our model, accuracy for sure is a good criteria to measure the model's competency. There can be others.<\/p>\n<p>Example optuna pruning, I want the model to continue re-training but only at my specific conditions. If intermediate value cannot defeat my best_accuracy and if steps are already more than half of my max iteration then prune this trial.<\/p>\n<pre><code>best_accuracy = 0.0\n\n\ndef objective(trial):\n    global best_accuracy\n\n    alpha = trial.suggest_float(&quot;alpha&quot;, 0.0, 1.0)\n    clf = SGDClassifier(alpha=alpha)\n\n    for step in range(n_train_iter):\n        clf.partial_fit(X_train, y_train, classes=classes)\n\n        if step &gt; n_train_iter\/\/2:\n            intermediate_value = clf.score(X_valid, y_valid)\n\n            if intermediate_value &lt; best_accuracy:\n                raise optuna.TrialPruned()\n\n    best_accuracy = clf.score(X_valid, y_valid)\n\n    return best_accuracy\n<\/code><\/pre>\n<p>Optuna has specialized pruners at <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/pruners.html\" rel=\"nofollow noreferrer\">https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/pruners.html<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.4,
        "Solution_reading_time":22.36,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":221.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1553609947192,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":221.1468555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've created an SageMaker Endpoint from a trained DeepAR-Model using following code:<\/p>\n<pre><code>job_name = estimator.latest_training_job.job_name\n\nendpoint_name = sagemaker_session.endpoint_from_job(\n    job_name=job_name,\n    initial_instance_count=1,\n    instance_type=&quot;ml.m4.xlarge&quot;,\n    image_uri=image_uri,\n    role=role\n)\n<\/code><\/pre>\n<p>Now I want to test my model using a <code>test.json<\/code>-Dataset (<strong>66.2MB<\/strong>).\nI've created that file according to various tutorials\/sample-notebooks (same as <code>train.json<\/code>, but with <code>prediction-length<\/code>-less values.<\/p>\n<p>For that, I've written the following code:<\/p>\n<pre><code>class DeepARPredictor(sagemaker.predictor.Predictor):\n    def set_prediction_parameters(self, freq, prediction_length):\n        self.freq = freq\n        self.prediction_length = prediction_length\n\n    def predict(self, ts, num_samples=100, quantiles=[&quot;0.1&quot;, &quot;0.5&quot;, &quot;0.9&quot;]):\n        prediction_times = [x.index[-1] + pd.Timedelta(1, unit=self.freq) for x in ts]\n        req = self.__encode_request(ts, num_samples, quantiles)\n        res = super(DeepARPredictor, self).predict(req, initial_args={&quot;ContentType&quot;: &quot;application\/json&quot;})\n        return self.__decode_response(res, prediction_times)\n\n    def __encode_request(self, ts, num_samples, quantiles):\n        instances = [{&quot;start&quot;: str(ts[k].index[0]), &quot;target&quot;: list(ts[k])} for k in range(len(ts))]\n        configuration = {\n            &quot;num_samples&quot;: num_samples,\n            &quot;output_types&quot;: [&quot;quantiles&quot;],\n            &quot;quantiles&quot;: quantiles,\n        }\n        http_request_data = {&quot;instances&quot;: instances, &quot;configuration&quot;: configuration}\n        return json.dumps(http_request_data).encode( &quot;utf-8&quot;)\n\n    def __decode_response(self, response, prediction_times):\n        response_data = json.loads(response.decode(&quot;utf-8&quot;))\n        list_of_df = []\n        for k in range(len(prediction_times)):\n            prediction_index = pd.date_range(\n                start=prediction_times[k], freq=self.freq, periods=self.prediction_length\n            )\n            list_of_df.append(\n                pd.DataFrame(data=response_data[&quot;predictions&quot;][k][&quot;quantiles&quot;], index=prediction_index)\n            )\n        return list_of_df\n<\/code><\/pre>\n<p>But after running the following block:<\/p>\n<pre><code>predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\npredictor.set_prediction_parameters(freq, prediction_length)\nlist_of_df = predictor.predict(time_series_training)\n<\/code><\/pre>\n<p>I've getting a BrokenPipeError:<\/p>\n<pre><code>---------------------------------------------------------------------------\nBrokenPipeError                           Traceback (most recent call last)\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    676                 headers=headers,\n--&gt; 677                 chunked=chunked,\n    678             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n    391         else:\n--&gt; 392             conn.request(method, url, **httplib_request_kw)\n    393 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\n   1261         &quot;&quot;&quot;Send a complete request to the server.&quot;&quot;&quot;\n-&gt; 1262         self._send_request(method, url, body, headers, encode_chunked)\n   1263 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_request(self, method, url, body, headers, *args, **kwargs)\n     92         rval = super(AWSConnection, self)._send_request(\n---&gt; 93             method, url, body, headers, *args, **kwargs)\n     94         self._expect_header_set = False\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\n   1307             body = _encode(body, 'body')\n-&gt; 1308         self.endheaders(body, encode_chunked=encode_chunked)\n   1309 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\n   1256             raise CannotSendHeader()\n-&gt; 1257         self._send_output(message_body, encode_chunked=encode_chunked)\n   1258 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_output(self, message_body, *args, **kwargs)\n    119             message_body = None\n--&gt; 120         self.send(msg)\n    121         if self._expect_header_set:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in send(self, str)\n    203             return\n--&gt; 204         return super(AWSConnection, self).send(str)\n    205 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in send(self, data)\n    995         try:\n--&gt; 996             self.sock.sendall(data)\n    997         except TypeError:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in sendall(self, data, flags)\n    974                 while count &lt; amount:\n--&gt; 975                     v = self.send(byte_view[count:])\n    976                     count += v\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in send(self, data, flags)\n    943                     self.__class__)\n--&gt; 944             return self._sslobj.write(data)\n    945         else:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in write(self, data)\n    641         &quot;&quot;&quot;\n--&gt; 642         return self._sslobj.write(data)\n    643 \n\nBrokenPipeError: [Errno 32] Broken pipe\n\nDuring handling of the above exception, another exception occurred:\n\nProtocolError                             Traceback (most recent call last)\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/httpsession.py in send(self, request)\n    319                 decode_content=False,\n--&gt; 320                 chunked=self._chunked(request.headers),\n    321             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    726             retries = retries.increment(\n--&gt; 727                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n    728             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/util\/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)\n    378             # Disabled, indicate to re-raise the error.\n--&gt; 379             raise six.reraise(type(error), error, _stacktrace)\n    380 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/packages\/six.py in reraise(tp, value, tb)\n    733             if value.__traceback__ is not tb:\n--&gt; 734                 raise value.with_traceback(tb)\n    735             raise value\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    676                 headers=headers,\n--&gt; 677                 chunked=chunked,\n    678             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n    391         else:\n--&gt; 392             conn.request(method, url, **httplib_request_kw)\n    393 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\n   1261         &quot;&quot;&quot;Send a complete request to the server.&quot;&quot;&quot;\n-&gt; 1262         self._send_request(method, url, body, headers, encode_chunked)\n   1263 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_request(self, method, url, body, headers, *args, **kwargs)\n     92         rval = super(AWSConnection, self)._send_request(\n---&gt; 93             method, url, body, headers, *args, **kwargs)\n     94         self._expect_header_set = False\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\n   1307             body = _encode(body, 'body')\n-&gt; 1308         self.endheaders(body, encode_chunked=encode_chunked)\n   1309 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\n   1256             raise CannotSendHeader()\n-&gt; 1257         self._send_output(message_body, encode_chunked=encode_chunked)\n   1258 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_output(self, message_body, *args, **kwargs)\n    119             message_body = None\n--&gt; 120         self.send(msg)\n    121         if self._expect_header_set:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in send(self, str)\n    203             return\n--&gt; 204         return super(AWSConnection, self).send(str)\n    205 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in send(self, data)\n    995         try:\n--&gt; 996             self.sock.sendall(data)\n    997         except TypeError:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in sendall(self, data, flags)\n    974                 while count &lt; amount:\n--&gt; 975                     v = self.send(byte_view[count:])\n    976                     count += v\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in send(self, data, flags)\n    943                     self.__class__)\n--&gt; 944             return self._sslobj.write(data)\n    945         else:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in write(self, data)\n    641         &quot;&quot;&quot;\n--&gt; 642         return self._sslobj.write(data)\n    643 \n\nProtocolError: ('Connection aborted.', BrokenPipeError(32, 'Broken pipe'))\n\nDuring handling of the above exception, another exception occurred:\n\nConnectionClosedError                     Traceback (most recent call last)\n&lt;ipython-input-14-95dda20e8a70&gt; in &lt;module&gt;\n      1 predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n      2 predictor.set_prediction_parameters(freq, prediction_length)\n----&gt; 3 list_of_df = predictor.predict(time_series_training)\n\n&lt;ipython-input-13-a0fbac2b9b07&gt; in predict(self, ts, num_samples, quantiles)\n      7         prediction_times = [x.index[-1] + pd.Timedelta(1, unit=self.freq) for x in ts]\n      8         req = self.__encode_request(ts, num_samples, quantiles)\n----&gt; 9         res = super(DeepARPredictor, self).predict(req, initial_args={&quot;ContentType&quot;: &quot;application\/json&quot;})\n     10         return self.__decode_response(res, prediction_times)\n     11 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant)\n    123 \n    124         request_args = self._create_request_args(data, initial_args, target_model, target_variant)\n--&gt; 125         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n    126         return self._handle_response(response)\n    127 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    355                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    356             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 357             return self._make_api_call(operation_name, kwargs)\n    358 \n    359         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    661         else:\n    662             http, parsed_response = self._make_request(\n--&gt; 663                 operation_model, request_dict, request_context)\n    664 \n    665         self.meta.events.emit(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_request(self, operation_model, request_dict, request_context)\n    680     def _make_request(self, operation_model, request_dict, request_context):\n    681         try:\n--&gt; 682             return self._endpoint.make_request(operation_model, request_dict)\n    683         except Exception as e:\n    684             self.meta.events.emit(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in make_request(self, operation_model, request_dict)\n    100         logger.debug(&quot;Making request for %s with params: %s&quot;,\n    101                      operation_model, request_dict)\n--&gt; 102         return self._send_request(request_dict, operation_model)\n    103 \n    104     def create_request(self, params, operation_model=None):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _send_request(self, request_dict, operation_model)\n    135             request, operation_model, context)\n    136         while self._needs_retry(attempts, operation_model, request_dict,\n--&gt; 137                                 success_response, exception):\n    138             attempts += 1\n    139             # If there is a stream associated with the request, we need\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _needs_retry(self, attempts, operation_model, request_dict, response, caught_exception)\n    254             event_name, response=response, endpoint=self,\n    255             operation=operation_model, attempts=attempts,\n--&gt; 256             caught_exception=caught_exception, request_dict=request_dict)\n    257         handler_response = first_non_none_response(responses)\n    258         if handler_response is None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in emit(self, event_name, **kwargs)\n    354     def emit(self, event_name, **kwargs):\n    355         aliased_event_name = self._alias_event_name(event_name)\n--&gt; 356         return self._emitter.emit(aliased_event_name, **kwargs)\n    357 \n    358     def emit_until_response(self, event_name, **kwargs):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in emit(self, event_name, **kwargs)\n    226                  handlers.\n    227         &quot;&quot;&quot;\n--&gt; 228         return self._emit(event_name, kwargs)\n    229 \n    230     def emit_until_response(self, event_name, **kwargs):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in _emit(self, event_name, kwargs, stop_on_response)\n    209         for handler in handlers_to_call:\n    210             logger.debug('Event %s: calling handler %s', event_name, handler)\n--&gt; 211             response = handler(**kwargs)\n    212             responses.append((handler, response))\n    213             if stop_on_response and response is not None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempts, response, caught_exception, **kwargs)\n    181 \n    182         &quot;&quot;&quot;\n--&gt; 183         if self._checker(attempts, response, caught_exception):\n    184             result = self._action(attempts=attempts)\n    185             logger.debug(&quot;Retry needed, action of: %s&quot;, result)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    249     def __call__(self, attempt_number, response, caught_exception):\n    250         should_retry = self._should_retry(attempt_number, response,\n--&gt; 251                                           caught_exception)\n    252         if should_retry:\n    253             if attempt_number &gt;= self._max_attempts:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in _should_retry(self, attempt_number, response, caught_exception)\n    275             # If we've exceeded the max attempts we just let the exception\n    276             # propogate if one has occurred.\n--&gt; 277             return self._checker(attempt_number, response, caught_exception)\n    278 \n    279 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    315         for checker in self._checkers:\n    316             checker_response = checker(attempt_number, response,\n--&gt; 317                                        caught_exception)\n    318             if checker_response:\n    319                 return checker_response\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    221         elif caught_exception is not None:\n    222             return self._check_caught_exception(\n--&gt; 223                 attempt_number, caught_exception)\n    224         else:\n    225             raise ValueError(&quot;Both response and caught_exception are None.&quot;)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in _check_caught_exception(self, attempt_number, caught_exception)\n    357         # the MaxAttemptsDecorator is not interested in retrying the exception\n    358         # then this exception just propogates out past the retry code.\n--&gt; 359         raise caught_exception\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _do_get_response(self, request, operation_model)\n    198             http_response = first_non_none_response(responses)\n    199             if http_response is None:\n--&gt; 200                 http_response = self._send(request)\n    201         except HTTPClientError as e:\n    202             return (None, e)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _send(self, request)\n    267 \n    268     def _send(self, request):\n--&gt; 269         return self.http_session.send(request)\n    270 \n    271 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/httpsession.py in send(self, request)\n    349                 error=e,\n    350                 request=request,\n--&gt; 351                 endpoint_url=request.url\n    352             )\n    353         except Exception as e:\n\nConnectionClosedError: Connection was closed before we received a valid response from endpoint URL\n<\/code><\/pre>\n\n<p>Somebody know's why this happens?<\/p>",
        "Challenge_closed_time":1616165232248,
        "Challenge_comment_count":1,
        "Challenge_created_time":1615369010763,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1615369103568,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66561959",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":16.1,
        "Challenge_reading_time":216.74,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":142,
        "Challenge_solved_time":221.1726347222,
        "Challenge_title":"Sagemaker Endpoint BrokenPipeError at DeepAR Prediction",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":244,
        "Challenge_word_count":1241,
        "Platform":"Stack Overflow",
        "Poster_created_time":1607069622448,
        "Poster_location":null,
        "Poster_reputation_count":101.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>I believe that Tarun might on the right path. The BrokenPipeError that you got is thrown when the connection is abruptly closed. See <a href=\"https:\/\/docs.python.org\/3\/library\/exceptions.html#BrokenPipeError\" rel=\"nofollow noreferrer\">the python docs for BrokenPipeError<\/a>.\nThe SageMaker endpoint probably drops the connection as soon as you go over the limit of 5MB. I suggest you try a smaller dataset. Also the data you send might get enlarged because of how sagemaker.tensorflow.model.TensorFlowPredictor encodes the data according to <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/799#issuecomment-492698717\" rel=\"nofollow noreferrer\">this comment<\/a> on a similar issue.<\/p>\n<p>If that doesn't work I've also seen a couple of people having problems with their networks in general. Specifically firewall\/antivirus (<a href=\"https:\/\/github.com\/aws\/aws-cli\/issues\/3999#issuecomment-531151161\" rel=\"nofollow noreferrer\">for example this comment<\/a>) or network timeout.<\/p>\n<p>Hope this points you in the right direction.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.4,
        "Solution_reading_time":13.62,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":119.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1402536093248,
        "Answerer_location":null,
        "Answerer_reputation_count":817703.0,
        "Answerer_view_count":106500.0,
        "Challenge_adjusted_solved_time":0.2592397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have dataframe with columns <\/p>\n\n<pre><code>date    open    high    low     close   adjclose    volume\n<\/code><\/pre>\n\n<p>I want to add one more column named \"result\"(1 if close > open, 0 if close &lt; open)<\/p>\n\n<p>I do<\/p>\n\n<pre><code># Map 1-based optional input ports to variables\ndata &lt;- maml.mapInputPort(1) # class: data.frame\n\n\n\n# calculate pass\/fail\ndata$result &lt;- as.factor(sapply(data$close,function(res) \n    if (res - data$open &gt;= 0) '1' else '0'))\n\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"data\");\n<\/code><\/pre>\n\n<p>But I have only 1 in result. Where is the problem?<\/p>",
        "Challenge_closed_time":1554572339496,
        "Challenge_comment_count":1,
        "Challenge_created_time":1554571406233,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55551617",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.3,
        "Challenge_reading_time":7.87,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.2592397222,
        "Challenge_title":"Azure ML and r scripts",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":77,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553239567403,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>The <code>if\/else<\/code> can return only a single TRUE\/FALSE and is not vectorized for length > 1.  It may be suitable to use <code>ifelse<\/code> (but that is also not required and would be less efficient compared to direct coersion of logical vector to binary (<code>as.integer<\/code>).   In the OP's code, the 'close' column elements are looped  (<code>sapply<\/code>) and subtracted from the whole 'open' column.  The intention might be to do elementwise subtraction.  In that case, <code>-<\/code> between the columns is much cleaner and efficient (as these operations are vectorized)<\/p>\n\n<pre><code>data$result &lt;- with(data, factor(as.integer((close - open) &gt;= 0)))\n<\/code><\/pre>\n\n<p>In the above, we get the difference between the columns ('close', 'open'), check if it is greater than or equal to 0 (returns logical vector), convert it to binary (<code>as.integer<\/code> - TRUE -> 1, FALSE -> 0) and then change it to <code>factor<\/code> type (if needed)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":12.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":137.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1460436951967,
        "Answerer_location":null,
        "Answerer_reputation_count":156.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":0.2497888889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an experiment in azure machine learning studio, and I would like to the see entire scored dataset.<\/p>\n\n<p>Naturally I used the 'visualise' option on the scored dataset but these yields only 100 rows (the test dataset is around 500 rows)<\/p>\n\n<p>I also tired the 'save as dataset' option, but then file does not open well with excel or text editor (special character encoding)<\/p>\n\n<p>Basically, I want to see the entire test data with scored labels as table or download as .csv maybe<\/p>",
        "Challenge_closed_time":1460437058280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1460436159040,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36563769",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.1,
        "Challenge_reading_time":6.92,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.2497888889,
        "Challenge_title":"How to download the entire scored dataset from Azure machine studio?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":5296,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Poster_created_time":1406266059940,
        "Poster_location":"Link\u00f6ping, Sweden",
        "Poster_reputation_count":1677.0,
        "Poster_view_count":221.0,
        "Solution_body":"<p>Please try the Convert to CSV module: <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/faa6ba63-383c-4086-ba58-7abf26b85814\" rel=\"noreferrer\">https:\/\/msdn.microsoft.com\/library\/azure\/faa6ba63-383c-4086-ba58-7abf26b85814<\/a><\/p>\n\n<p>After you run the experiment, right click on the output of the module to download the CSV file.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.5,
        "Solution_reading_time":4.51,
        "Solution_score_count":14.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":28.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":148.3679175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have written a pipeline that I want to run on a remote compute cluster within Azure Machine Learning. My aim is to process a large amount of historical data, and to do this I will need to run the pipeline on a large number of input parameter combinations.<\/p>\n\n<p>Is there a way to restrict the number of nodes that the pipeline uses on the cluster? By default it will use all the nodes available to the cluster, and I would like to restrict it so that it only uses a pre-defined maximum. This allows me to leave the rest of the cluster free for other users.<\/p>\n\n<p>My current code to start the pipeline looks like this:<\/p>\n\n<pre><code># Setup the pipeline\nsteps = [data_import_step] # Contains PythonScriptStep\npipeline = Pipeline(workspace=ws, steps=steps)\npipeline.validate()\n\n# Big long list of historical dates that I want to process data for\ndts = pd.date_range('2019-01-01', '2020-01-01', freq='6H', closed='left')\n# Submit the pipeline job\nfor dt in dts:\n    pipeline_run = Experiment(ws, 'my-pipeline-run').submit(\n        pipeline,\n        pipeline_parameters={\n            'import_datetime': dt.strftime('%Y-%m-%dT%H:00'),\n        }\n    )\n<\/code><\/pre>",
        "Challenge_closed_time":1592597207007,
        "Challenge_comment_count":2,
        "Challenge_created_time":1592308824897,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62407943",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.9,
        "Challenge_reading_time":14.85,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":80.1061416667,
        "Challenge_title":"Restrict the number of nodes used by an Azure Machine Learning pipeine",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":274,
        "Challenge_word_count":174,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403698315160,
        "Poster_location":"London, UK",
        "Poster_reputation_count":1534.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>For me, the killer feature of Azure ML is not having to worry about load balancing like this. Our team has a compute target with <code>max_nodes=100<\/code> for every feature branch and we have <code>Hyperdrive<\/code> pipelines that result in 130 runs for each pipeline.<\/p>\n<p>We can submit multiple <code>PipelineRun<\/code>s back-to-back and the orchestrator does the heavy lifting of queuing, submitting, all the runs so that the <code>PipelineRun<\/code>s execute in the serial order I submitted them, and that the cluster is never overloaded. This works without issue for us 99% of the time.<\/p>\n<p>If what you're looking for is that you'd like the <code>PipelineRun<\/code>s to be executed in parallel, then you should check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-parallel-run-step#build-and-run-the-pipeline-containing-parallelrunstep\" rel=\"nofollow noreferrer\"><code>ParallelRunStep<\/code><\/a>.<\/p>\n<p>Another option is to isolate your computes. You can have up to 200 <code>ComputeTarget<\/code>s per workspace. Two 50-node <code>ComputeTarget<\/code>s cost the same as one 100-node <code>ComputeTarget<\/code>.<\/p>\n<p>On our team, we use <a href=\"https:\/\/www.pygit2.org\/\" rel=\"nofollow noreferrer\"><code>pygit2<\/code><\/a> to have a <code>ComputeTarget<\/code> created for each feature branch, so that, as data scientists, we can be confident that we're not stepping on our coworkers' toes.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1592842949400,
        "Solution_link_count":2.0,
        "Solution_readability":11.9,
        "Solution_reading_time":18.57,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":177.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1551701850312,
        "Answerer_location":"Poland, Cracow",
        "Answerer_reputation_count":11273.0,
        "Answerer_view_count":1888.0,
        "Challenge_adjusted_solved_time":340.1792388889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create a python utility that will take dataset from vertex ai datasets and will generate statistics for that dataset. But I am unable to check the dataset using jupyter notebook. Is there any way out for this?<\/p>",
        "Challenge_closed_time":1631634873007,
        "Challenge_comment_count":3,
        "Challenge_created_time":1630410227747,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68998065",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":4.4,
        "Challenge_reading_time":3.36,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":340.1792388889,
        "Challenge_title":"Read vertex ai datasets in jupyter notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":737,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Poster_created_time":1616070829583,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>If I understand correctly, you want to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\" rel=\"nofollow noreferrer\">Vertex AI<\/a> dataset inside <code>Jupyter Notebook<\/code>. I don't think that this is currently possible. You are able to export <code>Vertex AI<\/code> datasets to <code>Google Cloud Storage<\/code> in JSONL format:<\/p>\n<blockquote>\n<p>Your dataset will be exported as a list of text items in JSONL format. Each row contains a Cloud Storage path, any label(s) assigned to that item, and a flag that indicates whether that item is in the training, validation, or test set.<\/p>\n<\/blockquote>\n<p>At this moment, you can use <code>BigQuery<\/code> data inside <code>Notebook<\/code> using <code>%%bigquery<\/code> like it's mentioned in <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter\" rel=\"nofollow noreferrer\">Visualizing BigQuery data in a Jupyter notebook.<\/a> or use <code>csv_read()<\/code> from machine directory or <code>GCS<\/code> like it's showed in the <a href=\"https:\/\/stackoverflow.com\/questions\/61956470\/\">How to read csv file in Google Cloud Platform jupyter notebook<\/a> thread.<\/p>\n<p>However, you can fill a <code>Feature Request<\/code> in <a href=\"https:\/\/developers.google.com\/issue-tracker\" rel=\"nofollow noreferrer\">Google Issue Tracker<\/a> to add the possibility to use <code>VertexAI<\/code> dataset directly in the <code>Jupyter Notebook<\/code> which will be considered by the <code>Google Vertex AI Team<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.8,
        "Solution_reading_time":19.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":174.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1418476563283,
        "Answerer_location":null,
        "Answerer_reputation_count":676.0,
        "Answerer_view_count":113.0,
        "Challenge_adjusted_solved_time":51185.3566025,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was hoping that someone would have tried to or had success in implementing it and would have knowledge of any pitfalls in using it.<\/p>",
        "Challenge_closed_time":1474621313252,
        "Challenge_comment_count":0,
        "Challenge_created_time":1458446579210,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1459352400923,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36110109",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":3.01,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4492.9816783333,
        "Challenge_title":"Has anyone any experience on implementing the R Package XGBoost within the Azure ML Studio environment?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":436,
        "Challenge_word_count":40,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428820274636,
        "Poster_location":"Lexington, KY, United States",
        "Poster_reputation_count":65.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>You need to zip &amp; load the package windows binaries in dataset &amp; import it to the R environment.<\/p>\n<p>You can follow the instructions over <a href=\"https:\/\/gallery.azure.ai\/Experiment\/Using-XGBoost-to-build-Graduation-Admit-Model-1\" rel=\"nofollow noreferrer\">here<\/a>. I couldn't import it for the latest version, so I simply downloaded the xgboost version from this experiment &amp; loaded it to my saved datasets<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/archive\/blogs\/benjguin\/how-to-upload-an-r-package-to-azure-machine-learning\" rel=\"nofollow noreferrer\">This<\/a> is for any generic packages which are not preloaded in the environment<\/p>\n<p>The following is a <a href=\"https:\/\/web.archive.org\/web\/20161020215633\/https:\/\/azure.microsoft.com\/en-in\/documentation\/articles\/machine-learning-r-csharp-web-service-examples\/\" rel=\"nofollow noreferrer\">list of experiments<\/a> to publish R models as a web service<\/p>\n<p>Hope this helps!<\/p>\n<p>Edit: You can also simply change the R version to Microsoft Open R (current version 3.2.2) and you can import xgboost as any common library<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1643619684692,
        "Solution_link_count":3.0,
        "Solution_readability":14.8,
        "Solution_reading_time":14.54,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":116.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1620426047396,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":386.5795511111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using the library weights and  biases. My model outputs a curve (a time series). I'd like to see how this curve changes throughout training. So, I'd need some kind of slider where I can select epoch and it shows me the curve for that epoch. It could be something very similar to what it's done with histograms (it shows an image of the histograms across epochs and when you hover it display the histogram corresponding to that epoch). Is there a way to do this or something similar using <code>wandb<\/code>?<\/p>\n<p>Currently my code looks like this:<\/p>\n<pre><code>for epoch in range(epochs):\n   output = model(input)\n   #output is shape (37,40) (lenght 40 and I have 37 samples)\n   #it's enough to plot the first sample\n   xs = torch.arange(40).unsqueeze(dim=1)\n   ys = output[0,:].unsqueeze(dim=1)\n   wandb.log({&quot;line&quot;: wandb.plot.line_series(xs=xs, ys=ys,title=&quot;Out&quot;)}, step=epoch)\n<\/code><\/pre>\n<p>I'd appreciate any help! Thanks!<\/p>",
        "Challenge_closed_time":1620426900467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619035214083,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67202711",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":12.43,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":386.5795511111,
        "Challenge_title":"How to get multiple lines exported to wandb",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":840,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1577734207070,
        "Poster_location":null,
        "Poster_reputation_count":422.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>You can use <code>wandb.log()<\/code> with matplotlib. Create your plot using matplotlib:<\/p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(0, 50)\nfor i in range(1, 4):\n    fig, ax = plt.subplots()\n    y = x ** i\n    ax.plot(x, y)\n    wandb.log({'chart': ax})\n<\/code><\/pre>\n<p>Then when you look on your wandb dashboard for the run, you will see the plot rendered as plotly plot. Click the gear in the upper left hand corner to see a slider that lets you slide over training steps and see the plot at each step.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.4,
        "Solution_reading_time":6.54,
        "Solution_score_count":3.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":85.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1443426419048,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Challenge_adjusted_solved_time":133.5554527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Azure ML support says to me that delimiter must be comma, this would cause too much hassle with data having semicolon as separator and with a lot of commas in the cell values. <\/p>\n\n<p>So how to process semicolon separated CSV files in Azure ML? <\/p>",
        "Challenge_closed_time":1485838771543,
        "Challenge_comment_count":1,
        "Challenge_created_time":1485357971913,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41855344",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":3.94,
        "Challenge_score_count":5,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":133.5554527778,
        "Challenge_title":"Azure ML: How to save and process CSV files with semicolon as delimiter?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2908,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Poster_created_time":1251372839052,
        "Poster_location":null,
        "Poster_reputation_count":48616.0,
        "Poster_view_count":3348.0,
        "Solution_body":"<p>Azure ML only accepts the comma <code>,<\/code> separated CSV. Do a little work around.\nOpen your data file using a text editor. (Notepad will do the trick). Find and replace all semicolons with 'tab' (Make it a TSV) and the commas in data values may not occur a problem then. Make sure to define that the input is a TSV; not a CSV. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.9,
        "Solution_reading_time":4.05,
        "Solution_score_count":7.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1625765161928,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":1392.0377008334,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>My scoring function needs to refer to an Azure ML Registered Dataset for which I need a reference to the AzureML Workspace object. When including this in the <code>init()<\/code> function of the scoring script it  gives the following error:<\/p>\n<pre><code> &quot;code&quot;: &quot;ScoreInitRestart&quot;,\n      &quot;message&quot;: &quot;Your scoring file's init() function restarts frequently. You can address the error by increasing the value of memory_gb in deployment_config.&quot;\n<\/code><\/pre>\n<p>On debugging the issue is:<\/p>\n<pre><code>To sign in, use a web browser to open the page https:\/\/microsoft.com\/devicelogin and enter the code [REDACTED] to authenticate.\n<\/code><\/pre>\n<p>How can I resolve this issue without exposing Service Principal Credentials in the scoring script?<\/p>",
        "Challenge_closed_time":1626961792156,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621951561220,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67689671",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":9.6,
        "Challenge_reading_time":10.89,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1391.7308155556,
        "Challenge_title":"How to get reference to AzureML Workspace Class in scoring script?",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1066,
        "Challenge_word_count":114,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>I found a workaround to reference the workspace in the scoring script. Below is a code snippet of how one can do that -<\/p>\n<p>My deploy script looks like this :<\/p>\n<pre><code>from azureml.core import Environment\nfrom azureml.core.model import InferenceConfig\n\n#Add python dependencies for the models\nscoringenv = Environment.from_conda_specification(\n                                   name = &quot;scoringenv&quot;,\n                                   file_path=&quot;config_files\/scoring_env.yml&quot;\n                                    )\n#Create a dictionary to set-up the env variables   \nenv_variables={'tenant_id':tenant_id,\n                        'subscription_id':subscription_id,\n                        'resource_group':resource_group,\n                        'client_id':client_id,\n                        'client_secret':client_secret\n                        }\n    \nscoringenv.environment_variables=env_variables\n            \n# Configure the scoring environment\ninference_config = InferenceConfig(\n                                   entry_script='score.py',\n                                   source_directory='scripts\/',\n                                   environment=scoringenv\n                                        )\n<\/code><\/pre>\n<p>What I am doing here is creating an image with the python dependencies(in the scoring_env.yml) and passing a dictionary of the secrets as environment variables. I have the secrets stored in the key-vault.\nYou may define and pass native python datatype variables.<\/p>\n<p>Now, In my score.py, I reference these environment variables in the init() like this -<\/p>\n<pre><code>tenant_id = os.environ.get('tenant_id')\nclient_id = os.environ.get('client_id')\nclient_secret = os.environ.get('client_secret')\nsubscription_id = os.environ.get('subscription_id')\nresource_group = os.environ.get('resource_group')\n<\/code><\/pre>\n<p>Once you have these variables, you may create a workspace object using Service Principal authentication like @Anders Swanson mentioned in his reply.<\/p>\n<p>Another way to resolve this may be by using managed identities for AKS. I did not explore that option.<\/p>\n<p>Hope this helps! Please let me know if you found a better way of solving this.<\/p>\n<p>Thanks!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1626962896943,
        "Solution_link_count":0.0,
        "Solution_readability":11.6,
        "Solution_reading_time":24.02,
        "Solution_score_count":2.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":200.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":17.5084694445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to build and push a custom ML model with docker to Amazon SageMaker. I know things are supposed to follow the general structure of being in opt\/ml. But there's no such bucket in Amazon S3??? Am I supposed to create this directory within my container before I build and push the image to AWS? I just have no idea where to put my training data, etc.<\/p>",
        "Challenge_closed_time":1565167576867,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565104546377,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57379173",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.9,
        "Challenge_reading_time":5.11,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":17.5084694445,
        "Challenge_title":"Where do I store my model's training data, artifacts, etc?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1022,
        "Challenge_word_count":77,
        "Platform":"Stack Overflow",
        "Poster_created_time":1399251405187,
        "Poster_location":null,
        "Poster_reputation_count":145.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>SageMaker is automating the deployment of the Docker image with your code using the convention of channel->local-folder. Everything that you define with a channel in your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"nofollow noreferrer\">input data configuration<\/a>, will be copied to the local Docker file system under <em>\/opt\/ml\/<\/em> folder, using the name of the channel as the name of the sub-folder.<\/p>\n\n<pre><code>{\n\"train\" : {\"ContentType\":  \"trainingContentType\", \n           \"TrainingInputMode\": \"File\", \n           \"S3DistributionType\": \"FullyReplicated\", \n           \"RecordWrapperType\": \"None\"},\n\"evaluation\" : {\"ContentType\":  \"evalContentType\", \n                \"TrainingInputMode\": \"File\", \n                \"S3DistributionType\": \"FullyReplicated\", \n                \"RecordWrapperType\": \"None\"},\n\"validation\" : {\"TrainingInputMode\": \"File\", \n                \"S3DistributionType\": \"FullyReplicated\", \n                \"RecordWrapperType\": \"None\"}\n} \n<\/code><\/pre>\n\n<p>to:<\/p>\n\n<pre><code>\/opt\/ml\/input\/data\/training\n\/opt\/ml\/input\/data\/validation\n\/opt\/ml\/input\/data\/testing\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":26.0,
        "Solution_reading_time":15.0,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1318047784840,
        "Answerer_location":null,
        "Answerer_reputation_count":6665.0,
        "Answerer_view_count":926.0,
        "Challenge_adjusted_solved_time":0.5016727778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to install the following library in my Azure ML instance:<\/p>\n<p><a href=\"https:\/\/github.com\/philferriere\/cocoapi#egg=pycocotools&amp;subdirectory=PythonAPI\" rel=\"nofollow noreferrer\">https:\/\/github.com\/philferriere\/cocoapi#egg=pycocotools&amp;subdirectory=PythonAPI<\/a><\/p>\n<p>My Dockerfile looks like this:<\/p>\n<pre><code>FROM mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04:20210615.v1\n\nENV AZUREML_CONDA_ENVIRONMENT_PATH \/azureml-envs\/pytorch-1.7\n\n# Create conda environment\nRUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\n    python=3.7 \\\n    pip=20.2.4 \\\n    pytorch=1.7.1 \\\n    torchvision=0.8.2 \\\n    torchaudio=0.7.2 \\\n    cudatoolkit=11.0 \\\n    nvidia-apex=0.1.0 \\\n    -c anaconda -c pytorch -c conda-forge\n\n# Prepend path to AzureML conda environment\nENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/bin:$PATH\n\n# Install pip dependencies\nRUN HOROVOD_WITH_PYTORCH=1 \\\n    pip install 'matplotlib&gt;=3.3,&lt;3.4' \\\n                'psutil&gt;=5.8,&lt;5.9' \\\n                'tqdm&gt;=4.59,&lt;4.60' \\\n                'pandas&gt;=1.1,&lt;1.2' \\\n                'scipy&gt;=1.5,&lt;1.6' \\\n                'numpy&gt;=1.10,&lt;1.20' \\\n                'azureml-core==1.31.0' \\\n                'azureml-defaults==1.31.0' \\\n                'azureml-mlflow==1.31.0' \\\n                'azureml-telemetry==1.31.0' \\\n                'tensorboard==2.4.0' \\\n                'tensorflow-gpu==2.4.1' \\\n                'onnxruntime-gpu&gt;=1.7,&lt;1.8' \\\n                'horovod[pytorch]==0.21.3' \\\n                'future==0.17.1' \\\n                'git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI'\n\n# This is needed for mpi to locate libpython\nENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/lib:$LD_LIBRARY_PATH\n<\/code><\/pre>\n<p>An error is thrown when the library is being installed:<\/p>\n<pre><code>  Cloning https:\/\/github.com\/philferriere\/cocoapi.git to \/tmp\/pip-install-_i3sjryy\/pycocotools\n[91m    ERROR: Command errored out with exit status 1:\n     command: \/azureml-envs\/pytorch-1.7\/bin\/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base \/tmp\/pip-pip-egg-info-o68by1_q\n         cwd: \/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\n    Complete output (5 lines):\n    Traceback (most recent call last):\n      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n      File &quot;\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py&quot;, line 2, in &lt;module&gt;\n        from Cython.Build import cythonize\n    ModuleNotFoundError: No module named 'Cython'\n<\/code><\/pre>\n<p>I've tried adding Cython as a dependecy in both the pip section and as part of the conda environment but the error is still thrown.<\/p>",
        "Challenge_closed_time":1625324240192,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625322434170,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68237132",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":20.7,
        "Challenge_reading_time":39.03,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.5016727778,
        "Challenge_title":"No module named 'Cython' setting up Azure ML docker instance",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":209,
        "Challenge_word_count":200,
        "Platform":"Stack Overflow",
        "Poster_created_time":1318047784840,
        "Poster_location":null,
        "Poster_reputation_count":6665.0,
        "Poster_view_count":926.0,
        "Solution_body":"<p>Solution was to add the following to the Dockerfile:<\/p>\n<pre><code># Install Cython\nRUN pip3 install Cython\n\n# Install pip dependencies\nRUN HOROVOD_WITH_PYTORCH=1 \\\n    pip install 'matplotlib&gt;=3.3,&lt;3.4' \\\n                ...\n                'git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI'\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":22.9,
        "Solution_reading_time":4.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1574932278796,
        "Answerer_location":null,
        "Answerer_reputation_count":18454.0,
        "Answerer_view_count":3762.0,
        "Challenge_adjusted_solved_time":12.6983352778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run a Python script in a DevOps pipeline upon check in. A basic 'hellow world' script works, but when I import azureml.core, it errors out with ModuleNotFoundError: No module named 'azureml'.<\/p>\n<p>That makes sense, since I don't know how it was going to find azureml.core. My question is: how do I get the Python script to find the module? Do I need to check it in as part of my code base in DevOps? Or is there some way to reference it via a hyperlink?<\/p>\n<p>Here is my YML file:<\/p>\n<pre><code>trigger:\n- master\n\npool:\n  vmImage: ubuntu-latest\n\nsteps:\n- task: PythonScript@0\n  inputs:\n    scriptSource: 'filepath'\n    scriptPath: test.py\n<\/code><\/pre>\n<p>And here is my python script:<\/p>\n<pre><code>print('hello world')\n\nimport azureml.core\nfrom azureml.core import Workspace\n\n# Load the workspace from the saved config file\nws = Workspace.from_config()\nprint('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))\n<\/code><\/pre>",
        "Challenge_closed_time":1623837729747,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623792015740,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67993641",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.4,
        "Challenge_reading_time":12.85,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":12.6983352778,
        "Challenge_title":"DevOps pipeline running python script error on import azureml.core",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":410,
        "Challenge_word_count":150,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320273319470,
        "Poster_location":"Calgary, Canada",
        "Poster_reputation_count":78.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You have to install the lost package into your python, the default python does not have that. Please use the below yml:<\/p>\n<pre><code>trigger:\n- master\n\npool:\n  vmImage: 'ubuntu-latest'\n\nvariables:\n  solution: '**\/*.sln'\n  buildPlatform: 'Any CPU'\n  buildConfiguration: 'Release'\n\nsteps:\n- task: UsePythonVersion@0\n  displayName: 'Use Python 3.8'\n  inputs:\n    versionSpec: 3.8\n\n- script: python3 -m pip install --upgrade pip\n  displayName: 'upgrade pip'\n\n- script: python3 -m pip install azureml.core\n  displayName: 'Install azureml.core'\n\n\n\n- task: PythonScript@0\n  inputs: \n    scriptSource: 'filepath'\n    scriptPath: test.py\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":7.82,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":81.9709777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking at <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">this<\/a>, which makes all sense. Let us focus on this bit of code:<\/p>\n<pre><code>from sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,\n    inputs=[\n        ProcessingInput(source=&quot;s3:\/\/your-bucket\/path\/to\/your\/data&quot;, destination=&quot;\/opt\/ml\/processing\/input&quot;),\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n\npreprocessing_job_description = sklearn_processor.jobs[-1].describe() \n<\/code><\/pre>\n<p>Here preprocessing.py has to be obviously in the cloud. I am curious, could one also put scripts into an S3 bucket and trigger the job remotely. I can easily to this with hyper parameter optimisation, which does not require dedicated scripts though as I use an OOTB training image.<\/p>\n<p>In this case I can fire off the job like so:<\/p>\n<pre><code>tuning_job_name = &quot;amazing-hpo-job-&quot; + strftime(&quot;%d-%H-%M-%S&quot;, gmtime())\n\nsmclient = boto3.Session().client(&quot;sagemaker&quot;)\nsmclient.create_hyper_parameter_tuning_job(\n    HyperParameterTuningJobName=tuning_job_name,\n    HyperParameterTuningJobConfig=tuning_job_config,\n    TrainingJobDefinition=training_job_definition\n)\n<\/code><\/pre>\n<p>and then monitor the job's progress:<\/p>\n<pre><code>smclient = boto3.Session().client(&quot;sagemaker&quot;)\n\ntuning_job_result = smclient.describe_hyper_parameter_tuning_job(\n    HyperParameterTuningJobName=tuning_job_name\n)\n\nstatus = tuning_job_result[&quot;HyperParameterTuningJobStatus&quot;]\nif status != &quot;Completed&quot;:\n    print(&quot;Reminder: the tuning job has not been completed.&quot;)\n\njob_count = tuning_job_result[&quot;TrainingJobStatusCounters&quot;][&quot;Completed&quot;]\nprint(&quot;%d training jobs have completed&quot; % job_count)\n\nobjective = tuning_job_result[&quot;HyperParameterTuningJobConfig&quot;][&quot;HyperParameterTuningJobObjective&quot;]\nis_minimize = objective[&quot;Type&quot;] != &quot;Maximize&quot;\nobjective_name = objective[&quot;MetricName&quot;]\n\nif tuning_job_result.get(&quot;BestTrainingJob&quot;, None):\n    print(&quot;Best model found so far:&quot;)\n    pprint(tuning_job_result[&quot;BestTrainingJob&quot;])\nelse:\n    print(&quot;No training jobs have reported results yet.&quot;) \n<\/code><\/pre>\n<p>I would think starting and monitoring a SageMaker processing job from a local machine should be possible as with an HPO job but what about the script(s)? Ideally I would like to develop and test them locally and the run remotely. Hope this makes sense?<\/p>",
        "Challenge_closed_time":1662504468407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662209372887,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73592371",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":21.4,
        "Challenge_reading_time":40.76,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":81.9709777778,
        "Challenge_title":"start, monitor and define script of SageMaker processing job from local machine",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":38,
        "Challenge_word_count":224,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>Im not sure I understand the comparison to a Tuning Job.<\/p>\n<p>Based on what you have described, in this case the <code>preprocessing.py<\/code> is actually stored locally. The SageMaker SDK will upload it to S3 for the remote Processing Job to access it. I suggest launching the Job and then taking a look at the inputs in the SageMaker Console.<\/p>\n<p>If you wanted to test the Processing Job locally you can do so using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a>. This will basically imitate the Job locally which aids in debugging the script before kicking off a remote Processing Job. Kindly note docker is required to make use of Local Mode.<\/p>\n<p>Example code for local mode:<\/p>\n<pre><code>from sagemaker.local import LocalSession\nfrom sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n\nsagemaker_session = LocalSession()\nsagemaker_session.config = {'local': {'local_code': True}}\n\n# For local training a dummy role will be sufficient\nrole = 'arn:aws:iam::111111111111:role\/service-role\/AmazonSageMaker-ExecutionRole-20200101T000001'\n\nprocessor = ScriptProcessor(command=['python3'],\n                    image_uri='sagemaker-scikit-learn-processing-local',\n                    role=role,\n                    instance_count=1,\n                    instance_type='local')\n\nprocessor.run(code='processing_script.py',\n                    inputs=[ProcessingInput(\n                        source='.\/input_data\/',\n                        destination='\/opt\/ml\/processing\/input_data\/')],\n                    outputs=[ProcessingOutput(\n                        output_name='word_count_data',\n                        source='\/opt\/ml\/processing\/processed_data\/')],\n                    arguments=['job-type', 'word-count']\n                    )\n\npreprocessing_job_description = processor.jobs[-1].describe()\noutput_config = preprocessing_job_description['ProcessingOutputConfig']\n\nprint(output_config)\n\nfor output in output_config['Outputs']:\n    if output['OutputName'] == 'word_count_data':\n        word_count_data_file = output['S3Output']['S3Uri']\n\nprint('Output file is located on: {}'.format(word_count_data_file))\n\n\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.2,
        "Solution_reading_time":26.06,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":177.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":16.1496972223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have followed the AWS tutorial(<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_image_json_format.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_image_json_format.ipynb<\/a>) and trained my first model using SageMaker.<\/p>\n\n<p>The end result is an archive containing the following files:\n- hyperparams.json\n- model_algo_1-0000.params\n- model_algo_1-symbol.json<\/p>\n\n<p>I am not familiar with this format, and was not able to load it into Keras via keras.models.model_from_json()<\/p>\n\n<p>I am assuming this is a different format or an AWS proprietary one.<\/p>\n\n<p>Can you please help me identify the format?\nIs it possible to load this into a Keras model and do inference without an EC2 instance(locally)?<\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1580119883430,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580061744520,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59921196",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":18.0,
        "Challenge_reading_time":13.51,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":16.1496972223,
        "Challenge_title":"Can you load a SageMaker trained model into Keras?",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":212,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Poster_created_time":1533465584552,
        "Poster_location":"Bucharest, Romania",
        "Poster_reputation_count":341.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>Built-in algorithms are implemented with Apache MXNet, so that's how you'd load the model locally. load_checkpoint() is the appropriate API: <a href=\"https:\/\/mxnet.apache.org\/api\/python\/docs\/api\/mxnet\/model\/index.html#mxnet.model.load_checkpoint\" rel=\"nofollow noreferrer\">https:\/\/mxnet.apache.org\/api\/python\/docs\/api\/mxnet\/model\/index.html#mxnet.model.load_checkpoint<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":22.9,
        "Solution_reading_time":5.23,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1619174589310,
        "Answerer_location":"Bangalore, Karnataka, India",
        "Answerer_reputation_count":1200.0,
        "Answerer_view_count":804.0,
        "Challenge_adjusted_solved_time":254.5180547222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have a Vertex AI model deployed on an endpoint and want to do some prediction from my app in Golang.<\/p>\n<p>To do this I create code inspired by this example : <a href=\"https:\/\/cloud.google.com\/go\/docs\/reference\/cloud.google.com\/go\/aiplatform\/latest\/apiv1?hl=en\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/go\/docs\/reference\/cloud.google.com\/go\/aiplatform\/latest\/apiv1?hl=en<\/a><\/p>\n<pre><code>const file = &quot;MY_BASE64_IMAGE&quot;\n\nfunc main() {\n\n    ctx := context.Background()\n\n    c, err := aiplatform.NewPredictionClient(cox)\n    if err != nil {\n        log.Printf(&quot;QueryVertex NewPredictionClient - Err:%s&quot;, err)\n    }\n    defer c.Close()\n\n    parameters, err := structpb.NewValue(map[string]interface{}{\n        &quot;confidenceThreshold&quot;: 0.2,\n        &quot;maxPredictions&quot;:      5,\n    })\n    if err != nil {\n        log.Printf(&quot;QueryVertex structpb.NewValue parameters - Err:%s&quot;, err)\n    }\n\n    instance, err := structpb.NewValue(map[string]interface{}{\n        &quot;content&quot;: file,\n    })\n    if err != nil {\n        log.Printf(&quot;QueryVertex structpb.NewValue instance - Err:%s&quot;, err)\n    }\n\n    reqP := &amp;aiplatformpb.PredictRequest{\n        Endpoint:   &quot;projects\/PROJECT_ID\/locations\/LOCATION_ID\/endpoints\/ENDPOINT_ID&quot;,\n        Instances:  []*structpb.Value{instance},\n        Parameters: parameters,\n    }\n\n    resp, err := c.Predict(cox, reqP)\n    if err != nil {\n        log.Printf(&quot;QueryVertex Predict - Err:%s&quot;, err)\n    }\n\n    log.Printf(&quot;QueryVertex Res:%+v&quot;, resp)\n}\n<\/code><\/pre>\n<p>I put the path to my service account JSON file on GOOGLE_APPLICATION_CREDENTIALS environment variable.\nBut when I run my test app I obtain this error message:<\/p>\n<pre><code>QueryVertex Predict - Err:rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found); transport: received unexpected content-type &quot;text\/html; charset=UTF-8&quot;\nQueryVertex Res:&lt;nil&gt;\n<\/code><\/pre>",
        "Challenge_closed_time":1652647881720,
        "Challenge_comment_count":3,
        "Challenge_created_time":1651731616723,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1654669807780,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72122744",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":15.0,
        "Challenge_reading_time":26.24,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":254.5180547222,
        "Challenge_title":"Google Cloud Vertex AI with Golang: rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found)",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":455,
        "Challenge_word_count":177,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651730962056,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>As @DazWilkin suggested, configure the client option to specify the specific <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest#service-endpoint\" rel=\"noreferrer\">regional endpoint<\/a> with a port 443:<\/p>\n<pre><code>option.WithEndpoint(&quot;&lt;region&gt;-aiplatform.googleapis.com:443&quot;)\n<\/code><\/pre>\n<p>Try like below:<\/p>\n<pre><code>func main() {\n \n   ctx := context.Background()\n   c, err := aiplatform.NewPredictionClient(\n       ctx,\n       option.WithEndpoint(&quot;&lt;region&gt;-aiplatform.googleapis.com:443&quot;),\n   )\n   if err != nil {\n       log.Printf(&quot;QueryVertex NewPredictionClient - Err:%s&quot;, err)\n   }\n   defer c.Close()\n       .\n       .\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1652712010567,
        "Solution_link_count":1.0,
        "Solution_readability":18.6,
        "Solution_reading_time":8.78,
        "Solution_score_count":5.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":43.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1412669622830,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":7.0974458333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using the SageMaker HuggingFace Processor to create a custom tokenizer on a large volume of text data.\nIs there a way to make this job data distributed - meaning read partitions of data across nodes and train the tokenizer leveraging multiple CPUs\/GPUs.<\/p>\n<p>At the moment, providing more nodes to the processing cluster merely replicates the tokenization process (basically duplicates the process of creation), which is redundant. You can primarily only scale vertically.<\/p>\n<p>Any insights into this?<\/p>",
        "Challenge_closed_time":1662653309532,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662621424647,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1662627758727,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73645084",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":7.58,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":8.8569125,
        "Challenge_title":"Create Hugging Face Transformers Tokenizer using Amazon SageMaker in a distributed way",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":27,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_created_time":1662621266503,
        "Poster_location":null,
        "Poster_reputation_count":48.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Considering the following example code for\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-job-frameworks-hugging-face.html\" rel=\"nofollow noreferrer\">HuggingFaceProcessor<\/a>:<\/p>\n<p>If you have 100 large files in S3 and use a ProcessingInput with\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ProcessingS3Input.html#:%7E:text=S3DataDistributionType\" rel=\"nofollow noreferrer\">s3_data_distribution_type<\/a>=&quot;ShardedByS3Key&quot; (instead of FullyReplicated), the objects in your S3 prefix will be sharded and distributed to your instances.<\/p>\n<p>For example, if you have 100 large files and want to filter records from them using HuggingFace on 5 instances, the s3_data_distribution_type=&quot;ShardedByS3Key&quot; will put 20 objects on each instance, and each instance can read the files from its own path, filter out records, and write (uniquely named) files to the output paths, and SageMaker Processing will put the filtered files in S3.<\/p>\n<p>However, if your filtering criteria is stateful or depends on doing a full pass over the dataset first (such as: filtering outliers based on mean and standard deviation on a feature - in case of using SKLean Processor for example): you'll need to pass that information in to the job so each instance can know how to filter. To send information to the instances launched, you have to use the\u00a0<code>\/opt\/ml\/config\/resourceconfig.json<\/code>\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-your-own-processing-container.html#byoc-config\" rel=\"nofollow noreferrer\">file<\/a>:<\/p>\n<p><code>{ &quot;current_host&quot;: &quot;algo-1&quot;, &quot;hosts&quot;: [&quot;algo-1&quot;,&quot;algo-2&quot;,&quot;algo-3&quot;] }<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":17.1,
        "Solution_reading_time":22.86,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":186.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1544524371740,
        "Answerer_location":"Cologne Germany",
        "Answerer_reputation_count":21.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":3.6875811111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running into a ModuleNotFoundError for pandas while using the following code to orchestrate my Azure Machine Learning Pipeline:<\/p>\n<pre><code># Loading run config\nprint(&quot;Loading run config&quot;)\ntask_1_run_config = RunConfiguration.load(\n    os.path.join(WORKING_DIR + '\/pipeline\/task_runconfigs\/T01_Test_Task.yml')\n    ) \n\ntask_1_script_run_config = ScriptRunConfig(\n    source_directory=os.path.join(WORKING_DIR + '\/pipeline\/task_scripts'),\n    run_config=task_1_run_config    \n)\n\ntask_1_py_script_step = PythonScriptStep(\n    name='Task_1_Step',\n    script_name=task_1_script_run_config.script,\n    source_directory=task_1_script_run_config.source_directory,\n    compute_target=compute_target\n)\n\npipeline_run_config = Pipeline(workspace=workspace, steps=[task_1_py_script_step])#, task_2])\n\npipeline_run = Experiment(workspace, 'Test_Run_New_Pipeline').submit(pipeline_run_config)\npipeline_run.wait_for_completion()\n<\/code><\/pre>\n<p>The environment.yml<\/p>\n<pre><code>name: phinmo_pipeline_env\ndependencies:\n- python=3.8\n- pip:\n  - pandas\n  - azureml-core==1.43.0\n  - azureml-sdk\n  - scipy\n  - scikit-learn\n  - numpy\n  - pyyaml==6.0\n  - datetime\n  - azure\nchannels:\n  - conda-forge\n<\/code><\/pre>\n<p>The loaded RunConfiguration in T01_Test_Task.yml looks like this:<\/p>\n<pre><code># The script to run.\nscript: T01_Test_Task.py\n# The arguments to the script file.\narguments: [\n  &quot;--test&quot;, False,\n  &quot;--date&quot;, &quot;2022-07-26&quot;\n]\n# The name of the compute target to use for this run.\ncompute_target: phinmo-compute-cluster\n# Framework to execute inside. Allowed values are &quot;Python&quot;, &quot;PySpark&quot;, &quot;CNTK&quot;, &quot;TensorFlow&quot;, and &quot;PyTorch&quot;.\nframework: Python\n# Maximum allowed duration for the run.\nmaxRunDurationSeconds: 6000\n# Number of nodes to use for running job.\nnodeCount: 1\n\n#Environment details.\nenvironment:\n  # Environment name\n  name: phinmo_pipeline_env\n  # Environment version\n  version:\n  # Environment variables set for the run.\n  #environmentVariables:\n  #  EXAMPLE_ENV_VAR: EXAMPLE_VALUE\n  # Python details\n  python:\n    # user_managed_dependencies=True indicates that the environmentwill be user managed. False indicates that AzureML willmanage the user environment.\n    userManagedDependencies: false\n    # The python interpreter path\n    interpreterPath: python\n    # Path to the conda dependencies file to use for this run. If a project\n    # contains multiple programs with different sets of dependencies, it may be\n    # convenient to manage those environments with separate files.\n    condaDependenciesFile: environment.yml\n    # The base conda environment used for incremental environment creation.\n    baseCondaEnvironment: AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\n  # Docker details\n  \n# History details.\nhistory:\n  # Enable history tracking -- this allows status, logs, metrics, and outputs\n  # to be collected for a run.\n  outputCollection: true\n  # Whether to take snapshots for history.\n  snapshotProject: true\n  # Directories to sync with FileWatcher.\n  directoriesToWatch:\n  - logs\n# data reference configuration details\ndataReferences: {}\n# The configuration details for data.\ndata: {}\n# Project share datastore reference.\nsourceDirectoryDataStore:\n<\/code><\/pre>\n<p>I already tried a few things like overwriting the environment attribute in the RunConfiguration object with a environment.python.conda_dependencies object or assigning a version number to pandas in the environment.yml, changing the location of the environment.yml. But I am at a loss at what else to try. the T01_Test_Task.py runs without issues on its own. But putting it into a pipeline just does not seem to work.<\/p>",
        "Challenge_closed_time":1658937635368,
        "Challenge_comment_count":1,
        "Challenge_created_time":1658922798813,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1658924360076,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73137433",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":47.64,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":4.1212652778,
        "Challenge_title":"ModuleNotFoundError while using AzureML pipeline with yml file based RunConfiguration and environment.yml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":51,
        "Challenge_word_count":366,
        "Platform":"Stack Overflow",
        "Poster_created_time":1544524371740,
        "Poster_location":"Cologne Germany",
        "Poster_reputation_count":21.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Okay I found the issue.\nI am unnecessarily using the ScriptRunConfig which overwrites the assigned environment with some default azureml environment. I was able to see that only in the Task description in the Azure Machine Learning Studio UI.<\/p>\n<p>I was able to just remove that part and now it works:<\/p>\n<pre><code>task_1_run_config = RunConfiguration.load(\n    os.path.join(WORKING_DIR + '\/pipeline\/task_runconfigs\/T01_Test_Task.yml')\n    ) \ntask_1_py_script_step = PythonScriptStep(\n    name='Task_1_Step',\n    script_name='T01_Test_Task.py',\n    source_directory=os.path.join(WORKING_DIR + '\/pipeline\/task_scripts'),\n    runconfig=task_1_run_config, \n    compute_target=compute_target\n)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.2,
        "Solution_reading_time":8.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1384530039387,
        "Answerer_location":"Ljubljana, Slovenia",
        "Answerer_reputation_count":2470.0,
        "Answerer_view_count":285.0,
        "Challenge_adjusted_solved_time":2971.3031783333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an interesting use case and a problem.<\/p>\n<p>We are leveraging <strong>Sagemaker Notebooks<\/strong> as a development environment for our data science teams. These notebooks are essentially EC2 instances with a (relatively) nice IDE (not as good as Cloud9, though).<\/p>\n<p>In addition, we are running some docker containers on these instances. However, we are forced to use <code>--network=host<\/code> mode, otherwise, the role assigned to the Notebook Instance is not assumed inside the docker container.<\/p>\n<p>On the host (here <code>1234567890<\/code> is our account number, and <code>DataScientist<\/code> is the role attached to the Sagemaker Notebook instance):<\/p>\n<pre><code>$ aws sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>Running the same command inside a Docker container with <code>--network=host<\/code> produces the same result:<\/p>\n<pre><code>$ docker run --network host amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>However, it doesn't work with Docker <code>--network=bridge<\/code>:<\/p>\n<pre><code>$ docker run amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAIMGPPFPT5T6N7BYX6:i-0b2a9080d5ed1cb98&quot;,\n    &quot;Account&quot;: &quot;366152344081&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::366152344081:assumed-role\/BaseNotebookInstanceEc2InstanceRole\/i-0b2a9080d5ed1cb98&quot;\n}\n<\/code><\/pre>\n<p>As you can see, it's a completely different role being assumed. Notice the account number 366152344081 and the role ARN - it's sth internal to AWS.<\/p>\n<p>We would like to keep the default networking option for Docker (bridge) and at the same time be able to assume the correct role (the one attached to SageMaker Notebook instance e.g. <code>DataScientist<\/code> in our case) attached to the host system (Sagemaker Notebook). Are there any hacks (e.g. iptable rules, etc.) to achieve that?<\/p>",
        "Challenge_closed_time":1638951877412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628255185970,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1639139946700,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68682085",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":30.67,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2971.3031783333,
        "Challenge_title":"Assume Sagemaker Notebook instance role from Docker container with default network mode",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":195,
        "Challenge_word_count":244,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384530039387,
        "Poster_location":"Ljubljana, Slovenia",
        "Poster_reputation_count":2470.0,
        "Poster_view_count":285.0,
        "Solution_body":"<p>If we look at the networks that were created on a clean Sagemaker Notebook instance, we can notice a user-defined bridge network named <code>sagemaker-local<\/code>:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nf1d5a59a8c9e        bridge              bridge              local\n6142e6764495        host                host                local\n194adfb00f0a        none                null                local\n99de6c086aa8        sagemaker-local     bridge              local\n<\/code><\/pre>\n<p>If we then attach to this custom bridge, we will be able to assume the correct role (the one attached to the Sagemaker Notebook instance itself):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run --network sagemaker-local amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<hr \/>\n<p><strong>UPDATE<\/strong><\/p>\n<p>As of this writing (10 Dec 2021) you don't need to attach to <code>sagemaker-local<\/code> bridge network anymore, the default <code>bridge<\/code> will work as well (note <code>--network bridge<\/code> is implicit in this call):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>Make sure you restart your SageMaker Notebook instance.<\/p>\n<p>Also, <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/setup.sh\" rel=\"nofollow noreferrer\">here<\/a> I found some manual patching (iptables etc.), but with the update it's already patched.<\/p>\n<p>Thanks to AWS who fixed this :)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1639131288447,
        "Solution_link_count":1.0,
        "Solution_readability":21.9,
        "Solution_reading_time":24.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":178.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1428432018420,
        "Answerer_location":"New York, United States",
        "Answerer_reputation_count":301.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":45.3193069444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm pretty new to Tensorflow and SageMaker and I'm trying to figure out how to write my <code>serving_input_fn()<\/code>. I've tried a number of ways to do it, but to no avail. <\/p>\n\n<p>my input function has 3 feature columns: <code>amount_normalized, x_month and y_month<\/code>:<\/p>\n\n<pre><code>def construct_feature_columns():\n    amount_normalized = tf.feature_column.numeric_column(key='amount_normalized')\n    x_month = tf.feature_column.numeric_column(key='x_month')\n    y_month = tf.feature_column.numeric_column(key='y_month')\n    return set([amount_normalized, x_month, y_month])\n<\/code><\/pre>\n\n<p>I want to be able to call my deployed model using something like <code>deployed_model.predict([1.23,0.3,0.8])<\/code> <\/p>\n\n<p>Where the first element is <code>amount_normalized<\/code>, second is <code>x_month<\/code> third is <code>y_month<\/code><\/p>\n\n<p>I've tried this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(params):\n    feature_placeholders = {\n      key : tf.placeholder(tf.float32, [None]) \\\n        for key in FEATURES\n    }\nreturn tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholders)()\n<\/code><\/pre>\n\n<p>But all I get is:\n<code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\".<\/code><\/p>\n\n<p>Any help would be <strong>really<\/strong> appreciated!<\/p>",
        "Challenge_closed_time":1525019634972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1524856485467,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50068941",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.2,
        "Challenge_reading_time":18.95,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":45.3193069444,
        "Challenge_title":"SageMaker Tensorflow - how to write my serving_input_fn()",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1337,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428432018420,
        "Poster_location":"New York, United States",
        "Poster_reputation_count":301.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>Posting this here in case anyone else has this issue.<\/p>\n\n<p>After a bunch of trial and error I managed to solve my issue by writing my serving input function like this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(hyperparameters):\n    feature_spec = {\n        key : tf.FixedLenFeature(shape=[], dtype = tf.float32) \\\n          for key in FEATURES\n    }\n    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)()\n<\/code><\/pre>\n\n<p>I can then call my deployed model by passing in a hash:<\/p>\n\n<pre><code>deployed_model.predict({\"amount_normalized\": 2.3, \"x_month\": 0.2, \"y_month\": -0.3})\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":8.4,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":68.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":79.1199802778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are defining in Databricks a PythonScriptStep(). When using PythonScriptStep() within our pipeline script we can't find the scoring.py file.<\/p>\n<pre><code>scoring_step = PythonScriptStep(\n    name=&quot;Scoring_Step&quot;,\n    source_directory=os.getenv(&quot;DATABRICKS_NOTEBOOK_PATH&quot;, &quot;\/Users\/USER_NAME\/source_directory&quot;),\n    script_name=&quot;.\/scoring.py&quot;,\n    arguments=[&quot;--input_dataset&quot;, ds_consumption],\n    compute_target=pipeline_cluster,\n    runconfig=pipeline_run_config,\n    allow_reuse=False)\n<\/code><\/pre>\n<p>We getting the following error message:<\/p>\n<pre><code>Step [Scoring_Step]: script not found at: \/databricks\/driver\/scoring.py. Make sure to specify an appropriate source_directory on the Step or default_source_directory on the Pipeline.\n<\/code><\/pre>\n<p>For some reason Databricks is searching for the file in '\/databricks\/driver\/' instead of the folder we entered.<\/p>\n<p>There is also the way to use DatabricksStep() instead of PythonScriptStep(), but because of specific reasons we need to use the PythonSriptStep() class.<\/p>\n<p>Could anybody help us with this specific problem?<\/p>\n<p>Thank you very much for any help!<\/p>",
        "Challenge_closed_time":1656324774632,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655997421433,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1656039942703,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72732616",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":16.23,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":90.9314441667,
        "Challenge_title":"Can't find scoring.py when using PythonScriptStep() in Databricks",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":68,
        "Challenge_word_count":123,
        "Platform":"Stack Overflow",
        "Poster_created_time":1544598969960,
        "Poster_location":"Germany",
        "Poster_reputation_count":37.0,
        "Poster_view_count":21.0,
        "Solution_body":"<pre><code>scoring_step = PythonScriptStep(\n    name=&quot;Scoring_Step&quot;,\n    source_directory=os.getenv(&quot;DATABRICKS_NOTEBOOK_PATH&quot;, &quot;\/Users\/USER_NAME\/source_directory&quot;),\n    script_name=&quot;.\/scoring.py&quot;,\n    arguments=[&quot;--input_dataset&quot;, ds_consumption],\n    compute_target=pipeline_cluster,\n    runconfig=pipeline_run_config,\n    allow_reuse=False)\n<\/code><\/pre>\n<p>Change the above code block with below code block. It will resolve the error<\/p>\n<pre><code>data_ref = OutputFileDatasetConfig(\n    name='data_ref',\n    destination=(ds, '\/data')\n).as_upload()\n\n\ndata_prep_step = PythonScriptStep(\n    name='data_prep',\n    script_name='pipeline_steps\/data_prep.py',\n    source_directory='\/.',\n    arguments=[\n        '--main_path', main_ref,\n        '--data_ref_folder', data_ref\n                ],\n    inputs=[main_ref, data_ref],\n    outputs=[data_ref],\n    runconfig=arbitrary_run_config,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p>Reference link for the <a href=\"https:\/\/scoring_step%20=%20PythonScriptStep(%20%20%20%20%20name=%22Scoring_Step%22,%20%20%20%20%20source_directory=os.getenv(%22DATABRICKS_NOTEBOOK_PATH%22,%20%22\/Users\/USER_NAME\/source_directory%22),%20%20%20%20%20script_name=%22.\/scoring.py%22,%20%20%20%20%20arguments=%5B%22--input_dataset%22,%20ds_consumption%5D,%20%20%20%20%20compute_target=pipeline_cluster,%20%20%20%20%20runconfig=pipeline_run_config,%20%20%20%20%20allow_reuse=False)\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":32.4,
        "Solution_reading_time":19.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":56.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1392296244356,
        "Answerer_location":"Chicago, IL, USA",
        "Answerer_reputation_count":1020.0,
        "Answerer_view_count":206.0,
        "Challenge_adjusted_solved_time":17.7042919445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am finetuning multiple models using for loop as follows.<\/p>\n<pre><code>for file in os.listdir(args.data_dir):\n    finetune(args, file)\n<\/code><\/pre>\n<p>BUT <code>wandb<\/code> website shows plots and logs only for the first file i.e., <code>file1<\/code> in <code>data_dir<\/code> although it is training and saving models for other files. It feels very strange behavior.<\/p>\n<pre><code>wandb: Synced bertweet-base-finetuned-file1: https:\/\/wandb.ai\/***\/huggingface\/runs\/***\n<\/code><\/pre>\n<p>This is a small snippet of <strong>finetuning<\/strong> code with Huggingface:<\/p>\n<pre><code>def finetune(args, file):\n    training_args = TrainingArguments(\n        output_dir=f'{model_name}-finetuned-{file}',\n        overwrite_output_dir=True,\n        evaluation_strategy='no',\n        num_train_epochs=args.epochs,\n        learning_rate=args.lr,\n        weight_decay=args.decay,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        fp16=True, # mixed-precision training to boost speed\n        save_strategy='no',\n        seed=args.seed,\n        dataloader_num_workers=4,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset['train'],\n        eval_dataset=None,\n        data_collator=data_collator,\n    )\n    trainer.train()\n    trainer.save_model()\n<\/code><\/pre>",
        "Challenge_closed_time":1650469853183,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650379721377,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1650406117732,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71926953",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":15.8,
        "Challenge_reading_time":17.51,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":25.0366127778,
        "Challenge_title":"Wandb website for Huggingface Trainer shows plots and logs only for the first model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":178,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1392296244356,
        "Poster_location":"Chicago, IL, USA",
        "Poster_reputation_count":1020.0,
        "Poster_view_count":206.0,
        "Solution_body":"<p><code>wandb.init(reinit=True)<\/code> and <code>run.finish()<\/code> helped me to log the models <strong>separately<\/strong> on wandb website.<\/p>\n<p>The working code looks like below:<\/p>\n<pre><code>\nfor file in os.listdir(args.data_dir):\n    finetune(args, file)\n\nimport wandb\ndef finetune(args, file):\n    run = wandb.init(reinit=True)\n    ...\n    run.finish()\n<\/code><\/pre>\n<p>Reference: <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script\" rel=\"nofollow noreferrer\">https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.5,
        "Solution_reading_time":8.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":40.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":70.57987,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I already post my problem <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/aff7df3f-afbc-4abc-8fb0-5597184fa6c1\/export-data-blob-storage-v2?forum=MachineLearning\" rel=\"nofollow noreferrer\">here<\/a> and they suggested me to post it here.\nI am trying to export data from Azure ML to Azure Storage but I have this error:<\/p>\n\n<p>Error writing to cloud storage: The remote server returned an error: (400) Bad Request.. Please check the url. . ( Error 0151 )<\/p>\n\n<p>My blob storage configuration is Storage v2 \/ Standard and  Require secure transfer set as enabled.<\/p>\n\n<p>If I set the Require secure transfer set as disabled, the export works fine.<\/p>\n\n<p><strong>How can I export data to my blob storage with the require secure transfer set as enabled ?<\/strong><\/p>",
        "Challenge_closed_time":1550477536900,
        "Challenge_comment_count":1,
        "Challenge_created_time":1550223149923,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1550223449368,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54706312",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":10.36,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":70.6630491667,
        "Challenge_title":"Azure ML studio export data Azure Storage V2",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":819,
        "Challenge_word_count":109,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521189557296,
        "Poster_location":null,
        "Poster_reputation_count":160.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>According to the offical tutorial <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-to-azure-blob-storage\" rel=\"nofollow noreferrer\"><code>Export to Azure Blob Storage<\/code><\/a>, there are two authentication types for exporting data to Azure Blob Storage: SAS and Account. The description for them as below.<\/p>\n\n<blockquote>\n  <ol start=\"4\">\n  <li><p>For <strong>Authentication type<\/strong>, choose <strong>Public (SAS URL)<\/strong> if you know that the storage supports access via a SAS URL.<\/p>\n  \n  <p>A SAS URL is a special type of URL that can be generated by using an Azure storage utility, and is available for only a limited time. It contains all the information that is needed for authentication and download.<\/p>\n  \n  <p>For <strong>URI<\/strong>, type or paste the full URI that defines the account and the public blob.<\/p><\/li>\n  <li><p>For private accounts, choose <strong>Account<\/strong>, and provide the account name and the account key, so that the experiment can write to the storage account.<\/p>\n  \n  <ul>\n  <li><p><strong>Account name<\/strong>: Type or paste the name of the account where you want to save the data. For example, if the full URL of the storage account is <a href=\"http:\/\/myshared.blob.core.windows.net\" rel=\"nofollow noreferrer\">http:\/\/myshared.blob.core.windows.net<\/a>, you would type myshared.<\/p><\/li>\n  <li><p><strong>Account key<\/strong>: Paste the storage access key that is associated with the account.<\/p><\/li>\n  <\/ul><\/li>\n  <\/ol>\n<\/blockquote>\n\n<p>I try to use a simple module combination as the figure and Python code below to test the issue you got.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9XXPV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9XXPV.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<pre><code>import pandas as pd\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    dataframe1 = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})\n    return dataframe1,\n<\/code><\/pre>\n\n<p>When I tried to use the authentication type <code>Account<\/code> of my Blob Storage V2 account, I got the same issue as yours which the error code is <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/errors\/error-0151\" rel=\"nofollow noreferrer\">Error 0151<\/a> as below via click the <code>View error log<\/code> Button under the link of <code>View output log<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/TFQgO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TFQgO.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<blockquote>\n  <p><strong>Error 0151<\/strong><\/p>\n  \n  <p>There was an error writing to cloud storage. Please check the URL.<\/p>\n  \n  <p>This error in Azure Machine Learning occurs when the module tries to write data to cloud storage but the URL is unavailable or invalid.<\/p>\n  \n  <p><strong>Resolution<\/strong>\n  Check the URL and verify that it is writable.<\/p>\n  \n  <p><strong>Exception Messages<\/strong><\/p>\n  \n  <ul>\n  <li>Error writing to cloud storage (possibly a bad url).<\/li>\n  <li>Error writing to cloud storage: {0}. Please check the url.<\/li>\n  <\/ul>\n<\/blockquote>\n\n<p>Based on the error description above, the error should be caused by the blob url with SAS incorrectly generated by the <code>Export Data<\/code> module code with account information. May I think the code is old and not compatible with the new V2 storage API or API version information. You can report it to <code>feedback.azure.com<\/code>.<\/p>\n\n<p>However, I switched to use <code>SAS<\/code> authentication type to type a blob url with a SAS query string of my container which I generated via <a href=\"https:\/\/azure.microsoft.com\/en-us\/features\/storage-explorer\/\" rel=\"nofollow noreferrer\">Azure Storage Explorer<\/a> tool as below, it works fine.<\/p>\n\n<p>Fig 1: Right click on the container of your Blob Storage account, and click the <code>Get Shared Access Signature<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/1fyFL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1fyFL.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 2: Enable the permission <code>Write<\/code> (recommended to use UTC timezone) and click <code>Create<\/code> button<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/IsbQQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IsbQQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 3: Copy the <code>Query string<\/code> value, and build a blob url with a container SAS query string like <code>https:\/\/&lt;account name&gt;.blob.core.windows.net\/&lt;container name&gt;\/&lt;blob name&gt;&lt;query string&gt;<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RGnxn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RGnxn.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Note: The blob must be not exist in the container, otherwise an <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/errors\/error-0057\" rel=\"nofollow noreferrer\">Error 0057<\/a> will be caused.<\/em><\/strong><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":17.0,
        "Solution_readability":10.9,
        "Solution_reading_time":65.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":42.0,
        "Solution_word_count":563.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1395235213383,
        "Answerer_location":"Norway",
        "Answerer_reputation_count":46807.0,
        "Answerer_view_count":3021.0,
        "Challenge_adjusted_solved_time":6.7343452778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I recently (and sceptically) started messing around with <strong>Azure Machine Learning Studio<\/strong>. When I stumbled accross the menu option for a machine learning work-flow <strong>Open in a new Notebook<\/strong> (For Python 3, 2 or R) I thought it was too good to be true:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/AYDbo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AYDbo.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And it most likely is, since this option is seemingly only available for the first step of the process. The option still exists in the <strong>right-click menu<\/strong> elsewhere, but it's greyed out:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Cy6MG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Cy6MG.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does anyone know why it is like this? Do I have to activate something in the menus, or buy some sort of a premium license? Is the functionality only available for <em>some<\/em> of the machine learning algorithms? Or is it just not supposed to be an available option in the menus?<\/p>\n\n<p>By the way, if you click <kbd>Python 3<\/kbd> 3 in the first step, you get a corresponding Python 3 code snippet in a Jupyter Notebook where you immediately can start messing around with the dataset:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/gTHGF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gTHGF.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I realize that making this functionality available for each step in each and every model that anyone chooses to design would be an extremely difficult and maybe even impossible thing to do. But again, why is the option still in the menu?<\/p>",
        "Challenge_closed_time":1529068177343,
        "Challenge_comment_count":1,
        "Challenge_created_time":1529043933700,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50870166",
        "Challenge_link_count":6,
        "Challenge_participation_count":2,
        "Challenge_readability":9.6,
        "Challenge_reading_time":22.89,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":6.7343452778,
        "Challenge_title":"Can not open new Python or R notebook in Azure Machine Learning Studio",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1386,
        "Challenge_word_count":243,
        "Platform":"Stack Overflow",
        "Poster_created_time":1395235213383,
        "Poster_location":"Norway",
        "Poster_reputation_count":46807.0,
        "Poster_view_count":3021.0,
        "Solution_body":"<p>Following the suggestion from @Jon in the comment section as well as a suggestion on <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/windowsdesktop\/en-US\/84a33ecc-1db2-4d11-83d2-3e96f0bcfaa7\/why-open-in-a-new-notebook-is-invalid?forum=MachineLearning\" rel=\"nofollow noreferrer\">microsoft.com<\/a>, I added a <strong>Convert to CSV Module<\/strong> at the end. After running the experiment, <strong>Open in a new Notebook<\/strong> is available when you right clik the <strong>Convert to CSV Module<\/strong>:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Bz7nJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Bz7nJ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>What you get by clicking <kbd>Python 3<\/kbd> is this:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ywbHz.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ywbHz.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The functionality is certainly not as magnificent as I was hoping, but it's still pretty cool. If anyone knows <em>anything<\/em> about other possibilites or plans for future development, please don't hesitate to contribute with an answer!<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":15.2,
        "Solution_reading_time":15.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":112.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1603263564903,
        "Answerer_location":null,
        "Answerer_reputation_count":488.0,
        "Answerer_view_count":59.0,
        "Challenge_adjusted_solved_time":0.3718936111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am able to read multiple csv files from S3 bucket with boto3 in python and finally combine those files in single dataframe in pandas.However, in some of the folders there are some empty files which results in the error &quot;No columns to parse from file&quot;. Can we skip those empty files in the below codes?<\/p>\n<pre><code>s3 = boto3.resource('s3')\nbucket = s3.Bucket('testbucket')\n\nprefix_objs = bucket.objects.filter(Prefix=&quot;extracted\/abc&quot;)\n\n    prefix_df = []\n\nfor obj in prefix_objs:\n    key = obj.key\n    body = obj.get()['Body'].read()\n    temp = pd.read_csv(io.BytesIO(body),header=None, encoding='utf8',sep=',')        \n    prefix_df.append(temp)\n<\/code><\/pre>\n<p>I have used this ans [https:\/\/stackoverflow.com\/questions\/52855221\/reading-multiple-csv-files-from-s3-bucket-with-boto3][1]<\/p>",
        "Challenge_closed_time":1613737607800,
        "Challenge_comment_count":1,
        "Challenge_created_time":1613736268983,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66277250",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":10.99,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.3718936111,
        "Challenge_title":"Reading multiple CSV files from S3 using Python with Boto3",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3048,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1486529134640,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<pre><code>s3 = boto3.resource('s3')\nbucket = s3.Bucket('testbucket')\n\nprefix_objs = bucket.objects.filter(Prefix=&quot;extracted\/abc&quot;)\n\nprefix_df = []\n\nfor obj in prefix_objs:\n    try:\n        key = obj.key\n        body = obj.get()['Body'].read()\n        temp = pd.read_csv(io.BytesIO(body),header=None, encoding='utf8',sep=',')        \n        prefix_df.append(temp)\n    except:\n        continue\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":4.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":23.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":1.0276408334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Created an azure ml dataset. how do I delete the dataset if it already exists?<\/p>\n<pre><code>#register dataset\npath='path'\nfile_ds=Dataset.File.from_files(path=path)\nfile_ds=file_ds.register(workspace=ws,name=&quot;Dataset&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1658911127743,
        "Challenge_comment_count":2,
        "Challenge_created_time":1658908144600,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73134073",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.7,
        "Challenge_reading_time":3.97,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.8286508334,
        "Challenge_title":"How to delete azureml dataset if it already exists",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":114,
        "Challenge_word_count":29,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606756004663,
        "Poster_location":null,
        "Poster_reputation_count":49.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>AFAIK, as of now, deleting the dataset using <a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-python\/search?q=delete+datasets\" rel=\"nofollow noreferrer\">AzureML Python SDK<\/a> is not possible via <code>delete.datasets()<\/code>. But it might be possible via <a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-python\/blob\/396853110f5c15463e5a531ee759446d3389d441\/sdk\/ml\/azure-ai-ml\/azure\/ai\/ml\/_restclient\/dataset_dataplane\/operations\/_delete_operations.py\" rel=\"nofollow noreferrer\">delete_operations.py<\/a><\/p>\n<p>As suggested by <a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/567611\/is-there-a-way-to-delete-datasets-on-azureml.html\" rel=\"nofollow noreferrer\">YutongTie<\/a>, you can delete the dataset using the Azure Machine Learning Studio.<\/p>\n<p>References: <a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/379022\/how-to-delete-data-backing-a-dataset.html\" rel=\"nofollow noreferrer\">How to Delete Data Backing a Dataset<\/a>, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-export-delete-data\" rel=\"nofollow noreferrer\">Export or delete your Machine Learning service workspace data<\/a> and <a href=\"https:\/\/rdrr.io\/cran\/AzureML\/man\/delete.datasets.html\" rel=\"nofollow noreferrer\">R interface to AzureML - delete dataset<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1658911844107,
        "Solution_link_count":6.0,
        "Solution_readability":20.6,
        "Solution_reading_time":17.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1619518958572,
        "Answerer_location":"Madrid, Espa\u00f1a",
        "Answerer_reputation_count":98.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":0.0476138889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to create a new experiment on mlflow but I have this problem:<\/p>\n<pre><code>Exception: Run with UUID l142ae5a7cf04a40902ae9ed7326093c is already active.\n\n<\/code><\/pre>\n<p>This is my code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\nimport mlflow.sklearn\nimport sys\n\nmlflow.set_experiment(&quot;New experiment 2&quot;)\n\nmlflow.set_tracking_uri('http:\/\/mlflow:5000')\nst= mlflow.start_run(run_name='Test2')\nid = st.info.run_id\nmlflow.log_metric(&quot;score&quot;, score)\nmlflow.sklearn.log_model(model, &quot;wineModel&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1620805914963,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620805743553,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67499339",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":8.4,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.0476138889,
        "Challenge_title":"Create mlflow experiment: Run with UUID is already active",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":675,
        "Challenge_word_count":56,
        "Platform":"Stack Overflow",
        "Poster_created_time":1620748388616,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>You have to run mlflow.end_run() to finish the first experiment. Then you can create another<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.1,
        "Solution_reading_time":1.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":15.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":14.5543175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to invoke a MultiModel Endpoint with a RandomCutForest Model. I receive error though, 'Error loading model'. I can invoke the endpoint with models given from the examples.\nAm I missing something e.g. limitations on what models I can use? <\/p>\n\n<p>For MultiModel inspiration I am using below:<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb<\/a><\/p>\n  \n  <p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/<\/a><\/p>\n<\/blockquote>\n\n<p>I am trying to deploy the outputted 'model.tar.gz' from below RCF example in the MultiModel endpoint:<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb<\/a><\/p>\n<\/blockquote>\n\n<pre><code>model_name = 'model'\nfull_model_name = '{}.tar.gz'.format(model_name)\nfeatures = data\n\nbody = ','.join(map(str, features)) + '\\n'\nresponse = runtime_sm_client.invoke_endpoint(\n                    EndpointName=endpoint_name,\n                    ContentType='text\/csv',\n                    TargetModel=full_model_name,\n                    Body=body)\nprint(response)\n<\/code><\/pre>\n\n<p><strong>Cloudwatch log Error:<\/strong><\/p>\n\n<pre><code>&gt; 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Error loading model: Unable\n&gt; to load model: invalid load key, '{'. [17:28:59]\n&gt; \/workspace\/src\/learner.cc:334: Check failed: fi-&gt;Read(&amp;mparam_,\n&gt; sizeof(mparam_)) == sizeof(mparam_) (25 vs. 136) : BoostLearner: wrong\n&gt; model format 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Stack trace: 2020-04-27\n&gt; 17:28:59,005 [INFO ] W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (0)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x24)\n&gt; [0x7f37ce1cacb4] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9 com.amazonaws.ml.mms.wlm.WorkerThread\n&gt; - Backend response time: 0 2020-04-27 17:28:59,005 [INFO ] W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (1)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(xgboost::LearnerImpl::Load(dmlc::Stream*)+0x4b5)\n&gt; [0x7f37ce266985] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (2)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(XGBoosterLoadModel+0x37)\n&gt; [0x7f37ce1bf417] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (3)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c)\n&gt; [0x7f37ee993ec0] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (4)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d)\n&gt; [0x7f37ee99387d] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (5)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce)\n&gt; [0x7f37eeba91de] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (6)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12c14)\n&gt; [0x7f37eeba9c14] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (7)\n&gt; \/miniconda3\/bin\/python(_PyObject_FastCallKeywords+0x48b)\n&gt; [0x563d71b4218b] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (8)\n&gt; \/miniconda3\/bin\/python(_PyEval_EvalFrameDefault+0x52cf)\n&gt; [0x563d71b91e8f] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  2020-04-27 17:28:59,005\n&gt; [WARN ] W-9003-b39b888fb4a3fa6cf83bb34a9\n&gt; com.amazonaws.ml.mms.wlm.WorkerThread - Backend worker thread\n&gt; exception. java.lang.IllegalArgumentException: reasonPhrase contains\n&gt; one of the following prohibited characters: \\r\\n: Unable to load\n&gt; model: Unable to load model: invalid load key, '{'. [17:28:59]\n&gt; \/workspace\/src\/learner.cc:334: Check failed: fi-&gt;Read(&amp;mparam_,\n&gt; sizeof(mparam_)) == sizeof(mparam_) (25 vs. 136) : BoostLearner: wrong\n&gt; model format Stack trace:   [bt] (0)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x24)\n&gt; [0x7f37ce1cacb4]   [bt] (1)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(xgboost::LearnerImpl::Load(dmlc::Stream*)+0x4b5)\n&gt; [0x7f37ce266985]   [bt] (2)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(XGBoosterLoadModel+0x37)\n&gt; [0x7f37ce1bf417]   [bt] (3)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c)\n&gt; [0x7f37ee993ec0]   [bt] (4)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d)\n&gt; [0x7f37ee99387d]   [bt] (5)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce)\n&gt; [0x7f37eeba91de]   [bt] (6)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12c14)\n&gt; [0x7f37eeba9c14]   [bt] (7)\n&gt; \/miniconda3\/bin\/python(_PyObject_FastCallKeywords+0x48b)\n&gt; [0x563d71b4218b]   [bt] (8)\n&gt; \/miniconda3\/bin\/python(_PyEval_EvalFrameDefault+0x52cf)\n&gt; [0x563d71b91e8f]\n<\/code><\/pre>",
        "Challenge_closed_time":1588061581360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588009185817,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61464960",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":22.8,
        "Challenge_reading_time":86.51,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":52,
        "Challenge_solved_time":14.5543175,
        "Challenge_title":"SageMaker multimodel and RandomCutForest",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":324,
        "Challenge_word_count":394,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426492930156,
        "Poster_location":null,
        "Poster_reputation_count":398.0,
        "Poster_view_count":84.0,
        "Solution_body":"<p>SageMaker Random Cut Forest is part of the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">built-in algorithm library<\/a> and cannot be deployed in multi-model endpoint (MME). Built-in algorithms currently cannot be deployed to MME. XGboost is an exception, since it has an open-source container <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-xgboost-container<\/a>.<\/p>\n\n<p>If you really need to deploy a RCF to a multi-model endpoint, one option is to find a reasonably similar open-source implementation (for example <a href=\"https:\/\/github.com\/kLabUM\/rrcf\" rel=\"nofollow noreferrer\"><code>rrcf<\/code><\/a> looks reasonably serious: based <a href=\"http:\/\/proceedings.mlr.press\/v48\/guha16.pdf\" rel=\"nofollow noreferrer\">on the same paper from Guha et al<\/a> and with 170+ github stars) and create a custom MME docker container. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">documentation is here<\/a> and there is <a href=\"https:\/\/github.com\/giuseppeporcelli\/sagemaker-custom-serving-containers\/blob\/master\/multi-model-server-container\/notebook\/multi-model-server-container.ipynb\" rel=\"nofollow noreferrer\">an excellent tuto here<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":19.2,
        "Solution_reading_time":17.98,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":116.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":53.8060766667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using aws sagemaker to invoke the endpoint : <\/p>\n\n<pre><code>payload = pd.read_csv('payload.csv', header=None)\n\n&gt;&gt; payload\n\n\n    0   1   2   3   4\n0   setosa  5.1     3.5     1.4     0.2\n1   setosa  5.1     3.5     1.4     0.2\n<\/code><\/pre>\n\n<p>with this code :<\/p>\n\n<pre><code>response = runtime.invoke_endpoint(EndpointName=r_endpoint,\n                                   ContentType='text\/csv',\n                                   Body=payload)\n<\/code><\/pre>\n\n<p>But I got this problem : <\/p>\n\n<pre><code>ParamValidationError                      Traceback (most recent call last)\n&lt;ipython-input-304-f79f5cf7e0e0&gt; in &lt;module&gt;()\n      1 response = runtime.invoke_endpoint(EndpointName=r_endpoint,\n      2                                    ContentType='text\/csv',\n----&gt; 3                                    Body=payload)\n      4 \n      5 result = json.loads(response['Body'].read().decode())\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    312                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    313             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 314             return self._make_api_call(operation_name, kwargs)\n    315 \n    316         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    584         }\n    585         request_dict = self._convert_to_request_dict(\n--&gt; 586             api_params, operation_model, context=request_context)\n    587 \n    588         handler, event_response = self.meta.events.emit_until_response(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _convert_to_request_dict(self, api_params, operation_model, context)\n    619             api_params, operation_model, context)\n    620         request_dict = self._serializer.serialize_to_request(\n--&gt; 621             api_params, operation_model)\n    622         prepare_request_dict(request_dict, endpoint_url=self._endpoint.host,\n    623                              user_agent=self._client_config.user_agent,\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/validate.py in serialize_to_request(self, parameters, operation_model)\n    289                                                     operation_model.input_shape)\n    290             if report.has_errors():\n--&gt; 291                 raise ParamValidationError(report=report.generate_report())\n    292         return self._serializer.serialize_to_request(parameters,\n    293                                                      operation_model)\n\nParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value:         0    1    2    3    4\n0  setosa  5.1  3.5  1.4  0.2\n1  setosa  5.1  3.5  1.4  0.2, type: &lt;class 'pandas.core.frame.DataFrame'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\n<\/code><\/pre>\n\n<p>I am just using the same code\/step like in the aws tutorial .  <\/p>\n\n<p>Can you help me to resolve this problem please?<\/p>\n\n<p>thank you<\/p>",
        "Challenge_closed_time":1536693194576,
        "Challenge_comment_count":0,
        "Challenge_created_time":1536499492700,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52244963",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":34.43,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":53.8060766667,
        "Challenge_title":"Impossible to invoke endpoint with sagemaker",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":4668,
        "Challenge_word_count":229,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518617852856,
        "Poster_location":null,
        "Poster_reputation_count":495.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>The payload variable is a Pandas' DataFrame, while <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint\" rel=\"nofollow noreferrer\">invoke_endpoint()<\/a> expects  <code>Body=b'bytes'|file<\/code>.<\/p>\n\n<p>Try something like this (coding blind):<\/p>\n\n<pre><code>response = runtime.invoke_endpoint(EndpointName=r_endpoint,\n                                   ContentType='text\/csv',\n                                   Body=open('payload.csv'))\n<\/code><\/pre>\n\n<p>More on the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-inference.html\" rel=\"nofollow noreferrer\">expected formats here<\/a>. \nMake sure the file doesn't include a header.<\/p>\n\n<p>Alternatively, convert your DataFrame to bytes, <a href=\"https:\/\/stackoverflow.com\/questions\/34666860\/converting-pandas-dataframe-to-bytes\">like in this example<\/a>, and pass those bytes instead of passing a DataFrame.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":21.3,
        "Solution_reading_time":12.25,
        "Solution_score_count":4.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":34.4616136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am from R background where we can use Plumber kind tool which provide visualization\/graph as Image via end points so we can integrate in our Java application.<\/p>\n\n<p>Now I want to integrate my Python\/Juypter visualization graph with my Java application but not sure how to host it and make it as endpoint. Right now I using AWS sagemaker to host Juypter notebook<\/p>",
        "Challenge_closed_time":1526073355692,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525949293883,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50271174",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":5.7,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":34.4616136111,
        "Challenge_title":"How to make a Python Visualization as service | Integrate with website | specially sagemaker",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":76,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501403168107,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>Amazon SageMaker is a set of different services for data scientists. You are using the notebook service that is used for developing ML models in an interactive way. The hosting service in SageMaker is creating an endpoint based on a trained model. You can call this endpoint with invoke-endpoint API call for real time inference. <\/p>\n\n<p>It seems that you are looking for a different type of hosting that is more suitable for serving HTML media rich pages, and doesn\u2019t fit into the hosting model of SageMaker. A combination of EC2 instances, with pre-built AMI or installation scripts, Congnito for authentication, S3 and EBS for object and block storage, and similar building blocks should give you a scalable and cost effective solution. <\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":9.18,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":123.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1520413126203,
        "Answerer_location":null,
        "Answerer_reputation_count":37123.0,
        "Answerer_view_count":4058.0,
        "Challenge_adjusted_solved_time":4.1983277778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How do I delete projects in my workspace ?when I click to delete a project the delete button is inactive. Tried clearing cache and whatnot but cannot delete the project from studio.azureml.net... how do I do this ? <\/p>",
        "Challenge_closed_time":1526865758888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526851287060,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50439489",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":2.0,
        "Challenge_reading_time":3.22,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4.0199522222,
        "Challenge_title":"Delete Project Azure ML Studio ( Web App)",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":576,
        "Challenge_word_count":44,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506435894640,
        "Poster_location":"Southeast Asia",
        "Poster_reputation_count":1922.0,
        "Poster_view_count":404.0,
        "Solution_body":"<p>I have reproduced your issue. Try to go to your project -> EDIT ->remove the <strong>ASSETS<\/strong> of your project. Then the delete button will be able.<\/p>\n\n<p>You could follow the screenshot.<\/p>\n\n<ol>\n<li>The <strong>DELETE<\/strong> button is disable.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/E850F.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E850F.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>2.Go to <strong>EDIT<\/strong> and remove the <strong>ASSETS<\/strong>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/PbEZC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PbEZC.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/2kBgO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2kBgO.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>3.Then the <strong>DELETE<\/strong> button will be able<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/08lOW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/08lOW.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1526866401040,
        "Solution_link_count":8.0,
        "Solution_readability":12.4,
        "Solution_reading_time":14.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1587507179987,
        "Answerer_location":null,
        "Answerer_reputation_count":1.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":0.0376611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to use AWS SageMaker following documentation. I successfully loaded data, trained and deployed the model.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4Mjew.png\" rel=\"nofollow noreferrer\">deployed-model<\/a><\/p>\n<p>My next step have to be using AWS Lambda, connect it to this SageMaker endpoint.\nI saw, that I need to give Lambda IAM execution role permission to invoke a model endpoint.\nI add some data to IAM policy JSON and now it has this view<\/p>\n<pre><code>{\n&quot;Version&quot;: &quot;2012-10-17&quot;,\n&quot;Statement&quot;: [\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: &quot;logs:CreateLogGroup&quot;,\n        &quot;Resource&quot;: &quot;arn:aws:logs:us-east-1:&lt;my-account&gt;:*&quot;\n    },\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: [\n            &quot;logs:CreateLogStream&quot;,\n            &quot;logs:PutLogEvents&quot;\n        ],\n        &quot;Resource&quot;: [\n            &quot;arn:aws:logs:us-east-1:&lt;my-account&gt;:log-group:\/aws\/lambda\/test-sagemaker:*&quot;\n        ]\n    },\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n        &quot;Resource&quot;: &quot;*&quot;\n    }\n]\n<\/code><\/pre>\n<p>}<\/p>\n<p>Problem that even with role that have permission for invoking SageMaker endpoint my Lambda function didn't see it<\/p>\n<pre><code>An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint xgboost-2020-10-02-12-15-36-097 of account &lt;my-account&gt; not found.: ValidationError\n<\/code><\/pre>",
        "Challenge_closed_time":1601906782043,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601650547150,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1601906646463,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64173739",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.8,
        "Challenge_reading_time":19.63,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":71.1763591667,
        "Challenge_title":"AWS Sagemaker + AWS Lambda",
        "Challenge_topic":"Lambda Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":623,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Poster_created_time":1587507179987,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I found an error by myself. Problem was in different regions. For training and deploying model I used us-east-2 and for lambda I used us-east-1. Just creating all in same region fixed this issue!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.4,
        "Solution_reading_time":2.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554060427012,
        "Answerer_location":null,
        "Answerer_reputation_count":2433.0,
        "Answerer_view_count":228.0,
        "Challenge_adjusted_solved_time":9.9895388889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to build a new conda environment in our Sagemaker ec2 environment in a terminal session.  Packages in the original copy of the environment were corrupted, and the environment became unusable. The issue couldn't be fixed by removing packages and re-installing or using <code>conda update<\/code>.<\/p>\n<p>I nuked the environment with <code>conda env remove -n python3-cn<\/code> and then attempted to recreate the environment with:<\/p>\n<pre><code>conda env create -p \/home\/ec2-user\/SageMaker\/anaconda3\/envs\/python3-cn --file=${HOME}\/SageMaker\/efs\/.sagemaker\/python3-cn_environment.yml --force\n<\/code><\/pre>\n<p>This environment has been created a number of times in several ec2 instances for individual Sagemaker users.<\/p>\n<p>Conda logs the following:<\/p>\n<pre><code>Collecting package metadata (repodata.json): done\nSolving environment: done\n\nDownloading and Extracting Packages\npytest-arraydiff-0.2 | 14 KB     | ##################################################################################################### | 100% \npartd-0.3.8          | 32 KB     | ##################################################################################################### | 100% \n\n... several progress bar lines later...\n\npsycopg2-2.7.5       | 507 KB    | ##################################################################################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nERROR conda.core.link:_execute(700): An error occurred while installing package 'defaults::mkl-2018.0.3-1'.\nRolling back transaction: done\n\n[Errno 28] No space left on device\n()\n<\/code><\/pre>\n<p>The <code>No space left on device<\/code> error is consistent. I've tried<\/p>\n<ul>\n<li><code>conda clean --all<\/code>, removing the environment, re-building the environment<\/li>\n<li>removing the caches, removing the environment, re-building the environment<\/li>\n<li>removing the environment, shutting down and restarting JuypiterLab (our Sagemaker is configured to create <code>python3-cn<\/code> if the environment doesn't exist when JupyterLab starts)<\/li>\n<\/ul>\n<p>In the first two, I get <code>Errno 28<\/code>.<\/p>\n<p>In the last one, the instance is not created, <code>conda env list<\/code> does not show the <code>python3-cn<\/code>, but I see there is a <code>python3-cn<\/code> directory in the <code>anaconda\/envs\/<\/code> directory. If I do <code>conda activate python3-cn<\/code>, I see the prompt change, but the environment is unusuable. If I try <code>conda update --all<\/code>, I get a notification that one of the package files has been corrupted.<\/p>\n<p>Not really sure what to do here. I'm looking for space hogs, but not really finding anything significant.<\/p>",
        "Challenge_closed_time":1594867069027,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594831106687,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62919671",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.8,
        "Challenge_reading_time":35.7,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":9.9895388889,
        "Challenge_title":"conda env build fails with \"[Errno 28] No space left on device\"",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":4104,
        "Challenge_word_count":307,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311046709507,
        "Poster_location":null,
        "Poster_reputation_count":56.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>Try increasing the ebs volume amount of your notebook ... this blog explains it well: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-notebook-volume-size-up-to-16-tb-with-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-notebook-volume-size-up-to-16-tb-with-amazon-sagemaker\/<\/a><\/p>\n<p>Also, best practice is to use lifecycle configuration scripts to build\/add new dependencies ... official docs: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html<\/a><\/p>\n<p>This github page has some great template examples ... for example setting up specific configs like conda, etc: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":31.7,
        "Solution_reading_time":14.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1492331396980,
        "Answerer_location":null,
        "Answerer_reputation_count":197.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":4174.8991611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I started to develop machine learning models on The Microsoft Azure Machine Learning Studio service. The tutorials and information related to this service are rather clear but I am looking for some information that I did not find concerning the deployment of the service.<\/p>\n\n<p>I would like to understand why the input schema requires the definition of the variable to predict and why the output returns all variable fields given in entry. In this response\/request exchange a part of information transmitted is useless. I wondering if it is possible to modify manually this schema.<\/p>\n\n<p>I searched in the configuration tab of the web service panel but I did not find any information to modify the schema passed to the model.<\/p>\n\n<p>The code below is the input schema that the model requires and the value to predict is <code>WallArea<\/code>. It is not really useful to pass this variable because it is the one we try to predict. (except if we want to compare the actual value and the predicted one for test purpose).<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"WallArea\",\n        \"RoofArea\",\n        \"OverallHeight\",\n        \"GlazingArea\",\n        \"HeatingLoad\"\n      ],\n      \"Values\": [\n        [\n          \"0\",\n          \"0\",\n          \"0\",\n          \"0\",\n          \"0\"\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>The json returned by the model with the predicted value sent all data. It is much more info to what we really need (\"Scored Label Mean\" and \"Scored Label Standard Deviation\")<\/p>\n\n<pre><code>{\n  \"Results\": {\n    \"output1\": {\n      \"type\": \"DataTable\",\n      \"value\": {\n        \"ColumnNames\": [\n          \"WallArea\",\n          \"RoofArea\",\n          \"OverallHeight\",\n          \"GlazingArea\",\n          \"HeatingLoad\",\n          \"Scored Label Mean\",\n          \"Scored Label Standard Deviation\"\n        ],\n        \"ColumnTypes\": [\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\",\n          \"Numeric\"\n        ],\n        \"Values\": [\n          [\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\",\n            \"0\"\n          ]\n        ]\n      }\n    }\n  }\n}\n<\/code><\/pre>\n\n<p>My question is how to reduce\/synthesize the input\/output schema if it is possible and why the variable to predict must be sent with the input schema?<\/p>",
        "Challenge_closed_time":1568622249776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568375681780,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1568376011443,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57923187",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":25.24,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":68.49111,
        "Challenge_title":"How to modify the input\/output schema for an Azure deployment service?",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":471,
        "Challenge_word_count":292,
        "Platform":"Stack Overflow",
        "Poster_created_time":1492331396980,
        "Poster_location":null,
        "Poster_reputation_count":197.0,
        "Poster_view_count":40.0,
        "Solution_body":"<p>I found the solution.<\/p>\n\n<p>For those who have the same problem, it is pretty simple in fact. You need to add two <strong>Select Columns in Dataset<\/strong> box in your <strong>Predictive experiment<\/strong> schema.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/JA3q2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JA3q2.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Update 2020:<\/strong> Following some updates done on the service, the solution proposed is partially broken. Indeed, if you decide to not include the outcome in the first Select columns box, you well not be able to retrieve it in the second <strong><em>Select Column box<\/em><\/strong> leading to an error. To solve that, you have to remove the first Select Column box and take all features. For the second <strong><em>Select Column box<\/em><\/strong> nothing change, you select the features you want for your predictive response.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1583405648423,
        "Solution_link_count":2.0,
        "Solution_readability":9.8,
        "Solution_reading_time":11.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":123.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1657058369727,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":92.6310866667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to perform <strong>distributed training<\/strong> on <strong>Amazon SageMaker<\/strong>. The code is written with <strong>TensorFlow<\/strong> and similar to the following code where I think CPU instance should be enough:\u00a0\n<a href=\"https:\/\/github.com\/horovod\/horovod\/blob\/master\/examples\/tensorflow_word2vec.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/horovod\/horovod\/blob\/master\/examples\/tensorflow_word2vec.py<\/a><\/p>\n<p>Can <strong>Horovod with TensorFlow<\/strong> work on <strong>non-GPU<\/strong> instances in Amazon SageMaker?<\/p>",
        "Challenge_closed_time":1663198075692,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662864603780,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73676483",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":8.34,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":92.6310866667,
        "Challenge_title":"Can Horovod with TensorFlow work on non-GPU instances in Amazon SageMaker?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":17,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>Yeah you should be able to use both CPU's and GPU's with Horovod on Amazon SageMaker. Please follow the below example for the same<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":29.3,
        "Solution_reading_time":6.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1369207318272,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":84.9611705556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to take the environment variables as parameters for the template:\n<a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html<\/a><\/p>\n<p>The type seems to be Json in the template and I dont understand how to populate it.<\/p>\n<p>It seems like I can define this if i hardcode environment variables as below:<\/p>\n<pre><code>Resources:\n  SageMakerModel:\n    Type: 'AWS::SageMaker::Model'\n    Properties:\n      ExecutionRoleArn: \n        Ref: ExecutionRoleArn\n      EnableNetworkIsolation: false\n      PrimaryContainer:\n        Environment:\n          REQUEST_KEEP_ALIVE_TIME_SEC: '90'\n        Image: \n          Ref: ImageURI\n<\/code><\/pre>\n<p>However, there doesnt seem to be a way pass this in ? Anyone figured this out or any recommended way to do this ?<\/p>",
        "Challenge_closed_time":1635277709907,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634971849693,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69685819",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.0,
        "Challenge_reading_time":12.79,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":84.9611705556,
        "Challenge_title":"Taking Json type as parameter for cloudformation template",
        "Challenge_topic":"Lambda Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":415,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369207318272,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation_count":35.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>I was able to get this to work by using AWS:Include and Fn:Transform and storing my environment variables as json in passed s3 file.<\/p>\n<p>My cfn template looks like:<\/p>\n<pre><code>Resources:\n  SageMakerModel:\n    Type: 'AWS::SageMaker::Model'\n    Properties:\n      ExecutionRoleArn: \n        Ref: ExecutionRoleArn\n      EnableNetworkIsolation: false\n      PrimaryContainer:\n        Environment:\n          Fn::Transform:\n            Name: AWS::Include\n            Parameters:\n              Location: &lt;your S3 file&gt;\n        Image: \n          Ref: ImageURI\n<\/code><\/pre>\n<p>My s3 file looks like:<\/p>\n<pre><code>{\n  &quot;REQUEST_KEEP_ALIVE_TIME_SEC&quot;: &quot;90&quot;\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.1,
        "Solution_reading_time":7.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":626.8732219444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a Docker image in AWS ECR and I open my Sagemaker Notebook instance---&gt;go to terminal--&gt;docker run....\nThis is how I start my Docker container.<\/p>\n<p>Now, I want to automate this process(running my docker image on Sagemaker Notebook Instance) instead of typing the docker run commands.<\/p>\n<p>Can I create a cron job on Sagemaker? or Is there any other approach?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1626977403510,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624722520977,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68143997",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.0,
        "Challenge_reading_time":5.73,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":626.3562591667,
        "Challenge_title":"Automate Docker Run command on Sagemaker's Notebook Instance",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":393,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1497621837832,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":230.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>For this you can create an inline Bash shell in your SageMaker notebook as follows. This will take your Docker container, create the image, ECR repo if it does not exist and push the image.<\/p>\n<pre><code>%%sh\n\n# Name of algo -&gt; ECR\nalgorithm_name=your-algo-name\n\ncd container #your directory with dockerfile and other sm components\n\nchmod +x randomForest-Petrol\/train #train file for container\nchmod +x randomForest-Petrol\/serve #serve file for container\n\naccount=$(aws sts get-caller-identity --query Account --output text)\n\n# Region, defaults to us-west-2\nregion=$(aws configure get region)\nregion=${region:-us-west-2}\n\nfullname=&quot;${account}.dkr.ecr.${region}.amazonaws.com\/${algorithm_name}:latest&quot;\n\n# If the repository doesn't exist in ECR, create it.\naws ecr describe-repositories --repository-names &quot;${algorithm_name}&quot; &gt; \/dev\/null 2&gt;&amp;1\n\nif [ $? -ne 0 ]\nthen\n    aws ecr create-repository --repository-name &quot;${algorithm_name}&quot; &gt; \/dev\/null\nfi\n\n# Get the login command from ECR and execute it directly\naws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n\n# Build the docker image locally with the image name and then push it to ECR\n# with the full name.\n\ndocker build  -t ${algorithm_name} .\ndocker tag ${algorithm_name} ${fullname}\n\ndocker push ${fullname}\n<\/code><\/pre>\n<p>I am contributing this on behalf of my employer, AWS. My contribution is licensed under the MIT license. See here for a more detailed explanation\n<a href=\"https:\/\/aws-preview.aka.amazon.com\/tools\/stackoverflow-samples-license\/\" rel=\"nofollow noreferrer\">https:\/\/aws-preview.aka.amazon.com\/tools\/stackoverflow-samples-license\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1626979264576,
        "Solution_link_count":2.0,
        "Solution_readability":14.8,
        "Solution_reading_time":21.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":192.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1436818579270,
        "Answerer_location":"Eugene, OR, USA",
        "Answerer_reputation_count":474.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":19.74615,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using a \"Split Data\" module set to recommender split to split data for training and testing a matchbox recommender. The input data is a valid user-item-rating tuple (for example, 575978 - 157381 - 3) and I've left the parameters for the recommender split as default (0s for everything), besides changing it to a .75 and .25 split. However, when this module finishes, it returns the complete, unsplit dataset for dataset1 and a completely empty (but labelled) dataset for dataset2. This also happens when doing a stratified split using the \"Split Rows\" mode. Any idea what's going on?<\/p>\n\n<p>Thanks.<\/p>\n\n<p>Edit: Including a sample of my data.<\/p>\n\n<pre><code>UserID  ItemID  Rating\n835793  165937  3\n154738  11214   3\n938459  748288  3\n819375  789768  6\n738571  98987   3\n847509  153777  3\n991757  124458  3\n968685  288070  2\n236349  8337    3\n127299  545885  3\n<\/code><\/pre>",
        "Challenge_closed_time":1529598877083,
        "Challenge_comment_count":4,
        "Challenge_created_time":1529519087767,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1529527790943,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50954802",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":7.9,
        "Challenge_reading_time":11.08,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":22.1636988889,
        "Challenge_title":"Recommender Split Returning Empty Dataset",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":88,
        "Challenge_word_count":142,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436818579270,
        "Poster_location":"Eugene, OR, USA",
        "Poster_reputation_count":474.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>Figured it out. In my \"Remove Duplicate Rows\" module up the chain a bit I was only removing duplicates by UserID instead of UserID <em>and<\/em> ItemID. This still left quite a bit of rows but I'm assuming it messed with the stratification. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":3.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1468179475927,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation_count":795.0,
        "Answerer_view_count":210.0,
        "Challenge_adjusted_solved_time":143.1555908334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to perform the following Python Pandas operation in Azure Machine Learning Studio, but cannot find a module that handles it:<\/p>\n\n<pre><code>df.credit_score = df.credit_score.mask(df.credit_score &gt; 800, df.credit_score \/ 10)\n<\/code><\/pre>\n\n<p>So I'm effectively just trying to find all values in my 'credit_score' column that are greater than 800 and divide them by 10.  I have been unable so far to find a module in AML Studio that does that.<\/p>\n\n<p>Also, I should add that I'm having issues with my Python script in AML Studio, which is why I'm attempting to replicate all of my code using AML built-in modules.<\/p>",
        "Challenge_closed_time":1485350815923,
        "Challenge_comment_count":0,
        "Challenge_created_time":1484834877413,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1484835455796,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41743792",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":8.95,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":143.3162527778,
        "Challenge_title":"Is there an Azure Machine Learning Studio module that works like the Pandas 'mask' method?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":57,
        "Challenge_word_count":112,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483888458947,
        "Poster_location":null,
        "Poster_reputation_count":107.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>To my knowledge, there's no built-in module to do this succinctly (to my knowledge). If you prefer to use built-ins, you could:<\/p>\n\n<ol>\n<li>Use a Split Dataset module to split the entries based on credit\nscore<\/li>\n<li>Divide the credit score in large-credit-score rows by 10 using\nApply Math Operation<\/li>\n<li>Concatenate the two datasets row-wise with an Add Rows module<\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.6,
        "Solution_reading_time":4.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1370505440848,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":3.8717241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've created a docker image using AWS SageMaker and am now trying to push said image to ECR. When I do <code>docker push ${fullname}<\/code> it retries a couple of times and then errors.<\/p>\n<p>In CloudTrail I can see that I'm getting an access denied error with message:<\/p>\n<p>&quot;User: arn:aws:sts::xxxxxxxxxx:assumed-role\/AmazonSageMaker-ExecutionRole-xxxxxxxxxxxx\/SageMaker is not authorized to perform: ecr:InitiateLayerUpload on resource: arn:aws:ecr:us-east-x:xxxxxxxxxx:repository\/image because no identity-based policy allows the ecr:InitiateLayerUpload action&quot;<\/p>\n<p>I have full permissions, but from the error message above it thinks the user is SageMaker and not me.<\/p>\n<p>How do I change the user? I'm guessing that's the problem.<\/p>",
        "Challenge_closed_time":1658175998627,
        "Challenge_comment_count":2,
        "Challenge_created_time":1658162060420,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73025706",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":13.7,
        "Challenge_reading_time":10.61,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":3.8717241667,
        "Challenge_title":"AccessDenied error when pushing docker image from SageMaker to ECR",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":143,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1657050754840,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>When you're running commands from SageMaker, you're executing them as the SageMaker execution role, instead of your role. There are two options -<\/p>\n<ol>\n<li>[Straighforward solution] Add <em>ecr:InitiateLayerUpload<\/em> permissions to the <code>AmazonSageMaker-ExecutionRole-xxxxxxxxxxxx<\/code> role<\/li>\n<li>Assume a different role using sts (in that case, <code>AmazonSageMaker-ExecutionRole-xxxxxxxxxxxx<\/code> needs to have permissions to assume your Admin role) and then run <code>docker push<\/code> command.<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.5,
        "Solution_reading_time":6.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":59.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1564118772683,
        "Answerer_location":null,
        "Answerer_reputation_count":4730.0,
        "Answerer_view_count":167.0,
        "Challenge_adjusted_solved_time":156.0215711111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I wrote a simple Keras code, in which I use CNN for fashion mnist dataset. Everything works great. I implemented my own class and classification is OK.<\/p>\n<p>However, I wanted to use Optuna, as OptKeras (Optuna wrapper for Keras), you can see an example here: <a href=\"https:\/\/medium.com\/@Minyus86\/optkeras-112bcc34ec73\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@Minyus86\/optkeras-112bcc34ec73<\/a>.<\/p>\n<p>However, something is wrong with my code. When I try to use optKeras inside my own class. Here's the code: (ordinary <code>run<\/code> method works, but <code>optuna_run<\/code> gives an error: <code>AttributeError: type object 'FrozenTrial' has no attribute '_field_types'<\/code>.<\/p>\n<pre><code>! pip install optkeras\n# -*- coding: utf-8 -*- \n#!\/usr\/bin\/env python3\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN \nfrom keras.callbacks import ModelCheckpoint\nfrom keras import backend as K\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics\n\nimport optuna\nfrom optkeras.optkeras import OptKeras\n\nimport sys\nimport math\nimport numpy\nimport scipy.io as sio   \nimport matplotlib.pyplot as plt\n\nclass OptunaTest():\n\n  def __init__(self):\n    self.fashion_mnist = keras.datasets.fashion_mnist\n    (self.train_images, self.train_labels), (self.test_images, self.test_labels) = self.fashion_mnist.load_data()\n    self.class_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n    self.train_images = self.train_images \/ 255.0\n    self.test_images = self.test_images \/ 255.0\n    self.model = None \n    self.study_name = 'FashionMnist' + '_Simple'\n    self.ok = OptKeras(study_name=self.study_name)\n\n  def run(self):\n    self.model = keras.Sequential()\n    self.model.add(keras.layers.Flatten(input_shape=(28, 28)))\n    self.model.add(keras.layers.Dense(128, activation='relu'))\n    self.model.add(keras.layers.Dense(10))\n    self.model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n    self.model.fit(self.train_images, self.train_labels, epochs=5)\n    test_loss, test_acc = self.model.evaluate(self.test_images, self.test_labels, verbose=0)\n    predictions = self.model.predict(self.test_images)\n\n    INDEX = 10\n    print(&quot;\\nPREDICTION: &quot; + str(predictions[INDEX]))\n    print(&quot;\\nMAX PREDICTION VAL: &quot; + str(numpy.argmax(predictions[INDEX])))\n    print(&quot;\\nLABEL: &quot; + str(self.test_labels[INDEX]))\n\n  def optuna_run(self, trial):\n    K.clear_session() \n    \n    self.model = keras.Sequential()\n    self.model.add(keras.layers.Flatten(input_shape=(28, 28)))\n    self.model.add(keras.layers.Dense(units = trial.suggest_categorical('units', [32, 64, 128]), activation = trial.suggest_categorical('activation', ['relu', 'linear'])))\n    self.model.add(keras.layers.Dense(units = trial.suggest_categorical('units', [32, 64, 128]), activation = trial.suggest_categorical('activation', ['relu', 'linear'])))\n\n    self.model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n    self.model.fit(self.train_images, self.train_labels, epochs=5, callbacks = self.ok.callbacks(trial), verbose = self.ok.keras_verbose)\n    test_loss, test_acc = self.model.evaluate(self.test_images, self.test_labels, verbose=0)\n    predictions = self.model.predict(self.test_images)\n    print(ok.trial_best_value)\n    \n    INDEX = 10\n    print(&quot;\\nPREDICTION: &quot; + str(predictions[INDEX]))\n    print(&quot;\\nMAX PREDICTION VAL: &quot; + str(numpy.argmax(predictions[INDEX])))\n    print(&quot;\\nLABEL: &quot; + str(self.test_labels[INDEX]))\n\n\nif __name__ == &quot;__main__&quot;:\n  ot = OptunaTest()\n  ot.run()\n\n  ot.ok.optimize(ot.optuna_run,  timeout = 60)\n<\/code><\/pre>\n<p>A code can also be found here: <a href=\"https:\/\/colab.research.google.com\/drive\/1uibWa80BdjatA5Kcw27eMUsS7SmwxaDk?usp=sharing\" rel=\"nofollow noreferrer\">https:\/\/colab.research.google.com\/drive\/1uibWa80BdjatA5Kcw27eMUsS7SmwxaDk?usp=sharing<\/a>.<\/p>\n<p>The full error message:<\/p>\n<pre><code>[W 2020-06-30 11:09:26,959] Setting status of trial#0 as TrialState.FAIL because of the following error: AttributeError(&quot;type object 'FrozenTrial' has no attribute '_field_types'&quot;,)\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py&quot;, line 230, in synch_with_optuna\n    self.best_trial = self.study.best_trial\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optuna\/study.py&quot;, line 97, in best_trial\n    return copy.deepcopy(self._storage.get_best_trial(self._study_id))\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optuna\/storages\/in_memory.py&quot;, line 293, in get_best_trial\n    raise ValueError(&quot;No trials are completed yet.&quot;)\nValueError: No trials are completed yet.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optuna\/study.py&quot;, line 734, in _run_trial\n    result = func(trial)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py&quot;, line 130, in fun_tf\n    return fun(trial)\n  File &quot;&lt;ipython-input-11-45495c9f2ae9&gt;&quot;, line 65, in optima_run\n    self.model.fit(self.train_images, self.train_labels, epochs=10, callbacks = self.ok.callbacks(trial), verbose = self.ok.keras_verbose)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py&quot;, line 172, in callbacks\n    self.synch_with_optuna()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py&quot;, line 232, in synch_with_optuna\n    self.best_trial = get_trial_default()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py&quot;, line 367, in get_trial_default\n    num_fields = optuna.structs.FrozenTrial._field_types.__len__()\nAttributeError: type object 'FrozenTrial' has no attribute '_field_types'\n\n---------------------------------------------------------------------------\n\nValueError                                Traceback (most recent call last)\n\n\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py in synch_with_optuna(self)\n    229         try:\n--&gt; 230             self.best_trial = self.study.best_trial\n    231         except:\n\n12 frames\n\nValueError: No trials are completed yet.\n\n\nDuring handling of the above exception, another exception occurred:\n\nAttributeError                            Traceback (most recent call last)\n\n\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py in get_trial_default()\n    365 \n    366 def get_trial_default():\n--&gt; 367     num_fields = optuna.structs.FrozenTrial._field_types.__len__()\n    368     assert num_fields in (10, 11, 12)\n    369     if num_fields == 12: # possible future version\n\nAttributeError: type object 'FrozenTrial' has no attribute '_field_types'\n<\/code><\/pre>",
        "Challenge_closed_time":1593917427550,
        "Challenge_comment_count":1,
        "Challenge_created_time":1593516495007,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62656411",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":17.9,
        "Challenge_reading_time":93.74,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":75,
        "Challenge_solved_time":111.3701508334,
        "Challenge_title":"OptKeras (Keras Optuna Wrapper) - use optkeras inside my own class, AttributeError: type object 'FrozenTrial' has no attribute '_field_types'",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":888,
        "Challenge_word_count":541,
        "Platform":"Stack Overflow",
        "Poster_created_time":1344355535128,
        "Poster_location":null,
        "Poster_reputation_count":3580.0,
        "Poster_view_count":962.0,
        "Solution_body":"<p>It seems that optkeras (version I got was 0.0.7) being not quite up-to-date with optuna library is the reason for the issue. I was able to make it work with optuna 1.5.0 by doing the following changes:<\/p>\n<p>First, you'll need to monkey-patch <code>get_default_trial<\/code> like this before running your code:<\/p>\n<pre><code>import optkeras\noptkeras.optkeras.get_trial_default = lambda: optuna.trial.FrozenTrial(\n            None, None, None, None, None, None, None, None, None, None, None)\n<\/code><\/pre>\n<p>After doing so I'm getting an error with <code>Callback<\/code> saying:<\/p>\n<pre><code>AttributeError: 'OptKeras' object has no attribute '_implements_train_batch_hooks'\n<\/code><\/pre>\n<p>To solve this you'll have to manually edit optkeras.py, but not too much - just add <code>tensorflow.<\/code> to first two lines imports, i.e. make them:<\/p>\n<pre><code>import tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import Callback, CSVLogger, ModelCheckpoint\n<\/code><\/pre>\n<p>instead of:<\/p>\n<pre><code>import keras.backend as K\nfrom keras.callbacks import Callback, CSVLogger, ModelCheckpoint\n<\/code><\/pre>\n<p>If you can't change the code after installation it might be a bit of a problem - I would probably just recommend to copy full code of optkeras library (it's just one file optkeras.py) and use fixed version of that in your script or something like that. Unfortunately I don't see a nice way of monkey-patching this import issue. That said I think it can be fairly easy to either change that on-fly even from python (i.e. change <code>optkeras.py<\/code> lines from within python before importing optkeras) or copying the optkeras.py (also from withing python script) + replacing the strings on fly, then importing from the new location.<\/p>\n<p>After that is done I just had to:<\/p>\n<ul>\n<li>fix typo in your code (<code>print(ok.trial_best_value)<\/code> should really be <code>print(self.ok.trial_best_value)<\/code>)<\/li>\n<li>add <code>validation_split=0.1<\/code> to <code>self.model.fit<\/code> call (or you may use something else for your tuning - just with existing code example callback won't get <code>val_loss<\/code> value because there is no validation set and optkeras is using <code>val_loss<\/code> by default - see <code>monitor<\/code> argument for <code>OptKeras<\/code> constructor). My guess would be that you probably will either want to create a fixed validation set instead or monitor training loss <code>loss<\/code> instead of <code>val_loss<\/code>.<\/li>\n<li>add <code>return test_loss<\/code> at the end of <code>optuna_run<\/code> method.<\/li>\n<\/ul>\n<p>After all of these changes everything seems be working.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":1594078172663,
        "Solution_link_count":0.0,
        "Solution_readability":9.6,
        "Solution_reading_time":33.73,
        "Solution_score_count":3.0,
        "Solution_sentence_count":26.0,
        "Solution_word_count":343.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1562055808543,
        "Answerer_location":null,
        "Answerer_reputation_count":895.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":3027.8199811111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to deploy a TF2.0 model to SageMaker. So far, I managed to train the model and save it into an S3 bucket but when I'm calling the <code>.deploy()<\/code> method, I get the following error from cloud Watch <\/p>\n\n<p><code>ValueError: no SavedModel bundles found!<\/code><\/p>\n\n<p>Here is my training script: <\/p>\n\n<pre><code>### Code to add in a tensorflow_estimator.py file\n\nimport argparse\nimport os\nimport pathlib\nimport tensorflow as tf\n\n\nif __name__ == '__main__':\n\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--learning_rate', type=float, default=0.1)\n\n    # Data, model, and output directories\n    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n    args, _ = parser.parse_known_args()\n\n    print(\"##### ARGS ##### \\n{}\".format(args))\n\n    # Get files \n    path = pathlib.Path(args.train)\n\n    # Print out folder content \n    for item in path.iterdir():\n        print(\"##### DIRECTORIES ##### \\n{}\".format(item))\n\n    # Get all images \n    all_images = list(path.glob(\"*\/*\"))\n    all_image_paths = [str(path) for path in list(path.glob(\"*\/*\"))]\n\n    # Transform images into tensors\n    def preprocess_and_load_images(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, [192, 192])\n        return image\n\n    # Apply preprocessing function\n    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    ds_images = ds_paths.map(preprocess_and_load_images)\n\n    # Map Labels\n    labels = []\n    for data in path.iterdir():  \n        if data.is_dir():               \n            labels += [data.name]    \n\n    labels_index = {}\n    for i,label in enumerate(labels):\n        labels_index[label]=i\n\n    print(\"##### Label Index ##### \\n{}\".format(labels_index))\n\n    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(\"*\/*\"))]\n\n    # Create a tf Dataset\n    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n\n    # Zip train and labeled dataset\n    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))\n\n    # Shuffle Dataset and batch it \n    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)\n\n    # Create a pre-trained model \n    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), \n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )\n\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(labels), activation=\"softmax\")\n            ])\n\n    initial_learning_rate = args.learning_rate\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=1000,\n        decay_rate=0.96,\n        staircase=True) \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n\n    # Train the model\n    model.fit(full_ds, epochs=args.epochs)\n\n    # Save the model \n    model.save(os.path.join(args.model_dir, \"tf_model\"), save_format=\"tf\")\n\ndef model_fn(model_dir):\n    classifier = tf.keras.models.load_model(os.path.join(model_dir, \"tf_model\"))\n    return classifier\n<\/code><\/pre>\n\n<p>And here is the code that I wrote into Colab <\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='tensorflow_estimator.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='ml.m5.large',\n                          framework_version='2.0.0', \n                          sagemaker_session=sagemaker_session,\n                          output_path=s3_output_location,\n                          hyperparameters={'epochs': 1,\n                                           'batch_size': 30,\n                                           'learning_rate': 0.001},\n                          py_version='py3')\n\n\ntf_estimator.fit({\"train\":train_data})\n\nfrom sagemaker.tensorflow.serving import Model\n\nmodel = Model(model_data='s3:\/\/path\/to\/model.tar.gz', \n              role=role,\n              framework_version=\"2.0.0\",\n              sagemaker_session=sagemaker_session)\n\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.large')\n<\/code><\/pre>\n\n<p>I already tried to look at <a href=\"https:\/\/stackoverflow.com\/questions\/57172147\/no-savedmodel-bundles-found-on-tensorflow-hub-model-deployment-to-aws-sagemak\">this thread<\/a> but I actually don't have the problem of versions in my tar.gz file as the structure is the following : <\/p>\n\n<pre><code>\u251c\u2500\u2500 assets\n\u251c\u2500\u2500 saved_model.pb\n\u2514\u2500\u2500 variables\n    \u251c\u2500\u2500 variables.data-00000-of-00001\n    \u2514\u2500\u2500 variables.index\n<\/code><\/pre>\n\n<p>I feel I might be wrong when defining <code>model_fn()<\/code> in my training script but definitely don't what to replace that with. Would you guys have an idea? <\/p>\n\n<p>Thanks a lot for your help!  <\/p>",
        "Challenge_closed_time":1579799847967,
        "Challenge_comment_count":1,
        "Challenge_created_time":1579796394240,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59882941",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":16.2,
        "Challenge_reading_time":64.69,
        "Challenge_score_count":4,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":68,
        "Challenge_solved_time":0.9593686111,
        "Challenge_title":"ValueError: no SavedModel bundles found! when trying to deploy a TF2.0 model to SageMaker",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":943,
        "Challenge_word_count":395,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562055808543,
        "Poster_location":null,
        "Poster_reputation_count":895.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>I actually tried to modify my training script to the following : <\/p>\n\n<pre><code>### Code to add in a tensorflow_estimator.py file\n\nimport argparse\nimport os\nimport pathlib\nimport tensorflow as tf\n\n\nif __name__ == '__main__':\n\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--learning_rate', type=float, default=0.1)\n\n    # Data, model, and output directories\n    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n    args, _ = parser.parse_known_args()\n\n    print(\"##### ARGS ##### \\n{}\".format(args))\n\n    # Get files \n    path = pathlib.Path(args.train)\n\n    # Print out folder content \n    for item in path.iterdir():\n        print(\"##### DIRECTORIES ##### \\n{}\".format(item))\n\n    # Get all images \n    all_images = list(path.glob(\"*\/*\"))\n    all_image_paths = [str(path) for path in list(path.glob(\"*\/*\"))]\n\n    # Transform images into tensors\n    def preprocess_and_load_images(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, [192, 192])\n        return image\n\n    # Apply preprocessing function\n    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    ds_images = ds_paths.map(preprocess_and_load_images)\n\n    # Map Labels\n    labels = []\n    for data in path.iterdir():  \n        if data.is_dir():               \n            labels += [data.name]    \n\n    labels_index = {}\n    for i,label in enumerate(labels):\n        labels_index[label]=i\n\n    print(\"##### Label Index ##### \\n{}\".format(labels_index))\n\n    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(\"*\/*\"))]\n\n    # Create a tf Dataset\n    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n\n    # Zip train and labeled dataset\n    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))\n\n    # Shuffle Dataset and batch it \n    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)\n\n    # Create a pre-trained model \n    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), \n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )\n\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(labels), activation=\"softmax\")\n            ])\n\n    initial_learning_rate = args.learning_rate\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=1000,\n        decay_rate=0.96,\n        staircase=True) \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n\n    # Train the model\n    model.fit(full_ds, epochs=args.epochs)\n\n    # Save the model \n    model.save(os.path.join(args.model_dir, \"tensorflow_model\/1\"), save_format=\"tf\")\n\n<\/code><\/pre>\n\n<p>It seems that it's important to have a numerical name for your folder:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Save the model\nmodel.save(os.path.join(args.model_dir, \"tensorflow_model\/1\"), save_format=\"tf\")\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1590696546172,
        "Solution_link_count":0.0,
        "Solution_readability":18.3,
        "Solution_reading_time":44.06,
        "Solution_score_count":4.0,
        "Solution_sentence_count":46.0,
        "Solution_word_count":244.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1492048364132,
        "Answerer_location":"Cambridge, MA, USA",
        "Answerer_reputation_count":4436.0,
        "Answerer_view_count":713.0,
        "Challenge_adjusted_solved_time":0.2273408334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created an endpoint on us-east-1. try to create a predictor:<\/p>\n\n<pre><code>In [106]: sagemaker.predictor.RealTimePredictor(&lt;endpoint name&gt;)\n<\/code><\/pre>\n\n<p>and get<\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the DescribeEndpoint operation: \nCould not find endpoint \"arn:aws:sagemaker:us-east-2:&lt;account number&gt;:endpoint\/&lt;endpoint name&gt;\".\n<\/code><\/pre>\n\n<p>which is perfectly correct, since the endpoint is on us-east-1.  Probably I could change some defaults, but I'd rather not - I work on us-east-2 99% of the time.<\/p>\n\n<p>So, how can I set a different region when initializing the predictor?<\/p>",
        "Challenge_closed_time":1573497881147,
        "Challenge_comment_count":1,
        "Challenge_created_time":1573497062720,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58806807",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.6,
        "Challenge_reading_time":9.31,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.2273408334,
        "Challenge_title":"Create a predictor from an endpoint in a different region",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":478,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553808322940,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>The (python) <code>Predictors<\/code> <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"nofollow noreferrer\">documentation<\/a> shows that you can pass a <code>Session<\/code> object. In turn, the <code>Session<\/code> can be <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.Session\" rel=\"nofollow noreferrer\">initialized<\/a> with a <em>client<\/em> and a <em>runtime client<\/em> - the former does everything except endpoint invocations, the latter does... endpoint invocations.<\/p>\n\n<p>Those clients are tied to specific regions. It seems like you should be able to set the runtime client region to match your endpoint, by manually instantiating it, while leaving the regular client alone (disclaimer here: I haven't tried this - if you do, let me\/us know how it goes :)).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.1,
        "Solution_reading_time":10.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":94.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1291793452900,
        "Answerer_location":"Berkeley, CA, United States",
        "Answerer_reputation_count":752.0,
        "Answerer_view_count":120.0,
        "Challenge_adjusted_solved_time":3886.8446766667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Short form:\nI am trying to figure out how can I run the hyperparam within a <strong>training step<\/strong> (i.e. train_step = PythonScriptStep(...)) in the pipeline, I am not sure where shall I put the &quot;config=hyperdrive&quot;<\/p>\n<p>Long form:<\/p>\n<p>General:<\/p>\n<pre><code># Register the environment \ndiabetes_env.register(workspace=ws)\nregistered_env = Environment.get(ws, 'diabetes-pipeline-env')\n\n# Create a new runconfig object for the pipeline\nrun_config = RunConfiguration()\n\n# Use the compute you created above. \nrun_config.target = ComputerTarget_Crea\n\n# Assign the environment to the run configuration\nrun_config.environment = registered_env\n<\/code><\/pre>\n<p>Hyperparam:<\/p>\n<pre><code>script_config = ScriptRunConfig(source_directory=experiment_folder,\n                                script='diabetes_training.py',\n                                # Add non-hyperparameter arguments -in this case, the training dataset\n                                arguments = ['--input-data', diabetes_ds.as_named_input('training_data')],\n                                environment=sklearn_env,\n                                compute_target = training_cluster)\n\n# Sample a range of parameter values\nparams = GridParameterSampling(\n    {\n        # Hyperdrive will try 6 combinations, adding these as script arguments\n        '--learning_rate': choice(0.01, 0.1, 1.0),\n        '--n_estimators' : choice(10, 100)\n    }\n)\n\n# Configure hyperdrive settings\nhyperdrive = HyperDriveConfig(run_config=script_config, \n                          hyperparameter_sampling=params, \n                          policy=None, # No early stopping policy\n                          primary_metric_name='AUC', # Find the highest AUC metric\n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                          max_total_runs=6, # Restict the experiment to 6 iterations\n                          max_concurrent_runs=2) # Run up to 2 iterations in parallel\n\n# Run the experiment if I only want to run hyperparam alone without the pipeline\n#experiment = Experiment(workspace=ws, name='mslearn-diabetes-hyperdrive')\n#run = experiment.submit(**config=hyperdrive**)\n<\/code><\/pre>\n<p>PipeLine:<\/p>\n<pre><code>prep_step = PythonScriptStep(name = &quot;Prepare Data&quot;,\n                                source_directory = experiment_folder,\n                                script_name = &quot;prep_diabetes.py&quot;,\n                                arguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n                                             '--prepped-data', prepped_data_folder],\n                                outputs=[prepped_data_folder],\n                                compute_target = ComputerTarget_Crea,\n                                runconfig = run_config,\n                                allow_reuse = True)\n\n# Step 2, run the training script\ntrain_step = PythonScriptStep(name = &quot;Train and Register Model&quot;,\n                                source_directory = experiment_folder,\n                                script_name = &quot;train_diabetes.py&quot;,\n                                arguments = ['--training-folder', prepped_data_folder],\n                                inputs=[prepped_data_folder],\n                                compute_target = ComputerTarget_Crea,\n                                runconfig = run_config,\n                                allow_reuse = True)\n# Construct the pipeline\npipeline_steps = [prep_step, train_step]\npipeline = Pipeline(workspace=ws, steps=pipeline_steps)\nprint(&quot;Pipeline is built.&quot;)\n\n# Create an experiment and run the pipeline\n**#How do I need to change these below lines to use hyperdrive????**\nexperiment = Experiment(workspace=ws, name = 'mslearn-diabetes-pipeline')\npipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n<\/code><\/pre>\n<p>Not sure where I need to put <strong>config=hyperdrive<\/strong> in the Pipeline section?<\/p>",
        "Challenge_closed_time":1633922581063,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619929277080,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1619929940227,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67352949",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":20.0,
        "Challenge_reading_time":41.98,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":3887.0288841667,
        "Challenge_title":"How to combine pipeline and hyperparameter in AzureML SDK in the training step",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":276,
        "Challenge_word_count":280,
        "Platform":"Stack Overflow",
        "Poster_created_time":1375186444008,
        "Poster_location":"Auckland, New Zealand",
        "Poster_reputation_count":912.0,
        "Poster_view_count":288.0,
        "Solution_body":"<p>here's how to combine hyperparameters with an AML pipeline: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.hyperdrivestep?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.hyperdrivestep?view=azure-ml-py<\/a><\/p>\n<p>Alternatively, here's a sample notebook: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":57.4,
        "Solution_reading_time":11.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1435766573232,
        "Answerer_location":null,
        "Answerer_reputation_count":10645.0,
        "Answerer_view_count":1173.0,
        "Challenge_adjusted_solved_time":37311.0782480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am sort of new to Python, so I probably don't understand fully how to exactly import the libraries correctly into Azure ML.<\/p>\n\n<p>I have a bunch of data stored in Table storage which I have local Python code to successfully join all of them as a preparation for the ML experiment. I learned that AzureML environment does not have the Azure-Storage libraries installed, and therefore procceded the steps according <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn955437.aspx\" rel=\"nofollow noreferrer\">this<\/a> to upload a ZIP file containing the Azure-storage libraries that I found under anaconda3\\lib\\site-packages. I took all of the azure directories and shoved them under one single zip file and followed the bottom of the document in the link to upload the zip file as a DataSet and attach the dataset to an Execute Python script node in ML.<\/p>\n\n<p>I am getting errors like this when I try to run the node:<\/p>\n\n<pre><code>requestId = 825883c7ccb74f7e869e68e60d3cd919 errorComponent=Module. taskStatusCode=400. e \"C:\\pyhome\\lib\\ssl.py\", line 856, in send return self._sslobj.write(data) File \"C:\\pyhome\\lib\\ssl.py\", line 581, in write return self._sslobj.write(data)socket.timeout: The write operation timed outDuring handling of the above exception, another exception occurred:Traceback (most recent call last): File \"C:\\pyhome\\lib\\site-packages\\requests\\adapters.py\", line 376, in send timeout=timeout File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 609, in urlopen _stacktrace=sys.exc_info()[2]) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\util\\retry.py\", line 247, in increment raise six.reraise(type(error), error, _stacktrace) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\packages\\six.py\", line 309, in reraise raise value.with_traceback(tb) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 559, in urlopen body=body, headers=headers) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 353, in _make_request conn.request(method, url, **httplib_request_kw) File \"C:\\pyhome\\lib\\http\\client.py\", line 1083, in request self._send_request(method, url, body, headers) File \"C:\\pyhome\\lib\\http\\client.py\", line 1128, in _send_request self.endheaders(body) File \"C:\\pyhome\\lib\\http\\client.py\", line 1079, in endheaders self._send_output(message_body) File \"C:\\pyhome\\lib\\http\\client.py\", line 911, in _send_output self.send(msg) File \"C:\\pyhome\\lib\\http\\client.py\", line 885, in send self.sock.sendall(data) File \"C:\\pyhome\\lib\\ssl.py\", line 886, in sendall v = self.send(data[count:]) File \"C:\\pyhome\\lib\\ssl.py\", line 856, in send return self._sslobj.write(data) File \"C:\\pyhome\\lib\\ssl.py\", line 581, in write return self._sslobj.write(data)requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', timeout('The write operation timed out',))During handling of the above exception, another exception occurred:Traceback (most recent call last): File \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 221, in _perform_request response = self._httpclient.perform_request(request) File \"c:\\temp\\script bundle\\azure\\storage\\_http\\httpclient.py\", line 114, in perform_request proxies=self.proxies) File \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 468, in request resp = self.send(prep, **send_kwargs) File \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 576, in send r = adapter.send(request, **kwargs) File \"C:\\pyhome\\lib\\site-packages\\requests\\adapters.py\", line 426, in send raise ConnectionError(err, request=request)requests.exceptions.ConnectionError: ('Connection aborted.', timeout('The write operation timed out',))During handling of the above exception, another exception occurred:Traceback (most recent call last): File \"C:\\server\\invokepy.py\", line 199, in batch odfs = mod.azureml_main(*idfs) File \"C:\\temp\\fa22884a19884f658d411dc0bdf05715.py\", line 33, in azureml_main data = table_service.query_entities(table_name) File \"c:\\temp\\script bundle\\azure\\storage\\table\\tableservice.py\", line 728, in query_entities resp = self._query_entities(*args, **kwargs) File \"c:\\temp\\script bundle\\azure\\storage\\table\\tableservice.py\", line 795, in _query_entities operation_context=_context) File \"c:\\temp\\script bundle\\azure\\storage\\table\\tableservice.py\", line 1093, in _perform_request return super(TableService, self)._perform_request(request, parser, parser_args, operation_context) File \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 279, in _perform_request raise ex File \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 251, in _perform_request raise AzureException(ex.args[0])azure.common.AzureException: ('Connection aborted.', timeout('The write operation timed out',))Process returned with non-zero exit code \n<\/code><\/pre>\n\n<p>I am not sure what I am doing wrong<\/p>",
        "Challenge_closed_time":1511661620620,
        "Challenge_comment_count":0,
        "Challenge_created_time":1511629802230,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47488544",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":65.99,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":62,
        "Challenge_solved_time":8.8384416667,
        "Challenge_title":"Using Azure Storage libraries in AzureML - Custom python library",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":150,
        "Challenge_word_count":471,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340380852680,
        "Poster_location":null,
        "Poster_reputation_count":73.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>In Azure ML Studio, Python scripts (and R scripts for that matter) run in a sandbox so cannot access resources over the network. See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts?#limitations\" rel=\"nofollow noreferrer\">limitations<\/a>:<\/p>\n<blockquote>\n<p>The Execute Python Script currently has the following limitations:<\/p>\n<ol>\n<li>Sandboxed execution. The Python runtime is currently sandboxed and, as\na result, does not allow access to the network...<\/li>\n<\/ol>\n<\/blockquote>\n<p>So if you want to read from a blob use the separate Import Data module and if you want to write to a blob use the separate Export Data module.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1645949683923,
        "Solution_link_count":1.0,
        "Solution_readability":11.5,
        "Solution_reading_time":8.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1552934828727,
        "Answerer_location":null,
        "Answerer_reputation_count":254.0,
        "Answerer_view_count":62.0,
        "Challenge_adjusted_solved_time":0.5847786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a object detection code in Aws. Although opencv is listed in the requirement file, i have the error &quot;no module named cv2&quot;. I am not sure how to fix this error. could someone help me please.<\/p>\n<p>My requirement.txt file has<\/p>\n<ul>\n<li>opencv-python<\/li>\n<li>numpy&gt;=1.18.2<\/li>\n<li>scipy&gt;=1.4.1<\/li>\n<li>wget&gt;=3.2<\/li>\n<li>tensorflow==2.3.1<\/li>\n<li>tensorflow-gpu==2.3.1<\/li>\n<li>tqdm==4.43.0<\/li>\n<li>pandas<\/li>\n<li>boto3<\/li>\n<li>awscli<\/li>\n<li>urllib3<\/li>\n<li>mss<\/li>\n<\/ul>\n<p>I tried installing &quot;imgaug&quot; and &quot;opencv-python headless&quot; as well.. but still not able to get rid of this error.<\/p>\n<pre><code>sh-4.2$ python train_launch.py \n[INFO-ROLE] arn:aws:iam::021945294007:role\/service-role\/AmazonSageMaker-ExecutionRole-20200225T145269\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_count has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\n2021-04-14 13:29:58 Starting - Starting the training job...\n2021-04-14 13:30:03 Starting - Launching requested ML instances......\n2021-04-14 13:31:11 Starting - Preparing the instances for training......\n2021-04-14 13:32:17 Downloading - Downloading input data...\n2021-04-14 13:32:41 Training - Downloading the training image..WARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n2021-04-14 13:33:03,970 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n2021-04-14 13:33:05,030 sagemaker-containers INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {\n        &quot;training&quot;: &quot;\/opt\/ml\/input\/data\/training&quot;\n    },\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;unfreezed_epochs&quot;: 2,\n        &quot;freezed_batch_size&quot;: 8,\n        &quot;freezed_epochs&quot;: 1,\n        &quot;unfreezed_batch_size&quot;: 8,\n        &quot;model_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {\n        &quot;training&quot;: {\n            &quot;TrainingInputMode&quot;: &quot;File&quot;,\n            &quot;S3DistributionType&quot;: &quot;FullyReplicated&quot;,\n            &quot;RecordWrapperType&quot;: &quot;None&quot;\n        }\n    },\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;yolov4-2021-04-14-15-29&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;train_indu&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 8,\n    &quot;num_gpus&quot;: 1,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;train_indu.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2}\nSM_USER_ENTRY_POINT=train_indu.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[&quot;training&quot;]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=train_indu\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=8\nSM_NUM_GPUS=1\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{&quot;training&quot;:&quot;\/opt\/ml\/input\/data\/training&quot;},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;yolov4-2021-04-14-15-29&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;train_indu&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:8,&quot;num_gpus&quot;:1,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;train_indu.py&quot;}\nSM_USER_ARGS=[&quot;--freezed_batch_size&quot;,&quot;8&quot;,&quot;--freezed_epochs&quot;,&quot;1&quot;,&quot;--model_dir&quot;,&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;--unfreezed_batch_size&quot;,&quot;8&quot;,&quot;--unfreezed_epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_CHANNEL_TRAINING=\/opt\/ml\/input\/data\/training\nSM_HP_UNFREEZED_EPOCHS=2\nSM_HP_FREEZED_BATCH_SIZE=8\nSM_HP_FREEZED_EPOCHS=1\nSM_HP_UNFREEZED_BATCH_SIZE=8\nSM_HP_MODEL_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\n\nInvoking script with the following command:\n\n\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2\n\n\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n[name: &quot;\/device:CPU:0&quot;\ndevice_type: &quot;CPU&quot;\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 4667030854237447206\n, name: &quot;\/device:XLA_CPU:0&quot;\ndevice_type: &quot;XLA_CPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 3059419181456814147\nphysical_device_desc: &quot;device: XLA_CPU device&quot;\n, name: &quot;\/device:XLA_GPU:0&quot;\ndevice_type: &quot;XLA_GPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 6024475084695919958\nphysical_device_desc: &quot;device: XLA_GPU device&quot;\n, name: &quot;\/device:GPU:0&quot;\ndevice_type: &quot;GPU&quot;\nmemory_limit: 14949928141\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 13034103301168381073\nphysical_device_desc: &quot;device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5&quot;\n]\nTraceback (most recent call last):\n  File &quot;train_indu.py&quot;, line 12, in &lt;module&gt;\n    from yolov3.dataset import Dataset\n  File &quot;\/opt\/ml\/code\/yolov3\/dataset.py&quot;, line 3, in &lt;module&gt;\n    import cv2\nModuleNotFoundError: No module named 'cv2'\n2021-04-14 13:33:08,453 sagemaker-containers ERROR    ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n\n2021-04-14 13:33:11 Uploading - Uploading generated training model\n2021-04-14 13:33:54 Failed - Training job failed\nTraceback (most recent call last):\n  File &quot;train_launch.py&quot;, line 41, in &lt;module&gt;\n    estimator.fit(s3_data_path, logs=True, job_name=job_name) #the argument logs is crucial if you want to see what happends\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 535, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 1210, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 3365, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 2957, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job yolov4-2021-04-14-15-29: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n<\/code><\/pre>",
        "Challenge_closed_time":1618410091900,
        "Challenge_comment_count":1,
        "Challenge_created_time":1618407986697,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67093041",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":30.4,
        "Challenge_reading_time":154.58,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":56,
        "Challenge_solved_time":0.5847786111,
        "Challenge_title":"Aws Sagemaker - ModuleNotFoundError: No module named 'cv2'",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1218,
        "Challenge_word_count":510,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558009697176,
        "Poster_location":"Cergy-Pontoise, Cergy, France",
        "Poster_reputation_count":53.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Make sure your estimator has<\/p>\n<ul>\n<li>framework_version = '2.3',<\/li>\n<li>py_version = 'py37',<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.3,
        "Solution_reading_time":1.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1357753352563,
        "Answerer_location":"Hamburg, Germany",
        "Answerer_reputation_count":131.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":58.5243405556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Why do I need Container for AWS SageMaker? If I want to run Scikit Learn on SageMaker's Jupyter notebook for self learning purposes, do I still need to configure Container for it?<\/p>\n\n<p>What is the minimum configuration on SageMaker I will need if I just want to learn Scikit Learn? For example, I want to run Scikit Learn's Decision Tree algorithm with a set of training data and a set of test data. What do I need to do on SageMaker to perform the tasks? Thanks.<\/p>",
        "Challenge_closed_time":1526309522536,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526098834910,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":1531211592336,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50302810",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.17,
        "Challenge_score_count":9,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":58.5243405556,
        "Challenge_title":"AWS SageMaker Minimum Configuration",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":849,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Poster_created_time":1461112434223,
        "Poster_location":"San Jose, CA, United States",
        "Poster_reputation_count":1075.0,
        "Poster_view_count":181.0,
        "Solution_body":"<p>You don't need much. Just an AWS Account with the correlated permissions on your role.\nInside the AWS SageMaker Console you can just run an AWS Notebook Instance with one click. There is Sklearn preinstalled and you can use it out of the box. No special container needed.<\/p>\n\n<p>As minimum you just need your AWS Account with the correlated permissions to create EC2 Instances and read \/ write from your S3. Thats all, just try it. :)<\/p>\n\n<p>Use this as a starting point: <a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/sagemaker\/\" rel=\"noreferrer\">Amazon SageMaker \u2013 Accelerating Machine Learning<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/98gRb.png\" rel=\"noreferrer\">You can also access it via the Jupyter Terminal<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.2,
        "Solution_reading_time":9.15,
        "Solution_score_count":5.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":100.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1579536819712,
        "Answerer_location":null,
        "Answerer_reputation_count":18.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":4.6947408334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I installed pyarrow using this command &quot;conda install pyarrow&quot;.\nI am running a sagemaker notebook and I am getting the error no module named pyarrow.\nI have python 3.8.3 installed on mac.<\/p>\n<p>I have numpy  1.18.5 , pandas 1.0.5 and pyarrow  0.15.1<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1604344726272,
        "Challenge_comment_count":1,
        "Challenge_created_time":1604343780117,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1604344440696,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64651724",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.1,
        "Challenge_reading_time":3.79,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.2628208333,
        "Challenge_title":"No Module named pyarrow",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":540,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1468599558956,
        "Poster_location":"San Francisco, CA, USA",
        "Poster_reputation_count":107.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>I have not yet used AWS Sagemaker notebooks, but they may be similar to GCP 'AI Platform notebooks', which I have used quite extensively. Additionally, if you're experiencing additional problems, could you describe how you're launching the notebooks (whether from command line or from GUI)?<\/p>\n<p>In GCP, I defaulted to using <code>pip install<\/code> for my packages, as the conda environments were a bit finicky and didn't provide much support when creating notebooks sourced from my own created conda environments.<\/p>\n<p>Assuming you're installing conda into your base directory, when you launch jupyter notebooks, this should be the default conda environment, else if you installed to a separate conda environment, you should be able to change this within jupyter notebooks using the <code>CONDA<\/code> tab and selecting which notebook uses which conda environment.\n-Spencer<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1604361341763,
        "Solution_link_count":0.0,
        "Solution_readability":16.1,
        "Solution_reading_time":11.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":131.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1579718832727,
        "Answerer_location":null,
        "Answerer_reputation_count":149.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":152.0926025,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I can see experiment 2 is in deleted, but when it will be deleted actually?<\/p>\n\n<pre><code>2   test    hdfs:\/\/\/1234\/mlflow deleted\n<\/code><\/pre>\n\n<p>If the experiment is not deleted automatically, how can I delete it?<\/p>",
        "Challenge_closed_time":1579736829916,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579189296547,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59773167",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":3.64,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":152.0926025,
        "Challenge_title":"When will experiment be deleted with lifecycle_stage is set as deleted",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":412,
        "Challenge_word_count":42,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408370821672,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":2521.0,
        "Poster_view_count":197.0,
        "Solution_body":"<p>I am assuming you use sql store?<\/p>\n\n<p>Currently there is no way to tell mlflow to hard-delete experiments. We are working with open source contributors to add a cli command that would perform garbage-collection of deleted experiments. This should be added soon in one of the upcoming mlflow releases. In the meantime, you can connect to your sql store and delete the experiments manually.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":4.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":64.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":13.3298980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Usually when running an MLProject, I would use something similar to:<\/p>\n<pre><code>mlflow run . -P alpha=0.1 -P l1_ratio=0.9\n<\/code><\/pre>\n<p>Is it possible to pass a file containing the key\/value pairs instead ? so something like:<\/p>\n<pre><code>mlflow run . --file .\/parametrs\n<\/code><\/pre>\n<p>where .\/parameters contains the key\/value pairs (like an env file or something)<\/p>\n<p>One way I thought of is to make a seperate bash script that accept the file and extracts the key\/value pairs to be included in the run command, but I wonder if there's a way more native to mlflow.<\/p>",
        "Challenge_closed_time":1621931875960,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621883888327,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67677780",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":8.09,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":13.3298980556,
        "Challenge_title":"MLFlow run: Pass parameters in a file instead of key\/value pairs",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":222,
        "Challenge_word_count":99,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540654775052,
        "Poster_location":"Tunisia",
        "Poster_reputation_count":606.0,
        "Poster_view_count":69.0,
        "Solution_body":"<p>It's not supported functionality according to <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-run\" rel=\"nofollow noreferrer\">documentation<\/a>, and <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/cli.py#L124\" rel=\"nofollow noreferrer\">source code<\/a>, so you'll need to add your own wrapper to read parameters from file &amp; pass them explicitly.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.7,
        "Solution_reading_time":5.04,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1663710134840,
        "Answerer_location":null,
        "Answerer_reputation_count":16.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":231.2612944445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Does  <strong>Amazon SageMaker built-in LightGBM<\/strong> algorithm support <strong>distributed training<\/strong>?<\/p>\n<p>I use Databricks for distributed training of LightGBM today. If SageMaker built-in LightGBM supports distributed training, I would consider migrating to SageMaker. It is not clear in the Amazon SageMaker's built-in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">LightGBM<\/a>'s documentation on whether it supports distributed training.<\/p>\n<p>Thanks very much for any suggestion or clarification on this.<\/p>",
        "Challenge_closed_time":1663711220203,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662878679543,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73677347",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":8.65,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":231.2612944445,
        "Challenge_title":"Does SageMaker built-in LightGBM algorithm support distributed training?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":27,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>I went through the LightGBM section of SageMaker documentation and there are no references that it supports distributed training. One of the example\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0uses single instance type. Also looked at lightGBM documentation\u00a0<a href=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parallel-Learning-Guide.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0. Here are the parameters that you need to specify<\/p>\n<p>tree_learner=your_parallel_algorithm,<\/p>\n<p>num_machines=your_num_machines,<\/p>\n<p>Given I couldnt find any reference of above in SageMaker documentation, I assume its not supported.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.1,
        "Solution_reading_time":9.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":26.7248641667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have keras training script on my machine. I am experimenting to run my script on AWS sagemaker container. For that I have used below code.<\/p>\n<pre><code>from sagemaker.tensorflow import TensorFlow\nest = TensorFlow(\n    entry_point=&quot;caller.py&quot;,\n    source_dir=&quot;.\/&quot;,\n    role='role_arn',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_type='ml.m5.large',\n    instance_count=1,\n    hyperparameters={'batch': 8, 'epochs': 10},\n)\n\nest.fit()\n<\/code><\/pre>\n<p>here <code>caller.py<\/code> is my entry point. After executing the above code I am getting <code>keras is not installed<\/code>. Here is the stacktrace.<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;executor.py&quot;, line 14, in &lt;module&gt;\n    est.fit()\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 682, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 1625, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3681, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3240, in _check_job_status\n    raise exceptions.UnexpectedStatusException(\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job tensorflow-training-2021-06-09-07-14-01-778: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/local\/bin\/python3.7 caller.py --batch 4 --epochs 10\n\nModuleNotFoundError: No module named 'keras'\n\n<\/code><\/pre>\n<ol>\n<li>Which instance has pre-installed keras?<\/li>\n<li>Is there any way I can install the python package to the AWS container? or any workaround for the issue?<\/li>\n<\/ol>\n<p>Note: I have tried with my own container uploading to ECR and successfully run my code. I am looking for AWS's existing container capability.<\/p>",
        "Challenge_closed_time":1623320398007,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623223568407,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1623224188496,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67899421",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":28.95,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":26.8971111111,
        "Challenge_title":"Training keras model in AWS Sagemaker",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":316,
        "Challenge_word_count":191,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426675778223,
        "Poster_location":"Coimbatore, Tamil Nadu, India",
        "Poster_reputation_count":10189.0,
        "Poster_view_count":1471.0,
        "Solution_body":"<p>Keras is now part of tensorflow, so you can just reformat your code to use <code>tf.keras<\/code> instead of <code>keras<\/code>. Since version 2.3.0 of tensorflow they are in sync, so it should not be that difficult.\nYou container is <a href=\"https:\/\/aws.amazon.com\/releasenotes\/aws-deep-learning-containers-for-tensorflow-2-3-1-with-cuda-11-0\/\" rel=\"nofollow noreferrer\">this<\/a>, as you can see from the list of the packages, there is no <code>Keras<\/code>.\nIf you instead want to extend a pre-built container you can take a look <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html\" rel=\"nofollow noreferrer\">here<\/a> but I don't recommend in this specific use-case, because also for future code maintainability you should go for <code>tf.keras<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":10.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":93.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":20.4671011111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We currently have a system running on AWS Sagemaker whereby several units have their own trained machine learning model artifact (using an SKLearn training script with the Sagemaker SKLearn estimator).<\/p>\n<p>Through the use of Sagemaker's multi-model endpoints, we are able to host all of these units on a single instance.<\/p>\n<p>The problem we have is that we need to scale this system up such that we can train individual models for hundreds of thousand of units and then host the resulting model artifacts on a multi-model endpoint. But, Sagemaker has a limit to the number of models you can train in parallel (our limit is 30).<\/p>\n<p>Aside from training our models in batches, does anyone have any ideas how to go about implementing a system in AWS Sagemaker whereby for hundreds of thousands of units, we can have a separate trained model artifact for each unit?<\/p>\n<p>Is there a way to output multiple model artifacts for 1 sagemaker training job with the use of an SKLearn estimator?<\/p>\n<p>Furthermore, how does Sagemaker make use of multiple CPUs when a training script is submitted? Does this have to be specified in the training script\/estimator object or is this handled automatically?<\/p>",
        "Challenge_closed_time":1603790179687,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603715439387,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1603716498123,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64537150",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":15.32,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":20.7611944445,
        "Challenge_title":"AWS Sagemaker Multiple Training Jobs",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1053,
        "Challenge_word_count":202,
        "Platform":"Stack Overflow",
        "Poster_created_time":1592311727163,
        "Poster_location":null,
        "Poster_reputation_count":153.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Here are some ideas:<\/p>\n<p><em><strong>1. does anyone have any ideas how to go about implementing a system in AWS Sagemaker whereby for hundreds of thousands of units, we can have a separate trained model artifact for each unit? Is there a way to output multiple model artifacts for 1 sagemaker training job with the use of an SKLearn estimator?<\/strong><\/em><\/p>\n<p>I don't know if the 30-training job concurrency is a hard limit, if it is a blocker you should try and open a support ticket to ask if it is and try and get it raised. Otherwise as you can point out, you can try and train multiple models in one job, and produce multiple artifacts that you can either (a) send to S3 manually, or (b) save to <code>opt\/ml\/model<\/code> so that they all get sent to the model.tar.gz artifact in S3. Note that if this artifact gets too big this could get impractical though<\/p>\n<p><em><strong>2. how does Sagemaker make use of multiple CPUs when a training script is submitted? Does this have to be specified in the training script\/estimator object or is this handled automatically?<\/strong><\/em><\/p>\n<p>This depends on the type of training container you are using. SageMaker built-in containers are developed by Amazon teams and designed to efficiently use available resources. If you use your own code such as custom python in the Sklearn container, you are responsible for making sure that your code is efficiently written and uses available hardware. Hence framework choice is quite important :) for example, some sklearn models support explicitly using multiple CPUs (eg the <code>n_jobs<\/code> parameter in the <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\" rel=\"nofollow noreferrer\">random forest<\/a>), but I don't think that Sklearn natively supports GPU, multi-GPU or multi-node training.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":23.18,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":281.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1254957460063,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation_count":2484.0,
        "Answerer_view_count":362.0,
        "Challenge_adjusted_solved_time":3.0221416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose I have a trained model in <a href=\"https:\/\/studio.azureml.net\" rel=\"nofollow noreferrer\">Azure ML<\/a> and I deployed it as a Web Service. Is it possible to export the model, embed it in an Android app and use it <em>locally<\/em>, without making any requests to Azure Web service?<\/p>",
        "Challenge_closed_time":1527019984807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1527009105097,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50473170",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":3.98,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":3.0221416667,
        "Challenge_title":"Embedded Azure MLmodel",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":85,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Poster_created_time":1485085240960,
        "Poster_location":"Ivanovo, Ivanovo Oblast, Russia",
        "Poster_reputation_count":490.0,
        "Poster_view_count":196.0,
        "Solution_body":"<p>From <a href=\"https:\/\/stackoverflow.com\/questions\/41236871\/how-to-download-the-trained-models-from-azure-machine-studio\">this answer<\/a> you won't be able to save the model locally if you do everything within Azure ML Studio.<\/p>\n\n<p>If you create the model using Python or R and execute it within Azure ML Studio, then you can save it from the library that you use.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":4.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1363369778320,
        "Answerer_location":null,
        "Answerer_reputation_count":7643.0,
        "Answerer_view_count":515.0,
        "Challenge_adjusted_solved_time":152.6492175,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have implemented a PyTorch <code>Dataset<\/code> that works locally (on my own desktop), but when executed on AWS SageMaker, it breaks. My <code>Dataset<\/code> implementation is as follows.<\/p>\n\n<pre><code>class ImageDataset(Dataset):\n    def __init__(self, path='.\/images', transform=None):\n        self.path = path\n        self.files = [join(path, f) for f in listdir(path) if isfile(join(path, f)) and f.endswith('.jpg')]\n        self.transform = transform\n        if transform is None:\n            self.transform = transforms.Compose([\n                transforms.Resize((128, 128)),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n\n    def __len__(self):\n        return len(files)\n\n    def __getitem__(self, idx):\n        img_name = self.files[idx]\n\n        # we may infer the label from the filename\n        dash_idx = img_name.rfind('-')\n        dot_idx = img_name.rfind('.')\n        label = int(img_name[dash_idx + 1:dot_idx])\n\n        image = Image.open(img_name)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n<\/code><\/pre>\n\n<p>I am following this <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/master\/src\/sagemaker\/pytorch\" rel=\"noreferrer\">example<\/a> and this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/pytorch_cnn_cifar10\/pytorch_local_mode_cifar10.ipynb\" rel=\"noreferrer\">one too<\/a>, and I run the <code>estimator<\/code> as follows.<\/p>\n\n<pre><code>inputs = {\n 'train': 'file:\/\/images',\n 'eval': 'file:\/\/images'\n}\nestimator = PyTorch(entry_point='pytorch-train.py',\n                            role=role,\n                            framework_version='1.0.0',\n                            train_instance_count=1,\n                            train_instance_type=instance_type)\nestimator.fit(inputs)\n<\/code><\/pre>\n\n<p>I get the following error.<\/p>\n\n<blockquote>\n  <p>FileNotFoundError: [Errno 2] No such file or directory: '.\/images'<\/p>\n<\/blockquote>\n\n<p>In the example that I am following, they upload the CFAIR dataset (which is downloaded locally) to S3.<\/p>\n\n<pre><code>inputs = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix='data\/cifar10')\n<\/code><\/pre>\n\n<p>If I take a peek at <code>inputs<\/code>, it is just a string literal <code>s3:\/\/sagemaker-us-east-3-184838577132\/data\/cifar10<\/code>. The code to create a <code>Dataset<\/code> and a <code>DataLoader<\/code> is shown <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/pytorch_mnist\/mnist.py#L41\" rel=\"noreferrer\">here<\/a>, which does not help unless I track down the source and step through the logic.<\/p>\n\n<p>I think what needs to happen inside my <code>ImageDataset<\/code> is to supply the <code>S3<\/code> path and use the <code>AWS CLI<\/code> or something to query the files and acquire their content. I do not think the <code>AWS CLI<\/code> is the right approach as this relies on the console and I will have to execute some sub-process commands and then parse through. <\/p>\n\n<p>There must be a recipe or something to create a custom <code>Dataset<\/code> backed by <code>S3<\/code> files, right?<\/p>",
        "Challenge_closed_time":1546965697560,
        "Challenge_comment_count":0,
        "Challenge_created_time":1546416160377,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54003052",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":11.8,
        "Challenge_reading_time":38.99,
        "Challenge_score_count":8,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":152.6492175,
        "Challenge_title":"How do I implement a PyTorch Dataset for use with AWS SageMaker?",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":4597,
        "Challenge_word_count":302,
        "Platform":"Stack Overflow",
        "Poster_created_time":1363369778320,
        "Poster_location":null,
        "Poster_reputation_count":7643.0,
        "Poster_view_count":515.0,
        "Solution_body":"<p>I was able to create a PyTorch <code>Dataset<\/code> backed by S3 data using <code>boto3<\/code>. Here's the snippet if anyone is interested.<\/p>\n\n<pre><code>class ImageDataset(Dataset):\n    def __init__(self, path='.\/images', transform=None):\n        self.path = path\n        self.s3 = boto3.resource('s3')\n        self.bucket = self.s3.Bucket(path)\n        self.files = [obj.key for obj in self.bucket.objects.all()]\n        self.transform = transform\n        if transform is None:\n            self.transform = transforms.Compose([\n                transforms.Resize((128, 128)),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n\n    def __len__(self):\n        return len(files)\n\n    def __getitem__(self, idx):\n        img_name = self.files[idx]\n\n        # we may infer the label from the filename\n        dash_idx = img_name.rfind('-')\n        dot_idx = img_name.rfind('.')\n        label = int(img_name[dash_idx + 1:dot_idx])\n\n        # we need to download the file from S3 to a temporary file locally\n        # we need to create the local file name\n        obj = self.bucket.Object(img_name)\n        tmp = tempfile.NamedTemporaryFile()\n        tmp_name = '{}.jpg'.format(tmp.name)\n\n        # now we can actually download from S3 to a local place\n        with open(tmp_name, 'wb') as f:\n            obj.download_fileobj(f)\n            f.flush()\n            f.close()\n            image = Image.open(tmp_name)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":16.51,
        "Solution_score_count":12.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":136.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":53.7180736111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently using react-native to build a mobile application. I need to access a machine learning model in order to send pictures for segmentation. I want to be able to receive a segmented picture back to have the background of the picture cut out. I am trying to use Amazon Sagemaker (because it seems to be a easy to work with package, but if there are other ways to do it, please let me know).<\/p>\n\n<p>On <a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/?sc_icampaign=pac-sagemaker-console-tutorial&amp;sc_ichannel=ha&amp;sc_icontent=awssm-2276&amp;sc_iplace=console-body&amp;trk=ha_awssm-2276\" rel=\"nofollow noreferrer\">this<\/a> Sagemaker quick-start guide, on step 5a, it states:<\/p>\n\n<blockquote>\n  <p>5a. To deploy the model on a server and create an endpoint that you can access, copy the following code into the next code cell and select Run:\n  xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')<\/p>\n<\/blockquote>\n\n<p>I want to host everything on AWS and not have to run a separate server. What service\/process could I use that would allow me to create an endpoint that I can access through react-native?<\/p>",
        "Challenge_closed_time":1569671934552,
        "Challenge_comment_count":4,
        "Challenge_created_time":1569478549487,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58110595",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":12.0,
        "Challenge_reading_time":16.16,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":53.7180736111,
        "Challenge_title":"How do I access Amazon Sagemaker through React Native?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":717,
        "Challenge_word_count":163,
        "Platform":"Stack Overflow",
        "Poster_created_time":1488334447848,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>To summarize the conversation in the comments:<\/p>\n\n<p>Once you have your model trained, tuned, and deployed (which is not a simple process), you can call the endpoint of the model using the <a href=\"https:\/\/github.com\/aws\/aws-sdk-js\" rel=\"nofollow noreferrer\">AWS SDK for JavaScript<\/a>, that you install by:<\/p>\n\n<pre><code>npm install aws-sdk\nvar AWS = require('aws-sdk\/dist\/aws-sdk-react-native');\n<\/code><\/pre>\n\n<p>you include in the HTML as:<\/p>\n\n<pre><code>&lt;script src=\"https:\/\/sdk.amazonaws.com\/js\/aws-sdk-2.538.0.min.js\"&gt;&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>And when you want to call the endpoint you invoke it like that:<\/p>\n\n<pre><code>var params = {\n  Body: Buffer.from('...') || 'STRING_VALUE' \/* Strings will be Base-64 encoded on your behalf *\/, \/* required *\/\n  EndpointName: 'STRING_VALUE', \/* required *\/\n  Accept: 'STRING_VALUE',\n  ContentType: 'STRING_VALUE',\n  CustomAttributes: 'STRING_VALUE'\n};\nsagemakerruntime.invokeEndpoint(params, function(err, data) {\n  if (err) console.log(err, err.stack); \/\/ an error occurred\n  else     console.log(data);           \/\/ successful response\n});\n<\/code><\/pre>\n\n<p>You can check out the <a href=\"https:\/\/aws-amplify.github.io\" rel=\"nofollow noreferrer\">Amplify Library<\/a> that can take some of the heavy liftings such as getting IAM permissions to call the API, a user log in and many others. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.9,
        "Solution_reading_time":17.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":145.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1443017464707,
        "Answerer_location":"Sweden",
        "Answerer_reputation_count":644.0,
        "Answerer_view_count":126.0,
        "Challenge_adjusted_solved_time":280.9621063889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have Python package for pre-processing the data for train and scoring\/inference purpose. I am using it as a python step in a pipeline. The entry script (which is in package) takes argument i.e task argument choices=(train,score) and does the pre-processing. Here is the step code:<\/p>\n<pre><code># Pipeline parameter: task, config_path\nparam_task = PipelineParameter(name='task', default_value='train')\nparam_config_path = PipelineParameter(name=&quot;config_path&quot;, default_value='Preprocess\/preprocess_config.json')\n\n\n# Define pipeline steps\nStepPreprocessing = PythonScriptStep(\n    name=&quot;Preprocessing&quot;,\n    script_name=e.preprocess_script_path,\n    arguments=[\n        &quot;--config_path&quot;, param_config_path, \n        &quot;--task&quot;, param_task,\n    ], \n    inputs=None,\n    compute_target=aml_compute,\n    runconfig=run_config,\n    source_directory=e.sources_directory,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p><strong>With argument task=='train'<\/strong> it loads data and does pre-processing according to steps mentioned in a config file. During this process it creates StandardScaler, SimpleImpute objects (sklearn objects) and stores the sklearn objects in a data\/output folder inside the package, and the processed data on azure storage.<\/p>\n<p>The problem is, when the pipeline is run again with <strong>task =='score'<\/strong> it is unable to find the sklearn objects with error.<\/p>\n<pre><code>User program failed with FileNotFoundError: [Errno 2] No such file or directory: 'data\/output\/StandardScaler.joblib'\n<\/code><\/pre>\n<p>What is the best way to save the sklearn objects so that these can be accessed by pipeline when pipeline in run again but with argument task=='score'.<\/p>\n<p>I don't want to register these objects in model registry and don't want to save them in datastores as well.<\/p>",
        "Challenge_closed_time":1645522336520,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644510872937,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71068837",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":23.69,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":280.9621063889,
        "Challenge_title":"Serialise objects in azure ML pipeline runs",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":50,
        "Challenge_word_count":210,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443017464707,
        "Poster_location":"Sweden",
        "Poster_reputation_count":644.0,
        "Poster_view_count":126.0,
        "Solution_body":"<p>The way to do that is either:<\/p>\n<ol>\n<li><p>Register the artifacts in model registry and get them in scoring.<\/p>\n<\/li>\n<li><p>Configure output of pipeline step as PipelineData or OutputFileDatasetConfig, write artifacts to configured output. While scoring, get run of the train pipeline, get its outputs, retrieve the artifacts. This involves experiment name to get run of the pipeline.<\/p>\n<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":5.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1319019150600,
        "Answerer_location":null,
        "Answerer_reputation_count":3073.0,
        "Answerer_view_count":341.0,
        "Challenge_adjusted_solved_time":4.8491525,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use a XGBoost model in Sage Maker and use it to score for a large data stored in S3 using Batch Transform.<\/p>\n<p>I build the model using existing Sagemaker Container as follows:<\/p>\n<pre><code>estimator = sagemaker.estimator.Estimator(image_name=container, \n                                          hyperparameters=hyperparameters,\n                                          role=sagemaker.get_execution_role(),\n                                          train_instance_count=1, \n                                          train_instance_type='ml.m5.2xlarge', \n                                          train_volume_size=5, # 5 GB \n                                          output_path=output_path,\n                                          train_use_spot_instances=True,\n                                          train_max_run=300,\n                                          train_max_wait=600)\n\nestimator.fit({'train': s3_input_train,'validation': s3_input_test})\n<\/code><\/pre>\n<p>The following code is used to do Batch Transform<\/p>\n<pre><code> The location of the test dataset\nbatch_input = 's3:\/\/{}\/{}\/test\/examples'.format(bucket, prefix)\n\n# The location to store the results of the batch transform job\nbatch_output = 's3:\/\/{}\/{}\/batch-inference'.format(bucket, prefix)\n\ntransformer = xgb_model.transformer(instance_count=1, instance_type='ml.m4.xlarge', output_path=batch_output)\n\ntransformer.transform(data=batch_input, data_type='S3Prefix', content_type='text\/csv', split_type='Line')\n\ntransformer.wait()\n<\/code><\/pre>\n<p>The above code works fine in Development environment (Jupyter notebook) when the model is built in Jupyter. However, I would like to deploy the model and call its endpoint for Batch Transform.<\/p>\n<p>Most examples for SageMaker endpoint creation is for scoring on a single data and not for batch transform.<\/p>\n<p>Can someone point to how to deploy and use the endpoints for Batch Transform in SageMaker? Thank you<\/p>",
        "Challenge_closed_time":1603999757616,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603982300667,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64593327",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":21.29,
        "Challenge_score_count":2,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":4.8491525,
        "Challenge_title":"AWS SageMaker Deployment for Batch Transform",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":2988,
        "Challenge_word_count":169,
        "Platform":"Stack Overflow",
        "Poster_created_time":1319019150600,
        "Poster_location":null,
        "Poster_reputation_count":3073.0,
        "Poster_view_count":341.0,
        "Solution_body":"<p>The following link has an example of how to call a stored model in SageMaker to run Batch Transform job.<\/p>\n<p><a href=\"https:\/\/github.com\/YiranJing\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/xgboost_customer_churn\/customised_xgboost_customer_churn_batch_transform.ipynb\" rel=\"nofollow noreferrer\">Batch Transform Reference<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":23.2,
        "Solution_reading_time":5.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1428654714763,
        "Answerer_location":null,
        "Answerer_reputation_count":596.0,
        "Answerer_view_count":80.0,
        "Challenge_adjusted_solved_time":2.4045722222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#search-modelversions\" rel=\"nofollow noreferrer\">this api endpoint<\/a>.\nI can call this in python, no problem, like the below<\/p>\n<pre><code>get_model_versions={\n    &quot;filter&quot;:&quot;name='model_name'&quot;,\n    &quot;order_by&quot;:[&quot;version DESC&quot;],\n    &quot;max_results&quot;:1\n}\n\ninit_get = requests.get(&quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search&quot;,headers=header_read,json=get_model_versions)\n<\/code><\/pre>\n<p>However, I just can't seem to find a way to make it work in Powershell.<\/p>\n<p>First the powershell &quot;get&quot; Invoke-RestMethod does not accept a body<\/p>\n<p>and then I can't seem to find a way to append it in Powershell as a query string.<\/p>\n<p>I have tried (among other failed attempts), the following<\/p>\n<pre><code>$get_model_versions=([PSCustomObject]@{\n  filter = &quot;name=`'model_name`'&quot;\n  order_by = @(&quot;version desc&quot;)\n} | ConvertTo-Json)\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get -Body $get_model_versions\n<\/code><\/pre>\n<p>But that gives me an error that body can't be used with a get method<\/p>\n<p>trying to append it as a query string (like if I even just keep the name filter and remove the others), also fails<\/p>\n<pre><code>$searchuri= &quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search?filter=&quot;&quot;name==model_name&quot;&quot;&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get\n<\/code><\/pre>\n<p>fails with<\/p>\n<pre><code>{&quot;error_code&quot;:&quot;INVALID_PARAMETER_VALUE&quot;,&quot;message&quot;:&quot;Unsupported filter query : `\\&quot;name==model_name\\&quot;`. Unsupported operator.&quot;}\n<\/code><\/pre>\n<p>How can I mimic the same behaviour in Powershell, as I do in Python?<\/p>\n<p>EDIT 1: I did try to encode the query param (maybe I did it wrong), but here's how my failed attempt looked like<\/p>\n<pre><code>$encodedvalue = [System.Web.HttpUtility]::UrlEncode(&quot;`&quot;name='model_name'`&quot;&quot;)\n$searchuri= &quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search?filter=$encodedvalue&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get\n<\/code><\/pre>\n<p>But that too gives me<\/p>\n<pre><code>&quot;Unsupported filter query : `\\&quot;name='model_name'\\&quot;`. Unsupported operator.&quot;\n<\/code><\/pre>\n<p>I have also tried it successfully in Postman by passing a raw json body (the same as python) and when I look at the generated PowerShell code in Postman I see this<\/p>\n<pre><code>$headers = New-Object &quot;System.Collections.Generic.Dictionary[[String],[String]]&quot;\n$headers.Add(&quot;Authorization&quot;, &quot;Bearer token&quot;)\n$headers.Add(&quot;Content-Type&quot;, &quot;application\/json&quot;)\n\n$body = &quot;{\n`n    `&quot;filter`&quot;:`&quot;name='model_name'`&quot;,\n`n    `&quot;order_by`&quot;:[`&quot;version DESC`&quot;],\n`n    `&quot;max_results`&quot;:1\n`n}\n`n&quot;\n\n$response = Invoke-RestMethod 'baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search' -Method 'GET' -Headers $headers -Body $body\n$response | ConvertTo-Json\n<\/code><\/pre>\n<p>But of course that fails (if you copy that in an powershell editor and run it<\/p>\n<pre><code>Invoke-RestMethod : Cannot send a content-body with this verb-type\n<\/code><\/pre>",
        "Challenge_closed_time":1652649592320,
        "Challenge_comment_count":3,
        "Challenge_created_time":1652637573657,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1652640935860,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72250896",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":16.7,
        "Challenge_reading_time":43.95,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":3.3385175,
        "Challenge_title":"PowerShell Get request with body",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":281,
        "Challenge_word_count":307,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428654714763,
        "Poster_location":null,
        "Poster_reputation_count":596.0,
        "Poster_view_count":80.0,
        "Solution_body":"<p>Finally, after struggling for a long time, I found the answer !<\/p>\n<p>The crux is in the documentation <a href=\"https:\/\/docs.microsoft.com\/en-us\/powershell\/module\/microsoft.powershell.utility\/invoke-restmethod?view=powershell-7.2\" rel=\"nofollow noreferrer\">here<\/a>.\nEspecially this section<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/h0gwk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/h0gwk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So, if you want to pass on a body for your &quot;get&quot; method in powershell, pass it as a hashtable.<\/p>\n<p>So, finally the answer is<\/p>\n<pre><code>$query=@{&quot;filter&quot;=&quot;name='model_name'&quot;;&quot;order_by&quot;=@(&quot;version DESC&quot;); &quot;max_results&quot;=1};\n$searchuri=&quot;baseurl\/api\/2.0\/preview\/mlflow\/model-versions\/search&quot;\n\n$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get -Body $query\n<\/code><\/pre>\n<p>Hope this helps someone looking for something similar.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":15.6,
        "Solution_reading_time":13.35,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":82.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":1076.4220325,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I created an event rule for the Sagemaker training job state change in cloudwatch to monitor my training jobs. Then I use this events to trigger a lambda function that send messages in a telegram group as a bot. In this way I receive a message every time one of the training job change its status. It works but there is a problem with the events, they are fired multiple times with the same exact payload, so I receive tons of duplicate messages.\nSince all the payploads are identical (except the field <code>LastModifiedTime<\/code>) I cannot filter them in the lambda. Unfortunately I don't have the AWS Developer plan so I cannot receive support from Amazon. Any idea?<\/p>\n<p><strong>EDIT<\/strong><\/p>\n<p>There are no duplicate rules\/events. I also noticed that enabling the Sagemaker profiler (which is now by default) cause the number of identical rule invocations literally explode. All of them have the same payload except for the <code>LastModifiedTime<\/code> so I suspect that there is a bug in AWS for that. One solution could be to implement some sort of data retention on the lambda and check if an invocation has already been processed, but I don't want complicate a thing that should be very simple. Just tried to launch a new training job and got this sequence (I only report the fields I parse):<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Launching requested ML instances<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Starting the training job<\/p>\n<p>Status: InProgress\nSecondary Status: Starting\nStatus Message: Preparing the instances for training<\/p>\n<p>Status: InProgress\nSecondary Status: Downloading\nStatus Message: Downloading input data<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Downloading the training image<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Training in-progres<\/p>\n<p>Status: InProgress\nSecondary Status: Training\nStatus Message: Training image download completed. Training in progress<\/p>",
        "Challenge_closed_time":1617006916700,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611574816563,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1613131797383,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65884046",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.4,
        "Challenge_reading_time":29.22,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":1508.9167047222,
        "Challenge_title":"AWS Eventbridge Events (Sagemaker training job status change) fired multiple times with the same payload",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":780,
        "Challenge_word_count":336,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416346350292,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation_count":2302.0,
        "Poster_view_count":227.0,
        "Solution_body":"<p>After a lot of experiments I can answer myself that Sagemaker generates multiple events with the same payload, except for the field <code>LastModifiedTime<\/code>. I don't know is this is a bug, but should not happen in my opinion. These are rules defined by AWS itself, so nothing I can customize. The situation is even worse if you enable the profiler.\nThere is nothing I can do, since I already posted on the official AWS forum multiple times without any luck.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":5.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1559910246180,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":2046.0,
        "Answerer_view_count":369.0,
        "Challenge_adjusted_solved_time":3949.4255963889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to schedule a data-quality monitoring job in AWS SageMaker by following steps mentioned in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-data-quality.html\" rel=\"nofollow noreferrer\">this AWS documentation page<\/a>. I have enabled data-capture for my endpoint. Then, trained a baseline on my training csv file and statistics and constraints are available in S3 like this:<\/p>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker import image_uris\nfrom sagemaker.model_monitor.dataset_format import DatasetFormat\n\nmy_data_monitor = DefaultModelMonitor(\n    role=get_execution_role(),\n    instance_count=1,\n    instance_type='ml.m5.large',\n    volume_size_in_gb=30,\n    max_runtime_in_seconds=3_600)\n\n# base s3 directory\nbaseline_dir_uri = 's3:\/\/api-trial\/data_quality_no_headers\/'\n# train data, that I have used to generate baseline\nbaseline_data_uri = baseline_dir_uri + 'ch_train_no_target.csv'\n# directory in s3 bucket that I have stored my baseline results to \nbaseline_results_uri = baseline_dir_uri + 'baseline_results_try17\/'\n\n\nmy_data_monitor.suggest_baseline(\n    baseline_dataset=baseline_data_uri,\n    dataset_format=DatasetFormat.csv(header=True),\n    output_s3_uri=baseline_results_uri,\n    wait=True, logs=False, job_name='ch-dq-baseline-try21'\n)\n<\/code><\/pre>\n<p>and data is available in S3:\n<a href=\"https:\/\/i.stack.imgur.com\/rNwZ0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rNwZ0.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Then I tried scheduling a monitoring job by following <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_model_monitor\/model_quality\/model_quality_churn_sdk.ipynb\" rel=\"nofollow noreferrer\">this example notebook for model-quality-monitoring in sagemaker-examples github repo<\/a>, to schedule my data-quality-monitoring job by making necessary modifications with feedback from error messages.<\/p>\n<p>Here's how tried to schedule the data-quality monitoring job from SageMaker Studio:<\/p>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker.model_monitor import EndpointInput\nfrom sagemaker import image_uris\nfrom sagemaker.model_monitor import CronExpressionGenerator\nfrom sagemaker.model_monitor import DefaultModelMonitor\nfrom sagemaker.model_monitor.dataset_format import DatasetFormat\n\n# base s3 directory\nbaseline_dir_uri = 's3:\/\/api-trial\/data_quality_no_headers\/'\n\n# train data, that I have used to generate baseline\nbaseline_data_uri = baseline_dir_uri + 'ch_train_no_target.csv'\n\n# directory in s3 bucket that I have stored my baseline results to \nbaseline_results_uri = baseline_dir_uri + 'baseline_results_try17\/'\n# s3 locations of baseline job outputs\nbaseline_statistics = baseline_results_uri + 'statistics.json'\nbaseline_constraints = baseline_results_uri + 'constraints.json'\n\n# directory in s3 bucket that I would like to store results of monitoring schedules in\nmonitoring_outputs = baseline_dir_uri + 'monitoring_results_try17\/'\n\nch_dq_ep = EndpointInput(endpoint_name=myendpoint_name,\n                         destination=&quot;\/opt\/ml\/processing\/input_data&quot;,\n                         s3_input_mode=&quot;File&quot;,\n                         s3_data_distribution_type=&quot;FullyReplicated&quot;)\n\nmonitor_schedule_name='ch-dq-monitor-schdl-try21'\n\nmy_data_monitor.create_monitoring_schedule(endpoint_input=ch_dq_ep,\n                                           monitor_schedule_name=monitor_schedule_name,\n                                           output_s3_uri=baseline_dir_uri,\n                                           constraints=baseline_constraints,\n                                           statistics=baseline_statistics,\n                                           schedule_cron_expression=CronExpressionGenerator.hourly(),\n                                           enable_cloudwatch_metrics=True)\n<\/code><\/pre>\n<p>after an hour or so, when I check the status of the schedule like this:<\/p>\n<pre><code>import boto3\nboto3_sm_client = boto3.client('sagemaker')\nboto3_sm_client.describe_monitoring_schedule(MonitoringScheduleName='ch-dq-monitor-schdl-try17')\n<\/code><\/pre>\n<p>I get failed status like below:<\/p>\n<pre><code>'MonitoringExecutionStatus': 'Failed',\n  ...\n  'FailureReason': 'Job inputs had no data'},\n<\/code><\/pre>\n<p>Entire Message:<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"true\" data-console=\"false\" data-babel=\"false\">\n<div class=\"snippet-code snippet-currently-hidden\">\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>```\n{'MonitoringScheduleArn': 'arn:aws:sagemaker:ap-south-1:&lt;my-account-id&gt;:monitoring-schedule\/ch-dq-monitor-schdl-try21',\n 'MonitoringScheduleName': 'ch-dq-monitor-schdl-try21',\n 'MonitoringScheduleStatus': 'Scheduled',\n 'MonitoringType': 'DataQuality',\n 'CreationTime': datetime.datetime(2021, 9, 14, 13, 7, 31, 899000, tzinfo=tzlocal()),\n 'LastModifiedTime': datetime.datetime(2021, 9, 14, 14, 1, 13, 247000, tzinfo=tzlocal()),\n 'MonitoringScheduleConfig': {'ScheduleConfig': {'ScheduleExpression': 'cron(0 * ? * * *)'},\n  'MonitoringJobDefinitionName': 'data-quality-job-definition-2021-09-14-13-07-31-483',\n  'MonitoringType': 'DataQuality'},\n 'EndpointName': 'ch-dq-nh-try21',\n 'LastMonitoringExecutionSummary': {'MonitoringScheduleName': 'ch-dq-monitor-schdl-try21',\n  'ScheduledTime': datetime.datetime(2021, 9, 14, 14, 0, tzinfo=tzlocal()),\n  'CreationTime': datetime.datetime(2021, 9, 14, 14, 1, 9, 405000, tzinfo=tzlocal()),\n  'LastModifiedTime': datetime.datetime(2021, 9, 14, 14, 1, 13, 236000, tzinfo=tzlocal()),\n  'MonitoringExecutionStatus': 'Failed',\n  'EndpointName': 'ch-dq-nh-try21',\n  'FailureReason': 'Job inputs had no data'},\n 'ResponseMetadata': {'RequestId': 'dd729244-fde9-44b5-9904-066eea3a49bb',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': 'dd729244-fde9-44b5-9904-066eea3a49bb',\n   'content-type': 'application\/x-amz-json-1.1',\n   'content-length': '835',\n   'date': 'Tue, 14 Sep 2021 14:27:53 GMT'},\n  'RetryAttempts': 0}}\n```<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p>Possible things you might think to have gone wrong at my side or might help me fix my issue:<\/p>\n<ol>\n<li>dataset used for baseline: I have tried to create a baseline with the dataset with and without my target-variable(or dependent variable or y) and the error persisted both times. So, I think the error has originated because of a different reason.<\/li>\n<li>there are no log groups created for these jobs for me to look at and try debug the issue. baseline jobs have log-groups, so i presume there is no problem with roles being used for monitoring-schedule-jobs  not having permissions to create a log group or stream.<\/li>\n<li>role: the role I have attached is defined by <code>get_execution_role()<\/code>, which points to a role with full access to sagemaker, cloudwatch, S3 and some other services.<\/li>\n<li>the data collected from my endpoint during my inference: here's how a line of data of .jsonl file saved to S3, which contains data collected during inference, looks like:<\/li>\n<\/ol>\n<pre><code>{&quot;captureData&quot;:{&quot;endpointInput&quot;:{&quot;observedContentType&quot;:&quot;application\/json&quot;,&quot;mode&quot;:&quot;INPUT&quot;,&quot;data&quot;:&quot;{\\&quot;longitude\\&quot;: [-122.32, -117.58], \\&quot;latitude\\&quot;: [37.55, 33.6], \\&quot;housing_median_age\\&quot;: [50.0, 5.0], \\&quot;total_rooms\\&quot;: [2501.0, 5348.0], \\&quot;total_bedrooms\\&quot;: [433.0, 659.0], \\&quot;population\\&quot;: [1050.0, 1862.0], \\&quot;households\\&quot;: [410.0, 555.0], \\&quot;median_income\\&quot;: [4.6406, 11.0567]}&quot;,&quot;encoding&quot;:&quot;JSON&quot;},&quot;endpointOutput&quot;:{&quot;observedContentType&quot;:&quot;text\/html; charset=utf-8&quot;,&quot;mode&quot;:&quot;OUTPUT&quot;,&quot;data&quot;:&quot;eyJtZWRpYW5faG91c2VfdmFsdWUiOiBbNDUyOTU3LjY5LCA0NjcyMTQuNF19&quot;,&quot;encoding&quot;:&quot;BASE64&quot;}},&quot;eventMetadata&quot;:{&quot;eventId&quot;:&quot;9804d438-eb4c-4cb4-8f1b-d0c832b641aa&quot;,&quot;inferenceId&quot;:&quot;ef07163d-ea2d-4730-92f3-d755bc04ae0d&quot;,&quot;inferenceTime&quot;:&quot;2021-09-14T13:59:03Z&quot;},&quot;eventVersion&quot;:&quot;0&quot;}\n<\/code><\/pre>\n<p>I would like to know what has gone wrong in this entire process, that led to data not being fed to my monitoring job.<\/p>",
        "Challenge_closed_time":1633064178830,
        "Challenge_comment_count":4,
        "Challenge_created_time":1631630733197,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1631632366336,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69179914",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":19.4,
        "Challenge_reading_time":107.91,
        "Challenge_score_count":4,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":398.1793425,
        "Challenge_title":"How to fix SageMaker data-quality monitoring-schedule job that fails with 'FailureReason': 'Job inputs had no data'",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":565,
        "Challenge_word_count":650,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559910246180,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":2046.0,
        "Poster_view_count":369.0,
        "Solution_body":"<p>This happens, during the ground-truth-merge job, when the spark can't find any data either in '\/opt\/ml\/processing\/groundtruth\/' or '\/opt\/ml\/processing\/input_data\/' directories. And that can happen when either you haven't sent any requests to the sagemaker endpoint or there are no ground truths.<\/p>\n<p>I got this error because, the folder <code>\/opt\/ml\/processing\/input_data\/<\/code> of the docker volume mapped to the monitoring container had no data to process. And that happened because, the thing that facilitates entire process, including fetching data couldn't find any in S3. and that happened because, there was an extra slash(<code>\/<\/code>) in the directory to which endpoint's captured-data will be saved. to elaborate, while creating the endpoint, I had mentioned the directory as <code>s3:\/\/&lt;bucket-name&gt;\/&lt;folder-1&gt;\/<\/code>, while it should have just been <code>s3:\/\/&lt;bucket-name&gt;\/&lt;folder-1&gt;<\/code>. so, while the thing that copies data from S3 to docker volume tried to fetch data of that hour, the directory it tried to extract the data from was <code>s3:\/\/&lt;bucket-name&gt;\/&lt;folder-1&gt;\/\/&lt;endpoint-name&gt;\/&lt;variant-name&gt;\/&lt;year&gt;\/&lt;month&gt;\/&lt;date&gt;\/&lt;hour&gt;<\/code>(notice the two slashes). So, when I created the endpoint-configuration again with the slash removed in S3 directory, this error wasn't present and ground-truth-merge operation was successful as part of model-quality-monitoring.<\/p>\n<p><em>I am answering this question because, someone read the question and upvoted it. meaning, someone else has faced this problem too. so, I have mentioned what worked for me. And I wrote this, so that StackExchange doesn't think I am spamming the forum with questions.<\/em><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1645850298483,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":22.46,
        "Solution_score_count":4.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":224.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":18.5281647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to register a data set as a Python step with the Azure Machine Learning Studio designer. Here is my code:<\/p>\n<pre><code>import pandas as pd\nfrom azureml.core import Workspace, Run, Dataset\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    run = Run.get_context()\n    ws = run. experiment.workspace\n    ds = Dataset.from_pandas_dataframe(dataframe1)\n    ds.register(workspace = ws,\n                name = &quot;data set name&quot;,\n                description = &quot;example description&quot;,\n                create_new_version = True)\n    return dataframe1, \n<\/code><\/pre>\n<p>I get an error saying that &quot;create_new_version&quot; in the ds.register line was an unexpected keyword argument. However, this keyword appears in the documentation and I need it to keep track of new versions of the file.<\/p>\n<p>If I remove the argument, I get a different error: &quot;Local data source path not supported for this operation&quot;, so it still does not work. Any help is appreciated. Thanks!<\/p>",
        "Challenge_closed_time":1628119362627,
        "Challenge_comment_count":3,
        "Challenge_created_time":1628113771587,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68658385",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.3,
        "Challenge_reading_time":13.29,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.5530666667,
        "Challenge_title":"Azure Machine Learning Studio designer - \"create new version\" unexpected when registering a data set",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":565,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1371499229816,
        "Poster_location":null,
        "Poster_reputation_count":1111.0,
        "Poster_view_count":191.0,
        "Solution_body":"<h2>update<\/h2>\n<p>sharing OP's solution here for easier discovery<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom azureml.core import Workspace, Run, Dataset\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    run = Run.get_context()\n    ws = run. experiment.workspace\n    datastore = ws.get_default_datastore()\n    ds = Dataset.Tabular.register_pandas_dataframe(\n        dataframe1, datastore, 'data_set_name',\n        description = 'data set description.')\n    return dataframe1,\n<\/code><\/pre>\n<h2>original answer<\/h2>\n<p>Sorry you're struggling. You're very close!<\/p>\n<p>A few things may be the culprit here.<\/p>\n<ol>\n<li>It looks like you're using the <code>Dataset<\/code> class, which has been deprecated. I recommend trying <code>Dataset.Tabular.register_pandas_dataframe()<\/code> (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py#register-pandas-dataframe-dataframe--target--name--description-none--tags-none--show-progress-true-\" rel=\"nofollow noreferrer\">docs link<\/a>) instead of <code>Dataset.from_pandas_dataframe()<\/code>. (<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/dataset-api-change-notice.md\" rel=\"nofollow noreferrer\">more about the Dataset API deprecation<\/a>)<\/li>\n<li>More conjectire here, but another thing is there might be some limitations to using dataset registration within an &quot;Execute Python Script&quot; (EPS) module due to:\n<ol>\n<li>the workspace object might not have the right permissions<\/li>\n<li>you might not be able to use the <code>register_pandas_dataframe<\/code> method inside the EPS module, but might have better luck with save the dataframe first to parquet, then calling <code>Dataset.Tabular.from_parquet_files<\/code><\/li>\n<\/ol>\n<\/li>\n<\/ol>\n<p>Hopefully something works here!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1628180472980,
        "Solution_link_count":2.0,
        "Solution_readability":14.7,
        "Solution_reading_time":25.08,
        "Solution_score_count":2.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":165.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1341441916656,
        "Answerer_location":null,
        "Answerer_reputation_count":5985.0,
        "Answerer_view_count":161.0,
        "Challenge_adjusted_solved_time":61.4723369444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Sagemaker in order to perform binary classification on time series, each sample being a numpy array of shape [24,11] (24h, 11features). I used a tensorflow model in script mode, my script being very similar to the one I used as reference:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py<\/a><\/p>\n\n<p>The training reported success and I was able to deploy a model for batch transformation. The transform job works fine when I input just a few samples (say, [10,24,11]), but it returns an <code>InternalServerError<\/code> when I input more samples for prediction (for example, [30000, 24, 11], which size is >100MB).<\/p>\n\n<p>Here is the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-6-0c46f7563389&gt; in &lt;module&gt;()\n     32 \n     33 # Then wait until transform job is completed\n---&gt; 34 tf_transformer.wait()\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    133     def wait(self):\n    134         self._ensure_last_transform_job()\n--&gt; 135         self.latest_transform_job.wait()\n    136 \n    137     def _ensure_last_transform_job(self):\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    207 \n    208     def wait(self):\n--&gt; 209         self.sagemaker_session.wait_for_transform_job(self.job_name)\n    210 \n    211     @staticmethod\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_transform_job(self, job, poll)\n    893         \"\"\"\n    894         desc = _wait_until(lambda: _transform_job_status(self.sagemaker_client, job), poll)\n--&gt; 895         self._check_job_status(job, desc, 'TransformJobStatus')\n    896         return desc\n    897 \n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n    915             reason = desc.get('FailureReason', '(No reason provided)')\n    916             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 917             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    918 \n    919     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Transform job Tensorflow-batch-transform-2019-05-29-02-56-00-477: Failed Reason: InternalServerError: We encountered an internal error.  Please try again.\n\n<\/code><\/pre>\n\n<p>I tried to use both SingleRecord and MultiRecord parameters when deploying the model but the result was the same, so I decided to keep MultiRecord. My transformer looks like that:<\/p>\n\n<pre><code>transformer = tf_estimator.transformer(\n    instance_count=1, \n    instance_type='ml.m4.xlarge',\n    max_payload = 100,\n    assemble_with = 'Line',\n    strategy='MultiRecord'\n)\n<\/code><\/pre>\n\n<p>At first I was using a json file as input for the transform job, and it threw the error : <\/p>\n\n<pre><code>Too much data for max payload size\n<\/code><\/pre>\n\n<p>So next I tried the jsonlines format (the .npy format is not supported as far as I understand), thinking that jsonlines could get split by Line and thus avoid the size error, but that's where I got the <code>InternalServerError<\/code>. Here is the related code:<\/p>\n\n<pre><code>#Convert test_x to jsonlines and save\ntest_x_list = test_x.tolist()\nfile_path ='data_cnn_test\/test_x.jsonl'\nfile_name='test_x.jsonl'\n\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)    \n\ninput_key = 'batch_transform_tf\/input\/{}'.format(file_name)\noutput_key = 'batch_transform_tf\/output'\ntest_input_location = 's3:\/\/{}\/{}'.format(bucket, input_key)\ntest_output_location = 's3:\/\/{}\/{}'.format(bucket, output_key)\n\ns3.upload_file(file_path, bucket, input_key)\n\n# Initialize the transformer object\ntf_transformer = sagemaker.transformer.Transformer(\n    base_transform_job_name='Tensorflow-batch-transform',\n    model_name='sagemaker-tensorflow-scriptmode-2019-05-29-02-46-36-162',\n    instance_count=1,\n    instance_type='ml.c4.2xlarge',\n    output_path=test_output_location,\n    assemble_with = 'Line'\n    )\n\n# Start the transform job\ntf_transformer.transform(test_input_location, content_type='application\/jsonlines', split_type='Line')\n<\/code><\/pre>\n\n<p>The list named test_x_list has a shape [30000, 24, 11], which corresponds to 30000 samples so I would like to return 30000 predictions.<\/p>\n\n<p>I suspect my jsonlines file isn't being split by Line and is of course too big to be processed in one batch, which throws the error, but I don't understand why it doesn't get split correctly. I am using the default output_fn and input_fn (I did not re-write those functions in my script).<\/p>\n\n<p>Any insight on what I could be doing wrong would be greatly appreciated.<\/p>",
        "Challenge_closed_time":1559329920120,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559108619707,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56353814",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":64.67,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":61.4723369444,
        "Challenge_title":"Batch transform job results in \"InternalServerError\" with data file >100MB",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1826,
        "Challenge_word_count":483,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559099281007,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I assume this is a duplicate of this AWS Forum post: <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0<\/a><\/p>\n\n<p>Anyway, for completeness I'll answer here as well.<\/p>\n\n<p>The issue is that you are serializing your dataset incorrectly when converting it into jsonlines:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)   \n<\/code><\/pre>\n\n<p>What the above is doing is creating a very large single-line containing your full dataset which is too big for single inference call to consume.<\/p>\n\n<p>I suggest you change your code to make each line a single sample so that inference can take place on individual samples instead of the whole dataset:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    for sample in test_x_list:\n        writer.write(sample)\n<\/code><\/pre>\n\n<p>If one sample at a time is too slow you can also play around with the <code>max_concurrent_transforms<\/code>, <code>strategy<\/code>, and <code>max_payload<\/code> parameters to be able to batch the data as well as run concurrent transforms if your algorithm can run in parallel - also, of course, you can split the data into multiple files and run transformations with more than just one node. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html<\/a> for additional detail on what these parameters do.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":13.3,
        "Solution_reading_time":23.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":191.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":15.8759286111,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I am new with <code>Jupyter<\/code>, and I use Amazon SageMaker so that everything is cloud based and not local.  I cannot use any resources locally, nor can I install <code>Jupyter<\/code> on this local computer that I want to do this on, so I cannot use the command line to put :<\/p>\n\n<pre><code>jupyter nbconvert Jupyter\\ Slides.ipynb --to slides --post serve\n<\/code><\/pre>\n\n<p>So, I am struggling to find a way to convert my notebook to a slideshow NOT using command line. Thanks in advance!<\/p>",
        "Challenge_closed_time":1532732192903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1532649972710,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1532675039560,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51549048",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.2,
        "Challenge_reading_time":7.1,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":22.8389425,
        "Challenge_title":"How to convert jupyter notebook (ipython) to slideshow NOT using command line",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":4610,
        "Challenge_word_count":93,
        "Platform":"Stack Overflow",
        "Poster_created_time":1528567336707,
        "Poster_location":"Kelowna, BC, Canada",
        "Poster_reputation_count":165.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>You can follow below steps to convert your notebook to slides on AWS Sagemaker (tried on sagemaker notebook instance) without installing any extensions.<\/p>\n\n<p><strong>Step 1:<\/strong> Follow this <a href=\"https:\/\/medium.com\/@mjspeck\/presenting-code-using-jupyter-notebook-slides-a8a3c3b59d67\" rel=\"nofollow noreferrer\">article<\/a> to chose which cells in your notebook can be presented or skipped.\n  - Go to View \u2192 Cell Toolbar \u2192 Slideshow\n  - A light gray bar will appear above each cell with a scroll down window on the top right\n  - Select type of slide each cell should be - regular slide, sub-slide, skip, notes<\/p>\n\n<p><strong>Step 2:<\/strong> Go to Sagemaker notebook home page and open terminal<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/kDl3d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kDl3d.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 3:<\/strong> Change directory in the instance where your notebook exists<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/rA1lZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rA1lZ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 4:<\/strong> Clone <code>reveal.js<\/code> in the directory where notebook exists from <a href=\"https:\/\/github.com\/hakimel\/reveal.js\" rel=\"nofollow noreferrer\">github<\/a>. <code>reveal.js<\/code> is used for rendering HTML file as presentation.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/dillF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/dillF.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 5:<\/strong> Run the below command (same as in your question) to convert the notebook to slides without serving them (since there is no browser on the Sagemaker instance). This will just convert notebook to slides html.<\/p>\n\n<pre><code>jupyter nbconvert Image-classification-fulltraining.ipynb --to slides\n[NbConvertApp] Converting notebook Image-classification-fulltraining.ipynb to slides\n[NbConvertApp] Writing 346423 bytes to Image-classification-fulltraining.slides.html\n<\/code><\/pre>\n\n<p><strong>Step 6:<\/strong> Now open the html file from Sagemaker notebook file browser <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/fykyl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fykyl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Now you can see the notebook rendered as slides based on how setup each cell in your notebook in Step 1<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9PLUA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9PLUA.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Hope it helps.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":12.0,
        "Solution_readability":12.5,
        "Solution_reading_time":34.51,
        "Solution_score_count":2.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":263.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":21.2124047222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I keep getting this error in sagemaker when iterating through pytorch dataloader batch cycles:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;main.py&quot;, line 371, in &lt;module&gt;\n    g_scaler=g_scaler, d_scaler=d_scaler, runtime_log_folder=runtime_log_folder, runtime_log_file_name=runtime_log_file_name)\n  File &quot;main.py&quot;, line 78, in train_fn\n    for idx, (x, y) in enumerate(loop):\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/tqdm\/std.py&quot;, line 1171, in __iter__\n    for obj in iterable:\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 525, in __next__\n    (data, worker_id) = self._next_data()\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 1252, in _next_data\n    return (self._process_data(data), w_id)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 1299, in _process_data\n    data.reraise()\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/_utils.py&quot;, line 429, in reraise\n    raise self.exc_type(msg)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/botocore\/exceptions.py&quot;, line 84, in __init__\n    super(HTTPClientError, self).__init__(**kwargs)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/botocore\/exceptions.py&quot;, line 40, in __init__\n    msg = self.fmt.format(**kwargs)\nKeyError: 'error'\n\n---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n&lt;ipython-input-1-81655136a841&gt; in &lt;module&gt;\n     58                             py_version='py3')\n     59 \n---&gt; 60 pytorch_estimator.fit({'train': Runtime.dataset_path}, job_name=Runtime.job_name)\n     61 \n     62 #print(pytorch_estimator.latest_job_tensorboard_artifacts_path())\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n    955         self.jobs.append(self.latest_training_job)\n    956         if wait:\n--&gt; 957             self.latest_training_job.wait(logs=logs)\n    958 \n    959     def _compilation_job_name(self):\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   1954         # If logs are requested, call logs_for_jobs.\n   1955         if logs != &quot;None&quot;:\n-&gt; 1956             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   1957         else:\n   1958             self.sagemaker_session.wait_for_job(self.job_name)\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in logs_for_job(self, job_name, wait, poll, log_type)\n   3751 \n   3752         if wait:\n-&gt; 3753             self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n   3754             if dot:\n   3755                 print()\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3304                 ),\n   3305                 allowed_statuses=[&quot;Completed&quot;, &quot;Stopped&quot;],\n-&gt; 3306                 actual_status=status,\n   3307             )\n   3308 \n\nUnexpectedStatusException: Error for Training job 2022-06-03-05-16-49-pix2pix-U12239-2022-05-09-14-39-18-training: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/opt\/conda\/bin\/python3.6 main.py --runtime_var dataset_name=U12239-2022-05-09-14-39-18,job_name=2022-06-03-05-16-49-pix2pix-U12239-2022-05-09-14-39-18-training,model_name=pix2pix&quot;\n\n  0%|          | 0\/248 [00:00&lt;?, ?it\/s]\n  0%|          | 1\/248 [00:30&lt;2:07:28, 30.97s\/it]\n  0%|          | 1\/248 [00:30&lt;2:07:28, 30.97s\/it]\nTraceback (most recent call last):\n  File &quot;main.py&quot;, line 371, in &lt;module&gt;\n    g_scaler=g_scaler, d_scaler=d_scaler, runtime_log_folder=runtime_log_folder, runtime_log_file_name=runtime_log_file_name)\n  File &quot;main.py&quot;, line 78, in train_fn\n    for idx, (x, y) in enumerate(loop):\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/tqdm\/std.py&quot;, line 1171, in __iter__\n    for obj in iterable:\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 525, in __next__\n    (data, worker_id) = self._next_data()\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 1252, in _next_data\n    return (self\n<\/code><\/pre>\n<p>Here is the code which results in the error:<\/p>\n<pre><code>def train_fn(disc, gen, loader, opt_disc, opt_gen, l1, bce, g_scaler, d_scaler,runtime_log_folder,runtime_log_file_name):\n\n    total_output=''\n    \n    loop = tqdm(loader, leave=True)\n    device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n\n    print(&quot;Loop&quot;)\n    print(loop)\n    print(&quot;Length loop&quot;)\n    print(len(loop))\n    for idx, (x, y) in enumerate(loop): #&lt;--error happens here\n        print(&quot;Loop index&quot;)\n        print(idx)\n        print(&quot;Loop item&quot;)\n        print(x,y)\n        x = x.to(device)\n        y = y.to(device)\n        \n        # train discriminator\n        with torch.cuda.amp.autocast():\n            y_fake = gen(x)\n\n            D_real = disc(x, y)\n            D_fake = disc(x, y_fake.detach())\n            # use detach so as to avoid breaking computational graph when do optimizer.step on discriminator\n            # can use detach, or when do loss.backward put loss.backward(retain_graph = True)\n\n            D_real_loss = bce(D_real, torch.ones_like(D_real))\n            D_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n\n            D_loss = (D_real_loss + D_fake_loss) \/ 2\n            \n            # log tensorboard\n            \n        disc.zero_grad()\n        d_scaler.scale(D_loss).backward()\n        d_scaler.step(opt_disc)\n        d_scaler.update()\n        \n        # train generator\n        with torch.cuda.amp.autocast():\n            \n            D_fake = disc(x, y_fake)\n\n            # compute fake loss\n            # trick discriminator to believe these are real, hence send in torch.oneslikedfake\n            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n\n            # compute L1 loss\n            L1 = l1(y_fake, y) * args.l1_lambda\n\n            G_loss = G_fake_loss + L1\n            \n            # log tensorboard\n           \n        opt_gen.zero_grad()\n        g_scaler.scale(G_loss).backward()\n        g_scaler.step(opt_gen)\n        g_scaler.update()\n        \n        # print epoch, generator loss, discriminator loss\n        print(f'[Epoch {epoch}\/{args.num_epochs} (b: {idx})] [D loss: {D_loss}, D real loss: {D_real_loss}, D fake loss: {D_fake_loss}] [G loss: ##{G_loss}, G fake loss: {G_fake_loss}, L1 loss: {L1}]')\n        output = f'[Epoch {epoch}\/{args.num_epochs} (b: {idx})] [D loss: {D_loss}, D real loss: {D_real_loss}, D fake loss: {D_fake_loss}] [G loss: ##{G_loss}, G fake loss: {G_fake_loss}, L1 loss: {L1}]\\n'\n        total_output+=output\n\n\n\n    runtime_log = get_json_file_from_s3(runtime_log_folder, runtime_log_file_name)\n    runtime_log += total_output\n    upload_json_file_to_s3(runtime_log_folder,runtime_log_file_name,json.dumps(runtime_log))\n\n\n\ndef __getitem__(self, index):\n    print(&quot;Index &quot;,index)\n    pair_key = self.list_files[index]\n    print(&quot;Pair key &quot;,pair_key)\n    pair = Boto.s3_client.list_objects(Bucket=Boto.bucket_name, Prefix=pair_key, Delimiter='\/')\n\n    input_image_key = pair.get('Contents')[1].get('Key')\n    input_image_path = f's3:\/\/{Boto.bucket_name}\/{input_image_key}'\n    print(&quot;Input image path &quot;,input_image_path)\n    input_image_s3_source = get_file_from_filepath(input_image_path)\n    input_image = np.array(Image.open(input_image_s3_source))\n\n    target_image_key = pair.get('Contents')[0].get('Key')\n    target_image_path = f's3:\/\/{Boto.bucket_name}\/{target_image_key}'\n    print(&quot;Target image path &quot;,target_image_path)\n    target_image_s3_source = get_file_from_filepath(target_image_path)\n    target_image = np.array(Image.open(target_image_s3_source))\n\n    augmentations = config.both_transform(image=input_image, image0=target_image)\n\n    # get input image and target image by doing augmentations of images\n    input_image, target_image = augmentations['image'], augmentations['image0']\n\n    input_image = config.transform_only_input(image=input_image)['image']\n    target_image = config.transform_only_mask(image=target_image)['image']\n    \n    print(&quot;Input image size &quot;,input_image.size())\n    print(&quot;Target image size &quot;,target_image.size())\n    \n    return input_image, target_image\n\n<\/code><\/pre>\n<p>I did multiple runs and here are the traces of the failure points<\/p>\n<pre><code>i) 2022-06-03-05-00-04-pix2pix-U12239-2022-05-09-14-39-18-training\nNo index shown\n[Epoch 0\/100 (b: 0)]\n\nii) 2022-06-03-05-16-49-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\n[Epoch 0\/100 (b: 0)]\n\niii) 2022-06-03-05-44-46-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\n[Epoch 0\/100 (b: 0)]\n\niv) 2022-06-03-06-08-33-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\n[Epoch 1\/100 (b: 0)]\n\nv) 2022-06-15-02-49-20-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\nPair key  datasets\/training-data\/testing\/2022-05-09-14-39-18\/match-raws-finals\/U12239\/P423712\/Pair_71\/\n[Epoch 0\/100 (b: 0)\n\nvi) 2022-06-15-02-59-43-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  64\nPair key  datasets\/training-data\/testing\/2022-05-09-14-39-18\/match-raws-finals\/U12239\/P425642\/Pair_27\/\n[Epoch 0\/100 (b: 247)]\n\nvii) 2022-06-15-04-49-33-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  64\nPair key  datasets\/training-data\/testing\/2022-05-09-14-39-18\/match-raws-finals\/U12239\/P415414\/Pair_124\/\nNo specific epoch \n<\/code><\/pre>\n<p>My batch size is 248, so as you can see it seems to fail either at the start of the batch (0) or at the end (247). Also there are some common Indexes in the get item which seems to cause it to fail, namely Index 64 and Index 160. However there doesn't seem to be a common data point in the dataset that causes it to fail, as can be seen from the pair key all 3 data points in the datasets are different.<\/p>\n<p>Does anyone have any idea why this error happens please?<\/p>",
        "Challenge_closed_time":1655379897027,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655303532370,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72633246",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":124.59,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":74,
        "Challenge_solved_time":21.2124047222,
        "Challenge_title":"Error in pytorch data loader batch cycles",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":55,
        "Challenge_word_count":745,
        "Platform":"Stack Overflow",
        "Poster_created_time":1643118380396,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>Try to run the same training script outside of a SageMaker training job and see what happens.<br \/>\nIf the error doesn't happen on a standalone script, try to run it as a <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"nofollow noreferrer\">Local SageMaker training job<\/a>, so you can reproduce it in seconds instead of minutes, and potentially use a debugger to figure out what is the problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":6.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1456986606312,
        "Answerer_location":null,
        "Answerer_reputation_count":757.0,
        "Answerer_view_count":80.0,
        "Challenge_adjusted_solved_time":2717.5608175,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I've got some data on S3 bucket that I want to work with. <\/p>\n\n<p>I've imported it using:<\/p>\n\n<pre><code>import boto3\nimport dask.dataframe as dd\n\ndef import_df(key):\n        s3 = boto3.client('s3')\n        df = dd.read_csv('s3:\/\/...\/' + key ,encoding='latin1')\n        return df\n\nkey = 'Churn\/CLEANED_data\/file.csv'\ntrain = import_df(key)\n<\/code><\/pre>\n\n<p>I can see that the data has been imported correctly using:<\/p>\n\n<pre><code>train.head()\n<\/code><\/pre>\n\n<p>but when I try simple operation (<a href=\"https:\/\/docs.dask.org\/en\/latest\/dataframe.html\" rel=\"nofollow noreferrer\">taken from this dask doc<\/a>):<\/p>\n\n<pre><code>train_churn = train[train['CON_CHURN_DECLARATION'] == 1]\ntrain_churn.compute()\n<\/code><\/pre>\n\n<p>I've got Error:<\/p>\n\n<blockquote>\n  <p>AttributeError                            Traceback (most recent call\n  last)  in ()<\/p>\n  \n  <p>1 train_churn = train[train['CON_CHURN_DECLARATION'] == 1]<\/p>\n  \n  <p>----> 2 train_churn.compute()<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/dask\/base.py in\n  compute(self, **kwargs)\n      152         dask.base.compute\n      153         \"\"\"\n  --> 154         (result,) = compute(self, traverse=False, **kwargs)\n      155         return result\n      156<\/p>\n  \n  <p>AttributeError: 'DataFrame' object has no attribute '_getitem_array'<\/p>\n<\/blockquote>\n\n<p>Full error here: <a href=\"https:\/\/textuploader.com\/11lg7\" rel=\"nofollow noreferrer\">Error Upload<\/a><\/p>",
        "Challenge_closed_time":1574121568172,
        "Challenge_comment_count":1,
        "Challenge_created_time":1564579246737,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57291797",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":10.7,
        "Challenge_reading_time":18.19,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2650.6448430556,
        "Challenge_title":"Dask: AttributeError: 'DataFrame' object has no attribute '_getitem_array'",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2742,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1429630461500,
        "Poster_location":"Warszawa, Polska",
        "Poster_reputation_count":938.0,
        "Poster_view_count":137.0,
        "Solution_body":"<p>I was facing a similar issue when trying to read from s3 files, ultimately solved by updating dask to most recent version (I think the one sagemaker instances start with by default is deprecated)<\/p>\n\n<h2>Install\/Upgrade packages and dependencies (from notebook)<\/h2>\n\n<pre><code>! python -m pip install --upgrade dask\n! python -m pip install fsspec\n! python -m pip install --upgrade s3fs\n<\/code><\/pre>\n\n<p>Hope this helps!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1574362465680,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":5.35,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1388584380832,
        "Answerer_location":null,
        "Answerer_reputation_count":396.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":42.3742536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to integrate mlflow ui to our website by using an iframe, but with the header hidden if possible. I found there is an environment variable setting in the source code \/mlflow\/server\/js\/components\/HomeView.js:\n<code>const headerHeight = process.env.HIDE_HEADER === 'true' ? 0 : 60;<\/code> But how can I specify this environment by running the server with <code>mlflow server<\/code>? I tried with <code>HIDE_HEADER=true mlflow server<\/code>, but this doesn't work. Or is there any other way to solve this?<\/p>",
        "Challenge_closed_time":1572468752163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572316204850,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58600732",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.9,
        "Challenge_reading_time":7.46,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":42.3742536111,
        "Challenge_title":"Is there a way to hide mlflow ui header when start the server with mlflow server?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":296,
        "Challenge_word_count":87,
        "Platform":"Stack Overflow",
        "Poster_created_time":1441151447408,
        "Poster_location":null,
        "Poster_reputation_count":626.0,
        "Poster_view_count":79.0,
        "Solution_body":"<p>@Jason good question, those environment variables are read at build-time for the MLflow UI's Javascript assets. Since the PyPI MLflow wheel comes with pre-built Javascript assets, it's difficult to achieve your use case using a PyPI installation of <code>mlflow<\/code>.<\/p>\n\n<p>However, you can build a custom MLflow wheel from source with the UI header hidden by following the instructions <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/branch-1.3\/CONTRIBUTING.rst#building-a-distributable-artifact\" rel=\"nofollow noreferrer\">here<\/a>, replacing the <code>npm run build<\/code> step with <code>HIDE_HEADER=true npm run build<\/code> (basically, the idea is to set the desired environment variables prior to building Javascript assets via <code>npm run build<\/code>). You can then pip-install that wheel on the node hosting your MLflow server &amp; launch the server via <code>mlflow server<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.0,
        "Solution_reading_time":11.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":112.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1285875105172,
        "Answerer_location":"Santa Cruz, CA, United States",
        "Answerer_reputation_count":4051.0,
        "Answerer_view_count":461.0,
        "Challenge_adjusted_solved_time":147.4295269444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Sagemaker and have a bunch of model.tar.gz files that I need to unpack and load in sklearn. I've been testing using list_objects with delimiter to get to the tar.gz files:<\/p>\n\n<pre><code>response = s3.list_objects(\nBucket = bucket,\nPrefix = 'aleks-weekly\/models\/',\nDelimiter = '.csv'\n)\n\n\nfor i in response['Contents']:\n    print(i['Key'])\n<\/code><\/pre>\n\n<p>And then I plan to extract with<\/p>\n\n<pre><code>import tarfile\ntf = tarfile.open(model.read())\ntf.extractall()\n<\/code><\/pre>\n\n<p>But how do I get to the actual tar.gz file from s3 instead of a some boto3 object? <\/p>",
        "Challenge_closed_time":1566337639600,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565806893303,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57500105",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":7.92,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":147.4295269444,
        "Challenge_title":"Python boto3 load model tar file from s3 and unpack it",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":4787,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421343783700,
        "Poster_location":null,
        "Poster_reputation_count":1387.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>You can download objects to files using <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.download_file\" rel=\"nofollow noreferrer\"><code>s3.download_file()<\/code><\/a>. This will make your code look like:<\/p>\n\n<pre><code>s3 = boto3.client('s3')\nbucket = 'my-bukkit'\nprefix = 'aleks-weekly\/models\/'\n\n# List objects matching your criteria\nresponse = s3.list_objects(\n    Bucket = bucket,\n    Prefix = prefix,\n    Delimiter = '.csv'\n)\n\n# Iterate over each file found and download it\nfor i in response['Contents']:\n    key = i['Key']\n    dest = os.path.join('\/tmp',key)\n    print(\"Downloading file\",key,\"from bucket\",bucket)\n    s3.download_file(\n        Bucket = bucket,\n        Key = key,\n        Filename = dest\n    )\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.7,
        "Solution_reading_time":9.49,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":1.0084252778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How to get the existing AKS from the notebook that I have already added to AML.\nCreate the cluster<\/p>\n\n<pre><code>attach_config = AksCompute.attach_configuration(resource_id=resource_id)\naks_target = ComputeTarget.attach(workspace=ws, name=create_name, attach_configuration=attach_config)\naks_target.wait_for_completion(True)\n<\/code><\/pre>",
        "Challenge_closed_time":1585037443208,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585033812877,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60826366",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.8,
        "Challenge_reading_time":5.21,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.0084252778,
        "Challenge_title":"How to get the existing AKS using compute target",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":279,
        "Challenge_word_count":35,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>List all ComputeTarget objects within the workspace:\nPlease follow the below link.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.compute.computetarget?view=azure-ml-py#list-workspace-\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.compute.computetarget?view=azure-ml-py#list-workspace-<\/a><\/p>\n\n<p>you can do like as shown below.<\/p>\n\n<pre><code>from azureml.core.compute import AksCompute, ComputeTarget\naks_name = 'YOUR_EXISTING_CLUSTER_NAME\u2019\naks_target =AksCompute(ws, aks_name)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":23.6,
        "Solution_reading_time":8.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":29.4160033333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to read a csv file on an s3 bucket (for which the sagemaker notebook has full access to) into a spark dataframe however I am hitting the following issue where <code>sagemaker-spark_2.11-spark_2.2.0-1.1.1.jar<\/code> can't be found. Any tips on how to resolve this is appreciate!<\/p>\n\n<pre><code>bucket = \"mybucket\"\nprefix = \"folder\/file.csv\"\ndf = spark.read.csv(\"s3:\/\/{}\/{}\/\".format(bucket,prefix))\n\nPy4JJavaError: An error occurred while calling o388.csv.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Error reading configuration file\nat java.util.ServiceLoader.fail(ServiceLoader.java:232)\nat java.util.ServiceLoader.parse(ServiceLoader.java:309)\nat java.util.ServiceLoader.access$200(ServiceLoader.java:185)\nat java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:357)\nat java.util.ServiceLoader$LazyIterator.hasNext(ServiceLoader.java:393)\nat java.util.ServiceLoader$1.hasNext(ServiceLoader.java:474)\nat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)\nat scala.collection.Iterator$class.foreach(Iterator.scala:893)\nat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\nat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\nat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\nat scala.collection.TraversableLike$class.filterImpl(TraversableLike.scala:247)\nat scala.collection.TraversableLike$class.filter(TraversableLike.scala:259)\nat scala.collection.AbstractTraversable.filter(Traversable.scala:104)\nat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:614)\nat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190)\nat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:596)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:498)\nat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\nat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\nat py4j.Gateway.invoke(Gateway.java:282)\nat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\nat py4j.commands.CallCommand.execute(CallCommand.java:79)\nat py4j.GatewayConnection.run(GatewayConnection.java:238)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker_pyspark\/jars\/sagemaker-spark_2.11-spark_2.2.0-1.1.1.jar (No such file or directory)\n    at java.util.zip.ZipFile.open(Native Method)\n    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:219)\n    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:149)\n    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:166)\n    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:103)\n    at sun.net.www.protocol.jar.URLJarFile.&lt;init&gt;(URLJarFile.java:93)\n    at sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:69)\n    at sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:84)\n    at sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:122)\n    at sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:150)\n    at java.net.URL.openStream(URL.java:1045)\n    at java.util.ServiceLoader.parse(ServiceLoader.java:304)\n    ... 26 more\n<\/code><\/pre>",
        "Challenge_closed_time":1532493818792,
        "Challenge_comment_count":2,
        "Challenge_created_time":1532387294417,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":1532387921180,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51488308",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":34.1,
        "Challenge_reading_time":49.06,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":29.5901041667,
        "Challenge_title":"Failing to read data from s3 to a spark dataframe in Sagemaker",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":2283,
        "Challenge_word_count":169,
        "Platform":"Stack Overflow",
        "Poster_created_time":1444524456120,
        "Poster_location":"Los Angeles, CA, USA",
        "Poster_reputation_count":973.0,
        "Poster_view_count":82.0,
        "Solution_body":"<p>(Making comment to the original question as answer)<\/p>\n\n<p>It looks like a jupyter kernel issue. I had a similar issue and I used <code>Sparkmagic (pyspark)<\/code> kernel instead of <code>Sparkmagic (pyspark3)<\/code> and it is working fine. Follow instructions on this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\" rel=\"nofollow noreferrer\">blog<\/a> and see if it helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":5.93,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":48.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":152.7426680556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I\u2019m having some issues trying to load a dataset in Azure ML Studio, a dataset containing a column that looks like a DateTime, but is in fact a string. Azure ML Studio converts the values to DateTimes internally, and no amount of wrangling seems to convince it of the that they\u2019re in fact strings.<\/p>\n\n<p>This is an issue, because during conversion the values lose precision and start appearing as duplicates whereas in fact they are unique. Does anybody know if ML Studio can be configured not to infer data types for columns while importing a dataset?<\/p>\n\n<p>Now, for the long(er) story :)<\/p>\n\n<p>I\u2019m working here with a public dataset - specifically <a href=\"https:\/\/www.kaggle.com\/c\/new-york-city-taxi-fare-prediction\" rel=\"noreferrer\">Kaggle\u2019s New York City Fare Prediction<\/a> competition. I wanted to see if I could do a quick-and-dirty solution using Azure ML Studio, however the dataset\u2019s unique key values are of the form\n<code>\n    2015-01-27 13:08:24.0000003\n    2015-01-27 13:08:24.0000002\n    2011-10-06 12:10:20.0000001\n<\/code>\nand so on. <\/p>\n\n<p>When importing them in my experiment the key values get converted to DateTime, making them no longer unique, even though they\u2019re unique in the csv. Needless to say, this prevents me from submitting any solution to Kaggle, since I can\u2019t identify the rows uniquely :).<\/p>\n\n<p>I\u2019ve tried the following:<\/p>\n\n<ul>\n<li>edit the metadata of the dataset after it has been loaded and setting the data type of the column to string, but this doesn\u2019t do much as the precision has already been lost<\/li>\n<li>import the dataset from an Azure blob, convert it to csv and then loading it in Jupyter\/Python - this brings me the same (duplicated) keys. <\/li>\n<li>loading the dataset locally with pandas works, as expected.<\/li>\n<\/ul>\n\n<p>I\u2019ve reproduced this behavior with both the big, 5.5GB <code>train<\/code> dataset, but also with the more manageable <code>sample_submission<\/code> dataset. <\/p>\n\n<p>Curious to know if there is some sort of workaround to tell ML Studio not to try converting this column while loading the dataset. I'm looking here specifically for Azure ML Studio-only solutions, as I don't want to do any preprocessing on the dataset.<\/p>",
        "Challenge_closed_time":1534433513132,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533883639527,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51780562",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":28.48,
        "Challenge_score_count":5,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":152.7426680556,
        "Challenge_title":"How to prevent Azure ML Studio from converting a feature column to DateTime while importing a dataset",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":401,
        "Challenge_word_count":354,
        "Platform":"Stack Overflow",
        "Poster_created_time":1250158552416,
        "Poster_location":"Romania",
        "Poster_reputation_count":7916.0,
        "Poster_view_count":801.0,
        "Solution_body":"<p>I have tried with you sample data and here is my quick and dirty solution:\n1) Add any symbol (I've added the '#') in front of each date\n2) Load it to AML Studio (it is now considered as a string feature)\n3) Add a Python\/R component to remove the '#' symbol and explicitly convert the column to string (as.string(columnname) or str(columnname))<\/p>\n\n<p>Hope this helps<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":4.54,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1538757797860,
        "Answerer_location":"Dallas, TX, USA",
        "Answerer_reputation_count":5671.0,
        "Answerer_view_count":629.0,
        "Challenge_adjusted_solved_time":4.7034825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm reading a file from my S3 bucket in a notebook in sagemaker studio (same account) using the following code:<\/p>\n<pre><code>dataset_path_in_h5=&quot;\/Mode1\/SingleFault\/SimulationCompleted\/IDV2\/Mode1_IDVInfo_2_100\/Run1\/processdata&quot;\ns3 = s3fs.S3FileSystem()\nh5_file = h5py.File(s3.open(s3url,'rb'), 'r')\ndata = h5_file.get(dataset_path_in_h5)\n<\/code><\/pre>\n<p>But I don't know what actually append behind the scene, does the whole h5 file is being transferred  ? that's seems unlikely as the code is executed quite fast while the whole file is 20GB. Or is just the dataset in dataset_path_in_h5 is transferred ?\nI suppose that if the whole file is transferred at each call it could cost me a lot.<\/p>",
        "Challenge_closed_time":1662041978820,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662025046283,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73567221",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":9.9,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":4.7034825,
        "Challenge_title":"reading hdf5 file from s3 to sagemaker, is the whole file transferred?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":18,
        "Challenge_word_count":101,
        "Platform":"Stack Overflow",
        "Poster_created_time":1576136255052,
        "Poster_location":null,
        "Poster_reputation_count":795.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>When you open the file, a file object is created. It has a tiny memory footprint. The dataset values aren't read into memory until you access them.<\/p>\n<p>You are returning <code>data<\/code> as a NumPy array. That loads the entire dataset into memory. (NOTE: the <code>.get()<\/code> method you are using is deprecated. Current syntax is provided in the example.)<\/p>\n<p>As an alternative to returning an array, you can create a dataset object (which also has a small memory foorprint). When you do, the data is read into memory as you need it. Dataset objects behave like NumPy arrays. (Use of a dataset object vs NumPy array depends on downstream usage. Frequently you don't need an array, but sometimes they are required.) Also, if chunked I\/O was enabled when the dataset was created, datasets are read in chunks.<\/p>\n<p>Differences shown below. Note, I used Python's file context manager to open the file. It avoids problems if the file isn't closed properly (you forget or the program exits prematurely).<\/p>\n<pre><code>dataset_path_in_h5=&quot;\/Mode1\/SingleFault\/SimulationCompleted\/IDV2\/Mode1_IDVInfo_2_100\/Run1\/processdata&quot;\ns3 = s3fs.S3FileSystem()\nwith h5py.File(s3.open(s3url,'rb'), 'r') as h5_file:\n     # your way to get a numpy array -- .get() is depreciated:\n     data = h5_file.get(dataset_path_in_h5)\n     # this is the preferred syntax to return an array:\n     data_arr = h5_file[dataset_path_in_h5][()]\n     # this returns a h5py dataset object:\n     data_ds = h5_file[dataset_path_in_h5]  # deleted [()] \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":19.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":207.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1623222646127,
        "Answerer_location":null,
        "Answerer_reputation_count":23.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":1.2744991667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have been struggling to run this code snippet in my Jupyter notebook in Sagemaker studio when I can run it locally. I previously imported Sagemaker. I can import Sagemaker smoothly and I pip installed it previously.<\/p>\n<pre><code>from sagemaker.workflow.lambda_step import LambdaStep\n<\/code><\/pre>\n<p>Here's the error thrown when I attempted to run it:<\/p>\n<pre><code>ModuleNotFoundError: No module named 'sagemaker.workflow.lambda_step'\n<\/code><\/pre>\n<p>I also tried different ways of importing such as 'from sagemaker.workflow import lambda_step' or 'sagemaker.workflow.lambda_step.LambdaStep' straight away when calling the function. Those ways also did not help me.<\/p>\n<p>I'm also not sure why I can't import LambdaStep when I can run the code snippet below smoothly in Sagemaker studio:<\/p>\n<pre><code>from sagemaker.workflow.pipeline import Pipeline\n<\/code><\/pre>\n<p>I am not sure how to solve this problem and would greatly appreciate if someone can help me out!<\/p>",
        "Challenge_closed_time":1628756732887,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628752144690,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68753045",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":13.82,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1.2744991667,
        "Challenge_title":"Why is it that I can import LambdaStep when I run my notebook locally but not when I run it in Sagemaker studio?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":107,
        "Challenge_word_count":153,
        "Platform":"Stack Overflow",
        "Poster_created_time":1623222646127,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Strangely, my issue is solved after I installed sagemaker again but by using this code snippet<\/p>\n<pre><code>!pip install -U sagemaker\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":1.94,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1606378353316,
        "Answerer_location":"Huskvarna, Sverige",
        "Answerer_reputation_count":275.0,
        "Answerer_view_count":36.0,
        "Challenge_adjusted_solved_time":3.7086241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The yaml template <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html\" rel=\"nofollow noreferrer\">documentation<\/a> for the AWS Cloudformation AWS::SageMaker::Model ContainerDefinition specifies that &quot;Environment&quot; is of type Json. I can't work out how to submit json in my yaml template that does not cause a &quot;CREATE_FAILED    Internal Failure&quot; after running a deploy with the below command.<\/p>\n<pre><code>aws cloudformation deploy --stack-name test1 --template-file test-template-export.yml\n<\/code><\/pre>\n<p>test-template-export.yml<\/p>\n<pre><code>Description: Example yaml\n\nResources:\n  Model:\n    Type: AWS::SageMaker::Model\n    Properties:\n      Containers:\n      - ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n      - Environment: '{&quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;}'\n      ExecutionRoleArn: arn:aws:iam::123456789123:role\/service-role\/AmazonSageMakerServiceCatalogProductsUseRole\n<\/code><\/pre>\n<p>I have also tried the below formats as well and still no luck.<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n  Environment: '{&quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;}'\n<\/code><\/pre>\n<p>--<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n- Environment: | \n         {\n            &quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;\n          }\n<\/code><\/pre>\n<p>--<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n- Environment:\n  - SAGEMAKER_CONTAINER_LOG_LEVEL: &quot;20&quot;\n<\/code><\/pre>\n<p>Running without Environment deploys fine.<\/p>\n<p>I have tried everything in <a href=\"https:\/\/stackoverflow.com\/questions\/39041209\/how-to-specify-json-formatted-string-in-cloudformation\">this answer.<\/a>\nHow do I format this Environment argument?<\/p>\n<p>My version of aws cli is &quot;aws-cli\/2.4.10 Python\/3.8.8&quot;<\/p>",
        "Challenge_closed_time":1643284402368,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643283742143,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70877982",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":24.0,
        "Challenge_reading_time":28.4,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.1833958333,
        "Challenge_title":"How to format parameter of data type json in a aws cloudformation yaml template?",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":391,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Poster_created_time":1452981020536,
        "Poster_location":"UK",
        "Poster_reputation_count":311.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>Hi when you see json format think more dict.\nSo write it like this:<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n  Environment:\n     SAGEMAKER_CONTAINER_LOG_LEVEL: 20\n<\/code><\/pre>\n<p>For IAM Policies the PolicyDocument is json type and this is how AWS do it in their exempel:<\/p>\n<pre><code>Type: 'AWS::IAM::Policy'\nProperties:\n  PolicyName: CFNUsers\n  PolicyDocument:\n    Version: &quot;2012-10-17&quot;\n    Statement:\n      - Effect: Allow\n        Action:\n          - 'cloudformation:Describe*'\n          - 'cloudformation:List*'\n          - 'cloudformation:Get*'\n        Resource: '*'\n  Groups:\n    - !Ref CFNUserGroup\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1643297093190,
        "Solution_link_count":0.0,
        "Solution_readability":18.1,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":59.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1600124498003,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":110.7464591667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've set up a ClearML server in GCP using the sub-domain approach. I can access all three domains (<code>https:\/\/app.clearml.mydomain.com<\/code>, <code>https:\/\/api.clearml.mydomain.com<\/code> and <code>https:\/\/files.clearml.mydomain.com<\/code>) in a browser and see what I think is the correct response, but when connecting with the python SDK via <code>clearml-init<\/code> I get the following error:<\/p>\n<pre><code>clearml.backend_api.session.session.LoginError: Failed getting token (error 400 from https:\/\/api.clearml.mydomain.com): Bad Request\n<\/code><\/pre>\n<p>Are there any likely causes of this error?<\/p>",
        "Challenge_closed_time":1640162037740,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639763350487,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70397010",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":8.87,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":110.7464591667,
        "Challenge_title":"What would stop credentials from validation on a ClearML server?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":194,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600124498003,
        "Poster_location":null,
        "Poster_reputation_count":46.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Following the discussion <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/517\" rel=\"nofollow noreferrer\">here<\/a>, it seemed that the load balancer being used was blocking <code>GET<\/code> requests with a payload which are used by ClearML. A <a href=\"https:\/\/github.com\/allegroai\/clearml\/pull\/521\" rel=\"nofollow noreferrer\">fix<\/a> is being worked on to allow the method to be changed to a <code>POST<\/code> request via an environment variable.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":5.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":51.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1499772840847,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":39.9339302778,
        "Challenge_answer_count":1,
        "Challenge_body":"<h1>Context<\/h1>\n<p>Hi!<\/p>\n<p>In <code>wandb<\/code> I can download a model based on a tag (<code>prod<\/code> for example), but I would like to also get all metrics associated to that run by using tags.<\/p>\n<p>The problem is that I don't know how to a get specific run ID based a tag.<\/p>\n<h1>Example<\/h1>\n<p>Using the code bellow we can extract a run summary metrics, but setting run IDs is setting me back.<\/p>\n<p>So if I can get run IDs based on tag or just explicitly download metrics  with another API call, like with a special sintax in <code>api.run<\/code>, that would be great! In the code example bellow I would like to use the <code>what_i_want_to_use<\/code> string to call the API instead of <code>what_i_use<\/code>.<\/p>\n<pre><code>import wandb\nfrom ast import literal_eval\napi = wandb.Api()\n\nwhat_i_use = &quot;team_name\/project_name\/runID_h3h3h4h4h4h4&quot;\n# what_i_want_to_use = &quot;team_name\/project_name\/artifact_name\/prod_tag&quot;\n\n# run is specified by &lt;entity&gt;\/&lt;project&gt;\/&lt;run_id&gt;\nrun = api.run(what_i_use)\n\n\n# save the metrics for the run to a csv file\nmetrics_dataframe = run.summary\nprint(metrics_dataframe['a_summary_metric'])\n\n<\/code><\/pre>\n<p>By running through the docs I didn't find any solution so far. Any ideias?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mWl3I.png\" rel=\"nofollow noreferrer\">wandb public api run details<\/a><\/p>\n<p>Thanks for reading!<\/p>",
        "Challenge_closed_time":1650458488356,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650314726207,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71916901",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":18.29,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":39.9339302778,
        "Challenge_title":"WANDB Getting a run id based on tag",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":645,
        "Challenge_word_count":184,
        "Platform":"Stack Overflow",
        "Poster_created_time":1444675970627,
        "Poster_location":null,
        "Poster_reputation_count":5.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>It is possible to filter runs by tags as well. You can read more about it <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\/api#runs\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<pre><code>You can filter by config.*, summary_metrics.*, tags, state, entity, createdAt, etc.\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":32.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":103.6265825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have started working with <strong>AWS SageMaker<\/strong> recently with the examples provided by AWS. I used this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/deepar_electricity\/DeepAR-Electricity.ipynb\" rel=\"nofollow noreferrer\">example<\/a> (<strong>DeepAR<\/strong> Model) in order to forecast a time series. After training, a model artifacts file has been created in my S3 bucket. <\/p>\n\n<p><strong>My question:<\/strong> Is there a way to host that trained model in a own hosting environment? (client premises)<\/p>",
        "Challenge_closed_time":1561921541280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561548485583,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56771758",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":8.49,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":103.6265825,
        "Challenge_title":"On-Premises Hosting of Trained DeepAR Model built on AWS SageMaker",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":379,
        "Challenge_word_count":71,
        "Platform":"Stack Overflow",
        "Poster_created_time":1439993560103,
        "Poster_location":"Lebanon",
        "Poster_reputation_count":155.0,
        "Poster_view_count":76.0,
        "Solution_body":"<p>Except SageMaker XGBoost, SageMaker built-in algorithms are not designed to be used out of Amazon. That does not mean that it's impossible, for example you can find here and there snippets peeking inside model artifacts (eg for <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations\/\" rel=\"nofollow noreferrer\">Factorization Machines<\/a> and <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/ntm_20newsgroups_topic_modeling\/ntm_20newsgroups_topic_model.ipynb\" rel=\"nofollow noreferrer\">Neural Topic Model<\/a>) but these things can be hacky and are usually not part of official service features. Regarding DeepAR specifically, the model was open-sourced couple weeks ago as part of <code>gluon-ts<\/code> python package (<a href=\"https:\/\/aws.amazon.com\/blogs\/opensource\/gluon-time-series-open-source-time-series-modeling-toolkit\/\" rel=\"nofollow noreferrer\">blog post<\/a>, <a href=\"https:\/\/gluon-ts.mxnet.io\/\" rel=\"nofollow noreferrer\">code<\/a>) so if you develop a model specifically for your own hosting environment I'd recommend to use that gluon-ts code in the MXNet container, so that you'll be able to open and read the artifact out of SageMaker.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":19.8,
        "Solution_reading_time":17.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":126.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1408529239847,
        "Answerer_location":"Dublin, Ireland",
        "Answerer_reputation_count":1652.0,
        "Answerer_view_count":293.0,
        "Challenge_adjusted_solved_time":17.9931061111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Weights and Biases to track my deep learning models. To monitor everything I use the <code>WandbCallback<\/code> in <code>.fit<\/code>.\nIn the <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/integrations\/keras\/wandbcallback\" rel=\"nofollow noreferrer\">WandbCallback documentation<\/a> there is the keyword <code>save_graph<\/code> which defaults to True. The description is very brief and I wondered what that saved graph is and what it's for? Is saving a graph a costly operation? For what is it needed? (like does it complement something else, like saving the best model?)<\/p>",
        "Challenge_closed_time":1636642289972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636577514790,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69920078",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":8.12,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":17.9931061111,
        "Challenge_title":"What does \"save_graph\" keyword in WandbCallback mean?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":60,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600299419448,
        "Poster_location":"Germany",
        "Poster_reputation_count":62.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>That is used to create log a <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/data-types\/graph\" rel=\"nofollow noreferrer\">wandb.Graph<\/a> of the model. This class is typically used for saving and diplaying neural net models. It represents the graph as an array of nodes and edges. The nodes can have labels that can be visualized by wandb. Here's an example of the graph that it produces: <a href=\"https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model\" rel=\"nofollow noreferrer\">https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model<\/a><\/p>\n<p>Here's the code that does that within the callback. <a href=\"https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":12.1,
        "Solution_reading_time":11.75,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":74.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1483548930012,
        "Answerer_location":null,
        "Answerer_reputation_count":1875.0,
        "Answerer_view_count":146.0,
        "Challenge_adjusted_solved_time":0.4867144444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>[UPDATED] We are currently working on creating a Multi-Arm Bandit model for sign up optimization using the Build Your Own workflow that can be found here (basically substituting the model for our own):<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own<\/a><\/p>\n<p>Our project directory is set up as:\n<a href=\"https:\/\/i.stack.imgur.com\/QwaIQ.png\" rel=\"nofollow noreferrer\">Project Directory<\/a><\/p>\n<p>The issue is that I added some code including the dataclasses library that is only available since Python 3.7, and our project seems to keep using 3.6, causing a failure when running the Cloud Formation set up. The error in our Cloudwatch Logs is:<\/p>\n<pre><code>2021-03-31T11:04:11.077-05:00 Copy\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker\n    worker.init_process()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process\n    self.load_wsgi()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load\n    return self.load_wsgiapp()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/util.py&quot;, line 359, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/usr\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/opt\/program\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import predictor as myapp\n  File &quot;\/opt\/program\/predictor.py&quot;, line 9, in &lt;module&gt;\n    from model_contents.model import MultiArmBandit, BanditParameters\n  File &quot;\/opt\/program\/model_contents\/model.py&quot;, line 7, in &lt;module&gt;\n    from dataclasses import dataclass, field, asdict\nTraceback (most recent call last): File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker worker.init_process() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process self.load_wsgi() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi self.wsgi = self.app.wsgi() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi self.callable = self.load() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load return self.load_wsgiapp() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp return util.import_app(self.app_uri) File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/util.py&quot;, line 359, in import_app mod = importlib.import_module(module) File &quot;\/usr\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed File &quot;\/opt\/program\/wsgi.py&quot;, line 1, in &lt;module&gt; import predictor as myapp File &quot;\/opt\/program\/predictor.py&quot;, line 9, in &lt;module&gt; from model_contents.model import MultiArmBandit, BanditParameters File &quot;\/opt\/program\/model_contents\/model.py&quot;, line 7, in &lt;module&gt; from dataclasses import dataclass, field, asdict\n\n    2021-03-31T11:04:11.077-05:00\n\nCopy\nModuleNotFoundError: No module named 'dataclasses'\nModuleNotFoundError: No module named 'dataclasses'\n<\/code><\/pre>\n<p>Our updated Dockerfile is:<\/p>\n<pre><code># This is a Python 3 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:18.04\n\n# Retrieves information about what packages can be installed\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python3-pip \\\n         python3.8 \\\n         python3-setuptools \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Set python 3.8 as default\nRUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n\n# Here we get all python packages.\nRUN pip --no-cache-dir install numpy boto3 flask gunicorn\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# model_output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=&quot;\/opt\/program:${PATH}&quot;\nENV PYTHONPATH \/model_contents\n\n# Set up the program in the image\nCOPY bandit\/ \/opt\/program\/\nWORKDIR \/opt\/program\/\n\nRUN chmod +x \/opt\/program\/serve &amp;&amp; chmod +x \/opt\/program\/train\nLABEL git_tag=$GIT_TAG\n<\/code><\/pre>\n<p>I'm not sure if the nginx.conf file defaults to Py 3.6 so I want to make sure that it's not a big hassle to upgrade to Py 3.7 or 3.8 without many changes.<\/p>",
        "Challenge_closed_time":1617311610732,
        "Challenge_comment_count":1,
        "Challenge_created_time":1617309858560,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1617383727407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66911321",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":13.5,
        "Challenge_reading_time":88.3,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":77,
        "Challenge_solved_time":0.4867144444,
        "Challenge_title":"Upgrading Python version for running and creating custom container for Sagemaker Endpoint",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1398,
        "Challenge_word_count":615,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526288719836,
        "Poster_location":"Madrid, Spain",
        "Poster_reputation_count":33.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>You can update the Dockerfile after it install Python3.8 using <code>apt-get<\/code> with the following <code>RUN<\/code> commands<\/p>\n<pre><code>RUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n<\/code><\/pre>\n<p>The first <code>RUN<\/code> command will link <code>\/usr\/bin\/python<\/code> to <code>\/usr\/bin\/python3.8<\/code> and the second one will link <code>\/usr\/bin\/python3<\/code> to <code>\/usr\/bin\/python3.8<\/code><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1617312029390,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":7.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.3934813889,
        "Challenge_answer_count":2,
        "Challenge_body":"<h3>Scenario description<\/h3>\n\n<p>I'm trying to submit a training script to AzureML (want to use AmlCompute, but I'm starting\/testing locally first, for debugging purposes).<\/p>\n\n<p>The <code>train.py<\/code> script I have uses a custom package (<code>arcus.ml<\/code>) and I believe I have specified the right settings and dependencies, but still I get the error: <\/p>\n\n<p><code>User program failed with ModuleNotFoundError: No module named 'arcus.ml'<\/code><\/p>\n\n<h3>Code and reproduction<\/h3>\n\n<p>This the python code I have:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>name='test'\nscript_params = {\n    '--test-par': 0.2\n}\n\nest = Estimator(source_directory='.\/' + name,\n                   script_params=script_params,\n                   compute_target='local',\n                   entry_script='train.py',\n                   pip_requirements_file='requirements.txt',\n                   conda_packages=['scikit-learn','tensorflow', 'keras'])\n\nrun = exp.submit(est)\nprint(run.get_portal_url())\n<\/code><\/pre>\n\n<p>This is the (fully simplified) train.py script in the <code>test<\/code>directory:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from arcus.ml import dataframes as adf\nfrom azureml.core import Workspace, Dataset, Datastore, Experiment, Run\n\n# get hold of the current run\nrun = Run.get_context()\nws = run.get_environment()\n\nprint('training finished')\n<\/code><\/pre>\n\n<p>And this is my requirements.txt file<\/p>\n\n<pre><code>arcus-azureml\narcus-ml\nnumpy\npandas\nazureml-core\ntqdm\njoblib\nscikit-learn\nmatplotlib\ntensorflow\nkeras\n<\/code><\/pre>\n\n<h3>Logs<\/h3>\n\n<p>In the logs file of the run, I can see this section, sot it seems the external module is being installed anyhow.<\/p>\n\n<pre><code>Collecting arcus-azureml\n  Downloading arcus_azureml-1.0.3-py3-none-any.whl (3.1 kB)\nCollecting arcus-ml\n  Downloading arcus_ml-1.0.6-py3-none-any.whl (2.1 kB)\n<\/code><\/pre>",
        "Challenge_closed_time":1591043858447,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591043054577,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62140446",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.9,
        "Challenge_reading_time":24.08,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":0.2232972222,
        "Challenge_title":"Dependency missing when running AzureML Estimator in docker environment",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":208,
        "Challenge_word_count":189,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>I think this error isn't necessarily about Azure ML. I think the error has to do w\/ the difference b\/w using a hyphen and a period in your package name. But I'm a python packaging newb. \nIn a new conda environment on my laptop, I ran the following<\/p>\n\n<pre><code>&gt; conda create -n arcus python=3.6 -y\n&gt; conda activate arcus\n&gt; pip install arcus-ml\n&gt; python\n&gt;&gt;&gt; from arcus.ml import dataframes as adf\nModuleNotFoundError: No module named 'arcus'\n<\/code><\/pre>\n\n<p>When I look in the env's site packages folder, I didn't see the <code>arcus\/ml<\/code> folder structure I was expecting. There's no arcus code there at all, only the <code>.dist-info<\/code> file<\/p>\n\n<h3><code>~\/opt\/anaconda3\/envs\/arcus\/lib\/python3.6\/site-packages<\/code><\/h3>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/caExn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/caExn.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1591044471110,
        "Solution_link_count":2.0,
        "Solution_readability":8.6,
        "Solution_reading_time":11.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":118.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":35.0730002778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own%20example\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own example<\/a> for product recommendations.<\/p>\n\n<p>I want to use the SVD from <a href=\"https:\/\/pypi.org\/project\/scikit-surprise\/\" rel=\"nofollow noreferrer\">scikit-surprise<\/a> library on Sagemaker.<\/p>\n\n<pre><code>from surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\n<\/code><\/pre>\n\n<p>I added the scikit-surprise package in the Dockerfile, but i am getting the following errors:<\/p>\n\n<h1>Dockerfile:<\/h1>\n\n<pre><code># Build an image that can do training and inference in SageMaker\n# This is a Python 2 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:16.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Here we get all python packages.\n# There's substantial overlap between scipy and numpy that we eliminate by\n# linking them together. Likewise, pip leaves the install caches populated which uses\n# a significant amount of space. These optimizations save a fair amount of space in the\n# image, which reduces start up time.\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp; \\\n    pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp; \\\n        (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nRUN pip install scikit-surprise\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=\"\/opt\/program:${PATH}\"\n\n# Set up the program in the image\nCOPY products_recommender \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n\n<h1>Docker build and deploy :<\/h1>\n\n<pre><code>fullname:XXXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender:latest\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\nSending build context to Docker daemon  67.58kB\nStep 1\/10 : FROM ubuntu:16.04\n ---&gt; 13c9f1285025\nStep 2\/10 : MAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n ---&gt; Using cache\n ---&gt; 44baf3286201\nStep 3\/10 : RUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends          wget          python          nginx          ca-certificates     &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n ---&gt; Using cache\n ---&gt; 8983fa906515\nStep 4\/10 : RUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp;     pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp;         (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp;         rm -rf \/root\/.cache\n ---&gt; Using cache\n ---&gt; 9dbfedf02b57\nStep 5\/10 : RUN pip install scikit-surprise\n ---&gt; Running in 82295cb0affe\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\nCollecting scikit-surprise\n  Downloading https:\/\/files.pythonhosted.org\/packages\/f5\/da\/b5700d96495fb4f092be497f02492768a3d96a3f4fa2ae7dea46d4081cfa\/scikit-surprise-1.1.0.tar.gz (6.4MB)\nCollecting joblib&gt;=0.11 (from scikit-surprise)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/28\/5c\/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf\/joblib-0.14.1-py2.py3-none-any.whl (294kB)\nRequirement already satisfied: numpy&gt;=1.11.2 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.16.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.2.1)\nRequirement already satisfied: six&gt;=1.10.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.12.0)\nBuilding wheels for collected packages: scikit-surprise\n  Building wheel for scikit-surprise (setup.py): started\n  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n  ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-Bb1_iT --python-tag cp27:\n  ERROR: running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build\/lib.linux-x86_64-2.7\n  creating build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running egg_info\n  writing requirements to scikit_surprise.egg-info\/requires.txt\n  writing scikit_surprise.egg-info\/PKG-INFO\n  writing top-level names to scikit_surprise.egg-info\/top_level.txt\n  writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n  writing entry points to scikit_surprise.egg-info\/entry_points.txt\n  reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running build_ext\n  building 'surprise.similarities' extension\n  creating build\/temp.linux-x86_64-2.7\n  creating build\/temp.linux-x86_64-2.7\/surprise\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n  ----------------------------------------\n  ERROR: Failed building wheel for scikit-surprise\n  Running setup.py clean for scikit-surprise\nFailed to build scikit-surprise\nInstalling collected packages: joblib, scikit-surprise\n  Running setup.py install for scikit-surprise: started\n    Running setup.py install for scikit-surprise: finished with status 'error'\n    ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile:\n    ERROR: running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib.linux-x86_64-2.7\n    creating build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running egg_info\n    writing requirements to scikit_surprise.egg-info\/requires.txt\n    writing scikit_surprise.egg-info\/PKG-INFO\n    writing top-level names to scikit_surprise.egg-info\/top_level.txt\n    writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n    writing entry points to scikit_surprise.egg-info\/entry_points.txt\n    reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    reading manifest template 'MANIFEST.in'\n    writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running build_ext\n    building 'surprise.similarities' extension\n    creating build\/temp.linux-x86_64-2.7\n    creating build\/temp.linux-x86_64-2.7\/surprise\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n    unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n    ----------------------------------------\nERROR: Command \"\/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in \/tmp\/pip-install-VsuzGr\/scikit-surprise\/\nWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nThe command '\/bin\/sh -c pip install scikit-surprise' returned a non-zero code: 1\nThe push refers to repository [XXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender]\n89c1adca7d35: Layer already exists \nddcb6879486f: Layer already exists \n4a02efecad74: Layer already exists \n92d3f22d44f3: Layer already exists \n10e46f329a25: Layer already exists \n24ab7de5faec: Layer already exists \n1ea5a27b0484: Layer already exists \nlatest: digest: sha256:5ed35f1964d10f13bc8a05d379913c24195ea31ec848157016381fbd1bb12f28 size: 1782\n<\/code><\/pre>",
        "Challenge_closed_time":1579273412888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579147150087,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59762829",
        "Challenge_link_count":7,
        "Challenge_participation_count":1,
        "Challenge_readability":20.5,
        "Challenge_reading_time":212.31,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":223,
        "Challenge_solved_time":35.0730002778,
        "Challenge_title":"AWS Sagemaker scikit_bring_your_own example",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":341,
        "Challenge_word_count":1082,
        "Platform":"Stack Overflow",
        "Poster_created_time":1241005356852,
        "Poster_location":"Kuala Lumpur, Malaysia",
        "Poster_reputation_count":15794.0,
        "Poster_view_count":1032.0,
        "Solution_body":"<p>The 'x86_64-linux-gnu-gcc' binary can't be found in environment where you're building the container. Make sure that gcc is installed, and that you use the right name (gcc?).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":2.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":27.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1389049461896,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":455.0,
        "Answerer_view_count":108.0,
        "Challenge_adjusted_solved_time":3574.5224019445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've so far seen people using tensorflow in Azure using in this <a href=\"http:\/\/www.mikelanzetta.com\/tensorflow-on-azure-using-docker.html\" rel=\"nofollow\">link<\/a>.\nAlso using the advantage of ubuntu in windows tensorflow can be run on\nwindows pc as well.Here is the <a href=\"http:\/\/www.hanselman.com\/blog\/PlayingWithTensorFlowOnWindows.aspx\" rel=\"nofollow\">link<\/a>.\nHowever during a conversation with Windows Azure engineer Hai Ning it came out\nthat \"Azure ML PaaS VMs use Windows OS; TensorFlow is not supported on Windows as of now.\"\nHence there is no direct way of running tensorflow in Azure ML.\nIs there any work around anyone figured out that allows running tensorflow in Azure ML.<\/p>",
        "Challenge_closed_time":1486144727963,
        "Challenge_comment_count":0,
        "Challenge_created_time":1473271529687,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1473276447316,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39376560",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.28,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3575.88841,
        "Challenge_title":"How to call Tensorflow in Azure ML",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1743,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1435075201580,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation_count":546.0,
        "Poster_view_count":59.0,
        "Solution_body":"<p>Quick update for you. As of TensorFlow r0.12 there is now a native TensorFlow package for Windows. I have it running successfully on my Windows 10 laptop. See this <a href=\"https:\/\/developers.googleblog.com\/2016\/11\/tensorflow-0-12-adds-support-for-windows.html\" rel=\"nofollow noreferrer\">blog post<\/a> from Google for more information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.4,
        "Solution_reading_time":4.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":231.7294944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have two datasets with multiple columns. I would like to join the two tables with the following keys: zip code, year, month, data, hour<\/p>\n\n<p>However whenever I use a <strong>Join Module<\/strong> on these two tables, the Join doesn't happen, and I just get a Table with Columns from Right Table with empty values.<\/p>\n\n<p>Here is the R equivalent of what I am trying to do:<\/p>\n\n<pre><code>YX &lt;- leftTableDT\nYX %&lt;&gt;% merge( rightTableDT, all.x = TRUE, by=c('zip','year','month','day','hour') )\n<\/code><\/pre>\n\n<p>Any ideas on why Join Module in Azure ML Studio doesn't work for multiple keys?<\/p>",
        "Challenge_closed_time":1493816926630,
        "Challenge_comment_count":0,
        "Challenge_created_time":1492982700450,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/43576656",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.21,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":231.7294944445,
        "Challenge_title":"Join 2 tables with with mutiple keys in Azure ML Studio",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":582,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1281811007747,
        "Poster_location":"Capitola, CA",
        "Poster_reputation_count":3675.0,
        "Poster_view_count":638.0,
        "Solution_body":"<p>Double-check that you've selected \"Allow duplicates and preserve column order in selection\" in column selection options, so it matches the columns in listed order.<\/p>\n\n<p>Also, you could try Apply SQL Transformation module to join datasets.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.5,
        "Solution_reading_time":3.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":35.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1601729162436,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":887.0,
        "Answerer_view_count":130.0,
        "Challenge_adjusted_solved_time":9.0815319445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>If we have an AzureML Pipeline published, how can we trigger it from Azure DevOps <strong>without using Python Script Step or Azure CLI Step<\/strong>?<\/p>\n<p>The AzureML Steps supported natively in Azure DevOps include Model_Deployment and Model_Profiling.<\/p>\n<p>Is there any step in Azure DevOps which can be used to directly trigger a published Azure Machine Learning Pipeline while maintaining capabilities like using Service Connections and passing environmental variables, Gated Release (Deployment)?<\/p>\n<p>Edit:\nThis process can then be used to run as an agentless job.<\/p>",
        "Challenge_closed_time":1612256282836,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612203126923,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1612249107728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65997961",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":8.05,
        "Challenge_score_count":2,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":14.7655313889,
        "Challenge_title":"How to trigger an AzureML Pipeline from Azure DevOps?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1923,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>Assumptions:<\/p>\n<ol>\n<li>An AzureML Pipeline is published and the REST endpoint is ready- To be referred to in this answer as &lt;AML_PIPELINE_REST_URI&gt;. And Published Pipeline ID is also ready- To be referred to in this answer as &lt;AML_PIPELINE_ID&gt;<\/li>\n<li>You have the Azure Machine Learning Extension installed: <a href=\"https:\/\/marketplace.visualstudio.com\/items?itemName=ms-air-aiagility.vss-services-azureml&amp;ssr=false#review-details\" rel=\"nofollow noreferrer\">Azure Machine Learning Extension<\/a><\/li>\n<\/ol>\n<p>To Invoke the Azure Machine Learning Pipeline we use the <code>Invoke ML Pipeline<\/code> step available in Azure DevOps. It is available when running an Agentless Job.<\/p>\n<p>To trigger it the workflow is as follows:<\/p>\n<ol>\n<li>Create a New Pipeline. Using the Classic Editor, delete the default Agent Job 1 stage.\n<a href=\"https:\/\/i.stack.imgur.com\/phzL3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/phzL3.png\" alt=\"enter image description here\" \/><\/a><\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/QkiPY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QkiPY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ol start=\"2\">\n<li><p>Add an agentless job:\n<a href=\"https:\/\/i.stack.imgur.com\/0PXwg.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0PXwg.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Add a task to this Agentless Job:\n<a href=\"https:\/\/i.stack.imgur.com\/trW7j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/trW7j.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Use AzureML Published Pipeline Task:\n<a href=\"https:\/\/i.stack.imgur.com\/3rl4z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3rl4z.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Use the Service Connection Mapped to the AML Workspace. You can find more on this at the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/library\/service-endpoints?view=azure-devops&amp;tabs=yaml\" rel=\"nofollow noreferrer\">official documentation<\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/mnV36.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mnV36.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Choose the Pipeline to trigger using the &lt;AML_PIPELINE_ID&gt;:\n<a href=\"https:\/\/i.stack.imgur.com\/fbpQW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fbpQW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Give The experiment name and Pipeline Parameters if any:\n<a href=\"https:\/\/i.stack.imgur.com\/og1kx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/og1kx.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>That's it, you can Save and Queue:\n<a href=\"https:\/\/i.stack.imgur.com\/iCwdl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/iCwdl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<\/ol>\n<p>Alternatively, you can simply use the following jobs:<\/p>\n<pre><code>- job: Job_2\n  displayName: Agentless job\n  pool: server\n  steps:\n  - task: MLPublishedPipelineRestAPITask@0\n    displayName: Invoke ML pipeline\n    inputs:\n      connectedServiceName: &lt;REDACTED-AML-WS-Level-Service_Connection-ID&gt;\n      PipelineId: &lt;AML_PIPELINE_ID&gt;\n      ExperimentName: experimentname\n      PipelineParameters: ''\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1612281801243,
        "Solution_link_count":20.0,
        "Solution_readability":14.9,
        "Solution_reading_time":45.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":26.0,
        "Solution_word_count":299.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1239374952692,
        "Answerer_location":"Sydney, Australia",
        "Answerer_reputation_count":30129.0,
        "Answerer_view_count":2937.0,
        "Challenge_adjusted_solved_time":23.0400269445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use the R <code>sentimentr<\/code> package on Azure ML Studio. As this package is not supported, I'm trying to install it and its dependencies as described <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/execute-r-script#bkmk_AddingANewPackage\" rel=\"nofollow noreferrer\">in the documentation<\/a>.<\/p>\n\n<p>The steps that I have performed are:<\/p>\n\n<ul>\n<li><p>downloaded Windows binaries from the R Open 3.4.4 snapshot at <a href=\"https:\/\/mran.microsoft.com\/timemachine\" rel=\"nofollow noreferrer\">CRAN time machine<\/a><\/p>\n\n<ul>\n<li><code>sentimentr_2.2.3.zip<\/code><\/li>\n<li><code>syuzhet_1.0.4.zip<\/code><\/li>\n<li><code>textclean_0.6.3.zip<\/code><\/li>\n<li><code>lexicon_0.7.4.zip<\/code><\/li>\n<li><code>textshape_1.5.0.zip<\/code> <\/li>\n<\/ul><\/li>\n<li><p>zipped those zip files into a zipped folder <code>packages.zip<\/code><\/p><\/li>\n<li>uploaded <code>packages.zip<\/code> as a dataset to Microsoft Azure ML Studio<\/li>\n<\/ul>\n\n<p>In my ML experiment I connect the <code>packages.zip<\/code> dataset to the \"Script Bundle (Zip)\" input port on \"Execute R Script\" and include this code:<\/p>\n\n<pre><code># install R package contained in src  \ninstall.packages(\"src\/lexicon_0.7.4.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\ninstall.packages(\"src\/textclean_0.6.3.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\ninstall.packages(\"src\/textshape_1.5.0.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\ninstall.packages(\"src\/syuzhet_1.0.4.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\ninstall.packages(\"src\/sentimentr_2.2.3.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\n# load libraries\nlibrary(sentimentr, lib.loc = \".\", verbose = TRUE)\n<\/code><\/pre>\n\n<p>The experiment runs successfully, until I include a function from <code>sentimentr<\/code>:<\/p>\n\n<pre><code>mydata &lt;- mydata %&gt;%\n  get_sentences() %&gt;%\n  sentiment()\n<\/code><\/pre>\n\n<p>This gives the error:<\/p>\n\n<blockquote>\n  <p>there is no package called 'textshape'<\/p>\n<\/blockquote>\n\n<p>Which is difficult to understand given that the output log does not indicate an issue with the packages:<\/p>\n\n<pre><code>[Information]         The following files have been unzipped for sourcing in path=[\"src\"]:\n[Information]                           Name  Length                Date\n[Information]         1 sentimentr_2.2.3.zip 3366245 2019-08-07 14:57:00\n[Information]         2    syuzhet_1.0.4.zip 2918474 2019-08-07 15:05:00\n[Information]         3  textclean_0.6.3.zip 1154814 2019-08-07 15:13:00\n[Information]         4    lexicon_0.7.4.zip 4551995 2019-08-07 15:17:00\n[Information]         5  textshape_1.5.0.zip  463095 2019-08-07 15:42:00\n[Information]         Loading objects:\n[Information]           port1\n[Information]         [1] \"Loading variable port1...\"\n[Information]         package 'lexicon' successfully unpacked and MD5 sums checked   \n[Information]         package 'textclean' successfully unpacked and MD5 sums checked\n[Information]         package 'textshape' successfully unpacked and MD5 sums checked\n[Information]         package 'syuzhet' successfully unpacked and MD5 sums checked\n[Information]         package 'sentimentr' successfully unpacked and MD5 sums checked\n<\/code><\/pre>\n\n<p>Has anyone seen this, or similar issues? Is it possible that \"successfully unpacked\" is not the same as successfully installed and usable?<\/p>",
        "Challenge_closed_time":1565302474167,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565219530070,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57403281",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":42.37,
        "Challenge_score_count":2,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":23.0400269445,
        "Challenge_title":"Installing textshape package for Microsoft R Open 3.4.4 on Azure ML Studio",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":157,
        "Challenge_word_count":337,
        "Platform":"Stack Overflow",
        "Poster_created_time":1239374952692,
        "Poster_location":"Sydney, Australia",
        "Poster_reputation_count":30129.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>I can now answer my own question thanks to <a href=\"https:\/\/twitter.com\/bryan_hepworth\/status\/1159432174225055749\" rel=\"nofollow noreferrer\">a hint on Twitter<\/a> from @bryan_hepworth.<\/p>\n\n<p>The R packages were installed correctly, but not in the standard library location. So when a function from <code>sentimentr<\/code> runs, R tries to load the dependency package <code>textshape<\/code>:<\/p>\n\n<pre><code>library(textshape)\n<\/code><\/pre>\n\n<p>Which of course does not exist <em>in the standard location<\/em> as Azure ML does not support it.<\/p>\n\n<p>The solution is to load <code>textshape<\/code> explicitly from its installed location:<\/p>\n\n<pre><code>library(textshape, lib.loc = \".\")\n<\/code><\/pre>\n\n<p>So the solution is: explicitly load packages that you installed at the start of your R code, rather than letting R try to load them as dependencies, which will fail.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.4,
        "Solution_reading_time":11.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1393524211332,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":745.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":968.2322455556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to make transfer learning method on MXNet on Sagemaker instance. Train and serve start locally without any problem and I'm using that python code to predict:<\/p>\n\n<pre><code>def predict_mx(net, fname):\n    with open(fname, 'rb') as f:\n      img = image.imdecode(f.read())\n      plt.imshow(img.asnumpy())\n      plt.show()\n    data = transform(img, -1, test_augs)\n    plt.imshow(data.transpose((1,2,0)).asnumpy()\/255)\n    plt.show()\n    data = data.expand_dims(axis=0)\n    return net.predict(data.asnumpy().tolist())\n<\/code><\/pre>\n\n<p>I checked <code>data.asnumpy().tolist()<\/code> that is ok and pyplot draw images (firts is the original image, the second is the resized image). But <code>net.predict<\/code> raise an error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nJSONDecodeError                           Traceback (most recent call last)\n&lt;ipython-input-171-ea0f1f5bdc72&gt; in &lt;module&gt;()\n----&gt; 1 predict_mx(predictor.predict, '.\/data2\/burgers-imgnet\/00103785.jpg')\n\n&lt;ipython-input-170-150a72b14997&gt; in predict_mx(net, fname)\n     30     plt.show()\n     31     data = data.expand_dims(axis=0)\n---&gt; 32     return net(data.asnumpy().tolist())\n     33 \n\n~\/Projects\/Lab\/ML\/AWS\/v\/lib64\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data)\n     89         if self.deserializer is not None:\n     90             # It's the deserializer's responsibility to close the stream\n---&gt; 91             return self.deserializer(response_body, response['ContentType'])\n     92         data = response_body.read()\n     93         response_body.close()\n\n~\/Projects\/Lab\/ML\/AWS\/v\/lib64\/python3.6\/site-packages\/sagemaker\/predictor.py in __call__(self, stream, content_type)\n    290         \"\"\"\n    291         try:\n--&gt; 292             return json.load(codecs.getreader('utf-8')(stream))\n    293         finally:\n    294             stream.close()\n\n\/usr\/lib64\/python3.6\/json\/__init__.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    297         cls=cls, object_hook=object_hook,\n    298         parse_float=parse_float, parse_int=parse_int,\n--&gt; 299         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n    300 \n    301 \n\n\/usr\/lib64\/python3.6\/json\/__init__.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    352             parse_int is None and parse_float is None and\n    353             parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354         return _default_decoder.decode(s)\n    355     if cls is None:\n    356         cls = JSONDecoder\n\n\/usr\/lib64\/python3.6\/json\/decoder.py in decode(self, s, _w)\n    337 \n    338         \"\"\"\n--&gt; 339         obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n    340         end = _w(s, end).end()\n    341         if end != len(s):\n\n\/usr\/lib64\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n    355             obj, end = self.scan_once(s, idx)\n    356         except StopIteration as err:\n--&gt; 357             raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n    358         return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n<\/code><\/pre>\n\n<p>I tried to json.dumps my data, and there is no problem with that.<\/p>\n\n<p>Note that I didn't deployed the service on AWS yet, I want to be able to test the model and prediction locally before to make a larger train and to serve it later.<\/p>\n\n<p>Thanks for your help<\/p>",
        "Challenge_closed_time":1531584172427,
        "Challenge_comment_count":4,
        "Challenge_created_time":1528098536343,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50675708",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":10.9,
        "Challenge_reading_time":41.5,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":968.2322455556,
        "Challenge_title":"Sagemaker Predict on local instance, JSON Error",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1358,
        "Challenge_word_count":332,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340280616088,
        "Poster_location":"Laval, France",
        "Poster_reputation_count":2835.0,
        "Poster_view_count":281.0,
        "Solution_body":"<p>The call to <strong>net.predict<\/strong> is working fine. <\/p>\n\n<p>It seems that you are using the SageMaker Python SDK <strong>predict_fn<\/strong> for hosting. After the <strong>predict_fn<\/strong> is invoked, the MXNet container will try to serialize your prediction to JSON before sending it back to the client. You can see code that does that here: <a href=\"https:\/\/github.com\/aws\/sagemaker-mxnet-container\/blob\/master\/src\/mxnet_container\/serve\/transformer.py#L132\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-mxnet-container\/blob\/master\/src\/mxnet_container\/serve\/transformer.py#L132<\/a><\/p>\n\n<p>The container is failing to serialize because <strong>net.predict<\/strong> does not return a serializable object. You can solve this issue by returning a list instead:<\/p>\n\n<pre><code>return net.predict(data.asnumpy().tolist()).asnumpy().tolist()\n<\/code><\/pre>\n\n<p>Another alternative is to use a <strong>transform_fn<\/strong> instead of <strong>prediction_fn<\/strong> so you can handle the output serialization yourself. You can see an example of a <strong>transform_fn<\/strong> here <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/e93eff66626c0ab1f292048451c4c3ac7c39a121\/examples\/cli\/host\/script.py#L41\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/e93eff66626c0ab1f292048451c4c3ac7c39a121\/examples\/cli\/host\/script.py#L41<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":16.4,
        "Solution_reading_time":18.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":114.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1341441916656,
        "Answerer_location":null,
        "Answerer_reputation_count":5985.0,
        "Answerer_view_count":161.0,
        "Challenge_adjusted_solved_time":0.5572202778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to train a model using Amazon Sagemaker (xgboost: eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'). But I always get the same error message shortly after starting the training job:<\/p>\n\n<blockquote>\n  <p>\"ClientError: Hidden file found in the data path! Remove that before\n  training.\"<\/p>\n<\/blockquote>\n\n<p>The S3 console shows that output path is empty (I also tried to create a new directory to no avail). Versioning is not enabled for the bucket.<\/p>\n\n<p>Surprisingly, google finds nothing under this error message.<\/p>\n\n<p>I have configured the input and outputs as follows:<\/p>\n\n<pre><code>   \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3:\/\/{}\/{}-inputdata\/train\".format(s3_utils.bucket, LABEL)\n                }\n            },\n            \"ContentType\": \"csv\",\n            \"CompressionType\": \"None\"\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3:\/\/{}\/{}-inputdata\/validation\".format(s3_utils.bucket, LABEL)\n                }\n            },\n            \"ContentType\": \"csv\",\n            \"CompressionType\": \"None\"\n        }\n    ],\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": \"s3:\/\/{}\/{}-xgboost-output\".format(s3_utils.bucket, LABEL)        },\n<\/code><\/pre>\n\n<p>The field<\/p>\n\n<pre><code>    \"RoleArn\": role,\n<\/code><\/pre>\n\n<p>where role comes from<\/p>\n\n<pre><code>    from sagemaker import get_execution_role\n    role = get_execution_role()\n<\/code><\/pre>\n\n<p>and is<\/p>\n\n<pre><code>    arn:aws:iam::&lt;ACCOUNT&gt;:role\/service-role\/AmazonSageMaker-ExecutionRole-&lt;HIDDEN&gt;\n<\/code><\/pre>\n\n<p>Here is a screenshot showing the data-path:\n<a href=\"https:\/\/i.stack.imgur.com\/bs8kl.png\" rel=\"nofollow noreferrer\">S3 dashboard view of data-path<\/a>. The two csv files is all there is. In particular, there is no empty \"directory\" which might be what \"hidden file\" could mean.<\/p>",
        "Challenge_closed_time":1531417705336,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531339613883,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1531415699343,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51293471",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.0,
        "Challenge_reading_time":24.58,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":21.6920702778,
        "Challenge_title":"AWS Sagemaker - \"Hidden file found in the data path! Remove that before training.\"",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1578,
        "Challenge_word_count":191,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531338135003,
        "Poster_location":"Netherlands",
        "Poster_reputation_count":23.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Ok, the prefix you set in the <code>S3Uri<\/code> matters here. Based on your screenshot I think your bucket looks something like this (in tree form):<\/p>\n\n<pre><code>s3:\/\/bucket\n\u2514\u2500\u2500 LABEL-inputdata\n    \u251c\u2500\u2500 train.csv\n    \u2514\u2500\u2500 validation.csv\n<\/code><\/pre>\n\n<p>Based on your <code>InputDataConfig<\/code> above, SageMaker has to download it to folders on the filesystem for the <code>xgboost<\/code> training algorithm to run. It does so based on the channel names and on the <code>S3Uri<\/code> prefix you provided. The prefix is chopped off to determine the name of the folder\/file to download to. So, in your example, the <code>train<\/code> channel gets downloaded as:<\/p>\n\n<pre><code>\/opt\/ml\/input\/data\/train\/.csv\n<\/code><\/pre>\n\n<p>Finally, the <code>xgboost<\/code> implementation sees the <code>.csv<\/code> file as a hidden file and complains about it.<\/p>\n\n<p>To get it to work you could rearrange your data in s3 like so...<\/p>\n\n<pre><code>s3:bucket\n\u2514\u2500\u2500 LABEL-inputdata\n    \u251c\u2500\u2500 train\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 data.csv\n    \u2514\u2500\u2500 validation\n        \u2514\u2500\u2500 data.csv\n<\/code><\/pre>\n\n<p>.. and change your input data config to:<\/p>\n\n<pre><code>   \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3:\/\/{}\/{}-inputdata\/train\/\".format(s3_utils.bucket, LABEL)\n                }\n            },\n            \"ContentType\": \"csv\",\n            \"CompressionType\": \"None\"\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3:\/\/{}\/{}-inputdata\/validation\/\".format(s3_utils.bucket, LABEL)\n                }\n            },\n            \"ContentType\": \"csv\",\n            \"CompressionType\": \"None\"\n        }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":20.04,
        "Solution_score_count":5.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":168.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1267440784443,
        "Answerer_location":"Somewhere",
        "Answerer_reputation_count":15705.0,
        "Answerer_view_count":2150.0,
        "Challenge_adjusted_solved_time":0.1769625,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I can train a XGBoost model using Sagemaker images like so:<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.inputs import TrainingInput\nimport os\n\nfolder = r&quot;C:\\Somewhere&quot;\nos.chdir(folder)\n\ns3_prefix = 'some_model'\ns3_bucket_name = 'the_bucket'\ntrain_file_name = 'train.csv'\nval_file_name = 'val.csv'\nrole_arn = 'arn:aws:iam::482777693429:role\/bla_instance_role'\n\nregion_name = boto3.Session().region_name\n\ns3_input_train = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, train_file_name), content_type='csv')\ns3_input_val = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, val_file_name), content_type='csv')\n\nprint(type(s3_input_train))\n\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;13&quot;,\n        &quot;eta&quot;:&quot;0.15&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;reg:squarederror&quot;,\n        &quot;num_round&quot;:&quot;50&quot;}\n\noutput_path = 's3:\/\/{}\/{}\/output'.format(s3_bucket_name, s3_prefix)\n\n# 1.5-1\n# 1.3-1\nestimator = sagemaker.estimator.Estimator(image_uri=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;), \n                                          hyperparameters=hyperparameters,\n                                          role=role_arn,\n                                          instance_count=1, \n                                          instance_type='ml.m5.2xlarge',\n                                          #instance_type='local', \n                                          volume_size=1, # 1 GB \n                                          output_path=output_path)\n\nestimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n<\/code><\/pre>\n<p>This work for all versions 1.2-2, 1.3-1 and 1.5-1. Unfortunately the following code only works for version 1.2-2:<\/p>\n<pre><code>import boto3\nimport os\nimport pickle as pkl \nimport tarfile\nimport pandas as pd\nimport xgboost as xgb\n\nfolder = r&quot;C:\\Somewhere&quot;\nos.chdir(folder)\n\ns3_prefix = 'some_model'\ns3_bucket_name = 'the_bucket'\nmodel_path = 'output\/sagemaker-xgboost-2022-04-30-10-52-29-877\/output\/model.tar.gz'\nsession = boto3.Session(profile_name='default')\nsession.resource('s3').Bucket(s3_bucket_name).download_file('{}\/{}'.format(s3_prefix, model_path), 'model.tar.gz')\nt = tarfile.open('model.tar.gz', 'r:gz')\nt.extractall()\n\nmodel_file_name = 'xgboost-model'\nwith open(model_file_name, &quot;rb&quot;) as input_file:\ne = pkl.load(input_file) \n<\/code><\/pre>\n<p>Otherwise I get a:<\/p>\n<pre><code>_pickle.UnpicklingError: unpickling stack underflow\n<\/code><\/pre>\n<p>Am I missing something? Is my &quot;pickle loading code wrong&quot;?<\/p>\n<p>The version of xgboost is 1.6.0 where I run the pickle code.<\/p>",
        "Challenge_closed_time":1651318339852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651317702787,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72068059",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.4,
        "Challenge_reading_time":34.95,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":0.1769625,
        "Challenge_title":"cannot load pickle files for xgboost images of version > 1.2-2 in sagemaker - UnpicklingError",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":140,
        "Challenge_word_count":185,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>I found the solution <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/2952\" rel=\"nofollow noreferrer\">here<\/a>. I will leave it in case someone come accross the same issue.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.8,
        "Solution_reading_time":2.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":24.0835044444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a model as a Webservice in Azure ML.Its a simple one and all it does is do a linear Regression .The underlying code is python . Now i need to pass which all columns have to selected as independent variables, dynamically, from the client side . How may i do this in Azure ML studio?<\/p>",
        "Challenge_closed_time":1458632118243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1458567955050,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1458612577147,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36132719",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":4.2,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":17.8231091667,
        "Challenge_title":"Select columns dynamically in Azure ML model",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":238,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1422821109972,
        "Poster_location":"India",
        "Poster_reputation_count":1238.0,
        "Poster_view_count":172.0,
        "Solution_body":"<p>Based on my understanding, I think you want to dynamically get the selected columns data via request the Azure ML webservice with some parameters on the client.<\/p>\n\n<p>You can refer to the offical document <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-web-service-parameters\/\" rel=\"nofollow\">Use Azure Machine Learning Web Service Parameters<\/a> and the blog <a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2014\/11\/25\/azureml-web-service-parameters\/\" rel=\"nofollow\">AzureML Web Service Parameters<\/a> to know how to set and use the web service parameters to implement your needs via add the selected column names as array into the json parameter <code>GlobalParameters<\/code>.<\/p>\n\n<p>Meanwhile, there is a client sample on GitHub <a href=\"https:\/\/github.com\/nk773\/AzureML_RRSApp\" rel=\"nofollow\">https:\/\/github.com\/nk773\/AzureML_RRSApp<\/a>. Althought it was writen in Java, I think it is easy to understand, then you can rewrite in Python with <code>requests<\/code> package.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1458699277763,
        "Solution_link_count":4.0,
        "Solution_readability":16.5,
        "Solution_reading_time":13.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":113.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1569996433956,
        "Answerer_location":null,
        "Answerer_reputation_count":191.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":22.0598044444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use optuna lib in Python to optimise parameters for recommender systems' models. Those models are custom and look like standard fit-predict sklearn models (with methods get\/set params). <\/p>\n\n<p>What I do: simple objective function that selects two parameters from uniform int distribution, set these params to model, predicts the model (there no fit stage as it simple model that uses params only in predict stage) and calculates some metric. <\/p>\n\n<p>What I get: the first trial runs normal, it samples params and prints results to log. But on the second and next trial I have some strange errors (look code below) that I can't solve or google. When I run study on just 1 trial everything is okay.<\/p>\n\n<p>What I tried: to rearrange parts of objective function, put fit stage inside, try to calculate more simpler metrics - nothing helps. <\/p>\n\n<p>Here is my objective function: <\/p>\n\n<pre><code># getting train, test\n# fitting model\nself.model = SomeRecommender()\nself.model.fit(train, some_other_params)\n\ndef objective(trial: optuna.Trial):\n    # save study\n    if path is not None:\n        joblib.dump(study, some_path)\n\n    # sampling params\n    alpha = trial.suggest_uniform('alpha', 0, 100)\n    beta = trial.suggest_uniform('beta', 0, 100)\n\n    # setting params to model\n    params = {'alpha': alpha,\n              'beta': beta}\n    self.model.set_params(**params)\n\n    # getting predict\n    recs = self.model.predict(some_other_params)\n\n    # metric computing\n    metric_result = Metrics.hit_rate_at_k(recs, test, k=k)\n\n    return metric_result\n\n# starting study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=3, n_jobs=1)\n<\/code><\/pre>\n\n<p>That's what I get on three trials:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>[I 2019-10-01 12:53:59,019] Finished trial#0 resulted in value: 0.1. Current best value is 0.1 with parameters: {'alpha': 59.6135986324444, 'beta': 40.714559720597585}.\n[W 2019-10-01 13:39:58,140] Setting status of trial#1 as TrialState.FAIL because of the following error: AttributeError(\"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\")\nTraceback (most recent call last):\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/study.py\", line 448, in _run_trial\n    result = func(trial)\n  File \"\/Users\/roseaysina\/code\/project\/model.py\", line 100, in objective\n    'alpha', 0, 100)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 180, in suggest_uniform\n    return self._suggest(name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 453, in _suggest\n    self.study, trial, name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 127, in sample_independent\n    values, scores = _get_observation_pairs(study, param_name)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 558, in _get_observation_pairs\n    param_value = distribution.to_internal_repr(trial.params[param_name])\nAttributeError: '_BaseUniformDistribution' object has no attribute 'to_internal_repr'\n[W 2019-10-01 13:39:58,206] Setting status of trial#2 as TrialState.FAIL because of the following error: AttributeError(\"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\")\nTraceback (most recent call last):\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/study.py\", line 448, in _run_trial\n    result = func(trial)\n  File \"\/Users\/roseaysina\/code\/project\/model.py\", line 100, in objective\n    'alpha', 0, 100)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 180, in suggest_uniform\n    return self._suggest(name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 453, in _suggest\n    self.study, trial, name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 127, in sample_independent\n    values, scores = _get_observation_pairs(study, param_name)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 558, in _get_observation_pairs\n    param_value = distribution.to_internal_repr(trial.params[param_name])\nAttributeError: '_BaseUniformDistribution' object has no attribute 'to_internal_repr'\n<\/code><\/pre>\n\n<p>I can't understand where is the problem and why the first trial is working. Please, help. <\/p>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1570006754996,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569926663223,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":1569927339700,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58183158",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":61.64,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":22.2477147222,
        "Challenge_title":"How to fix error \"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\" - strange behaviour in optuna",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":666,
        "Challenge_word_count":446,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432898903270,
        "Poster_location":"Moscow, Russia",
        "Poster_reputation_count":125.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>Your code seems to have no problems.<\/p>\n\n<p>I ran a simplified version of your code (see below), and it worked well in my environment:<\/p>\n\n<pre><code>import optuna\n\ndef objective(trial: optuna.Trial):\n    # sampling params\n    alpha = trial.suggest_uniform('alpha', 0, 100)\n    beta = trial.suggest_uniform('beta', 0, 100)\n\n    # evaluating params\n    return alpha + beta\n\n# starting study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=3, n_jobs=1)\n<\/code><\/pre>\n\n<p>Could you tell me about your environment in order to investigate the problem? (e.g., OS, Python version, Python interpreter (CPython, PyPy, IronPython or Jython), Optuna version)<\/p>\n\n<blockquote>\n  <p>why the first trial is working.<\/p>\n<\/blockquote>\n\n<p>This error is raised by <a href=\"https:\/\/github.com\/pfnet\/optuna\/blob\/389a176c8cd1c860001a7a4562670006643e5e11\/optuna\/samplers\/tpe\/sampler.py#L558\" rel=\"noreferrer\">optuna\/samplers\/tpe\/sampler.py#558<\/a>, and this line is only executed when the number of completed trials in the study is greater than zero.<\/p>\n\n<p>BTW, you might be able to avoid this problem by using <code>RandomSampler<\/code> as follows:<\/p>\n\n<pre><code>sampler = optuna.samplers.RandomSampler()\nstudy = optuna.create_study(direction='maximize', sampler=sampler)\n<\/code><\/pre>\n\n<p>Notice that the optimization performance of <code>RandomSampler<\/code> tends to be worse than <code>TPESampler<\/code> that is the default sampler of Optuna.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.2,
        "Solution_reading_time":18.86,
        "Solution_score_count":5.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":153.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1432655047272,
        "Answerer_location":null,
        "Answerer_reputation_count":463.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":264.5982422222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>What is a is the curl command to make a POST request to sage-maker and receive a ML inference?<\/p>",
        "Challenge_closed_time":1514326523572,
        "Challenge_comment_count":1,
        "Challenge_created_time":1513373969900,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47840209",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":8.2,
        "Challenge_reading_time":1.73,
        "Challenge_score_count":11,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":264.5982422222,
        "Challenge_title":"How to Curl an Amazon Sagemaker Endpoint",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":6889,
        "Challenge_word_count":25,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443201378360,
        "Poster_location":null,
        "Poster_reputation_count":749.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>Rather than using curl, it's recommended that you use the SageMaker Runtime client to send data and get back inferences from a SageMaker Endpoint:<\/p>\n\n<p><a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html\" rel=\"nofollow noreferrer\">http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.7,
        "Solution_reading_time":4.86,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":28.8293777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a file I want to import into a Sagemaker Jupyter notebook python 3 instance for use.  The exact code would be 'import lstm.'  I can store the file in s3 (which would probably be ideal) or locally, whichever you prefer.  I have been searching the internet for a while and have been unable to find a solution to this.  I am actually just trying to run\/understand this code from Suraj Raval's youtube channel: <a href=\"https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot\" rel=\"nofollow noreferrer\">https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot<\/a>.  The 'import lstm' line is failing when I run, and I am trying to figure out how to make this work.  <\/p>\n\n<p>I have tried:\nfrom s3:\/\/... import lstm.  failed\nI have tried some boto3 methods and wasn't able to get it to work.  <\/p>\n\n<pre><code>import time\nimport threading\nimport lstm, etl, json. ##this line\nimport numpy as np\nimport pandas as pd\nimport h5py\nimport matplotlib.pyplot as plt\nconfigs = json.loads(open('configs.json').read())\ntstart = time.time()\n<\/code><\/pre>\n\n<p>I would just like to be able to import the lstm file and all the others into a Jupyter notebook instance.<\/p>",
        "Challenge_closed_time":1555252725128,
        "Challenge_comment_count":0,
        "Challenge_created_time":1555242786160,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55674959",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":14.66,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2.7608244444,
        "Challenge_title":"Importing a file into jupyterlabs from s3",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1863,
        "Challenge_word_count":180,
        "Platform":"Stack Overflow",
        "Poster_created_time":1555241852167,
        "Poster_location":"Chicago, IL, USA",
        "Poster_reputation_count":393.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>I think you should  be cloning the Github repo in SageMaker instance and not importing the files from S3. I was able to reproduce the Bitcoin Trading Bot notebook from SageMaker by cloning it. You can follow the below steps<\/p>\n\n<h3>Cloning Github Repo to SageMaker Notebook<\/h3>\n\n<ol>\n<li>Open JupyterLab from the AWS SageMaker console.<\/li>\n<li>From the JupyterLab  Launcher, open the Terminal.<\/li>\n<li>Change directory to SageMaker<\/li>\n<\/ol>\n\n<pre><code>cd ~\/SageMaker\n<\/code><\/pre>\n\n<ol start=\"4\">\n<li>Clone the BitCoin Trading Bot <a href=\"https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot\" rel=\"nofollow noreferrer\">git repo<\/a><\/li>\n<\/ol>\n\n<pre><code>git clone https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot.git\ncd Bitcoin_Trading_Bot\n<\/code><\/pre>\n\n<ol start=\"5\">\n<li>Now you can open the notebook <code>Bitcoin LSTM Prediction.ipynb<\/code> and select the Tensorflow Kernel to run the notebook.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YbKic.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YbKic.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<h3>Adding files from local machine to SageMaker Notebook<\/h3>\n\n<p>To add files from your local machine to SageMaker Notebook instance, you can use <a href=\"https:\/\/jupyterlab.readthedocs.io\/en\/stable\/user\/files.html\" rel=\"nofollow noreferrer\">file upload<\/a> functionality in JupyterLab<\/p>\n\n<h3>Adding files from S3 to SageMaker Notebook<\/h3>\n\n<p>To add files from S3 to SageMaker Notebook instance, use AWS CLI or Python SDK to upload\/download files. <\/p>\n\n<p>For example, to download <code>lstm.py<\/code> file from S3 to SageMaker using AWS CLI<\/p>\n\n<pre><code>aws s3 cp s3:\/\/mybucket\/bot\/src\/lstm.py .\n<\/code><\/pre>\n\n<p>Using <code>boto3<\/code> API<\/p>\n\n<pre><code>import boto3\ns3 = boto3.resource('s3')\ns3.meta.client.download_file('mybucket', 'bot\/src\/lstm.py', '.\/lstm.py')\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1555346571920,
        "Solution_link_count":5.0,
        "Solution_readability":12.1,
        "Solution_reading_time":24.44,
        "Solution_score_count":4.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":203.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1394547235287,
        "Answerer_location":"Warsaw, Poland",
        "Answerer_reputation_count":352.0,
        "Answerer_view_count":22.0,
        "Challenge_adjusted_solved_time":1.4516513889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm struggling with the DVC experiment management. Suppose the following scenario:<\/p>\n<p>I have <code>params.yaml<\/code> file:<\/p>\n<pre><code>recommendations:\n  k: 66\n  q: 5\n<\/code><\/pre>\n<p>I run the experiment with <code>dvc exp run -n exp_66<\/code>, and then I do <code>dvc exp push origin exp_66<\/code>. After this, I modify <code>params.yaml<\/code> file:<\/p>\n<pre><code>recommendations:\n  k: 99\n  q: 5\n<\/code><\/pre>\n<p>and then run another experiment <code>dvc exp run -n exp_99<\/code>, after which I commit with <code>dvc exp push origin exp_99<\/code>.<\/p>\n<p>Now, when I pull the corresponding branch with Git, I try to pull <code>exp_66<\/code> from dvc by running <code>dvc exp pull origin exp_66<\/code>. This does the pull (no error messages), but the content of the <code>params.yaml<\/code> file is with <code>k: 99<\/code> (and I would expect <code>k: 66<\/code>). What am I doing wrong? Does <code>git push<\/code> have to be executed after <code>dvc push<\/code>? Apart from that, I also found <code>dvc exp apply exp_66<\/code>, but I'm not sure what it does (it is suggested that after <code>apply<\/code> one should execute <code>git add .<\/code>, then <code>git commit<\/code>?<\/p>\n<p>I would really appreciate if you could write down the workflow with committing different experiments, pushing, pulling, applying, etc.<\/p>",
        "Challenge_closed_time":1646319959072,
        "Challenge_comment_count":1,
        "Challenge_created_time":1646314733127,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71338160",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.7,
        "Challenge_reading_time":17.29,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1.4516513889,
        "Challenge_title":"DVC Experiment management workflow",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":152,
        "Challenge_word_count":188,
        "Platform":"Stack Overflow",
        "Poster_created_time":1643373769632,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You did everything alright. In the end, after pulling, you can see that when using <code>dvc exp show<\/code> your experiments will be there. To restore the experiment available from your experiment list into your workspace, you simply need to run <code>dvc exp apply exp_66<\/code>. DVC will make sure that the changes corresponding to this experiment will be checked out.<\/p>\n<p>Your workflow seems correct so far. One addition: once you make sure one of the experiments is what you want to &quot;keep&quot; in git history, you can use <code>dvc exp branch {exp_id} {branch_name}<\/code> to create a separate branch for this experiment. Then you can use <code>git<\/code> commands to save the changes.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":8.77,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":110.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1522794798772,
        "Answerer_location":null,
        "Answerer_reputation_count":157.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":7.1225861111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>when I create a run using <code>mlflow.start_run()<\/code> ,even if my script is interrupted before executing <code>mlflow.end_run()<\/code>, the run gets tagged as finished instead of unfinished in Status?<\/p>",
        "Challenge_closed_time":1618223603527,
        "Challenge_comment_count":2,
        "Challenge_created_time":1618197962217,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67052295",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.8,
        "Challenge_reading_time":3.33,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":7.1225861111,
        "Challenge_title":"MLflow unfinished experiment saved as finished",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":332,
        "Challenge_word_count":32,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578750761196,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>When your notebook stops the run gets the status finished. However, if you want to continue logging metrics or artifacts to that run, you just need to use <code>mlflow.start_run(run_id=&quot;YourRunIDYouCanGetItFromUI&quot;)<\/code>. This is explained in the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":5.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":38.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":24.1372647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am passing the values of lables as below to create a featurestore with labels. But after creation of the featurestore, I do not see the featurestore created with labels. Is it still not supported in VertexAI<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>    fs = aiplatform.Featurestore.create(\n        featurestore_id=featurestore_id,\n        labels=dict(project='retail', env='prod'),\n        online_store_fixed_node_count=online_store_fixed_node_count,\n        sync=sync\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/viOSu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/viOSu.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1651709813300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651616413553,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1651623411107,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72106030",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":9.21,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":25.9443741667,
        "Challenge_title":"I am not able to create a feature store in vertexAI using labels",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":83,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530457174832,
        "Poster_location":null,
        "Poster_reputation_count":1043.0,
        "Poster_view_count":212.0,
        "Solution_body":"<p>As mentioned in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-featurestores\" rel=\"nofollow noreferrer\">featurestore documentation<\/a>:<\/p>\n<blockquote>\n<p>A <strong>featurestore<\/strong> is a top-level container for entity types, features,\nand feature values.<\/p>\n<\/blockquote>\n<p>With this, the GCP console UI &quot;labels&quot; are the &quot;labels&quot; at the <strong>Feature<\/strong> level.<\/p>\n<p>Once a <strong>featurestore<\/strong> is created, you will need to create an <strong>entity<\/strong> and then create a <strong>Feature<\/strong> that has the <em>labels<\/em> parameter as shown on the below sample python code.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from google.cloud import aiplatform\n\ntest_label = {'key1' : 'value1'}\n\ndef create_feature_sample(\n    project: str,\n    location: str,\n    feature_id: str,\n    value_type: str,\n    entity_type_id: str,\n    featurestore_id: str,\n):\n\n    aiplatform.init(project=project, location=location)\n\n    my_feature = aiplatform.Feature.create(\n        feature_id=feature_id,\n        value_type=value_type,\n        entity_type_name=entity_type_id,\n        featurestore_id=featurestore_id,\n        labels=test_label,\n    )\n\n    my_feature.wait()\n\n    return my_feature\n\ncreate_feature_sample('your-project','us-central1','test_feature3','STRING','test_entity3','test_fs3')\n<\/code><\/pre>\n<p>Below is the screenshot of the GCP console which shows that <em>labels<\/em> for <strong>test_feature3<\/strong> feature has the values defined in the above sample python code.\n<a href=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You may refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-features#create-feature\" rel=\"nofollow noreferrer\">creation of feature documentation<\/a> using python for more details.<\/p>\n<p>On the other hand, you may still view the <em>labels<\/em> you defined for your featurestore using the REST API as shown on the below sample.<\/p>\n<pre><code>curl -X GET \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n&quot;https:\/\/&lt;your-location&gt;-aiplatform.googleapis.com\/v1\/projects\/&lt;your-project&gt;\/locations\/&lt;your-location&gt;\/featurestores&quot;\n<\/code><\/pre>\n<p>Below is the result of the REST API which also shows the value of the <em>labels<\/em> I defined for my &quot;test_fs3&quot; featurestore.\n<a href=\"https:\/\/i.stack.imgur.com\/gW45X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gW45X.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1651710305260,
        "Solution_link_count":7.0,
        "Solution_readability":16.7,
        "Solution_reading_time":34.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":226.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1577353307072,
        "Answerer_location":null,
        "Answerer_reputation_count":491.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":4.3477575,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to load files as a dataset in the GUI of Azure ML Studio. These parquet files have been created through Spark.<\/p>\n<p>In my folder, Spark creates files such as &quot;_SUCCESS&quot; or &quot;_committed_8998000&quot;.<\/p>\n<p>Azure ML Studio is not able to read them or ignore them and tells me:<\/p>\n<pre><code>The provided file(s) have invalid byte(s) for the specified file encoding.\n{\n  &quot;message&quot;: &quot; &quot;\n}\n<\/code><\/pre>\n<p>I selected &quot;Ignore unmatched files path&quot; and yet, it still does not work.<\/p>\n<p>If I remove the &quot;_SUCCESS&quot; and other Spark files, it works.<\/p>",
        "Challenge_closed_time":1601483944070,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601468292143,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64137409",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.0,
        "Challenge_reading_time":9.15,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":4.3477575,
        "Challenge_title":"How can I create an Azure dataset in Azure ML studio (through the GUI) from a parquet file created with Azure Spark",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":178,
        "Challenge_word_count":109,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423640080283,
        "Poster_location":"Lyon, France",
        "Poster_reputation_count":457.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>Thanks for the feedback. You can use globing in path. e.g. path = '**\/*.parquet' to select only the parquet files<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.4,
        "Solution_reading_time":1.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1221667848150,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":849.0,
        "Answerer_view_count":142.0,
        "Challenge_adjusted_solved_time":1013.8935497222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>AWS pricing page describes how much it costs per hour to run AWS Sagemaker for online realtime inference.\n<a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/<\/a><\/p>\n<p>But AWS usually also charges for API requests.\nDo they charge extra per every API inference request to the Sagemaker model?<\/p>",
        "Challenge_closed_time":1608159311612,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604509294833,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64684503",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":5.38,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1013.8935497222,
        "Challenge_title":"Does AWS Sagemaker charges you per API request?",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":187,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501710710163,
        "Poster_location":"London, \u0412\u0435\u043b\u0438\u043a\u043e\u0431\u0440\u0438\u0442\u0430\u043d\u0438\u044f",
        "Poster_reputation_count":404.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>I am on the AWS SageMaker team.  For &quot;Real-Time Inference&quot; you are only charged for:<\/p>\n<ol>\n<li>usage of the instance types you choose (instance hours)<\/li>\n<li>storage attached to those instance (GB storage hours)<\/li>\n<li>data in and out of your Endpoint (Bytes in\/out)<\/li>\n<\/ol>\n<p>See &quot;Pricing Example #6: Real-Time Inference&quot; as well.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":4.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":51.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1265234764768,
        "Answerer_location":"Denver, CO",
        "Answerer_reputation_count":30577.0,
        "Answerer_view_count":6460.0,
        "Challenge_adjusted_solved_time":19.0194533333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If I have a column of data of type string in an incoming Azure ML dataset that contains HTML tags screwing up my results, how can I remove those tags?<\/p>",
        "Challenge_closed_time":1484610622880,
        "Challenge_comment_count":0,
        "Challenge_created_time":1484610622880,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41686871",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":2.79,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"How to strip HTML from a text column in Azure ML Execute Python Script step",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":325,
        "Challenge_word_count":44,
        "Platform":"Stack Overflow",
        "Poster_created_time":1265234764768,
        "Poster_location":"Denver, CO",
        "Poster_reputation_count":30577.0,
        "Poster_view_count":6460.0,
        "Solution_body":"<p>Like this:<\/p>\n\n<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):\n  dataframe1[1] = dataframe1['text'].str.replace('&lt;[^&lt;]+?&gt;', ' ', case=False)\n  return dataframe1,\n<\/code><\/pre>\n\n<p>Remember to precede the <code>Execute Python Script<\/code> step with <code>Clean Missing Data<\/code> step and change the action to remove the entire row (if appropriate). This is important because the <code>Execute Python Script<\/code> step cannot return an empty <code>dataframe<\/code>. Only you know your data, in this case.<\/p>\n\n<p>Let me also point out that the <code>Preprocessing Text<\/code> step allows you to apply a Regular Expression. That is another alternative that might be right for your situation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1484679092912,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":9.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1386098048127,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":619.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":1733.7563858333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am afraid that my Neural Network in MXNet, written in Python, has a memory leak. I have tried the MXNet profiler and the tracemalloc module to get an understanding of memory profiling, but I want to get information on any potential memory leaks, just like I'd do with valgrind in C.<\/p>\n\n<p>I found <a href=\"https:\/\/cwiki.apache.org\/confluence\/display\/MXNET\/Detecting+Memory+Leaks+and+Buffer+Overflows+in+MXNet\" rel=\"nofollow noreferrer\">Detecting Memory Leaks and Buffer Overflows in MXNet<\/a>, and after managing to build like described in section \"Using ASAN builds with MXNet\", by replacing the \"ubuntu_cpu\" part in <code>docker\/Dockerfile.build.ubuntu_cpu -t mxnetci\/build.ubuntu_cpu<\/code> with \"ubuntu_cpu_python\", I tried executing in an AWS Sagemaker Notebook like this:<\/p>\n\n<pre><code>root@33e38e00f825:\/work\/mxnet# nosetests3 --verbose \/home\/ec2-user\/SageMaker\/run_predict.py\n<\/code><\/pre>\n\n<p>and I get this import error:<\/p>\n\n<blockquote>\n  <p>Failure: ImportError (No module named 'run_predict') ... ERROR<\/p>\n<\/blockquote>\n\n<p>My run_predict.py looks like this:<\/p>\n\n<pre><code>#!\/usr\/bin\/env python\ndef run_predict(n):\n  # calling MXNet inference method\n\nrun_predict(-1)  # tried it putting it under 'if __name__ == \"__main__\":'\n<\/code><\/pre>\n\n<p>What I am missing in my script, what should I change?<\/p>\n\n<p>The example script they use in the link is <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/blob\/faccd91071cc34ed0b3a192d3c7932441fe7e35e\/tests\/python\/unittest\/test_rnn.py\" rel=\"nofollow noreferrer\">rnn_test.py<\/a>, but even when I run this example, I still get an analogous Import Error.<\/p>",
        "Challenge_closed_time":1598820674152,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592493163510,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1592579151163,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62453292",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":21.46,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1757.641845,
        "Challenge_title":"How to find memory leak in Python MXNet?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":263,
        "Challenge_word_count":188,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369257942212,
        "Poster_location":"London, UK",
        "Poster_reputation_count":70285.0,
        "Poster_view_count":13121.0,
        "Solution_body":"<p>In MXNet, we automatically test for this through examining the garbage collection records. You can find how it's implemented here: <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/blob\/c3aff732371d6177e5d522c052fb7258978d8ce4\/tests\/python\/conftest.py#L26-L79\" rel=\"nofollow noreferrer\">https:\/\/github.com\/apache\/incubator-mxnet\/blob\/c3aff732371d6177e5d522c052fb7258978d8ce4\/tests\/python\/conftest.py#L26-L79<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.7,
        "Solution_reading_time":5.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1588516515763,
        "Answerer_location":"UK",
        "Answerer_reputation_count":29087.0,
        "Answerer_view_count":3080.0,
        "Challenge_adjusted_solved_time":0.2190208333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following <a href=\"https:\/\/github.com\/mtm12\/SageMakerDemo\" rel=\"noreferrer\">this example<\/a> on how to train a machine learning model in Amazon-sagemaker.<\/p>\n<pre><code>data_location = 's3:\/\/{}\/kmeans_highlevel_example\/data'.format(bucket)\noutput_location = 's3:\/\/{}\/kmeans_highlevel_example\/output'.format(bucket)\n\nprint('training data will be uploaded to: {}'.format(data_location))\nprint('training artifacts will be uploaded to: {}'.format(output_location))\n\nkmeans = KMeans(role=role,\n                train_instance_count=2,\n                train_instance_type='ml.c4.8xlarge',\n                output_path=output_location,\n                k=10,\n                epochs=100,\n                data_location=data_location)\n<\/code><\/pre>\n<p>So after calling the fit function the model should be saved in the S3 bucket?? How can you load this model next time?<\/p>",
        "Challenge_closed_time":1595162985808,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595162197333,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62980380",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":10.9,
        "Challenge_score_count":6,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.2190208333,
        "Challenge_title":"How to load trained model in amazon sagemaker?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":4776,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Poster_created_time":1310821356880,
        "Poster_location":"Slovenia",
        "Poster_reputation_count":14913.0,
        "Poster_view_count":1093.0,
        "Solution_body":"<p>This can be done by using the sagemaker library combined with the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"noreferrer\">Inference Model<\/a>.<\/p>\n<pre><code>model = sagemaker.model.Model(\n    image=image\n    model_data='s3:\/\/bucket\/model.tar.gz',\n    role=role_arn)\n<\/code><\/pre>\n<p>The options you're passing in are:<\/p>\n<ul>\n<li><code>image<\/code> - This is the ECR image you're using for inference (which should be for the algorithm you're trying to use). Paths are available <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"noreferrer\">here<\/a>.<\/li>\n<li><code>model_data<\/code> - This is the path of where your model is stored (in a <code>tar.gz<\/code> compressed archive).<\/li>\n<li><code>role<\/code> - This is the arn of a role that is capable of both pulling the image from ECR and getting the s3 archive.<\/li>\n<\/ul>\n<p>Once you've successfully done this you will need to setup an endpoint, this can be done by performing the following in your notebook through the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.Model.deploy\" rel=\"noreferrer\">deploy function<\/a>.<\/p>\n<pre><code>model.deploy(\n   initial_instance_count=1,\n   instance_type='ml.p2.xlarge'\n)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.4,
        "Solution_reading_time":17.2,
        "Solution_score_count":8.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":128.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":71.702525,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>If I am not using the notebook on AWS but instead just the Sagemaker CLI and want to train a model, can I specify a local path to read from and write to?<\/p>",
        "Challenge_closed_time":1530570724340,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530312595250,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51110274",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.4,
        "Challenge_reading_time":2.31,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":71.702525,
        "Challenge_title":"Can I use AWS Sagemaker without S3",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":826,
        "Challenge_word_count":39,
        "Platform":"Stack Overflow",
        "Poster_created_time":1528500562963,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>If you use local mode with the SageMaker Python SDK, you can train using local data:<\/p>\n\n<pre><code>from sagemaker.mxnet import MXNet\n\nmxnet_estimator = MXNet('train.py',\n                        train_instance_type='local',\n                        train_instance_count=1)\n\nmxnet_estimator.fit('file:\/\/\/tmp\/my_training_data')\n<\/code><\/pre>\n\n<p>However, this only works if you are training a model locally, not on SageMaker. If you want to train on SageMaker, then yes, you do need to use S3.<\/p>\n\n<p>For more about local mode: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.3,
        "Solution_reading_time":8.2,
        "Solution_score_count":4.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1324129118823,
        "Answerer_location":null,
        "Answerer_reputation_count":11756.0,
        "Answerer_view_count":517.0,
        "Challenge_adjusted_solved_time":8.0286480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created an EventBridge rule that triggers a Sagemaker Pipeline when someone uploads a new file to an S3 bucket. As new input files become available, they will be uploaded to the bucket for processing. I'd like the pipeline to process only the uploaded file, and so thought to pass in the S3 URL of the file as a parameter to the Pipeline. Since the full URL doesn't exist as a single field value in the S3 event, I was wondering if there is some way to concatenate multiple field values into a single parameter value that EventBridge will pass on to the target.<\/p>\n<p>For example, I know the name of the uploaded file can be sent from EventBridge using <code>$.detail.object.key<\/code> and the bucket name can be sent using <code>$.detail.bucket.name<\/code>, so I'm wondering if I can send both somehow to get something like this to the Sagemaker Pipeline <code>s3:\/\/my-bucket\/path\/to\/file.csv<\/code><\/p>\n<p>For what it's worth, I tried splitting the parameter into two (one being <code>s3:\/\/bucket-name\/<\/code> and the other being <code>default_file.csv<\/code>) when defining the pipeline, but got an error saying <code>Pipeline variables do not support concatenation<\/code> when combining the two into one.<\/p>\n<p>The relevant pipeline step is<\/p>\n<p><code>step_transform = TransformStep(name = &quot;Name&quot;, transformer=transformer,inputs=TransformInput(data=variable_of_s3_path)<\/code><\/p>",
        "Challenge_closed_time":1651734900863,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651705997730,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72120382",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":18.52,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":8.0286480556,
        "Challenge_title":"AWS EventBridge: Add multiple event details to a target parameter",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":809,
        "Challenge_word_count":207,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324682328743,
        "Poster_location":null,
        "Poster_reputation_count":969.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/eventbridge\/latest\/userguide\/eb-transform-target-input.html\" rel=\"nofollow noreferrer\">Input transformers<\/a> manipulate the event payload that EventBridge sends to the target.  Transforms consist of (1) an &quot;input path&quot; that maps substitution variable names to JSON-paths in the event and (2) a &quot;template&quot; that references the substitution variables.<\/p>\n<p>Input path:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;detail-bucket-name&quot;: &quot;$.detail.bucket.name&quot;,\n  &quot;detail-object-key&quot;: &quot;$.detail.object.key&quot;\n}\n<\/code><\/pre>\n<p>Input template that concatenates the s3 url and outputs it along with the original event payload:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;s3Url&quot;: &quot;s3:\/\/&lt;detail-bucket-name&gt;\/&lt;detail-object-key&gt;&quot;,\n  &quot;original&quot;: &quot;$&quot;\n}\n<\/code><\/pre>\n<p>Define the transform in the EventBridge console by editing the rule: <code>Rule &gt; Select Targets &gt; Additional Settings<\/code>.<\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.8,
        "Solution_reading_time":14.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":93.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1601920258310,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":123.8963886111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to create a Time series dataset from a folder that contains parquet files this way:<\/p>\n<ul>\n<li>timestamp=2018-01-06<\/li>\n<li>timestamp=2018-01-07<\/li>\n<\/ul>\n<p>How can I make Azure Dataset, through the GUI, recognises the timestamp partition as a date and mark my dataset as a time series dataset?<\/p>\n<p>It is supposed to be automatic, but it doesn't work.<\/p>",
        "Challenge_closed_time":1601920685456,
        "Challenge_comment_count":1,
        "Challenge_created_time":1601474658457,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64139290",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.8,
        "Challenge_reading_time":6.07,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":123.8963886111,
        "Challenge_title":"How can I mark an Azure Dataset as a time series dataset reading from a parquet folder with date partitions?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":109,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423640080283,
        "Poster_location":"Lyon, France",
        "Poster_reputation_count":457.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>Thanks for reaching out to us.<\/p>\n<p>In Azure Machine Learning Studio, you would need to setup partition format similar to python SDK, as follows, assuming your data path is &quot;timeseries\/timestamp=2020-01-01\/data.parquet&quot;:\n<a href=\"https:\/\/i.stack.imgur.com\/HwYfF.png\" rel=\"nofollow noreferrer\">Set up partition format when creating time series dataset<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.1,
        "Solution_reading_time":4.88,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1294388296987,
        "Answerer_location":"Italy",
        "Answerer_reputation_count":32174.0,
        "Answerer_view_count":3457.0,
        "Challenge_adjusted_solved_time":1.2159166667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I use Azure Machine Learning Workspace Notebooks, connected to a DevOps Repository - using terminal git commands to manage my code. I work on different branches, often has to switch back and forth between them.<\/p>\n<p>I reviewed this thread before: <a href=\"https:\/\/stackoverflow.com\/questions\/18615428\/switching-branches-keeps-new-files-from-other-branch\">switching branches keeps new files from other branch<\/a><\/p>\n<p>In my case it does not only keep the files that should be ignored with the use of the gitignore file, but others too.<\/p>\n<p>I tested it with a totally empty branch, that should not have any files in it, checked it out, and it still has files from the branch that I worked with previously. When I check it manually on DevOps, in the repo, the empty branch is actually empty there.<\/p>\n<p>Has anyone seen similar issues?<\/p>",
        "Challenge_closed_time":1622721091067,
        "Challenge_comment_count":2,
        "Challenge_created_time":1622716713767,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67819912",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":8.8,
        "Challenge_reading_time":11.63,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.2159166667,
        "Challenge_title":"Azure ML, DevOps: Switching between branches keeps some files from another branch",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":189,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1438890950452,
        "Poster_location":"Nordhavnen, Copenhagen, Denmark",
        "Poster_reputation_count":570.0,
        "Poster_view_count":187.0,
        "Solution_body":"<p>Some files that are tracked in a branch could be not tracked in another. So when you switch back to the &quot;non tracking&quot; branch, that files remain in the file system. Git does not clean stuff that does not track directly. Do not exchange the term not tracked by ignored. Files are not tracked until we &quot;add&quot; them in stage and commit.\nYou could cleanup the working git by running <code>git clean -f -d<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.4,
        "Solution_reading_time":5.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":74.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":6278.0877944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Python code generated from an ml software with mlflow to read a dataframe, perform some table operations and output a dataframe. I am able to run the code successfully and save the new dataframe as an artifact. However I am unable to log the model using log_model because it is not a lr or classifier model where we train and fit. I want to log a model for this so that it can be served with new data and deployed with a rest API<\/p>\n<pre><code>df = pd.read_csv(r&quot;\/home\/xxxx.csv&quot;)\n\n\nwith mlflow.start_run():\n    def getPrediction(row):\n        \n        perform_some_python_operaions \n\n        return [Status_prediction, Status_0_probability, Status_1_probability]\n    columnValues = []\n    for column in columns:\n        columnValues.append([])\n\n    for index, row in df.iterrows():\n        results = getPrediction(row)\n        for n in range(len(results)):\n            columnValues[n].append(results[n])\n\n    for n in range(len(columns)):\n        df[columns[n]] = columnValues[n]\n\n    df.to_csv('dataset_statistics.csv')\n    mlflow.log_artifact('dataset_statistics.csv')\n   \n<\/code><\/pre>",
        "Challenge_closed_time":1611592914947,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611586824463,
        "Challenge_favorite_count":3,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65887231",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":13.53,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.6918011111,
        "Challenge_title":"Use mlflow to serve a custom python model for scoring",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3026,
        "Challenge_word_count":134,
        "Platform":"Stack Overflow",
        "Poster_created_time":1573739890560,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>MLflow supports <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">custom models<\/a> of mlflow.pyfunc flavor.  You can create a custom  class  inherited from the <code>mlflow.pyfunc.PythonModel<\/code>, that needs to provide function <code>predict<\/code> for performing predictions, and optional <code>load_context<\/code> to load the necessary artifacts, like this (adopted from the docs):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class MyModel(mlflow.pyfunc.PythonModel):\n\n    def load_context(self, context):\n        # load your artifacts\n\n    def predict(self, context, model_input):\n        return my_predict(model_input.values)\n<\/code><\/pre>\n<p>You can log to MLflow whatever artifacts you need for your models, define Conda environment if necessary, etc.<br \/>\nThen you can use <code>save_model<\/code> with your class to save your implementation, that could be loaded with <code>load_model<\/code> and do the <code>predict<\/code> using your model:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.pyfunc.save_model(\n        path=mlflow_pyfunc_model_path, \n        python_model=MyModel(), \n        artifacts=artifacts)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1634187940523,
        "Solution_link_count":1.0,
        "Solution_readability":17.3,
        "Solution_reading_time":16.79,
        "Solution_score_count":9.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":118.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1407449881432,
        "Answerer_location":null,
        "Answerer_reputation_count":228.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":732.1807,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I am attempting to run the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">example code<\/a> for Amazon Sagemaker on a local GPU.  I have copied the code from the Jupyter notebook to the following Python script:<\/p>\n\n<pre><code>import boto3\nimport subprocess\nimport sagemaker\nfrom sagemaker.mxnet import MXNet\nfrom mxnet import gluon\nfrom sagemaker import get_execution_role\nimport os\n\nsagemaker_session = sagemaker.Session()\ninstance_type = 'local'\nif subprocess.call('nvidia-smi') == 0:\n    # Set type to GPU if one is present\n    instance_type = 'local_gpu'\n# role = get_execution_role()\n\ngluon.data.vision.MNIST('.\/data\/train', train=True)\ngluon.data.vision.MNIST('.\/data\/test', train=False)\n\n# successfully connects and uploads data\ninputs = sagemaker_session.upload_data(path='data', key_prefix='data\/mnist')\n\nhyperparameters = {\n    'batch_size': 100,\n    'epochs': 20,\n    'learning_rate': 0.1,\n    'momentum': 0.9,\n    'log_interval': 100\n}\n\nm = MXNet(\"mnist.py\",\n          role=role,\n          train_instance_count=1,\n          train_instance_type=instance_type,\n          framework_version=\"1.1.0\",\n          hyperparameters=hyperparameters)\n\n# fails in Docker container\nm.fit(inputs)\npredictor = m.deploy(initial_instance_count=1, instance_type=instance_type)\nm.delete_endpoint()\n<\/code><\/pre>\n\n<p>where the referenced <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">mnist.py<\/a> file is exactly as specified on Github. The script fails on <code>m.fit<\/code> in Docker container with the following error: <\/p>\n\n<pre><code>algo-1-1DUU4_1  | Downloading s3:\/\/&lt;S3-BUCKET&gt;\/sagemaker-mxnet-2018-10-07-00-47-10-435\/source\/sourcedir.tar.gz to \/tmp\/script.tar.gz\nalgo-1-1DUU4_1  | 2018-10-07 00:47:29,219 ERROR - container_support.training - uncaught exception during training: Unable to locate credentials\nalgo-1-1DUU4_1  | Traceback (most recent call last):\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/training.py\", line 36, in start\nalgo-1-1DUU4_1  |     fw.train()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/mxnet_container\/train.py\", line 169, in train\nalgo-1-1DUU4_1  |     mxnet_env.download_user_module()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/environment.py\", line 89, in download_user_module\nalgo-1-1DUU4_1  |     cs.download_s3_resource(self.user_script_archive, tmp)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/utils.py\", line 37, in download_s3_resource\nalgo-1-1DUU4_1  |     script_bucket.download_file(script_key_name, target)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 246, in bucket_download_file\nalgo-1-1DUU4_1  |     ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 172, in download_file\nalgo-1-1DUU4_1  |     extra_args=ExtraArgs, callback=Callback)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/transfer.py\", line 307, in download_file\nalgo-1-1DUU4_1  |     future.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 73, in result\nalgo-1-1DUU4_1  |     return self._coordinator.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 233, in result\nalgo-1-1DUU4_1  |     raise self._exception\nalgo-1-1DUU4_1  | NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n\n<p>I am confused that I can authenticate to S3 outside of the container (to pload the training\/test data) but I cannot within the Docker container.  So I am guessing the issues has to do with passing the AWS credentials to the Docker container.  Here is the generated Docker-compose file:<\/p>\n\n<pre><code>networks:\n  sagemaker-local:\n    name: sagemaker-local\nservices:\n  algo-1-1DUU4:\n    command: train\n    environment:\n    - AWS_REGION=us-west-2\n    - TRAINING_JOB_NAME=sagemaker-mxnet-2018-10-07-00-47-10-435\n    image: 123456789012.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.1.0-gpu-py2\n    networks:\n      sagemaker-local:\n        aliases:\n        - algo-1-1DUU4\n    stdin_open: true\n    tty: true\n    volumes:\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/input:\/opt\/ml\/input\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output:\/opt\/ml\/output\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output\/data:\/opt\/ml\/output\/data\n    - \/tmp\/tmpSkaR3x\/model:\/opt\/ml\/model\nversion: '2.1'\n<\/code><\/pre>\n\n<p>Should the AWS credentials be passed in as enviromental variables?<\/p>\n\n<p>I upgraded my <code>sagemaker<\/code> install to after reading <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/403\" rel=\"nofollow noreferrer\">Using boto3 in install local mode?<\/a>, but that had no effect.  I checked the credentials that are being fetched in the Sagemaker session (outside the container) and they appear to be blank, even though I have an <code>~\/.aws\/config<\/code> and <code>~\/.aws\/credentials<\/code> file:<\/p>\n\n<pre><code>{'_token': None, '_time_fetcher': &lt;function _local_now at 0x7f4dbbe75230&gt;, '_access_key': None, '_frozen_credentials': None, '_refresh_using': &lt;bound method AssumeRoleCredentialFetcher.fetch_credentials of &lt;botocore.credentials.AssumeRoleCredentialFetcher object at 0x7f4d2de48bd0&gt;&gt;, '_secret_key': None, '_expiry_time': None, 'method': 'assume-role', '_refresh_lock': &lt;thread.lock object at 0x7f4d9f2aafd0&gt;}\n<\/code><\/pre>\n\n<p>I am new to AWS so I do not know how to diagnose the issue regarding AWS credentials.  My <code>.aws\/config<\/code> file has the following information (with placeholder values):<\/p>\n\n<pre><code>[default]\noutput = json\nregion = us-west-2\nrole_arn = arn:aws:iam::123456789012:role\/SageMakers\nsource_profile = sagemaker-test\n\n[profile sagemaker-test]\noutput = json\nregion = us-west-2\n<\/code><\/pre>\n\n<p>Where the <code>sagemaker-test<\/code> profile has <code>AmazonSageMakerFullAccess<\/code> in the IAM Management Console.<\/p>\n\n<p>The <code>.aws\/credentials<\/code> file has the following information (represented by placeholder values):<\/p>\n\n<pre><code>[default]\naws_access_key_id = 1234567890\naws_secret_access_key = zyxwvutsrqponmlkjihgfedcba\n[sagemaker-test]\naws_access_key_id = 0987654321\naws_secret_access_key = abcdefghijklmopqrstuvwxyz\n<\/code><\/pre>\n\n<p>Lastly, these are versions of the applicable libraries from a <code>pip freeze<\/code>:<\/p>\n\n<pre><code>awscli==1.16.19\nboto==2.48.0\nboto3==1.9.18\nbotocore==1.12.18\ndocker==3.5.0\ndocker-compose==1.22.0\nmxnet-cu91==1.1.0.post0\nsagemaker==1.11.1\n<\/code><\/pre>\n\n<p>Please let me know if I left out any relevant information and thanks for any help\/feedback that you can provide.<\/p>\n\n<p><strong>UPDATE<\/strong>: Thanks for your help, everyone! While attempting some of your suggested fixes, I noticed that <code>boto3<\/code> was out of date, and update it (to <code>boto3-1.9.26<\/code> and <code>botocore-1.12.26<\/code>) which appeared to resolve the issue.  I was not able to find any documentation on that being an issue with <code>boto3==1.9.18<\/code>.  If someone could help me understand what the issue was with <code>boto3<\/code>, I would happy to make mark their answer as correct.<\/p>",
        "Challenge_closed_time":1539818808056,
        "Challenge_comment_count":3,
        "Challenge_created_time":1538879533133,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1539893383687,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52684987",
        "Challenge_link_count":3,
        "Challenge_participation_count":7,
        "Challenge_readability":16.6,
        "Challenge_reading_time":96.04,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":63,
        "Challenge_solved_time":260.9097008333,
        "Challenge_title":"AWS NoCredentials in training",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1374,
        "Challenge_word_count":621,
        "Platform":"Stack Overflow",
        "Poster_created_time":1380496984176,
        "Poster_location":"Gloucester, VA, USA",
        "Poster_reputation_count":544.0,
        "Poster_view_count":85.0,
        "Solution_body":"<p>SageMaker local mode is designed to pick up whatever credentials are available in your boto3 session, and pass them into the docker container as environment variables. <\/p>\n\n<p>However, the version of the sagemaker sdk that you are using (1.11.1 and earlier) will ignore the credentials if they include a token, because that usually indicates short-lived credentials that won't remain valid long enough for a training job to complete or endpoint to be useful.<\/p>\n\n<p>If you are using temporary credentials, try replacing them with permanent ones, or running from an ec2 instance (or SageMaker notebook!) that has an appropriate instance role assigned.<\/p>\n\n<p>Also, the sagemaker sdk's handling of credentials changed in v1.11.2 and later -- temporary credentials will be passed to local mode containers, but with a warning message. So you could just upgrade to a newer version and try again (<code>pip install -U sagemaker<\/code>). <\/p>\n\n<p>Also, try upgrading <code>boto3<\/code> can change, so try using the latest version.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1542529234207,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":12.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":156.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1363541433296,
        "Answerer_location":"Twin Cities, MN, USA",
        "Answerer_reputation_count":348.0,
        "Answerer_view_count":24.0,
        "Challenge_adjusted_solved_time":7.8782044444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying in Amazon Sagemaker to deploy an existing Scikit-Learn model. So a model that wasn't trained on SageMaker, but locally on my machine.<\/p>\n<p>On my local (windows) machine I've saved my model as model.joblib and tarred the model to model.tar.gz.<\/p>\n<p>Next, I've uploaded this model to my S3 bucket ('my_bucket') in the following path s3:\/\/my_bucket\/models\/model.tar.gz. I can see the tar file in S3.<\/p>\n<p>But when I'm trying to deploy the model, it keeps giving the error message &quot;Failed to extract model data archive&quot;.<\/p>\n<p>The .tar.gz is generated on my local machine by running 'tar -czf model.tar.gz model.joblib' in a powershell command window.<\/p>\n<p>The code for uploading to S3<\/p>\n<pre><code>import boto3\ns3 = boto3.client(&quot;s3&quot;, \n              region_name='eu-central-1', \n              aws_access_key_id=AWS_KEY_ID, \n              aws_secret_access_key=AWS_SECRET)\ns3.upload_file(Filename='model.tar.gz', Bucket=my_bucket, Key='models\/model.tar.gz')\n<\/code><\/pre>\n<p>The code for creating the estimator and deploying:<\/p>\n<pre><code>import boto3\nfrom sagemaker.sklearn.estimator import SKLearnModel\n\n...\n\nmodel_data = 's3:\/\/my_bucket\/models\/model.tar.gz'\nsklearn_model = SKLearnModel(model_data=model_data,\n                             role=role,\n                             entry_point=&quot;my-script.py&quot;,\n                             framework_version=&quot;0.23-1&quot;)\npredictor = sklearn_model.deploy(instance_type=&quot;ml.t2.medium&quot;, initial_instance_count=1)                             \n<\/code><\/pre>\n<p>The error message:<\/p>\n<blockquote>\n<p>error message: UnexpectedStatusException: Error hosting endpoint\nsagemaker-scikit-learn-2021-01-24-17-24-42-204: Failed. Reason: Failed\nto extract model data archive for container &quot;container_1&quot; from URL\n&quot;s3:\/\/my_bucket\/models\/model.tar.gz&quot;. Please ensure that the object\nlocated at the URL is a valid tar.gz archive<\/p>\n<\/blockquote>\n<p>Is there a way to see why the archive is invalid?<\/p>",
        "Challenge_closed_time":1611852069143,
        "Challenge_comment_count":5,
        "Challenge_created_time":1611565411923,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":1611823707607,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65881699",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":11.7,
        "Challenge_reading_time":25.46,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":79.6270055556,
        "Challenge_title":"SageMaker failed to extract model data archive tar.gz for container when deploying",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1859,
        "Challenge_word_count":208,
        "Platform":"Stack Overflow",
        "Poster_created_time":1472970520888,
        "Poster_location":"Amersfoort, Nederland",
        "Poster_reputation_count":424.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>I had a similar issue as well, along with a similar fix to Bas (per comment above).<\/p>\n<p>I was finding I wasn't necessarily having issues with the .tar.gz step, this command does work fine:<\/p>\n<p><code>tar -czf &lt;filename&gt; .\/&lt;directory-with-files&gt;<\/code><\/p>\n<p>but rather with the uploading step.<\/p>\n<p>Manually uploading to S3 should take care of this, however, if you're doing this step programmatically, you might need to double check the steps taken. Bas appears to have had filename issues, mine were around using boto properly. Here's some code that works (Python only here, but watch for similar issues with other libraries):<\/p>\n<pre><code>bucket = 'bucket-name'\nkey = 'directory-inside-bucket'\nfile = 'the file name of the .tar.gz'\n\ns3_client = boto3.client('s3')\ns3_client.upload_file(file, bucket, key)\n<\/code><\/pre>\n<p>Docs: <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.9,
        "Solution_reading_time":14.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":120.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":6.0827630556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I would like to test <a href=\"https:\/\/github.com\/allegroai\/trains\" rel=\"nofollow noreferrer\">trains<\/a> usage during grid search and it not clear how to do so.<\/p>\n\n<pre><code>from trains import Task \nTask.init(project_name=\"project name\", task_name='name')\n<\/code><\/pre>\n\n<p>creates an experiment in the demo server and logs all but you can't call init twice no matter the 'task_name' and <\/p>\n\n<pre><code>from trains import Task \nTask.create(project_name=\"project name\", task_name='name')\n<\/code><\/pre>\n\n<p>can be called with different 'task_name' but thus not log any data into the server and creates only 'Draft'.<\/p>\n\n<p>here is a sample code:<\/p>\n\n<pre><code> epochs=[160,300]\n for epoch in epochs:\n    model = define_model_run(epoch)\n    model.fit(x_train,y_train)\n    score = model.score(...)\n<\/code><\/pre>\n\n<p>my final try was:<\/p>\n\n<pre><code> epochs=[160,300]\n task=Task.init(project_name=\"demo\", task_name='search')\n for epoch in epochs:\n    task.create(project_name=\"demo\", task_name=f'search_{epoch}')\n    model = define_model_run(epoch)\n    model.fit(x_train,y_train)\n    score = model.score(...)\n<\/code><\/pre>\n\n<p>which logs all information under the experiments tab and none under the 'Draft'.\nI tried the last two hour the read the few documentations provided and reading the source code, but no luck.<\/p>\n\n<p>any help? <\/p>",
        "Challenge_closed_time":1566240704540,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566218806593,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1609778640296,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57557070",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":17.17,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":6.0827630556,
        "Challenge_title":"trains with grid search",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":121,
        "Challenge_word_count":154,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416942229380,
        "Poster_location":null,
        "Poster_reputation_count":161.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>Declaimer: I'm a member of TRAINS team<\/p>\n\n<p>Yes, that's exactly the answer.\nThe idea is that you always have one main Task, in order to create a new one you need to close the running Task, and re-initialize with a new name.\nKudos on solving it so quickly :)<\/p>\n\n<p>BTW: You can see examples <a href=\"https:\/\/stackoverflow.com\/q\/56744397\/11682840\">here<\/a>\/<a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/master\/docs\/faq.md#can-i-create-a-graph-comparing-hyper-parameters-vs-model-accuracy-\" rel=\"nofollow noreferrer\">and here<\/a>, showing how to send accuracy logs so it is easier to compare the experiments, especially when running hyper-parameter search.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1566242122212,
        "Solution_link_count":2.0,
        "Solution_readability":14.6,
        "Solution_reading_time":8.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":79.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1541523185320,
        "Answerer_location":"Ottawa, ON, Canada",
        "Answerer_reputation_count":1003.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":1.5810183334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What is the difference between these two? We used git-lfs in my previous job and we are starting to use dvc alongside git in my current one. They both place some kind of index instead of file and can be downloaded on demand. Has dvc some improvements over the former one?<\/p>",
        "Challenge_closed_time":1571925277763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1571919586097,
        "Challenge_favorite_count":5,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58541260",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.2,
        "Challenge_reading_time":3.8,
        "Challenge_score_count":27,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.5810183334,
        "Challenge_title":"Difference between git-lfs and dvc",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":6255,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373630643248,
        "Poster_location":null,
        "Poster_reputation_count":382.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>DVC is a better replacement for <code>git-lfs<\/code>. <\/p>\n\n<p>Unlike git-lfs, DVC doesn't require installing a dedicated server; It can be used on-premises (NAS, SSH, for example) or with any major cloud provider (S3, Google Cloud, Azure).<\/p>\n\n<p>For more information: <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-and-model-files-versioning\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/data-and-model-files-versioning<\/a><\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.7,
        "Solution_reading_time":5.71,
        "Solution_score_count":10.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":42.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":3.4510725,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to know if I can change the port of my MLflow server.<\/p>\n<p>By default it is running on port 5000, but my company's VPN only allows HTTP (port 80) and HTTPS (port 443) traffic.<\/p>\n<p>This might be a very beginner's question, but is it possible, and if yes, is there any problem on running the MLflow server on port 83 (HTTP) ?<\/p>\n<p>Thank you<\/p>",
        "Challenge_closed_time":1635152056288,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635139632427,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69703225",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.5,
        "Challenge_reading_time":5.02,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3.4510725,
        "Challenge_title":"Can I change the port of my MLflow tracking server?",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":540,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1561106497312,
        "Poster_location":null,
        "Poster_reputation_count":133.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>Yes, you can do that by passing the <code>-p port_number<\/code> command-line switch when starting MLflow server (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#cmdoption-mlflow-server-p\" rel=\"nofollow noreferrer\">docs<\/a>). Please note, that to be able to use ports below 1024, the server needs to be run as root.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.5,
        "Solution_reading_time":4.22,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":40.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":0.4279386111,
        "Challenge_answer_count":1,
        "Challenge_body":"<h1>Difficulty in understanding<\/h1>\n<p>Q2) How to download a file from S3?<\/p>\n<p><strong>From<\/strong>  <a href=\"https:\/\/medium.com\/akeneo-labs\/machine-learning-workflow-with-sagemaker-b83b293337ff\" rel=\"nofollow noreferrer\">The Machine Learning Workflow with SageMaker<\/a><\/p>\n<p>And also why are we using this piece of code?<\/p>\n<p><code>estimator.fit(train_data_location)<\/code><\/p>",
        "Challenge_closed_time":1568928522076,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568926981497,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58018893",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":6.05,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.4279386111,
        "Challenge_title":"How do S3 file download and estimator.fit() work in this blog post?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":220,
        "Challenge_word_count":43,
        "Platform":"Stack Overflow",
        "Poster_created_time":1556451987416,
        "Poster_location":"India",
        "Poster_reputation_count":1309.0,
        "Poster_view_count":288.0,
        "Solution_body":"<h2>Downloading a file from S3:<\/h2>\n\n<p>This code block in the Q2 section defines the function that downloads a file from S3. The user instantiates an S3 client, and then passes the S3 URL along to the <code>s3.Bucket.download_file()<\/code> method.<\/p>\n\n<pre><code>def download_from_s3(url):\n    \"\"\"ex: url = s3:\/\/sagemakerbucketname\/data\/validation.tfrecords\"\"\"\n    url_parts = url.split(\"\/\")  # =&gt; ['s3:', '', 'sagemakerbucketname', 'data', ...\n    bucket_name = url_parts[2]\n    key = os.path.join(*url_parts[3:])\n    filename = url_parts[-1]\n    if not os.path.exists(filename):\n        try:\n            # Create an S3 client\n            s3 = boto3.resource('s3')\n            print('Downloading {} to {}'.format(url, filename))\n            s3.Bucket(bucket_name).download_file(key, filename)\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] == \"404\":\n                print('The object {} does not exist in bucket {}'.format(\n                    key, bucket_name))\n            else:\n                raise\n<\/code><\/pre>\n\n<h2>Estimator.fit() explanation:<\/h2>\n\n<p>The <code>estimator.fit(train_data_location)<\/code> line is what initiates the training process with SageMaker. When run, SageMaker will provision the necessary infrastructure, fetch the data from the location the user designated (here, <code>train_data_location<\/code> which is a path to Amazon S3) and distribute it amongst the training cluster, carry out the training process, return the resulting model, and tear down the training infrastructure. <\/p>\n\n<p>You can find the result of this training job in the SageMaker console.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":19.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":166.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":20.1381897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to AWS infra and currently doing some POC\/Feasibility for new work.<\/p>\n\n<p>So I have created a S3 bucket in Ireland server, train and publish Sagemaker endpoint in Ireland server and its giving result in Jupyter notebook there. Now I want to use that endpoint in my browser javascript library to show some graphics. When I try to test my endpoint in Postman then its giving region specific error <\/p>\n\n<pre><code> {\n        \"message\": \"Credential should be scoped to a valid region, not 'us-east-1'. \nCredential should be scoped to correct service: 'sagemaker'. \"\n }\n<\/code><\/pre>\n\n<p>My AWS account is not yet enterprise managed so I am using as 'root user', Whenever I go to my profile>Security_Credential page and generate any security credential then it always create for 'us-east-1' region, As Sagemaker is region specific service, I am not able to find the way to create region specific security key for root user, can someone please help<\/p>",
        "Challenge_closed_time":1526179930343,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526107432860,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50303607",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":12.49,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":20.1381897222,
        "Challenge_title":"AWS Sagemaker | region specific security credentials for endpoint",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":750,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501403168107,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>You should create an IAM role first that defines what should be permitted (mainly calling the invoke-endpoint API call for SageMaker runtime). Then you should create an IAM user, add the above role to that user, and then generate credentials that you can use in your Postman to call the service. Here you can find some details about the IAM role for SageMaker that you can use in this process: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/using-identity-based-policies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/using-identity-based-policies.html<\/a><\/p>\n\n<p>A popular option to achieve external access to a SageMaker endpoint, is to create an API Gateway that calls a Lambda Function that is then calling the invoke-endpoint API. This chain is giving you various options such as different authentication options for the users and API keys as part of API-GW, processing the user input and inference output using API-GW and Lambda code, and giving the permission to call the SageMaker endpoint to the Lambda function. This chain removes the need for the credentials creation, update and distribution.  <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.6,
        "Solution_reading_time":14.59,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":163.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":23.0099397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running local unsupervised learning (predominantly clustering) on a large, single node with GPU.<\/p>\n<p>Does SageMaker support <strong>distributed unsupervised learning<\/strong> using <strong>clustering<\/strong>?<\/p>\n<p>If yes, please provide the relevant example (preferably non-TensorFlow).<\/p>",
        "Challenge_closed_time":1663485400223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663402564440,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73753271",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":4.64,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":23.0099397222,
        "Challenge_title":"Distributed Unsupervised Learning in SageMaker",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":14,
        "Challenge_word_count":36,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>SageMaker Training allow you to bring your own training scripts, and supports various forms of distributed training, like data\/model parallel, and frameworks like PyTorch DDP, Horovod, DeepSpeed, etc.\nAdditionally, if you want to bring your data, but not code, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algorithms-unsupervised.html\" rel=\"nofollow noreferrer\">SageMaker training offers various unsupervised built-in algorithms<\/a>, some of which are parallelizable.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.2,
        "Solution_reading_time":6.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1401922736652,
        "Answerer_location":"Brisbane",
        "Answerer_reputation_count":748.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":510.2445766667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I'm looking to make some hyper parameters available to the serving endpoint in SageMaker. The training instances is given access to input parameters using hyperparameters in:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='autocat.py',\n                       role=role,\n                       output_path=params['output_path'],\n                       code_location=params['code_location'],\n                       train_instance_count=1,\n                       train_instance_type='ml.c4.xlarge',\n                       training_steps=10000,\n                       evaluation_steps=None,\n                       hyperparameters=params)\n<\/code><\/pre>\n\n<p>However, when the endpoint is deployed, there is no way to pass in parameters that are used to control the data processing in the <code>input_fn(serialized_input, content_type)<\/code> function.<\/p>\n\n<p>What would be the best way to pass parameters to the serving instance?? Is the <code>source_dir<\/code> parameter defined in the <code>sagemaker.tensorflow.TensorFlow<\/code> class copied to the serving instance? If so, I could use a config.yml or similar.<\/p>",
        "Challenge_closed_time":1523591814356,
        "Challenge_comment_count":0,
        "Challenge_created_time":1521754623920,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1521754933880,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49438903",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":13.4,
        "Challenge_reading_time":13.07,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":510.3306766667,
        "Challenge_title":"How to make parameters available to SageMaker Tensorflow Endpoint",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1426,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443201378360,
        "Poster_location":null,
        "Poster_reputation_count":749.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>Ah i have had a similar problem to you where I needed to download something off S3 to use in the input_fn for inference. In my case it was a dictionary.<\/p>\n\n<p>Three options:<\/p>\n\n<ol>\n<li>use your config.yml approach, and download and import the s3 file from within your entrypoint file before any function declarations. This would make it available to the input_fn <\/li>\n<li>Keep using the hyperparameter approach, download and import the vectorizer in <code>serving_input_fn<\/code> and make it available via a global variable so that <code>input_fn<\/code> has access to it.<\/li>\n<li>Download the file from s3 before training and include it in the source_dir directly.<\/li>\n<\/ol>\n\n<p>Option 3 would only work if you didnt need to make changes to the vectorizer seperately after initial training.<\/p>\n\n<p>Whatever you do, don't download the file directly in input_fn. I made that mistake and the performance is terrible as each invoking of the endpoint would result in the s3 file being downloaded.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":12.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":157.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1585590244876,
        "Answerer_location":null,
        "Answerer_reputation_count":55.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":0.0392458333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>We created a webservice endpoint and tested it with the following code, and also with POSTMAN.<\/p>\n\n<p>We deployed the service to an AKS in the same resource group and subscription as the AML resource.<\/p>\n\n<p><strong>UPDATE: the attached AKS had a custom networking configuration and rejected external connections.<\/strong><\/p>\n\n<pre><code>import numpy\nimport os, json, datetime, sys\nfrom operator import attrgetter\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.image import Image\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.authentication import AzureCliAuthentication\n\ncli_auth = AzureCliAuthentication()\n# Get workspace\nws = Workspace.from_config(auth=cli_auth)\n\n# Get the AKS Details\ntry:\n    with open(\"..\/aml_config\/aks_webservice.json\") as f:\n        config = json.load(f)\nexcept:\n    print(\"No new model, thus no deployment on AKS\")\n    # raise Exception('No new model to register as production model perform better')\n    sys.exit(0)\n\nservice_name = config[\"aks_service_name\"]\n# Get the hosted web service\nservice = Webservice(workspace=ws, name=service_name)\n\n# Input for Model with all features\ninput_j = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]]\nprint(input_j)\ntest_sample = json.dumps({\"data\": input_j})\ntest_sample = bytes(test_sample, encoding=\"utf8\")\ntry:\n    prediction = service.run(input_data=test_sample)\n    print(prediction)\nexcept Exception as e:\n    result = str(e)\n    print(result)\n    raise Exception(\"AKS service is not working as expected\")\n<\/code><\/pre>\n\n<p>In AML Studio, the deployment state is \"Healthy\".<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RTB10.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RTB10.png\" alt=\"Endpoint attributes\"><\/a><\/p>\n\n<p>We get the following error when testing:<\/p>\n\n<pre><code>Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'\n<\/code><\/pre>\n\n<p><strong>Log just after deploying the AKS Webservice <a href=\"http:\/\/t.ly\/t79b\" rel=\"nofollow noreferrer\">here<\/a>.<\/strong><\/p>\n\n<p><strong>Log after running the test script <a href=\"http:\/\/t.ly\/79k5\" rel=\"nofollow noreferrer\">here<\/a>.<\/strong><\/p>\n\n<p>How can we know what is causing this problem and fix it?<\/p>",
        "Challenge_closed_time":1592590202852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592508291480,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1592590061567,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62457880",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":12.7,
        "Challenge_reading_time":30.97,
        "Challenge_score_count":2,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":22.7531588889,
        "Challenge_title":"AML - Web service TimeoutError",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":332,
        "Challenge_word_count":275,
        "Platform":"Stack Overflow",
        "Poster_created_time":1585590244876,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>We checked the AKS networking configuration and realized it has an Azure CNI profile.<\/p>\n\n<p>In order to test the webservice we need to do it from inside the created virtual network.\nIt worked well!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":2.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":5.6297925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Microsoft Azure Machine Learning and was wondering if anyone had done some experiments on date time features. Doe sit automatically derive additional features like \"day of week\", \"day of month\", \"hour of day\" from them, or do I have to provide these?<\/p>\n\n<p>I could not find any info in the official documentation (and a lack of a Microsoft support forum =)<\/p>",
        "Challenge_closed_time":1434736380420,
        "Challenge_comment_count":2,
        "Challenge_created_time":1434716113167,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1445833326870,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30937903",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.3,
        "Challenge_reading_time":5.41,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":5.6297925,
        "Challenge_title":"How are date features utilized in Microsoft Azure Machine Studio",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":806,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1221999894423,
        "Poster_location":"Delaware",
        "Poster_reputation_count":2603.0,
        "Poster_view_count":225.0,
        "Solution_body":"<p>Azure ML supports \"execute-R\" module which can be easily used to accomplish this in R - few examples below<\/p>\n\n<p>x&lt;-as.Date(\"12\/3\/2009\", \"%m\/%d\/%Y\")<\/p>\n\n<blockquote>\n  <p>months.Date(x)<\/p>\n<\/blockquote>\n\n<p>[1] \"December\"<\/p>\n\n<blockquote>\n  <p>weekdays.Date(x)<\/p>\n<\/blockquote>\n\n<p>[1] \"Thursday\"<\/p>\n\n<blockquote>\n  <p>quarters(x)<\/p>\n<\/blockquote>\n\n<p>[1] \"Q4\"<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":4.85,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":35.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1251372839052,
        "Answerer_location":null,
        "Answerer_reputation_count":48616.0,
        "Answerer_view_count":3348.0,
        "Challenge_adjusted_solved_time":226.94532,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed my code in Azure Machine Learning and run the batch request in R with different operating systems, such as Unix and W10. For some reason, the host outputs are properly formatted only in R of W10 but I am unable to get properly formatted output in Unix systems. Only way I can get properly formatted outputs in all systems is through the Azure GUI and manually download the file. In W10, I have the luxury to get the properly formatted file directly with my Rscript\/Rstudio thing. In R, I have used <code>system(&quot;defaults write org.R-project.R force.LANG en_US.UTF-8&quot;)<\/code> as hinted <a href=\"https:\/\/stackoverflow.com\/questions\/41873359\/r-encoding-failing-with-umlauts-such-as-%C3%A4-and-%C3%B6\">here<\/a> to explicitly specify the encoding but this does not have any effect on the batch request R script that is executed in Azure servers run by Microsoft.<\/p>\n<p>What is happening is that <code>UTF-8 characters bytes are returned as Latin-1 characters bytes<\/code>, for example<\/p>\n<blockquote>\n<ol>\n<li><p><code>\u00f6<\/code> as <code>\u00c3 \u00b6<\/code><\/p>\n<\/li>\n<li><p><code>\u00e4<\/code> as <code>\u00c3 \u00a4<\/code><\/p>\n<\/li>\n<li><p><code>\u00c4<\/code> as <code>\u00c3 \u00a5<\/code><\/p>\n<\/li>\n<\/ol>\n<\/blockquote>\n<p>as can be demonstrated and tested with this tool <a href=\"http:\/\/www.ltg.ed.ac.uk\/%7Erichard\/utf-8.cgi?input=%C3%84&amp;mode=char\" rel=\"nofollow noreferrer\">here<\/a> about Latin-1 characters. So what are best ways to deal with this encoding issue, can it be addressed somehow inside Azure ML? Where can you do bug reports? Does there exist some tool to convert Latin-1 to UTF-8 in R?<\/p>\n<p><strong>How can you get properly formatted UTF-8 files with umlauts with R batch requests in Azure ML (not in Latin-1 characters)?<\/strong><\/p>",
        "Challenge_closed_time":1486366682512,
        "Challenge_comment_count":1,
        "Challenge_created_time":1485549679360,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41902672",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":10.2,
        "Challenge_reading_time":23.2,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":226.94532,
        "Challenge_title":"Azure Machine Learning Batch request returning mal-formatted umlauts with Unixes",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":62,
        "Challenge_word_count":248,
        "Platform":"Stack Overflow",
        "Poster_created_time":1251372839052,
        "Poster_location":null,
        "Poster_reputation_count":48616.0,
        "Poster_view_count":3348.0,
        "Solution_body":"<p>The Batch request R command has a <code>saveBlobToFile<\/code> function. The problem is in the <code>saveBlobToFile<\/code> function that uses wrong encoding with <code>getUrl<\/code>.  <code>getUrl<\/code> function needs to specify the encodings explicitly. Do the following changes<\/p>\n\n<pre><code>blobContent = getURL(blobUrl, .encoding=\"UTF-8\")\n<\/code><\/pre>\n\n<p>where without <code>.encoding<\/code>, the output is <code>ISO8859-1('latin1')<\/code> or something inherited from your system.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1486367568768,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":6.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1526004205792,
        "Answerer_location":"China",
        "Answerer_reputation_count":28087.0,
        "Answerer_view_count":3298.0,
        "Challenge_adjusted_solved_time":2.4671630556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After having solved <a href=\"https:\/\/stackoverflow.com\/questions\/55347910\/\">Why does my ML model deployment in Azure Container Instance still fail?<\/a> and having deployed on ACI, I am using Azure Machine Learning Service to deploy a ML model as web service on AKS.<\/p>\n\n<p>My current (working) ACI-deployment code is<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice, AciWebservice\nfrom azureml.core.image import ContainerImage\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                      memory_gb=8, \n                      tags={\"data\": \"text\",  \"method\" : \"NB\"}, \n                      description='Predict something')\n\n\nimage_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n                      docker_file=\"Dockerfile\",\n                      runtime=\"python\", \n                      conda_file=\"myenv.yml\")\n\nimage = ContainerImage.create(name = \"scorer-image\",\n                      models = [model],\n                      image_config = image_config,\n                      workspace = ws\n                      )\n\nservice_name = 'scorer-svc'\nservice = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                        image = image,\n                                        name = service_name,\n                                        workspace = ws)\n<\/code><\/pre>\n\n<p>I would like to modify it so to deploy on AKS, but looks more convoluted than I expected, as I imagined moving from ACI to AKS (i.e. from test to production) to be a routine operation. Still, it seems to need a bit more of changes in the code than I thought: <\/p>\n\n<ul>\n<li>AKS seems to require an <code>InferenceConfig<\/code> object (?) <\/li>\n<li>with AKS there's no method like <code>deploy_from_image<\/code> for deployment from my existing Docker <code>image<\/code> (?)<\/li>\n<\/ul>\n\n<p>Can deployment be done on AKS by performing minimal changes to the ACI code instead?  <\/p>",
        "Challenge_closed_time":1557660771803,
        "Challenge_comment_count":2,
        "Challenge_created_time":1557392548383,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1557651890016,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56055868",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":11.5,
        "Challenge_reading_time":21.26,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":74.5065055556,
        "Challenge_title":"What's the easiest way to move from ACI to AKS deployment?",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":460,
        "Challenge_word_count":192,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>From the code that you have provided, when you deploy the application in the ACI using the method <code>Webservice.deploy_from_image<\/code> with the parameters <code>deployment_config<\/code> and container image. The deployment_config makes by the <code>AciWebservice.deploy_configuration<\/code>.<\/p>\n\n<p>When you take a look at the ML about AKS, you can also find the method <code>AksWebservice.deploy_configuration<\/code>. So you just need to change the method <code>AciWebservice.deploy_configuration<\/code> into <code>AksWebservice.deploy_configuration<\/code>, then the application can be deployed from ACI into AKS. And it's the minimal changes. Also, it can deploy from the docker image.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.6,
        "Solution_reading_time":9.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1440024011336,
        "Answerer_location":null,
        "Answerer_reputation_count":335.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":4.1729647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy(local) using this line:<\/p>\n\n<pre><code>local_service = Model.deploy(ws, \"test\", [model], inference_config, deployment_config)\n<\/code><\/pre>\n\n<p>Then I get this output in the terminal:<\/p>\n\n<pre><code>tarfile.ReadError: file could not be opened successfully\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WXbPe.png\" rel=\"nofollow noreferrer\">Screenshot of the output<\/a><\/p>",
        "Challenge_closed_time":1570725963836,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570710941163,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58323029",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":6.39,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":4.1729647222,
        "Challenge_title":"Azure ML deploy locally: tarfile.ReadError: file could not be opened successfully",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":77,
        "Challenge_word_count":48,
        "Platform":"Stack Overflow",
        "Poster_created_time":1570704481987,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>There was a bug with the retry logic when files were being uploaded. That bug has since been fixed, so updating your SDK should fix the issue.<\/p>\n\n<p>Similar post: <a href=\"https:\/\/stackoverflow.com\/questions\/57854136\/registering-and-downloading-a-fasttext-bin-model-fails-with-azure-machine-learn\">Registering and downloading a fastText .bin model fails with Azure Machine Learning Service<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":5.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":105.0398277778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a training script in Sagemaker like,<\/p>\n\n<pre><code>def train(current_host, hosts, num_cpus, num_gpus, channel_input_dirs, model_dir, hyperparameters, **kwargs):\n    ... Train a network ...\n    return net\n\ndef save(net, model_dir):\n    # save the model\n    logging.info('Saving model')\n    y = net(mx.sym.var('data'))\n    y.save('%s\/model.json' % model_dir)\n    net.collect_params().save('%s\/model.params' % model_dir)\n\ndef model_fn(model_dir):\n    symbol = mx.sym.load('%s\/model.json' % model_dir)\n    outputs = mx.symbol.softmax(data=symbol, name='softmax_label')\n    inputs = mx.sym.var('data')\n    param_dict = gluon.ParameterDict('model_')\n    net = gluon.SymbolBlock(outputs, inputs, param_dict)\n    net.load_params('%s\/model.params' % model_dir, ctx=mx.cpu())\n    return net\n<\/code><\/pre>\n\n<p>Most of which I stole from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">MNIST Example<\/a>.<\/p>\n\n<p>When I train, everything goes fine, but when trying to deploy like,<\/p>\n\n<pre><code>m = MXNet(\"lstm_trainer.py\", \n          role=role, \n          train_instance_count=1, \n          train_instance_type=\"ml.c4.xlarge\",\n          hyperparameters={'batch_size': 100, \n                         'epochs': 20, \n                         'learning_rate': 0.1, \n                         'momentum': 0.9, \n                         'log_interval': 100})\nm.fit(inputs) # No errors\npredictor = m.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>I get, (<a href=\"https:\/\/gist.github.com\/aidan-plenert-macdonald\/7eb7ba7402790b61596938b5cbf605b6\" rel=\"nofollow noreferrer\">full output<\/a>)<\/p>\n\n<pre><code>INFO:sagemaker:Creating model with name: sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\n---------------------------------------------------------------------------\n  ... Stack dump ...\nClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3:\/\/sagemaker-us-west-2-01234567890\/sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\/output\/model.tar.gz.\n<\/code><\/pre>\n\n<p>Looking in my S3 bucket <code>s3:\/\/sagemaker-us-west-2-01234567890\/sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\/output\/model.tar.gz<\/code>, I in fact don't see the model.<\/p>\n\n<p>What am I missing?<\/p>",
        "Challenge_closed_time":1516602490380,
        "Challenge_comment_count":2,
        "Challenge_created_time":1516224347000,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48310237",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":16.3,
        "Challenge_reading_time":30.07,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":105.0398277778,
        "Challenge_title":"Sagemaker \"Could not find model data\" when trying to deploy my model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2950,
        "Challenge_word_count":164,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384712661608,
        "Poster_location":"San Diego, CA, United States",
        "Poster_reputation_count":5151.0,
        "Poster_view_count":1038.0,
        "Solution_body":"<p>When you are calling the training job you should specify the output directory:<\/p>\n\n<pre><code>#Bucket location where results of model training are saved.\nmodel_artifacts_location = 's3:\/\/&lt;bucket-name&gt;\/artifacts'\n\nm = MXNet(entry_point='lstm_trainer.py',\n          role=role,\n          output_path=model_artifacts_location,\n          ...)\n<\/code><\/pre>\n\n<p>If you don't specify the output directory the function will use a default location, that it might not have the permissions to create or write to.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":6.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1343167997556,
        "Answerer_location":null,
        "Answerer_reputation_count":191.0,
        "Answerer_view_count":27.0,
        "Challenge_adjusted_solved_time":132.4537347222,
        "Challenge_answer_count":2,
        "Challenge_body":"<h2><strong>ASKING THIS HERE AT THE EXPLICIT REQUEST OF THE MICROSOFT AZURE SUPPORT TEAM.<\/strong><\/h2>\n\n<p>I've been attempting to call the MS Luis.ai <em>programmatic<\/em> API (bit.ly\/2iev01n) and have been receiving a 401 unauthorized response to every request. Here's a simple GET example: <code>https:\/\/api.projectoxford.ai\/luis\/v1.0\/prog\/apps\/{appId}\/entities?subscription-key={subscription_key}<\/code>.  <\/p>\n\n<p>I am providing my appId from the Luis.ai GUI (as specified by the API docs), here:<br>\n<img src=\"https:\/\/i.stack.imgur.com\/Cg2Fw.png\" alt=\"Luis.ai App Settings App Id\"><\/p>\n\n<p>I am providing my subscription key from Azure (as specified by the API docs), here:<br>\n<img src=\"https:\/\/i.stack.imgur.com\/GS2Fe.png\" alt=\"Azure Console\"><\/p>\n\n<p>The app ID and subscription key, sourced from above, are the exact same as what I'm using to hit the query API successfully (see note at bottom). My account is pay-as-you-go (not free).<\/p>\n\n<p><strong><em>Am I doing something wrong here? Is this API deprecated, moved, down, or out-of-sync with the docs?<\/em><\/strong><\/p>\n\n<p><strong>NOTE:<\/strong> I can manipulate my model through the online GUI but that approach will be far too manual for our business needs where our model will need to be programmatically updated as new business entities come into existence.  <\/p>\n\n<p><strong>NOTE:<\/strong> The programmatic API is different from the query API which has this request URL, which is working fine for me:<br>\n<code>https:\/\/api.projectoxford.ai\/luis\/v2.0\/apps\/{appId}?subscription-key={subscription_key}&amp;verbose=true&amp;q={utterance}<\/code>  <\/p>\n\n<p><strong>NOTE:<\/strong> There doesn't seem to be a Luis.ai programmatic API for v2.0--which is why the URLs from the query and programmatic APIs have different versions.  <\/p>",
        "Challenge_closed_time":1484669845332,
        "Challenge_comment_count":2,
        "Challenge_created_time":1484180085280,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1484193011887,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41603082",
        "Challenge_link_count":4,
        "Challenge_participation_count":4,
        "Challenge_readability":10.4,
        "Challenge_reading_time":23.71,
        "Challenge_score_count":4,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":136.0444588889,
        "Challenge_title":"401 Errors Calling the Microsoft Luis.ai Programmatic API",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1280,
        "Challenge_word_count":229,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343167997556,
        "Poster_location":null,
        "Poster_reputation_count":191.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>Answering my own question here:<\/p>\n\n<p>I have found my LUIS.ai programmatic API key. It is found by:\nLUIS.ai dashboard -> username (upper-right) -> settings in dropdown -> Subscription Keys tab -> Programmatic API Key<\/p>\n\n<p>It was not immediately obvious since it's found nowhere else: not alongside any of the other key listings in cognitive services or the LUIS.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.63,
        "Solution_score_count":7.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1441651557140,
        "Answerer_location":"Parker, CO, USA",
        "Answerer_reputation_count":851.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":0.9014630556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to follow AWS Sagemaker tutorial to train a machine learning model with a Jupyter notebook environment. <\/p>\n\n<p>According to the tutorial, I'm supposed to copy the following code and run it to import required libraries and set environment variables. <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                \nimport pandas as pd                               \nimport matplotlib.pyplot as plt                   \nfrom IPython.display import Image                 \nfrom IPython.display import display               \nfrom time import gmtime, strftime                 \nfrom sagemaker.predictor import csv_serializer   \n\n# Define IAM role\nrole = get_execution_role()\nprefix = 'sagemaker\/DEMO-xgboost-dm'\ncontainers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com\/xgboost:latest',\n              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest',\n              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest',\n              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'} # each region has its XGBoost container\nmy_region = boto3.session.Session().region_name # set the region of the instance\nprint(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")\n<\/code><\/pre>\n\n<p>And the expected outcome is below.  <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wLJ2j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wLJ2j.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>However, I am getting this error.<\/p>\n\n<blockquote>\n  <p>KeyError                                  Traceback (most recent call last)\n   in ()\n       18               'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'} # each region has its XGBoost container\n       19 my_region = boto3.session.Session().region_name # set the region of the instance\n  ---> 20 print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")<\/p>\n  \n  <p>KeyError: 'ap-northeast-2'<\/p>\n<\/blockquote>\n\n<p>I assume that this is happening because my region is <strong>\"ap-northeast-2\"<\/strong>. \nI have a feeling that I need to change the containers for my region.  <\/p>\n\n<p><strong>If my guess is correct, how can I find containers for my region?<\/strong><br>\n<strong>Also, am I overlooking anything else?<\/strong> <\/p>",
        "Challenge_closed_time":1576464384063,
        "Challenge_comment_count":1,
        "Challenge_created_time":1576462664100,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59349805",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":32.29,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":0.4777675,
        "Challenge_title":"How to find XGBoost containers for different regions in AWS Sagemaker",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2197,
        "Challenge_word_count":264,
        "Platform":"Stack Overflow",
        "Poster_created_time":1539556112483,
        "Poster_location":"Seoul, South Korea",
        "Poster_reputation_count":2954.0,
        "Poster_view_count":1143.0,
        "Solution_body":"<p>I expect your rational is correct. There isn't an entry for your region in the code. I don't know if there's a list of these containers per region. That being said, you find them in ECR (Elastic Container Registry). <\/p>\n\n<p>Keep in mind, that you can probably fix this quickly by switching to one of the supported regions. Otherwise:<\/p>\n\n<p>If AWS doesn't have a publicly listed container in your region, you can register the container yourself in AWS with ECR. You'll need to login to ECR using the AWS CLI and docker login.<\/p>\n\n<p>You can use the command <code>aws ecr get-login --region ap-northeast-2<\/code> in order to get the token you'll need for docker login.<\/p>\n\n<p>Then, clone this repo: <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-xgboost-container<\/a><\/p>\n\n<p>You can build this image locally and push it up to ECR. After that, login to the AWS console (or use the AWS CLI) and find the ARN of the image. It should match the format of the others in your code. <\/p>\n\n<p>After that, just add another key\/value entry into the code for your <code>containers<\/code> variable and use <code>'ap-northeast-2': '&lt;ARN of the docker image&gt;'<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1576465909367,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":15.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":187.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1559910246180,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":2046.0,
        "Answerer_view_count":369.0,
        "Challenge_adjusted_solved_time":1095.2252230556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>based on the aws documentation, maximum timeout limit is less that 30 seconds in api gateway.so hooking up an sagemaker endpoint with api gateway wouldn't make sense, if the request\/response is going to take more than 30 seconds. is there any workaround ? adding a lambda in between api gateway and sagemaker endpoint is going to add more time to process request\/response, which i would like to avoid. also, there will be added time for lambda cold starts and sagemaker serverless endpoints are built on top of lambda so that will also add cold start time. is there a way to invoke the serverless sagemaker endpoints , without these overhead?<\/p>",
        "Challenge_closed_time":1645795642143,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645755688313,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71260306",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":8.68,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":11.0982861111,
        "Challenge_title":"How to access\/invoke a sagemaker endpoint without lambda?",
        "Challenge_topic":"Lambda Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1122,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1590797441983,
        "Poster_location":null,
        "Poster_reputation_count":525.0,
        "Poster_view_count":98.0,
        "Solution_body":"<p>It is indeed possible to invoke sagemaker endpoints from sagemaker without using any other AWS services and that is also manifested by the fact that they have invocation URLs.<\/p>\n<p>Here's how you set it up:<\/p>\n<ol>\n<li>create an user with only programmatic access and attach a policy json that should look something like below:<\/li>\n<\/ol>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:sagemaker:&lt;region&gt;:&lt;account-id&gt;:endpoint\/&lt;endpoint-name&gt;&quot;\n        }\n    ]\n} \n<\/code><\/pre>\n<p>you can replace <code>&lt;endpoint-name&gt;<\/code> with <code>*<\/code> to let this user invoke all endpoints.<\/p>\n<ol start=\"2\">\n<li><p>use the ACCESS-KEY and SECRET-ACCESS-KEY to configure authorisation in postman like shown in this screenshot. also add the parameters in advanced tab like shown in the screenshot.\n<a href=\"https:\/\/i.stack.imgur.com\/cYkTf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cYkTf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>then fill up your body with the relevant content type.<\/p>\n<\/li>\n<li><p>then add or remove additional headers like variant-name or model-name, if you have them set up and the headers should look like shown in this screenshot: <a href=\"https:\/\/i.stack.imgur.com\/NLqkV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NLqkV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>send the request to receive reponse like this\n<a href=\"https:\/\/i.stack.imgur.com\/uA4kF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uA4kF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<\/ol>\n<p><em><strong>URL and credentials in the above screenshots doesn't work anymore, duh!<\/strong><\/em><\/p>\n<p>and if you want code to invoke the endpoint directly using some back-end language, <a href=\"https:\/\/stackoverflow.com\/a\/70803026\/11814996\">here's code for python<\/a>.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1649698499116,
        "Solution_link_count":7.0,
        "Solution_readability":13.4,
        "Solution_reading_time":27.72,
        "Solution_score_count":3.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":219.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":394.9302947222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I understand that you can pass a CSV file from S3 into a Sagemaker XGBoost container using the following code<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>train_channel = sagemaker.session.s3_input(train_data, content_type='text\/csv')\nvalid_channel = sagemaker.session.s3_input(validation_data, content_type='text\/csv')\n\ndata_channels = {'train': train_channel, 'validation': valid_channel}\nxgb_model.fit(inputs=data_channels,  logs=True)\n<\/code><\/pre>\n\n<p>But I have an ndArray stored in S3 bucket. These are processed, label encoded, feature engineered arrays. I would want to pass this into the container instead of the csv. I do understand I can always convert my ndarray into csv files before saving it in S3. Just checking if there is an array option.<\/p>",
        "Challenge_closed_time":1568417822248,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566996073187,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57692681",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":10.81,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":394.9302947222,
        "Challenge_title":"Sagemaker to use processed pickled ndarray instead of csv files from S3",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":92,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1363437641347,
        "Poster_location":"Chennai, India",
        "Poster_reputation_count":468.0,
        "Poster_view_count":97.0,
        "Solution_body":"<p>There are multiple options for algorithms in SageMaker:<\/p>\n\n<ol>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">Built-in algorithms<\/a>, like the SageMaker XGBoost you mention<\/li>\n<li>Custom, user-created algorithm code, which can be:\n\n<ul>\n<li>Written for a pre-built docker image, available for Sklearn, TensorFlow, Pytorch, MXNet<\/li>\n<li>Written in your own container<\/li>\n<\/ul><\/li>\n<\/ol>\n\n<p>When you use built-ins (option 1), your choice of data format options is limited to what the built-ins support, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#InputOutput-XGBoost\" rel=\"nofollow noreferrer\">which is only csv and libsvm in the case of the built-in XGBoost<\/a>. If you want to use custom data formats and pre-processing logic before XGBoost, it is absolutely possible if you use your own script leveraging the open-source XGBoost. You can get inspiration from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">Random Forest demo<\/a> to see how to create custom models in pre-built containers<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":17.7,
        "Solution_reading_time":16.03,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":131.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":66.8869986111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a limit on the rate of inferences one can make for a SageMaker endpoint?<\/p>\n\n<p>Is it determined somehow by the instance type behind the endpoint or the number of instances?<\/p>\n\n<p>I tried looking for this info as <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws_service_limits.html#limits_sagemaker\" rel=\"nofollow noreferrer\">AWS Service Quotas for SageMaker<\/a> but couldn't find it.<\/p>\n\n<p>I am invoking the endpoint from a Spark job abd wondered if the number of concurrent tasks is a factor I should be taking care of when running inference (assuming each task runs one inference at a time) <\/p>\n\n<p>Here's the throttling error I got:<\/p>\n\n<pre><code>com.amazonaws.services.sagemakerruntime.model.AmazonSageMakerRuntimeException: null (Service: AmazonSageMakerRuntime; Status Code: 400; Error Code: ThrottlingException; Request ID: b515121b-f3d5-4057-a8a4-6716f0708980)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.doInvoke(AmazonSageMakerRuntimeClient.java:236)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.invoke(AmazonSageMakerRuntimeClient.java:212)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.executeInvokeEndpoint(AmazonSageMakerRuntimeClient.java:176)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.invokeEndpoint(AmazonSageMakerRuntimeClient.java:151)\n    at lineefd06a2d143b4016906a6138a6ffec15194.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$a5cddfc4633c5dd8aa603ddc4f9aad5$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Predictor.predict(command-2334973:41)\n    at lineefd06a2d143b4016906a6138a6ffec15200.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$50a9225beeac265557e61f69d69d7d$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(command-2307906:11)\n    at lineefd06a2d143b4016906a6138a6ffec15200.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$50a9225beeac265557e61f69d69d7d$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(command-2307906:11)\n    at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n    at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:2000)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1220)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1220)\n    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n    at org.apache.spark.scheduler.Task.run(Task.scala:113)\n    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n<\/code><\/pre>",
        "Challenge_closed_time":1579951518923,
        "Challenge_comment_count":2,
        "Challenge_created_time":1579709239420,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1579710725728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59863842",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":47.7,
        "Challenge_reading_time":60.07,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":67.2998619445,
        "Challenge_title":"Limit on the rate of inferences one can make for a SageMaker endpoint",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1716,
        "Challenge_word_count":186,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>Amazon SageMaker is offering model hosting service (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-hosting.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-hosting.html<\/a>), which gives you a lot of flexibility based on your inference requirements. <\/p>\n\n<p>As you noted, first you can choose the instance type to use for your model hosting. The large set of options is important to tune to your models. You can host the model on a GPU based machines (P2\/P3\/P4) or CPU ones. You can have instances with faster CPU (C4, for example), or more RAM (R4, for example). You can also choose instances with more cores (16xl, for example) or less (medium, for example). Here is a list of the full range of instances that you can choose: <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/<\/a> . It is important to balance your performance and costs. The selection of the instance type and the type and size of your model will determine the invocations-per-second that you can expect from your model in this single-node configuration. It is important to measure this number to avoid hitting the throttle errors that you saw. <\/p>\n\n<p>The second important feature of the SageMaker hosting that you use is the ability to auto-scale your model to multiple instances. You can configure the endpoint of your model hosting to automatically add and remove instances based on the load on the endpoint. AWS is adding a load balancer in front of the multiple instances that are hosting your models and distributing the requests among them. Using the autoscaling functionality allows you to keep a smaller instance for low traffic hours, and to be able to scale up during peak traffic hours, and still keep your costs low and your throttle errors to the minimum. See here for documentation on the SageMaker autoscaling options: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling.html<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":11.6,
        "Solution_reading_time":27.59,
        "Solution_score_count":4.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":289.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1533754693910,
        "Answerer_location":null,
        "Answerer_reputation_count":801.0,
        "Answerer_view_count":80.0,
        "Challenge_adjusted_solved_time":1373.19555,
        "Challenge_answer_count":1,
        "Challenge_body":"<ol>\n<li><p>task : object_detection<\/p>\n<\/li>\n<li><p>environment: AWS sagemaker<\/p>\n<\/li>\n<li><p>instance type: 'ml.p2.xlarge' | num_instances = 1<\/p>\n<\/li>\n<li><p>Main file to be run: <a href=\"https:\/\/github.com\/tensorflow\/models\/blob\/master\/research\/object_detection\/model_main_tf2.py\" rel=\"nofollow noreferrer\">original<\/a><\/p>\n<\/li>\n<li><p>Problematic code segment from the main file:<\/p>\n<pre><code>    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n    FLAGS.tpu_name)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n    elif FLAGS.num_workers &gt; 1:\n        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n    else:\n        strategy = tf.compat.v2.distribute.MirroredStrategy()\n<\/code><\/pre>\n<\/li>\n<li><p>Problem : Can't find the proper value to be given as <code>tpu_name<\/code> argument.<\/p>\n<\/li>\n<li><p>My research on the problem:<\/p>\n<\/li>\n<\/ol>\n<p>According to the tensorflow documentation in <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/cluster_resolver\/TPUClusterResolver\" rel=\"nofollow noreferrer\">tf.distribute.cluster_resolver.TPUClusterResolver<\/a>, it says that this resolver works only on Google Cloud platform.<\/p>\n<blockquote>\n<p>This is an implementation of cluster resolvers for the Google Cloud\nTPU service.<\/p>\n<p>TPUClusterResolver supports the following distinct environments:\nGoogle Compute Engine Google Kubernetes Engine Google internal<\/p>\n<p>It can be passed into tf.distribute.TPUStrategy to support TF2\ntraining on Cloud TPUs.<\/p>\n<\/blockquote>\n<p>But from <a href=\"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/39721\" rel=\"nofollow noreferrer\">this issue in github<\/a>, I found out that a similar code also works in Azure.<\/p>\n<ol start=\"8\">\n<li>My question :<\/li>\n<\/ol>\n<p>Is there a way I can bypass this resolver and initialize my tpu in <strong>sagemaker<\/strong> ?<\/p>\n<p>Even better, if I can find a way to insert the name or url of sagemaker gpu to the resolver and initiate it from there ?<\/p>",
        "Challenge_closed_time":1614007459500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609059338347,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1609063955520,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65464181",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":15.6,
        "Challenge_reading_time":28.92,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":1374.4780980556,
        "Challenge_title":"An alternative to tf.distribute.cluster_resolver.TPUClusterResolver( tpu_name) to be used in Sagemaker?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":355,
        "Challenge_word_count":210,
        "Platform":"Stack Overflow",
        "Poster_created_time":1517147266416,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>Let me clarify some confusion here. TPUs are only offered on Google Cloud and the <code>TPUClusterResolver<\/code> implementation queries GCP APIs to get the cluster config for the TPU node. Thus, no you can't use <code>TPUClusterResolver<\/code> with AWS sagemaker, but you should try it out with TPUs on GCP instead or try find some other documentation on Sagemaker's end on how they enable cluster resolving on their end (if they do).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.2,
        "Solution_reading_time":5.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":0.3643536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to AWS and I'm trying out AWS Sagemaker. I'm currently doing my project which involves quite a long time to finish and I don't think I can finish it in a day. I'm worried if I close my JupyterLab of my notebook instance in SageMaker, my code will be gone. How do I save my code and cell run progress when using Sagemaker?<\/p>",
        "Challenge_closed_time":1619085494700,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619084183027,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67210677",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.8,
        "Challenge_reading_time":4.88,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.3643536111,
        "Challenge_title":"If I close my JupyterLab from notebook instance, would my code be gone?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":305,
        "Challenge_word_count":77,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605938672327,
        "Poster_location":"Jakarta Selatan, South Jakarta City, Jakarta, Indonesia",
        "Poster_reputation_count":97.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>If you are training directly in the notebook the answer is yes.\nHowever the best practice is not to train directly with the notebook.\nUse instead the notebook (you can choose a very cheap instance for the notebook) to launch your training job (in the instance type you desire) adapting you code to be the entrypoint of the estimator. In that way, you can close the notebook after launching the training job and monitor the training job using cloudwatch. You can also define some regex to capture metrics from the stout and cloudwatch will automatically plot for you, which is very useful!\nAs a quick example.. in my notebook I have this cell:<\/p>\n<pre><code>import sagemaker from sagemaker.tensorflow import TensorFlow from sagemaker import get_execution_role\n\nbucket = 'mybucket'\n\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\n\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\n\ns3_output_location = 's3:\/\/{}'.format(bucket)\n\nhyperparameters = {'epochs': 70, 'batch-size' : 32, 'learning-rate' :\n0.01}\n\nmetrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\\.]+)'},\n           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\\.]+)'},\n           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\\.]+)'},\n           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\\.]+)'},\n           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\\.]+)'}]\n\ntf_estimator = TensorFlow(entry_point='training.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          train_max_run=172800,\n                          output_path=s3_output_location,\n                          framework_version='1.12',\n                          py_version='py3',\n                          metric_definitions = metrics,\n                          hyperparameters = hyperparameters)\n\ninputs = {'train': train_data, 'test': validation_data}\n\nmyJobName = 'myname'\n\ntf_estimator.fit(inputs=inputs, job_name=myJobName)\n<\/code><\/pre>\n<p>My training script training.py is something like this:<\/p>\n<pre><code>if __name__ =='__main__':\n\n    parser = argparse.ArgumentParser()\n\n    # input data and model directories\n    parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--learning-rate', type=float, default=0.0001)\n    parser.add_argument('--batch-size', type=int, default=32)\n    parser.add_argument('--epochs', type=int, default=1)\n....\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":32.13,
        "Solution_score_count":2.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":227.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":63.6310063889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a python notebook which is initiating a Batch Transform Job in Sagemaker. However, I want to also print the status &quot;Failed&quot;, &quot;In Progress&quot; and &quot;Completed&quot; once the job is complete running. As of now, I am only able to start the Batch Transform Job (rf=random forest) but I am not certain how to get the job status print outs. Can someone help with that given my script below?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/SU0ln.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/SU0ln.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code>rf_transformer = rf.transformer(\n                                instance_count,\n                                instance_type,\n                                strategy=strategy,\n                                output_path=output_path,\n                                max_payload=max_payload)\n\nrf_transformer.transform(\n                                str('s3:\/\/batch_scoring\/rf_output),\n                                content_type='text\/csv',\n                                compression_type='Gzip'\n                         )\n<\/code><\/pre>",
        "Challenge_closed_time":1594154231750,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593925160127,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62737008",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":12.34,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":63.6310063889,
        "Challenge_title":"How to get Sagemaker Batch Transform Job status printed out in my python notebook?",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":612,
        "Challenge_word_count":106,
        "Platform":"Stack Overflow",
        "Poster_created_time":1591222877740,
        "Poster_location":null,
        "Poster_reputation_count":91.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>You can do it with:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>job_name = rf_transformer.latest_transform_job.name\nrf_transformer.sagemaker_session.describe_transform_job(job_name)['TransformJobStatus']\n<\/code><\/pre>\n<p>You can also use the AWS SDK directly, if you wish:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nsagemaker_client = boto3.client('sagemaker')\nsagemaker_client.describe_transform_job(job_name)['TransformJobStatus']\n<\/code><\/pre>\n<p>API documentation: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTransformJob.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":37.5,
        "Solution_reading_time":10.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1384322523967,
        "Answerer_location":null,
        "Answerer_reputation_count":574.0,
        "Answerer_view_count":66.0,
        "Challenge_adjusted_solved_time":42.996675,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using mlflow to log parameters and artifacts of a Logistic Regression, but when I try to log the model so I can see all the files in the Mlflow UI, I see two folders: one named 'model' and the other one named 'logger' (the one I set).<\/p>\n<pre><code>model = LogisticRegression()\n\nmlflow.set_tracking_uri('file:\/\/\/artifacts')\nmlflow.set_experiment('test')\nmlflow.autolog()\n\nwith mlflow.start_run(run_name=run_name) as run:\n   model.train(X_train, y_train)\n   mlflow.sklearn.log_model(model, 'logreg')\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BtIHo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BtIHo.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Not sure if I'm missing something or if there's a configuration for that.<\/p>\n<p>I hope someone out there can help me!<\/p>",
        "Challenge_closed_time":1655933087183,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655778299153,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72694707",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":11.36,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":42.996675,
        "Challenge_title":"Multiple artifact paths when logging a model using mlflow and sklearn",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":122,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1544467691223,
        "Poster_location":"Zacatecas, Mexico",
        "Poster_reputation_count":131.0,
        "Poster_view_count":42.0,
        "Solution_body":"<p>You have set <code>autolog<\/code> and you are also logging the model explicitly. Remove one and then try.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.41,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":2.3951186111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've got a basic ScriptStep in my AML Pipeline and it's just trying to read an attached dataset. When i execute this simple example, the pipeline fails with the following in the driver log:<\/p>\n\n<blockquote>\n  <p>ImportError: azureml-dataprep is not installed. Dataset cannot be used\n  without azureml-dataprep. Please make sure\n  azureml-dataprep[fuse,pandas] is installed by specifying it in the\n  conda dependencies. pandas is optional and should be only installed if\n  you intend to create a pandas DataFrame from the dataset.<\/p>\n<\/blockquote>\n\n<p>I then modified my step to include the conda package but then the driver fails with \"ResolvePackageNotFound: azureml-dataprep\". The entire log file can be accessed <a href=\"https:\/\/www.dropbox.com\/s\/372ht6jkvzu9loo\/conda.err.txt?dl=0\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<pre><code># create a new runconfig object\nrun_config = RunConfiguration()\nrun_config.environment.docker.enabled = True\nrun_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\nrun_config.environment.python.user_managed_dependencies = False\nrun_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['azureml-dataprep[pandas,fuse]'])\n\nsource_directory = '.\/read-step'\nprint('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))\nstep2 = PythonScriptStep(name=\"read_step\",\n                         script_name=\"Read.py\", \n                         arguments=[\"--dataFilePath\", dataset.as_named_input('local_ds').as_mount() ],\n                         compute_target=aml_compute, \n                         source_directory=source_directory,\n                         runconfig=run_config,\n                         allow_reuse=False)\n<\/code><\/pre>\n\n<p>I'm out of ideas, would deeply appreciate any help here!<\/p>",
        "Challenge_closed_time":1587162956780,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587154334353,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1591825691630,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61279914",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":22.39,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":2.3951186111,
        "Challenge_title":"AzureML: ResolvePackageNotFound azureml-dataprep",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1244,
        "Challenge_word_count":155,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330016065408,
        "Poster_location":null,
        "Poster_reputation_count":1704.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>The <code>azureml-sdk<\/code> isn't available on conda, you need to install it with <code>pip<\/code>.<\/p>\n\n<pre><code>myenv = Environment(name=\"myenv\")\nconda_dep = CondaDependencies().add_pip_package(\"azureml-dataprep[pandas,fuse]\")\nmyenv.python.conda_dependencies=conda_dep\nrun_config.environment = myenv\n<\/code><\/pre>\n\n<p>For more information, about this error, the logs tab has a log named <code>20_image_build_log.txt<\/code> which Docker build logs. It contains the error where <code>conda<\/code> failed to failed to find <code>azureml-dataprep<\/code><\/p>\n\n<p>EDIT:<\/p>\n\n<p>Soon, you won't have to specify this dependency anymore. the Azure Data4ML team says <code>azureml-dataprep[pandas,fuse]<\/code> is getting added as a dependency for <code>azureml-defaults<\/code> which is automatically installed on all images. <\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1587416360076,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":10.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1513106638900,
        "Answerer_location":null,
        "Answerer_reputation_count":276.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":1546.0056783333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am building a time series usecase to automate the preprocess and retrain tasks.At first the data is preprocessed using numpy, pandas, statsmodels etc &amp; later a machine learning algorithm is applied to make predictions.\nThe reason for using inference pipeline is that it reuses the same preprocess code for training and inference. I have checked the examples given by AWS sagemaker team with spark and sci-kit learn. In both the examples they use a sci-kit learn container to fit &amp; transform their preprocess code. Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code? <\/p>\n\n<p>Can someone give me a custom example of using these pipelines? Any help is appreciated!<\/p>\n\n<p><strong>Sources looked into:<\/strong><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline<\/a>\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia<\/a><\/p>",
        "Challenge_closed_time":1575505485728,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574408498230,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58989610",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":19.05,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":304.7187494444,
        "Challenge_title":"How to custom code an inference pipeline in AWS sagemaker?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1186,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553712330910,
        "Poster_location":null,
        "Poster_reputation_count":103.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Apologies for the late response.<\/p>\n\n<p>Below is some documentation on inference pipelines:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html<\/a>\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-real-time.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-real-time.html<\/a><\/p>\n\n<blockquote>\n  <p>Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code?<\/p>\n<\/blockquote>\n\n<p>Your container is an encapsulation of the environment needed for your custom code needed to run properly. Based on the requirements listed above, <code>numpy, pandas, statsmodels etc &amp; later a machine learning algorithm<\/code>, I would create a container if you wish to isolate your dependencies or modify an existing predefined SageMaker container, such as the scikit-learn one, and add your dependencies into that.<\/p>\n\n<blockquote>\n  <p>Can someone give me a custom example of using these pipelines? Any help is appreciated!<\/p>\n<\/blockquote>\n\n<p>Unfortunately, the two example notebooks referenced above are the only examples utilizing inference pipelines. The biggest hurdle most likely is creating containers that fulfill the preprocessing and prediction task you are seeking and then combining those two together into the inference pipeline.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1579974118672,
        "Solution_link_count":4.0,
        "Solution_readability":15.8,
        "Solution_reading_time":19.52,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":168.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1487197909143,
        "Answerer_location":"Brussels, Belgium",
        "Answerer_reputation_count":2206.0,
        "Answerer_view_count":169.0,
        "Challenge_adjusted_solved_time":0.3434,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to gunicorn and heroku so I would appreciate any help. I want to deploy my python Dash app on to heroku and I know I need a Procfile. The thing is that my project structure uses the Kedro structure and my structure looks like this:<\/p>\n<pre><code>myproject\n    .... # Kedro-generated files\n    src\/\n        package1\/\n            package2\/\n                __init__.py\n                index.py\n    Procfile\n<\/code><\/pre>\n<p>index.py is a Dash application like so<\/p>\n<pre><code>#imports up here\n\napp = dash.Dash(__name__, external_stylesheets=external_stylesheets)\nserver = app.server\n\n.......  # main code chunk\n\nif __name__ == '__main__':\napp.run_server(debug=True)\n<\/code><\/pre>\n<p>Currently, my Procfile looks like this:<\/p>\n<pre><code>web: gunicorn src frontend.index:app\n<\/code><\/pre>\n<p>My project uploads to heroku just fine but I'm getting this error in my log:<\/p>\n<pre><code>2020-08-21T06:46:46.433935+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 941, in _find_and_load_unlocked\n2020-08-21T06:46:46.433935+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 953, in _find_and_load_unlocked\n2020-08-21T06:46:46.433962+00:00 app[web.1]: ModuleNotFoundError: No module named 'frontend'\n2020-08-21T06:46:46.434082+00:00 app[web.1]: [2020-08-21 06:46:46 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:46:46.464346+00:00 app[web.1]: Traceback (most recent call last):\n2020-08-21T06:46:46.464367+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 202, in run\n2020-08-21T06:46:46.464715+00:00 app[web.1]: self.manage_workers()\n2020-08-21T06:46:46.464732+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 545, in manage_workers\n2020-08-21T06:46:46.465049+00:00 app[web.1]: self.spawn_workers()\n2020-08-21T06:46:46.465054+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 617, in spawn_workers\n2020-08-21T06:46:46.465412+00:00 app[web.1]: time.sleep(0.1 * random.random())\n2020-08-21T06:46:46.465417+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n2020-08-21T06:46:46.465617+00:00 app[web.1]: self.reap_workers()\n2020-08-21T06:46:46.465622+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n2020-08-21T06:46:46.465905+00:00 app[web.1]: raise HaltServer(reason, self.WORKER_BOOT_ERROR)\n2020-08-21T06:46:46.465950+00:00 app[web.1]: gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n2020-08-21T06:46:46.465964+00:00 app[web.1]: \n2020-08-21T06:46:46.465965+00:00 app[web.1]: During handling of the above exception, another exception occurred:\n2020-08-21T06:46:46.465965+00:00 app[web.1]: \n2020-08-21T06:46:46.465969+00:00 app[web.1]: Traceback (most recent call last):\n2020-08-21T06:46:46.465969+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/bin\/gunicorn&quot;, line 8, in &lt;module&gt;\n2020-08-21T06:46:46.466103+00:00 app[web.1]: sys.exit(run())\n2020-08-21T06:46:46.466107+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in run\n2020-08-21T06:46:46.466254+00:00 app[web.1]: WSGIApplication(&quot;%(prog)s [OPTIONS] [APP_MODULE]&quot;).run()\n2020-08-21T06:46:46.466258+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 228, in run\n2020-08-21T06:46:46.466464+00:00 app[web.1]: super().run()\n2020-08-21T06:46:46.466470+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 72, in run\n2020-08-21T06:46:46.466601+00:00 app[web.1]: Arbiter(self).run()\n2020-08-21T06:46:46.466606+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 229, in run\n2020-08-21T06:46:46.466790+00:00 app[web.1]: self.halt(reason=inst.reason, exit_status=inst.exit_status)\n2020-08-21T06:46:46.466794+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 342, in halt\n2020-08-21T06:46:46.467031+00:00 app[web.1]: self.stop()\n2020-08-21T06:46:46.467032+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 393, in stop\n2020-08-21T06:46:46.467262+00:00 app[web.1]: time.sleep(0.1)\n2020-08-21T06:46:46.467267+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n2020-08-21T06:46:46.467468+00:00 app[web.1]: self.reap_workers()\n2020-08-21T06:46:46.467469+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n2020-08-21T06:46:46.467750+00:00 app[web.1]: raise HaltServer(reason, self.WORKER_BOOT_ERROR)\n2020-08-21T06:46:46.467754+00:00 app[web.1]: gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n2020-08-21T06:46:46.559947+00:00 heroku[web.1]: Process exited with status 1\n2020-08-21T06:46:46.610907+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:47:03.000000+00:00 app[api]: Build succeeded\n2020-08-21T06:49:12.915422+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=781ff03f-db0d-40ad-996f-1d25ff3fd026 fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:49:13.357185+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=51bff951-d0fa-4e01-ba38-60f44cbe373b fwd=&quot;18.217.223.118&quot; dyno= connect= service= status=503 bytes= protocol=http\n2020-08-21T06:49:13.955353+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/favicon.ico&quot; host=protected-coast-07061.herokuapp.com request_id=632cc0a9-e052-43c9-a90c-62f99dfbba5c fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n<\/code><\/pre>\n<pre><code>2020-08-21T06:52:18.372623+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:52:32.487313+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index`\n2020-08-21T06:52:34.595212+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:52:34.595933+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:17241 (4)\n2020-08-21T06:52:34.596051+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:52:34.600183+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:52:34.603725+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:34.603887+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:52:34.626625+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [11] [INFO] Booting worker with pid: 11\n2020-08-21T06:52:34.629877+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:34.629978+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:52:34.733270+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:52:34.733356+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:52:34.800675+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:52:34.837697+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:52:34.839731+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:52:49.188229+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index`\n2020-08-21T06:52:50.000000+00:00 app[api]: Build succeeded\n2020-08-21T06:52:51.154243+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:52:51.154956+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:46031 (4)\n2020-08-21T06:52:51.155075+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:52:51.158999+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:52:51.162147+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:51.162261+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:52:51.189291+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:52:51.189375+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:52:51.249579+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:52:51.281288+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:53:27.313026+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=67b1f83d-37a8-4ad1-b522-2e7cc0bd7b7d fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:53:28.196639+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/favicon.ico&quot; host=protected-coast-07061.herokuapp.com request_id=0d13d80e-ca9b-4857-970e-47cfcf602017 fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:57:12.000000+00:00 app[api]: Build started by user \n2020-08-21T06:58:51.667324+00:00 app[api]: Deploy 1f77e9e8 by user \n2020-08-21T06:58:51.667324+00:00 app[api]: Release v12 created by user \n2020-08-21T06:58:51.832220+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:59:07.062252+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index:app`\n2020-08-21T06:59:10.383357+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:59:10.384213+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:54641 (4)\n2020-08-21T06:59:10.384357+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:59:10.388913+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:59:10.392276+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:59:10.392426+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:59:10.403239+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [11] [INFO] Booting worker with pid: 11\n2020-08-21T06:59:10.407880+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:59:10.408006+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:59:10.525402+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:59:10.525558+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:59:10.607473+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:59:11.643239+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:59:54.000000+00:00 app[api]: Build succeeded\n2020-08-21T07:08:53.300472+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T07:09:16.319403+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index:app`\n2020-08-21T07:09:19.182910+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T07:09:19.228761+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T07:09:19.057971+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T07:09:19.058760+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:25408 (4)\n2020-08-21T07:09:19.058888+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Using worker: sync\n2020-08-21T07:09:19.063236+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T07:09:19.066629+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T07:09:19.066758+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T07:09:19.102247+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T07:09:19.102349+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Reason: App failed to load.\n<\/code><\/pre>\n<p>Apologies, I am new to this so I am not sure where to even start with the debugging as well. To summarise: I think my gunicorn is not firing as my line may be wrong; and I am not sure what is causing my app to not launch. How do I solve this issue?<\/p>",
        "Challenge_closed_time":1597994468467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597994063423,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63518174",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":180.73,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":143,
        "Challenge_solved_time":0.1125122222,
        "Challenge_title":"using gunicorn for nested folders",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":586,
        "Challenge_word_count":1101,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597993100672,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>The logic of gunicorn is the following:\n<code>.<\/code> (dot) for directories, <code>:<\/code> (column) for objects defined inside a file.<\/p>\n<p>Assuming the given structure, you should have something like this:<\/p>\n<pre><code>$ cat Procfile\nweb: gunicorn src.package1.package2.index:app\n<\/code><\/pre>\n<p>[EDIT] If you get an error, you should consider using <code>server<\/code> instead of <code>app<\/code>. As an example, these files are from <a href=\"https:\/\/gitlab.com\/qmeeus\/datathon\" rel=\"nofollow noreferrer\">one of my old projects<\/a> (also a Dash app):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># app.py\n\nimport flask\nfrom src import dashboard\n\nserver = flask.Flask(__name__)\nserver.secret_key = os.environ.get('secret_key', str(randint(0, 1000000)))\napp = dashboard.main(server)\n\nif __name__ == '__main__':\n    app.server.run(debug=True, threaded=True)\n<\/code><\/pre>\n<pre class=\"lang-sh prettyprint-override\"><code># Procfile\nweb: gunicorn app:server --timeout 300\n<\/code><\/pre>\n<pre class=\"lang-sh prettyprint-override\"><code>$ ls *\nProcfile app.py\n\nsrc:\nconfig.py  dashboard.py ...\n \n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1597995299663,
        "Solution_link_count":1.0,
        "Solution_readability":12.0,
        "Solution_reading_time":14.56,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":113.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1351154914716,
        "Answerer_location":null,
        "Answerer_reputation_count":2564.0,
        "Answerer_view_count":451.0,
        "Challenge_adjusted_solved_time":25.1791094445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>i am trying to deploy a model in Vertex AI pipeline component using <code>DeployModelRequest<\/code>. I try to get the model using <code>GetModelRequest<\/code><\/p>\n<pre><code>    model_name = f'projects\/{project}\/locations\/{location}\/models\/{model_id}'\n    model_request = aiplatform_v1.types.GetModelRequest(name=model_name)\n    model_info = client_model.get_model(request=model_request)       \n    \n    deploy_request = aiplatform_v1.types.DeployModelRequest(endpoint=end_point, \n                                                                deployed_model=model_info)\n    client.deploy_model(request=deploy_request)\n<\/code><\/pre>\n<p>but this gives:<\/p>\n<pre><code>TypeError: Parameter to MergeFrom() must be instance of same class: expected \ngoogle.cloud.aiplatform.v1.DeployedModel got Model\n<\/code><\/pre>\n<p>I have also tried <code>deployed_model=model_info.deployed_models[0]<\/code> but this gave:<\/p>\n<pre><code>TypeError: Parameter to MergeFrom() must be instance of same class: expected\n google.cloud.aiplatform.v1.DeployedModel got DeployedModelRef.\n<\/code><\/pre>\n<p>So what do I use for <code>deployed_model<\/code>?<\/p>",
        "Challenge_closed_time":1663344904267,
        "Challenge_comment_count":6,
        "Challenge_created_time":1663254259473,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73733464",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":20.8,
        "Challenge_reading_time":15.04,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":25.1791094445,
        "Challenge_title":"What should be used for deployed_model in DeployModelRequest in Vertex AI pipeline?",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":39,
        "Challenge_word_count":93,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>As simple as:<\/p>\n<pre><code>machine_spec = MachineSpec(machine_type=&quot;n1-standard-2&quot;)\ndedicated_resources = DedicatedResources(machine_spec=machine_spec, \n                                         min_replica_count=1, \n                                         max_replica_count=1)\ndepmodel= DeployedModel(model=model_name, dedicated_resources=dedicated_resources) \n\ndeploy_request = aiplatform_v1.types.DeployModelRequest(\n                   endpoint=end_point, deployed_model=depmodel, \n                   traffic_split={'0':100})\nclient.deploy_model(request=deploy_request)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":49.5,
        "Solution_reading_time":6.68,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":3.8905663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying work on bring your own model. I have R code. when i try to run the job its failing.<\/p>\n<p><strong>Training Image:<\/strong><\/p>\n<pre><code>FROM r-base:3.6.3\n\nMAINTAINER Amazon SageMaker Examples &lt;amazon-sagemaker-examples@amazon.com&gt;\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n    wget \\\n    r-base \\\n    r-base-dev \\\n    apt-transport-https \\\n    ca-certificates \\\n    python3 python3-dev pip\n\nENV AWS_DEFAULT_REGION=&quot;us-east-2&quot;\nRUN R -e &quot;install.packages('reticulate', dependencies = TRUE, warning = function(w) stop(w))&quot;\nRUN R -e &quot;install.packages('readr', dependencies = TRUE, warning = function(w) stop(w))&quot;\nRUN R -e &quot;install.packages('dplyr', dependencies = TRUE, warning = function(w) stop(w))&quot;\n\nRUN pip install --quiet --no-cache-dir \\\n    'boto3&gt;1.0&lt;2.0' \\\n    'sagemaker&gt;2.0&lt;3.0'    \n\nENTRYPOINT [&quot;\/usr\/bin\/Rscript&quot;]\n<\/code><\/pre>\n<p><strong>Source code:<\/strong><\/p>\n<pre><code>rcode\n    \u2514\u2500\u2500 train.R\n    \u2514\u2500\u2500 train.tar.gz\n<\/code><\/pre>\n<p>Build<\/p>\n<pre><code>- aws s3 cp $CODEBUILD_SRC_DIR\/rcode\/ s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/code\/training --recursive\n<\/code><\/pre>\n<p><strong>Serverless.com yaml<\/strong><\/p>\n<pre><code>           SagemakerRCodeTrainingStep:\n            Type: Task\n            Resource: ${self:custom.sageMakerTrainingJob}\n            Parameters:\n              TrainingJobName.$: &quot;$.sageMakerTrainingJobName&quot;\n              DebugHookConfig:\n                S3OutputPath: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/models\/rmodel&quot;\n              AlgorithmSpecification:\n                TrainingImage: ${self:custom.sagemakerRExecutionContainerURI}\n                TrainingInputMode: &quot;File&quot;\n              OutputDataConfig:\n                S3OutputPath: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/models\/rmodel&quot;\n              StoppingCondition:\n                MaxRuntimeInSeconds: ${self:custom.maxRuntime}\n              ResourceConfig:\n                InstanceCount: 1\n                InstanceType: &quot;ml.m5.xlarge&quot;\n                VolumeSizeInGB: 30\n              RoleArn: ${self:custom.stateMachineRoleARN}\n              InputDataConfig:\n                - DataSource:\n                    S3DataSource:\n                      S3DataType: &quot;S3Prefix&quot;\n                      S3Uri: &quot;s3:\/\/${self:custom.datasetsFilePath}\/data\/processed\/train&quot;\n                      S3DataDistributionType: &quot;FullyReplicated&quot;\n                  ChannelName: &quot;train&quot;\n              HyperParameters:\n                sagemaker_submit_directory: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/code\/training\/train.tar.gz&quot;\n                sagemaker_program: &quot;train.R&quot;\n                sagemaker_enable_cloudwatch_metrics: &quot;false&quot;\n                sagemaker_container_log_level: &quot;20&quot;\n                sagemaker_job_name: &quot;sagemaker-r-learn-2022-02-28-09-56-33-234&quot;\n                sagemaker_region: ${self:provider.region}\n<\/code><\/pre>",
        "Challenge_closed_time":1646690790176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646676784137,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1646720798767,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71385524",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":22.5,
        "Challenge_reading_time":36.25,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":3.8905663889,
        "Challenge_title":"Sagemaker training job Fatal error: cannot open file 'train': No such file or directory",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":369,
        "Challenge_word_count":174,
        "Platform":"Stack Overflow",
        "Poster_created_time":1285763771347,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":6958.0,
        "Poster_view_count":787.0,
        "Solution_body":"<p>I am not sure which <code>TrainingImage<\/code> you are using and all the files in your container.\nThat being said, I suspect you are using a custom container.<\/p>\n<p>SageMaker Training Jobs look for a <code>train<\/code> file and run your container as <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">follows<\/a>:<\/p>\n<pre><code>docker run image train\n<\/code><\/pre>\n<p>You can change this behavior by setting the <code>ENTRYPOINT<\/code> in your Dockerfile. Please see this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/r_examples\/r_byo_r_algo_hpo\/Dockerfile#L47\" rel=\"nofollow noreferrer\">Dockerfile<\/a> from the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/r_examples\/r_byo_r_algo_hpo\" rel=\"nofollow noreferrer\">r_byo_r_algo_hpo<\/a> example.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":18.2,
        "Solution_reading_time":11.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":76.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":147.1668483334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is AWS Sage Maker Auto Pilot suitable for NLP?<\/p>\n\n<p>We currently have a tensorflow model that does classification on input of a sequence of URLS (\nWe transform the URLs to Word vec and Char vec to feed it to the model).<\/p>\n\n<p>Looking at Sage Maker Auto Pilot documentation it says that it works on input in tabular form.\nI was wondering if we could use it to for our use case.<\/p>",
        "Challenge_closed_time":1578756147747,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578226347093,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59599721",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":5.2,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":147.1668483334,
        "Challenge_title":"Is AWS Sage Maker Auto Pilot suitable for NLP?",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":261,
        "Challenge_word_count":79,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>No. SageMaker AutoPilot doesn't support deep learning at the moment, only classification and regression problems on tabular data. Technically, I guess you could pass embeddings in CSV format, and pray that XGBoost figures them out, but I seriously doubt that this would deliver meaningful results :)<\/p>\n\n<p>Amazon Comprehend does support fully managed custom classification models <a href=\"https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html<\/a>. It may be worth taking a look at it.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.4,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":68.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1448457543732,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":211.1152302778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there possibility to get stream from Spark Streaming or Apache Storm into Azure Machine Learning? In <strong><em>reader<\/em><\/strong> option there is an input to read data from Hive database\n<a href=\"https:\/\/i.stack.imgur.com\/8Em26.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8Em26.png\" alt=\"hive\"><\/a><\/p>\n\n<p>but how to achive real time stream of data from Spark or Storm, for example <strong><em>Real-time fraud detection<\/em><\/strong><\/p>",
        "Challenge_closed_time":1448457555592,
        "Challenge_comment_count":0,
        "Challenge_created_time":1447697540763,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1456849966663,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/33741912",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":7.04,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":211.1152302778,
        "Challenge_title":"How connect Azure Machine Learning and Spark Streaming or Apache Storm",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":549,
        "Challenge_word_count":63,
        "Platform":"Stack Overflow",
        "Poster_created_time":1327481639092,
        "Poster_location":"Poznan, Poland",
        "Poster_reputation_count":2923.0,
        "Poster_view_count":838.0,
        "Solution_body":"<p>To do real time Fraud detection typically you will create a Model on Azure ML, then publish that model to oWeb service, then on you Spark or Storm system you will call that Web service, in  sequence ( like payment happened on commercial sites for example), then you will get an immediate answer about the actual parameters you had sent in you web service call.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":27.1,
        "Solution_reading_time":4.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1579718832727,
        "Answerer_location":null,
        "Answerer_reputation_count":149.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":11055.1413697222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to install mlflow in R and im getting this error message saying <\/p>\n\n<blockquote>\n  <p>mlflow::install_mlflow()\n  Error in mlflow_conda_bin() :\n    Unable to find conda binary. Is Anaconda installed?\n    If you are not using conda, you can set the environment variable MLFLOW_PYTHON_BIN to the path of yourpython executable.<\/p>\n<\/blockquote>\n\n<p>I have tried the following<\/p>\n\n<pre><code>export MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\" \nsource ~\/.bashrc\necho $MLFLOW_PYTHON_BIN  -&gt; this prints the \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>or in R,<\/p>\n\n<pre><code>sys.setenv(MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\")\nsys.getenv() -&gt; prints MLFLOW_PYTHON_BIN is set to \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>however, it still does not work<\/p>\n\n<p>I do not want to use conda environment.<\/p>\n\n<p>how to I get past this error?<\/p>",
        "Challenge_closed_time":1584554585176,
        "Challenge_comment_count":1,
        "Challenge_created_time":1583947052940,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1584403666972,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60641337",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.3,
        "Challenge_reading_time":10.97,
        "Challenge_score_count":5,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":168.7589544444,
        "Challenge_title":"mlflow R installation MLFLOW_PYTHON_BIN",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1141,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1539211301843,
        "Poster_location":null,
        "Poster_reputation_count":117.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>The install_mlflow command only works with conda right now, sorry about the confusing message. You can either:<\/p>\n<ul>\n<li>install conda - this is the recommended way of installing and using mlflow<\/li>\n<\/ul>\n<p>or<\/p>\n<ul>\n<li>install mlflow python package yourself via pip<\/li>\n<\/ul>\n<p>To install mlflow yourself, pip install correct (matching the the R package) python version of mlflow and set the MLFLOW_PYTHON_BIN environment variable as well as MLFLOW_BIN evn variable: e.g.<\/p>\n<pre><code>library(mlflow)\nsystem(paste(&quot;pip install -U mlflow==&quot;, mlflow:::mlflow_version(), sep=&quot;&quot;))\nSys.setenv(MLFLOW_BIN=system(&quot;which mlflow&quot;))\nSys.setenv(MLFLOW_PYTHON_BIN=system(&quot;which python&quot;))\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1624202175903,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":9.75,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":82.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1326780552283,
        "Answerer_location":null,
        "Answerer_reputation_count":71.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":17.3923927778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a prediction web service to Azure using ML Workbench process using cluster mode in this tutorial (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/tutorial-classifying-iris-part-3#prepare-to-operationalize-locally\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/tutorial-classifying-iris-part-3#prepare-to-operationalize-locally<\/a>)<\/p>\n\n<p>The model gets sent to the manifest, the scoring script and schema <\/p>\n\n<blockquote>\n  <p>Creating\n  service..........................................................Error\n  occurred: {'Error': {'Code': 'KubernetesDeploymentFailed', 'Details':\n  [{'Message': 'Back-off 40s restarting failed container=...pod=...',\n  'Code': 'CrashLoopBackOff'}], 'StatusCode': 400, 'Message':\n  'Kubernetes Deployment failed'}, 'OperationType': 'Service',\n  'State':'Failed', 'Id': '...', 'ResourceLocation':\n  '\/api\/subscriptions\/...', 'CreatedTime':\n  '2017-10-26T20:30:49.77362Z','EndTime': '2017-10-26T20:36:40.186369Z'}<\/p>\n<\/blockquote>\n\n<p>Here is the result of checking the ml service realtime logs <\/p>\n\n<pre><code>C:\\Users\\userguy\\Documents\\azure_ml_workbench\\projecto&gt;az ml service logs realtime -i projecto\n2017-10-26 20:47:16,118 CRIT Supervisor running as root (no user in config file)\n2017-10-26 20:47:16,120 INFO supervisord started with pid 1\n2017-10-26 20:47:17,123 INFO spawned: 'rsyslog' with pid 9\n2017-10-26 20:47:17,124 INFO spawned: 'program_exit' with pid 10\n2017-10-26 20:47:17,124 INFO spawned: 'nginx' with pid 11\n2017-10-26 20:47:17,125 INFO spawned: 'gunicorn' with pid 12\n2017-10-26 20:47:18,160 INFO success: rsyslog entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2017-10-26 20:47:18,160 INFO success: program_exit entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2017-10-26 20:47:22,164 INFO success: nginx entered RUNNING state, process has stayed up for &gt; than 5 seconds (startsecs)\n2017-10-26T20:47:22.519159Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting gunicorn 19.6.0\n2017-10-26T20:47:22.520097Z, INFO, 00000000-0000-0000-0000-000000000000, , Listening at: http:\/\/127.0.0.1:9090 (12)\n2017-10-26T20:47:22.520375Z, INFO, 00000000-0000-0000-0000-000000000000, , Using worker: sync\n2017-10-26T20:47:22.521757Z, INFO, 00000000-0000-0000-0000-000000000000, , worker timeout is set to 300\n2017-10-26T20:47:22.522646Z, INFO, 00000000-0000-0000-0000-000000000000, , Booting worker with pid: 22\n2017-10-26 20:47:27,669 WARN received SIGTERM indicating exit request\n2017-10-26 20:47:27,669 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\n2017-10-26T20:47:27.669556Z, INFO, 00000000-0000-0000-0000-000000000000, , Handling signal: term\n2017-10-26 20:47:30,673 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\n2017-10-26 20:47:33,675 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\nInitializing logger\n2017-10-26T20:47:36.564469Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting up app insights client\n2017-10-26T20:47:36.564991Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting up request id generator\n2017-10-26T20:47:36.565316Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting up app insight hooks\n2017-10-26T20:47:36.565642Z, INFO, 00000000-0000-0000-0000-000000000000, , Invoking user's init function\n2017-10-26 20:47:36.715933: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instruc\ntions, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36,716 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\n2017-10-26 20:47:36.716376: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instruc\ntions, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36.716542: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructio\nns, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36.716703: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructi\nons, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36.716860: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructio\nns, but these are available on your machine and could speed up CPU computations.\nthis is the init\n2017-10-26T20:47:37.551940Z, INFO, 00000000-0000-0000-0000-000000000000, , Users's init has completed successfully\nUsing TensorFlow backend.\n2017-10-26T20:47:37.553751Z, INFO, 00000000-0000-0000-0000-000000000000, , Worker exiting (pid: 22)\n2017-10-26T20:47:37.885303Z, INFO, 00000000-0000-0000-0000-000000000000, , Shutting down: Master\n2017-10-26 20:47:37,885 WARN killing 'gunicorn' (12) with SIGKILL\n2017-10-26 20:47:37,886 INFO stopped: gunicorn (terminated by SIGKILL)\n2017-10-26 20:47:37,889 INFO stopped: nginx (exit status 0)\n2017-10-26 20:47:37,890 INFO stopped: program_exit (terminated by SIGTERM)\n2017-10-26 20:47:37,891 INFO stopped: rsyslog (exit status 0)\n\nReceived 41 lines of log\n<\/code><\/pre>\n\n<p>My best guess is theres something silent happening to cause \"WARN received SIGTERM indicating exit request\". The rest of the scoring.py script seems to kick off - see tensorflow get initiated and the \"this is the init\" print statement.<\/p>\n\n<p><a href=\"http:\/\/127.0.0.1:63437\" rel=\"nofollow noreferrer\">http:\/\/127.0.0.1:63437<\/a> is accessible from my local machine, but the ui endpoint is blank.<\/p>\n\n<p>Any ideas on how to get this up and running in an Azure cluster? I'm not very familiar with how Kubernetes works, so any basic debugging guidance would be appreciated.<\/p>",
        "Challenge_closed_time":1509114740967,
        "Challenge_comment_count":2,
        "Challenge_created_time":1509052128353,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1511310430992,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46963846",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":10.2,
        "Challenge_reading_time":77.84,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":42,
        "Challenge_solved_time":17.3923927778,
        "Challenge_title":"Azure ML Workbench Kubernetes Deployment Failed",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":447,
        "Challenge_word_count":620,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421081882987,
        "Poster_location":null,
        "Poster_reputation_count":588.0,
        "Poster_view_count":64.0,
        "Solution_body":"<p>We discovered a bug in our system that could have caused this. The fix was deployed last night. Can you please try again and let us know if you still encounter this issue?<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":2.14,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1415906440767,
        "Answerer_location":"Kyiv",
        "Answerer_reputation_count":12948.0,
        "Answerer_view_count":363.0,
        "Challenge_adjusted_solved_time":720.5025569444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I use the following script to automate upgrading of my libraries.<\/p>\n\n<p><strong>My script (Start Notebook):<\/strong><\/p>\n\n<pre><code>#!\/bin\/bash\n\nset -e\n\necho 'Before:'\necho $PATH\n\nexport PATH=\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin:\/home\/ec2-user\/anaconda3\/bin\/:\/usr\/libexec\/gcc\/x86_64-amazon-linux\/4.8.5:\/usr\/local\/cuda\/bin:\/usr\/local\/bin:\/opt\/aws\/bin:\/usr\/local\/mpi\/bin:\/usr\/local\/bin:\/bin:\/usr\/bin:\/usr\/local\/sbin:\/usr\/sbin:\/sbin:\/opt\/aws\/bin:$PATH\n\necho 'After:'\necho $PATH\n\necho `pwd`\n\nsource \/home\/ec2-user\/anaconda3\/bin\/activate tensorflow_p36\n\npip install pandas --upgrade\n\npip install tensorflow-gpu --upgrade\n<\/code><\/pre>\n\n<p><strong>Error:<\/strong><\/p>\n\n<p>I get the following error, how can I point to the correct location(\/home\/ec2-user) of keras instead of <strong><em>\/root<\/em><\/strong><\/p>\n\n<pre><code>cp: cannot stat \u2018\/root\/.keras\/keras_tensorflow.json\u2019: No such file or directory \n<\/code><\/pre>\n\n<p><strong>Full Logs:<\/strong><\/p>\n\n<pre><code>Before:\n\/sbin:\/bin:\/usr\/sbin:\/usr\/bin\nAfter:\n\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin:\/home\/ec2-user\/anaconda3\/bin\/:\/usr\/libexec\/gcc\/x86_64-amazon-linux\/4.8.5:\/usr\/local\/cuda\/bin:\/usr\/local\/bin:\/opt\/aws\/bin:\/usr\/local\/mpi\/bin:\/usr\/local\/bin:\/bin:\/usr\/bin:\/usr\/local\/sbin:\/usr\/sbin:\/sbin:\/opt\/aws\/bin:\/sbin:\/bin:\/usr\/sbin:\/usr\/bin\n\/home\/ec2-user\ncp: cannot stat \u2018\/root\/.keras\/keras_tensorflow.json\u2019: No such file or directory\n<\/code><\/pre>\n\n<p><strong>Without lifecycle configuration:<\/strong><\/p>\n\n<p>All the commands in the above script works.<\/p>\n\n<p>Actual keras.json file is exising, under \/home\/ec2-user when I remove the lifecycle configuration with the following value.<\/p>\n\n<pre><code>sh-4.2$ cat .keras\/keras.json\n{\n    \"backend\": \"tensorflow\"\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1542128119243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1542016525347,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53259647",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":22.1,
        "Challenge_reading_time":24.83,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":30.9983044445,
        "Challenge_title":"Issues in upgrading ML libraries using AWS Sagemaker Notebook's Lifecycle configurations",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":856,
        "Challenge_word_count":132,
        "Platform":"Stack Overflow",
        "Poster_created_time":1270801176840,
        "Poster_location":"Finland",
        "Poster_reputation_count":2406.0,
        "Poster_view_count":117.0,
        "Solution_body":"<p>Looks like <code>pip install<\/code> in your case executed \"outside\" of virtualenv  <\/p>\n\n<p>try to change from:  <\/p>\n\n<p><code>source \/home\/ec2-user\/anaconda3\/bin\/activate tensorflow_p36<\/code>  <\/p>\n\n<p>to: <\/p>\n\n<p><code>source \/home\/ec2-user\/anaconda3\/bin\/activate tensorflow_p36 &amp;&amp; pip install pandas tensorflow-gpu --upgrade<\/code>  <\/p>\n\n<p>and delete redundant lines<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1544610334552,
        "Solution_link_count":0.0,
        "Solution_readability":23.6,
        "Solution_reading_time":5.05,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":37.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1621658973823,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":3213.0,
        "Answerer_view_count":1896.0,
        "Challenge_adjusted_solved_time":5.4727108333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been following the learning path for <a href=\"https:\/\/docs.microsoft.com\/en-us\/learn\/certifications\/exams\/ai-900\" rel=\"nofollow noreferrer\">Microsoft Azure AI 900<\/a>. In the second module, I have deployed my model as an endpoint. It says Container instances for compute type. How much will this cost me. Azure doesn't seem to show any pricing for this. Is this endpoint always active? If yes how much does it cost?<\/p>",
        "Challenge_closed_time":1635505501916,
        "Challenge_comment_count":1,
        "Challenge_created_time":1635485800157,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69764100",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":6.1,
        "Challenge_reading_time":5.95,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":5.4727108333,
        "Challenge_title":"Endpoints cost on Azure Machine Learning",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":633,
        "Challenge_word_count":66,
        "Platform":"Stack Overflow",
        "Poster_created_time":1566078293736,
        "Poster_location":"Mumbai, Maharashtra, India",
        "Poster_reputation_count":449.0,
        "Poster_view_count":77.0,
        "Solution_body":"<p>The price depends on the number of <strong>vCPU<\/strong> and <strong>GBs<\/strong> of memory requested for the container group. You are charged based on the <strong>vCPU request<\/strong> for your container group rounded up to the nearest whole number for the duration (measured in seconds) <strong>your instance is running<\/strong>. You are also charged for the <strong>GB request<\/strong> for your container group rounded up to the nearest tenths place for the duration (measured in seconds) your <strong>container group is running<\/strong>. There is an additional charge of $0.000012 per vCPU second for Windows software duration on Windows container groups. Check here <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/container-instances\/\" rel=\"nofollow noreferrer\">Pricing - Container Instances | Microsoft Azure<\/a> for details<\/p>\n<ul>\n<li>After Deployed the Azure Machine Learning managed online endpoint (preview).<\/li>\n<li>Have at least <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/role-based-access-control\/role-assignments-portal.md\" rel=\"nofollow noreferrer\">Billing Reader<\/a> access on the subscription where the endpoint is deployed<\/li>\n<\/ul>\n<p>To know the costs estimation<\/p>\n<ol>\n<li><p>In the <a href=\"https:\/\/portal.azure.com\/\" rel=\"nofollow noreferrer\">Azure portal<\/a>, Go to your subscription<\/p>\n<\/li>\n<li><p>Select <strong>Cost Analysis<\/strong> for your subscription.<\/p>\n<\/li>\n<\/ol>\n<p><img src=\"https:\/\/i.imgur.com\/W2eaRIO.png\" alt=\"enter image description here\" \/><\/p>\n<p>Create a filter to scope data to your Azure Machine learning workspace resource:<\/p>\n<ol>\n<li><p>At the top navigation bar, select <strong>Add filter<\/strong>.<\/p>\n<\/li>\n<li><p>In the first filter dropdown, select <strong>Resource<\/strong> for the filter type.<\/p>\n<\/li>\n<li><p>In the second filter dropdown, select your Azure Machine Learning workspace.<\/p>\n<\/li>\n<\/ol>\n<p><img src=\"https:\/\/i.imgur.com\/HEvprph.png\" alt=\"enter image description here\" \/><\/p>\n<p>Create a tag filter to show your managed online endpoint and\/or managed online deployment:<\/p>\n<ol>\n<li><p>Select <strong>Add filter<\/strong> &gt; <strong>Tag<\/strong> &gt; <strong>azuremlendpoint<\/strong>: &quot;&lt; your endpoint name&gt;&quot;<\/p>\n<\/li>\n<li><p>Select <strong>Add filter<\/strong> &gt; <strong>Tag<\/strong> &gt; <strong>azuremldeployment<\/strong>: &quot;&lt; your deployment name&gt;&quot;.<\/p>\n<\/li>\n<\/ol>\n<p><img src=\"https:\/\/i.imgur.com\/1aapYGB.png\" alt=\"enter image description here\" \/><\/p>\n<p>Refer  <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/how-to-view-online-endpoints-costs.md\" rel=\"nofollow noreferrer\">here <\/a> for more detailed steps<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":12.6,
        "Solution_reading_time":35.84,
        "Solution_score_count":2.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":280.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1466783310736,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":2701.0,
        "Answerer_view_count":260.0,
        "Challenge_adjusted_solved_time":27.5513030556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to train YOLOv5 on aws sagemaker also deploy the model on sagemaker itself,need to know about entrypoint python script as well. how can I build a pipeline for this?<\/p>",
        "Challenge_closed_time":1632853607047,
        "Challenge_comment_count":1,
        "Challenge_created_time":1632686467053,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1632754422356,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69338516",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.0,
        "Challenge_reading_time":2.76,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":46.4277761111,
        "Challenge_title":"how to train and deploy YOLOv5 on aws sagemaker",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":2510,
        "Challenge_word_count":39,
        "Platform":"Stack Overflow",
        "Poster_created_time":1556022524712,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>This official AWS <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/speed-up-yolov4-inference-to-twice-as-fast-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">blog post<\/a> has information on how to deploy YOLOv4. I wonder if you can use it as a guide and change the model to v5.<\/p>\n<p>If not, there is a 3rd party implementation of YOLOv5 <a href=\"https:\/\/github.com\/HKT-SSA\/yolov5-on-sagemaker\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.7,
        "Solution_reading_time":5.89,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1549666221947,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":2121.3562466667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created 2 models which are not too complex and renamed them and placed them into a same location in S3 bucket.<\/p>\n\n<p>I need to create a multi model endpoint such that the 2 models have a same end point. \nThe model i am using is AWS in built Linear-learner model type regressor. <\/p>\n\n<p>I am stuck as to how they should be deployed. <\/p>",
        "Challenge_closed_time":1582590253856,
        "Challenge_comment_count":1,
        "Challenge_created_time":1574954456480,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59091944",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.7,
        "Challenge_reading_time":4.82,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2121.0548266667,
        "Challenge_title":"Create a Multi Model Endpoint using AWS Sagemaker Boto",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":364,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Poster_created_time":1574954057270,
        "Poster_location":"Carlow, Ireland",
        "Poster_reputation_count":13.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>SageMaker's Linear Learner algorithm container does not currently implement the requirements for <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">multi-model endpoints<\/a>. You could request support in the <a href=\"https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285&amp;start=0\" rel=\"nofollow noreferrer\">AWS Forums<\/a>.<\/p>\n\n<p>You could also build your own version of the Linear Learner algorithm. To deploy the models to a multi-model endpoint you would need to build your own container that meets the requirements for multi-model endpoints and implement your own version of the Linear Learner algorithm. This sample notebook gives an example of how you would create your multi-model compatible container that serves MxNet models, but you could adapt it to implement a Linear Learner algorithm:<\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1582591338968,
        "Solution_link_count":4.0,
        "Solution_readability":21.5,
        "Solution_reading_time":16.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":107.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1429811498812,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation_count":1910.0,
        "Answerer_view_count":316.0,
        "Challenge_adjusted_solved_time":17.1826736111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a pre-trained ML model (saved as .h5 file) to Azure ML. I have created an AKS cluster and trying to deploy the model as shown below:<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.model import Model\n\nfrom azureml.core.environment import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.model import InferenceConfig\n\nfrom azureml.core.webservice import AksWebservice, LocalWebservice\nfrom azureml.core.compute import ComputeTarget\n\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\nenv = Environment.get(workspace, name='AzureML-TensorFlow-1.13-GPU')\n\n# Installing packages present in my requirements file\nwith open('requirements.txt') as f:\n    dependencies = f.readlines()\ndependencies = [x.strip() for x in dependencies if '# ' not in x]\ndependencies.append(&quot;azureml-defaults&gt;=1.0.45&quot;)\n\nenv.python.conda_dependencies = CondaDependencies.create(conda_packages=dependencies)\n\n# Including the source folder so that all helper scripts are included in my deployment\ninference_config = InferenceConfig(entry_script='app.py', environment=env, source_directory='.\/ProcessImage')\n\naks_target = ComputeTarget(workspace=workspace, name='sketch-ppt-vm')\n\n# Deployment with suitable config\ndeployment_config = AksWebservice.deploy_configuration(cpu_cores=4, memory_gb=32)\nmodel = Model(workspace, 'sketch-inference')\nservice = Model.deploy(workspace, &quot;process-sketch-dev&quot;, [model], inference_config, deployment_config, deployment_target=aks_target, overwrite=True)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n<\/code><\/pre>\n<p>My main entry script requires some additional helper scripts, which I include by mentioning the source folder in my inference config.<\/p>\n<p>I was expecting that the helper scripts I add should be able to access the packages installed while setting up the environment during deployment, but I get ModuleNotFoundError.<\/p>\n<p>Here is the error output, along with the a couple of environment variables I printed while executing entry script:<\/p>\n<pre><code>    AZUREML_MODEL_DIR ----  azureml-models\/sketch-inference\/1\n    PYTHONPATH ----  \/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages:\/var\/azureml-server:\n    PATH ----  \/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/bin:\/opt\/miniconda\/bin:\/usr\/local\/nvidia\/bin:\/usr\/local\/cuda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin:\/opt\/intel\/compilers_and_libraries\/linux\/mpi\/bin64\n    Exception in worker process\n    Traceback (most recent call last):\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n        worker.init_process()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 129, in init_process\n        self.load_wsgi()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 138, in load_wsgi\n        self.wsgi = self.app.wsgi()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n        self.callable = self.load()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 52, in load\n        return self.load_wsgiapp()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 41, in load_wsgiapp\n        return util.import_app(self.app_uri)\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/util.py&quot;, line 350, in import_app\n        __import__(module)\n    File &quot;\/var\/azureml-server\/wsgi.py&quot;, line 1, in &lt;module&gt;\n        import create_app\n    File &quot;\/var\/azureml-server\/create_app.py&quot;, line 3, in &lt;module&gt;\n        from app import main\n    File &quot;\/var\/azureml-server\/app.py&quot;, line 32, in &lt;module&gt;\n        from aml_blueprint import AMLBlueprint\n    File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 25, in &lt;module&gt;\n        import main\n    File &quot;\/var\/azureml-app\/main.py&quot;, line 12, in &lt;module&gt;\n        driver_module_spec.loader.exec_module(driver_module)\n    File &quot;\/structure\/azureml-app\/ProcessImage\/app.py&quot;, line 16, in &lt;module&gt;\n        from ProcessImage.samples.coco.inference import run as infer\n    File &quot;\/var\/azureml-app\/ProcessImage\/samples\/coco\/inference.py&quot;, line 1, in &lt;module&gt;\n        import skimage.io\n    ModuleNotFoundError: No module named 'skimage'\n<\/code><\/pre>\n<p>The existing answers related to this aren't of much help. I believe there must be a simpler way to fix this, since AzureML specifically provides the feature to setup environment with pip\/conda packages installed either by supplying requirements.txt file or individually.<\/p>\n<p>What am I missing here? Kindly help.<\/p>",
        "Challenge_closed_time":1606249620652,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606187763027,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64979752",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":67.52,
        "Challenge_score_count":3,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":52,
        "Challenge_solved_time":17.1826736111,
        "Challenge_title":"Unable to access python packages installed in Azure ML",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1381,
        "Challenge_word_count":396,
        "Platform":"Stack Overflow",
        "Poster_created_time":1429811498812,
        "Poster_location":"Boston, MA, USA",
        "Poster_reputation_count":1910.0,
        "Poster_view_count":316.0,
        "Solution_body":"<p>So, after some trial and error, creating a fresh environment and then adding the packages solved the problem for me. I am still not clear on why this didn't work when I tried to use <a href=\"http:\/\/from%20azureml.core%20import%20Workspace%20from%20azureml.core.model%20import%20Model%20from%20azureml.core.environment%20import%20Environment,%20DEFAULT_GPU_IMAGE%20from%20azureml.core.conda_dependencies%20import%20CondaDependencies%20from%20azureml.core.model%20import%20InferenceConfig%20from%20azureml.core.webservice%20import%20AksWebservice,%20LocalWebservice%20from%20azureml.core.compute%20import%20ComputeTarget%20%20%20#%201.%20Instantiate%20the%20workspace%20workspace%20=%20Workspace.from_config(path=%22config.json%22)%20%20#%202.%20Setup%20the%20environment%20env%20=%20Environment(%27sketchenv%27)%20with%20open(%27requirements.txt%27)%20as%20f:%20#%20Fetch%20all%20dependencies%20as%20a%20list%20%20%20%20%20dependencies%20=%20f.readlines()%20dependencies%20=%20%5Bx.strip()%20for%20x%20in%20dependencies%20if%20%27#%20%27%20not%20in%20x%5D%20env.docker.base_image%20=%20DEFAULT_GPU_IMAGE%20env.python.conda_dependencies%20=%20CondaDependencies.create(conda_packages=%5B%27numpy==1.17.4%27,%20%27Cython%27%5D,%20pip_packages=dependencies)%20%20#%203.%20Inference%20Config%20inference_config%20=%20InferenceConfig(entry_script=%27app.py%27,%20environment=env,%20source_directory=%27.\/ProcessImage%27)%20%20#%204.%20Compute%20target%20(using%20existing%20cluster%20from%20the%20workspacke)%20aks_target%20=%20ComputeTarget(workspace=workspace,%20name=%27sketch-ppt-vm%27)%20%20#%205.%20Deployment%20config%20deployment_config%20=%20AksWebservice.deploy_configuration(cpu_cores=6,%20memory_gb=100)%20%20#%206.%20Model%20deployment%20model%20=%20Model(workspace,%20%27sketch-inference%27)%20#%20Registered%20model%20(which%20contains%20model%20files\/folders)%20service%20=%20Model.deploy(workspace,%20%22process-sketch-dev%22,%20%5Bmodel%5D,%20inference_config,%20deployment_config,%20deployment_target=aks_target,%20overwrite=True)%20service.wait_for_deployment(show_output%20=%20True)%20print(service.state)\" rel=\"nofollow noreferrer\">Environment.from_pip_requirements()<\/a>. A detailed answer in this regard would be interesting to read.<\/p>\n<p>My primary task was inference - object detection given an image, and we have our own model developed by our team. There are two types of imports I wanted to have:<\/p>\n<p><strong>1. Standard python packages (installed through pip)<\/strong><br \/>\nThis was solved by creating conda dependencies and add it to env object (Step 2)<\/p>\n<p><strong>2. Methods\/vars from helper scripts<\/strong> (if you have pre\/post processing to be done during model inference):<br \/>\nThis was done by mentioning <code>source_directory<\/code> in InferenceConfig (step 3)<\/p>\n<p>Here is my updated script which combines Environment creation, Inference and Deployment configs and using existing compute in the workspace (created through portal).<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.environment import Environment, DEFAULT_GPU_IMAGE\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.webservice import AksWebservice, LocalWebservice\nfrom azureml.core.compute import ComputeTarget\n\n\n# 1. Instantiate the workspace\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\n# 2. Setup the environment\nenv = Environment('sketchenv')\nwith open('requirements.txt') as f: # Fetch all dependencies as a list\n    dependencies = f.readlines()\ndependencies = [x.strip() for x in dependencies if '# ' not in x]\nenv.docker.base_image = DEFAULT_GPU_IMAGE\nenv.python.conda_dependencies = CondaDependencies.create(conda_packages=['numpy==1.17.4', 'Cython'], pip_packages=dependencies)\n\n# 3. Inference Config\ninference_config = InferenceConfig(entry_script='app.py', environment=env, source_directory='.\/ProcessImage')\n\n# 4. Compute target (using existing cluster from the workspacke)\naks_target = ComputeTarget(workspace=workspace, name='sketch-ppt-vm')\n\n# 5. Deployment config\ndeployment_config = AksWebservice.deploy_configuration(cpu_cores=6, memory_gb=100)\n\n# 6. Model deployment\nmodel = Model(workspace, 'sketch-inference') # Registered model (which contains model files\/folders)\nservice = Model.deploy(workspace, &quot;process-sketch-dev&quot;, [model], inference_config, deployment_config, deployment_target=aks_target, overwrite=True)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n<\/code><\/pre>\n<hr \/>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":25.5,
        "Solution_reading_time":63.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":32.0,
        "Solution_word_count":271.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645519217332,
        "Answerer_location":null,
        "Answerer_reputation_count":336.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":166.7237425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How can I store additional information in an <code>optuna trial<\/code> when using it via the Hydra sweep plugin?<\/p>\n<p>My use case is as follows:\nI want to optimize a bunch of hyperparameters. I am storing all reproducibility information of all experiments (i.e., trials) in a separate database.\nI know I can get the best values via <code>optuna.load_study().best_params<\/code> or even <code>best_trial<\/code>. However, that only allows me to replicate the experiment - potentially this takes quite some time. To overcome this issue, I need to somehow link it to my own database. I would like to store the ID of my own database somewhere in the <code>trial<\/code> object.<\/p>\n<p>Without using Hydra, I suppose I'd set <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/003_attributes.html#sphx-glr-tutorial-20-recipes-003-attributes-py\" rel=\"nofollow noreferrer\">User Attributes<\/a>. However, with Hydra <a href=\"https:\/\/github.com\/facebookresearch\/hydra\/blob\/535dc7aacfe607e25848b2c4b8068317095a730b\/plugins\/hydra_optuna_sweeper\/hydra_plugins\/hydra_optuna_sweeper\/_impl.py#L183\" rel=\"nofollow noreferrer\">abstracting all that away<\/a>, there seems no option to do so.<\/p>\n<p>I know that I can just query my own database for the exact combination of best params that optuna found, but that just seems like a difficult solution to a simple problem.<\/p>\n<p>Some minimal code:<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>from dataclasses import dataclass\n\nimport hydra\nfrom hydra.core.config_store import ConfigStore\nfrom omegaconf import MISSING\n\n\n@dataclass\nclass TrainConfig:\n    x: float | int = MISSING\n    y: int = MISSING\n    z: int | None = None\n\n\nConfigStore.instance().store(name=&quot;config&quot;, node=TrainConfig)\n\n\n@hydra.main(version_base=None, config_path=&quot;conf&quot;, config_name=&quot;sweep&quot;)\ndef sphere(cfg: TrainConfig) -&gt; float:\n    x: float = cfg.x\n    y: float = cfg.y\n    return x**2 + y**2\n\n\nif __name__ == &quot;__main__&quot;:\n    sphere()\n<\/code><\/pre>\n<pre class=\"lang-yaml prettyprint-override\"><code>defaults:\n  - override hydra\/sweeper: optuna\n  - override hydra\/sweeper\/sampler: tpe\n\nhydra:\n  sweeper:\n    sampler:\n      seed: 123\n    direction: minimize\n    study_name: sphere\n    storage: sqlite:\/\/\/trials.db\n    n_trials: 20\n    n_jobs: 1\n    params:\n      x: range(-5.5, 5.5, step=0.5)\n      y: choice(-5 ,0 ,5)\n      z: choice(0, 3, 5)\n\nx: 1\ny: 1\nz: 1\n<\/code><\/pre>",
        "Challenge_closed_time":1657793209687,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657194655610,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72897321",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":31.17,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":166.2650213889,
        "Challenge_title":"Store user attributes in Optuna Sweeper plugin for Hydra",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":83,
        "Challenge_word_count":275,
        "Platform":"Stack Overflow",
        "Poster_created_time":1645519217332,
        "Poster_location":null,
        "Poster_reputation_count":336.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>A hacky solution via the <a href=\"https:\/\/hydra.cc\/docs\/plugins\/optuna_sweeper\/#experimental--custom-search-space-optimization\" rel=\"nofollow noreferrer\"><code>custom_search_space<\/code><\/a>.<\/p>\n<pre><code>hydra:\n  sweeper:\n    sampler:\n      seed: 123\n    direction: minimize\n    study_name: sphere\n    storage: sqlite:\/\/\/trials.db\n    n_trials: 20\n    n_jobs: 1\n    params:\n      x: range(-5.5, 5.5, step=0.5)\n      y: choice(-5 ,0 ,5)\n      z: choice([0, 1], [2, 3], [2, 5])\n    custom_search_space: package.run.configure\n<\/code><\/pre>\n<pre><code>def configure(_, trial: Trial) -&gt; None:\n    trial.set_user_attr(&quot;experiment_db_id&quot;, 123456)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1657794861083,
        "Solution_link_count":1.0,
        "Solution_readability":15.6,
        "Solution_reading_time":8.21,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":52.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1657058369727,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":101.3011433333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If we use multiple instances for training will the built-in algorithm automatically exploit it? For example, what if we used 2 instances for training using built-in XGBoost container and we used the same customer churn example? Will one instance be ignored?<\/p>",
        "Challenge_closed_time":1663199408803,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662834395330,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1662834724687,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73674278",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":4.27,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":101.3926313889,
        "Challenge_title":"How to use multiple instances with the SageMaker XGBoost built-in algorithm?",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":19,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1507661294190,
        "Poster_location":null,
        "Poster_reputation_count":98.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Yes SageMaker XGBoost supports distributed training. If you set instance count &gt; 1, SageMaker XGBoost will distribute the files from S3 to individual instances and perform distributed training. This, however, requires number of files on S3 &gt;= number of instances. Otherwise, you will be charged for using two training instances without the benefit of using distributed training.<\/p>\n<p>You can find an example here<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.6,
        "Solution_reading_time":10.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1502815666600,
        "Answerer_location":"Memphis, TN, USA",
        "Answerer_reputation_count":5028.0,
        "Answerer_view_count":957.0,
        "Challenge_adjusted_solved_time":0.08215,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a Window 10 machine and trying to pip install mlflow but I'm getting the following error message.<\/p>\n\n<pre><code>THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\nmlflow from https:\/\/files.pythonhosted.org\/packages\/01\/ec\/8c9448968d4662e8354b9c3a62e635f8929ed507a45af3d9fdb84be51270\/mlflow-1.0.0-py3-none-any.whl#sha256=0f2f116a377b9da538642eaf688caa0a7166ee1ede30c8734830eb9e789574b4:\n    Expected sha256 0f2f116a377b9da538642eaf688caa0a7166ee1ede30c8734830eb9e789574b4\n         Got        eb34ea16ecfe02d474ce50fd1f88aba82d56dcce9e8fdd30193ab39edf32ac9e\n<\/code><\/pre>",
        "Challenge_closed_time":1560545869140,
        "Challenge_comment_count":1,
        "Challenge_created_time":1560545573400,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1560799785056,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56604989",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":10.44,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.08215,
        "Challenge_title":"How to install mlflow using pip install",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":365,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1355343131932,
        "Poster_location":null,
        "Poster_reputation_count":5655.0,
        "Poster_view_count":629.0,
        "Solution_body":"<p>It is trying to check cache for packages. They were likely compiled in linux or some other OS and you are trying to install them in Windows.<\/p>\n\n<p>This should fix your issue:<\/p>\n\n<pre><code>pip install --no-cache-dir mlflow\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":3.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":34.3028483334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The problem is my local internet connection is unstable, I could run the code through Jupyter to the interactive google cloud platform vertexAI, while it seems that there're always outputs returns back to the Jupyter interface. So when my local internet connection is interrupted, the code running is also interrupted.<\/p>\n<p>Is there any methods that I could let the codes just run on the vertexAI backends? Then outputs could be written in the log file at last.<\/p>\n<p>This could be a very basic question. Thanks.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1643782095707,
        "Challenge_comment_count":9,
        "Challenge_created_time":1643658605453,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1645284643350,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70932012",
        "Challenge_link_count":2,
        "Challenge_participation_count":10,
        "Challenge_readability":7.9,
        "Challenge_reading_time":9.75,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":34.3028483334,
        "Challenge_title":"Could google cloud platform vertextAI running code on backend without output?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":514,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1622195346030,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>To be able to run your notebook on the background, I did the following steps:<\/p>\n<ol>\n<li>Open Jupyter notebook in GCP &gt; Vertex AI &gt; Workbench &gt; Open Jupyterlab<\/li>\n<li>Open a terminal<\/li>\n<li>Use the command below.\n<pre><code>nohup jupyter nbconvert --to notebook --execute test.ipynb &amp;\n<\/code><\/pre>\n<ul>\n<li><code>nohup<\/code> and <code>&amp;<\/code> is added so that the command will run on the background<\/li>\n<li>Output logs for the actual command will be appened to file <strong>nohup.out<\/strong><\/li>\n<li>Use <code>jupyter nbconvert --to notebook --execute test.ipynb<\/code> to execute the notebook specified after <code>--execute<\/code>. <code>--to notebook<\/code> will create a new notebook that contains the executed notebook with its logs.<\/li>\n<li>There other formats other than notebook to convert it. You can read thru more in <a href=\"https:\/\/nbconvert.readthedocs.io\/en\/latest\/usage.html\" rel=\"nofollow noreferrer\">nbconvert documentation<\/a>.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<p>For testing I made notebook <strong>(test.ipynb)<\/strong> that has a loop that runs for 1.5 hours, that should emulate a long process.<\/p>\n<pre><code>import time\n\nfor x in range(1,1080):\n    print(x)\n    time.sleep(5)\n<\/code><\/pre>\n<p>I ran the command provided above and closed my notebook and anything related to GCP. After 1.5 hours I opened the notebook and terminal says its done.<\/p>\n<p><strong>Terminal upon checking back after 1.5 hours:<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Content of <strong>nohup.out<\/strong>:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It created a new notebook named <strong>&quot;test.nbconvert.ipynb&quot;<\/strong> that contains the code from test.ipynb and its output.<\/p>\n<p>Snippet of test.nbconvert.ipynb as seen below. It completed the loop up to 1080 iterations that took 1.5 hours:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":9.0,
        "Solution_reading_time":29.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":255.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":19.9010738889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I ran into this issue yesterday, while trying to use the same sqlite script I used in \"Apply SQL Transformation\" module in Azure ML, in Sqlite over Python module in Azure ML:<\/p>\n\n<pre><code>with tbl as (select * from t1)\nselect * from tbl\n<\/code><\/pre>\n\n<p>Here is the error I got:<\/p>\n\n<pre><code>[Critical]     Error: Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\n  File \"C:\\server\\invokepy.py\", line 169, in batch\ndata:text\/plain,Caught exception while executing function: Traceback (most recent call last):\n    odfs = mod.azureml_main(*idfs)\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\io\\sql.py\", line 388, in read_sql\n  File \"C:\\temp\\azuremod.py\", line 193, in azureml_main\n    results = pd.read_sql(query,con)\n    coerce_float=coerce_float, parse_dates=parse_dates)\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\io\\sql.py\", line 1017, in execute\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\io\\sql.py\", line 1022, in read_sql\n    cursor = self.execute(*args)\n    raise_with_traceback(ex)\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\io\\sql.py\", line 1006, in execute\n---------- End of error message from Python  interpreter  ----------\n    cur.execute(*args)\nDatabaseError: Execution failed on sql:  with tbl as (select * from t1)\n                    select * from tbl\n<\/code><\/pre>\n\n<p>and the Python code:<\/p>\n\n<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):\n    import pandas as pd\n    import sqlite3 as lite\n    import sys\n    con = lite.connect('data1.db')\n    con.text_factory = str\n    with con:\n        cur = con.cursor()\n\n        if (dataframe1 is not None):\n            cur.execute(\"DROP TABLE IF EXISTS t1\")\n            dataframe1.to_sql('t1',con)\n        query = '''with tbl as (select * from t1)\n                    select * from tbl'''                      \n        results = pd.read_sql(query,con)    \n\n    return results,\n<\/code><\/pre>\n\n<p>when replacing the query with:<\/p>\n\n<pre><code>select * from t1\n<\/code><\/pre>\n\n<p>It worked as expected.\nAs you probably know, Common table expressions is a key feature in Sqlite, the ability to run recursive code is a \"must have\" in any functional language such as Sqlite.<\/p>\n\n<p>I also tried to run my Python script in Jupyter Notebook in Azure, that also worked as expected.<\/p>\n\n<p>Is it possible we have a different configuration for Sqlite in the Python module than in Jupyter Notebook and in \"Apply SQL Transformation\" module?<\/p>",
        "Challenge_closed_time":1456485065183,
        "Challenge_comment_count":1,
        "Challenge_created_time":1456413421317,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1456812883803,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35631267",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":31.19,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":19.9010738889,
        "Challenge_title":"Azure ML in \"Execute Python Script\" module :Common table expressions is not supported in sqlite3",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":280,
        "Challenge_word_count":304,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320061998252,
        "Poster_location":null,
        "Poster_reputation_count":778.0,
        "Poster_view_count":89.0,
        "Solution_body":"<p>I reproduced your issue and reviewed the <code>SQL Queries<\/code> doc of <code>pandas.io.sql<\/code> at <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/stable\/io.html#sql-queries\" rel=\"nofollow\">http:\/\/pandas.pydata.org\/pandas-docs\/stable\/io.html#sql-queries<\/a>. I tried to use <code>read_sql_query<\/code> to solve it, but failed.<\/p>\n\n<p>According to the <code>pandas<\/code> doc, tt seems that <code>Pandas<\/code> not support the usage for this SQL syntax.<\/p>\n\n<p>Base on my experience and according to your SQL, I tried to do the SQL <code>select * from (select * from t1) as tbl<\/code> instead of your SQL that work for <code>Pandas<\/code>.<\/p>\n\n<p>Hope it helps. Best Regards. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.6,
        "Solution_reading_time":8.87,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":79.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":45.0161775,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>How do I replace the values in a specific column with a particular value based on a condition in Azure ML Studio. I can do this using pandas in python as foolows:<\/p>\n\n<pre><code>df.loc[df['col_name'] &gt; 1990, 'col_name'] = 1\n<\/code><\/pre>\n\n<p>I'm trying to find a Module in Azure Machine Learning Studio that does the equivalent of this. <\/p>\n\n<p>I understand there is a replace option under the ConverToDataset module and a Replace Discrete Values module. But neither of these seems to do what I want. Is there an option to replace the values in just one column to a specific value based on a condition?<\/p>",
        "Challenge_closed_time":1557391929596,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557229871357,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56021977",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":8.27,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":45.0161775,
        "Challenge_title":"Replace values in a column based on a condition in Azure ML Studio",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":564,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1450260166772,
        "Poster_location":null,
        "Poster_reputation_count":1587.0,
        "Poster_view_count":540.0,
        "Solution_body":"<p>You can use either the more general <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/apply-sql-transformation\" rel=\"nofollow noreferrer\">Apply SQL Transformation<\/a>, or the dedicated <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/clip-values\" rel=\"nofollow noreferrer\">Clip Values<\/a> module. If all else fails, there's also <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/execute-python-script\" rel=\"nofollow noreferrer\">Execute Python Script<\/a>.<\/p>\n\n<p>Personally, for your example I'd use <code>Clip Values<\/code> with <code>Clip Peaks<\/code> and <code>Upper Threshold<\/code> set. For more complex rules I'd use either <code>Apply SQL Transformation<\/code> or <code>Execute Python Script<\/code>, depending on the rules but favouring SQL :).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":17.8,
        "Solution_reading_time":11.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":71.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":1.6198194445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a Tensorflow model which is working perfectly fine on my laptop (Tf 1.8 on OS HighSierra). However, I wanted to scale my operations up and use Amazon's Virtual Machine to run predictions faster. What is the best way to use my saved model and classify images in jpeg format which are stored locally? Thank you! <\/p>",
        "Challenge_closed_time":1532521671907,
        "Challenge_comment_count":0,
        "Challenge_created_time":1532515840557,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51517103",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":4.67,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.6198194445,
        "Challenge_title":"Deploy my own tensorflow model on a virtual machine with AWS",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":196,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530739110150,
        "Poster_location":null,
        "Poster_reputation_count":391.0,
        "Poster_view_count":57.0,
        "Solution_body":"<p>you have two options:<\/p>\n\n<p>1) Start a virtual machine on AWS (known as an Amazon EC2 instance). You can pick from many different instance types, including GPU instances. You'll have full administrative access on this machine, meaning that you can copy you TF model to it and predict just like you would on your own machine. <\/p>\n\n<p>More details on getting started with EC2 here: <a href=\"https:\/\/aws.amazon.com\/ec2\/getting-started\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/ec2\/getting-started\/<\/a> <\/p>\n\n<p>I would also recommend using the Deep Learning Amazon Machine Image, which bundles all the popular ML\/DL tools as well as the NVIDIA environment for GPU training\/prediction : <a href=\"https:\/\/aws.amazon.com\/machine-learning\/amis\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/machine-learning\/amis\/<\/a><\/p>\n\n<p>2) If you don't want to manage virtual machines, I'd recommend looking at Amazon SageMaker. You'll be able to import your TF model and to deploy it on fully-managed infrastructure for prediction. <\/p>\n\n<p>Here's a sample notebook showing you how to bring your own TF model to SageMaker: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/tensorflow_iris_byom\/tensorflow_BYOM_iris.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/tensorflow_iris_byom\/tensorflow_BYOM_iris.ipynb<\/a><\/p>\n\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":14.3,
        "Solution_reading_time":19.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":156.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":8.6530769444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm training a large-ish model, trying to use for the purpose <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#create-an-estimator\" rel=\"nofollow noreferrer\">Azure Machine Learning service<\/a> in Azure notebooks.<\/p>\n\n<p>I thus create an <code>Estimator<\/code> to train locally:<\/p>\n\n<pre><code>from azureml.train.estimator import Estimator\n\nestimator = Estimator(source_directory='.\/source_dir',\n                      compute_target='local',\n                      entry_script='train.py')\n<\/code><\/pre>\n\n<p>(my <code>train.py<\/code> should load and train starting from a large word vector file).<\/p>\n\n<p>When running with <\/p>\n\n<pre><code>run = experiment.submit(config=estimator)\n<\/code><\/pre>\n\n<p>I get <\/p>\n\n<blockquote>\n  <p>TrainingException: <\/p>\n  \n  <p>====================================================================<\/p>\n  \n  <p>While attempting to take snapshot of\n  \/data\/home\/username\/notebooks\/source_dir Your total\n  snapshot size exceeds the limit of 300.0 MB. Please see\n  <a href=\"http:\/\/aka.ms\/aml-largefiles\" rel=\"nofollow noreferrer\">http:\/\/aka.ms\/aml-largefiles<\/a> on how to work with large files.<\/p>\n  \n  <p>====================================================================<\/p>\n<\/blockquote>\n\n<p>The link provided in the error is likely <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/26076\" rel=\"nofollow noreferrer\">broken<\/a>. \nContents in my <code>.\/source_dir<\/code> indeed exceed 300 MB.<br>\nHow can I solve this?<\/p>",
        "Challenge_closed_time":1554445793380,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554414642303,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1554642637940,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55525445",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":13.3,
        "Challenge_reading_time":20.73,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":8.6530769444,
        "Challenge_title":"How to overcome TrainingException when training a large model with Azure Machine Learning service?",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1127,
        "Challenge_word_count":135,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>You can place the training files outside <code>source_dir<\/code> so that they don't get uploaded as part of submitting the experiment, and then upload them separately to the data store (which is basically using the Azure storage associated with your workspace). All you need to do then is reference the training files from <code>train.py<\/code>. <\/p>\n\n<p>See the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml\" rel=\"nofollow noreferrer\">Train model tutorial<\/a> for an example of how to upload data to the data store and then access it from the training file.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1554449103928,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":7.95,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":45.3303,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I encountered and error when I deploy my training job in my notebook instance.\nThis what it says:\n<code>&quot;UnexpectedStatusException: Error for Training job tensorflow-training-2021-01-26-09-55-05-768: Failed. Reason: ClientError: Data download failed:Could not download s3:\/\/forex-model-data\/data\/train2001_2020.npz: insufficient disk space&quot;<\/code><\/p>\n<p>I deploy training job to try running it to different instances in 3 epoch. I use ml.c5.4xlarge, ml.c5.18xlarge, ml.m5.24xlarge, also I have two sets of training data, train2001_2020.npz and train2016_2020.npz.<\/p>\n<p>First, I run train2001_2020 to ml.c5.18xlarge and ml.c5.18xlarge and the training job completed, then I switch to train2016_2020 and run it to ml.c5.4xlarge and ml.c5.18xlarge and it goes well. Then when I tried to run it using ml.m5.24xlarge I got an error (quoted above), but my dataset is train2016_2020 not train2001_2020 then when I rerun it again with all other instances it has the same error. What happen?<\/p>\n<p>I stopped the instances and refresh everything, but I encountered same issue.<\/p>",
        "Challenge_closed_time":1611831268823,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611668079743,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65902366",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":14.73,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":45.3303,
        "Challenge_title":"AWS Sagemaker - ClientError: Data download failed:Could not download",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":774,
        "Challenge_word_count":153,
        "Platform":"Stack Overflow",
        "Poster_created_time":1570029753880,
        "Poster_location":"Philippines",
        "Poster_reputation_count":98.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>It's not really clear to all the test are you doing, but that error usually means that there is not enough disk space on the instance you are using for the training job. You can try to increase the additional storage for the instance (you can do in the estimator parameters if you are using the sagemaker SDK in a notebook).<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.8,
        "Solution_reading_time":3.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1353403873547,
        "Answerer_location":null,
        "Answerer_reputation_count":4909.0,
        "Answerer_view_count":583.0,
        "Challenge_adjusted_solved_time":370.54581,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Optuna's FAQ has a <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#id10\" rel=\"nofollow noreferrer\">clear answer<\/a> when it comes to dynamically adjusting the range of parameter during a study: it poses no problem since each sampler is defined individually.<\/p>\n<p>But what about adding and\/or removing parameters? Is Optuna able to handle such adjustments?<\/p>\n<p>One thing I noticed when doing this is that in the results dataframe these parameters get <code>nan<\/code> entries for other trials. Would there be any benefit to being able to set these <code>nan<\/code>s to their (default) value that they had when not being sampled? Is the study still sound with all these unknown values?<\/p>",
        "Challenge_closed_time":1609649865303,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608315900387,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65362133",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":9.9,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":370.54581,
        "Challenge_title":"What happens when I add\/remove parameters dynamically during an Optuna study?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":227,
        "Challenge_word_count":110,
        "Platform":"Stack Overflow",
        "Poster_created_time":1353403873547,
        "Poster_location":null,
        "Poster_reputation_count":4909.0,
        "Poster_view_count":583.0,
        "Solution_body":"<p>Question was answered <a href=\"https:\/\/github.com\/optuna\/optuna\/issues\/2141\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<blockquote>\n<p>Thanks for the question. Optuna internally supports two types of sampling: <code>optuna.samplers.BaseSampler.sample_independent<\/code> and <code>optuna.samplers.BaseSampler.sample_relative<\/code>.<\/p>\n<p>The former <code>optuna.samplers.BaseSampler.sample_independent<\/code> is a method that samples independently on each parameter, and is not affected by the addition or removal of parameters. The added parameters are taken into account from the timing when they are added.<\/p>\n<p>The latter <code>optuna.samplers.BaseSampler.sample_relative<\/code> is a method that samples by considering the correlation of parameters and is affected by the addition or removal of parameters. Optuna's default search space for correlation is the product set of the domains of the parameters that exist from the beginning of the hyperparameter tuning to the present. Developers who implement samplers can implement their own search space calculation method <code>optuna.samplers.BaseSampler.infer_relative_search_space<\/code>. This may allow correlations to be considered for hyperparameters that have been added or removed, but this depends on the sampling algorithm, so there is no API for normal users to modify.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.3,
        "Solution_reading_time":17.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":157.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1384343462316,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":478.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":48.2131977778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I tried to create MLproject with zero parameters as:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    parameters:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>when I get an error:<\/p>\n\n<pre><code>  Traceback (most recent call last):\n File \"\/home\/ubuntu\/.local\/bin\/mlflow\", line 11, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/cli.py\", line 137, in run\n    run_id=run_id,\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 230, in run\n    use_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 85, in _run\n    project = _project_spec.load_project(work_dir)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 40, in load_project\n    entry_points[name] = EntryPoint(name, parameters, command)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 87, in __init__\n    self.parameters = {k: Parameter(k, v) for (k, v) in parameters.items()}\nAttributeError: 'NoneType' object has no attribute 'items'\n<\/code><\/pre>\n\n<p>Am I missing something or mlflow does not allow project with  zero parameters?<\/p>\n\n<p>I have also posted this at my public repo of: <a href=\"https:\/\/github.com\/sameermahajan\/mlflow-try\" rel=\"nofollow noreferrer\">https:\/\/github.com\/sameermahajan\/mlflow-try<\/a> if someone would like to try out:<\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/sameermahajan\/mlflow-try.git\n<\/code><\/pre>",
        "Challenge_closed_time":1562240543612,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562066976100,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56851463",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":30.73,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":48.2131977778,
        "Challenge_title":"How do I specify mlflow MLproject with zero parameters?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":353,
        "Challenge_word_count":185,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384343462316,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":478.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>For this, you completely drop the 'parameters' section as below:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>(I thought I had tried it earlier but I was trying too many different ways to may be miss out on this one)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":3.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1369156710532,
        "Answerer_location":"Japan",
        "Answerer_reputation_count":189.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":0.2048830556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I execute code without parallel computation, <code>n_trials<\/code> in the <code>optimize<\/code> function means how many trials the program runs. When executed via parallel computation (following the tutorial <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/10_key_features\/004_distributed.html\" rel=\"nofollow noreferrer\">here<\/a> via launching it again in another console), it does <code>n_trials<\/code> for each process, not for all the sum of processes like I would like.<\/p>\n<p>Is there a way to make sure that the sum of all parallel processes' trials are equal to a fixed number, regardless of how many process I launch?<\/p>",
        "Challenge_closed_time":1629887227052,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629886489473,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68920952",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":9.2,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2048830556,
        "Challenge_title":"How to set n_trials for multiple processes when using parallelization?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":330,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529092998780,
        "Poster_location":null,
        "Poster_reputation_count":788.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>Yes, <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.study.MaxTrialsCallback.html#optuna.study.MaxTrialsCallback\" rel=\"nofollow noreferrer\"><code>MaxTrialsCallback<\/code><\/a> is the exact feature for such a situation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":38.9,
        "Solution_reading_time":3.53,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1585824581960,
        "Answerer_location":"Berlin",
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":0.6095908333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a <em>Training Job<\/em> using the Sagemaker API. The code for configuring the estimator looks as follows (I shrinked the full path names a bit):<\/p>\n<pre><code>s3_input = &quot;s3:\/\/sagemaker-studio-****\/training-inputs&quot;.format(bucket)\ns3_images = &quot;s3:\/\/sagemaker-studio-****\/dataset&quot;\ns3_labels = &quot;s3:\/\/sagemaker-studio-****\/labels&quot;\ns3_output = 's3:\/\/sagemaker-studio-****\/output'.format(bucket)\n\ncfg='{}\/input\/models\/'.format(s3_input)\nweights='{}\/input\/data\/weights\/'.format(s3_input)\noutpath='{}\/'.format(s3_output)\nimages='{}\/'.format(s3_images)\nlabels='{}\/'.format(s3_labels)\n\nhyperparameters = {\n    &quot;epochs&quot;: 1,\n    &quot;batch-size&quot;: 2\n}\n\ninputs = {\n    &quot;cfg&quot;: TrainingInput(cfg),\n    &quot;images&quot;: TrainingInput(images),\n    &quot;weights&quot;: TrainingInput(weights),\n    &quot;labels&quot;: TrainingInput(labels)\n}\n\nestimator = PyTorch(\n    entry_point='train.py',\n    source_dir='s3:\/\/sagemaker-studio-****\/input\/input.tar.gz',\n    image_uri=container,\n    role=get_execution_role(),\n    instance_count=1,\n    instance_type='ml.g4dn.xlarge',\n    input_mode='File',\n    output_path=outpath,\n    train_output=outpath,\n    base_job_name='visualsearch',\n    hyperparameters=hyperparameters,\n    framework_version='1.9',\n    py_version='py38'\n)\n\nestimator.fit(inputs)\n<\/code><\/pre>\n<p>Everything runs fine and I get the success message:<\/p>\n<pre><code>Results saved to #033[1mruns\/train\/exp#033[0m\n2022-07-08 08:38:35,766 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n2022-07-08 08:38:35,766 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n2022-07-08 08:38:35,767 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n\n2022-07-08 08:39:08 Uploading - Uploading generated training model\n2022-07-08 08:39:08 Completed - Training job completed\nProfilerReport-1657268881: IssuesFound\nTraining seconds: 558\nBillable seconds: 558\nCPU times: user 1.34 s, sys: 146 ms, total: 1.48 s\nWall time: 11min 20s\n<\/code><\/pre>\n<p>When I call <code>estimator.model_data<\/code> I get a path poiting to a model.tar.gz file <code>s3:\/\/sagemaker-studio-****\/output\/...\/model.tar.gz<\/code><\/p>\n<p>Sagemaker generated subfoldes into the output folder (which in turn contain a lot of json files and other artifacts):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/WymlH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WymlH.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But the file <code>model.tar.gz<\/code> is missing. This file is nowhere to be found. Is there anything I need to change or to add, in order to obtain my model?<\/p>\n<p>Any help is much appreciated.<\/p>",
        "Challenge_closed_time":1657273188600,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657270994073,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72909085",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.6,
        "Challenge_reading_time":36.77,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":0.6095908333,
        "Challenge_title":"Sagemaker creates output folders but no model.tar.gz after successful completion of the Training Job",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":94,
        "Challenge_word_count":248,
        "Platform":"Stack Overflow",
        "Poster_created_time":1640612109920,
        "Poster_location":"Zedtwitz, Germany",
        "Poster_reputation_count":215.0,
        "Poster_view_count":46.0,
        "Solution_body":"<p>you need to make sure to store your model output to the right location inside the training container. Sagemaker will upload everything that is stored in the MODEL_DIR directory. You can find the location in the ENV of the training job:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_dir = os.environ.get(&quot;SM_MODEL_DIR&quot;)\n<\/code><\/pre>\n<p>Normally it is set to <code>opt\/ml\/model<\/code><\/p>\n<p>Ref:<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_model_dir\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_model_dir<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":23.3,
        "Solution_reading_time":12.49,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1604747085276,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":24.4361597222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on a CNN project and I would like to log the model.summary to neptune.ai. The intention of that is to have an idea about the model parameters while comparing different models. Any help\/tips would be much appreciated!<\/p>",
        "Challenge_closed_time":1604748950752,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604660980577,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1660057709880,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64713492",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":3.58,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":24.4361597222,
        "Challenge_title":"Is there a way to log the keras model summary to neptune?",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":285,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604146329127,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You can log <code>model.summary<\/code> (assuming it's keras), like this:<\/p>\n<pre><code>neptune.init('workspace\/project')\nneptune.create_experiment()\n\nmodel = keras.Sequential(...)\nmodel.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n<\/code><\/pre>\n<p>This will log entire summary as lines of text. You can later browse it in the <em>Logs<\/em> section of the experiment. Look for tile: &quot;model_summary&quot; in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/e\/HELLO-325\/logs\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n<p>Another option - for easier compare - is to log hyper-parameters at experiment creation, like this:<\/p>\n<pre><code># Define parameters as Python dict\nPARAMS = {'batch_size': 64,\n          'n_epochs': 100,\n          'shuffle': True,\n          'activation': 'elu'}\n\n# Pass PARAMS dict to params at experiment creation\nneptune.create_experiment(params=PARAMS)\n<\/code><\/pre>\n<p>You will have them in <em>Parameters<\/em> tab of the experiment, like in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/e\/HELLO-44\/parameters\" rel=\"nofollow noreferrer\">example<\/a>. You will be able to add each parameter as a column to the dashboard for quick compare. Look for greenish columns in this <a href=\"https:\/\/ui.neptune.ai\/o\/USERNAME\/org\/example-project\/experiments?viewId=d7f80ebe-5bfe-4d12-97c1-2b1e6184a2ed\" rel=\"nofollow noreferrer\">dashboard<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.0,
        "Solution_reading_time":18.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":132.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":1.2039644444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run below sample code in my notebook, Running on python 3.6 kernel.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-train-models-with-aml\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-train-models-with-aml<\/a>\nDownload the MNIST dataset<\/p>\n\n<p>The following code failed with the attribute error, on line of the following code from azureml.opendatasets import MNIST <\/p>\n\n<pre><code>from azureml.core import Dataset\nfrom azureml.opendatasets import MNIST\n\n<\/code><\/pre>",
        "Challenge_closed_time":1581481802847,
        "Challenge_comment_count":3,
        "Challenge_created_time":1581475311887,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1581477772576,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60180314",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":13.6,
        "Challenge_reading_time":8.11,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.8030444445,
        "Challenge_title":"Azure machine learning failing on sample for training",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":555,
        "Challenge_word_count":59,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Prerequisites:\nThe tutorial and accompanying utils.py file is also available on <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/tutorials\" rel=\"nofollow noreferrer\">GitHub<\/a> if you wish to use it on your own <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment#local\" rel=\"nofollow noreferrer\">local environment<\/a>. Run pip install azureml-sdk[notebooks] azureml-opendatasets matplotlib to install dependencies for this tutorial.<\/p>\n\n<p>If you are using older version then upgrade to the latest Azure ML SDK Version 1.0.85.<\/p>\n\n<p>!pip install --upgrade azureml-sdk<\/p>\n\n<pre><code># check core SDK version number\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)\n<\/code><\/pre>\n\n<p>Also <\/p>\n\n<p>!pip install --upgrade azureml-opendataset <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1581482106848,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":10.78,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1407761610168,
        "Answerer_location":"Geneva, Switzerland",
        "Answerer_reputation_count":118.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":7430.662325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to mlflow and I can't figure out why the <code>artifact store<\/code> can't be the same as the <code>backend store<\/code>? <\/p>\n\n<p>The only reason I can think of is to be able to query the experiments with SQL syntax... but since we can interact with the runs using <code>mlflow ui<\/code> I just don't understand why all artifacts and parameters can't go to a same location (which is what happens when using local storage).<\/p>\n\n<p>Can anyone shed some light on this?<\/p>",
        "Challenge_closed_time":1611045077983,
        "Challenge_comment_count":1,
        "Challenge_created_time":1584294693613,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60695933",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.8,
        "Challenge_reading_time":6.58,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":7430.662325,
        "Challenge_title":"MLflow: Why can't backend-store-uri be an s3 location?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":768,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_created_time":1528574640848,
        "Poster_location":null,
        "Poster_reputation_count":133.0,
        "Poster_view_count":76.0,
        "Solution_body":"<p>MLflow's Artifacts are typically ML models, i.e. relatively large binary files. On the other hand, run data are typically a couple of floats.<\/p>\n<p>In the end it is not a question of what is possible or not (many things are possible if you put enough effort into it), but rather to follow good practices:<\/p>\n<ul>\n<li>storing large binary artifacts in an SQL database is possible but is bound the degrade the performance of the database sooner or later, and this in turn will degrade your user experience.<\/li>\n<li>storing a couple of floats from a SQL database for quick retrieval for display in a front-end or via command line is a robust industry-proven classic<\/li>\n<\/ul>\n<p>It remains true that the documentation of MLflow on the architecture design rationale could be improved (as of 2020)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.5,
        "Solution_reading_time":9.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":133.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":66.9054508334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We know that Horovod is suppported. Is there an example script which uses DistributedDataParallel and Pytorch estimator?<\/p>",
        "Challenge_closed_time":1574100412503,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573859552880,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58885873",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.4,
        "Challenge_reading_time":2.53,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":66.9054508334,
        "Challenge_title":"Is it possible to use DistributedDataParallel with PyTorch Estimator",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":98,
        "Challenge_word_count":25,
        "Platform":"Stack Overflow",
        "Poster_created_time":1488866265607,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>You should be able to specify nccl or gloo as distributed data parallel backend, in addition to MPI with Horovod. See the <em>distributed_training<\/em> parameter of <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.pytorch?view=azure-ml-py\" rel=\"nofollow noreferrer\">PyTorch Estimator<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.5,
        "Solution_reading_time":4.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1365624651363,
        "Answerer_location":null,
        "Answerer_reputation_count":335.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":593.8890980556,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have created two models in azure ml studio and i want to download those models.<\/p>\n\n<p>Is it possible to download train and score models from azure ml studio?<\/p>",
        "Challenge_closed_time":1484356723536,
        "Challenge_comment_count":1,
        "Challenge_created_time":1482218722783,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41236871",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":5.9,
        "Challenge_reading_time":2.82,
        "Challenge_score_count":7,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":593.8890980556,
        "Challenge_title":"How to download the trained models from Azure machine studio?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":7873,
        "Challenge_word_count":38,
        "Platform":"Stack Overflow",
        "Poster_created_time":1482218454343,
        "Poster_location":null,
        "Poster_reputation_count":73.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Models can be trained, scored, saved, and run in AzureML studio, but can't downloaded to your local machine. There's no way to do anything with a model outside of AzureML.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":2.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1555488556820,
        "Answerer_location":"Baillargues, France",
        "Answerer_reputation_count":56447.0,
        "Answerer_view_count":9158.0,
        "Challenge_adjusted_solved_time":5.0890466667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a Vertex AI pipeline similar to <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/google_cloud_pipeline_components_automl_images.ipynb\" rel=\"nofollow noreferrer\">this.<\/a><\/p>\n<p>Now the pipeline has reference to a csv file. So if this csv file changes the pipeline needs to be recreated.<\/p>\n<p>Is there any way to pass a new csv as a parameter to the pipeline when it is re-run? That is without recreating the pipeline using the notebook?<\/p>\n<p>If not, is there a best practice way of auto updating the dataset, model and deployment?<\/p>",
        "Challenge_closed_time":1641588471208,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641570150640,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70623713",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":8.61,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":5.0890466667,
        "Challenge_title":"How can I pass parameters to a Vertex AI Platform Pipeline?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":611,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>Have a look to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/build-pipeline\" rel=\"nofollow noreferrer\">that documentation<\/a>.<\/p>\n<p>You can define your pipeline like that<\/p>\n<pre><code>...\n# Define the workflow of the pipeline.\n@kfp.dsl.pipeline(\n    name=&quot;automl-image-training-v2&quot;,\n    pipeline_root=pipeline_root_path)\ndef pipeline(project_id: str):\n...\n<\/code><\/pre>\n<p>(you have something very similar in your notebook sample)<\/p>\n<p>Then, when you invoke your pipeline, you can pass some parameter<\/p>\n<pre><code>import google.cloud.aiplatform as aip\n\njob = aip.PipelineJob(\n    display_name=&quot;automl-image-training-v2&quot;,\n    template_path=&quot;image_classif_pipeline.json&quot;,\n    pipeline_root=pipeline_root_path,\n    parameter_values={\n        'project_id': project_id\n    }\n)\n\njob.submit()\n<\/code><\/pre>\n<p>You can see the <code>project_id<\/code> a dict parameter in the parameter values, and in parameter of your pipeline function.<\/p>\n<p>Do the same for your CSV file name!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":13.16,
        "Solution_score_count":3.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":91.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1508836189288,
        "Answerer_location":"Finland",
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":5.5372291667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need to connect with Azure ML Workspace during deployment over container instance.<\/p>\n<pre><code>ws = Workspace(subscription_id=&quot;your-sub-id&quot;,\n              resource_group=&quot;your-resource-group-id&quot;,\n              workspace_name=&quot;your-workspace-name&quot;\n              )\n<\/code><\/pre>\n<p>Interactive Authentication to the ML Workspace prompts to login and then fails with below error message.<\/p>\n<pre><code>AttributeError: 'BasicTokenAuthentication' object has no attribute 'get_token'\n<\/code><\/pre>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication#interactive-authentication\" rel=\"nofollow noreferrer\">i have been following this Azure Authentication document.<\/a><\/p>\n<p>Any help is much appreciated.<\/p>",
        "Challenge_closed_time":1600775316867,
        "Challenge_comment_count":3,
        "Challenge_created_time":1600761544463,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64005433",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":20.4,
        "Challenge_reading_time":10.71,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":3.8256677778,
        "Challenge_title":"Azure ML operations : workspace authentication error",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":706,
        "Challenge_word_count":63,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530258066240,
        "Poster_location":null,
        "Poster_reputation_count":125.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>For me this was fixed by updating azureml-core from 1.13.0 to 1.14.0.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1600781478488,
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":0.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1491327759476,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":111.3944786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any way to use a tsv instead of a csv as the input into sagemaker's autopilot ?<\/p>\n\n<p>Currently I'm inputting the data as such:<\/p>\n\n<pre><code>input_data_config = [{\n      'DataSource': {\n        'S3DataSource': {\n          'S3DataType': 'S3Prefix',\n          'S3Uri': 's3:\/\/{}\/{}\/train'.format(bucket,prefix)\n        }\n      },\n      'TargetAttributeName': 'sentiment'\n    }\n  ]\n<\/code><\/pre>\n\n<p>this seems to work file for .csv files but fails for my .tsv files.<\/p>",
        "Challenge_closed_time":1580778531436,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580377511313,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59983062",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":5.67,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":111.3944786111,
        "Challenge_title":"TSV as input to sagemaker",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":204,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_created_time":1479115407580,
        "Poster_location":"Earth",
        "Poster_reputation_count":2944.0,
        "Poster_view_count":381.0,
        "Solution_body":"<p>I am a developer at AWS SageMaker. Autopilot currently only supports CSV data. While we are working on extending the support to more file formats: JSON, TSV, etc, this might be something that you can try to convert your .tsv file to .csv:<\/p>\n\n<pre><code>import csv\n\n# read tab-delimited file\nwith open('yourfile.tsv','rb') as fin:\n    cr = csv.reader(fin, delimiter='\\t')\n    filecontents = [line for line in cr]\n\n# write comma-delimited file (comma is the default delimiter)\nwith open('yourfile.csv','wb') as fou:\n    cw = csv.writer(fou, quotechar='', quoting=csv.QUOTE_NONE)\n    cw.writerows(filecontents)\n<\/code><\/pre>\n\n<p>Hope this helps.<\/p>\n\n<p>Ref: <a href=\"https:\/\/stackoverflow.com\/questions\/5590631\/how-to-convert-a-tab-separated-file-to-csv-format\">How to convert a tab separated file to CSV format?<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.7,
        "Solution_reading_time":10.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":94.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":108.6307483334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to integrate AWS sage maker and delta lake?<\/p>\n<p>thanks\nRamabadran<\/p>",
        "Challenge_closed_time":1641839983127,
        "Challenge_comment_count":1,
        "Challenge_created_time":1641448912433,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70603137",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.1,
        "Challenge_reading_time":1.85,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":108.6307483334,
        "Challenge_title":"Is it possible to integrate AWS sagemaker and delta lake",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":174,
        "Challenge_word_count":22,
        "Platform":"Stack Overflow",
        "Poster_created_time":1627004194367,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Yes, though it depends on what part of SageMaker you mean (Training, Notebook, Inference, etc).<\/p>\n<p>Last week, an integration between SageMaker and Delta Lake was documented here (custom docker in the SageMaker Processing API)<\/p>\n<p><a href=\"https:\/\/github.com\/eitansela\/sagemaker-delta-sharing-demo\/tree\/main\/delta_lake_bring_your_own_container_processing\" rel=\"nofollow noreferrer\">https:\/\/github.com\/eitansela\/sagemaker-delta-sharing-demo\/tree\/main\/delta_lake_bring_your_own_container_processing<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.9,
        "Solution_reading_time":6.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":9.3981805555,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to return output(or predictions) from a SageMaker endpoint with 'text\/csv' or 'text\/csv; charset=utf-8' as &quot;Content-Type&quot;. I have tried multiple ways, but the sagemaker always returns with 'text\/html; charset=utf-8' as the &quot;Content-Type&quot;, and I would like SageMaker to return 'text\/csv' or 'text\/csv; charset=utf-8'.<\/p>\n<p>Here's the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#process-output\" rel=\"nofollow noreferrer\"><code>output_fn<\/code><\/a> from my inference-code:<\/p>\n<pre><code>** my other code **\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return output_float\n<\/code><\/pre>\n<p>above function returns number with float data-type and I got error(in cloudwatch logs) that  this function should only be returning string, tuple, dict or Respoonse instance.<\/p>\n<p>So, here are all the different ways I have tried to have SageMaker return my number with 'text\/csv' but only receives 'text\/html; charset=utf-8'<\/p>\n<ol>\n<li><code>return json.dumps(output_float)<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float}&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float},&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float}\\n&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float},\\n&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li>by using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\" rel=\"nofollow noreferrer\"><code>sagemaker.serializers.CSVSerializer<\/code><\/a> like this:\n<pre><code>from sagemaker.serializers import CSVSerializer\ncsv_serialiser = CSVSerializer(content_type='text\/csv')\n\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return csv_serialiser.serialize(output_float)\n<\/code><\/pre>\nI got <code>'NoneType' object has no attribute 'startswith'<\/code> error with this.<\/li>\n<li>as a tuple: <code>return (output_float,)<\/code>. I haven't noted down what this did, but it sure didn't return the number with 'text\/csv' as &quot;Content-Type&quot;.<\/li>\n<li>made changes in my model object to return a float on calling <code>.predict_proba<\/code> on my model-object and deployed it from sagemaker studio without using any custom inference code and deployed this from SageMaker studio. but got this error after sending a request to the endpoint: <code>'NoneType' object has no attribute 'startswith'<\/code>, but at my side, when I pass proper inputs to the unpicked model and call .predict_proba, i get float as expected.<\/li>\n<li>by returning <a href=\"https:\/\/tedboy.github.io\/flask\/generated\/generated\/flask.Response.html#flask.Response\" rel=\"nofollow noreferrer\"><code>flask.Response<\/code><\/a> like this:\n<pre><code>from flask import Response\n\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return Response(response=output_float, status=200, headers={'Content-Type':'text\/csv; charset=utf-8'})\n<\/code><\/pre>\nbut, I got some IndexError with this(I didn't notedown the traceback.)<\/li>\n<\/ol>\n<p>Some other info:<\/p>\n<ol>\n<li>Model I am using is completely outside of SageMaker, not from a training job or anything like that.<\/li>\n<li>All of the aforementioned endpoints have been deployed entirely from aws-cli with relevant .json files, except the one in point-8 above.<\/li>\n<\/ol>\n<p>How do I return number with &quot;text\/csv&quot; as content-type from sagemaker? I need my output in &quot;text\/csv&quot; content-type specifically for Model-Quality-Monitor. How do I do this?<\/p>",
        "Challenge_closed_time":1646685903080,
        "Challenge_comment_count":2,
        "Challenge_created_time":1646135687137,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1646652069630,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71308112",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":12.5,
        "Challenge_reading_time":49.56,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":152.8377619444,
        "Challenge_title":"How to return a float with 'text\/csv' as \"Content-Type\" from SageMaker endpoint that uses custom inference code?",
        "Challenge_topic":"Lambda Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":461,
        "Challenge_word_count":398,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559910246180,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":2046.0,
        "Poster_view_count":369.0,
        "Solution_body":"<p>From the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py#L88\" rel=\"nofollow noreferrer\">scikit_bring_your_own<\/a> example, I suggest testing by setting the Response as follows:<\/p>\n<pre><code>return flask.Response(response= output_float, status=200, mimetype=&quot;text\/csv&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":27.4,
        "Solution_reading_time":5.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1546942930440,
        "Answerer_location":null,
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":18.6231941667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to follow the tutorial <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"noreferrer\">here<\/a> to implement a custom inference pipeline for feature preprocessing. It uses the python sklearn sdk to bring in custom preprocessing pipeline from a script. For example:<\/p>\n\n<pre><code>from sagemaker.sklearn.estimator import SKLearn\n\nscript_path = 'preprocessing.py'\n\nsklearn_preprocessor = SKLearn(\n    entry_point=script_path,\n    role=role,\n    train_instance_type=\"ml.c4.xlarge\",\n    sagemaker_session=sagemaker_session)\n<\/code><\/pre>\n\n<p>However I can't find a way to send multiple files. The reason I need multiple files is because I have a custom class used in the sklearn pipeline needs to be imported from a custom module. Without importing,  it raises error <code>AttributeError: module '__main__' has no attribute 'CustomClassName'<\/code> when having the custom class in the same preprocessing.py file due to the way pickle works (at least I think it's related to pickle). <\/p>\n\n<p>Anyone know if sending multiple files is even possible?<\/p>\n\n<p>Newbie to Sagemaker, thanks!!<\/p>",
        "Challenge_closed_time":1548251021872,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548183978373,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54314876",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.3,
        "Challenge_reading_time":16.83,
        "Challenge_score_count":8,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":18.6231941667,
        "Challenge_title":"AWS Sagemaker SKlearn entry point allow multiple script",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":2019,
        "Challenge_word_count":142,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455047963123,
        "Poster_location":null,
        "Poster_reputation_count":85.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>There's a source_dir parameter which will \"lift\" a directory of files to the container and put it on your import path.<\/p>\n\n<p>You're entrypoint script should be put there to and referenced from that location.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":2.67,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1466260908296,
        "Answerer_location":null,
        "Answerer_reputation_count":71.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":149.6007655556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In the Azure Recommendation API sample there is a snippet like this:<\/p>\n\n<pre><code>     if (itemSets.RecommendedItemSetInfo != null)\n        {\n            ...\n        }\n        else\n        {\n            Console.WriteLine(\"No recommendations found.\");\n        }\n<\/code><\/pre>\n\n<p>So I assume that nullable recommended set means no recommendations. But what is the case with this set being not nullable but still empty ( as I am having it running the example)?<\/p>\n\n<p>I provided my own usages and catalog files. I have not too many entries there however for i2i recommendations I have results and for u2i there is an empty set.\nAllowColdItemPlacement doesn't change a think here.<\/p>",
        "Challenge_closed_time":1478186851716,
        "Challenge_comment_count":0,
        "Challenge_created_time":1477648288960,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40302499",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":8.71,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":149.6007655556,
        "Challenge_title":"Recommendation API: what is the difference between null results and empty results",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":130,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1354118434116,
        "Poster_location":"Wroc\u0142aw, Poland",
        "Poster_reputation_count":393.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>We did not mean to convey a difference in meaning between null recommendations and empty recommendations. I will check why we may be sending two different types of results. Either way, don't treat those two cases as different cases. <\/p>\n\n<p>If you are not getting results for user-to-item recommendations, most likely there was no data for that user when the build was created or the items that the user interacted with do not have enough co-occurrences with other items in the usage.<\/p>\n\n<p>What to do when you get empty recommendations is up to you, you may decide to not show any recommendations, or back-fill with popular items you may want to promote.<\/p>\n\n<p>Thanks!<\/p>\n\n<p>Luis Cabrera\nProgram Manager - Recommendations API.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":9.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":119.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":192.6107861111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Does anyone know if any SageMaker built-in algorithm supports multiple object detection in image recognition? I am thinking something like multi-label image training and detection \/ inference. <\/p>\n\n<p>Thus, can we: <\/p>\n\n<p><strong>a) train using multi-label images<\/strong> <\/p>\n\n<p>and\/or <\/p>\n\n<p><strong>b) infer multiple objects from images (sort of like AWS Rekognition but with custom labels and training \/ transfer learning).<\/strong><\/p>\n\n<p>Also, I know that the doc for SageMaker Image Classification Algorithm says \"takes an image as input and classifies it into <strong>one<\/strong> of multiple output categories\". <\/p>\n\n<p>Any recommendations are also welcome.<\/p>",
        "Challenge_closed_time":1531456253520,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530762854690,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1535620469950,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51183169",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":9.61,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":192.6107861111,
        "Challenge_title":"AWS Sagemaker Multiple Object Detection in Image Recognition \/ Classification",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1764,
        "Challenge_word_count":98,
        "Platform":"Stack Overflow",
        "Poster_created_time":1300941578956,
        "Poster_location":"San Francisco Bay Area",
        "Poster_reputation_count":645.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p>There is a new built-in algorithm released with Amazon Sagemaker today for object detection. Based on the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection.html\" rel=\"nofollow noreferrer\">documentation<\/a>, Amazon SageMaker Object Detection uses the Single Shot multibox Detector (SSD) algorithm. The response from the inference contains an array consists of a predicted class label of the object detected, associated confidence score and bounding box co-ordinates.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.0,
        "Solution_reading_time":6.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1620132280447,
        "Answerer_location":null,
        "Answerer_reputation_count":146.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":13.6342236111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using DAGsHub storage as a remote and running into the following error message (when trying to DVC pull):<\/p>\n<blockquote>\n<p>ERROR: Lockfile 'bias_tagging_model\/dvc.lock' is corrupted.<\/p>\n<\/blockquote>\n<p>I thought I might have messed something up, but when cloning the git repo again and DVC pulling I am still running into this.\nThe data looks ok when viewed in the browser.\nIf you have any ideas, I would appreciate your help!<\/p>",
        "Challenge_closed_time":1620292401492,
        "Challenge_comment_count":1,
        "Challenge_created_time":1620243318287,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67407702",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.6,
        "Challenge_reading_time":5.76,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":13.6342236111,
        "Challenge_title":"Corrupted dvc.lock",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":362,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1620132740443,
        "Poster_location":"New York, NY, USA",
        "Poster_reputation_count":75.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Usually, the reason for this error is the DVC version.<\/p>\n<p>If the dvc.lock file has a DVC 2.* schema and you are using a lower version, it will throw this error.<\/p>\n<p>Upgrade your DVC version, and it should work.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.8,
        "Solution_reading_time":2.73,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":39.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1523264005616,
        "Answerer_location":"Yokohama, Kanagawa, Japan",
        "Answerer_reputation_count":1635.0,
        "Answerer_view_count":143.0,
        "Challenge_adjusted_solved_time":1.0014597222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm pretty new to SageMaker, so I'm sorry if I miss something obvious.<\/p>\n\n<p>I've trained a DL model which uses frames from a video to make a prediction. The current script, that runs in the SageMaker jupyter-notebook, takes a video URL as an input and uses an FFMPEG subprocess pipe to extract the frames and predict them afterwards. This works fine, but now I want to start that script from Lambda.<\/p>\n\n<p>As far as I understood, I could deploy my model with sagemaker and make predictions for every single frame from Lambda, unfortunately this is not an option, as ffprobe, ffmpeg and numpy are too large to fit into the limited lambda space.<\/p>\n\n<p>tl;dr: Is it possible to run my custom script (ffmpeg frame extraction + tensorflow model prediction) as an endpoint in SageMaker?<\/p>",
        "Challenge_closed_time":1580863000328,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580859395073,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60067075",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":10.08,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.0014597222,
        "Challenge_title":"SageMaker deploy custom script",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":386,
        "Challenge_word_count":136,
        "Platform":"Stack Overflow",
        "Poster_created_time":1460444230316,
        "Poster_location":null,
        "Poster_reputation_count":404.0,
        "Poster_view_count":127.0,
        "Solution_body":"<p>Sagemaker allows you to use a custom Docker image (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms.html\" rel=\"nofollow noreferrer\">AWS document<\/a>)<\/p>\n\n<blockquote>\n  <p>Build your own custom container image: If there is no pre-built Amazon\n  SageMaker container image that you can use or modify for an advanced\n  scenario, you can package your own script or algorithm to use with\n  Amazon SageMaker.You can use any programming language or framework to\n  develop your container<\/p>\n<\/blockquote>\n\n<ul>\n<li>Create a docker image with your code (FFmpeg, TensorFlow)<\/li>\n<li>Testing the docker container locally<\/li>\n<li>Deploying the image on Amazon ECR (Elastic Container Repository)<\/li>\n<li>Create a SageMaker model and point to the image<\/li>\n<\/ul>\n\n<p>For details, you can learn more from <a href=\"https:\/\/towardsdatascience.com\/brewing-up-custom-ml-models-on-aws-sagemaker-e09b64627722\" rel=\"nofollow noreferrer\">this tutorial<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.9,
        "Solution_reading_time":12.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":111.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":156.2246444445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In the sagemaker pipeline example shown here<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html<\/a><\/p>\n<p>I see two lines joining from AbaloneProcess to AbaloneTrain and AbaloneEval respectively. However, based on the code, I would expect it to actually be connected from AbaloneTrain only, then to AbaloneEval in a single path. Can somebody explain to me what is actually happening here because I am struggling to wrap my head around this. Much appreciated and apologies for the inconvenience in advance<\/p>",
        "Challenge_closed_time":1661363764683,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660801355963,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73397959",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":9.05,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":156.2246444445,
        "Challenge_title":"Question regarding sagemaker pipeline example on AWS?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":16,
        "Challenge_word_count":81,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644981356940,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The connections are indicated data dependencies between steps, not only the order of execution. This is what you see AbaloneProcess connected to AbaloneEval, since the output of AbaloneProcess is used in AbaloneEval.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":2.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1395737095150,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":154.6126952778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using the SageMaker TensorFlow estimator for training, and specifying an output path for my model artifacts with the <code>output_path<\/code> argument, with a value of <code>s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/<\/code>. <\/p>\n\n<p>After model training, a directory named <code>&lt;training_job_name&gt;\/output<\/code> is created in the specified <code>output_path<\/code>. <\/p>\n\n<p>The issue I'm having is, the source code that's used for training is also uploaded to S3 by default, but instead of being placed in <code>s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/&lt;training_job_name&gt;\/source<\/code>, it's placed in <code>s3:\/\/&lt;bucket&gt;\/&lt;training_job_name&gt;\/source<\/code>. <\/p>\n\n<p>So how can I specify the S3 upload path for the training job's source code in order to make it use the bucket AND prefix name of <code>output_path<\/code>?<\/p>",
        "Challenge_closed_time":1558768756470,
        "Challenge_comment_count":0,
        "Challenge_created_time":1558212150767,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56202697",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":15.2,
        "Challenge_reading_time":11.68,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":154.6126952778,
        "Challenge_title":"SageMaker TensorFlow Estimator source code S3 upload path",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":645,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>Have you tried using the \u201ccode_location\u201d argument: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a> to specify the location for the source code?<\/p>\n\n<p>Below is a snippet code example that use code_location<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ncode-path = \"s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\"\noutput-path = \"s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\"\n\nabalone_estimator = TensorFlow(entry_point='abalone.py',\n                           role=role,\n                           framework_version='1.12.0',\n                           training_steps= 100, \n                           image_name=image,\n                           evaluation_steps= 100,\n                           hyperparameters={'learning_rate': 0.001},\n                           train_instance_count=1,\n                           train_instance_type='ml.c4.xlarge',\n                           code_location= code-path,\n                           output_path = output-path,\n                           base_job_name='my-job-name'\n                           )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":22.2,
        "Solution_reading_time":11.38,
        "Solution_score_count":4.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1342713532568,
        "Answerer_location":"Seoul, South Korea",
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":4.4646297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have one Linux machine and one Windows machine for developments. For data sharing, we have set up a shared Windows directory in another Windows machine, which both my Linux and Windows can access.<\/p>\n<p>I am now using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> for version control of the shared data. To make it easy, I mount the shared Windows folder both in Windows and in Linux development machine. In Windows, it looks like<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>In Linux, it looks like:<\/p>\n<pre><code>[core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>As you can see, Windows and Linux have different mounting points. So my question is: is there a way to make that both Windows and Linux have the same <code>\u00f9rl<\/code> in the DVC configuration file?<\/p>\n<p>If this is impossible, is there another alternative solution for DVC keeps data in remote shared Windows folder? Thanks.<\/p>",
        "Challenge_closed_time":1641179335767,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641163263100,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1641199464667,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70560288",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":14.57,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":4.4646297222,
        "Challenge_title":"DVC Shared Windows Directory Setup",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":128,
        "Challenge_word_count":156,
        "Platform":"Stack Overflow",
        "Poster_created_time":1331553057367,
        "Poster_location":null,
        "Poster_reputation_count":10643.0,
        "Poster_view_count":504.0,
        "Solution_body":"<p>If you are using a local remote this way, you won't be able to have to the same <code>url<\/code> on both platforms since the mount points are different (as you already realized).<\/p>\n<p>The simplest way to configure this would be to pick one (Linux or Windows) <code>url<\/code> to use as your default case that gets git-committed into <code>.dvc\/config<\/code>. On the other platform you (or your users) can override that <code>url<\/code> in the local configuration file: <code>.dvc\/config.local<\/code>.<\/p>\n<p>(Note that <code>.dvc\/config.local<\/code> is a git-ignored file and will not be included in any commits)<\/p>\n<p>So if you wanted Windows to be the default case, in <code>.dvc\/config<\/code> you would have:<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>and on your Linux machine you would add the file <code>.dvc\/config.local<\/code> containing:<\/p>\n<pre><code>['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>See the DVC docs for <code>dvc config --local<\/code> and <code>dvc remote modify --local<\/code> for more details:<\/p>\n<ul>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/config#description<\/a><\/li>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.4,
        "Solution_reading_time":20.54,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":160.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":46.4007888889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I just started MLflow today and fail to display the log result on MLflow ui interface.\nWill appreciate a lot if someone can give me some hint..<\/p>\n<p>tried the sample code below<\/p>\n<pre><code>import os\nfrom random import random, randint\nfrom mlflow import log_metric, log_param, log_artifacts\n\nif __name__ == &quot;__main__&quot;:\n    # Log a parameter (key-value pair)\n    log_param(&quot;param1&quot;, randint(0, 100))\n\n    # Log a metric; metrics can be updated throughout the run\n    log_metric(&quot;foo&quot;, random())\n    log_metric(&quot;foo&quot;, random() + 1)\n    log_metric(&quot;foo&quot;, random() + 2)\n\n    # Log an artifact (output file)\n    if not os.path.exists(&quot;outputs&quot;):\n        os.makedirs(&quot;outputs&quot;)\n    with open(&quot;outputs\/test.txt&quot;, &quot;w&quot;) as f:\n        f.write(&quot;hello world!&quot;)\n    log_artifacts(&quot;outputs&quot;)\n<\/code><\/pre>\n<p>ran the script above for 3 times and it gave me the result in the following structure. 3 folders representing 3 runs separately:<\/p>\n<pre><code>file:\/\/\/home\/devuser\/project\/mlruns\/0\n0 - 0737fec7d4824384b6320070cd688b78\n    355d57e092a242b7aa263451d280b497 \n    ed2614ffe2fd4f2db991d5d7166635f8  \n    meta.yaml\n<\/code><\/pre>\n<p>with folders\/files <code>artifacts, meta.yaml, metrics, params, tags<\/code> in each folder separately.<\/p>\n<p>I ran <code>mlflow ui<\/code> under <code>file:\/\/\/home\/devuser\/project\/mlruns\/<\/code> but nothing was showed on the interface. tried to look this up but no one has come across this problem with this kind of simple code.<\/p>\n<p>Appreciate a lot if someone could kindly let me know how I can change my setting.. Thank you..<\/p>",
        "Challenge_closed_time":1622968783720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622801740880,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67835498",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":21.51,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":46.4007888889,
        "Challenge_title":"MLflow - How to point interface path to show the expected result",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":379,
        "Challenge_word_count":193,
        "Platform":"Stack Overflow",
        "Poster_created_time":1445157877636,
        "Poster_location":null,
        "Poster_reputation_count":267.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>You need to run <code>mlflow ui<\/code> in the project directory itself, not inside the <code>mlruns<\/code> - if you look into the <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-ui\" rel=\"nofollow noreferrer\">documentation for <code>mlflow ui<\/code> command<\/a>, it says:<\/p>\n<blockquote>\n<p><code>--default-artifact-root &lt;URI&gt;<\/code><\/p>\n<p>Path to local directory to store artifacts, for new experiments. Note that this flag does not impact already-created experiments. <strong>Default: .\/mlruns<\/strong><\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.1,
        "Solution_reading_time":7.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":18.8037608333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have built an XGBoost model using Amazon Sagemaker, but I was unable to find anything which will help me interpret the model and validate if it has learned the right dependencies.<\/p>\n\n<p>Generally, we can see Feature Importance for XGBoost by get_fscore() function in the python API (<a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html<\/a>) I see nothing of that sort in the sagemaker api(<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a>).<\/p>\n\n<p>I know I can build my own model and then deploy that using sagemaker but I am curious if anyone has faced this problem and how they overcame it.<\/p>\n\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1555001695856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554934002317,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55621967",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":14.7,
        "Challenge_reading_time":11.49,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":18.8037608333,
        "Challenge_title":"Feature Importance for XGBoost in Sagemaker",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3637,
        "Challenge_word_count":99,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419327925267,
        "Poster_location":"Mountain View, CA, USA",
        "Poster_reputation_count":123.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>SageMaker XGBoost currently does not provide interface to retrieve feature importance from the model. You can write some code to get the feature importance from the XGBoost model. You have to get the booster object artifacts from the model in S3 and then use the following snippet <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pickle as pkl\nimport xgboost\nbooster = pkl.load(open(model_file, 'rb'))\nbooster.get_score()\nbooster.get_fscore()\n<\/code><\/pre>\n\n<p>Refer <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">XGBoost doc<\/a> for methods to get feature importance from the Booster object such as <code>get_score()<\/code> or <code>get_fscore()<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.4,
        "Solution_reading_time":9.42,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":83.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":1.9786919444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The results can be written to SQL Azure using the writer module in the experiment but after publishing the web service the output comes in the Json Structure and it doesn't go to the writer module <\/p>",
        "Challenge_closed_time":1430897690128,
        "Challenge_comment_count":0,
        "Challenge_created_time":1430890566837,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1431065815883,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30068341",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":4.05,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":1.9786919444,
        "Challenge_title":"How to write the results of Azure ML web service to the azure sql database (The output of Azure ML web service is in Json structure)",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":990,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403541426412,
        "Poster_location":"Bengaluru, India",
        "Poster_reputation_count":191.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>Don't set output port and use Batch execution service - details are provided here - <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-publish-a-machine-learning-web-service\/\" rel=\"nofollow\">Publish web service<\/a> and <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-consume-web-services\/\" rel=\"nofollow\">consume web service<\/a><\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":28.8,
        "Solution_reading_time":5.51,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":29.7083297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a plain text csv file, which i am trying to read in Azure ML studio - the file format is pretty much like this<\/p>\n\n<pre><code>Geolife trajectory\nWGS 84\nAltitude is in Feet\nReserved 3\n0,2,255,My Track,0,0,2,8421376\n0\n39.984702,116.318417,0,492,39744.1201851852,2008-10-23,02:53:04\n39.984683,116.31845,0,492,39744.1202546296,2008-10-23,02:53:10\n39.984686,116.318417,0,492,39744.1203125,2008-10-23,02:53:15\n39.984688,116.318385,0,492,39744.1203703704,2008-10-23,02:53:20\n39.984655,116.318263,0,492,39744.1204282407,2008-10-23,02:53:25\n39.984611,116.318026,0,493,39744.1204861111,2008-10-23,02:53:30\n<\/code><\/pre>\n\n<p>The real data starts from Line 7, how could i strip it off, these files need to be downloaded on the fly so I don't think i would like to strip off the data by some code.<\/p>",
        "Challenge_closed_time":1463946230187,
        "Challenge_comment_count":0,
        "Challenge_created_time":1463839280200,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37363883",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":11.18,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":29.7083297222,
        "Challenge_title":"Azure Machine learning - Strip top X rows from dataset",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":53,
        "Challenge_word_count":90,
        "Platform":"Stack Overflow",
        "Poster_created_time":1331022293612,
        "Poster_location":"India",
        "Poster_reputation_count":4743.0,
        "Poster_view_count":811.0,
        "Solution_body":"<p>What is your source location - SQL or Blob or http?<\/p>\n\n<p>If SQL, then you can use query to start from line 6.<\/p>\n\n<p>If Blob\/http, I would suggest reading a file as a single column TSV format, use simple R\/Python script to drop first 6 rows and convert to csv<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.9,
        "Solution_reading_time":3.22,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":49.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1367453125248,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":2284.4218233333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>During a training script executed on a compute target, we're trying to download a registered Dataset from an ADLS2 Datastore. The problem is that it takes <strong>hours<\/strong> to download ~1.5Gb (splitted into ~8500 files) to the compute target with the following method : <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Datastore, Dataset, Run, Workspace\n\n# Retrieve the run context to get Workspace\nRUN = Run.get_context(allow_offline=True)\n\n# Retrieve the workspace\nws = RUN.experiment.workspace\n\n# Creating the Dataset object based on a registered Dataset\ndataset = Dataset.get_by_name(ws, name='my_dataset_registered')\n\n# Download the Dataset locally\ndataset.download(target_path='\/tmp\/data', overwrite=False)\n<\/code><\/pre>\n\n<p><strong>Important note :<\/strong> the Dataset is registered to a path in the Datalake that contains a lot of subfolders (as well subsubfolders, ..) containing small files of around 170Kb.<\/p>\n\n<p><strong>Note:<\/strong> I'm able to download the complete dataset to local computer within a few minutes using <code>az copy<\/code> or the Storage Explorer. Also, the Dataset is defined at a folder stage with the ** wildcard for scanning subfolders : <code>datalake\/relative\/path\/to\/folder\/**<\/code><\/p>\n\n<p>Is that a known issue ? How can I improve transfer speed ?<\/p>\n\n<p>Thanks !<\/p>",
        "Challenge_closed_time":1583895127880,
        "Challenge_comment_count":1,
        "Challenge_created_time":1583493276803,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1583526482776,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60562966",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.7,
        "Challenge_reading_time":18.02,
        "Challenge_score_count":5,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":111.6252991667,
        "Challenge_title":"Transfer from ADLS2 to Compute Target very slow Azure Machine Learning",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":593,
        "Challenge_word_count":175,
        "Platform":"Stack Overflow",
        "Poster_created_time":1447320137140,
        "Poster_location":null,
        "Poster_reputation_count":313.0,
        "Poster_view_count":40.0,
        "Solution_body":"<p><em>Edited to be more answer-like:<\/em><\/p>\n\n<p>It'd be helpful to include: what versions of azureml-core and azureml-dataprep SDK you are using, what type of VM you are running as the compute instance, and what types of files (e.g. jpg? txt?) your dataset is using. Also, what are you trying to achieve by downloading the complete dataset to your compute?<\/p>\n\n<p>Currently, compute instance image comes with azureml-core 1.0.83 and azureml-dataprep 1.1.35 pre-installed, which are 1-2 months old. You might be using even older versions. You can try upgrading by running in your notebook:<\/p>\n\n<pre><code>%pip install -U azureml-sdk\n<\/code><\/pre>\n\n<p>If you don't see any improvements to your scenario, you can file an issue on the official docs page to get someone to help debug your issue, such as the ref page for <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.file_dataset.filedataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">FileDataset<\/a>.<\/p>\n\n<p><em>(edited on June 9, 2020 to remove mention of experimental release because that is not happening anymore)<\/em><\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1591750401340,
        "Solution_link_count":1.0,
        "Solution_readability":9.1,
        "Solution_reading_time":14.06,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":150.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1398051491376,
        "Answerer_location":"Massachusetts",
        "Answerer_reputation_count":459.0,
        "Answerer_view_count":108.0,
        "Challenge_adjusted_solved_time":0.1220702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have close to 100 processing jobs to which I want to add certain tags. I've found commands that you can use to tag one resource with a list of tags. Is there any way I can do this for multiple jobs? Through CLI or through python+boto?<\/p>",
        "Challenge_closed_time":1657516175783,
        "Challenge_comment_count":2,
        "Challenge_created_time":1657515736330,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1657607287156,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72933908",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":3.4,
        "Challenge_reading_time":3.82,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1220702778,
        "Challenge_title":"Can I use AWS CLI to add tags to all processing jobs matching a certain regex",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":38,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1498814861883,
        "Poster_location":null,
        "Poster_reputation_count":37.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You can <code>ResourceGroupsTaggingAPI<\/code>'s method <code>tag_resources()<\/code>.<br \/>\nThis is used to apply one or more tags to the specified list of resources.<\/p>\n<p>References:<\/p>\n<ol>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/resourcegroupstaggingapi.html#ResourceGroupsTaggingAPI.Client.tag_resources\" rel=\"nofollow noreferrer\">Tag Resources using boto3<\/a><\/li>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/resourcegroupstaggingapi.html#ResourceGroupsTaggingAPI.Client.untag_resources\" rel=\"nofollow noreferrer\">UnTag Resources using boto3<\/a><\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":31.0,
        "Solution_reading_time":9.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":37.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1375058329287,
        "Answerer_location":null,
        "Answerer_reputation_count":762.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":0.1746386111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a project with DVC (Data Version Control), when I push files in my remote storage, the name of the files are changed. How I can conserve the names?<\/p>",
        "Challenge_closed_time":1620171651067,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620170453717,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1620171022368,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67393339",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.9,
        "Challenge_reading_time":2.75,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.3325972222,
        "Challenge_title":"dvc push, change the names of files on the remote storage",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":447,
        "Challenge_word_count":41,
        "Platform":"Stack Overflow",
        "Poster_created_time":1379685720448,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>Short answer: there is no way to do that.<\/p>\n<p>Long answer:\nDvc remote is a content-based storage, so names are not preserved. Dvc creates metafiles (*.dvc files) in your workspace that contain names and those files are usually tracked by git, so you need to use git remote and dvc remote together to have both filenames and their contents. Here is a more detailed explanation about the format of local and remote storage <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a> . Also, checkout <a href=\"https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.5,
        "Solution_reading_time":10.87,
        "Solution_score_count":5.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":81.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1443426419048,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Challenge_adjusted_solved_time":166.3439791667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I can only export data from AzureML by write instead to database that created previously. I need to know How to insert and fetch the data continuously the database because I need to use old data as well as the new data that get as the AzureML output to plot graph.<\/p>",
        "Challenge_closed_time":1480308703072,
        "Challenge_comment_count":0,
        "Challenge_created_time":1479709864747,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1480314865163,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40714064",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":4.14,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":166.3439791667,
        "Challenge_title":"How \"Azure ML export data to SQL database by insert in a row of database.\"",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":193,
        "Challenge_word_count":65,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455005478600,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":3.0,
        "Solution_body":"<ol>\n<li>Create a web service from the AzureML experiment. <\/li>\n<li>Access the web service using a program you written from C# or any language.<\/li>\n<li>You can get the output of the web service as a JSON.<\/li>\n<li>Use typical SQL ADD\/UPDATE queries to update the table<\/li>\n<li>When giving an input for the web service, fetch the data from the DB and pass as the JSON for it. <\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":4.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":68.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1550902509267,
        "Answerer_location":null,
        "Answerer_reputation_count":2669.0,
        "Answerer_view_count":3292.0,
        "Challenge_adjusted_solved_time":429.6992497222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>Context<\/strong><\/p>\n<p>In AzureML, we are facing an error when running a pipeline. It fails on <code>to_pandas_dataframe<\/code> because a particular dataset &quot;could not be read beyond end of stream&quot;. On its own, this seems to be an issue with the parquet file that is being registered, maybe special characters being misinterpreted.<\/p>\n<p>However, when we explicitly load a previous &quot;version&quot; of this Dataset--which points to the exact same location of data--it works as expected. In the documentation (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-version-track-datasets#versioning-best-practice\" rel=\"nofollow noreferrer\">here<\/a>), Azure says that &quot;when you load data from a dataset, the current data content referenced by the dataset is always loaded.&quot; This makes me think that a new version of the dataset with the same schema will be, well, the same.<\/p>\n<p><strong>Questions<\/strong><\/p>\n<ol>\n<li><p>What makes a Dataset version <em>different<\/em> from another version when both point to the same location? Is it only the schema definition?<\/p>\n<\/li>\n<li><p>Based on these differences, is there a way to figure out why one version would be succeeding and another failing?<\/p>\n<\/li>\n<\/ol>\n<p><strong>Attempts<\/strong><\/p>\n<ul>\n<li>The schemas of the two versions are identical. We can profile both in AzureML, and all the fields have the same profile information.<\/li>\n<\/ul>",
        "Challenge_closed_time":1641212437096,
        "Challenge_comment_count":1,
        "Challenge_created_time":1639665519797,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70380861",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":19.41,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":429.6992497222,
        "Challenge_title":"Azure ML Dataset Versioning: What is Different if it Points to the Same Data?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":245,
        "Challenge_word_count":206,
        "Platform":"Stack Overflow",
        "Poster_created_time":1591385782727,
        "Poster_location":"Chicago, IL, USA",
        "Poster_reputation_count":594.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>As rightly suggested by @Anand Sowmithiran in comment section, This looks more like a bug with the SDK.<\/p>\n<p>You can raise <a href=\"https:\/\/azure.microsoft.com\/en-us\/support\/create-ticket\/\" rel=\"nofollow noreferrer\">Azure support ticket<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.3,
        "Solution_reading_time":3.28,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1298484007147,
        "Answerer_location":"New York, NY, United States",
        "Answerer_reputation_count":9271.0,
        "Answerer_view_count":1819.0,
        "Challenge_adjusted_solved_time":6.4954608333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Cannot read AWS open data datasets into Sagemaker. Error is<\/p>\n\n<pre><code>download failed: s3:\/\/fast-ai-imageclas\/cifar100.tgz to ..\/..\/..\/tmp\/fastai-images\/cifar100.tgz An error occurred (AccessDenied) when calling the GetObject operation: Access Denied\n<\/code><\/pre>\n\n<p>code\n<a href=\"https:\/\/i.stack.imgur.com\/2b73H.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2b73H.png\" alt=\"sagemaker notebook s3 download access denied\"><\/a><\/p>\n\n<p>The user has the s3:getObjects * permission<\/p>\n\n<p>The user's permissions are the full s3 read policy and the full Sagemaker policies. The policies are<\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:Get*\",\n                \"s3:List*\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"application-autoscaling:DeleteScalingPolicy\",\n                \"application-autoscaling:DeleteScheduledAction\",\n                \"application-autoscaling:DeregisterScalableTarget\",\n                \"application-autoscaling:DescribeScalableTargets\",\n                \"application-autoscaling:DescribeScalingActivities\",\n                \"application-autoscaling:DescribeScalingPolicies\",\n                \"application-autoscaling:DescribeScheduledActions\",\n                \"application-autoscaling:PutScalingPolicy\",\n                \"application-autoscaling:PutScheduledAction\",\n                \"application-autoscaling:RegisterScalableTarget\",\n                \"aws-marketplace:ViewSubscriptions\",\n                \"cloudwatch:DeleteAlarms\",\n                \"cloudwatch:DescribeAlarms\",\n                \"cloudwatch:GetMetricData\",\n                \"cloudwatch:GetMetricStatistics\",\n                \"cloudwatch:ListMetrics\",\n                \"cloudwatch:PutMetricAlarm\",\n                \"cloudwatch:PutMetricData\",\n                \"codecommit:BatchGetRepositories\",\n                \"codecommit:CreateRepository\",\n                \"codecommit:GetRepository\",\n                \"codecommit:ListBranches\",\n                \"codecommit:ListRepositories\",\n                \"cognito-idp:AdminAddUserToGroup\",\n                \"cognito-idp:AdminCreateUser\",\n                \"cognito-idp:AdminDeleteUser\",\n                \"cognito-idp:AdminDisableUser\",\n                \"cognito-idp:AdminEnableUser\",\n                \"cognito-idp:AdminRemoveUserFromGroup\",\n                \"cognito-idp:CreateGroup\",\n                \"cognito-idp:CreateUserPool\",\n                \"cognito-idp:CreateUserPoolClient\",\n                \"cognito-idp:CreateUserPoolDomain\",\n                \"cognito-idp:DescribeUserPool\",\n                \"cognito-idp:DescribeUserPoolClient\",\n                \"cognito-idp:ListGroups\",\n                \"cognito-idp:ListIdentityProviders\",\n                \"cognito-idp:ListUserPoolClients\",\n                \"cognito-idp:ListUserPools\",\n                \"cognito-idp:ListUsers\",\n                \"cognito-idp:ListUsersInGroup\",\n                \"cognito-idp:UpdateUserPool\",\n                \"cognito-idp:UpdateUserPoolClient\",\n                \"ec2:CreateNetworkInterface\",\n                \"ec2:CreateNetworkInterfacePermission\",\n                \"ec2:CreateVpcEndpoint\",\n                \"ec2:DeleteNetworkInterface\",\n                \"ec2:DeleteNetworkInterfacePermission\",\n                \"ec2:DescribeDhcpOptions\",\n                \"ec2:DescribeNetworkInterfaces\",\n                \"ec2:DescribeRouteTables\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeVpcEndpoints\",\n                \"ec2:DescribeVpcs\",\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:BatchGetImage\",\n                \"ecr:CreateRepository\",\n                \"ecr:GetAuthorizationToken\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:Describe*\",\n                \"elastic-inference:Connect\",\n                \"glue:CreateJob\",\n                \"glue:DeleteJob\",\n                \"glue:GetJob\",\n                \"glue:GetJobRun\",\n                \"glue:GetJobRuns\",\n                \"glue:GetJobs\",\n                \"glue:ResetJobBookmark\",\n                \"glue:StartJobRun\",\n                \"glue:UpdateJob\",\n                \"groundtruthlabeling:*\",\n                \"iam:ListRoles\",\n                \"kms:DescribeKey\",\n                \"kms:ListAliases\",\n                \"lambda:ListFunctions\",\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:DescribeLogStreams\",\n                \"logs:GetLogEvents\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ecr:SetRepositoryPolicy\",\n                \"ecr:CompleteLayerUpload\",\n                \"ecr:BatchDeleteImage\",\n                \"ecr:UploadLayerPart\",\n                \"ecr:DeleteRepositoryPolicy\",\n                \"ecr:InitiateLayerUpload\",\n                \"ecr:DeleteRepository\",\n                \"ecr:PutImage\"\n            ],\n            \"Resource\": \"arn:aws:ecr:*:*:repository\/*sagemaker*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"codecommit:GitPull\",\n                \"codecommit:GitPush\"\n            ],\n            \"Resource\": [\n                \"arn:aws:codecommit:*:*:*sagemaker*\",\n                \"arn:aws:codecommit:*:*:*SageMaker*\",\n                \"arn:aws:codecommit:*:*:*Sagemaker*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:CreateSecret\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:ListSecrets\",\n                \"secretsmanager:TagResource\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetSecretValue\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"secretsmanager:ResourceTag\/SageMaker\": \"true\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"robomaker:CreateSimulationApplication\",\n                \"robomaker:DescribeSimulationApplication\",\n                \"robomaker:DeleteSimulationApplication\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"robomaker:CreateSimulationJob\",\n                \"robomaker:DescribeSimulationJob\",\n                \"robomaker:CancelSimulationJob\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::*SageMaker*\",\n                \"arn:aws:s3:::*Sagemaker*\",\n                \"arn:aws:s3:::*sagemaker*\",\n                \"arn:aws:s3:::*aws-glue*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:CreateBucket\",\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\",\n                \"s3:ListAllMyBuckets\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEqualsIgnoreCase\": {\n                    \"s3:ExistingObjectTag\/SageMaker\": \"true\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"lambda:InvokeFunction\"\n            ],\n            \"Resource\": [\n                \"arn:aws:lambda:*:*:function:*SageMaker*\",\n                \"arn:aws:lambda:*:*:function:*sagemaker*\",\n                \"arn:aws:lambda:*:*:function:*Sagemaker*\",\n                \"arn:aws:lambda:*:*:function:*LabelingFunction*\"\n            ]\n        },\n        {\n            \"Action\": \"iam:CreateServiceLinkedRole\",\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:iam::*:role\/aws-service-role\/sagemaker.application-autoscaling.amazonaws.com\/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"iam:AWSServiceName\": \"sagemaker.application-autoscaling.amazonaws.com\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:CreateServiceLinkedRole\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"iam:AWSServiceName\": \"robomaker.amazonaws.com\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:PassRole\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"iam:PassedToService\": [\n                        \"sagemaker.amazonaws.com\",\n                        \"glue.amazonaws.com\",\n                        \"robomaker.amazonaws.com\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>The Sagemaker instance is in us-east-1 same as the dataset.<\/p>\n\n<p>The dataset is <a href=\"https:\/\/registry.opendata.aws\/fast-ai-imageclas\/\" rel=\"nofollow noreferrer\">https:\/\/registry.opendata.aws\/fast-ai-imageclas\/<\/a><\/p>",
        "Challenge_closed_time":1549864663812,
        "Challenge_comment_count":2,
        "Challenge_created_time":1549841280153,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54622191",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":40.9,
        "Challenge_reading_time":86.83,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":6.4954608333,
        "Challenge_title":"Access Denied read open data into Sagemaker",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1192,
        "Challenge_word_count":309,
        "Platform":"Stack Overflow",
        "Poster_created_time":1298484007147,
        "Poster_location":"New York, NY, United States",
        "Poster_reputation_count":9271.0,
        "Poster_view_count":1819.0,
        "Solution_body":"<p>thanks to Matthew I looked into the permissions of the notebook itself, not just the user using Sagemaker.<\/p>\n\n<p>The policies on the notebook look like this and I can download from the aws open data datasets!<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/s0jwV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s0jwV.png\" alt=\"notebook settings\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/OqEHi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OqEHi.png\" alt=\"notebook permissions\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.1,
        "Solution_reading_time":7.05,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":1.4933055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In the API docs about <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.html#\" rel=\"nofollow noreferrer\"><code>kedro.io<\/code><\/a> and <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.contrib.io.html\" rel=\"nofollow noreferrer\"><code>kedro.contrib.io<\/code><\/a> I could not find info about how to read\/write data from\/to network attached storage such as e.g. <a href=\"https:\/\/en.avm.de\/guide\/using-the-fritzbox-nas-function\/\" rel=\"nofollow noreferrer\">FritzBox NAS<\/a>.<\/p>",
        "Challenge_closed_time":1589448276967,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589441422903,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1589442901067,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61791713",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":14.1,
        "Challenge_reading_time":7.54,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.9039066667,
        "Challenge_title":"How can I read\/write data from\/to network attached storage with kedro?",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":675,
        "Challenge_word_count":46,
        "Platform":"Stack Overflow",
        "Poster_created_time":1441627039648,
        "Poster_location":"Augsburg, Germany",
        "Poster_reputation_count":3635.0,
        "Poster_view_count":383.0,
        "Solution_body":"<p>So I'm a little rusty on network attached storage, but:<\/p>\n\n<ol>\n<li><p>If you can mount your network attached storage onto your OS and access it like a regular folder, then it's just a matter of providing the right <code>filepath<\/code> when writing the config for a given catalog entry. See for example: <a href=\"https:\/\/stackoverflow.com\/questions\/7169845\/using-python-how-can-i-access-a-shared-folder-on-windows-network\">Using Python, how can I access a shared folder on windows network?<\/a><\/p><\/li>\n<li><p>Otherwise, if accessing the network attached storage requires anything special, you might want to <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/14_create_a_new_dataset.html\" rel=\"nofollow noreferrer\">create a custom dataset<\/a> that uses a Python library for interfacing with your network attached storage. Something like <a href=\"https:\/\/pysmb.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">pysmb<\/a> comes to mind.<\/p><\/li>\n<\/ol>\n\n<p>The custom dataset could borrow heavily from the logic in existing <code>kedro.io<\/code> or <code>kedro.extras.datasets<\/code> datasets, but you replace the filepath\/fsspec handling code with <code>pysmb<\/code> instead.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.0,
        "Solution_reading_time":15.63,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":132.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1533910280492,
        "Answerer_location":null,
        "Answerer_reputation_count":49.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":172.1121238889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to write a numpy.ndarray as the labels for Amazon Sagemaker's conversion tool: write_numpy_to_dense_tensor(). It converts a numpy array of  features and labels to a RecordIO for better use of Sagemaker algorithms.<\/p>\n\n<p>However, if I try to pass a multilabel output for the labels, I get an error stating it can only be a vector (i.e. a scalar for every feature row).<\/p>\n\n<p>Is there any way of having multiple values in the label? This is useful for multidimensional regressions which can be achieved with XGBoost, Random Forests, Neural Networks, etc.<\/p>\n\n<p><strong>Code<\/strong><\/p>\n\n<pre><code>import sagemaker.amazon.common as smac\nprint(\"Types: {}, {}\".format(type(X_train), type(y_train)))\nprint(\"X_train shape: {}\".format(X_train.shape))\nprint(\"y_train shape: {}\".format(y_train.shape))\nf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(f, X_train.astype('float32'), y_train.astype('float32'))\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong><\/p>\n\n<pre><code>Types: &lt;class 'numpy.ndarray'&gt;, &lt;class 'numpy.ndarray'&gt;\nX_train shape: (9919, 2684)\ny_train shape: (9919, 20)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-14-fc1033b7e309&gt; in &lt;module&gt;()\n      3 print(\"y_train shape: {}\".format(y_train.shape))\n      4 f = io.BytesIO()\n----&gt; 5 smac.write_numpy_to_dense_tensor(f, X_train.astype('float32'), y_train.astype('float32'))\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/amazon\/common.py in write_numpy_to_dense_tensor(file, array, labels)\n     94     if labels is not None:\n     95         if not len(labels.shape) == 1:\n---&gt; 96             raise ValueError(\"Labels must be a Vector\")\n     97         if labels.shape[0] not in array.shape:\n     98             raise ValueError(\"Label shape {} not compatible with array shape {}\".format(\n\nValueError: Labels must be a Vector\n<\/code><\/pre>",
        "Challenge_closed_time":1534186228336,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533566624690,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51710241",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":25.49,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":172.1121238889,
        "Challenge_title":"Using numpy.ndarray type (multilabel) for labels in Sagemaker RecordIO format?",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":683,
        "Challenge_word_count":203,
        "Platform":"Stack Overflow",
        "Poster_created_time":1448724556760,
        "Poster_location":null,
        "Poster_reputation_count":454.0,
        "Poster_view_count":44.0,
        "Solution_body":"<p>Tom, XGBoost does not support RecordIO format. It only supports csv and libsvm. Also, the algorithm itself doesn\u2019t natively support multi-label. But there are a couple of ways around it: <a href=\"https:\/\/stackoverflow.com\/questions\/40916939\/xg-boost-for-multilabel-classification\">Xg boost for multilabel classification?<\/a><\/p>\n\n<p>Random Cut Forest does not support multiple labels either. If more than one label is provided it picks up the first only.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":5.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1363352099923,
        "Answerer_location":"Washington, DC, USA",
        "Answerer_reputation_count":828.0,
        "Answerer_view_count":530.0,
        "Challenge_adjusted_solved_time":2983.5262194445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a custom Image Classifier in Amazon SageMaker. It is giving me the following error:<\/p>\n\n<p><code>\"ClientError: Data download failed:NoSuchKey (404): The specified key does not exist.\"<\/code><\/p>\n\n<p>I'm assuming this means one of the pictures in my <code>.lst<\/code> file is missing from the directory. Is there some way to find out which <code>.lst<\/code> listing it is specifically having trouble with?<\/p>",
        "Challenge_closed_time":1545063624710,
        "Challenge_comment_count":1,
        "Challenge_created_time":1534309681237,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1534322930320,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51853249",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":5.89,
        "Challenge_score_count":2,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2987.2065202778,
        "Challenge_title":"Error Tracking in Amazon SageMaker",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":305,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1363352099923,
        "Poster_location":"Washington, DC, USA",
        "Poster_reputation_count":828.0,
        "Poster_view_count":530.0,
        "Solution_body":"<p>Upon further examination (of the log files), it appears the issue does not lie with the .lst file itself, but with the image files it was referencing (which now leaves me wondering why AWS doesn't just say that instead of saying the .lst file is corrupt). I'm going through the image files one-by-one to verify they are correct, hopefully that will solve the problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":4.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1512520584492,
        "Answerer_location":"Bloomington, IN, USA",
        "Answerer_reputation_count":868.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":7347.4097222222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to run a machine learning experiment in azureml.<\/p>\n<p>I can't figure out how to get the workspace context from the control script.  Examples like <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-bring-data#control-script\" rel=\"nofollow noreferrer\">this one<\/a> in the microsoft docs use Workspace.from_config().  When I use this in the control script I get the following error:<\/p>\n<blockquote>\n<p>&quot;message&quot;: &quot;We could not find config.json in: [path] or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.&quot;<\/p>\n<\/blockquote>\n<p>I've also tried including my subscription id and the resource specs like so:<\/p>\n<pre><code>subscription_id = 'id'\nresource_group = 'name'\nworkspace_name = 'name'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n<\/code><\/pre>\n<p>In this case I have to monitor the log and authenticate on each run as I would locally.<\/p>\n<p>How do you get the local workspace from a control script for azureml?<\/p>",
        "Challenge_closed_time":1641958092267,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615507417267,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66592313",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":9.5,
        "Challenge_reading_time":14.56,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":7347.4097222222,
        "Challenge_title":"Get local workspace in azureml",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":333,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1512520584492,
        "Poster_location":"Bloomington, IN, USA",
        "Poster_reputation_count":868.0,
        "Poster_view_count":51.0,
        "Solution_body":"<p>This had no answers for 10 months, and now they are coming in :).  I figuerd this out quite a while ago but haven't gotten around to posting the answer.  Here it is.<\/p>\n<p>From the training script, you can get the workspace from the run context as follows:<\/p>\n<pre><code>from azureml.core import Run\nRun.get_context()\nws = run.experiment.workspace\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.5,
        "Solution_reading_time":4.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1631802016976,
        "Answerer_location":"Monterrey, Nuevo Le\u00f3n, M\u00e9xico",
        "Answerer_reputation_count":1415.0,
        "Answerer_view_count":2151.0,
        "Challenge_adjusted_solved_time":31.9663961111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using GCP Vertex AI pipeline (KFP) and using <code>google-cloud-aiplatform==1.10.0<\/code>, <code>kfp==1.8.11<\/code>, <code>google-cloud-pipeline-components==0.2.6<\/code>\nIn a component I am getting a gcp_resources <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/google-cloud\/google_cloud_pipeline_components\/proto\/README.md\" rel=\"nofollow noreferrer\">documentation<\/a> :<\/p>\n<pre><code>gcp_resources (str):\n            Serialized gcp_resources proto tracking the create endpoint's long running operation.\n<\/code><\/pre>\n<p>To extract the endpoint_id to do online prediction of my deployed model, I am doing:<\/p>\n<pre><code>from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\nfrom google.protobuf.json_format import Parse\ninput_gcp_resources = Parse(endpoint_ressource_name, GcpResources())\ngcp_resources=input_gcp_resources.resources.__getitem__(0).resource_uri.split('\/')\nendpoint_id=gcp_resources[gcp_resources.index('endpoints')+1]\n<\/code><\/pre>\n<p>Is there a better\/native way of extracting such info ?<\/p>",
        "Challenge_closed_time":1644873889256,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644758810230,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71101070",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.2,
        "Challenge_reading_time":15.45,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":31.9663961111,
        "Challenge_title":"How to properly extract endpoint id from gcp_resources of a Vertex AI pipeline on GCP?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":225,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1465222092252,
        "Poster_location":"Z\u00fcrich, Switzerland",
        "Poster_reputation_count":1414.0,
        "Poster_view_count":478.0,
        "Solution_body":"<p>In this case is the best way to extract the information. But, I recommend using the <a href=\"http:\/\/ttps:\/\/github.com\/aio-libs\/yarl\" rel=\"nofollow noreferrer\">yarl<\/a> library for complex uri to parse.<\/p>\n<p>You can see this example:<\/p>\n<pre><code>&gt;&gt;&gt; from yarl import URL\n&gt;&gt;&gt; url = URL('https:\/\/www.python.org\/~guido?arg=1#frag')\n&gt;&gt;&gt; url\nURL('https:\/\/www.python.org\/~guido?arg=1#frag')\n<\/code><\/pre>\n<p>All URL parts can be accessed by these properties.<\/p>\n<pre><code>&gt;&gt;&gt; url.scheme\n'https'\n&gt;&gt;&gt; url.host\n'www.python.org'\n&gt;&gt;&gt; url.path\n'\/~guido'\n&gt;&gt;&gt; url.query_string\n'arg=1'\n&gt;&gt;&gt; url.query\n&lt;MultiDictProxy('arg': '1')&gt;\n&gt;&gt;&gt; url.fragment\n'frag'\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":9.0,
        "Solution_reading_time":9.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":72.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":30.4097575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an already trained a TensorFlow model outside of SageMaker.<\/p>\n<p>I am trying to focus on deployment\/inference but I am facing issues with inference.<\/p>\n<p>For deployment I did this:<\/p>\n<pre><code>from sagemaker.tensorflow.serving import TensorFlowModel\ninstance_type = 'ml.c5.xlarge' \n\nmodel = TensorFlowModel(\n    model_data=model_data,\n    name= 'tfmodel1',\n    framework_version=&quot;2.2&quot;,\n    role=role, \n    source_dir='code',\n)\n\npredictor = model.deploy(endpoint_name='test', \n                                       initial_instance_count=1, \n                                       tags=tags,\n                                       instance_type=instance_type)\n<\/code><\/pre>\n<p>When I tried to infer the model I did this:<\/p>\n<pre><code>import PIL\nfrom PIL import Image\nimport numpy as np\nimport json\nimport boto3\n\nimage = PIL.Image.open('img_test.jpg')\nclient = boto3.client('sagemaker-runtime')\nbatch_size = 1\nimage = np.asarray(image.resize((512, 512)))\nimage = np.concatenate([image[np.newaxis, :, :]] * batch_size)\nbody = json.dumps({&quot;instances&quot;: image.tolist()})\n\nioc_predictor_endpoint_name = &quot;test&quot;\ncontent_type = 'application\/x-image'   \nioc_response = client.invoke_endpoint(\n    EndpointName=ioc_predictor_endpoint_name,\n    Body=body,\n    ContentType=content_type\n )\n<\/code><\/pre>\n<p>But I have this error:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: application\/x-image&quot;}&quot;.\n<\/code><\/pre>\n<p>I also tried:<\/p>\n<pre><code>from sagemaker.predictor import Predictor\n\npredictor = Predictor(ioc_predictor_endpoint_name)\ninference_response = predictor.predict(data=body)\nprint(inference_response)\n<\/code><\/pre>\n<p>And have this error:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: application\/octet-stream&quot;}&quot;.\n<\/code><\/pre>\n<p>What can I do ? I don't know if I missed something<\/p>",
        "Challenge_closed_time":1651191185627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651077733827,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1651081710500,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72032469",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.5,
        "Challenge_reading_time":27.63,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":31.5143888889,
        "Challenge_title":"How to infer a tensorflow pre trained deployed model with an image?",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":145,
        "Challenge_word_count":191,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606642099552,
        "Poster_location":null,
        "Poster_reputation_count":371.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>Have you tested this model locally? How does inference work with your TF model locally? This should show you how the input needs to be formatted for inference with that model in specific. Application\/x-image data format should be fine. Do you have a custom inference script? Check out this link here for adding an inference script with will let you control pre\/post processing and you can log each line to capture the error: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.9,
        "Solution_reading_time":7.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":77.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1505264548896,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":343.0856288889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The MLFlow Tracking is great for monitoring experiments, but I wonder if there is a solution on MLFlow or another open-source platform that can be integrated to monitor data and model drift.<\/p>\n<p>There is a <a href=\"https:\/\/databricks.com\/blog\/2019\/09\/18\/productionizing-machine-learning-from-deployment-to-drift-detection.html\" rel=\"noreferrer\">post<\/a> from Databricks showing how to achieve that with Delta Lake, however, as you can deploy and serve models with MLFlow, it looks to me that it would be easy to monitor the predictions made by the model, the same way we monitor the experiments run.<\/p>",
        "Challenge_closed_time":1613074603467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611839495203,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65937786",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.2,
        "Challenge_reading_time":8.3,
        "Challenge_score_count":5,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":343.0856288889,
        "Challenge_title":"Data and model drift monitoring with MLflow",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2539,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525393797416,
        "Poster_location":"Philadelphia, USA",
        "Poster_reputation_count":130.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>My team has recently added integration between MLflow and our open source data monitoring library called <a href=\"https:\/\/github.com\/whylabs\/whylogs-python\" rel=\"nofollow noreferrer\">whylogs<\/a>. This lets you log statistical profiles of the data passing through the model and\/or the output of the model. You can then collect these profiles from MLflow run artifacts and analyze them for drift.<\/p>\n<p>We have a <a href=\"https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/MLFlow%20Integration%20Example.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a> that walks you through the integration process and a <a href=\"https:\/\/whylabs.ai\/blog\/posts\/on-model-lifecycle-and-monitoring\" rel=\"nofollow noreferrer\">blog post<\/a> to go along with it. Lmk if you have any questions or additional feature requests!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.4,
        "Solution_reading_time":10.72,
        "Solution_score_count":4.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":90.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":8283.79234,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>What is the best way to run TensorFlow 2.0 with AWS Sagemeker?<\/p>\n\n<p>As of today (Aug 7th, 2019) AWS does not provide TensorFlow 2.0 <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"noreferrer\">SageMaker containers<\/a>, so my understanding is that I need to build my own.<\/p>\n\n<p>What is the best Base image to use? Example Dockerfile?<\/p>",
        "Challenge_closed_time":1567635751827,
        "Challenge_comment_count":1,
        "Challenge_created_time":1565186451297,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1565186790803,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57396212",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":8.5,
        "Challenge_reading_time":4.99,
        "Challenge_score_count":13,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":680.3612583334,
        "Challenge_title":"SageMaker and TensorFlow 2.0",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":4136,
        "Challenge_word_count":52,
        "Platform":"Stack Overflow",
        "Poster_created_time":1446859510543,
        "Poster_location":"Toronto, Canada",
        "Poster_reputation_count":3259.0,
        "Poster_view_count":233.0,
        "Solution_body":"<p>EDIT: <strong>Amazon SageMaker does now support TF 2.0 and higher.<\/strong><\/p>\n<ul>\n<li>SageMaker + TensorFlow docs: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html<\/a><\/li>\n<li>Supported Tensorflow versions (and Docker URIs): <a href=\"https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images<\/a><\/li>\n<\/ul>\n<hr \/>\n<p><em>Original answer<\/em><\/p>\n<p>Here is an example Dockerfile that uses <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"nofollow noreferrer\">the underlying SageMaker Containers library<\/a> (this is what is used in the official pre-built Docker images):<\/p>\n<pre><code>FROM tensorflow\/tensorflow:2.0.0b1\n\nRUN pip install sagemaker-containers\n\n# Copies the training code inside the container\nCOPY train.py \/opt\/ml\/code\/train.py\n\n# Defines train.py as script entrypoint\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>For more information on this approach, see <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html<\/a><\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1595008443227,
        "Solution_link_count":7.0,
        "Solution_readability":25.0,
        "Solution_reading_time":19.1,
        "Solution_score_count":10.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":94.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1500629225150,
        "Answerer_location":null,
        "Answerer_reputation_count":5939.0,
        "Answerer_view_count":886.0,
        "Challenge_adjusted_solved_time":133.8006258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to create a module for Sagemaker endpoints. There's an optional object variable called <code>async_inference_config<\/code>. If you omit it, the endpoint being deployed is synchronous, but if you include it, the endpoint deployed is asynchronous. To satisfy both of these usecases, the <code>async_inference_config<\/code> needs to be an optional block.<\/p>\n<p>I am unsure of how to make this block optional though.<br \/>\nAny guidance would be greatly appreciated. See example below of structure of the optional parameter.<\/p>\n<p><strong>Example:<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_endpoint_configuration&quot; &quot;sagemaker_endpoint_configuration&quot; {\n  count = var.create ? 1 : 0\n\n  name = var.endpoint_configuration_name\n  production_variants {\n    instance_type          = var.instance_type\n    initial_instance_count = var.instance_count\n    model_name             = var.model_name\n    variant_name           = var.variant_name\n  }\n  async_inference_config {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n  lifecycle {\n    create_before_destroy = true\n    ignore_changes        = [&quot;name&quot;]\n  }\n\n  tags = var.tags\n\n  depends_on = [aws_sagemaker_model.sagemaker_model]\n}\n<\/code><\/pre>\n<p><strong>Update:<\/strong> What I tried based on the below suggestion, which seemed to work<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.async_inference_config == null ? [] : [true]\n    content {\n      output_config {\n        s3_output_path = lookup(var.async_inference_config, &quot;s3_output_path&quot;, null)\n      }\n      client_config {\n        max_concurrent_invocations_per_instance = lookup(var.async_inference_config, &quot;max_concurrent_invocations_per_instance&quot;, null)\n      }\n    }\n  }\n<\/code><\/pre>",
        "Challenge_closed_time":1658387166296,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658386596203,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1658442017223,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73061907",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.7,
        "Challenge_reading_time":23.65,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":0.1583591667,
        "Challenge_title":"Terraform - Optional Nested Variable",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":55,
        "Challenge_word_count":146,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443225809767,
        "Poster_location":"Vancouver, BC, Canada",
        "Poster_reputation_count":2332.0,
        "Poster_view_count":560.0,
        "Solution_body":"<p>You could use a <code>dynamic<\/code> block [1] in combination with <code>for_each<\/code> meta-argument [2]. It would look something like:<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.s3_output_path != null &amp;&amp; var.max_concurrent_invocations_per_instance != null ? [1] : []\n    content {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n}\n<\/code><\/pre>\n<p>Of course, you could come up with a different variable, say <code>enable_async_inference_config<\/code> (probalby of type <code>bool<\/code>) and base the <code>for_each<\/code> on that, e.g.:<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.enable_async_inference_config ? [1] : []\n    content {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n}\n<\/code><\/pre>\n<hr \/>\n<p>[1] <a href=\"https:\/\/www.terraform.io\/language\/expressions\/dynamic-blocks\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/expressions\/dynamic-blocks<\/a><\/p>\n<p>[2] <a href=\"https:\/\/www.terraform.io\/language\/meta-arguments\/for_each\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/meta-arguments\/for_each<\/a><\/p>",
        "Solution_comment_count":13.0,
        "Solution_last_edit_time":1658923699476,
        "Solution_link_count":4.0,
        "Solution_readability":23.4,
        "Solution_reading_time":18.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":82.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":5.3960230556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am developing machine learning repositories that require fairly large trained model files to run. These files are not part of the git remote but is tracked by DVC and is saved in a separate remote storage. I am running into issues when I am trying to run unit tests in the CI pipeline for functions that require these model files to make their prediction. Since I don't have access them in the git remote, I can't test them.<\/p>\n<p>What is the best practice that people usually do in this situation? I can think of couple of options -<\/p>\n<ul>\n<li>Pull the models from the DVC remote inside the CI pipeline. I don't want to do this becasue downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/li>\n<li>Use <code>unittest.mock<\/code> to simulate the output of from the model prediction and test other parts of my code. This is what I am doing now but it's sort of a pain with unittest's mock functionalities. That module wasn't really developed with ML in mind from what I can tell. It's missing (or is hard to find) some functionalities that I would have really liked. Are there any good tools for doing this geared specifically towards ML?<\/li>\n<li>Do weird reformatting of the function definition that allows me to essentially do option 2 but without a mock module. That is, just test the surrounding logic and don't worry about the model output.<\/li>\n<li>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/li>\n<\/ul>\n<p>What do people usually do in this situation?<\/p>",
        "Challenge_closed_time":1603314290768,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603252093370,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1603305923907,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64456396",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":20.6,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":17.277055,
        "Challenge_title":"How do I unit test a function in the CI pipeline that uses model files that are not part of the git remote?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":289,
        "Challenge_word_count":304,
        "Platform":"Stack Overflow",
        "Poster_created_time":1446746840592,
        "Poster_location":null,
        "Poster_reputation_count":2545.0,
        "Poster_view_count":382.0,
        "Solution_body":"<p>If we talk about unit tests, I think it's indeed better to do a mock. It's best to have unit tests small, testing actual logic of the unit, etc. It's good to have other tests though that would pull the model and run some logic on top of that - I would call them integration tests.<\/p>\n<p>It's not black and white though. If you for some reason see that it's easier to use an actual model (e.g. it changes a lot and it is easier to use it instead of maintaining and updating stubs\/fixtures), you could potentially cache it.<\/p>\n<p>I think, to help you with the mock, you would need to share some technical details- how does the function look like, what have you tried, what breaks, etc.<\/p>\n<blockquote>\n<p>to do this because downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/p>\n<\/blockquote>\n<p>I think you can potentially utilize CI systems cache to avoid downloading it over and over again. This is the GitHub Actions related <a href=\"https:\/\/github.com\/actions\/cache#cache-limits\" rel=\"nofollow noreferrer\">repository<\/a>, this is <a href=\"https:\/\/circleci.com\/docs\/2.0\/caching\" rel=\"nofollow noreferrer\">CircleCI<\/a>. The idea is the same across all common CI providers. Which one are considering to use, btw?<\/p>\n<blockquote>\n<p>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/p>\n<\/blockquote>\n<p>This can be the way, but if models are large enough you will pollute Git history significantly. On some CI systems it can become even slower since they will be fetching this with regular <code>git clone<\/code>. Effectively, downloading models anyway.<\/p>\n<p>Btw, if you use DVC or not take a look at another open-source project that is made specifically to do CI\/CD for ML - <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">CML<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1603325349590,
        "Solution_link_count":3.0,
        "Solution_readability":7.6,
        "Solution_reading_time":23.14,
        "Solution_score_count":3.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":297.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1341273154903,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":5996.0,
        "Answerer_view_count":666.0,
        "Challenge_adjusted_solved_time":0.1159838889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running an ML pipeline and the training component\/step (see code below) continues to fail with the following error: &quot;RuntimeError: Unknown return type: &lt;class 'inspect._empty'&gt;. Must be one of <code>str<\/code>, <code>int<\/code>, <code>float<\/code>, a subclass of <code>Artifact<\/code>, or a NamedTuple collection of these types.&quot;<\/p>\n<p>Any ideas on what might be causing the issue\/error and how to resolve it?<\/p>\n<p>Thank you!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>\n@component(\n    # this component builds an xgboost classifier with xgboost\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;\n)\n\ndef build_xgb_xgboost(project: str, \n                            bq_dataset: str, \n                            test_view_name: str,\n                            bq_location: str,\n                            metrics: Output[Metrics],\n                            model: Output[Model]\n\n):\n    from google.cloud import bigquery\n    import xgboost as xgb\n    import pandas as pd\n    from xgboost import XGBRegressor\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.metrics import mean_squared_error as MSE\n    from sklearn.metrics import mean_absolute_error\n    import joblib\n    import pyarrow\n    import db_dtypes\n     \n\n    client = bigquery.Client(project=project) \n\n    view_uri = f&quot;{project}.{bq_dataset}.{test_view_name}&quot; #replace view_name with test_view_name\n    \n    build_df_for_xgboost = '''\n    SELECT * FROM `{view_uri}`\n    '''.format(view_uri = view_uri)\n\n    job_config = bigquery.QueryJobConfig()\n    df_1 = client.query(build_df_for_xgboost).to_dataframe()\n    \n    #client.query(build_df_for_xgboost, job_config=job_config).to_dataframe()  \n    \n    df = df_1.drop(['int64_field_0'], axis=1)\n    \n    def onehot_encode(df, column):\n        df = df.copy()\n        dummies = pd.get_dummies(df[column], prefix=column)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    return df\n    \n    # Binary encoding\n    df['preferred_foot'] = df['preferred_foot'].replace({'left': 0, 'right': 1})\n    \n    # One-hot encoding\n    for column in ['attacking_work_rate', 'defensive_work_rate']:\n        df = onehot_encode(df, column=column)\n    \n    # Split df into X and y\n    y = df['overall_rating']\n    X = df.drop('overall_rating', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n\n    #specify parameters\n    \n    #define your model \n    bst = XGBRegressor(\n    objective='reg:linear',\n    learning_rate = '.1',\n    alpha = '0.001'\n    )\n    \n    #fit your model\n    bst.fit(X_train, y_train)\n    \n    # Predict the model \n    y_pred = bst.predict(X_test)\n    rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n    mae = mean_absolute_error(y_test, y_pred)\n    \n    metrics.log_metric(&quot;RMSE&quot;, rmse)\n    metrics.log_metric(&quot;framework&quot;, &quot;xgboost&quot;)\n    metrics.log_metric(&quot;dataset_size&quot;, len(df))\n    metrics.log_metric(&quot;MAE&quot;, mae)\n    \n    dump(bst, model.path + &quot;.joblib&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1659257407332,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655350561587,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1659256989790,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72640182",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.5,
        "Challenge_reading_time":44.17,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":37,
        "Challenge_solved_time":1085.2349291667,
        "Challenge_title":"Kubeflow Pipeline Training Component Failing | Unknown return type: <class 'inspect._empty'>",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":107,
        "Challenge_word_count":278,
        "Platform":"Stack Overflow",
        "Poster_created_time":1621620820567,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>I think this might just be a bug in the version of KFP v2 SDK code you're using.<\/p>\n<p>I mostly use the stable KFPv1 methods to avoid problems.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\n\n\ndef train_xgboost_model(\n    project: str, \n    bq_dataset: str, \n    test_view_name: str,\n    bq_location: str,\n    metrics_path: OutputPath(Metrics),\n    model_path: OutputPath(Model),\n):\n    import json\n    from pathlib import Path\n\n    metrics = {\n       ...\n    }\n    Path(metrics_path).write_text(json.dumps(metrics))\n\n    dump(bst, model_path)\n\ntrain_xgboost_model_op = create_component_from_func(\n    func=train_xgboost_model,\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;,\n)\n\n<\/code><\/pre>\n<p>You can also find many examples of real-world components in this repo: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components<\/a><\/p>\n<p>including an XGBoost trainer <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py<\/a><\/p>\n<p>and a full XGBoost pipeline: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":28.3,
        "Solution_reading_time":27.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":107.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1425802890212,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":14.3147588889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Getting the following response even when I make one request (concurrency set to 200) to a web service. <\/p>\n\n<p>{ status: 503, headers: '{\"content-length\":\"174\",\"content-type\":\"application\/json; charset=utf-8\",\"etag\":\"\\\"8ce068bf420a485c8096065ea3e4f436\\\"\",\"server\":\"Microsoft-HTTPAPI\/2.0\",\"x-ms-request-id\":\"d5c56cdd-644f-48ba-ba2b-6eb444975e4c\",\"date\":\"Mon, 15 Feb 2016 04:54:01 GMT\",\"connection\":\"close\"}',  body: '{\"error\":{\"code\":\"ServiceUnavailable\",\"message\":\"Service is temporarily unavailable.\",\"details\":[{\"code\":\"NoMoreResources\",\"message\":\"No resources available for request.\"}]}}' }<\/p>\n\n<p>The request-response web service is a recommender retraining web service with the training set containing close to 200k records. The training set is already present in my ML studio dataset, only 10-15 extra records are passed in the request. The same experiment was working flawlessly till 13th Feb 2016. I have already tried increasing the concurrency but still the same issue. I even reduced the size of the training set to 20 records, still didn't work.<\/p>\n\n<p>I have two web service both doing something similar and both aren't working since 13th Feb 2016. <\/p>\n\n<p>Finally, I created a really small experiment ( skill.csv --> split row ---> web output )   which doesn't take any input. It just has to return some part of the dataset. Did not work, response code 503.<\/p>\n\n<p>The logs I got are as follows<\/p>\n\n<p>{\n  \"version\": \"2014-10-01\",\n  \"diagnostics\": [{\n    .....\n    {\n      \"type\": \"GetResourceEndEvent\",\n      \"timestamp\": 13.1362,\n      \"resourceId\": \"5e2d653c2b214e4dad2927210af4a436.865467b9e7c5410e9ebe829abd0050cd.v1-default-111\",\n      \"status\": \"Failure\",\n      \"error\": \"The Uri for the target storage location is not specified. Please consider changing the request's location mode.\"\n    },\n    {\n      \"type\": \"InitializationSummary\",\n      \"time\": \"2016-02-15T04:46:18.3651714Z\",\n      \"status\": \"Failure\",\n      \"error\": \"The Uri for the target storage location is not specified. Please consider changing the request's location mode.\"\n    }\n  ]\n}<\/p>\n\n<p>What am I missing? Or am I doing it completely wrong?<\/p>\n\n<p>Thank you in advance.<\/p>\n\n<p>PS: Data is stored in mongoDB and then imported as CSV<\/p>",
        "Challenge_closed_time":1455597422632,
        "Challenge_comment_count":0,
        "Challenge_created_time":1455545889500,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1456850010663,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35411741",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":28.87,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":14.3147588889,
        "Challenge_title":"Azure ML: Getting Error 503: NoMoreResources to any web service API even when I only make 1 request",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":283,
        "Challenge_word_count":272,
        "Platform":"Stack Overflow",
        "Poster_created_time":1425802890212,
        "Poster_location":"Boston, MA, USA",
        "Poster_reputation_count":41.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>This was an Azure problem. I quote the Microsoft guy, <\/p>\n\n<blockquote>\n  <p>We believe we have isolated the issue impacting tour service and we are currently working on a fix. We will be able to deploy this in the next couple of days. The problem is impacting only the ASIA AzureML region at this time, so if this is an option for you, might I suggest using a workspace in either the US or EU region until the fix gets rolled out here.<\/p>\n<\/blockquote>\n\n<p>To view the complete discussion, click <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/985e253e-5e54-45a5-a359-5c501152c445\/getting-error-503-nomoreresources-to-any-web-service-api-even-when-i-only-make-1-request?forum=MachineLearning&amp;prof=required\" rel=\"nofollow\">here<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":9.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":93.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":542.3006655556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Can I specify SageMaker estimator's entry point script to be in a subdirectory? So far, it fails for me. Here is what I want to do:<\/p>\n\n<pre><code>sklearn = SKLearn(\n    entry_point=\"RandomForest\/my_script.py\",\n    source_dir=\"..\/\",\n    hyperparameters={...\n<\/code><\/pre>\n\n<p>I want to do this so I don't have to break my directory structure. I have some modules, which I use in several sagemaker projects, and each project lives in its own directory:<\/p>\n\n<pre><code>my_git_repo\/\n\n  RandomForest\/\n    my_script.py\n    my_sagemaker_notebook.ipynb\n\n  TensorFlow\/\n    my_script.py\n    my_other_sagemaker_notebook.ipynb\n\nmodule_imported_in_both_scripts.py\n<\/code><\/pre>\n\n<p>If I try to run this, SageMaker fails because it seems to parse the name of the entry point script to make a module name out of it, and it does not do a good job:<\/p>\n\n<pre><code>\/usr\/bin\/python3 -m RandomForest\/my_script --bootstrap True --case nf_2 --max_features 0.5 --min_impurity_decrease 5.323785009485933e-06 --model_name model --n_estimators 455 --oob_score True\n\n...\n\n\/usr\/bin\/python3: No module named RandomForest\/my_script\n\n<\/code><\/pre>\n\n<p>Anyone knows a way around this other than putting <code>my_script.py<\/code> in the <code>source_dir<\/code>?<\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/54314876\/aws-sagemaker-sklearn-entry-point-allow-multiple-script\">Related to this question<\/a><\/p>",
        "Challenge_closed_time":1571939829096,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569987546700,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58194899",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.1,
        "Challenge_reading_time":18.16,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":542.3006655556,
        "Challenge_title":"AWS SageMaker SKLearn entry point in a subdirectory?",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":843,
        "Challenge_word_count":158,
        "Platform":"Stack Overflow",
        "Poster_created_time":1368039192832,
        "Poster_location":null,
        "Poster_reputation_count":1245.0,
        "Poster_view_count":109.0,
        "Solution_body":"<p>Unfortunately, this is a gap in functionality. There is some related work in <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/941\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/941<\/a> which should also solve this issue, but for now, you do need to put <code>my_script.py<\/code> in <code>source_dir<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":4.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1508797229168,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":1115.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":998.4904036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am training an image segmentation model on azure ML pipeline. During the testing step, I'm saving the output of the model to the associated blob storage. Then I want to find the IOU (Intersection over Union) between the calculated output and the ground truth. Both of these set of images lie on the blob storage. However, IOU calculation is extremely slow, and I think it's disk bound. In my IOU calculation code, I'm just loading the two images (commented out other code), still, it's taking close to 6 seconds per iteration, while training and testing were fast enough. <\/p>\n\n<p>Is this behavior normal? How do I debug this step?<\/p>",
        "Challenge_closed_time":1568735787296,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568713801943,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57971689",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":8.3,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":6.1070425,
        "Challenge_title":"Disk I\/O extremely slow on P100-NC6s-V2",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":415,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408145271463,
        "Poster_location":"India",
        "Poster_reputation_count":65.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>A few notes on the drives that an AzureML remote run has available:<\/p>\n\n<p>Here is what I see when I run <code>df<\/code> on a remote run (in this one, I am using a blob <code>Datastore<\/code> via <code>as_mount()<\/code>):<\/p>\n\n<pre><code>Filesystem                             1K-blocks     Used  Available Use% Mounted on\noverlay                                103080160 11530364   86290588  12% \/\ntmpfs                                      65536        0      65536   0% \/dev\ntmpfs                                    3568556        0    3568556   0% \/sys\/fs\/cgroup\n\/dev\/sdb1                              103080160 11530364   86290588  12% \/etc\/hosts\nshm                                      2097152        0    2097152   0% \/dev\/shm\n\/\/danielscstorageezoh...-620830f140ab 5368709120  3702848 5365006272   1% \/mnt\/batch\/tasks\/...\/workspacefilestore\nblobfuse                               103080160 11530364   86290588  12% \/mnt\/batch\/tasks\/...\/workspaceblobstore\n<\/code><\/pre>\n\n<p>The interesting items are <code>overlay<\/code>, <code>\/dev\/sdb1<\/code>, <code>\/\/danielscstorageezoh...-620830f140ab<\/code> and <code>blobfuse<\/code>:<\/p>\n\n<ol>\n<li><code>overlay<\/code> and <code>\/dev\/sdb1<\/code> are both the mount of the <strong>local SSD<\/strong> on the machine (I am using a STANDARD_D2_V2 which has a 100GB SSD).<\/li>\n<li><code>\/\/danielscstorageezoh...-620830f140ab<\/code> is the mount of the <strong>Azure File Share<\/strong> that contains the project files (your script, etc.). It is also the <em>current working directory<\/em> for your run.<\/li>\n<li><strong><code>blobfuse<\/code><\/strong> is the blob store that I had requested to mount in the <code>Estimator<\/code> as I executed the run.<\/li>\n<\/ol>\n\n<p>I was curious about the performance differences between these 3 types of drives. My mini benchmark was to download and extract this file: <a href=\"http:\/\/download.tensorflow.org\/example_images\/flower_photos.tgz\" rel=\"nofollow noreferrer\">http:\/\/download.tensorflow.org\/example_images\/flower_photos.tgz<\/a> (it is a 220 MB tar file that contains about 3600 jpeg images of flowers).<\/p>\n\n<p>Here the results:<\/p>\n\n<pre><code>Filesystem\/Drive         Download_and_save       Extract\nLocal_SSD                               2s            2s  \nAzure File Share                        9s          386s\nPremium File Share                     10s          120s\nBlobfuse                               10s          133s\nBlobfuse w\/ Premium Blob                8s          121s\n<\/code><\/pre>\n\n<p>In summary, writing small files is much, much slower on the network drives, so it is highly recommended to use \/tmp or Python <code>tempfile<\/code> if you are writing smaller files. <\/p>\n\n<p>For reference, here the script I ran to measure: <a href=\"https:\/\/gist.github.com\/danielsc\/9f062da5e66421d48ac5ed84aabf8535\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/danielsc\/9f062da5e66421d48ac5ed84aabf8535<\/a><\/p>\n\n<p>And this is how I ran it: <a href=\"https:\/\/gist.github.com\/danielsc\/6273a43c9b1790d82216bdaea6e10e5c\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/danielsc\/6273a43c9b1790d82216bdaea6e10e5c<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1572308367396,
        "Solution_link_count":6.0,
        "Solution_readability":11.2,
        "Solution_reading_time":34.57,
        "Solution_score_count":4.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":293.0,
        "Tool":"Azure Machine Learning"
    }
]