[
    {
        "Answerer_created_time":1384730587840,
        "Answerer_location":"France",
        "Answerer_reputation_count":717.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":71.1103577778,
        "Challenge_answer_count":1,
        "Challenge_body":"<pre><code>pins 1.0.1\nAzureStor 3.7.0\n<\/code><\/pre>\n<p>I'm getting this error<\/p>\n<pre><code>Error in withr::local_options(azure_storage_progress_bar = progress, .local_envir = env) : \n  unused argument (azure_storage_progress_bar = progress)\nCalls: %&gt;% ... pin_meta.pins_board_azure -&gt; azure_download -&gt; local_azure_progress\nExecution halted\n<\/code><\/pre>\n<p>when running <code>pin_read()<\/code> in the following code (<code>pin_list()<\/code> works fine)<\/p>\n<pre><code>bl_endp_key &lt;- storage_endpoint(endpoint = &lt;endpoint URL&gt;, key =&lt;endpoint key&gt;&quot;)\ncontainer &lt;- storage_container(endpoint = bl_endp_key, name = &lt;blob name&gt;)\nboard &lt;- board_azure(container = container, path = &quot;accidentsdata&quot;)\ncat(&quot;Testing pins:\\n&quot;)\nprint(board %&gt;% pin_list())\naccidents2 &lt;- board %&gt;% pins::pin_read('accidents') %&gt;% as_tibble()\n<\/code><\/pre>\n<p>My goal is to &quot;pin_read&quot; a dataset located on a Azure Blob Storage from an R script being run from <strong>pipelineJoB (YAML)<\/strong> including a <code>command: Rscript script.R ...<\/code> and an <code>environment:<\/code> based on a dockerfile installing <strong>R version 4.0.0<\/strong> (2020-04-24) -- &quot;Arbor Day&quot;<\/p>\n<p>The pipelineJob is being called from an Azure DevOps Pipeline task with <code>az ml job create &lt;pipelineJob YAML&gt; &lt;resource grp&gt; &lt;aml workspace name&gt;<\/code>.<\/p>\n<p>Note: the R script runs fine on my Windows RStudio desktop, with R version 4.1.3 (2022-03-10) -- &quot;One Push-Up&quot;.<\/p>\n<p>I've already tried with<\/p>\n<p><code>options(azure_storage_progress_bar=FALSE)<\/code> or<\/p>\n<p><code>withr::local_options(azure_storage_progress_bar=FALSE)<\/code><\/p>\n<p>but I'm getting the same <code>unused argument (azure_storage_progress_bar ...<\/code> error.<\/p>\n<p>FYI: <code>local_azure_progress<\/code> is defined here <a href=\"https:\/\/rdrr.io\/github\/rstudio\/pins\/src\/R\/board_azure.R#sym-local_azure_progress\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Challenge_closed_time":1656936659776,
        "Challenge_comment_count":1,
        "Challenge_created_time":1656349974407,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1656680662488,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72775967",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":27.76,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":162.9681580556,
        "Challenge_title":"R, pins and AzureStor: unused argument (azure_storage_progress_bar = progress)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":80,
        "Challenge_word_count":189,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384730587840,
        "Poster_location":"France",
        "Poster_reputation_count":717.0,
        "Poster_view_count":112.0,
        "Solution_body":"<p>Issue has been filed in <a href=\"https:\/\/github.com\/rstudio\/pins\/issues\/624\" rel=\"nofollow noreferrer\">pins<\/a>, it seems that is not an AzureStor issue.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":9.0,
        "Solution_reading_time":2.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":105.3469,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to build a binary classifier based on a tabular dataset that is rather sparse, but training is failing with the following message:<\/p>\n<blockquote>\n<p>Training pipeline failed with error message: Too few input rows passed validation. Of 1169548 inputs, 194 were valid. At least 50% of rows must pass validation.<\/p>\n<\/blockquote>\n<p>My understanding was that tabular AutoML should be able to handle Null values, so I'm not sure what's happening here, and I would appreciate any suggestions. The <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/tabular-data\/tabular101\" rel=\"nofollow noreferrer\">documentation<\/a> explicitly mentions reviewing each column's nullability, but I don't see any way to set or check a column's nullability on the dataset tab (perhaps the documentation is out of date?). Additionally, the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-types-tabular#what_values_are_treated_as_null_values\" rel=\"nofollow noreferrer\">documentation<\/a> explicitly mentions that missing values are treated as null, which is how I've set up my CSV. The <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-types-tabular#numeric\" rel=\"nofollow noreferrer\">documentation for numeric<\/a> however does not explicitly list support for missing values, just NaN and inf.<\/p>\n<p>The dataset is 1 million rows, 34 columns, and only 189 rows are null-free. My most sparse column has data in 5,000 unique rows, with the next rarest having data in 72k and 274k rows respectively. Columns are a mix of categorical and numeric, with only a handful of columns without nulls.<\/p>\n<p>The data is stored as a CSV, and the Dataset import seems to run without issue. Generate statistics ran on the dataset, but for some reason the missing % column failed to populate. What might be the best way to address this? I'm not sure if this is a case where I need to change my null representation in the CSV, change some dataset\/training setting, or if its an AutoML bug (less likely). Thanks!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bF1Nn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bF1Nn.png\" alt=\"Image of missing % column being blank\" \/><\/a><\/p>",
        "Challenge_closed_time":1657366819400,
        "Challenge_comment_count":3,
        "Challenge_created_time":1656554846450,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1656987570560,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72809603",
        "Challenge_link_count":5,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":11.2,
        "Challenge_reading_time":28.6,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":225.5480416667,
        "Challenge_title":"VertexAI Tabular AutoML rejecting rows containing nulls",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":122,
        "Challenge_word_count":300,
        "Platform":"Stack Overflow",
        "Poster_created_time":1614739500312,
        "Poster_location":null,
        "Poster_reputation_count":51.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>To allow invalid &amp; null values during training &amp; prediction, we have to explicitly set the <code>allow invalid values<\/code> flag to <code>Yes<\/code> during training as shown in the image below. You can find this setting under model training settings on the dataset page. The flag has to be set on a column by column basis.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jWDGm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jWDGm.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"set allow valu flag train column column basi allow null valu train predict set model train set dataset page",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"allow null valu train predict explicitli set allow valu flag train shown imag set model train set dataset page flag set column column basi",
        "Solution_preprocessed_content":"allow null valu train predict explicitli set flag train shown imag set model train set dataset page flag set column column basi",
        "Solution_readability":7.5,
        "Solution_reading_time":6.49,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":65.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1468951834403,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation_count":645.0,
        "Answerer_view_count":101.0,
        "Challenge_adjusted_solved_time":33.9211666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am deploying a machine learning image to Azure Container Instances from Azure Machine Learning services according to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml\" rel=\"nofollow noreferrer\">this article<\/a>, but am always stuck with the error message:<\/p>\n\n<blockquote>\n  <p>Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.<br>\n  Please check the logs for your container instance xxxxxxx'.<\/p>\n<\/blockquote>\n\n<p>I tried:<\/p>\n\n<ol>\n<li>increasing memory_gb=4 in aci_config.<\/li>\n<li>I did\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-troubleshoot-deployment#debug-the-docker-image-locally\" rel=\"nofollow noreferrer\">troubleshooting<\/a> locally, but I could not have found any.<\/li>\n<\/ol>\n\n<p>Below is my score.py<\/p>\n\n<pre><code>def init():\n    global model\n    model_path = Model.get_model_path('pofc_fc_model')\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    data = np.array(json.loads(raw_data)['data'])\n    y_hat = model.predict(data)\n    return y_hat.tolist()\n<\/code><\/pre>",
        "Challenge_closed_time":1553715289510,
        "Challenge_comment_count":0,
        "Challenge_created_time":1553593173310,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1562618473092,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55353889",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":15.85,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":33.9211666667,
        "Challenge_title":"Azure container instances deployment failed",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3020,
        "Challenge_word_count":110,
        "Platform":"Stack Overflow",
        "Poster_created_time":1510960409296,
        "Poster_location":"Bangkok Thailand",
        "Poster_reputation_count":306.0,
        "Poster_view_count":32.0,
        "Solution_body":"<p>Have you registered the model <code>'pofc_fc_model'<\/code> in your workspace using the <code>register()<\/code> function on the model object? If not, there will be no model path and that can cause failure.<\/p>\n\n<p>See this section on model registration: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#registermodel\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#registermodel<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"regist model pofc model workspac regist function model object instruct section model registr document",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"regist model pofc model workspac regist function model object model path section model registr http doc com servic deploi registermodel",
        "Solution_preprocessed_content":"regist model workspac function model object model path section model registr",
        "Solution_readability":17.8,
        "Solution_reading_time":6.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":103.6265825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have started working with <strong>AWS SageMaker<\/strong> recently with the examples provided by AWS. I used this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/deepar_electricity\/DeepAR-Electricity.ipynb\" rel=\"nofollow noreferrer\">example<\/a> (<strong>DeepAR<\/strong> Model) in order to forecast a time series. After training, a model artifacts file has been created in my S3 bucket. <\/p>\n\n<p><strong>My question:<\/strong> Is there a way to host that trained model in a own hosting environment? (client premises)<\/p>",
        "Challenge_closed_time":1561921541280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561548485583,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56771758",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":8.49,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":103.6265825,
        "Challenge_title":"On-Premises Hosting of Trained DeepAR Model built on AWS SageMaker",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":379,
        "Challenge_word_count":71,
        "Platform":"Stack Overflow",
        "Poster_created_time":1439993560103,
        "Poster_location":"Lebanon",
        "Poster_reputation_count":155.0,
        "Poster_view_count":76.0,
        "Solution_body":"<p>Except SageMaker XGBoost, SageMaker built-in algorithms are not designed to be used out of Amazon. That does not mean that it's impossible, for example you can find here and there snippets peeking inside model artifacts (eg for <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations\/\" rel=\"nofollow noreferrer\">Factorization Machines<\/a> and <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/ntm_20newsgroups_topic_modeling\/ntm_20newsgroups_topic_model.ipynb\" rel=\"nofollow noreferrer\">Neural Topic Model<\/a>) but these things can be hacky and are usually not part of official service features. Regarding DeepAR specifically, the model was open-sourced couple weeks ago as part of <code>gluon-ts<\/code> python package (<a href=\"https:\/\/aws.amazon.com\/blogs\/opensource\/gluon-time-series-open-source-time-series-modeling-toolkit\/\" rel=\"nofollow noreferrer\">blog post<\/a>, <a href=\"https:\/\/gluon-ts.mxnet.io\/\" rel=\"nofollow noreferrer\">code<\/a>) so if you develop a model specifically for your own hosting environment I'd recommend to use that gluon-ts code in the MXNet container, so that you'll be able to open and read the artifact out of SageMaker.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"deepar model host host environ open sourc deepar model gluon packag model host environ gluon mxnet open read artifact",
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_original_content":"built algorithm design imposs peek insid model artifact factor neural model hacki offici servic featur deepar model open sourc coupl week ago gluon packag blog model host environ gluon mxnet open read artifact",
        "Solution_preprocessed_content":"algorithm design imposs peek insid model artifact hacki offici servic featur deepar model coupl week ago packag model host environ mxnet open read artifact",
        "Solution_readability":19.8,
        "Solution_reading_time":17.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":126.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1386279656143,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":1352.0,
        "Answerer_view_count":229.0,
        "Challenge_adjusted_solved_time":1.7925027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose we got some database (any database, that supports csv dumping), collecting raw data in real time for further usage in ML.\nOn the other side, we got DVC, that can work with csv files.<\/p>\n<p>I want to organize a scheduled run of stored SELECT to that DB with datetime parameters (and also support a manual run), to make a new csv files, and send them to DVC.<\/p>\n<p>In DVC documentation and examples I found, csv file already exists.<\/p>\n<p>Can I make this interaction with database with DVC itself, or I got something wrong, and there is a separate tool for csv dump?<\/p>",
        "Challenge_closed_time":1619081803750,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619078487020,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67209146",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":7.43,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.9213138889,
        "Challenge_title":"DVC - make scheduled csv dumps",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":65,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1580932981007,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>There are 3 steps in this process:<\/p>\n<ol>\n<li>Create a CSV dump. Many DBs have these tools but DVC does not support this natively.<\/li>\n<li>Version the CSV dump and move it to some storage. DVC does this job.<\/li>\n<li>Schedule periodical dump. You can use Cron (easy), AirFlow (not easy) or <a href=\"https:\/\/docs.github.com\/en\/actions\/reference\/events-that-trigger-workflows\" rel=\"nofollow noreferrer\">periodical jobs in GitHub Actions<\/a>\/<a href=\"https:\/\/docs.gitlab.com\/ee\/ci\/pipelines\/schedules.html\" rel=\"nofollow noreferrer\">GitLab CI\/CD<\/a>. Another project from the DVC team can help with CI\/CD option: <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">https:\/\/cml.dev<\/a>.<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":1619084940030,
        "Solution_link_count":4.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":9.9,
        "Solution_reading_time":9.14,
        "Solution_score_count":4.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":78.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":6.7239763889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created an AKS cluster using Azure Machine Learning SDK extension and I attached to the workspace created. When the cluster is created and attached, I doesn't show any error. When I am trying to detach it from workspace, it is not accepting the operations.<\/p>\n<p>I would like to detach the existing AKS cluster from workspace either by program manner, using CLI or even using Azure portal.<\/p>",
        "Challenge_closed_time":1659333006088,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659308799773,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73187536",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":5.71,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":6.7239763889,
        "Challenge_title":"Error while detaching AKS cluster through Azure ML SDK extension",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":47,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651093614703,
        "Poster_location":"Netherland",
        "Poster_reputation_count":19.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>If we are using any <strong>extensions of SDK or Azure CLI<\/strong> for machine learning to detach AKS cluster, it <strong>will not work<\/strong> and it will not get deleted or detached. Instead, we need to use <strong>Azure CLI with AKS<\/strong>. There are two types of implementations we can perform.<\/p>\n<p><strong>Python:<\/strong><\/p>\n<pre><code>Aks_target.detach()\n<\/code><\/pre>\n<p><strong>Azure CLI:<\/strong><\/p>\n<p>Before performing this step, we need to get the details of the working AKS cluster name attached to our workspace. Resource Group details and workspace name<\/p>\n<pre><code>az ml computertarget detach -n youraksname -g yourresourcegroup -w yourworkspacename\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"detach ak cluster workspac creat sdk extens programmat cli extens cli ak ak target detach cli computertarget detach youraksnam yourresourcegroup yourworkspacenam ak cluster attach workspac resourc group workspac",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"extens sdk cli detach ak cluster delet detach cli ak type implement perform ak target detach cli perform step ak cluster attach workspac resourc group workspac computertarget detach youraksnam yourresourcegroup yourworkspacenam",
        "Solution_preprocessed_content":"extens sdk cli detach ak cluster delet detach cli ak type implement perform cli perform step ak cluster attach workspac resourc group workspac",
        "Solution_readability":8.0,
        "Solution_reading_time":8.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1374169767267,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":548.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":1035.8951980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>with <a href=\"https:\/\/github.com\/svpino\/tensorflow-object-detection-sagemaker\" rel=\"nofollow noreferrer\">this<\/a> I successfully created a training job on sagemaker using the Tensorflow Object Detection API in a docker container. Now I'd like to monitor the training job using sagemaker, but cannot find anything explaining how to do it. I don't use a sagemaker notebook.\nI think I can do it by saving the logs into a S3 bucket and point there a local tensorboard instance .. but don't know how to tell the tensorflow object detection API where to save the logs (is there any command line argument for this ?).\nSomething like <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/keras_script_mode_pipe_mode_horovod\/tensorflow_keras_CIFAR10.ipynb\" rel=\"nofollow noreferrer\">this<\/a>, but the script <code>generate_tensorboard_command.py<\/code> fails because my training job don't have the <code>sagemaker_submit_directory<\/code> parameter..<\/p>\n<p>The fact is when I start the training job nothing is created on my s3 until the job finish and upload everything. There should be a way tell tensorflow where to save the logs (s3) during the training, hopefully without modifying the API source code..<\/p>\n<p><strong>Edit<\/strong><\/p>\n<p>I can finally make it works with the accepted solution (tensorflow natively supports read\/write to s3), there are however additional steps to do:<\/p>\n<ol>\n<li>Disable network isolation in the training job configuration<\/li>\n<li>Provide credentials to the docker image to write to S3 bucket<\/li>\n<\/ol>\n<p>The only thing is that Tensorflow continuously polls filesystem (i.e. looking for an updated model in serving mode) and this cause useless requests to S3, that you will have to pay (together with a buch of errors in the console). I opened a new question <a href=\"https:\/\/stackoverflow.com\/q\/64969198\/4267439\">here<\/a> for this. At least it works.<\/p>\n<p><strong>Edit 2<\/strong><\/p>\n<p>I was wrong, TF just write logs, is not polling so it's an expected behavior and the extra costs are minimal.<\/p>",
        "Challenge_closed_time":1594137982056,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590408759343,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1606323198012,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62002183",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":27.16,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1035.8951980556,
        "Challenge_title":"Use tensorboard with object detection API in sagemaker",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":311,
        "Challenge_word_count":286,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416346350292,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation_count":2302.0,
        "Poster_view_count":227.0,
        "Solution_body":"<p>Looking through the example you posted, it looks as though the <code>model_dir<\/code> passed to the TensorFlow Object Detection package is configured to <code>\/opt\/ml\/model<\/code>:<\/p>\n<pre><code># These are the paths to where SageMaker mounts interesting things in your container.\nprefix = '\/opt\/ml\/'\ninput_path = os.path.join(prefix, 'input\/data')\noutput_path = os.path.join(prefix, 'output')\nmodel_path = os.path.join(prefix, 'model')\nparam_path = os.path.join(prefix, 'input\/config\/hyperparameters.json')\n<\/code><\/pre>\n<p>During the training process, tensorboard logs will be written to <code>\/opt\/ml\/model<\/code>, and then uploaded to s3 as a final model artifact AFTER training: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html<\/a>.<\/p>\n<p>You <em>might<\/em> be able to side-step the SageMaker artifact upload step and point the <code>model_dir<\/code> of TensorFlow Object Detection API directly at an s3 location during training:<\/p>\n<pre><code>model_path = &quot;s3:\/\/your-bucket\/path\/here\n<\/code><\/pre>\n<p>This means that the TensorFlow library within the SageMaker job is directly writing to S3 instead of the filesystem inside of it's container. Assuming the underlying TensorFlow Object Detection code can write directly to S3 (something you'll have to verify), you should be able to see the tensorboard logs and checkpoints there in realtime.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"configur model dir pass tensorflow object detect packag opt model tensorboard log written train process upload final model artifact train model dir tensorflow object detect api directli locat train set model path bucket path tensorflow librari job directli write filesystem insid underli tensorflow object detect write directli tensorboard log checkpoint time",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"model dir pass tensorflow object detect packag configur opt model path mount prefix opt input path path prefix input data output path path prefix output model path path prefix model param path path prefix input config hyperparamet json train process tensorboard log written opt model upload final model artifact train http doc com latest algorithm train algo envvari html step artifact upload step model dir tensorflow object detect api directli locat train model path bucket path tensorflow librari job directli write filesystem insid underli tensorflow object detect write directli verifi tensorboard log checkpoint realtim",
        "Solution_preprocessed_content":"pass tensorflow object detect packag configur train process tensorboard log written upload final model artifact train artifact upload step tensorflow object detect api directli locat train tensorflow librari job directli write filesystem insid underli tensorflow object detect write directli tensorboard log checkpoint realtim",
        "Solution_readability":16.2,
        "Solution_reading_time":20.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":163.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":30.0886975,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Im trying to learn Azure, with little luck (yet). All the tutorials show using PipelineData just as a file, when configured in &quot;upload&quot; mode. However, im getting &quot;FileNotFoundError: [Errno 2] No such file or directory: ''&quot; error. I would love to ask a more specific question, but i just can't see what im doing wrong.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore,Dataset,Environment\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.pipeline.steps import PythonScriptStep\nfrom azureml.pipeline.core import Pipeline, PipelineData\nimport os\n\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n\ncompute_name = &quot;cpucluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=compute_name)\naml_run_config = RunConfiguration()\naml_run_config.target = compute_target\naml_run_config.environment.python.user_managed_dependencies = False\naml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['pandas','scikit-learn'], \n    pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]'], \n    pin_sdk_version=False)\n\noutput1 = PipelineData(&quot;processed_data1&quot;,datastore=datastore, output_mode=&quot;upload&quot;)\nprep_step = PythonScriptStep(\n    name=&quot;dataprep&quot;,\n    script_name=&quot;dataprep.py&quot;,\n    source_directory=os.path.join(os.getcwd(),'dataprep'),\n    arguments=[&quot;--output&quot;, output1],\n    outputs = [output1],\n    compute_target=compute_target,\n    runconfig=aml_run_config,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>In the dataprep.py i hve the following:<\/p>\n<pre><code>import numpy, argparse, pandas\nfrom azureml.core import Run\nrun = Run.get_context()\nparser = argparse.ArgumentParser()\nparser.add_argument('--output', dest='output', required=True)\nargs = parser.parse_args()\ndf = pandas.DataFrame(numpy.random.rand(100,3))\ndf.iloc[:, 2] = df.iloc[:,0] + df.iloc[:,1]\nprint(df.iloc[:5,:])\ndf.to_csv(args.output)\n\n<\/code><\/pre>\n<p>So, yeah. pd is supposed to write to the output, but my compute cluster says the following:<\/p>\n<pre><code>&quot;User program failed with FileNotFoundError: [Errno 2] No such file or directory: ''\\&quot;.\n<\/code><\/pre>\n<p>When i dont include the to_csv() function, the cluster does not complain<\/p>",
        "Challenge_closed_time":1626667519372,
        "Challenge_comment_count":2,
        "Challenge_created_time":1626541888290,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1626559748576,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68422680",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":15.6,
        "Challenge_reading_time":32.96,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":34.8975227778,
        "Challenge_title":"how to write to Azure PipelineData properly?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":404,
        "Challenge_word_count":207,
        "Platform":"Stack Overflow",
        "Poster_created_time":1588424911652,
        "Poster_location":null,
        "Poster_reputation_count":59.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Here is an <a href=\"https:\/\/github.com\/james-tn\/highperformance_python_in_azure\/blob\/master\/parallel_python_processing\/pipeline_definition.ipynb\" rel=\"nofollow noreferrer\">example<\/a> for PRS.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipelinedata?view=azure-ml-py\" rel=\"nofollow noreferrer\">PipelineData<\/a> was intended to represent &quot;transient&quot; data from one step to the next one, while OutputDatasetConfig was intended for capturing the final state of a dataset (and hence why you see features like lineage, ADLS support, etc). PipelineData always outputs data in a folder structure like {run_id}{output_name}. OutputDatasetConfig allows to decouple the data from the run and hence it allows you to control where to land the data (although by default it will produce similar folder structure). The OutputDatasetConfig allows even to register the output as a Dataset, where getting rid of such folder structure makes sense. From the docs itself: &quot;Represent how to copy the output of a run and be promoted as a FileDataset. The OutputFileDatasetConfig allows you to specify how you want a particular local path on the compute target to be uploaded to the specified destination&quot;.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-batch-scoring-classification#create-dataset-objects\" rel=\"nofollow noreferrer\">OutFileDatasetConfig<\/a> is a control plane concept to pass data between pipeline steps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"outputdatasetconfig pipelinedata captur final state dataset outputdatasetconfig allow control land data regist output dataset outfiledatasetconfig control plane concept pass data pipelin step",
        "Solution_last_edit_time":1626668067887,
        "Solution_link_count":3.0,
        "Solution_original_content":"pr pipelinedata intend repres transient data step outputdatasetconfig intend captur final state dataset featur lineag adl pipelinedata output data folder structur run output outputdatasetconfig allow decoupl data run allow control land data default produc folder structur outputdatasetconfig allow regist output dataset rid folder structur sens doc repres copi output run promot filedataset outputfiledatasetconfig allow specifi local path comput target upload specifi destin outfiledatasetconfig control plane concept pass data pipelin step",
        "Solution_preprocessed_content":"pr pipelinedata intend repres transient data step outputdatasetconfig intend captur final state dataset pipelinedata output data folder structur outputdatasetconfig allow decoupl data run allow control land data outputdatasetconfig allow regist output dataset rid folder structur sens doc repres copi output run promot filedataset outputfiledatasetconfig allow specifi local path comput target upload specifi destin outfiledatasetconfig control plane concept pass data pipelin step",
        "Solution_readability":14.5,
        "Solution_reading_time":19.88,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":167.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":51.1702544445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a model with VPC, Private subnet, and appropriate security group. The endpoint URL can, however, be reached through the internet though failing due to the lack of security token<\/p>\n\n<p>Things I need clarification on now are<\/p>\n\n<ol>\n<li>Is there a way to avoid the URL being accessible from the internet<\/li>\n<li>Are we not charged for requests failed on AUTH(like for API Gateway)<\/li>\n<li>Does that make our deployment vulnerable to any attacks<\/li>\n<\/ol>",
        "Challenge_closed_time":1550250221603,
        "Challenge_comment_count":0,
        "Challenge_created_time":1550066008687,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54671841",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":6.65,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":51.1702544445,
        "Challenge_title":"Sagemaker endpoint(with VPC) url accessible from internet",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":679,
        "Challenge_word_count":81,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320419229500,
        "Poster_location":null,
        "Poster_reputation_count":4924.0,
        "Poster_view_count":358.0,
        "Solution_body":"<p>You are not hitting your endpoint, but the endpoint of AWS SageMaker runtime. This endpoint is checking all the permissions to access your hosted model, and only if the credentials and requirements are met, the request is forwarded to your instances and models. <\/p>\n\n<p>Therefore, you can't prevent this URL from being accessible from the Internet, but at the same time, you don't need to protect it or pay for it. AWS has a high level of security on these endpoints, and I don't think that you have a more secure way to protect these endpoints. <\/p>",
        "Solution_comment_count":4.0,
        "Solution_gpt_summary":"endpoint runtim permiss access host model credenti met request forward instanc model high level secur endpoint protect pai prevent internet access url auth request charg",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"hit endpoint endpoint runtim endpoint permiss access host model credenti met request forward instanc model prevent url access internet time protect pai high level secur endpoint secur protect endpoint",
        "Solution_preprocessed_content":"hit endpoint endpoint runtim endpoint permiss access host model credenti met request forward instanc model prevent url access internet time protect pai high level secur endpoint secur protect endpoint",
        "Solution_readability":10.4,
        "Solution_reading_time":6.71,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":97.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":16.7924258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When running locally, my Jupyter notebook is able to reference Google BigQuery like so:<\/p>\n\n<pre><code>%%bigquery some_bq_table\n\nSELECT *\nFROM\n  `some_bq_dataset.some_bq_table` \n<\/code><\/pre>\n\n<p>So that later in my notebook I can reference some_bq_table as a pandas dataframe, as exemplified here: <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter<\/a><\/p>\n\n<p>I want to run my notebook on AWS SageMaker to test a few things. To authenticate with BigQuery it seems that the only two ways are using a service account on GCP (or locally) or pointing the the SDK to a credentials JSON using an env var (as explained here: <a href=\"https:\/\/cloud.google.com\/docs\/authentication\/getting-started\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/docs\/authentication\/getting-started<\/a>).<\/p>\n\n<p>For example<\/p>\n\n<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"\/home\/user\/Downloads\/[FILE_NAME].json\"\n<\/code><\/pre>\n\n<p>Is there an easy way to connect to bigquery from SageMaker? My best idea right now is to download the JSON from somewhere to the SageMaker instnace and then set the env var from the python code.<\/p>\n\n<p>For example, I would do this:<\/p>\n\n<pre><code>os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"\/home\/user\/Downloads\/[FILE_NAME].json\"\n<\/code><\/pre>\n\n<p>However, this isn't very secure - I don't like the idea of downloading my credentials JSON to a SageMaker instance (this means I would have to upload the credentials to some private s3 bucket and then store them on the SageMaker instance). Not the end of the world but I rather avoid this. <\/p>\n\n<p>Any ideas?<\/p>",
        "Challenge_closed_time":1554514774276,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554454321543,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55531608",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":22.17,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":16.7924258333,
        "Challenge_title":"Accessing Google BigQuery from AWS SageMaker",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1078,
        "Challenge_word_count":206,
        "Platform":"Stack Overflow",
        "Poster_created_time":1372517243827,
        "Poster_location":null,
        "Poster_reputation_count":5921.0,
        "Poster_view_count":388.0,
        "Solution_body":"<p>As you mentioned GCP currently authenticates using service account, credentials JSON and API tokens. Instead of storing credentials in S3 bucket you can consider using AWS Secrets Manager or AWS Systems Manager Parameter Store to store the GCP credentials and then fetch them in Jupyter notebook. This way credentials can be secured and the credentials file will be created from Secrets Manager only when needed. <\/p>\n\n<p>This is sample code I used previously to connect to BigQuery from SageMaker instance.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nimport boto3\nfrom google.cloud.bigquery import magics\nfrom google.oauth2 import service_account\n\ndef get_gcp_credentials_from_ssm(param_name):\n    # read credentials from SSM parameter store\n    ssm = boto3.client('ssm')\n    # Get the requested parameter\n    response = ssm.get_parameters(Names=[param_name], WithDecryption=True)\n    # Store the credentials in a variable\n    gcp_credentials = response['Parameters'][0]['Value']\n    # save credentials temporarily to a file\n    credentials_file = '\/tmp\/.gcp\/service_credentials.json'\n    with open(credentials_file, 'w') as outfile:  \n        json.dump(json.loads(gcp_credentials), outfile)\n    # create google.auth.credentials.Credentials to use for queries \n    credentials = service_account.Credentials.from_service_account_file(credentials_file)\n    # remove temporary file\n    if os.path.exists(credentials_file):\n        os.remove(credentials_file)\n    return credentials\n\n# this will set the context credentials to use for queries performed in jupyter \n# using bigquery cell magic\nmagics.context.credentials = get_gcp_credentials_from_ssm('my_gcp_credentials')\n<\/code><\/pre>\n\n<p>Please note that SageMaker execution role should have access to SSM and of course other necessary route to connect to GCP. I am not sure if this is the best way though. Hope someone has better way.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"secret system paramet store store credenti secur fetch jupyt notebook sampl connect bigqueri instanc read credenti ssm paramet store save temporarili file creat auth credenti credenti queri remov temporari file execut role access ssm rout connect",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"authent servic account credenti json api token store credenti bucket secret system paramet store store credenti fetch jupyt notebook credenti secur credenti file creat secret sampl previous connect bigqueri instanc import import json import boto cloud bigqueri import magic oauth import servic account credenti ssm param read credenti ssm paramet store ssm boto client ssm request paramet respons ssm paramet param withdecrypt store credenti variabl credenti respons paramet valu save credenti temporarili file credenti file tmp servic credenti json open credenti file outfil json dump json load credenti outfil creat auth credenti credenti queri credenti servic account credenti servic account file credenti file remov temporari file path credenti file remov credenti file return credenti set context credenti queri perform jupyt bigqueri cell magic magic context credenti credenti ssm credenti note execut role access ssm cours rout connect hope",
        "Solution_preprocessed_content":"authent servic account credenti json api token store credenti bucket secret system paramet store store credenti fetch jupyt notebook credenti secur credenti file creat secret sampl previous connect bigqueri instanc note execut role access ssm cours rout connect hope",
        "Solution_readability":14.0,
        "Solution_reading_time":23.8,
        "Solution_score_count":7.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":209.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1411212343683,
        "Answerer_location":"Vancouver, BC, Canada",
        "Answerer_reputation_count":3592.0,
        "Answerer_view_count":268.0,
        "Challenge_adjusted_solved_time":1.5132372222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've uploaded my own Jupyter notebook to Sagemaker, and am trying to create an iterator for my training \/ validation data which is in S3, as follow:<\/p>\n\n<pre><code>train = mx.io.ImageRecordIter(\n        path_imgrec         = \u2018s3:\/\/bucket-name\/train.rec\u2019 \u2026\u2026 )\n<\/code><\/pre>\n\n<p>I receive the following exception: <\/p>\n\n<pre><code>MXNetError: [04:33:32] src\/io\/s3_filesys.cc:899: Need to set enviroment variable AWS_SECRET_ACCESS_KEY to use S3\n<\/code><\/pre>\n\n<p>I've checked that the IAM role attached with this notebook instance has S3 access. Any clues on what might be needed to fix this?<\/p>",
        "Challenge_closed_time":1522305475727,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522300028073,
        "Challenge_favorite_count":3,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49548422",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":7.79,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.5132372222,
        "Challenge_title":"Training data in S3 in AWS Sagemaker",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":1353,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449031669083,
        "Poster_location":null,
        "Poster_reputation_count":777.0,
        "Poster_view_count":103.0,
        "Solution_body":"<p>If your IAM roles are setup correctly, then you need to download the file to the Sagemaker instance first and then work on it. Here's how:<\/p>\n\n<pre><code># Import roles\nimport sagemaker\nrole = sagemaker.get_execution_role()\n\n# Download file locally\ns3 = boto3.resource('s3')\ns3.Bucket(bucket).download_file('your_training_s3_file.rec', 'training.rec')\n\n#Access locally\ntrain = mx.io.ImageRecordIter(path_imgrec=\u2018training.rec\u2019 \u2026\u2026 )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"download file instanc import role download file local access local download file",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"iam role setup download file instanc import role import role execut role download file local boto resourc bucket bucket download file train file rec train rec access local train imagerecordit path imgrec train rec",
        "Solution_preprocessed_content":"iam role setup download file instanc",
        "Solution_readability":13.6,
        "Solution_reading_time":5.8,
        "Solution_score_count":4.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1581378257476,
        "Answerer_location":null,
        "Answerer_reputation_count":131.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":207.8509333333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to generate prediction intervals for time series forecasts when using a Azure AutoML trained models? Could we get the training errors out of the process and use them for bootstrapping?<\/p>",
        "Challenge_closed_time":1618942132740,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618193869380,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67051900",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":3.57,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":207.8509333333,
        "Challenge_title":"How can I generate prediction intervals for Azure AutoML timeseries forecasts?",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":205,
        "Challenge_word_count":43,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324351066392,
        "Poster_location":"Christchurch, New Zealand",
        "Poster_reputation_count":1306.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>You can generate forecast quantiles. See the following notebook for more details: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-forecast-function\/auto-ml-forecasting-function.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-forecast-function\/auto-ml-forecasting-function.ipynb<\/a><\/p>",
        "Solution_comment_count":4.0,
        "Solution_gpt_summary":"gener forecast quantil notebook gener forecast quantil",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"gener forecast quantil notebook http github com machinelearningnotebook blob master autom forecast forecast function auto forecast function ipynb",
        "Solution_preprocessed_content":"gener forecast quantil notebook",
        "Solution_readability":64.4,
        "Solution_reading_time":6.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1369156710532,
        "Answerer_location":"Japan",
        "Answerer_reputation_count":189.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":708.9712822222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Optuna 2.5 to optimize a couple of hyperparameters on a tf.keras CNN model. I want to use pruning so that the optimization skips the less promising corners of the hyperparameters space. I'm using something like this:<\/p>\n<pre><code>study0 = optuna.create_study(study_name=study_name,\n                             storage=storage_name,\n                             direction='minimize', \n                             sampler=TPESampler(n_startup_trials=25, multivariate=True, seed=123),\n                             pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource='auto',\n                             reduction_factor=4, min_early_stopping_rate=0),\n                             load_if_exists=True)\n<\/code><\/pre>\n<p>Sometimes the model stops after 2 epochs, some other times it stops after 12 epochs, 48 and so forth. What I want is to ensure that the model always trains at least 30 epochs before being pruned. I guess that the parameter <code>min_early_stopping_rate<\/code> might have some control on this but I've tried to change it from 0 to 30 and then  the models never get pruned. Can someone explain me a bit better than the Optuna documentation, what these parameters in the  <code>SuccessiveHalvingPruner()<\/code> really do (specially <code>min_early_stopping_rate<\/code>)?\nThanks<\/p>",
        "Challenge_closed_time":1616058414416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613506117800,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66231467",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":15.66,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":708.9712822222,
        "Challenge_title":"How to set a minimum number of epoch in Optuna SuccessiveHalvingPruner()?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":568,
        "Challenge_word_count":148,
        "Platform":"Stack Overflow",
        "Poster_created_time":1556636382232,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p><code>min_resource<\/code>'s explanation on <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.pruners.SuccessiveHalvingPruner.html\" rel=\"nofollow noreferrer\">the documentation<\/a> says<\/p>\n<blockquote>\n<p>A trial is never pruned until it executes <code>min_resource * reduction_factor ** min_early_stopping_rate<\/code> steps.<\/p>\n<\/blockquote>\n<p>So, I suppose that we need to replace the value of <code>min_resource<\/code> with a specific number depending on <code>reduction_factor<\/code> and <code>min_early_stopping_rate<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"replac valu resourc depend reduct factor earli stop rate explan document valu formula calcul valu resourc",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"resourc explan document sai trial prune execut resourc reduct factor earli stop rate step suppos replac valu resourc depend reduct factor earli stop rate",
        "Solution_preprocessed_content":"explan document sai trial prune execut step suppos replac valu depend",
        "Solution_readability":22.0,
        "Solution_reading_time":7.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":44.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1553088438367,
        "Answerer_location":"Tehran, Tehran Province, Iran",
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":3103.0390416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am a beginner in mlflow and was trying to set it up locally using Anaconda 3.\nI have created a new environment in anaconda and install mlflow and sklearn in it. Now I am using jupyter notebook to run my sample code for mlflow.<\/p>\n<p>'''<\/p>\n<pre><code>import os\nimport warnings\nimport sys\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nfrom urllib.parse import urlparse\nimport mlflow\nimport mlflow.sklearn\n\nimport logging\n\nlogging.basicConfig(level=logging.WARN)\nlogger = logging.getLogger(__name__)\n\nwarnings.filterwarnings(&quot;ignore&quot;)\nnp.random.seed(40)\n\n\nmlflow.set_tracking_uri(&quot;file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun&quot;)\n\nmlflow.get_tracking_uri()\n\nmlflow.get_experiment\n\n#experiment_id = mlflow.create_experiment(&quot;Mlflow_demo&quot;)\nexperiment_id = mlflow.create_experiment(&quot;Demo3&quot;)\nexperiment = mlflow.get_experiment(experiment_id)\nprint(&quot;Name: {}&quot;.format(experiment.name))\nprint(&quot;Experiment_id: {}&quot;.format(experiment.experiment_id))\nprint(&quot;Artifact Location: {}&quot;.format(experiment.artifact_location))\nprint(&quot;Tags: {}&quot;.format(experiment.tags))\nprint(&quot;Lifecycle_stage: {}&quot;.format(experiment.lifecycle_stage))\n\nmlflow.set_experiment(&quot;Demo3&quot;)\n\ndef eval_metrics(actual, pred):\n    rmse = np.sqrt(mean_squared_error(actual, pred))\n    mae = mean_absolute_error(actual, pred)\n    r2 = r2_score(actual, pred)\n    return rmse, mae, r2\n\n# Read the wine-quality csv file from the URL\ncsv_url =\\\n    'http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine-quality\/winequality-red.csv'\ntry:\n    data = pd.read_csv(csv_url, sep=';')\nexcept Exception as e:\n    logger.exception(\n        &quot;Unable to download training &amp; test CSV, check your internet connection. Error: %s&quot;, e)\n\ndata.head(2)\n\n\ndef train_model(data, alpha, l1_ratio):\n    \n    # Split the data into training and test sets. (0.75, 0.25) split.\n    train, test = train_test_split(data)\n\n    # The predicted column is &quot;quality&quot; which is a scalar from [3, 9]\n    train_x = train.drop([&quot;quality&quot;], axis=1)\n    test_x = test.drop([&quot;quality&quot;], axis=1)\n    train_y = train[[&quot;quality&quot;]]\n    test_y = test[[&quot;quality&quot;]]\n\n    # Set default values if no alpha is provided\n    alpha = alpha\n    l1_ratio = l1_ratio\n\n\n    # Execute ElasticNet\n    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    lr.fit(train_x, train_y)\n\n    # Evaluate Metrics\n    predicted_qualities = lr.predict(test_x)\n    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n    # Print out metrics\n    print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))\n    print(&quot;  RMSE: %s&quot; % rmse)\n    print(&quot;  MAE: %s&quot; % mae)\n    print(&quot;  R2: %s&quot; % r2)\n    \n    # Log parameter, metrics, and model to MLflow\n    with mlflow.start_run(experiment_id = experiment_id):\n        mlflow.log_param(&quot;alpha&quot;, alpha)\n        mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)\n        mlflow.log_metric(&quot;rmse&quot;, rmse)\n        mlflow.log_metric(&quot;r2&quot;, r2)\n        mlflow.log_metric(&quot;mae&quot;, mae)\n        mlflow.sklearn.log_model(lr, &quot;model&quot;)\n        \n\ntrain_model(data, 0.5, 0.5)\n\ntrain_model(data, 0.5, 0.3)\n\ntrain_model(data, 0.4, 0.3)\n<\/code><\/pre>\n<p>'''<\/p>\n<p>using above code, I am successfully able to create 3 different experiment as I can see the folders created in my local directory as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jKqgX.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Now, I am trying to run the mlflow ui using the jupyter terminal in my chrome browser and I am able to open the mlflow ui but cannot see and experiments as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6KaQK.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Could you help me in finding where I am going wrong?<\/p>",
        "Challenge_closed_time":1650761211590,
        "Challenge_comment_count":1,
        "Challenge_created_time":1648821712310,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71708147",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":15.0,
        "Challenge_reading_time":52.77,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":43,
        "Challenge_solved_time":538.7498,
        "Challenge_title":"MLFlow tracking ui not showing experiments on local machine (laptop)",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":936,
        "Challenge_word_count":377,
        "Platform":"Stack Overflow",
        "Poster_created_time":1648820658172,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Where do you run <code>mlflow ui<\/code> command?<\/p>\n<p>I think if you pass tracking ui path in the arguments, it would work:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>mlflow ui --backend-store-uri file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":1659992652860,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":11.1,
        "Solution_reading_time":3.82,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":28.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1341842709088,
        "Answerer_location":"Jo\u00e3o Pessoa - PB, Brasil",
        "Answerer_reputation_count":314.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":38.6383375,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to build an autoencoder, which I'm sure I'm doing something wrong. I tried separating the creation of the model from the actual training but this is not really working out for me and is giving me the following error.<\/p>\n<pre><code>AssertionError: Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 310), dtype=tf.float32, name=None), name='dense_7\/Sigmoid:0', description=&quot;created by layer 'dense_7'&quot;)\n<\/code><\/pre>\n<p>I'm doing this all using the Kedro framework. I have a pipeline.py file with the pipeline definition and a nodes.py with the functions that I want to use. So far, this is my project structure:<\/p>\n<p>pipelines.py:<\/p>\n<pre><code>from kedro.pipeline import Pipeline, node\nfrom .nodes.autoencoder_nodes import *\n\ndef train_autoencoder_pipeline():\n    return Pipeline([\n        # Build neural network\n        node(\n            build_models, \n            inputs=[\n                &quot;train_x&quot;, \n                &quot;params:autoencoder_n_hidden_layers&quot;,\n                &quot;params:autoencoder_latent_space_size&quot;,\n                &quot;params:autoencoder_regularization_strength&quot;,\n                &quot;params:seed&quot;\n                ],\n            outputs=dict(\n                pre_train_autoencoder=&quot;pre_train_autoencoder&quot;,\n                pre_train_encoder=&quot;pre_train_encoder&quot;,\n                pre_train_decoder=&quot;pre_train_decoder&quot;\n            ), name=&quot;autoencoder-create-models&quot;\n        ),\n        # Scale features\n        node(fit_scaler, inputs=&quot;train_x&quot;, outputs=&quot;autoencoder_scaler&quot;, name=&quot;autoencoder-fit-scaler&quot;),\n        node(tranform_scaler, inputs=[&quot;autoencoder_scaler&quot;, &quot;train_x&quot;], outputs=&quot;autoencoder_scaled_train_x&quot;, name=&quot;autoencoder-scale-train&quot;),\n        node(tranform_scaler, inputs=[&quot;autoencoder_scaler&quot;, &quot;test_x&quot;], outputs=&quot;autoencoder_scaled_test_x&quot;, name=&quot;autoencoder-scale-test&quot;),\n\n        # Train autoencoder\n        node(\n            train_autoencoder, \n            inputs=[\n                &quot;autoencoder_scaled_train_x&quot;,\n                &quot;autoencoder_scaled_test_x&quot;,\n                &quot;pre_train_autoencoder&quot;, \n                &quot;pre_train_encoder&quot;, \n                &quot;pre_train_decoder&quot;,\n                &quot;params:autoencoder_epochs&quot;,\n                &quot;params:autoencoder_batch_size&quot;,\n                &quot;params:seed&quot;\n            ],\n            outputs= dict(\n                autoencoder=&quot;autoencoder&quot;,\n                encoder=&quot;encoder&quot;,\n                decoder=&quot;decoder&quot;,\n                autoencoder_history=&quot;autoencoder_history&quot;,\n            ),\n            name=&quot;autoencoder-train-model&quot;\n        )])\n<\/code><\/pre>\n<p>nodes.py:<\/p>\n<pre><code>from sklearn.preprocessing import MinMaxScaler\nfrom tensorflow import keras\nimport tensorflow as tf\n\nfrom typing import Dict, Any, Tuple\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport logging\n\n\ndef build_models(data: pd.DataFrame, n_hidden_layers: int, latent_space_size: int, retularization_stregth: float, seed: int) -&gt; Tuple[keras.Model, keras.Model, keras.Model]:\n    assert n_hidden_layers &gt;= 1, &quot;There must be at least 1 hidden layer for the autoencoder&quot;\n    \n    n_features = data.shape[1]\n    tf.random.set_seed(seed)\n    input_layer = keras.Input(shape=(n_features,))\n    \n    hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(input_layer)\n    hidden = keras.layers.LeakyReLU()(hidden)\n    \n    for _ in range(n_hidden_layers - 1):\n        hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(hidden)\n        hidden = keras.layers.LeakyReLU()(hidden)\n    \n    encoded = keras.layers.Dense(latent_space_size, activation=&quot;sigmoid&quot;)(hidden)\n\n    hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(encoded)\n    hidden = keras.layers.LeakyReLU()(hidden)\n    \n    for _ in range(n_hidden_layers - 1):\n        hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(hidden)\n        hidden = keras.layers.LeakyReLU()(hidden)\n    \n\n    decoded = keras.layers.Dense(n_features, activation=&quot;sigmoid&quot;)(hidden)\n\n    # Defines the neural networks\n    autoencoder = keras.models.Model(inputs=input_layer, outputs=decoded)\n    encoder = keras.models.Model(inputs=input_layer, outputs=encoded)\n    decoder = keras.models.Model(inputs=input_layer, outputs=decoded)\n    autoencoder.compile(optimizer=&quot;adam&quot;, loss=&quot;mean_absolute_error&quot;)\n\n    return dict(\n        pre_train_autoencoder=autoencoder,\n        pre_train_encoder=encoder,\n        pre_train_decoder=decoder\n    )\n\ndef fit_scaler(data: pd.DataFrame) -&gt; MinMaxScaler:\n    scaler = MinMaxScaler()\n    scaler.fit(data)\n    return scaler\n\ndef tranform_scaler(scaler: MinMaxScaler, data: pd.DataFrame) -&gt; np.array:\n    return scaler.transform(data)\n\ndef train_autoencoder(\n    train_x: pd.DataFrame, test_x: pd.DataFrame, \n    autoencoder: keras.Model, encoder: keras.Model, decoder: keras.Model, \n    epochs: int, batch_size: int, seed: int) -&gt; Dict[str, Any]:\n\n    tf.random.set_seed(seed)\n    callbacks = [\n        keras.callbacks.History(),\n        keras.callbacks.EarlyStopping(patience=3)\n    ]\n    logging.info(train_x.shape)\n    logging.info(test_x.shape)\n\n    history = autoencoder.fit(\n        train_x, train_x,\n        validation_data=(test_x, test_x),\n        callbacks=callbacks, \n        epochs=epochs,\n        batch_size=batch_size\n    )\n\n    return dict(\n        autoencoder=autoencoder,\n        encoder=encoder,\n        decoder=decoder,\n        autoencoder_history=history,\n    )\n<\/code><\/pre>\n<p>catalog.yaml:<\/p>\n<pre><code>autoencoder_scaler:\n  type: pickle.PickleDataSet\n  filepath: data\/06_models\/autoencoder_scaler.pkl\n\nautoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/autoencoder.h5\n\nencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/encoder.h5\n\ndecoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/decoder.h5\n\nautoencoder_train_x:\n  type: pandas.CSVDataSet\n  filepath: data\/04_feature\/autoencoder_train_x.csv\n\nautoencoder_test_x:\n  type: pandas.CSVDataSet\n  filepath: data\/04_feature\/autoencoder_test_x.csv\n<\/code><\/pre>\n<p>And finally parameters.yaml:<\/p>\n<pre><code>seed: 200\n# Autoencoder\nautoencoder_n_hidden_layers: 3\nautoencoder_latent_space_size: 15\nautoencoder_epochs: 100\nautoencoder_batch_size: 32\nautoencoder_regularization_strength: 0.001\n<\/code><\/pre>\n<p>I believe that Keras is not seeing the whole graph since they will be out of the scope for the buld_models function, but I'm not sure whether this is the case, or how to fix it. Any help would be appreciated.<\/p>",
        "Challenge_closed_time":1629132605447,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629078707297,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68796641",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":19.0,
        "Challenge_reading_time":85.22,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":62,
        "Challenge_solved_time":14.9717083334,
        "Challenge_title":"Building an autoencoder with Keras and Kedro",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":245,
        "Challenge_word_count":439,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423164285360,
        "Poster_location":"Itabira, Brazil",
        "Poster_reputation_count":856.0,
        "Poster_view_count":106.0,
        "Solution_body":"<p>I was able to set up your project locally and reproduce the error. To fix it, I had to add the <code>pre_train_*<\/code> outputs to the catalog as well. Therefore, it's my <code>catalog.yaml<\/code> file:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>autoencoder_scaler:\n  type: pickle.PickleDataSet\n  filepath: data\/06_models\/autoencoder_scaler.pkl\n\npre_train_autoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_autoencoder.h5\n\npre_train_encoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_encoder.h5\n\npre_train_decoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_decoder.h5\n\nautoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/autoencoder.h5\n\nencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/encoder.h5\n\ndecoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/decoder.h5\n<\/code><\/pre>\n<p>Also, I changed the return of <code>train_autoencoder<\/code> node to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>return dict(\n    autoencoder=autoencoder,\n    autoencoder_history=history.history,\n)\n<\/code><\/pre>\n<p>Note that I changed the <code>autoencoder_history<\/code> to return <code>history.history<\/code> since <code>MemoryDataset<\/code> can't pickle the object <code>history<\/code> by itself. The <code>history.history<\/code> is a dictionary with losses of train and validation sets.<\/p>\n<p>You can find the complete code <a href=\"https:\/\/gist.github.com\/arnaldog12\/a3f7801fe3910c02f4bcea8be61b910c\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_gpt_summary":"assertionerror pre train output catalog yaml file return train autoencod node return dictionari autoencod autoencod histori",
        "Solution_last_edit_time":1629217805312,
        "Solution_link_count":1.0,
        "Solution_original_content":"set local reproduc add pre train output catalog catalog yaml file autoencod scaler type pickl pickledataset filepath data model autoencod scaler pkl pre train autoencod type extra dataset tensorflow tensorflowmodeldataset filepath data model pre train autoencod pre train encod type extra dataset tensorflow tensorflowmodeldataset filepath data model pre train encod pre train decod type extra dataset tensorflow tensorflowmodeldataset filepath data model pre train decod autoencod type extra dataset tensorflow tensorflowmodeldataset filepath data model autoencod encod type extra dataset tensorflow tensorflowmodeldataset filepath data model encod decod type extra dataset tensorflow tensorflowmodeldataset filepath data model decod return train autoencod node return dict autoencod autoencod autoencod histori histori histori note autoencod histori return histori histori memorydataset pickl object histori histori histori dictionari loss train set complet",
        "Solution_preprocessed_content":"set local reproduc add output catalog file return node note return pickl object dictionari loss train set complet",
        "Solution_readability":19.6,
        "Solution_reading_time":23.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":127.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1479363468550,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":108.0,
        "Answerer_view_count":35.0,
        "Challenge_adjusted_solved_time":131.9191969444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to do some kind of web job application that can run for period time and make prediction on azure machine learning studio. After that i want get the result of this experiment and do something with that in my console application. What is the best way to do this in azure with machine learning or maybe some similiar stuff to prediction data from data series ? <\/p>",
        "Challenge_closed_time":1486537080116,
        "Challenge_comment_count":0,
        "Challenge_created_time":1486062508683,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42010405",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":5.58,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":131.8253980556,
        "Challenge_title":"The way to pass input for azure machine experiment from app ( for example console app )",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":62,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432141466928,
        "Poster_location":null,
        "Poster_reputation_count":327.0,
        "Poster_view_count":78.0,
        "Solution_body":"<p>You can try using Azure Data Factory to create a Machine Learning pipeline or use Azure ML Studio's Predictive Web Services.<\/p>\n\n<ol>\n<li><p>With Azure Data Factory\nFollow <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/data-factory-azure-ml-batch-execution-activity\" rel=\"nofollow noreferrer\">this link<\/a> for details. Azure Data Factory implementations would seem difficult at first but they do work great with Azure ML experiments. <\/p>\n\n<p>Azure Data Factory can run your ML Experiment on a schedule or one-off at a specified time (I guess you can set only for UTC Timezone right now) and monitor it through a dashboard (which is pretty cool).<\/p>\n\n<p>As an example you can look @ <a href=\"https:\/\/github.com\/Microsoft\/azure-docs\/blob\/master\/articles\/data-factory\/data-factory-azure-ml-batch-execution-activity.md\" rel=\"nofollow noreferrer\">ML Batch Execution<\/a>. I used this in one of our implementations (we do have latency issues, but trying to solve that).<\/p><\/li>\n<li><p>If you directly want to use the experiment in your console (assuming it is a web application), use create a Predictive Web service out of your ML Experiment, details <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-walkthrough-5-publish-web-service\" rel=\"nofollow noreferrer\">here<\/a><\/p><\/li>\n<\/ol>\n\n<p>I couldn't exactly understand your use case so I posted two alternatives that should help you. Hope this might lead you to a better solution\/approach.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"data factori creat pipelin run schedul specifi time monitor dashboard creat predict web servic directli consol",
        "Solution_last_edit_time":1486537417792,
        "Solution_link_count":3.0,
        "Solution_original_content":"data factori creat pipelin studio predict web servic data factori link data factori implement great data factori run schedul specifi time guess set utc timezon monitor dashboard pretti cool batch execut implement latenc directli consol web creat predict web servic exactli hope",
        "Solution_preprocessed_content":"data factori creat pipelin studio predict web servic data factori link data factori implement great data factori run schedul specifi time monitor dashboard batch execut implement directli consol creat predict web servic exactli hope",
        "Solution_readability":14.4,
        "Solution_reading_time":19.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":180.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1612497172983,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":51.4987111111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a bucket in GCP containing my images dataset.<\/p>\n<p>The path to it is: xray-competition-bucket\/img_align_celeba<\/p>\n<p>How do I read it from GCP to Jupyter Lab in Vertex AI?<\/p>\n<p>My code is:<\/p>\n<pre><code>MAIN_PATH = '\/gcs\/xray-competition-bucket\/img_align_celeba'\n\nimage_paths = glob((MAIN_PATH + &quot;\/*.jpg&quot;))\n<\/code><\/pre>\n<p>and the result is that image_paths is an empty array.<\/p>\n<p>Note: I also tried the path gs:\/\/my_bucket\/...<\/p>",
        "Challenge_closed_time":1661266806083,
        "Challenge_comment_count":3,
        "Challenge_created_time":1661081410723,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73434003",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":7.5,
        "Challenge_reading_time":6.49,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":51.4987111111,
        "Challenge_title":"Read images from a bucket in GCP for ML",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":79,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Poster_created_time":1639312590127,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>You will need to <a href=\"https:\/\/cloud.google.com\/storage\/docs\/downloading-objects#storage-download-object-python\" rel=\"nofollow noreferrer\">download the GCS file locally<\/a> using <code>gsutil<\/code> or the python SDK if you want to use glob. There are also libraries like <a href=\"https:\/\/gcsfs.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">GCSFS<\/a> or TensorFlow's <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/gfile\/GFile\" rel=\"nofollow noreferrer\">GFile<\/a> which offer a pythonic file-system interface for working with GCS. For example, here is <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/gfile\/glob\" rel=\"nofollow noreferrer\">GFile.glob<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"download gc file local gsutil sdk glob librari gcsf tensorflow gfile file interfac gc gfile glob read imag",
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_original_content":"download gc file local gsutil sdk glob librari gcsf tensorflow gfile file interfac gc gfile glob",
        "Solution_preprocessed_content":"download gc file local sdk glob librari gcsf tensorflow gfile interfac gc",
        "Solution_readability":17.1,
        "Solution_reading_time":9.31,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":57.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1537462795807,
        "Answerer_location":null,
        "Answerer_reputation_count":99.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":2937.8422519445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any way to load containers stored in docker hub registry in Amazon Sagemaker.\nAccording to some documentation, it should be possible, but I have not been able to find any relevan example or guide for it.<\/p>",
        "Challenge_closed_time":1582242007587,
        "Challenge_comment_count":0,
        "Challenge_created_time":1571665775480,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58487710",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":3.35,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2937.8422519445,
        "Challenge_title":"User Docker Hub registry containers in AWS Sagemaker",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":135,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1382876091092,
        "Poster_location":"Valencia, Espa\u00f1a",
        "Poster_reputation_count":149.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>While you can use any registry when working with Docker on a SageMaker notebook, as of this writing other SageMaker components presently only support images from Amazon ECR repositories.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"registri docker notebook write compon present imag ecr repositori",
        "Solution_preprocessed_content":"registri docker notebook write compon present imag ecr repositori",
        "Solution_readability":15.8,
        "Solution_reading_time":2.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1522870754323,
        "Answerer_location":"Redmond, WA, USA",
        "Answerer_reputation_count":366.0,
        "Answerer_view_count":173.0,
        "Challenge_adjusted_solved_time":166.7495119444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In my asp.net application, I want to do a sentiment analysis on each discussion forum item as they are posted by the users. I wonder if it is a good practice to make a request to Azure Text Analytics server to do a new Sentiment Analysis each time a text is posted by any user. Or, is it better to do this somehow once a day on all posts as a batch. I wonder what is the best practice about this.<\/p>",
        "Challenge_closed_time":1562697661556,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562097728357,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56859324",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.9,
        "Challenge_reading_time":5.76,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":166.6481108333,
        "Challenge_title":"Best practice about the frequency of sentiment analysis in Azure Text Analytics",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":59,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1353596362916,
        "Poster_location":"USA",
        "Poster_reputation_count":8007.0,
        "Poster_view_count":792.0,
        "Solution_body":"<p>We are working on a similar project as you. My suggestion for your question is, you should select proper interval based on your requirement and need. For example, if you want to react based on the sentiment result of every post in a timely manner, you need to do a analysis every time you pull a new post. Also you can adjust your time interval of \"pull and analyze\" based on your business\/ research need. IF you just want to train your model and predict something based on the data you get and you have no need for timely reaction, I think once a day is enough.<\/p>\n\n<p>For our project, we are in the situation 1. So we will do a quick analysis when we receive any new post so that we can have a quick reaction.<\/p>\n\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"select interv base react base sentiment time conduct analysi time pull train model predict base data time reaction dai",
        "Solution_last_edit_time":1562698026600,
        "Solution_link_count":0.0,
        "Solution_original_content":"select interv base react base sentiment time analysi time pull adjust time interv pull analyz base busi research train model predict base data time reaction dai quick analysi receiv quick reaction hope",
        "Solution_preprocessed_content":"select interv base react base sentiment time analysi time pull adjust time interv pull analyz base busi research train model predict base data time reaction dai quick analysi receiv quick reaction hope",
        "Solution_readability":6.6,
        "Solution_reading_time":8.86,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":140.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1465222092252,
        "Answerer_location":"Z\u00fcrich, Switzerland",
        "Answerer_reputation_count":1414.0,
        "Answerer_view_count":478.0,
        "Challenge_adjusted_solved_time":649.8259375,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Kubeflow pipelines (KFP) with GCP Vertex AI pipelines. I am using <code>kfp==1.8.5<\/code> (kfp SDK) and <code>google-cloud-pipeline-components==0.1.7<\/code>. Not sure if I can find which version of Kubeflow is used on GCP.<\/p>\n<p>I am bulding a component (yaml) using python inspired form this <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3748#issuecomment-627698554\" rel=\"nofollow noreferrer\">Github issue<\/a>. I am defining an output like:<\/p>\n<pre><code>outputs=[(OutputSpec(name='drt_model', type='Model'))]\n<\/code><\/pre>\n<p>This will be a base output directory to store few artifacts on Cloud Storage like model checkpoints and model.<\/p>\n<p>I would to keep one base output directory but add sub directories depending of the artifact:<\/p>\n<ul>\n<li>&lt;output_dir_base&gt;\/model<\/li>\n<li>&lt;output_dir_base&gt;\/checkpoints<\/li>\n<li>&lt;output_dir_base&gt;\/tensorboard<\/li>\n<\/ul>\n<p>but I didn't find how to concatenate the <strong>OutputPathPlaceholder('drt_model')<\/strong> with a string like <strong>'\/model'<\/strong>.<\/p>\n<p>How can append extra folder structure like \/model or \/tensorboard to the OutputPathPlaceholder that KFP will set during run time ?<\/p>",
        "Challenge_closed_time":1637263566352,
        "Challenge_comment_count":2,
        "Challenge_created_time":1634924192977,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69681031",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":12.4,
        "Challenge_reading_time":16.63,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":649.8259375,
        "Challenge_title":"how to concatenate the OutputPathPlaceholder with a string with Kubeflow pipelines?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":312,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1465222092252,
        "Poster_location":"Z\u00fcrich, Switzerland",
        "Poster_reputation_count":1414.0,
        "Poster_view_count":478.0,
        "Solution_body":"<p>I didn't realized in the first place that <code>ConcatPlaceholder<\/code> accept both Artifact and string. This is exactly what I wanted to achieve:<\/p>\n<pre><code>ConcatPlaceholder([OutputPathPlaceholder('drt_model'), '\/model'])\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"concatplacehold function pass outputpathplacehold desir concaten concatplacehold function accept artifact concaten outputpathplacehold desir subdirectori",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"realiz concatplacehold accept artifact exactli achiev concatplacehold outputpathplacehold drt model model",
        "Solution_preprocessed_content":"realiz accept artifact exactli achiev",
        "Solution_readability":15.2,
        "Solution_reading_time":3.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1483444144907,
        "Answerer_location":"Hoth",
        "Answerer_reputation_count":312.0,
        "Answerer_view_count":65.0,
        "Challenge_adjusted_solved_time":138.6800833333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Crowd HTML Elements to perform bounding box annotation, but when I attempt to load some of my images, I get this error in the dev tools console:<\/p>\n<pre><code>crowd-html-elements.js:1 window.onError received an event without an error:  {event: ErrorEvent}\n(anonymous) @ crowd-html-elements.js:1\nerror (async)\ne @ crowd-html-elements.js:1\ne @ crowd-html-elements.js:1\n.\/src\/crowd-html-elements-loader.ts @ crowd-html-elements.js:1\ns @ crowd-html-elements.js:1\n(anonymous) @ crowd-html-elements.js:1\n(anonymous) @ crowd-html-elements.js:1\ncrowd-html-elements-without-ce-polyfill.js:6282 window.onError received an event without an error:  {event: ErrorEvent}\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:6282\nerror (async)\ne @ crowd-html-elements-without-ce-polyfill.js:6282\ne @ crowd-html-elements-without-ce-polyfill.js:6282\n.\/src\/index.ts @ crowd-html-elements-without-ce-polyfill.js:6282\nr @ crowd-html-elements-without-ce-polyfill.js:1\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:1\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:1\ncrowd-html-elements-without-ce-polyfill.js:6282 Uncaught Error: Unexpected image dimensions during normalization\n    at Function.normalizeHeight (crowd-html-elements-without-ce-polyfill.js:6282)\n    at Function.normalizeDimensions (crowd-html-elements-without-ce-polyfill.js:6282)\n    at new a (crowd-html-elements-without-ce-polyfill.js:6282)\n    at ie.handleTargetImageLoaded (crowd-html-elements-without-ce-polyfill.js:6282)\n    at Image.r.onload (crowd-html-elements-without-ce-polyfill.js:6282)\nnormalizeHeight @ crowd-html-elements-without-ce-polyfill.js:6282\nnormalizeDimensions @ crowd-html-elements-without-ce-polyfill.js:6282\na @ crowd-html-elements-without-ce-polyfill.js:6282\nhandleTargetImageLoaded @ crowd-html-elements-without-ce-polyfill.js:6282\nr.onload @ crowd-html-elements-without-ce-polyfill.js:6282\nload (async)\nsetBackgroundImage @ crowd-html-elements-without-ce-polyfill.js:6282\nrenderImageSrcChange @ crowd-html-elements-without-ce-polyfill.js:6282\nshouldComponentUpdate @ crowd-html-elements-without-ce-polyfill.js:6282\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nG @ crowd-html-elements-without-ce-polyfill.js:6278\nw @ crowd-html-elements-without-ce-polyfill.js:6278\nS @ crowd-html-elements-without-ce-polyfill.js:6278\ne.reactMount @ crowd-html-elements-without-ce-polyfill.js:3\ne.updateRegion @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\ne.reactBatchUpdate @ crowd-html-elements-without-ce-polyfill.js:3\ni @ crowd-html-elements-without-ce-polyfill.js:3\nf.componentDidUpdate @ crowd-html-elements-without-ce-polyfill.js:3\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nG @ crowd-html-elements-without-ce-polyfill.js:6278\nw @ crowd-html-elements-without-ce-polyfill.js:6278\n_renderReactComponent @ crowd-html-elements-without-ce-polyfill.js:6282\n_updateReactComponent @ crowd-html-elements-without-ce-polyfill.js:6282\nY @ crowd-html-elements-without-ce-polyfill.js:5984\nC @ crowd-html-elements-without-ce-polyfill.js:5984\nk @ crowd-html-elements-without-ce-polyfill.js:5984\n_propertiesChanged @ crowd-html-elements-without-ce-polyfill.js:5984\n_flushProperties @ crowd-html-elements-without-ce-polyfill.js:5954\n_flushProperties @ crowd-html-elements-without-ce-polyfill.js:5984\n_invalidateProperties @ crowd-html-elements-without-ce-polyfill.js:5984\n_setProperty @ crowd-html-elements-without-ce-polyfill.js:5984\nObject.defineProperty.set @ crowd-html-elements-without-ce-polyfill.js:5954\n(anonymous) @ labeling.html:199\nasync function (async)\n(anonymous) @ labeling.html:198\nPromise.then (async)\n(anonymous) @ labeling.html:196\n<\/code><\/pre>\n<p>The <strong>Unexpected image dimensions during normalization<\/strong> portion seems like the issue, but I've found nothing with regard to troubleshooting.  Can someone explain what expected image dimensions are and why some are failing?<\/p>\n<p>Here's a snippet of the code that's throwing the error.<\/p>\n<pre><code>            static normalizeHeight(e) {\n                if (e.height === e.naturalHeight)\n                    return e.height;\n                if (e.height === e.naturalWidth)\n                    return e.height;\n                if (Math.abs(e.height - e.naturalHeight) &lt; 2)\n                    return e.naturalHeight;\n                if (Math.abs(e.height - e.naturalWidth) &lt; 2)\n                    return e.naturalWidth;\n                throw new Error(&quot;Unexpected image dimensions during normalization&quot;)\n            }\n            static normalizeWidth(e) {\n                if (e.width === e.naturalWidth)\n                    return e.width;\n                if (e.width === e.naturalHeight)\n                    return e.width;\n                if (Math.abs(e.width - e.naturalWidth) &lt; 2)\n                    return e.naturalWidth;\n                if (Math.abs(e.width - e.naturalHeight) &lt; 2)\n                    return e.naturalHeight;\n                throw new Error(&quot;Unexpected image dimensions during normalization&quot;)\n<\/code><\/pre>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1603367517400,
        "Challenge_comment_count":6,
        "Challenge_created_time":1601930941820,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1602868269100,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64215998",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":25.9,
        "Challenge_reading_time":94.02,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":108,
        "Challenge_solved_time":399.0487722222,
        "Challenge_title":"Why is Crowd HTML breaking this image?",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":168,
        "Challenge_word_count":380,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483444144907,
        "Poster_location":"Hoth",
        "Poster_reputation_count":312.0,
        "Poster_view_count":65.0,
        "Solution_body":"<p>The issue turned out to be related to the css styling that was being applied to the canvas portion of my site that was loading the labeling tools.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"turn relat css style appli canva portion site load label",
        "Solution_preprocessed_content":"turn relat css style appli canva portion site load label",
        "Solution_readability":11.8,
        "Solution_reading_time":1.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":1.4933055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In the API docs about <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.html#\" rel=\"nofollow noreferrer\"><code>kedro.io<\/code><\/a> and <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.contrib.io.html\" rel=\"nofollow noreferrer\"><code>kedro.contrib.io<\/code><\/a> I could not find info about how to read\/write data from\/to network attached storage such as e.g. <a href=\"https:\/\/en.avm.de\/guide\/using-the-fritzbox-nas-function\/\" rel=\"nofollow noreferrer\">FritzBox NAS<\/a>.<\/p>",
        "Challenge_closed_time":1589448276967,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589441422903,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1589442901067,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61791713",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.1,
        "Challenge_reading_time":7.54,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.9039066667,
        "Challenge_title":"How can I read\/write data from\/to network attached storage with kedro?",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":675,
        "Challenge_word_count":46,
        "Platform":"Stack Overflow",
        "Poster_created_time":1441627039648,
        "Poster_location":"Augsburg, Germany",
        "Poster_reputation_count":3635.0,
        "Poster_view_count":383.0,
        "Solution_body":"<p>So I'm a little rusty on network attached storage, but:<\/p>\n\n<ol>\n<li><p>If you can mount your network attached storage onto your OS and access it like a regular folder, then it's just a matter of providing the right <code>filepath<\/code> when writing the config for a given catalog entry. See for example: <a href=\"https:\/\/stackoverflow.com\/questions\/7169845\/using-python-how-can-i-access-a-shared-folder-on-windows-network\">Using Python, how can I access a shared folder on windows network?<\/a><\/p><\/li>\n<li><p>Otherwise, if accessing the network attached storage requires anything special, you might want to <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/14_create_a_new_dataset.html\" rel=\"nofollow noreferrer\">create a custom dataset<\/a> that uses a Python library for interfacing with your network attached storage. Something like <a href=\"https:\/\/pysmb.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">pysmb<\/a> comes to mind.<\/p><\/li>\n<\/ol>\n\n<p>The custom dataset could borrow heavily from the logic in existing <code>kedro.io<\/code> or <code>kedro.extras.datasets<\/code> datasets, but you replace the filepath\/fsspec handling code with <code>pysmb<\/code> instead.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_gpt_summary":"network attach storag mount access regular folder filepath write config catalog entri access network attach storag creat dataset librari interfac network attach storag pysmb dataset borrow heavili logic extra dataset replac filepath fsspec pysmb",
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"littl rusti network attach storag mount network attach storag access regular folder matter filepath write config catalog entri access share folder window network access network attach storag creat dataset librari interfac network attach storag pysmb come dataset borrow heavili logic extra dataset dataset replac filepath fsspec pysmb",
        "Solution_preprocessed_content":"littl rusti network attach storag mount network attach storag access regular folder matter write config catalog entri access share folder window network access network attach storag creat dataset librari interfac network attach storag pysmb come dataset borrow heavili logic dataset replac",
        "Solution_readability":12.0,
        "Solution_reading_time":15.63,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":132.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1350768619470,
        "Answerer_location":"Tunisia",
        "Answerer_reputation_count":13575.0,
        "Answerer_view_count":1140.0,
        "Challenge_adjusted_solved_time":187.9993902778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was evaluating what is needed to write your own Estimator in Sagemaker. I was following this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\/container\" rel=\"nofollow noreferrer\">here<\/a> and it's well explained and quite simple.<\/p>\n<p>My question is regarding the inference <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py\" rel=\"nofollow noreferrer\">here<\/a>. I see an example in which we can feed the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/e648e9a6f596263c7683635d1a55f1729b08277d\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py#L60\" rel=\"nofollow noreferrer\">invocations endpoint<\/a> a CSV. What if I want to just post a string or even individual parameters? What's the best practise for that? I see there is a condition like:<\/p>\n<pre><code>if flask.request.content_type == &quot;text\/csv&quot;:\n<\/code><\/pre>\n<p>Should we add more like those to support different formats or should we create a new endpoint?<\/p>",
        "Challenge_closed_time":1631096060112,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630420918507,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69000752",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.2,
        "Challenge_reading_time":16.06,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":187.5393347222,
        "Challenge_title":"Amazon Sagemaker: write your own inference",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":168,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1322785469840,
        "Poster_location":"Cork, Ireland",
        "Poster_reputation_count":482.0,
        "Poster_view_count":40.0,
        "Solution_body":"<p>You need to add support for more content types.<\/p>\n<p>Since you would like to pass a string or a parameter, I suggest you add support for &quot;application\/json&quot; MIME media type (<a href=\"https:\/\/stackoverflow.com\/questions\/477816\/what-is-the-correct-json-content-type\">What is the correct JSON content type?<\/a>). Then your users will call the API with a Json that you can parse and extract parameters from in the backend.<\/p>\n<p>For example, if you have two parameters <code>age<\/code> and <code>gender<\/code> you want to pass to your model. You can put them in the following Json datastructure:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n &quot;age&quot;: ...,\n &quot;gender&quot;: ...\n}\n<\/code><\/pre>\n<p>Then add support for loading the Json and extracting the parameters in the backend as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if flask.request.content_type == &quot;application\/json&quot;:\n    data = flask.request.data.decode(&quot;utf-8&quot;)\n    data = json.loads(data)\n    parameter1 = data['age']\n    parameter2 = data['gender']\n    ...\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"add json mime media type pass individu paramet pars extract paramet json data structur backend load json extract paramet backend",
        "Solution_last_edit_time":1631097716312,
        "Solution_link_count":1.0,
        "Solution_original_content":"add type pass paramet add json mime media type json type api json pars extract paramet backend paramet ag gender pass model json datastructur ag gender add load json extract paramet backend flask request type json data flask request data decod utf data json load data paramet data ag paramet data gender",
        "Solution_preprocessed_content":"add type pass paramet add mime media type api json pars extract paramet backend paramet pass model json datastructur add load json extract paramet backend",
        "Solution_readability":11.5,
        "Solution_reading_time":14.03,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":121.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1513106638900,
        "Answerer_location":null,
        "Answerer_reputation_count":276.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":272.3565766667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>As in the documentation \/ tutorial mentioned, we can call <code>Estimator.fit()<\/code> to start Training Job. <\/p>\n\n<p>Required parameter for the method would be the <code>inputs<\/code> that is s3 \/ file reference to the Training File. Example:<\/p>\n\n<pre><code>estimator.fit({'train':'s3:\/\/my-bucket\/training_data})\n<\/code><\/pre>\n\n<p><strong>training-script.py<\/strong><\/p>\n\n<pre><code>parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n<\/code><\/pre>\n\n<p>I would expect <code>os.environ['SM_CHANNEL_TRAIN']<\/code> to be the S3 path. But instead, it returns <code>\/opt\/ml\/input\/data\/train<\/code>.<\/p>\n\n<p>Anyone know why?<\/p>\n\n<p><strong>Update<\/strong><\/p>\n\n<p>I also tried to call estimator.fit('s3:\/\/my-bucket\/training_data'). \nAnd somehow training instance didn't get the SM_CHANNEL_TRAIN Environment Variables. In fact, I didn't see the s3 URI in Environment Variables at all.<\/p>",
        "Challenge_closed_time":1567193332323,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565949262600,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1566221662207,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57522553",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":13.3,
        "Challenge_reading_time":13.07,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":345.5749230556,
        "Challenge_title":"SageMaker Estimator.fit() didn't pass the 'train' input to the Training instance",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":1411,
        "Challenge_word_count":98,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506308535436,
        "Poster_location":"Yogyakarta, Indonesia",
        "Poster_reputation_count":3575.0,
        "Poster_view_count":274.0,
        "Solution_body":"<p>When running training jobs in SageMaker the S3 URL containing your training data provided ends up being copied into the docker container (aka training job) from the specified url. Thus the environment variable SM_CHANNEL_TRAIN is pointing to the local path of the training data that was copied from the S3 URL provided.<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html#SageMaker-CreateTrainingJob-request-InputDataConfig\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html#SageMaker-CreateTrainingJob-request-InputDataConfig<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"environ variabl channel train local path train data copi url",
        "Solution_last_edit_time":1567202145883,
        "Solution_link_count":2.0,
        "Solution_original_content":"run train job url train data end copi docker aka train job specifi url environ variabl channel train local path train data copi url http doc com latest api createtrainingjob html createtrainingjob request inputdataconfig",
        "Solution_preprocessed_content":"run train job url train data end copi docker specifi url environ variabl local path train data copi url",
        "Solution_readability":20.6,
        "Solution_reading_time":8.4,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1592311727163,
        "Answerer_location":null,
        "Answerer_reputation_count":153.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":138.6078019445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a multi-model endpoint in AWS sagemaker using Scikit-learn and a custom training script. When I attempt to train my model using the following code:<\/p>\n<pre><code>estimator = SKLearn(\n    entry_point=TRAINING_FILE, # script to use for training job\n    role=role,\n    source_dir=SOURCE_DIR, # Location of scripts\n    train_instance_count=1,\n    train_instance_type=TRAIN_INSTANCE_TYPE,\n    framework_version='0.23-1',\n    output_path=s3_output_path,# Where to store model artifacts\n    base_job_name=_job,\n    code_location=code_location,# This is where the .tar.gz of the source_dir will be stored\n    hyperparameters = {'max-samples'    : 100,\n                       'model_name'     : key})\n\nDISTRIBUTION_MODE = 'FullyReplicated'\n\ntrain_input = sagemaker.s3_input(s3_data=inputs+'\/train', \n                                  distribution=DISTRIBUTION_MODE, content_type='csv')\n    \nestimator.fit({'train': train_input}, wait=True)\n<\/code><\/pre>\n<p>where 'TRAINING_FILE' contains:<\/p>\n<pre><code>\nimport argparse\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport sys\n\nfrom sklearn.ensemble import IsolationForest\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--max_samples', type=int, default=100)\n    \n    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--model_name', type=str)\n\n    args, _ = parser.parse_known_args()\n\n    print('reading data. . .')\n    print('model_name: '+args.model_name)    \n    \n    train_file = os.path.join(args.train, args.model_name + '_train.csv')    \n    train_df = pd.read_csv(train_file) # read in the training data\n    train_tgt = train_df.iloc[:, 1] # target column is the second column\n    \n    clf = IsolationForest(max_samples = args.max_samples)\n    clf = clf.fit([train_tgt])\n    \n    path = os.path.join(args.model_dir, 'model.joblib')\n    joblib.dump(clf, path)\n    print('model persisted at ' + path)\n<\/code><\/pre>\n<p>The training script succeeds but sagemaker throws an <code>UnexpectedStatusException<\/code>:\n<a href=\"https:\/\/i.stack.imgur.com\/mXgjS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mXgjS.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Has anybody ever experienced anything like this before? I've checked all the cloudwatch logs and found nothing of use, and I'm completely stumped on what to try next.<\/p>",
        "Challenge_closed_time":1603707766790,
        "Challenge_comment_count":1,
        "Challenge_created_time":1603208778703,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64448720",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":14.1,
        "Challenge_reading_time":32.2,
        "Challenge_score_count":2,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":138.6078019445,
        "Challenge_title":"AWS Sagemaker Multi-Model Endpoint with Scikit Learn: UnexpectedStatusException whilst using a training script",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":263,
        "Challenge_word_count":221,
        "Platform":"Stack Overflow",
        "Poster_created_time":1592311727163,
        "Poster_location":null,
        "Poster_reputation_count":153.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>To anyone that comes across this issue in future, the problem has been solved.<\/p>\n<p>The issue was nothing to do with the training, but with invalid characters in directory names being sent to S3. So the script would produce the artifacts correctly, but sagemaker would throw an exception when trying to save them to S3<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"charact directori sent directori charact",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"come futur train charact directori sent produc artifact throw except save",
        "Solution_preprocessed_content":"come futur train charact directori sent produc artifact throw except save",
        "Solution_readability":8.1,
        "Solution_reading_time":4.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1370505440848,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":29.4236144444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I use visual studio code with jupyter notebook, I have an &quot;outline&quot; tab in the left panels that display the Markdown section of my notebook for quick access.<\/p>\n<p>But in Sagemaker studio I don't have this and I would like to add it.<\/p>",
        "Challenge_closed_time":1652459529952,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652353604940,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72214443",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":3.98,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":29.4236144444,
        "Challenge_title":"Get notebook outline in sagemaker studio like in Visual Studio Code",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":164,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_created_time":1576136255052,
        "Poster_location":null,
        "Poster_reputation_count":795.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>The 'Outline' tab (Table of Contents extension in Jupyter) is not available for Studio yet.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi.html\" rel=\"nofollow noreferrer\">SageMaker notebook instances<\/a> come with the extension prebuilt.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"outlin tab studio notebook instanc tabl extens prebuilt",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"outlin tab tabl extens jupyt studio notebook instanc come extens prebuilt",
        "Solution_preprocessed_content":"outlin tab studio notebook instanc come extens prebuilt",
        "Solution_readability":13.8,
        "Solution_reading_time":3.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1645110475503,
        "Answerer_location":null,
        "Answerer_reputation_count":15.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":104.3447397222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I follow the official tutotial from microsoft: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool<\/a><\/p>\n<p>When I execute:<\/p>\n<pre><code>#Bind model within Spark session\n model = pcontext.bind_model(\n     return_types=RETURN_TYPES, \n     runtime=RUNTIME, \n     model_alias=&quot;Sales&quot;, #This alias will be used in PREDICT call to refer  this   model\n     model_uri=AML_MODEL_URI, #In case of AML, it will be AML_MODEL_URI\n     aml_workspace=ws #This is only for AML. In case of ADLS, this parameter can be removed\n ).register()\n<\/code><\/pre>\n<p>I got : No module named 'azureml.automl'<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/g0UCX.png\" rel=\"nofollow noreferrer\">My Notebook<\/a><\/p>",
        "Challenge_closed_time":1648911550928,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648558433353,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71662401",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":17.4,
        "Challenge_reading_time":12.66,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":98.0882152778,
        "Challenge_title":"Synapse Analytics Auto ML Predict No module named 'azureml.automl'",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":271,
        "Challenge_word_count":81,
        "Platform":"Stack Overflow",
        "Poster_created_time":1645110475503,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I solved it. In my case it works best like this:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/JKEmr.png\" rel=\"nofollow noreferrer\">Imports<\/a><\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>#Import libraries\nfrom pyspark.sql.functions import col, pandas_udf,udf,lit\nfrom notebookutils.mssparkutils import azureML\nfrom azureml.core import Workspace, Model\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nfrom azureml.core.model import Model\nimport joblib\nimport pandas as pd\n\nws = azureML.getWorkspace(\"AzureMLService\")\nspark.conf.set(\"spark.synapse.ml.predict.enabled\",\"true\")<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ij760.png\" rel=\"nofollow noreferrer\">Predict function<\/a><\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>def forecastModel():\n    model_path = Model.get_model_path(model_name=\"modelName\", _workspace=ws)\n    modeljob = joblib.load(model_path + \"\/model.pkl\")\n\n    validation_data = spark.read.format(\"csv\") \\\n                            .option(\"header\", True) \\\n                            .option(\"inferSchema\",True) \\\n                            .option(\"sep\", \";\") \\\n                            .load(\"abfss:\/\/....csv\")\n\n    validation_data_pd = validation_data.toPandas()\n\n\n    predict = modeljob.forecast(validation_data_pd)\n\n    return predict<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"messag import librari function notebookutil mssparkutil workspac forecastmodel function load model perform forecast data",
        "Solution_last_edit_time":1648934074416,
        "Solution_link_count":2.0,
        "Solution_original_content":"import import librari pyspark sql function import col panda udf udf lit notebookutil mssparkutil import core import workspac model core authent import serviceprincipalauthent core model import model import joblib import panda getworkspac servic spark conf set spark synaps predict enabl predict function forecastmodel model path model model path model modelnam workspac modeljob joblib load model path model pkl data spark read format csv option header option inferschema option sep load abfss csv data data topanda predict modeljob forecast data return predict",
        "Solution_preprocessed_content":null,
        "Solution_readability":20.1,
        "Solution_reading_time":20.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1310482059760,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":10753.0,
        "Answerer_view_count":895.0,
        "Challenge_adjusted_solved_time":265.4097633333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a way to view\/monitor AWS Sagemaker's usage of EC2 instances?\nI am running a Sagemaker endpoint and tried to find its instances (ml.p3.2xlarge in this case) in the EC2 UI, but couldn't find them. <\/p>",
        "Challenge_closed_time":1588575738648,
        "Challenge_comment_count":3,
        "Challenge_created_time":1587620263500,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61380051",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":4.9,
        "Challenge_reading_time":3.01,
        "Challenge_score_count":5,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":265.4097633333,
        "Challenge_title":"Sagemaker usage of EC2 instances",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":615,
        "Challenge_word_count":41,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>ml EC2 instances do not appear in the EC2 console. You can find their metrics in Cloudwatch though, and create dashboards to monitor what you need:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/BgUkm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BgUkm.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/wD4w0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wD4w0.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"instanc consol metric cloudwatch creat dashboard monitor usag instanc",
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_original_content":"instanc consol metric cloudwatch creat dashboard monitor",
        "Solution_preprocessed_content":"instanc consol metric cloudwatch creat dashboard monitor",
        "Solution_readability":10.9,
        "Solution_reading_time":6.4,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1515259002820,
        "Answerer_location":null,
        "Answerer_reputation_count":153.0,
        "Answerer_view_count":74.0,
        "Challenge_adjusted_solved_time":505.1086980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What is underlying algorithm for Sagemaker's <a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ntm.html\" rel=\"nofollow noreferrer\">Neural Topic Model<\/a>? I have hard time googling for details, and the documentation doesn't mention any paper.<\/p>\n\n<p>Googling for 'neural topic model' doesn't exactly answer my question, since a couple of methods seems to be called that.<\/p>",
        "Challenge_closed_time":1515260139876,
        "Challenge_comment_count":1,
        "Challenge_created_time":1513441748563,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47847736",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":11.9,
        "Challenge_reading_time":5.39,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":505.1086980556,
        "Challenge_title":"AWS Sagemaker Neural Topic Model",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":222,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Poster_created_time":1374572439352,
        "Poster_location":"Poland",
        "Poster_reputation_count":2177.0,
        "Poster_view_count":262.0,
        "Solution_body":"<p>Seems like AWS SageMaker team answered the question, \n<a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=270493&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=270493&amp;tstart=0<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"team forum thread link",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"team http forum com thread jspa threadid tstart",
        "Solution_preprocessed_content":null,
        "Solution_readability":25.7,
        "Solution_reading_time":3.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1557535777467,
        "Answerer_location":"Stockholm, Sweden",
        "Answerer_reputation_count":373.0,
        "Answerer_view_count":41.0,
        "Challenge_adjusted_solved_time":53.4253730556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>On Amazon Forecast, let\u2019s say I have a variable (Z) which is supposed to help me predict a set of target variables (Y1, Y2, Y3).<\/p>\n<p>First question is, what is the difference between:<\/p>\n<ol>\n<li>put Z as an extra attribute in the TARGET_TIME_SERIES, that is, as an extra column<\/li>\n<li>put Z as an attribute in the RELATED_TIME_SERIES<\/li>\n<\/ol>\n<p>Second question is, given that Z has just one value per day (let\u2019s say this is a stock price), how should I deal with the fact that I have 3x-repeated timestamps? Should I just repeat Z for each repeated date?<\/p>\n<p>I understand that, if I'm not training my model to predict Z, I need to provide future values for it. But this makes option 1) even it more confusing to me. In which cases should one add an extra attribute in TARGET_TIME_SERIES?<\/p>",
        "Challenge_closed_time":1611263092840,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611070761497,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65794703",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":10.62,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":53.4253730556,
        "Challenge_title":"Amazon Forecast - extra attributes in the TARGET_TIME_SERIES?",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":36,
        "Challenge_word_count":146,
        "Platform":"Stack Overflow",
        "Poster_created_time":1557535777467,
        "Poster_location":"Stockholm, Sweden",
        "Poster_reputation_count":373.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>I got a very nice explanation of it in here, so I'll leave it as the answer:\n<a href=\"https:\/\/github.com\/aws-samples\/amazon-forecast-samples\/issues\/104#issuecomment-764896502\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-forecast-samples\/issues\/104#issuecomment-764896502<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"direct link explan forecast predict target variabl variabl",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"nice explan leav http github com sampl forecast sampl issuecom",
        "Solution_preprocessed_content":null,
        "Solution_readability":22.7,
        "Solution_reading_time":4.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":36.3344202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to predict from my past data which has around 20 attribute columns and a label. Out of those 20, only 4 are significant for prediction. But i also want to know that if a row falls into one of the classified categories, what other important correlated columns apart from those 4 and what are their weight. I want to get that result from my deployed web service on Azure.<\/p>",
        "Challenge_closed_time":1461985174656,
        "Challenge_comment_count":1,
        "Challenge_created_time":1461854370743,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36917948",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.2,
        "Challenge_reading_time":5.52,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":36.3344202778,
        "Challenge_title":"Feature weightage from Azure Machine Learning Deployed Web Service",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":303,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1461853741067,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>You can use permutation feature importance module but that will give importance of the features across the sample set. Retrieving the weights on per call basis is not available in Azure ML.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"permut featur import modul import featur sampl set retriev weight basi",
        "Solution_preprocessed_content":"permut featur import modul import featur sampl set retriev weight basi",
        "Solution_readability":8.4,
        "Solution_reading_time":2.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1373018880576,
        "Answerer_location":null,
        "Answerer_reputation_count":305.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":17.1053752778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am following the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-deploy-model.html\" rel=\"noreferrer\">sage maker documentation<\/a> to train and deploy an ML model. I am using the high-level Python library provided by Amazon SageMaker to achieve this. <\/p>\n\n<pre><code>kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n                                 instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>The deployment fails with error<\/p>\n\n<p>ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.c4.8xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. <\/p>\n\n<p>Where am I going wrong?<\/p>",
        "Challenge_closed_time":1543906305888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1543844726537,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53595157",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":9.92,
        "Challenge_score_count":7,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":17.1053752778,
        "Challenge_title":"AWS Sagemaker Deploy fails",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":5932,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373018880576,
        "Poster_location":null,
        "Poster_reputation_count":305.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>I resolved the issue by changing the instance type:<\/p>\n\n<pre><code>kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n                                 instance_type='ml.t2.medium')\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"instanc type medium deploi model",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"instanc type kmean predictor kmean deploi initi instanc count instanc type medium",
        "Solution_preprocessed_content":null,
        "Solution_readability":19.0,
        "Solution_reading_time":2.34,
        "Solution_score_count":7.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":13.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":15.7482980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Our use case is as follows:\nWe have multiple custom trained models (in the hundreds, and the number increases as we allow the user of our application to create models through the UI, which we then train and deploy on the fly) and so deploying each model to a separate endpoint is expensive as Vertex AI <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/deploy-model-console#custom-trained\" rel=\"nofollow noreferrer\">charges per node used<\/a>. Based on the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> it seems that we can deploy models of different types to the same endpoint but I am not sure how that would work. Let's say I have 2 different custom trained models deployed using custom containers for prediction to the same endpoint. Also, say I specify the traffic split to be 50% for the two models. Now, how do I send a request to a specific model? Using the python SDK, we make calls to the endpoint, like so:<\/p>\n<pre><code>from google.cloud import aiplatform\nendpoint = aiplatform.Endpoint(endpoint_id)\nprediction = endpoint.predict(instances=instances)\n\n# where endpoint_id is the id of the endpoint and instances are the observations for which a prediction is required\n<\/code><\/pre>\n<p>My understanding is that in this scenario, vertex AI will route some calls to one model and some to the other based on the traffic split. I could use the parameters field, as specified in the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#prediction\" rel=\"nofollow noreferrer\">docs<\/a>, to specify the model and then process the request accordingly in the custom prediction container, but still some calls will end up going to a model which it will not be able to process (because Vertex AI is not going to be sending all requests to all models, otherwise the traffic split wouldn't make sense). How do I then deploy multiple models to the same endpoint and make sure that every prediction request is guaranteed to be served?<\/p>",
        "Challenge_closed_time":1636404890008,
        "Challenge_comment_count":9,
        "Challenge_created_time":1636348555970,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69878915",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":10,
        "Challenge_readability":12.3,
        "Challenge_reading_time":26.69,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":15.6483438889,
        "Challenge_title":"Deploying multiple models to same endpoint in Vertex AI",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1271,
        "Challenge_word_count":303,
        "Platform":"Stack Overflow",
        "Poster_created_time":1471292986790,
        "Poster_location":null,
        "Poster_reputation_count":700.0,
        "Poster_view_count":90.0,
        "Solution_body":"<p>This <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> talks about a use case where 2 models are trained on the same feature set and are sharing the ingress prediction traffic. As you have understood correctly, this does not apply to models that have been trained on different feature sets, that is, different models.<\/p>\n<p>Unfortunately, deploying different models to the same endpoint utilizing only one node is not possible in Vertex AI at the moment. There is an ongoing feature request that is being worked on. However, we cannot provide an exact ETA on when that feature will be available.<\/p>\n<p>I reproduced the multi-model setup and noticed the below points.<\/p>\n<p><strong>Traffic Splitting<\/strong><\/p>\n<blockquote>\n<p>I deployed 2 different models to the same endpoint and sent predictions to it. I set a 50-50 traffic splitting rule and saw errors that implied requests being sent to the wrong model.<\/p>\n<\/blockquote>\n<p><strong>Cost Optimization<\/strong><\/p>\n<blockquote>\n<p>When multiple models are deployed to the same endpoint, they are deployed to separate, independent nodes. So, you will still be charged for each node used. Also, node autoscaling happens at the model level, not at the endpoint level.<\/p>\n<\/blockquote>\n<p>A plausible workaround would be to pack all your models into a single container and use a custom HTTP server logic to send prediction requests to the appropriate model. This could be achieved using the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#request_requirements\" rel=\"nofollow noreferrer\"><code>parameters<\/code><\/a> field of the prediction request body. The custom logic would look something like this.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@app.post(os.environ['AIP_PREDICT_ROUTE'])\nasync def predict(request: Request):\n    body = await request.json()\n    parameters = body[&quot;parameters&quot;]\n    instances = body[&quot;instances&quot;]\n    inputs = np.asarray(instances)\n    preprocessed_inputs = _preprocessor.preprocess(inputs)\n\n    if(parameters[&quot;model_name&quot;]==&quot;random_forest&quot;):\n        print(parameters[&quot;model_name&quot;])\n        outputs = _random_forest_model.predict(preprocessed_inputs)\n    else:\n        print(parameters[&quot;model_name&quot;])\n        outputs = _decision_tree_model.predict(inputs)\n\n    return {&quot;predictions&quot;: [_class_names[class_num] for class_num in outputs]}\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_gpt_summary":"direct deploi multipl train model endpoint workaround pack model singl http server logic send predict request model paramet field predict request bodi import note deploi model endpoint util node moment ongo featur request multipl model deploi endpoint deploi separ independ node node autosc model level endpoint level",
        "Solution_last_edit_time":1636405249843,
        "Solution_link_count":2.0,
        "Solution_original_content":"document model train featur set share ingress predict traffic understood appli model train featur set model unfortun deploi model endpoint util node moment ongo featur request exact eta featur reproduc multi model setup notic traffic split deploi model endpoint sent predict set traffic split rule request sent model cost optim multipl model deploi endpoint deploi separ independ node charg node node autosc model level endpoint level plausibl workaround pack model singl http server logic send predict request model achiev paramet field predict request bodi logic app environ aip predict rout async predict request request bodi await request json paramet bodi paramet instanc bodi instanc input asarrai instanc preprocess input preprocessor preprocess input paramet model random forest print paramet model output random forest model predict preprocess input print paramet model output decis tree model predict input return predict class class num class num output",
        "Solution_preprocessed_content":"document model train featur set share ingress predict traffic understood appli model train featur set model unfortun deploi model endpoint util node moment ongo featur request exact eta featur reproduc setup notic traffic split deploi model endpoint sent predict set traffic split rule request sent model cost optim multipl model deploi endpoint deploi separ independ node charg node node autosc model level endpoint level plausibl workaround pack model singl http server logic send predict request model achiev field predict request bodi logic",
        "Solution_readability":12.7,
        "Solution_reading_time":32.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":273.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":169.5453825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I followed Microsoft's tutorial on the German credit card risk model, step by step and without mistakes. The algorithm runs, it is deployed successfully, etc.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/tuyDt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tuyDt.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am using the <code>Select Columns in Dataset<\/code> to select the columns to input, and I do the same to select the output columns. <\/p>\n\n<p>I noticed that when I look at the <code>Request\/Response<\/code> tab of the deployed model, the Sample Request includes <em>all<\/em> columns, ignoring the selection I provided. This includes the field to be predicted, which is column 21:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"Col1\",\n        \"Col2\",\n        \"Col3\",\n        \"Col4\",\n        \"Col5\",\n        \"Col6\",\n        \"Col7\",\n        \"Col8\",\n        \"Col9\",\n        \"Col10\",\n        \"Col11\",\n        \"Col12\",\n        \"Col13\",\n        \"Col14\",\n        \"Col15\",\n        \"Col16\",\n        \"Col17\",\n        \"Col18\",\n        \"Col19\",\n        \"Col20\",\n        \"Col21\"\n<\/code><\/pre>\n\n<p><strong>The problem<\/strong>: column 21 is the credit risk itself, so the API is expecting to receive that value. Instead, <strong>that is the value that should be predicted!<\/strong><\/p>\n\n<p>There clearly is a problem with the input schema, but how can I change that? How can I make sure that field is not requested by the API?<\/p>",
        "Challenge_closed_time":1570554263830,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569943900453,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58188104",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":17.55,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":169.5453825,
        "Challenge_title":"Azure Machine Learning REST API: why is the prediction included in the Sample Request?",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":119,
        "Challenge_word_count":189,
        "Platform":"Stack Overflow",
        "Poster_created_time":1445878520940,
        "Poster_location":"Humboldt, Saskatchewan, Canada",
        "Poster_reputation_count":1271.0,
        "Poster_view_count":171.0,
        "Solution_body":"<p>Don't worry about the input schema for the <code>Col21<\/code> field. The <code>Col21<\/code> field in the input data just adapt for the <code>Edit Metadata<\/code> module which requires the <code>Col21<\/code> data in the training stage.<\/p>\n\n<p>You just fill an invalid value like <code>0<\/code> (<code>0<\/code> is an invalid classified value for risk) into <code>Col21<\/code> field, and then the web service will return a prediction classified value to replace the <code>Col21<\/code> value of your input data.<\/p>\n\n<p>At here, I use the first data record of the sample data with the <code>Col21<\/code> value <code>0<\/code> for testing via the link of <code>Test<\/code> feature on portal, it works fine and return <code>1<\/code> for <code>Credit risk<\/code><\/p>\n\n<p>Fig 1. To click <code>Test<\/code> link to test for <code>Col21<\/code> with <code>0<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9jqyo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9jqyo.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 2. Use the first record of sample to test<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/lIUiP.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lIUiP.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 3. The <code>Col21<\/code> value of <code>input1<\/code> is <code>0<\/code>, and the <code>Credit risk<\/code> value of <code>output1<\/code> is <code>1<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/h5OEH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/h5OEH.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"valu field predict api return predict valu replac valu input data test",
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_original_content":"worri input schema col field col field input data adapt edit metadata modul col data train stage valu classifi valu risk col field web servic return predict classifi valu replac col valu input data data record sampl data col valu test link test featur portal return credit risk fig click test link test col fig record sampl test fig col valu input credit risk valu output",
        "Solution_preprocessed_content":"worri input schema field field input data adapt modul data train stage valu field web servic return predict classifi valu replac valu input data data record sampl data valu test link featur portal return fig click link test fig record sampl test fig valu valu",
        "Solution_readability":9.7,
        "Solution_reading_time":20.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":175.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":3.4073972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to use azuremlsdk to deploy a locally trained model (a perfectly valid use case AFIK). I follow <a href=\"https:\/\/cran.r-project.org\/web\/packages\/azuremlsdk\/vignettes\/train-and-deploy-first-model.html\" rel=\"nofollow noreferrer\">this<\/a> and managed to create a ML workspace and register a &quot;model&quot; like so:<\/p>\n<pre><code>library(azuremlsdk)\n\ninteractive_auth &lt;- interactive_login_authentication(tenant_id=&quot;xxx&quot;)\nws &lt;- get_workspace(\n        name = &quot;xxx&quot;, \n        subscription_id = &quot;xxx&quot;, \n        resource_group =&quot;xxx&quot;, \n        auth = interactive_auth\n)\n\nadd &lt;- function(a, b) {\n    return(a + b)\n}\n\nadd(1,2)\n\nsaveRDS(add, file = &quot;D:\/add.rds&quot;)\n\nmodel &lt;- register_model(\n    ws, \n    model_path = &quot;D:\/add.rds&quot;, \n    model_name = &quot;add_model&quot;,\n    description = &quot;An amazing model&quot;\n)\n<\/code><\/pre>\n<p>This seemed to work fine, as I get some nice log messages telling me that the model was registered. For my sanity, I wonder where can I find this registered (&quot;materialised&quot;) model\/object\/function in the Azure UI please?<\/p>",
        "Challenge_closed_time":1621001633740,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620989367110,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67533091",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":14.76,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.4073972222,
        "Challenge_title":"where are registered models in azure machine learning",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":41,
        "Challenge_word_count":118,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>On ml.azure.com, there is a &quot;Models&quot; option on the left-hand blade.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Y7cZe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Y7cZe.png\" alt=\"UI Sidebar\" \/><\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_gpt_summary":"navig model option left hand blade com locat regist model",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"com model option left hand blade",
        "Solution_preprocessed_content":null,
        "Solution_readability":11.1,
        "Solution_reading_time":3.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1224733422316,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":7707.0,
        "Answerer_view_count":583.0,
        "Challenge_adjusted_solved_time":80.3919036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a model quality monitor job, using the class ModelQualityMonitor from Sagemaker model_monitor, and i think i have all the import statements defined yet i get the message cannot import name error<\/p>\n<pre><code>from sagemaker import get_execution_role, session, Session\nfrom sagemaker.model_monitor import ModelQualityMonitor\n                \nrole = get_execution_role()\nsession = Session()\n\nmodel_quality_monitor = ModelQualityMonitor(\n    role=role,\n    instance_count=1,\n    instance_type='ml.m5.xlarge',\n    volume_size_in_gb=20,\n    max_runtime_in_seconds=1800,\n    sagemaker_session=session\n)\n<\/code><\/pre>\n<p>Any pointers are appreciated<\/p>",
        "Challenge_closed_time":1608016613856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607727203003,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65259702",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":20.9,
        "Challenge_reading_time":9.39,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":80.3919036111,
        "Challenge_title":"AWS sagemaker model monitor- ImportError: cannot import name 'ModelQualityMonitor'",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":544,
        "Challenge_word_count":71,
        "Platform":"Stack Overflow",
        "Poster_created_time":1546959992036,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Are you using an Amazon SageMaker Notebook? When I run your code above in a new <code>conda_python3<\/code> Amazon SageMaker notebook, I don't get any errors at all.<\/p>\n<p>Example screenshot output showing no errors:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZTP5D.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZTP5D.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>If you're getting something like <code>NameError: name 'ModelQualityMonitor' is not defined<\/code> then I suspect you are running in a Python environment that doesn't have the Amazon SageMaker SDK installed in it. Perhaps try running <code>pip install sagemaker<\/code> and then see if this resolves your error.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"notebook instal sdk instal pip instal",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"notebook run conda notebook screenshot output nameerror modelqualitymonitor defin run environ sdk instal run pip instal",
        "Solution_preprocessed_content":"notebook run notebook screenshot output run environ sdk instal run",
        "Solution_readability":11.4,
        "Solution_reading_time":9.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":193.1276480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm very excited on the newly released Azure Machine Learning service (preview), which is a great step up from the previous (and deprecated) Machine Learning Workbench.<\/p>\n\n<p>However, I am thinking a lot about the best practice on structuring the folders and files in my project(s). I'll try to explain my thoughts.<\/p>\n\n<p>Looking at the documentation for the training of a model (e.g. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#create-an-estimator\" rel=\"nofollow noreferrer\">Tutorial #1<\/a>), there seems to be good-practice to put all training scripts and necessary additional scripts inside a subfolder, so that it can be passed into the <code>Estimator<\/code> object without also passing all other files in the project. This is fine.<\/p>\n\n<p>But when working with the deployment of the service, specifically the deployment of the image, the documentation (e.g. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml#deploy-in-aci\" rel=\"nofollow noreferrer\">Tutorial #2<\/a>) seems to indicate that the scoring script need to be located in the root folder. If I try to refer to a script located in a subfolder, I get an error message saying<\/p>\n\n<p><code>WebserviceException: Unable to use a driver file not in current directory. Please navigate to the location of the driver file and try again.<\/code><\/p>\n\n<p>This may not be a big deal. Except, I have some additional scripts that I import both in the training script and in the scoring script, and I don't want to duplicate those additional scripts to be able to import them in both the training and the scoring scripts.<\/p>\n\n<p>I am working mainly in Jupyter Notebooks when executing the training and the deployment, and I could of course use some tricks to read the particular scripts from some other folder, save them to disk as a copy, execute the training or deployment while referring to the copies and finally delete the copies. This would be a decent workaround, but it seems to me that there should be a better way than just decent.<\/p>\n\n<p>What do you think?<\/p>",
        "Challenge_closed_time":1540309291403,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539614031870,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52819122",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":28.09,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":193.1276480556,
        "Challenge_title":"What is the best practice on folder structure for Azure Machine Learning service (preview) projects",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":782,
        "Challenge_word_count":326,
        "Platform":"Stack Overflow",
        "Poster_created_time":1463756509236,
        "Poster_location":"Uppsala, Sverige",
        "Poster_reputation_count":400.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>Currently, the score.py needs to be in current working directory, but dependency scripts - the <em>dependencies<\/em> argument to  <em>ContainerImage.image_configuration<\/em> - can be in a subfolder.<\/p>\n\n<p>Therefore, you should be able to use folder structure like this:<\/p>\n\n<pre><code>.\/score.py \n.\/myscripts\/train.py \n.\/myscripts\/common.py\n<\/code><\/pre>\n\n<p>Note that the relative folder structure is preserved during web service deployment; if you reference the common file in subfolder from your score.py, that reference should be valid within deployed image.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_gpt_summary":"folder structur score file locat root folder train addit locat subfold depend locat subfold rel folder structur preserv web servic deploy common file subfold score file deploi imag",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"score directori depend depend argument containerimag imag configur subfold folder structur score myscript train myscript common note rel folder structur preserv web servic deploy common file subfold score deploi imag",
        "Solution_preprocessed_content":"directori depend depend argument subfold folder structur note rel folder structur preserv web servic deploy common file subfold deploi imag",
        "Solution_readability":10.1,
        "Solution_reading_time":7.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1629385138956,
        "Answerer_location":null,
        "Answerer_reputation_count":395.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":26.6152038889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I would like to know what is the OPTIMAL way to store the result of a Google BigQuery table query, to Google Cloud storage. My code, which is currently being run in some Jupyter Notebook (in Vertex AI Workbench, same project than both the BigQuery data source, as well as the Cloud Storage destination), looks as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># CELL 1 OF 2\n\nfrom google.cloud import bigquery\nbqclient = bigquery.Client()\n\n# The query string can vary:\nquery_string = &quot;&quot;&quot;\n        SELECT *  \n        FROM `my_project-name.my_db.my_table` \n        LIMIT 2000000\n        &quot;&quot;&quot;\n\ndataframe = (\n    bqclient.query(query_string)\n    .result()\n    .to_dataframe(\n        create_bqstorage_client=True,\n    )\n)\nprint(&quot;Dataframe shape: &quot;, dataframe.shape)\n\n# CELL 2 OF 2:\n\nimport pandas as pd\ndataframe.to_csv('gs:\/\/my_bucket\/test_file.csv', index=False)\n<\/code><\/pre>\n<p>This code takes around 7.5 minutes to successfully complete.<\/p>\n<p><strong>Is there a more OPTIMAL way to achive what was done above?<\/strong> (It would mean <em>faster<\/em>, but maybe something else could be improved).<\/p>\n<p>Some additional notes:<\/p>\n<ol>\n<li>I want to run it &quot;via a Jupyter Notebook&quot; (in Vertex AI Workbench), because sometimes some data preprocessing, or special filtering must be done, which cannot be easily accomplished via SQL queries.<\/li>\n<li>For the first part of the code, I have discarded <a href=\"https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_gbq.html\" rel=\"nofollow noreferrer\">pandas.read_gbq<\/a>, as it was giving me some weird EOF errors, when (experimentally) &quot;storing as .CSV and reading back&quot;.<\/li>\n<li>Intuitively, I would focus the optimization efforts in the second half of the code (<code>CELL 2 OF 2<\/code>), as the first one was borrowed from <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/bigquery-storage-python-pandas\" rel=\"nofollow noreferrer\">the official Google documentation<\/a>. I have tried <a href=\"https:\/\/stackoverflow.com\/a\/57404119\/16706763\">this<\/a> but it does not work, however in the same thread <a href=\"https:\/\/stackoverflow.com\/a\/60644694\/16706763\">this<\/a> option worked OK.<\/li>\n<li>It is likley that this code will be included in some Docker image afterwards, so &quot;as little libraries as possible&quot; must be used.<\/li>\n<\/ol>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1651612810316,
        "Challenge_comment_count":2,
        "Challenge_created_time":1651600671333,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72103557",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":9.2,
        "Challenge_reading_time":30.6,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":3.3719397222,
        "Challenge_title":"Save the result of a query in a BigQuery Table, in Cloud Storage",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1409,
        "Challenge_word_count":292,
        "Platform":"Stack Overflow",
        "Poster_created_time":1629385138956,
        "Poster_location":null,
        "Poster_reputation_count":395.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>After some experiments, I think I have got to a solution for my original post. First, the updated code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd  # Just one library is imported this time\n\n# This SQL query can vary, modify it to match your needs\nquery_string = &quot;&quot;&quot;\nSELECT *\nFROM `my_project.my_db.my_table`\nLIMIT 2000000\n&quot;&quot;&quot;\n\n# One liner to query BigQuery data.\ndownloaded_dataframe = pd.read_gbq(query_string, dialect='standard', use_bqstorage_api=True)\n\n# Data processing (OPTIONAL, modify it to match your needs)\n# I won't do anything this time, just upload the previously queried data\n\n# Data store in GCS\ndownloaded_dataframe.to_csv('gs:\/\/my_bucket\/uploaded_data.csv', index=False)\n<\/code><\/pre>\n<p>Some final notes:<\/p>\n<ol>\n<li>I have not done an &quot;in-depth research&quot; about the processing speed VS the number of rows existing in a BigQuery table, however I saw that the processing time with the updated code and the original query, now takes ~6 minutes; that's enough for the time being. <em>This answer might have some room for further improvements<\/em> therefore, but it's better than the original situation.<\/li>\n<li>The EOF error I mentioned in  my original post was: <code>ParserError: Error tokenizing data. C error: EOF inside string starting at row 70198<\/code>. In the end I got to realize that it did not have anything to do with <a href=\"https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_gbq.html\" rel=\"nofollow noreferrer\">pandas_gbq<\/a> function, but with &quot;how I was saving the data&quot;. See, <em>I was 'experimentally' storing the .csv file in the Vertex AI Workbench local storage, then downloading it to my local device, and when trying to open that data from my local device, I kept stumbling upon that error, however not getting the same when downloading the .csv data from Cloud Storage<\/em> ... Why? Well, it happens that if you download the .csv data &quot;very quickly&quot; after &quot;it gets generated&quot; (i.e., after few seconds), from Vertex AI Workbench local storage, the data is simply still incomplete, but it does not give any error or warning message: it will simply &quot;let you start with the download&quot;. For this reason, I think it is safer to export your data to Cloud Storage, and then download safely from there. This behaviour is more noticeable on large files (i.e. my own generated file, which had ~3.1GB in size).<\/li>\n<\/ol>\n<p>Hope this helps.<\/p>\n<p>Thank you.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"updat librari panda queri bigqueri data store cloud storag export data cloud storag download workbench local storag avoid incomplet data",
        "Solution_last_edit_time":1651696486067,
        "Solution_link_count":1.0,
        "Solution_original_content":"origin updat import panda librari import time sql queri vari modifi match queri select tabl limit liner queri bigqueri data download datafram read gbq queri dialect standard bqstorag api data process option modifi match time upload previous queri data data store gc download datafram csv bucket upload data csv index final note depth research process speed row bigqueri tabl process time updat origin queri minut time room improv origin eof origin parsererror token data eof insid start row end realiz panda gbq function save data experiment store csv file workbench local storag download local devic open data local devic kept stumbl download csv data cloud storag download csv data quickli gener workbench local storag data simpli incomplet warn messag simpli start download reason safer export data cloud storag download safe notic larg file gener file size hope",
        "Solution_preprocessed_content":"origin updat final note research process speed row bigqueri tabl process time updat origin queri minut time room improv origin eof origin end realiz function save data experiment store csv file workbench local storag download local devic open data local devic kept stumbl download csv data cloud storag download csv data quickli gener workbench local storag data simpli incomplet warn messag simpli start download reason safer export data cloud storag download safe notic larg file hope",
        "Solution_readability":9.6,
        "Solution_reading_time":31.41,
        "Solution_score_count":2.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":357.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1540117021307,
        "Answerer_location":null,
        "Answerer_reputation_count":226.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":5.1148786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Let me prefix this by saying I'm very new to tensorflow and even newer to AWS Sagemaker.<\/p>\n\n<p>I have some tensorflow\/keras code that I wrote and tested on a local dockerized Jupyter notebook and it runs fine. In it, I import a csv file as my input.<\/p>\n\n<p>I use Sagemaker to spin up a jupyter notebook instance with conda_tensorflow_p36. I modified the pandas.read_csv() code to point to my input file, now hosted on a S3 bucket.<\/p>\n\n<p>So I changed this line of code from<\/p>\n\n<pre><code>import pandas as pd\n\ndata = pd.read_csv(\"\/input.csv\", encoding=\"latin1\")\n<\/code><\/pre>\n\n<p>to this<\/p>\n\n<pre><code>import pandas as pd\n\ndata = pd.read_csv(\"https:\/\/s3.amazonaws.com\/my-sagemaker-bucket\/input.csv\", encoding=\"latin1\")\n<\/code><\/pre>\n\n<p>and I get this error<\/p>\n\n<pre><code>AttributeError: module 'pandas' has no attribute 'core'\n<\/code><\/pre>\n\n<p>I'm not sure if it's a permissions issue. I read that as long as I name my bucket with the string \"sagemaker\" it should have access to it.<\/p>",
        "Challenge_closed_time":1540283269856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1540264856293,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52940677",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":13.34,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":5.1148786111,
        "Challenge_title":"AWS Sagemaker: AttributeError: module 'pandas' has no attribute 'core'",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1028,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Poster_created_time":1319234288808,
        "Poster_location":null,
        "Poster_reputation_count":4966.0,
        "Poster_view_count":304.0,
        "Solution_body":"<p>Pull our data from S3 for example:<\/p>\n\n<pre><code>import boto3\nimport io\nimport pandas as pd\n\n\n# Set below parameters\nbucket = '&lt;bucket name&gt;'\nkey = 'data\/training\/iris.csv'\nendpointName = 'decision-trees'\n\n# Pull our data from S3\ns3 = boto3.client('s3')\nf = s3.get_object(Bucket=bucket, Key=key)\n\n# Make a dataframe\nshape = pd.read_csv(io.BytesIO(f['Body'].read()), header=None)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"pull data bucket boto panda read csv file panda read csv bytesio creat datafram directli address messag relat panda modul read csv file bucket",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"pull data import boto import import panda set paramet bucket kei data train iri csv endpointnam decis tree pull data boto client object bucket bucket kei kei datafram shape read csv bytesio bodi read header",
        "Solution_preprocessed_content":null,
        "Solution_readability":8.9,
        "Solution_reading_time":5.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":42.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":14416.4976566667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to start my way with <a href=\"https:\/\/github.com\/allegroai\/clearml\" rel=\"nofollow noreferrer\">ClearML<\/a> (formerly known as Trains).<\/p>\n<p>I see on the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/getting_started\/index.html\" rel=\"nofollow noreferrer\">documentation<\/a> that I need to have server running, either on the ClearML platform itself, or on a remote machine using AWS etc.<\/p>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<p>According to <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/deploying_clearml\/index.html\" rel=\"nofollow noreferrer\">this<\/a> I can install the <code>trains-server<\/code> on any remote machine, so in theory I should also be able to install it on my local machine, but it still requires me to have Kubernetes or Docker, but I am not using any of them.<\/p>\n<p>Anyone had any luck using ClearML (or Trains, I think it's still quite the same API and all) on a local server?<\/p>\n<ul>\n<li>My OS is Ubuntu 18.04.<\/li>\n<\/ul>",
        "Challenge_closed_time":1609345139523,
        "Challenge_comment_count":2,
        "Challenge_created_time":1609343679217,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1609427009848,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65509754",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":10.3,
        "Challenge_reading_time":14.07,
        "Challenge_score_count":5,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.4056405556,
        "Challenge_title":"Can ClearML (formerly Trains) work a local server?",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":740,
        "Challenge_word_count":150,
        "Platform":"Stack Overflow",
        "Poster_created_time":1383083414230,
        "Poster_location":null,
        "Poster_reputation_count":2801.0,
        "Poster_view_count":131.0,
        "Solution_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<blockquote>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<\/blockquote>\n<p>A few options:<\/p>\n<ol>\n<li>The Clearml Free trier offers free hosting for your experiments, these experiment are only accessible to you, unless you specifically want to share them among your colleagues. This is probably the easiest way to <a href=\"https:\/\/app.community.clear.ml\" rel=\"nofollow noreferrer\">get started<\/a>.<\/li>\n<li>Install the ClearML-Server basically all you need is docker installed and you should be fine. There are full instructions <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/deploying_clearml\/clearml_server_linux_mac\/\" rel=\"nofollow noreferrer\">here<\/a> , this is the summary:<\/li>\n<\/ol>\n<pre class=\"lang-bash prettyprint-override\"><code>echo &quot;vm.max_map_count=262144&quot; &gt; \/tmp\/99-trains.conf\nsudo mv \/tmp\/99-trains.conf \/etc\/sysctl.d\/99-trains.conf\nsudo sysctl -w vm.max_map_count=262144\nsudo service docker restart\n\nsudo curl -L &quot;https:\/\/github.com\/docker\/compose\/releases\/latest\/download\/docker-compose-$(uname -s)-$(uname -m)&quot; -o \/usr\/local\/bin\/docker-compose\nsudo chmod +x \/usr\/local\/bin\/docker-compose\n\nsudo mkdir -p \/opt\/trains\/data\/elastic_7\nsudo mkdir -p \/opt\/trains\/data\/mongo\/db\nsudo mkdir -p \/opt\/trains\/data\/mongo\/configdb\nsudo mkdir -p \/opt\/trains\/data\/redis\nsudo mkdir -p \/opt\/trains\/logs\nsudo mkdir -p \/opt\/trains\/config\nsudo mkdir -p \/opt\/trains\/data\/fileserver\n\nsudo curl https:\/\/raw.githubusercontent.com\/allegroai\/trains-server\/master\/docker-compose.yml -o \/opt\/trains\/docker-compose.yml\ndocker-compose -f \/opt\/trains\/docker-compose.yml up -d\n<\/code><\/pre>\n<ol start=\"3\">\n<li>ClearML also supports full offline mode (i.e. no outside connection is made). Once your experiment completes, you can manually import the run to your server (either self hosted or free tier server)<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.set_offline(True)\ntask = Task.init(project_name='examples', task_name='offline mode experiment')\n<\/code><\/pre>\n<p>When the process ends you will get a link to a zip file containing the output of the entire offline session:<\/p>\n<pre><code>ClearML Task: Offline session stored in \/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip\n<\/code><\/pre>\n<p>Later you can import the session with:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.import_offline_session('\/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip')\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"free tier host instal server local docker offlin mode run manual import run server later bias summari",
        "Solution_last_edit_time":1661326401412,
        "Solution_link_count":4.0,
        "Solution_original_content":"disclaim member team train bypass restrict run local connect remot destin option free trier free host access share colleagu probabl easiest start instal server docker instal instruct summari echo map count tmp train conf sudo tmp train conf sysctl train conf sudo sysctl map count sudo servic docker restart sudo curl http github com docker compos releas latest download docker compos unam unam usr local bin docker compos sudo chmod usr local bin docker compos sudo mkdir opt train data elast sudo mkdir opt train data mongo sudo mkdir opt train data mongo configdb sudo mkdir opt train data redi sudo mkdir opt train log sudo mkdir opt train config sudo mkdir opt train data fileserv sudo curl http raw githubusercont com allegroai train server master docker compos yml opt train docker compos yml docker compos opt train docker compos yml offlin mode outsid connect complet manual import run server host free tier server import task task set offlin task task init task offlin mode process end link zip file output entir offlin session task offlin session store home cach offlin offlin dbbdeacfeead zip later import session import task task import offlin session home cach offlin offlin dbbdeacfeead zip",
        "Solution_preprocessed_content":"disclaim member team bypass restrict run local connect remot destin option free trier free host access share colleagu probabl easiest start instal server docker instal instruct summari offlin mode complet manual import run server process end link zip file output entir offlin session later import session",
        "Solution_readability":14.4,
        "Solution_reading_time":35.39,
        "Solution_score_count":6.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":265.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1448655975827,
        "Answerer_location":null,
        "Answerer_reputation_count":1478.0,
        "Answerer_view_count":135.0,
        "Challenge_adjusted_solved_time":1.6806736111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to train a custom ML model with SageMaker. The model is written in Python and should be shipped to SageMaker in a Docker image. Here is a simplified version of my Dockerfile (the model sits in the train.py file):<\/p>\n\n<pre><code>FROM amazonlinux:latest\n\n# Install Python 3\nRUN yum -y update &amp;&amp; yum install -y python3-pip python3-devel gcc &amp;&amp; yum clean all\n\n# Install sagemaker-containers (the official SageMaker utils package)\nRUN pip3 install --target=\/usr\/local\/lib\/python3.7\/site-packages sagemaker-containers &amp;&amp; rm -rf \/root\/.cache\n\n# Bring the script with the model to the image \nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n\n<p>Now, if I initialize this image as a SageMaker estimator and then run the <code>fit<\/code> method on this estimator I get the following error:<\/p>\n\n<p>\"AlgorithmError: CannotStartContainerError. Please make sure the container can be run with 'docker run  train'.\"<\/p>\n\n<p>In other words: SageMaker is not able to get into the container and run the train.py file. But why? The way I am specifying the entrypoint with <code>ENV SAGEMAKER_PROGRAM train.py<\/code> is recommended in the <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\/blob\/master\/README.rst\" rel=\"nofollow noreferrer\">docs of the sagemaker-containers package<\/a> (see 'How a script is executed inside the container').<\/p>",
        "Challenge_closed_time":1585671760392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585665709967,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60953289",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.9,
        "Challenge_reading_time":18.64,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1.6806736111,
        "Challenge_title":"SageMaker gives CannotStartContainerError although I specified an entrypoint",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":1133,
        "Challenge_word_count":189,
        "Platform":"Stack Overflow",
        "Poster_created_time":1448655975827,
        "Poster_location":null,
        "Poster_reputation_count":1478.0,
        "Poster_view_count":135.0,
        "Solution_body":"<p>I found a hint in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">the AWS docs<\/a> and came up with this solution:<\/p>\n\n<pre><code>ENTRYPOINT [\"python3.7\", \"\/opt\/ml\/code\/train.py\"]\n<\/code><\/pre>\n\n<p>With this the container <a href=\"https:\/\/docs.docker.com\/engine\/reference\/builder\/#entrypoint\" rel=\"nofollow noreferrer\">will run as an executable<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"document specifi entrypoint entrypoint opt train allow run execut",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"hint doc came entrypoint opt train run execut",
        "Solution_preprocessed_content":"hint doc came run execut",
        "Solution_readability":18.3,
        "Solution_reading_time":5.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1351085228507,
        "Answerer_location":null,
        "Answerer_reputation_count":98.0,
        "Answerer_view_count":41.0,
        "Challenge_adjusted_solved_time":7728.2646775,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm creating a SageMaker GroundTruth Labeling Job using the console UI. I'm looking for a way to configure &quot;Task title&quot;, which is shown in the workers Job list.<\/p>\n<p>I think this is related to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-create-labeling-job-api.html\" rel=\"nofollow noreferrer\">TaskTitle<\/a> configuration of AWS CLI. However, I cannot configure it from the AWS console. Can we configure it from the console GUI?<\/p>",
        "Challenge_closed_time":1656482770812,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628661017973,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68736614",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":6.74,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":7728.2646775,
        "Challenge_title":"How to set \"Task title\" of a SageMaker GrountTruth labeling job",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":55,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1354705336800,
        "Poster_location":"Pittsburgh",
        "Poster_reputation_count":2517.0,
        "Poster_view_count":78.0,
        "Solution_body":"<p>I was also doing the similar stuff and running a sagemaker GT labeling job.\nI am following the below git repo by amazon :<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline<\/a><\/p>\n<p>If you look into the below file in the repo:<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/main\/notebooks\/util\/smgt.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/main\/notebooks\/util\/smgt.py<\/a><\/p>\n<p>at line no:349 ,you will see how they do it and you can do the similar change in your code.<\/p>\n<p><strong>&quot;TaskTitle&quot;: &quot;Credit Card Agreement Entities&quot;<\/strong><\/p>\n<p>Hope this will help.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"file github repositori file line tasktitl credit card agreement entiti",
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_original_content":"stuff run label job git repo http github com sampl textract transform pipelin file repo http github com sampl textract transform pipelin blob notebook util smgt line tasktitl credit card agreement entiti hope",
        "Solution_preprocessed_content":"stuff run label job git repo file repo line tasktitl credit card agreement entiti hope",
        "Solution_readability":16.7,
        "Solution_reading_time":11.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1589293508567,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":140.5201130556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>On AzureML Batchendpoint, I'm recently hitting the following error:<\/p>\n<pre><code>Unable to get image details : Environment version Autosave_(date)T(time)Z_******** provided in request doesn't match environ.\n<\/code><\/pre>\n<p>when I setup the batch-endpoint with a <code>yml<\/code> config:<\/p>\n<p><code>environment: azureml:env-name:env-version<\/code><\/p>\n<p>So, AzureML creates and builds the environment with the version I specify <code>env-version<\/code>, which is just a number (in my case = 3).<\/p>\n<p>and then for some weird reason, AzureML creates an extra environment version called <code>Autosave_(date)T(time)Z_********<\/code>, which is not built, but based on the previous one just created, and then it becomes the <code>latest<\/code> version of that environment.<\/p>\n<p>In summary, AzureML instead of looking for the version that I specified as <code>env-name:3<\/code> it seems to be looking for <code>env-name:Autosave_(date)T(time)Z_********<\/code> and then throws the error message mentioned above.<\/p>",
        "Challenge_closed_time":1648808005063,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648738692993,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71694816",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.2,
        "Challenge_reading_time":14.81,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":19.2533527778,
        "Challenge_title":"Unable to get image details : Environment version Autosave_(date)T(time)Z_******** provided in request doesn't match environ",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":92,
        "Challenge_word_count":135,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589293508567,
        "Poster_location":null,
        "Poster_reputation_count":833.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>I found the problem was that when creating an environment from a YAML specification file, one of my <strong>conda dependencies<\/strong> was <code>cmake<\/code>, which I needed to allow installation of another python module. The docker image is exactly the same as a previously created environment.<\/p>\n<p>Removing the <code>cmake<\/code> dependency from the YAML file, eliminated the issue. So the workaround is to install it using a Dockerfile.<\/p>\n<p>The error message was very misleading to start with, but got there in the end after understanding that AzureML reuses a cached image, based on the hash value, from the environment definition accordingly to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-environments#image-caching-and-reuse\" rel=\"nofollow noreferrer\">this<\/a><\/p>\n<p>So for that reason, the automatically created <code>Autosave<\/code> docker image  references to that same build, which only happens once when the first job is sent.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"conda depend cmake yaml file creat environ remov depend yaml file workaround instal cmake dockerfil messag mislead understood reus cach imag base hash valu environ definit autosav docker imag build job sent",
        "Solution_last_edit_time":1649244565400,
        "Solution_link_count":1.0,
        "Solution_original_content":"creat environ yaml file conda depend cmake allow instal modul docker imag exactli previous creat environ remov cmake depend yaml file elimin workaround instal dockerfil messag mislead start end reus cach imag base hash valu environ definit accordingli reason automat creat autosav docker imag build job sent",
        "Solution_preprocessed_content":"creat environ yaml file conda depend allow instal modul docker imag exactli previous creat environ remov depend yaml file elimin workaround instal dockerfil messag mislead start end reus cach imag base hash valu environ definit accordingli reason automat creat docker imag build job sent",
        "Solution_readability":15.1,
        "Solution_reading_time":12.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":128.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":0.0226744444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to build a machine learning model locally using AWS SageMaker, but I got a validation error on IAM Role name. Although it's the exact role name that I created on the console.<\/p>\n<p>This is my code<\/p>\n<pre><code>    import boto3\n    import sagemaker\n    from sagemaker import get_execution_role\n    from sagemaker.amazon.amazon_estimator import image_uris\n    from sagemaker.amazon.amazon_estimator import RecordSet\n\n    sess = sagemaker.Session()\n\n\n    bucket = sagemaker.Session().default_bucket()\n    prefix = 'sagemaker\/ccard19'\n\n    role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access '\n\n    container = image_uris.retrieve('linear-learner',boto3.Session().region_name)\n    \n    # Some other code\n\n   linear = sagemaker.LinearLearner(role=role,\n                                               instance_count=1,\n                                               instance_type='ml.m4.xlarge',\n                                               predictor_type='binary_classifier')\n  \n  # Some other code\n\n  ### Fit the classifier\n  linear.fit([train_records,val_records,test_records], wait=True, logs='All')\n\n<\/code><\/pre>\n<p>And this is the error message<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: 1 validation error detected: Value 'arn:aws:iam::949010940542:role\/SageMaker-Full-Access ' at 'roleArn' failed to satisfy constraint: Member must satisfy regular expression pattern: ^arn:aws[a-z\\-]*:iam::\\d{12}:role\/?[a-zA-Z_0-9+=,.@\\-_\/]+$\n<\/code><\/pre>\n<p>Any Help please?<\/p>",
        "Challenge_closed_time":1617256809808,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617256728180,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66899120",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.7,
        "Challenge_reading_time":19.05,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.0226744444,
        "Challenge_title":"Validation error on Role name when running AWS SageMaker linear-learner locally",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":560,
        "Challenge_word_count":132,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378039539503,
        "Poster_location":null,
        "Poster_reputation_count":340.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>You have <strong>space<\/strong> in the name. It should be:<\/p>\n<pre><code>role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access'\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"remov space iam role format arn iam account role role",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"space role arn iam role access",
        "Solution_preprocessed_content":null,
        "Solution_readability":12.7,
        "Solution_reading_time":2.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1343828614448,
        "Answerer_location":"Seville, Spain",
        "Answerer_reputation_count":359.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":85.6601075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an inference pipeline with some PythonScriptStep with a ParallelRunStep in the middle. Everything works fine except for the fact that all mini batches are run on one node during the ParallelRunStep, no matter how many nodes I put in the <code>node_count<\/code> config argument.<\/p>\n<p>All the nodes seem to be up and running in the cluster, and according to the logs the <code>init()<\/code> function has been run on them multiple times. Diving into the logs I can see in <strong>sys\/error\/10.0.0.*<\/strong> that all the workers except the one that is working are saying:<\/p>\n<p><code>FileNotFoundError: [Errno 2] No such file or directory: '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/virtualstage\/azureml\/c36eb050-adc9-4c34-8a33-5f6d42dcb19c\/wd\/tmp8_txakpm\/bg.png'<\/code><\/p>\n<p><strong>bg.png<\/strong> happens to be a side argument created in a previous PythonScriptStep that I'm passing to the ParallelRunStep:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>bg_file = PipelineData('bg',  datastore=data_store)\nbg_file_ds = bg_file.as_dataset()\nbg_file_named = bg_file_ds.as_named_input(&quot;bg&quot;)\nbg_file_dw = bg_file_named.as_download()\n\n...\n\nparallelrun_step = ParallelRunStep(\n    name='batch-inference',\n    parallel_run_config=parallel_run_config,\n    inputs=[frames_data_named.as_download()],\n    arguments=[&quot;--bg_folder&quot;, bg_file_dw],\n    side_inputs=[bg_file_dw],\n    output=inference_frames_ds,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>What's happening here? Why the side argument seems to be available only in one worker while it fails in the others?<\/p>\n<p>BTW I found <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/957\" rel=\"nofollow noreferrer\">this<\/a> similar but unresolved question.<\/p>\n<p>Any help is much appreciated, thanks!<\/p>",
        "Challenge_closed_time":1619694485790,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619386109403,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67258465",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":23.72,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":85.6601075,
        "Challenge_title":"AzureML ParallelRunStep runs only on one node",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":244,
        "Challenge_word_count":186,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343828614448,
        "Poster_location":"Seville, Spain",
        "Poster_reputation_count":359.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>Apparently you need to specify a local mount path to use side_inputs in more than one node:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>bg_file_named = bg_file_ds.as_named_input(f&quot;bg&quot;)\nbg_file_mnt = bg_file_named.as_mount(f&quot;\/tmp\/{str(uuid.uuid4())}&quot;)\n\n...\n\nparallelrun_step = ParallelRunStep(\n    name='batch-inference',\n    parallel_run_config=parallel_run_config,\n    inputs=[frames_data_named.as_download()],\n    arguments=[&quot;--bg_folder&quot;, bg_file_mnt],\n    side_inputs=[bg_file_mnt],\n    output=inference_frames_ds,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>Sources:<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-parallel-run-step\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-parallel-run-step<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/18355\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/18355<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"specifi local mount path input node creat input dataset mount local path mount argument worker sourc document github",
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_original_content":"appar specifi local mount path input node file file input file mnt file mount tmp str uuid uuid parallelrun step parallelrunstep batch infer parallel run config parallel run config input frame data download argument folder file mnt input file mnt output infer frame allow reus sourc http doc com debug parallel run step http github com sdk",
        "Solution_preprocessed_content":"appar specifi local mount path node sourc",
        "Solution_readability":31.7,
        "Solution_reading_time":13.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1587069846163,
        "Answerer_location":"France",
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":0.0910288889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have to build yolact++ in docker enviromment (i'm using sagemaker notebook). Like this<\/p>\n<pre><code>ARG PYTORCH=&quot;1.3&quot;\nARG CUDA=&quot;10.1&quot;\nARG CUDNN=&quot;7&quot;\n \nFROM pytorch\/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel\n<\/code><\/pre>\n<p>And i want to run this<\/p>\n<pre><code>COPY yolact\/external\/DCNv2\/setup.py \/opt\/ml\/code\/external\/DCNv2\/setup.py\nRUN cd \/opt\/ml\/code\/external\/DCNv2 &amp;&amp; \\\npython setup.py build develop\n<\/code><\/pre>\n<p>But i got this error :<\/p>\n<pre><code>No CUDA runtime is found, using CUDA_HOME='\/usr\/local\/cuda'\nTraceback (most recent call last):\nFile &quot;setup.py&quot;, line 64, in &lt;module&gt;\next_modules=get_extensions(),\nFile &quot;setup.py&quot;, line 41, in get_extensions\nraise NotImplementedError('Cuda is not available')\nNotImplementedError: Cuda is not available\n<\/code><\/pre>\n<p>But the enviromment supports CUDA. Anyone have an idea where is the problem ?<\/p>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1621519129332,
        "Challenge_comment_count":4,
        "Challenge_created_time":1621258222833,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1621519151352,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67570694",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":12.9,
        "Challenge_reading_time":13.0,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":72.4740275,
        "Challenge_title":"How to build YOLACT++ using Docker?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":486,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1587069846163,
        "Poster_location":"France",
        "Poster_reputation_count":11.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>SOLUTION :<\/p>\n<p>i edit the \/etc\/docker\/daemon.json with content:<\/p>\n<pre><code>{\n&quot;runtimes&quot;: {\n    &quot;nvidia&quot;: {\n        &quot;path&quot;: &quot;\/usr\/bin\/nvidia-container-runtime&quot;,\n        &quot;runtimeArgs&quot;: []\n     } \n},\n&quot;default-runtime&quot;: &quot;nvidia&quot; \n}\n<\/code><\/pre>\n<p>Then i Restart docker daemon:<\/p>\n<pre><code>sudo system restart docker\n<\/code><\/pre>\n<p>it solved my problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"edit docker daemon json file restart docker daemon cuda build yolact docker environ notebook",
        "Solution_last_edit_time":1621519479056,
        "Solution_link_count":0.0,
        "Solution_original_content":"edit docker daemon json runtim nvidia path usr bin nvidia runtim runtimearg default runtim nvidia restart docker daemon sudo restart docker",
        "Solution_preprocessed_content":"edit restart docker daemon",
        "Solution_readability":18.8,
        "Solution_reading_time":5.52,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1262067470272,
        "Answerer_location":"Los Angeles, CA, United States",
        "Answerer_reputation_count":11653.0,
        "Answerer_view_count":727.0,
        "Challenge_adjusted_solved_time":162.6529575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I'm trying to use <code>tf.contrib.learn.preprocessing.VocabularyProcessor.restore()<\/code> to restore a vocabulary file from an S3 bucket. First, I tried to get the path name to the bucket to use in <code>.restore()<\/code> and I kept getting 'object doesn't exist' error. Afterwards, upon further research, I found a method people use to load text files and JSON files and applied the same method here:<\/p>\n\n<pre><code>obj = s3.Object(BUCKET_NAME, KEY).get()['Body'].read()\nvocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(obj)\n<\/code><\/pre>\n\n<p>This worked for a while until the contents of file increased and eventually got a 'File name too long' error. Is there a better way to load and restore a file from an S3 bucket? <\/p>\n\n<p>By the way, I tested this out locally on my machine and it works perfectly fine since there it just needs to take the path to the file, not the entire contents of the file. <\/p>",
        "Challenge_closed_time":1521485629848,
        "Challenge_comment_count":0,
        "Challenge_created_time":1521471311783,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49365900",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":12.77,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":3.9772402778,
        "Challenge_title":"What's a better way to load a file using boto? (getting filename too long error)",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":532,
        "Challenge_word_count":153,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521470035783,
        "Poster_location":null,
        "Poster_reputation_count":210.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>It looks like you\u2019re passing in the actual contents of the file as the file name?<\/p>\n\n<p>I think you\u2019ll need to download the object from S3 to a tmp file and pass the path to that file into restore.<\/p>\n\n<p>Try using the method here: <a href=\"http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Object.download_file\" rel=\"nofollow noreferrer\">http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Object.download_file<\/a><\/p>\n\n<p>Update:\nI went through the code here: <a href=\"https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py<\/a> and it looks like this just saves a pickle so you can really easily just import pickle and call the following:<\/p>\n\n<pre><code>import pickle\nobj = s3.Object(BUCKET_NAME, KEY).get()['Body']\nvocab_processor = pickle.loads(obj.read())\n<\/code><\/pre>\n\n<p>Hopefully that works?<\/p>",
        "Solution_comment_count":3.0,
        "Solution_gpt_summary":"download object temporari file pass path file restor boto document download file import pickl load file import pickl obj object bucket kei bodi vocab processor pickl load obj read",
        "Solution_last_edit_time":1522056862430,
        "Solution_link_count":4.0,
        "Solution_original_content":"your pass file file youll download object tmp file pass path file restor http boto readthedoc latest servic html object download file updat went http github com tensorflow tensorflow blob master tensorflow contrib preprocess text save pickl easili import pickl import pickl obj object bucket kei bodi vocab processor pickl load obj read hopefulli",
        "Solution_preprocessed_content":"your pass file file youll download object tmp file pass path file restor updat went save pickl easili import pickl hopefulli",
        "Solution_readability":16.7,
        "Solution_reading_time":13.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1254829817772,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":2595.0,
        "Answerer_view_count":357.0,
        "Challenge_adjusted_solved_time":197.5463266667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to implement a input_handler() in inference.py for a sagemaker inference container.<\/p>\n<p>The images\/arrays are very big (3D). So I want to pass in a S3 URI, then the input_handler() function should load the image\/array from s3 and return the actual numpy array for the model (which expects a tensor):<\/p>\n<pre><code>def input_handler(data, context):\n\n    d = data.read().decode('utf-8')\n\n    body = json.loads(d)\n    s3path = body['s3_path']\n\n    s3 = S3FileSystem()\n    df = np.load(s3.open(s3path))\n\n    return df\n<\/code><\/pre>\n<p>Returning a numpy array worked with the Sagemaker python api version &lt; 1.0 and input_fn(), but does not work with the new container used by sagemaker python api &gt; 2.0 that expects input_handler().<\/p>\n<p>The actual container image is &quot;763104351884.dkr.ecr.eu-central-1.amazonaws.com\/tensorflow-inference:1.15-gpu&quot;.<\/p>\n<p>During inference, I get the following error in CloudWatch thrown by the container:<\/p>\n<pre><code>ERROR:python_service:exception handling request: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(\n\nTraceback (most recent call last):\n  File &quot;\/sagemaker\/python_service.py&quot;, line 289, in _handle_invocation_post\n    res.body, res.content_type = self._handlers(data, context)\n  File &quot;\/sagemaker\/python_service.py&quot;, line 322, in handler\n    response = requests.post(context.rest_uri, data=processed_input)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/api.py&quot;, line 116, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/api.py&quot;, line 60, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/sessions.py&quot;, line 512, in request\n    data=data or \n{}\n,\n<\/code><\/pre>\n<p>What is the correct return type? All examples I found were for json &amp; text...<\/p>",
        "Challenge_closed_time":1600259722303,
        "Challenge_comment_count":3,
        "Challenge_created_time":1599496754993,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":1599548555527,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63781356",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":10.6,
        "Challenge_reading_time":26.28,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":211.9353638889,
        "Challenge_title":"How to correctly write a sagemaker tensorflow input_handler() that returns a numpy array?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":636,
        "Challenge_word_count":222,
        "Platform":"Stack Overflow",
        "Poster_created_time":1254829817772,
        "Poster_location":"Germany",
        "Poster_reputation_count":2595.0,
        "Poster_view_count":357.0,
        "Solution_body":"<p>This seems to work:<\/p>\n<p><code>return json.dumps({&quot;inputs&quot;: df.tolist() }).<\/code><\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"return numpi arrai json object return json dump input tolist",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"return json dump input tolist",
        "Solution_preprocessed_content":null,
        "Solution_readability":8.8,
        "Solution_reading_time":1.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":8.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1508924024027,
        "Answerer_location":null,
        "Answerer_reputation_count":118.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":504.0913175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have registered a scikit learn model on my MLflow Tracking server, and I am loading it with <code>sklearn.load_model(model_uri)<\/code>.<\/p>\n<p>Now, I would like to access the signature of the model so I can get a list of the model's required inputs\/features so I can retrieve them from my feature store by name. I can't seem to find any utility or method in the <code>mlflow<\/code> API or the <code>MLFlowClient<\/code> API that will let me access a signature or inputs\/outputs attribute, even though I can see a list of inputs and outputs under each version of the model in the UI.<\/p>\n<p>I know that I can find the input sample and the model configuration in the model's artifacts, but that would require me actually downloading the artifacts and loading them manually in my script. I don't need to avoid that, but I am surprised that I can't just return the signature as a dictionary the same way I can return a run's parameters or metrics.<\/p>",
        "Challenge_closed_time":1645469817663,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643655088920,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70931309",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":12.43,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":504.0913175,
        "Challenge_title":"How to retrieve the model signature from the MLflow Model Registry",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":904,
        "Challenge_word_count":173,
        "Platform":"Stack Overflow",
        "Poster_created_time":1466188731112,
        "Poster_location":"Michigan",
        "Poster_reputation_count":414.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>The way to access the model's signature without downloading the MLModel file is under the loaded model. And then you'll access the model's attributes, such as its signature or even other Pyfunc-defined methods.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmodel = mlflow.pyfunc.load_model(&quot;runs:\/&lt;run_id&gt;\/model&quot;)\nprint(model._model_meta._signature)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":14.5,
        "Solution_reading_time":5.3,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1443426419048,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Challenge_adjusted_solved_time":133.5554527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Azure ML support says to me that delimiter must be comma, this would cause too much hassle with data having semicolon as separator and with a lot of commas in the cell values. <\/p>\n\n<p>So how to process semicolon separated CSV files in Azure ML? <\/p>",
        "Challenge_closed_time":1485838771543,
        "Challenge_comment_count":1,
        "Challenge_created_time":1485357971913,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41855344",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":3.94,
        "Challenge_score_count":5,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":133.5554527778,
        "Challenge_title":"Azure ML: How to save and process CSV files with semicolon as delimiter?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":2908,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Poster_created_time":1251372839052,
        "Poster_location":null,
        "Poster_reputation_count":48616.0,
        "Poster_view_count":3348.0,
        "Solution_body":"<p>Azure ML only accepts the comma <code>,<\/code> separated CSV. Do a little work around.\nOpen your data file using a text editor. (Notepad will do the trick). Find and replace all semicolons with 'tab' (Make it a TSV) and the commas in data values may not occur a problem then. Make sure to define that the input is a TSV; not a CSV. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"open semicolon separ csv file text editor replac semicolon tab creat tab separ file tsv workaround tsv file import defin input tsv csv",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"accept comma separ csv littl open data file text editor notepad trick replac semicolon tab tsv comma data valu defin input tsv csv",
        "Solution_preprocessed_content":"accept comma separ csv littl open data file text editor replac semicolon tab comma data valu defin input tsv csv",
        "Solution_readability":3.9,
        "Solution_reading_time":4.05,
        "Solution_score_count":7.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":285.6506852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an endpoint in <code>us-east<\/code> which serves a custom imported model (docker image).<\/p>\n<p>This endpoint uses <code>min replicas = 1<\/code> and <code>max replicas = 100<\/code>.<\/p>\n<p>Sometimes, Vertex AI will require the model to scale from 1 to 2.<\/p>\n<p>However, there seems to be an issue causing the number of replicas to go from <code>1 -&gt; 0 -&gt; 2<\/code> instead of <code>1 -&gt; 2<\/code>.<\/p>\n<p>This causes several 504 (Gateway Timeout) errors in my API and the way to solve that was setting <code>min replicas &gt; 1<\/code>, highly impacting the monthly cost of the application.<\/p>\n<p>Is this some known issue to Vertex AI\/GCP services, is there anyway to fix it?<\/p>",
        "Challenge_closed_time":1641201707430,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640176354140,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70449117",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":9.52,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":284.8203583334,
        "Challenge_title":"Vertex AI Endpoints scales to 0 before increasing number of replicas",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":77,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400036379907,
        "Poster_location":"Rio de Janeiro - RJ, Brasil",
        "Poster_reputation_count":490.0,
        "Poster_view_count":65.0,
        "Solution_body":"<p>The intermittent <code>504<\/code> errors could be a result of an endpoint that is under-provisioned to handle the load. It can also happen if too many prediction requests are sent to the endpoint before it has a chance to scale up.<\/p>\n<p>Traffic splitting of the incoming prediction requests is done randomly. So, multiple requests may end up on the same model server at the same time. This can happen even if the overall Queries Per Second (QPS) is low, and especially when the QPS is spiky. This contributes to the requests being queued up if the model server isn't able to handle the load. This is what results in a 504 error.<\/p>\n<p><strong>Recommendations to mitigate the <code>504<\/code> errors are as follows:<\/strong><\/p>\n<ul>\n<li><p>Improve the container's ability to use all resources in the container. One thing to keep in mind about resource utilization is whether the model server is single-threaded or multi-threaded. The container may not be using up all the cores and\/or requests may be queuing up, hence are served only one-at-a-time.<\/p>\n<\/li>\n<li><p>Autoscaling is happening, it just might need to be tuned to the prediction workload and expectations. A lower utilization threshold would trigger autoscaling sooner.<\/p>\n<\/li>\n<li><p>Perform an exponential backoff while the deployment is scaling. This way, there is a retry mechanism to handle failed requests.<\/p>\n<\/li>\n<li><p>Provision a higher minimum replica count for the endpoint, which you have already implemented.<\/p>\n<\/li>\n<\/ul>\n<p>If the above recommendations do not solve the problem or in general require further investigation of these errors, please reach out to <a href=\"https:\/\/cloud.google.com\/support-hub#section-2\" rel=\"nofollow noreferrer\">GCP support<\/a> in case you have a <a href=\"https:\/\/cloud.google.com\/support\/\" rel=\"nofollow noreferrer\">support plan<\/a>. Otherwise, please open an issue in the <a href=\"https:\/\/issuetracker.google.com\/issues\/new?component=1134259&amp;template=1640573\" rel=\"nofollow noreferrer\">issue tracker<\/a>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"mitig scale endpoint improv resourc util tune autosc perform exponenti backoff provis higher minimum replica count reach open tracker",
        "Solution_last_edit_time":1641204696607,
        "Solution_link_count":3.0,
        "Solution_original_content":"intermitt endpoint provis load predict request sent endpoint chanc scale traffic split incom predict request randomli multipl request end model server time overal queri qp low qp spiki contribut request queu model server isn load mitig improv abil resourc resourc util model server singl thread multi thread core request queu serv time autosc tune predict workload lower util threshold trigger autosc sooner perform exponenti backoff deploy scale retri mechan request provis higher minimum replica count endpoint implement gener reach plan open tracker",
        "Solution_preprocessed_content":"intermitt endpoint load predict request sent endpoint chanc scale traffic split incom predict request randomli multipl request end model server time overal queri low qp spiki contribut request queu model server isn load mitig improv abil resourc resourc util model server core request queu serv autosc tune predict workload lower util threshold trigger autosc sooner perform exponenti backoff deploy scale retri mechan request provis higher minimum replica count endpoint implement gener reach plan open tracker",
        "Solution_readability":10.0,
        "Solution_reading_time":25.75,
        "Solution_score_count":1.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":283.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":0.4228347222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am triggering a Step Function execution via a Python cell in a SageMaker Notebook, like this:<\/p>\n<pre><code>state_machine_arn = 'arn:aws:states:us-west-1:1234567891:stateMachine:alexanderMyPackageStateMachineE3411O13-A1vQWERTP9q9'\nsfn = boto3.client('stepfunctions')\n..\nsfn.start_execution(**kwargs)  # Non Blocking Call\nrun_arn = response['executionArn']\nprint(f&quot;Started run {run_name}. ARN is {run_arn}.&quot;)\n<\/code><\/pre>\n<p>and then in order to check that the execution (which might take hours to complete depending on the input) has been completed, before I start doing some custom post-analysis on the results, I manually execute a cell with:<\/p>\n<pre><code>response = sfn.list_executions(\n    stateMachineArn=state_machine_arn,\n    maxResults=1\n)\nprint(response)\n<\/code><\/pre>\n<p>where I can see from the output the status of the execution, e.g. <code>'status': 'RUNNING'<\/code>.<\/p>\n<p>How can I automate this, i.e. trigger the Step Function and continue the execution on my post-analysis custom logic only after the execution has finished? Is there for example a blocking call to start the execution, or a callback method I could use?<\/p>\n<p>I can think of putting a sleep method, so that the Python Notebook cell would periodically call <code>list_executions()<\/code> and check the status, and only when the execution is completed, continue to rest of the code. I can statistically determine the sleep period, but I was wondering if there is a simpler\/more accurate way.<\/p>\n<hr \/>\n<p>PS: Related: <a href=\"https:\/\/stackoverflow.com\/questions\/46878423\/how-to-avoid-simultaneous-execution-in-aws-step-function\">How to avoid simultaneous execution in aws step function<\/a>, however I would like to avoid creating any new AWS resource, just for this, I would like to do everything from within the Notebook.<\/p>\n<p>PPS: I cannot make any change to <code>MyPackage<\/code> and the Step Function definition.<\/p>",
        "Challenge_closed_time":1618568305232,
        "Challenge_comment_count":2,
        "Challenge_created_time":1618566783027,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1618585069787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67123040",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":12.6,
        "Challenge_reading_time":25.63,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.4228347222,
        "Challenge_title":"How to tell programmatically that an AWS Step Function execution has been completed?",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1216,
        "Challenge_word_count":251,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369257942212,
        "Poster_location":"London, UK",
        "Poster_reputation_count":70285.0,
        "Poster_view_count":13121.0,
        "Solution_body":"<p>Based on the comments.<\/p>\n<p>If no new resources are to be created (no CloudWatch Event rules, lambda functions) nor any changes to existing Step Function are allowed, then <strong>pooling iteratively<\/strong> <code>list_executions<\/code> would be the best solution.<\/p>\n<p>AWS CLI and boto3 have implemented similar solutions (not for Step Functions), but for some other services. They are called <code>waiters<\/code> (e.g. <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/ec2.html#waiters\" rel=\"nofollow noreferrer\">ec2 waiters<\/a>). So basically you would have to create your own <strong>waiter for Step Function<\/strong>, as AWS does not provide one for that. AWS uses <strong>15 seconds<\/strong> sleep time from what I recall for its waiters.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"pool iter list execut statu step function execut creat waiter step function waiter implement cli boto servic creat resourc simpler accur creat resourc",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"base comment resourc creat cloudwatch event rule lambda function step function allow pool iter list execut cli boto implement step function servic call waiter waiter creat waiter step function sleep time recal waiter",
        "Solution_preprocessed_content":"base comment resourc creat step function allow pool iter cli boto implement servic call creat waiter step function sleep time recal waiter",
        "Solution_readability":11.1,
        "Solution_reading_time":10.19,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":97.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1370074627432,
        "Answerer_location":"Munich, Germany",
        "Answerer_reputation_count":51580.0,
        "Answerer_view_count":11462.0,
        "Challenge_adjusted_solved_time":1.0600122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using AWS SageMaker. I already used it before and I had no problems reading data from an S3 bucket.\nSo, I set up a new notebook instance and id this:<\/p>\n<pre><code>from sagemaker import get_execution_role\nrole = get_execution_role()\n\nbucket='my-bucket'\n\ndata_key = 'myfile.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n\ndf = pd.read_csv(data_location)\n<\/code><\/pre>\n<p>What I got is this:<\/p>\n<pre><code>PermissionError: Access Denied\n<\/code><\/pre>\n<p>Note: I checked the IAM Roles and also the policies and it seems to me that I have all the necessary rights to access the S3 bucket (AmazonS3FullAccess etc. are granted). What is different from the situation before is that my data is encrypted. Is there something I have to set up besides the roles?<\/p>\n<p>Edit:<\/p>\n<p>The role I use consist of three policies. These are<\/p>\n<ul>\n<li>AmazonS3FullAccess<\/li>\n<li>AmazonSageMakerFullAccess<\/li>\n<\/ul>\n<p>and an Execution Role where I added kms:encrypt and kms:decrypt. It looks like this one:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;xyz&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:PutObject&quot;,\n                &quot;s3:GetObject&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:DeleteObject&quot;,\n                &quot;kms:Encrypt&quot;,\n                &quot;kms:Decrypt&quot;\n            ],\n            &quot;Resource&quot;: &quot;arn:aws:s3:::*&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>Is there something missing?<\/p>",
        "Challenge_closed_time":1616081046907,
        "Challenge_comment_count":7,
        "Challenge_created_time":1616075760093,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1616077230863,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66692579",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":8,
        "Challenge_readability":12.4,
        "Challenge_reading_time":19.88,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1.4685594445,
        "Challenge_title":"AWS SageMaker: PermissionError: Access Denied - Reading data from S3 bucket",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":1327,
        "Challenge_word_count":172,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559131080072,
        "Poster_location":null,
        "Poster_reputation_count":1166.0,
        "Poster_view_count":248.0,
        "Solution_body":"<p>You need to add (or modify) an IAM policy to grant access to the key the bucket uses for its encryption:<\/p>\n<pre><code>{\n  &quot;Sid&quot;: &quot;KMSAccess&quot;,\n  &quot;Action&quot;: [\n    &quot;kms:Decrypt&quot;\n  ],\n  &quot;Effect&quot;: &quot;Allow&quot;,\n  &quot;Resource&quot;: &quot;arn:aws:kms:example-region-1:123456789098:key\/111aa2bb-333c-4d44-5555-a111bb2c33dd&quot;\n}\n<\/code><\/pre>\n<p>Alternatively you can change the key policy of the KMS key directly to grant the sagemaker role access directly. <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-bucket-access-default-encryption\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-bucket-access-default-encryption\/<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"add modifi iam polici grant access kei bucket encrypt kei polici km kei directli grant role access directli",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"add modifi iam polici grant access kei bucket encrypt sid kmsaccess action km decrypt allow resourc arn km region kei aabb abbcdd kei polici km kei directli grant role access directli http com premiumsupport knowledg center bucket access default encrypt",
        "Solution_preprocessed_content":"add iam polici grant access kei bucket encrypt kei polici km kei directli grant role access directli",
        "Solution_readability":28.0,
        "Solution_reading_time":9.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1521276815912,
        "Answerer_location":null,
        "Answerer_reputation_count":731.0,
        "Answerer_view_count":41.0,
        "Challenge_adjusted_solved_time":0.2378675,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to download the best performing model for a certain ClearlML project. I have the following content in my ClearML experiment platform:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/LAWT9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LAWT9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>According to: <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/clearml_sdk\/model_sdk#querying-models\" rel=\"nofollow noreferrer\">https:\/\/clear.ml\/docs\/latest\/docs\/clearml_sdk\/model_sdk#querying-models<\/a> I can get a list of models for a specific project:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_list = Model.query_models(\n    # Only models from `examples` project\n    project_name='YOLOv5', \n    # Only models with input name\n    model_name=None,\n    # Only models with `demo` tag but without `TF` tag\n    tags=['demo', '-TF'],\n    # If `True`, only published models\n    only_published=False,\n    # If `True`, include archived models\n    include_archived=True,\n    # Maximum number of models returned\n    max_results=5\n)\n\nprint(model_list)\n<\/code><\/pre>\n<p>Which prints:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>[&lt;clearml.model.Model object at 0x7fefbaf22130&gt;, &lt;clearml.model.Model object at 0x7fefbaf22340&gt;]\n<\/code><\/pre>\n<p>So I can run:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_list[0].get_local_copy()\n<\/code><\/pre>\n<p>and get this specific model. But how do I download the best performing one for this project on a certain metric (in this case mAP_0.5:0.95 MAX)?<\/p>",
        "Challenge_closed_time":1662538633323,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662537777000,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1662645914387,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73632015",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":20.74,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.2378675,
        "Challenge_title":"ClearML, how to query the best performing model for a specific project and metric",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":48,
        "Challenge_word_count":157,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521276815912,
        "Poster_location":null,
        "Poster_reputation_count":731.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>I ended up doing the following:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>try:\n    import clearml\n    from clearml import Dataset, Task, Model, OutputModel\n    assert hasattr(clearml, '__version__')  # verify package import not local dir\nexcept (ImportError, AssertionError):\n    clearml = None\n\ntasks = Task.get_tasks(project_name='YOLOv5', task_name='exp', task_filter={'status': ['completed']})\n\nresults = {}\nbest_task = None\nfor task in tasks:\n    results[task.id] = task.get_last_scalar_metrics()['metrics']['mAP_0.5:0.95']['max']\n\nbest_model_task_id = max(results, key=results.get)\nmodel_list = Task.get_task(best_model_task_id).get_models()\ndest = model_list['output'][0].get_local_copy()\nprint('Saved model at:', dest)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"queri list model download perform base metric packag list complet task metric select task highest metric valu download output model",
        "Solution_last_edit_time":1662553432007,
        "Solution_link_count":0.0,
        "Solution_original_content":"end import import dataset task model outputmodel assert hasattr version verifi packag import local dir importerror assertionerror task task task yolov task exp task filter statu complet task task task task task scalar metric metric map model task kei model list task task model task model dest model list output local copi print save model dest",
        "Solution_preprocessed_content":null,
        "Solution_readability":15.3,
        "Solution_reading_time":9.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":58.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1342709052703,
        "Answerer_location":"D\u00fcsseldorf, Germany",
        "Answerer_reputation_count":1889.0,
        "Answerer_view_count":654.0,
        "Challenge_adjusted_solved_time":16.1329663889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was playing with the AWS instances and trying to deploy some locally trained Keras models, but I find no documentation on that. Has anyone already been able to do it? <\/p>\n\n<p>I tried to use a similar approach to <a href=\"https:\/\/aws.amazon.com\/pt\/blogs\/machine-learning\/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/pt\/blogs\/machine-learning\/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker\/<\/a>, but I had no success. I also found some examples for training keras models in the cloud, but I was not able to get the entry_point + artifacts right. <\/p>\n\n<p>Thanks for your time!<\/p>",
        "Challenge_closed_time":1538644957792,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538586879113,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52632388",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.3,
        "Challenge_reading_time":9.83,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":16.1329663889,
        "Challenge_title":"Is it possible to deploy a already trained Keras to Sagemaker?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1291,
        "Challenge_word_count":87,
        "Platform":"Stack Overflow",
        "Poster_created_time":1448029868996,
        "Poster_location":"Portugal",
        "Poster_reputation_count":260.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>Yes, it is possible, and yes, the official documentation is not much of help.\nHowever, I wrote an <a href=\"https:\/\/gnomezgrave.com\/2018\/07\/05\/using-a-custom-model-for-ml-inference-with-amazon-sagemaker\" rel=\"nofollow noreferrer\">article on that<\/a>, and I hope it will help you.<\/p>\n\n<p>Let me know if you need more details. Cheers!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"articl written articl explain deploi local train kera model",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"offici document wrote articl hope cheer",
        "Solution_preprocessed_content":"offici document wrote articl hope cheer",
        "Solution_readability":10.7,
        "Solution_reading_time":4.39,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1565528932887,
        "Answerer_location":"United Kingdom",
        "Answerer_reputation_count":1579.0,
        "Answerer_view_count":91.0,
        "Challenge_adjusted_solved_time":17.8840719444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am deploying 50 NLP models on Azure Container Instances via the Azure Machine Learning service. All 50 models are quite similar and have the same input\/output format with just the model implementation changing slightly. <\/p>\n\n<p>I want to write a generic score.py entry file and pass in the model name as a parameter. The interface method signature does not allow a parameter in the init() method of score.py, so I moved the model loading into the run method. I am assuming the init() method gets run once whereas Run(data) will get executed on every invocation, so this is possibly not ideal (the models are 1 gig in size)<\/p>\n\n<p>So how can I pass in some value to the init() method of my container to tell it what model to load? <\/p>\n\n<p>Here is my current, working code:<\/p>\n\n<pre><code>def init():\n\ndef loadModel(model_name):\n    model_path = Model.get_model_path(model_name)  \n    return fasttext.load_model(model_path)\n\ndef run(raw_data):\n    # extract model_name from raw_data omitted...\n    model = loadModel(model_name)\n\n    ...\n<\/code><\/pre>\n\n<p>but this is what I would like to do (which breaks the interface)<\/p>\n\n<pre><code>def init(model_name):\n    model = loadModel(model_name)\n\ndef loadModel(model_name):\n    model_path = Model.get_model_path(model_name)  \n    return fasttext.load_model(model_path)\n\ndef run(raw_data):\n    ...\n<\/code><\/pre>",
        "Challenge_closed_time":1572381894847,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572325608637,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58601697",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":17.45,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":15.6350583334,
        "Challenge_title":"How to pass in the model name during init in Azure Machine Learning Service?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":851,
        "Challenge_word_count":195,
        "Platform":"Stack Overflow",
        "Poster_created_time":1256089885500,
        "Poster_location":"Sydney, Australia",
        "Poster_reputation_count":4947.0,
        "Poster_view_count":531.0,
        "Solution_body":"<p>If you're looking to use the same deployed container and switch models between requests; it's not the preferred design choice for Azure machine learning service, we need to specify the model name to load during build\/deploy.<\/p>\n\n<p>Ideally, each deployed web-service endpoint should allow inference of one model only; with the model name defined before the container the image starts building\/deploying. <\/p>\n\n<p>It is mandatory that the entry script has both <code>init()<\/code> and <code>run(raw_data)<\/code> with those <strong>exact<\/strong> signatures. <\/p>\n\n<p>At the moment, we can't change the signature of <code>init()<\/code> method to take a parameter like in <code>init(model_name)<\/code>.  <\/p>\n\n<p>The only dynamic user input you'd ever get to pass into this web-service is via <code>run(raw_data)<\/code> method. As you have tried, given the size of your model passing it via run is not feasible. <\/p>\n\n<p><code>init()<\/code> is run first and only <strong>once<\/strong> after your web-service deploy. Even if <code>init()<\/code> took the <code>model_name<\/code> parameter, there isn't a straight forward way to call this method directly and pass your desired model name.<\/p>\n\n<hr>\n\n<p>But, one possible solution is: <\/p>\n\n<p>You can create params file like below and store the file in azure blob storage.<\/p>\n\n<p>Example runtime parameters generation script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pickle\n\nparams = {'model_name': 'YOUR_MODEL_NAME_TO_USE'}\n\nwith open('runtime_params.pkl', 'wb') as file:\n    pickle.dump(params, file)\n\n<\/code><\/pre>\n\n<p>You'll need to use <a href=\"https:\/\/github.com\/Azure\/azure-storage-python\" rel=\"nofollow noreferrer\">Azure Storage Python SDK<\/a> to write code that can read from your blob storage account. This also mentioned in the official docs <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#prepare-to-deploy\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Then you can access this from <code>init()<\/code> function in your score script. <\/p>\n\n<p>Example <code>score.py<\/code> script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azure.storage.blob import BlockBlobService\nimport pickle\n\ndef init():\n\n  global model\n\n  block_blob_service = BlockBlobService(connection_string='your_connection_string')\n\n  blob_item = block_blob_service.get_blob_to_bytes('your-container-name','runtime_params.pkl')\n\n  params = pickle.load(blob_item.content)\n\n  model = loadModel(params['model_name'])\n<\/code><\/pre>\n\n<p>You can store connection strings in Azure KeyVault for secure access. Azure ML Workspaces comes with built-in KeyVault integration. More info <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.keyvault.keyvault?view=azure-ml-py#get-secret-name-\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>With this approach, you're abstracting runtime params config to another cloud location rather than the container itself. So you wouldn't need to re-build the image or deploy the web-service again. Simply restarting the container will work.<\/p>\n\n<hr>\n\n<p>If you're looking to simply re-use <code>score.py<\/code> (not changing code) for <strong>multiple model deployments in multiple containers<\/strong> then here's another possible solution.<\/p>\n\n<p>You can define your model name to use in web-service in a text file and read it in score.py. You'll need to pass this text file as a dependency when setting up the image config.<\/p>\n\n<p>This would, however, need multiple params files for each container deployment.<\/p>\n\n<p>Passing 'runtime_params.pkl' in <code>dependencies<\/code> to your image config (More detail example <a href=\"https:\/\/github.com\/rithinch\/heartfulness-similar-content-service\/blob\/master\/experiments\/notebooks\/Deploy%20Model%20-%20Azure.ipynb\" rel=\"nofollow noreferrer\">here<\/a>):<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>image_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n                                                  runtime=\"python\", \n                                                  conda_file=\"myenv.yml\",\n                                                  dependencies=[\"runtime_params.pkl\"],\n                                                  docker_file=\"Dockerfile\")\n<\/code><\/pre>\n\n<p>Reading this in your score.py <code>init()<\/code> function:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def init():\n\n  global model\n\n  with open('runtime_params.pkl', 'rb') as file:\n    params = pickle.load(file)\n\n  model = loadModel(params['model_name'])\n\n<\/code><\/pre>\n\n<p>Since your creating a new image config with this approach, you'll need to build the image and re-deploy the service.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"creat param file store blob storag runtim paramet gener creat param file access init function score defin model text file read score pass text file depend set imag config multipl param file deploy",
        "Solution_last_edit_time":1572389991296,
        "Solution_link_count":4.0,
        "Solution_original_content":"deploi switch model request prefer design choic servic specifi model load build deploi ideal deploi web servic endpoint allow infer model model defin imag start build deploi mandatori entri init run raw data exact signatur moment signatur init paramet init model dynam input pass web servic run raw data tri size model pass run feasibl init run web servic deploi init took model paramet isn straight forward directli pass desir model creat param file store file blob storag runtim paramet gener import pickl param model model open runtim param pkl file pickl dump param file storag sdk write read blob storag account offici doc access init function score score storag blob import blockblobservic import pickl init global model block blob servic blockblobservic connect connect blob item block blob servic blob byte runtim param pkl param pickl load blob item model loadmodel param model store connect keyvault secur access workspac come built keyvault integr abstract runtim param config cloud locat build imag deploi web servic simpli restart simpli score multipl model deploy multipl defin model web servic text file read score pass text file depend set imag config multipl param file deploy pass runtim param pkl depend imag config imag config containerimag imag configur execut score runtim conda file myenv yml depend runtim param pkl docker file dockerfil read score init function init global model open runtim param pkl file param pickl load file model loadmodel param model creat imag config build imag deploi servic",
        "Solution_preprocessed_content":"deploi switch model request prefer design choic servic specifi model load ideal deploi endpoint allow infer model model defin imag start mandatori entri exact signatur moment signatur paramet dynam input pass tri size model pass run feasibl run deploi took paramet isn straight forward directli pass desir model creat param file store file blob storag runtim paramet gener storag sdk write read blob storag account offici doc access function score store connect keyvault secur access workspac come keyvault integr abstract runtim param config cloud locat imag deploi simpli restart simpli multipl model deploy multipl defin model text file read pass text file depend set imag config multipl param file deploy pass imag config read function creat imag config build imag servic",
        "Solution_readability":12.7,
        "Solution_reading_time":58.32,
        "Solution_score_count":3.0,
        "Solution_sentence_count":40.0,
        "Solution_word_count":476.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1424548126510,
        "Answerer_location":"India",
        "Answerer_reputation_count":665.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":129.3699730556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My understanding is Dev Endpoints in AWS Glue can be used to develop code iteratively and then deploy it to a Glue job. I find this specially useful when developing Spark jobs because every time you run a job, it takes several minutes to launch a Hadoop cluster in the background. However, I am seeing a discrepancy when using Python shell in Glue instead of Spark. <code>Import pg<\/code> doesn't work in a Dev Endpoint I created using Sagemaker JupyterLab Python notebook, but works in AWS Glue when I create a job using Python shell. Shouldn't the same libraries exist in the dev endpoint that exist in Glue? What is the point of having a dev endpoint if you cannot reproduce the same code in both places (dev endpoint and the Glue job)?<\/p>",
        "Challenge_closed_time":1552530714763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1552064664580,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1552064982860,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55067802",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":9.67,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":129.4583841667,
        "Challenge_title":"Discrepancy between AWS Glue and its Dev Endpoint",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":261,
        "Challenge_word_count":139,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462207957683,
        "Poster_location":null,
        "Poster_reputation_count":1305.0,
        "Poster_view_count":174.0,
        "Solution_body":"<p>Firstly, Python shell jobs would not launch a Hadooo Cluster in the backend as it does not give you a Spark environment for your jobs.\nSecondly, since PyGreSQL is not written in Pure Python, it will not work with Glue's native environment (Glue Spark Job, Dev endpoint etc)\nThirdly, Python Shell has additional support for certain package built-in.<\/p>\n\n<p>Thus, I don't see a point of using DevEndpoint for Python Shell jobs.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"shell job launch cluster backend spark environ job pygresql written pure glue nativ environ glue spark job dev endpoint shell addit packag built devendpoint shell job",
        "Solution_preprocessed_content":"shell job launch cluster backend spark environ job pygresql written pure glue nativ environ shell addit packag devendpoint shell job",
        "Solution_readability":10.2,
        "Solution_reading_time":5.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1266513014020,
        "Answerer_location":null,
        "Answerer_reputation_count":2280.0,
        "Answerer_view_count":136.0,
        "Challenge_adjusted_solved_time":487.0128825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to cache part of the stream executed successfully (marked as tick) in Azure ML so that next time run will start from the same point onwards.\nAny help is appreciable.<\/p>",
        "Challenge_closed_time":1429645020147,
        "Challenge_comment_count":0,
        "Challenge_created_time":1427891773770,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1488123478287,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/29391016",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":2.89,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":487.0128825,
        "Challenge_title":"Cache part of experiment in AzureML same as SPSS modeler?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":60,
        "Challenge_word_count":41,
        "Platform":"Stack Overflow",
        "Poster_created_time":1427358932900,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I believe Azure ML already does this.  When you run it the second time, if nothing upstream from that tick has changed it should just load the results from the previous run.  It may take a few seconds for Azure ML to recognize that it is cached and reload it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"cach previou run upstream load cach recogn cach reload",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"believ run time upstream tick load previou run recogn cach reload",
        "Solution_preprocessed_content":"believ run time upstream tick load previou run recogn cach reload",
        "Solution_readability":5.1,
        "Solution_reading_time":3.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1264671735676,
        "Answerer_location":"San Francisco, CA",
        "Answerer_reputation_count":8619.0,
        "Answerer_view_count":1286.0,
        "Challenge_adjusted_solved_time":90.8566333334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to use a post-startup script to create a Vertex AI User Managed Notebook whose Jupyter Lab has a dedicated virtual environment and corresponding computing kernel when first launched. I have had success creating the instance and then, as a second manual step from within the Jupyter Lab &gt; Terminal, running a bash script like so:<\/p>\n<pre><code>#!\/bin\/bash\ncd \/home\/jupyter\nmkdir -p env\ncd env\npython3 -m venv envName --system-site-packages\nsource envName\/bin\/activate\nenvName\/bin\/python3 -m pip install --upgrade pip\npython -m ipykernel install --user --name=envName\npip3 install geemap --user \npip3 install earthengine-api --user \npip3 install ipyleaflet --user \npip3 install folium --user \npip3 install voila --user \npip3 install jupyterlab_widgets\ndeactivate\njupyter labextension install --no-build @jupyter-widgets\/jupyterlab-manager jupyter-leaflet\njupyter lab build --dev-build=False --minimize=False\njupyter labextension enable @jupyter-widgets\/jupyterlab-manager\n<\/code><\/pre>\n<p>However, I have not had luck using this code as a post-startup script (being supplied through the console creation tools, as opposed to command line, thus far). When I open Jupyter Lab and look at the relevant structures, I find that there is no environment or kernel. Could someone please provide a working example that accomplishes my aim, or otherwise describe the order of build steps that one would follow?<\/p>",
        "Challenge_closed_time":1662967888203,
        "Challenge_comment_count":2,
        "Challenge_created_time":1662640804323,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73649262",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":17.8,
        "Challenge_reading_time":18.96,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":90.8566333334,
        "Challenge_title":"Create custom kernel via post-startup script in Vertex AI User Managed notebook",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":85,
        "Challenge_word_count":206,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459350905808,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>Post startup scripts run as root.\nWhen you run:<\/p>\n<pre><code>python -m ipykernel install --user --name=envName\n<\/code><\/pre>\n<p>Notebook is using current user which is <code>root<\/code> vs when you use Terminal, which is running as <code>jupyter<\/code> user.<\/p>\n<p>Option 1) Have 2 scripts:<\/p>\n<ul>\n<li>Script A. Contents specified in original post. Example: <code>gs:\/\/newsml-us-central1\/so73649262.sh<\/code><\/li>\n<li>Script B. Downloads script and execute it as <code>jupyter<\/code>. Example: <code>gs:\/\/newsml-us-central1\/so1.sh<\/code> and use it as post-startup script.<\/li>\n<\/ul>\n<pre><code>#!\/bin\/bash\n\nset -x\n\ngsutil cp gs:\/\/newsml-us-central1\/so73649262.sh \/home\/jupyter\nchown jupyter \/home\/jupyter\/so73649262.sh\nchmod a+x \/home\/jupyter\/so73649262.sh\nsu -c '\/home\/jupyter\/so73649262.sh' jupyter\n<\/code><\/pre>\n<p>Option 2) Create a file in bash using EOF. Write the contents into a single file and execute it as mentioned above.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_gpt_summary":"convers involv creat origin startup download execut jupyt involv creat singl file bash eof write singl file execut",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"startup run root run ipykernel instal envnam notebook root termin run jupyt option specifi origin newsml central download execut jupyt newsml central startup bin bash set gsutil newsml central home jupyt chown jupyt home jupyt chmod home jupyt home jupyt jupyt option creat file bash eof write singl file execut",
        "Solution_preprocessed_content":"startup run root run notebook termin run option specifi origin download execut option creat file bash eof write singl file execut",
        "Solution_readability":7.5,
        "Solution_reading_time":12.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":108.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1631803441500,
        "Answerer_location":"mexico",
        "Answerer_reputation_count":1258.0,
        "Answerer_view_count":685.0,
        "Challenge_adjusted_solved_time":28.3038852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been using <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/08-model-monitoring.ipynb\" rel=\"nofollow noreferrer\">this<\/a> example of creating a Vertex AI monitoring job. It sends an email.<\/p>\n<pre><code>alerting_config = vertex_ai_beta.ModelMonitoringAlertConfig(\nemail_alert_config=vertex_ai_beta.ModelMonitoringAlertConfig.EmailAlertConfig(\n    user_emails=NOTIFY_EMAILS\n)\n<\/code><\/pre>\n<p>Is there any way to instead send a Pubsub message?<\/p>",
        "Challenge_closed_time":1660246628340,
        "Challenge_comment_count":1,
        "Challenge_created_time":1660144734353,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73308825",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":18.1,
        "Challenge_reading_time":7.49,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":28.3038852778,
        "Challenge_title":"Can Vertex AI model monitoring job send a pubsub message instead of email?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":86,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>You can configure the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/using-model-monitoring#set-up-alerts\" rel=\"nofollow noreferrer\">alert to be sent to Cloud Logging<\/a>. To enable Cloud Logging alerts you have to set the <code>enableLogging<\/code> field on your <code>ModelMonitoringAlertConfig<\/code> configuration to <code>TRUE<\/code>. Then you can forward the logs to any service that Cloud Logging supports, Pub\/Sub is one of these.<\/p>\n<p>For this you\u2019ll need one of the following permissions:<\/p>\n<ul>\n<li>Owner (roles\/owner)<\/li>\n<li>Logging Admin (roles\/logging.admin)<\/li>\n<li>Logs Configuration Writer (roles\/logging.configWriter)<\/li>\n<\/ul>\n<p>Then you need to <a href=\"https:\/\/cloud.google.com\/logging\/docs\/export\/configure_export_v2#creating_sink\" rel=\"nofollow noreferrer\">create a sink<\/a>.<\/p>\n<p>After that you have created the sink you\u2019ll need to set the <a href=\"https:\/\/cloud.google.com\/logging\/docs\/export\/configure_export_v2#dest-auth\" rel=\"nofollow noreferrer\">destination permissions<\/a>.<\/p>\n<p>While Cloud Logging provides you with the ability to exclude logs from being ingested, you might want to consider keeping logs that help with supportability. Using these logs can help you quickly troubleshoot and identify issues with your applications.<\/p>\n<p>Logs routed to Pub\/Sub are generally available within seconds, with 99% of logs available in less than 60 seconds.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"configur alert sent cloud log forward log pub sub set enablelog field modelmonitoringalertconfig configur creat sink permiss creat sink set destin permiss log rout pub sub gener",
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"configur alert sent cloud log enabl cloud log alert set enablelog field modelmonitoringalertconfig configur forward log servic cloud log pub sub youll permiss owner role owner log admin role log admin log configur writer role log configwrit creat sink creat sink youll set destin permiss cloud log abil exclud log ingest keep log log quickli troubleshoot identifi log rout pub sub gener log",
        "Solution_preprocessed_content":"configur alert sent cloud log enabl cloud log alert set field configur forward log servic cloud log youll permiss owner log admin log configur writer creat sink creat sink youll set destin permiss cloud log abil exclud log ingest keep log log quickli troubleshoot identifi log rout gener log",
        "Solution_readability":12.7,
        "Solution_reading_time":18.63,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":155.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":1.6854797222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am deploying a trained model to an ACI endpoint on Azure Machine Learning, using the Python SDK.\nI have created my score.py file, but I would like that file to be called with an argument being passed (just like with a training file) that I can interpret using <code>argparse<\/code>.\nHowever, I don't seem to find how I can pass arguments\nThis is the code I have to create the InferenceConfig environment and which obviously does not work.  Should I fall back on using the extra Docker file steps or so?<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment('my_hosted_environment')\nenv.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['scikit-learn'],\n    pip_packages=['azureml-defaults'])\nscoring_script = 'score.py --model_name ' + model_name\ninference_config = InferenceConfig(entry_script=scoring_script, environment=env)\n<\/code><\/pre>\n\n<p>Adding the score.py for reference on how I'd love to use the arguments in that script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#removed imports\nimport argparse\n\ndef init():\n    global model\n\n    parser = argparse.ArgumentParser(description=\"Load sklearn model\")\n    parser.add_argument('--model_name', dest=\"model_name\", required=True)\n    args, _ = parser.parse_known_args()\n\n    model_path = Model.get_model_path(model_name=args.model_name)\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    try:\n        data = json.loads(raw_data)['data']\n        data = np.array(data)\n        result = model.predict(data)\n        return result.tolist()\n\n    except Exception as e:\n        result = str(e)\n        return result\n<\/code><\/pre>\n\n<p>Interested to hear your thoughts<\/p>",
        "Challenge_closed_time":1584005785480,
        "Challenge_comment_count":2,
        "Challenge_created_time":1583933260433,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1584005920356,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60637170",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":12.7,
        "Challenge_reading_time":23.59,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":20.1458463889,
        "Challenge_title":"How to pass arguments to scoring file when deploying a Model in AzureML",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1681,
        "Challenge_word_count":196,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>How to deploy using environments can be found here <a href=\"https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2FAzure%2FMachineLearningNotebooks%2Fblob%2Fmaster%2Fhow-to-use-azureml%2Fdeployment%2Fdeploy-to-cloud%2Fmodel-register-and-deploy.ipynb&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7Ce06d310b0447416ab46b08d7bc836a81%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637185146436156499&amp;sdata=uQo332dpjuiNqWFCguvs3Kgg7UUMN8MBEzLxTPyH4MM%3D&amp;reserved=0\" rel=\"nofollow noreferrer\">model-register-and-deploy.ipynb<\/a> .  InferenceConfig class accepts  source_directory and entry_script <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py#parameters\" rel=\"nofollow noreferrer\">parameters<\/a>, where source_directory  is a path to the folder that contains all files(score.py and any other additional files) to create the image. <\/p>\n\n<p>This <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">multi-model-register-and-deploy.ipynb<\/a> has code snippets on how to create InferenceConfig with source_directory and entry_script.<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\n\nmyenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\ninference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                       name='sklearn-mnist-svc',\n                       models=[model], \n                       inference_config=inference_config,\n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n\nprint(service.scoring_uri)\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":1584011988083,
        "Solution_link_count":3.0,
        "Solution_original_content":"deploi environ model regist deploi ipynb inferenceconfig class accept sourc directori entri paramet sourc directori path folder file score addit file creat imag multi model regist deploi ipynb creat inferenceconfig sourc directori entri core webservic import webservic core model import inferenceconfig core environ import environ myenv environ conda myenv file path myenv yml infer config inferenceconfig entri score environ myenv servic model deploi workspac sklearn mnist svc model model infer config infer config deploy config aciconfig servic wait deploy output print servic score uri",
        "Solution_preprocessed_content":"deploi environ inferenceconfig class accept paramet path folder addit file creat imag creat inferenceconfig",
        "Solution_readability":35.1,
        "Solution_reading_time":25.25,
        "Solution_score_count":-2.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":89.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1347532849952,
        "Answerer_location":"Tel Aviv, Israel",
        "Answerer_reputation_count":2529.0,
        "Answerer_view_count":172.0,
        "Challenge_adjusted_solved_time":1.6824691667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to download a file to sagemaker from my S3 bucket.<\/p>\n\n<p>the path of the file is\n<code>s3:\/\/vemyone\/input\/dicom-images-train\/1.2.276.0.7230010.3.1.2.8323329.1000.1517875165.878026\/1.2.276.0.7230010.3.1.3.8323329.1000.1517875165.878025\/1.2.276.0.7230010.3.1.4.8323329.1000.1517875165.878027.dcm<\/code><\/p>\n\n<p>The path of that file is stored as a list element at <code>train_fns[0]<\/code>.<\/p>\n\n<p>the value of <code>train_fns[0]<\/code> is <\/p>\n\n<p><code>input\/dicom-images-train\/1.2.276.0.7230010.3.1.2.8323329.1000.1517875165.878026\/1.2.276.0.7230010.3.1.3.8323329.1000.1517875165.878025\/1.2.276.0.7230010.3.1.4.8323329.1000.1517875165.878027.dcm<\/code><\/p>\n\n<p>I used the following code:<\/p>\n\n<pre><code>s3 = boto3.resource('s3')\nbucketname = 'vemyone'\n\ns3.Bucket(bucketname).download_file(train_fns[0][:], train_fns[0])\n<\/code><\/pre>\n\n<p>but I get the following error:<\/p>\n\n<p><code>FileNotFoundError: [Errno 2] No such file or directory: 'input\/dicom-images-train\/1.2.276.0.7230010.3.1.2.8323329.1000.1517875165.878026\/1.2.276.0.7230010.3.1.3.8323329.1000.1517875165.878025\/1.2.276.0.7230010.3.1.4.8323329.1000.1517875165.878027.dcm.5b003ba1'<\/code><\/p>\n\n<p>I notice that some characters have appended itself at the end of the path.<\/p>\n\n<p>how do I solve this problem?<\/p>",
        "Challenge_closed_time":1562759076923,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562758462203,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56969859",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":18.32,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.1707555556,
        "Challenge_title":"AWS: FileNotFoundError: [Errno 2] No such file or directory",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":14766,
        "Challenge_word_count":95,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521737834360,
        "Poster_location":"Pondicherry, Puducherry, India",
        "Poster_reputation_count":1303.0,
        "Poster_view_count":139.0,
        "Solution_body":"<p>please see <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Bucket.download_file\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Bucket.download_file<\/a><\/p>\n\n<p>by the doc, first argument is file key, second argument is path for local file:<\/p>\n\n<pre><code>s3 = boto3.resource('s3')\nbucketname = 'vemyone'\n\ns3.Bucket(bucketname).download_file(train_fns[0], '\/path\/to\/local\/file')\n<\/code><\/pre>",
        "Solution_comment_count":8.0,
        "Solution_gpt_summary":"download file boto librari download file argument file kei argument path local file",
        "Solution_last_edit_time":1562764519092,
        "Solution_link_count":2.0,
        "Solution_original_content":"http boto amazonaw com document api latest servic html bucket download file doc argument file kei argument path local file boto resourc bucketnam vemyon bucket bucketnam download file train fn path local file",
        "Solution_preprocessed_content":"doc argument file kei argument path local file",
        "Solution_readability":30.3,
        "Solution_reading_time":6.99,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1327588060552,
        "Answerer_location":null,
        "Answerer_reputation_count":802.0,
        "Answerer_view_count":91.0,
        "Challenge_adjusted_solved_time":599.2239861111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In my pipeline multiple steps are independent and so I would like them to run in parallel based on input dependencies.<\/p>\n<p>As the compute I use has multiple nodes I would have expected this to be the default.<\/p>\n<p>For example:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Iye85.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iye85.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>All 3 upper steps should run in parallel, then both <code>finetune<\/code> steps in parallel as soon as their inputs are satisfied and the same for <code>rgb_test<\/code>.<\/p>\n<p>Currently only 1 step runs at a time, the other are <code>Queued<\/code>.<\/p>",
        "Challenge_closed_time":1632769372630,
        "Challenge_comment_count":2,
        "Challenge_created_time":1630612166280,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69036277",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.17,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":599.2239861111,
        "Challenge_title":"Run independent `PythonScriptStep` steps in parallel",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":156,
        "Challenge_word_count":93,
        "Platform":"Stack Overflow",
        "Poster_created_time":1327588060552,
        "Poster_location":null,
        "Poster_reputation_count":802.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p>It ended up being because of vCPU quota.<\/p>\n<p>After increasing the quota, parallel tasks can run at the same time as expected.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"vcpu quota increas quota allow parallel task run time",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"end vcpu quota increas quota parallel task run time",
        "Solution_preprocessed_content":"end vcpu quota increas quota parallel task run time",
        "Solution_readability":6.4,
        "Solution_reading_time":1.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1341273154903,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":5996.0,
        "Answerer_view_count":666.0,
        "Challenge_adjusted_solved_time":0.1159838889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running an ML pipeline and the training component\/step (see code below) continues to fail with the following error: &quot;RuntimeError: Unknown return type: &lt;class 'inspect._empty'&gt;. Must be one of <code>str<\/code>, <code>int<\/code>, <code>float<\/code>, a subclass of <code>Artifact<\/code>, or a NamedTuple collection of these types.&quot;<\/p>\n<p>Any ideas on what might be causing the issue\/error and how to resolve it?<\/p>\n<p>Thank you!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>\n@component(\n    # this component builds an xgboost classifier with xgboost\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;\n)\n\ndef build_xgb_xgboost(project: str, \n                            bq_dataset: str, \n                            test_view_name: str,\n                            bq_location: str,\n                            metrics: Output[Metrics],\n                            model: Output[Model]\n\n):\n    from google.cloud import bigquery\n    import xgboost as xgb\n    import pandas as pd\n    from xgboost import XGBRegressor\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.metrics import mean_squared_error as MSE\n    from sklearn.metrics import mean_absolute_error\n    import joblib\n    import pyarrow\n    import db_dtypes\n     \n\n    client = bigquery.Client(project=project) \n\n    view_uri = f&quot;{project}.{bq_dataset}.{test_view_name}&quot; #replace view_name with test_view_name\n    \n    build_df_for_xgboost = '''\n    SELECT * FROM `{view_uri}`\n    '''.format(view_uri = view_uri)\n\n    job_config = bigquery.QueryJobConfig()\n    df_1 = client.query(build_df_for_xgboost).to_dataframe()\n    \n    #client.query(build_df_for_xgboost, job_config=job_config).to_dataframe()  \n    \n    df = df_1.drop(['int64_field_0'], axis=1)\n    \n    def onehot_encode(df, column):\n        df = df.copy()\n        dummies = pd.get_dummies(df[column], prefix=column)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    return df\n    \n    # Binary encoding\n    df['preferred_foot'] = df['preferred_foot'].replace({'left': 0, 'right': 1})\n    \n    # One-hot encoding\n    for column in ['attacking_work_rate', 'defensive_work_rate']:\n        df = onehot_encode(df, column=column)\n    \n    # Split df into X and y\n    y = df['overall_rating']\n    X = df.drop('overall_rating', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n\n    #specify parameters\n    \n    #define your model \n    bst = XGBRegressor(\n    objective='reg:linear',\n    learning_rate = '.1',\n    alpha = '0.001'\n    )\n    \n    #fit your model\n    bst.fit(X_train, y_train)\n    \n    # Predict the model \n    y_pred = bst.predict(X_test)\n    rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n    mae = mean_absolute_error(y_test, y_pred)\n    \n    metrics.log_metric(&quot;RMSE&quot;, rmse)\n    metrics.log_metric(&quot;framework&quot;, &quot;xgboost&quot;)\n    metrics.log_metric(&quot;dataset_size&quot;, len(df))\n    metrics.log_metric(&quot;MAE&quot;, mae)\n    \n    dump(bst, model.path + &quot;.joblib&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1659257407332,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655350561587,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1659256989790,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72640182",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.5,
        "Challenge_reading_time":44.17,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":37,
        "Challenge_solved_time":1085.2349291667,
        "Challenge_title":"Kubeflow Pipeline Training Component Failing | Unknown return type: <class 'inspect._empty'>",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":107,
        "Challenge_word_count":278,
        "Platform":"Stack Overflow",
        "Poster_created_time":1621620820567,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>I think this might just be a bug in the version of KFP v2 SDK code you're using.<\/p>\n<p>I mostly use the stable KFPv1 methods to avoid problems.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\n\n\ndef train_xgboost_model(\n    project: str, \n    bq_dataset: str, \n    test_view_name: str,\n    bq_location: str,\n    metrics_path: OutputPath(Metrics),\n    model_path: OutputPath(Model),\n):\n    import json\n    from pathlib import Path\n\n    metrics = {\n       ...\n    }\n    Path(metrics_path).write_text(json.dumps(metrics))\n\n    dump(bst, model_path)\n\ntrain_xgboost_model_op = create_component_from_func(\n    func=train_xgboost_model,\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;,\n)\n\n<\/code><\/pre>\n<p>You can also find many examples of real-world components in this repo: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components<\/a><\/p>\n<p>including an XGBoost trainer <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py<\/a><\/p>\n<p>and a full XGBoost pipeline: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_original_content":"version kfp sdk stabl kfpv avoid kfp compon import inputpath outputpath creat compon func train model str dataset str test str locat str metric path outputpath metric model path outputpath model import json pathlib import path metric path metric path write text json dump metric dump bst model path train model creat compon func func train model packag instal cloud bigqueri panda sklearn joblib pyarrow dtype base imag output compon file creat xgb model yaml world compon repo http github com ark kun pipelin compon tree master compon trainer http github com ark kun pipelin compon blob dccf compon train compon pipelin http github com ark kun pipelin compon blob fbefeaafbaddbea sampl cloud vertex train tabular regress model import vertex pipelin",
        "Solution_preprocessed_content":"version kfp sdk stabl kfpv avoid compon repo trainer pipelin",
        "Solution_readability":28.3,
        "Solution_reading_time":27.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":107.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1421401313787,
        "Answerer_location":null,
        "Answerer_reputation_count":326.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":7.6450513889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running the below code to store tags and then to retrieve them. As you can see below, Mlflow is storing one set of tags and returning another.<\/p>\n<pre><code>import mlflow\nwith mlflow.start_run() as active_run:\n    tw = { &quot;run_id&quot;: 1}\n    mlflow.set_tags(tw)            \n    print(&quot;Tags are &quot;, active_run.data.tags)\n    print(type(active_run.data.tags))\n<\/code><\/pre>\n<p>Output<\/p>\n<pre><code>Tags are  {'mlflow.source.name': '\/media\/Space\/AI\/anaconda4\/lib\/python3.7\/site-packages\/ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'adeel'}\n<\/code><\/pre>\n<p>Looking at the stored tags through mlflow ui, I can see that the tag &quot;run_id&quot; set by the code is actually stored in the run. However, only the header information of the run seems to be getting returned by active_run.data.tags.<\/p>",
        "Challenge_closed_time":1610128097448,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610100575263,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1611139529420,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65627039",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":11.12,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":7.6450513889,
        "Challenge_title":"MLflow stores tags but does not return them",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":173,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1445990517172,
        "Poster_location":"Sydney, New South Wales, Australia",
        "Poster_reputation_count":689.0,
        "Poster_view_count":87.0,
        "Solution_body":"<p>At the moment, you have to query your run again in MLflow to get the run with all the info that you logged. In the example below, I call <code>mlflow.get_run(&lt;run_id&gt;)<\/code> to achieve this.<\/p>\n<pre><code>import mlflow\n\n\nwith mlflow.start_run() as active_run:\n  tags = { &quot;my_tag&quot;: 1}\n  mlflow.set_tags(tags)            \n  # Keep track of the run ID of the active run\n  run_id = active_run.info.run_id\n\nrun = mlflow.get_run(run_id)\nprint(&quot;The tags are &quot;, run.data.tags)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":4.0,
        "Solution_reading_time":6.21,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":63.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":231.7294944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have two datasets with multiple columns. I would like to join the two tables with the following keys: zip code, year, month, data, hour<\/p>\n\n<p>However whenever I use a <strong>Join Module<\/strong> on these two tables, the Join doesn't happen, and I just get a Table with Columns from Right Table with empty values.<\/p>\n\n<p>Here is the R equivalent of what I am trying to do:<\/p>\n\n<pre><code>YX &lt;- leftTableDT\nYX %&lt;&gt;% merge( rightTableDT, all.x = TRUE, by=c('zip','year','month','day','hour') )\n<\/code><\/pre>\n\n<p>Any ideas on why Join Module in Azure ML Studio doesn't work for multiple keys?<\/p>",
        "Challenge_closed_time":1493816926630,
        "Challenge_comment_count":0,
        "Challenge_created_time":1492982700450,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/43576656",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.21,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":231.7294944445,
        "Challenge_title":"Join 2 tables with with mutiple keys in Azure ML Studio",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":582,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1281811007747,
        "Poster_location":"Capitola, CA",
        "Poster_reputation_count":3675.0,
        "Poster_view_count":638.0,
        "Solution_body":"<p>Double-check that you've selected \"Allow duplicates and preserve column order in selection\" in column selection options, so it matches the columns in listed order.<\/p>\n\n<p>Also, you could try Apply SQL Transformation module to join datasets.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"doubl allow duplic preserv column order select select column select option appli sql transform modul dataset",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"doubl select allow duplic preserv column order select column select option match column list order appli sql transform modul dataset",
        "Solution_preprocessed_content":"select allow duplic preserv column order select column select option match column list order appli sql transform modul dataset",
        "Solution_readability":12.5,
        "Solution_reading_time":3.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":35.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":6278.0877944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Python code generated from an ml software with mlflow to read a dataframe, perform some table operations and output a dataframe. I am able to run the code successfully and save the new dataframe as an artifact. However I am unable to log the model using log_model because it is not a lr or classifier model where we train and fit. I want to log a model for this so that it can be served with new data and deployed with a rest API<\/p>\n<pre><code>df = pd.read_csv(r&quot;\/home\/xxxx.csv&quot;)\n\n\nwith mlflow.start_run():\n    def getPrediction(row):\n        \n        perform_some_python_operaions \n\n        return [Status_prediction, Status_0_probability, Status_1_probability]\n    columnValues = []\n    for column in columns:\n        columnValues.append([])\n\n    for index, row in df.iterrows():\n        results = getPrediction(row)\n        for n in range(len(results)):\n            columnValues[n].append(results[n])\n\n    for n in range(len(columns)):\n        df[columns[n]] = columnValues[n]\n\n    df.to_csv('dataset_statistics.csv')\n    mlflow.log_artifact('dataset_statistics.csv')\n   \n<\/code><\/pre>",
        "Challenge_closed_time":1611592914947,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611586824463,
        "Challenge_favorite_count":3,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65887231",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":13.53,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.6918011111,
        "Challenge_title":"Use mlflow to serve a custom python model for scoring",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3026,
        "Challenge_word_count":134,
        "Platform":"Stack Overflow",
        "Poster_created_time":1573739890560,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>MLflow supports <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">custom models<\/a> of mlflow.pyfunc flavor.  You can create a custom  class  inherited from the <code>mlflow.pyfunc.PythonModel<\/code>, that needs to provide function <code>predict<\/code> for performing predictions, and optional <code>load_context<\/code> to load the necessary artifacts, like this (adopted from the docs):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class MyModel(mlflow.pyfunc.PythonModel):\n\n    def load_context(self, context):\n        # load your artifacts\n\n    def predict(self, context, model_input):\n        return my_predict(model_input.values)\n<\/code><\/pre>\n<p>You can log to MLflow whatever artifacts you need for your models, define Conda environment if necessary, etc.<br \/>\nThen you can use <code>save_model<\/code> with your class to save your implementation, that could be loaded with <code>load_model<\/code> and do the <code>predict<\/code> using your model:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.pyfunc.save_model(\n        path=mlflow_pyfunc_model_path, \n        python_model=MyModel(), \n        artifacts=artifacts)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":1634187940523,
        "Solution_link_count":1.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":17.3,
        "Solution_reading_time":16.79,
        "Solution_score_count":9.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":118.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1533921190820,
        "Answerer_location":"New York, USA",
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":7532.4821652778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have successfully created SageMaker Studio- Status in service. However, it asks me to asks to assign users to it and I don't have any listed. Are these users from my IAM (I have many) or my organization (I have a couple).<\/p>\n\n<p>Where am I supposed to find these users?<\/p>",
        "Challenge_closed_time":1603758115467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576591296847,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1576641179672,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59375896",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":5.3,
        "Challenge_reading_time":3.88,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":7546.3385055556,
        "Challenge_title":"How to assign users in SageMaker Studio?",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":1029,
        "Challenge_word_count":56,
        "Platform":"Stack Overflow",
        "Poster_created_time":1451164178848,
        "Poster_location":"Bulgaria",
        "Poster_reputation_count":61.0,
        "Poster_view_count":9.0,
        "Solution_body":"<hr \/>\n<p>Did you setup SageMaker Studio to use AWS SSO or IAM for the authentication method?<\/p>\n<p>From what I have gathered, the SageMaker Studio users, when setup using IAM for the authentication method are not actually users. They just provide partitions within Studio for different work environments. You can then control access to these partitions using IAM policies for your IAM users \/ roles for federated users.<\/p>\n<p>Each Studio user has it's own URL to access that environment.<\/p>\n<p>Here is the SageMaker developer guide: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf#%5B%7B%22num%22%3A14340%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C533.986%2Cnull%5D\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf#%5B%7B%22num%22%3A14340%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C533.986%2Cnull%5D<\/a><\/p>\n<p>Page 36 discusses on-boarding with IAM.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"studio set iam authent partit studio environ access partit control iam polici iam role feder studio url access environ guid board iam page",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"setup studio sso iam authent gather studio setup iam authent partit studio environ control access partit iam polici iam role feder studio url access environ guid http doc com latest pdf num gen xyz cnull page board iam",
        "Solution_preprocessed_content":"setup studio sso iam authent gather studio setup iam authent partit studio environ control access partit iam polici iam role feder studio url access environ guid page iam",
        "Solution_readability":12.5,
        "Solution_reading_time":12.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":93.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1548341556520,
        "Answerer_location":"Mumbai, Maharashtra, India",
        "Answerer_reputation_count":2907.0,
        "Answerer_view_count":238.0,
        "Challenge_adjusted_solved_time":701.3743433333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm not able to upload statsmodels 0.9rc1 python package in Azure ML studio for Time series analysis.<\/p>\n\n<p>I have downloaded <a href=\"https:\/\/files.pythonhosted.org\/packages\/df\/6f\/df6cf5faecd8082ee23916ff45d396dfee5a1f17aa275da7bab4f5c8926a\/statsmodels-0.9.0rc1-cp36-cp36m-win_amd64.whl\" rel=\"nofollow noreferrer\">statsmodels 0.9rc1<\/a>, unzipped contents and added statsmodels folder and model.pkl file to zip folder.<\/p>\n\n<p>But, while uploading to Microsoft Azure ML studio it says <strong>failed to build schema and visualization<\/strong><\/p>\n\n<p>I'm using this external package in <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts\" rel=\"nofollow noreferrer\">Execute Python script<\/a><\/p>\n\n<p>PS: I have succesfully uploaded packages like Adal, dateutils etc.<\/p>",
        "Challenge_closed_time":1573144655863,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570616046857,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1570619708227,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58301879",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":11.75,
        "Challenge_score_count":2,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":702.3913905556,
        "Challenge_title":"Unable to upload statsmodels 0.9rc1 python package in Azure ML studio",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":141,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1548341556520,
        "Poster_location":"Mumbai, Maharashtra, India",
        "Poster_reputation_count":2907.0,
        "Poster_view_count":238.0,
        "Solution_body":"<p>I have switched to Azure Jupyter Notebook where I installed package using pip<\/p>\n\n<pre><code>!pip install statsmodels==0.9.0rc1\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"switch jupyt notebook instal statsmodel packag pip",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"switch jupyt notebook instal packag pip pip instal statsmodel",
        "Solution_preprocessed_content":"switch jupyt notebook instal packag pip",
        "Solution_readability":6.6,
        "Solution_reading_time":1.88,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1553088438367,
        "Answerer_location":"Tehran, Tehran Province, Iran",
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":68.1138777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For the first time, it is proceeding mlflow with port 5000.<\/p>\n<p>Testing Mlflow, problem is no attribute last_active_run in mlflow<\/p>\n<p>But, It was an example provided by Mlflow. <br \/>\nlink is here <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/sklearn_autolog\" rel=\"nofollow noreferrer\">mlflow<\/a><\/p>\n<p>What is problem and how can I change code?<\/p>\n<p>shell<\/p>\n<pre><code>wget https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/master\/examples\/sklearn_autolog\/utils.py\nwget https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/master\/examples\/sklearn_autolog\/pipeline.py\n<\/code><\/pre>\n<p>pipeline.py<\/p>\n<pre><code>from pprint import pprint\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nimport mlflow\nfrom utils import fetch_logged_data\n\n\ndef main():\n    # enable autologging\n    mlflow.sklearn.autolog()\n\n    # prepare training data\n    X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n    y = np.dot(X, np.array([1, 2])) + 3\n\n    # train a model\n    pipe = Pipeline([(&quot;scaler&quot;, StandardScaler()), (&quot;lr&quot;, LinearRegression())])\n    pipe.fit(X, y)\n    run_id = mlflow.last_active_run().info.run_id\n    print(&quot;Logged data and model in run: {}&quot;.format(run_id))\n\n    # show logged data\n    for key, data in fetch_logged_data(run_id).items():\n        print(&quot;\\n---------- logged {} ----------&quot;.format(key))\n        pprint(data)\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>utils.py<\/p>\n<pre><code>import mlflow\nfrom mlflow.tracking import MlflowClient\n\n\ndef yield_artifacts(run_id, path=None):\n    &quot;&quot;&quot;Yield all artifacts in the specified run&quot;&quot;&quot;\n    client = MlflowClient()\n    for item in client.list_artifacts(run_id, path):\n        if item.is_dir:\n            yield from yield_artifacts(run_id, item.path)\n        else:\n            yield item.path\n\n\ndef fetch_logged_data(run_id):\n    &quot;&quot;&quot;Fetch params, metrics, tags, and artifacts in the specified run&quot;&quot;&quot;\n    client = MlflowClient()\n    data = client.get_run(run_id).data\n    # Exclude system tags: https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#system-tags\n    tags = {k: v for k, v in data.tags.items() if not k.startswith(&quot;mlflow.&quot;)}\n    artifacts = list(yield_artifacts(run_id))\n    return {\n        &quot;params&quot;: data.params,\n        &quot;metrics&quot;: data.metrics,\n        &quot;tags&quot;: tags,\n        &quot;artifacts&quot;: artifacts,\n    }\n<\/code><\/pre>\n<p>Error message<\/p>\n<pre><code>INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '8cc3f4e03b4e417b95a64f1a9a41be63', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\nTraceback (most recent call last):\n  File &quot;\/Users\/taein\/Desktop\/mlflow\/pipeline.py&quot;, line 33, in &lt;module&gt;\n    main()\n  File &quot;\/Users\/taein\/Desktop\/mlflow\/pipeline.py&quot;, line 23, in main\n    run_id = mlflow.last_active_run().info.run_id\nAttributeError: module 'mlflow' has no attribute 'last_active_run'\n<\/code><\/pre>\n<p>Thanks for your helping<\/p>",
        "Challenge_closed_time":1658706466923,
        "Challenge_comment_count":1,
        "Challenge_created_time":1658461256963,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73074887",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":40.68,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":68.1138777778,
        "Challenge_title":"'mlflow' has no attribute 'last_active_run'",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":215,
        "Challenge_word_count":275,
        "Platform":"Stack Overflow",
        "Poster_created_time":1598180751547,
        "Poster_location":"Seoul, Repulic of Korea",
        "Poster_reputation_count":158.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>It's because of the mlflow version that you mentioned in the comments. <code>mlflow.last_active_run()<\/code> API was introduced in <a href=\"https:\/\/github.com\/mlflow\/mlflow\/releases\/tag\/v1.25.0\" rel=\"nofollow noreferrer\">mlflow 1.25.0\n<\/a>. So you should upgrade the mlflow or you can use the previous version of the code available <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/5e2cb3baef544b00a972dff9dd6fb764be20510b\/examples\/sklearn_autolog\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<pre><code>wget https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/5e2cb3baef544b00a972dff9dd6fb764be20510b\/examples\/sklearn_autolog\/utils.py\nwget https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/5e2cb3baef544b00a972dff9dd6fb764be20510b\/examples\/sklearn_autolog\/pipeline.py\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":15.5,
        "Solution_reading_time":10.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":49.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":12.1803666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What is the proper way to connect DVC to Min.IO that is connected to some buckets on S3.<\/p>\n<pre><code>AWS-S3(My_Bucket) &gt; Min.io(MY_Bucket aliased as S3)\n<\/code><\/pre>\n<p>Right now i am accessing my bucket by using mc for example <code>mc cp s3\/my_bucket\/datasets datasets<\/code> to copy stuff from there. But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC so i can use for example <code>&quot;DVC mc-S3 pull&quot;<\/code> and <code>&quot;DVC AWS-S3 pull&quot;<\/code>.<\/p>\n<p>How do i got for it because while googling i couldn't find anything that i could easily follow.<\/p>",
        "Challenge_closed_time":1627513406643,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627469557323,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68559059",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":4.9,
        "Challenge_reading_time":8.01,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":12.1803666667,
        "Challenge_title":"DVC connect to Min.IO to access S3",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":375,
        "Challenge_word_count":101,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578574709920,
        "Poster_location":"Poland",
        "Poster_reputation_count":85.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>It looks like you are looking for a combination of things.<\/p>\n<p>First, Jorge mentioned you can set <code>endpointurl<\/code> to access Minio the same way as you would access regular S3:<\/p>\n<pre><code>dvc remote add -d minio-remote s3:\/\/mybucket\/path\ndvc remote modify minio-remote endpointurl https:\/\/minio.example.com                          \n<\/code><\/pre>\n<p>Second, it seems you can create <em>two<\/em> remotes - one for S3, one for Minio and use <code>-r<\/code> option that is available for many data management related commands:<\/p>\n<pre><code>dvc pull -r minio-remote\ndvc pull -r s3-remote\ndvc push -r minio-remote\n...\n<\/code><\/pre>\n<p>This way you could <code>push<\/code>\/<code>pull<\/code> data to\/from a specific storage.<\/p>\n<blockquote>\n<p>But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC<\/p>\n<\/blockquote>\n<p>There are other possible ways, I think to organize this. It indeed depends on what semantics you expect from <code>DVC mc-S3 pull<\/code>. Please let us know if <code>-r<\/code> is not enough and clarify the question- that would help us here.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":8.7,
        "Solution_reading_time":13.51,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":153.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1554860971800,
        "Answerer_location":"Tokyo, Japan",
        "Answerer_reputation_count":301.0,
        "Answerer_view_count":65.0,
        "Challenge_adjusted_solved_time":22.4942855556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I would like to get the best model to use later in the notebook to predict using a different test batch.<\/p>\n\n<p>reproducible example (taken from Optuna Github) :<\/p>\n\n<pre><code>import lightgbm as lgb\nimport numpy as np\nimport sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\n\nimport optuna\n\n\n# FYI: Objective functions can take additional arguments\n# (https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#objective-func-additional-args).\ndef objective(trial):\n    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n    dtrain = lgb.Dataset(train_x, label=train_y)\n    dvalid = lgb.Dataset(valid_x, label=valid_y)\n\n    param = {\n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n\n    # Add a callback for pruning.\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"auc\")\n    gbm = lgb.train(\n        param, dtrain, valid_sets=[dvalid], verbose_eval=False, callbacks=[pruning_callback]\n    )\n\n    preds = gbm.predict(valid_x)\n    pred_labels = np.rint(preds)\n    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n    return accuracy\n\n<\/code><\/pre>\n\n<p>my understanding is that the study below will tune for accuracy. I would like to somehow retrieve the best model from the study (not just the parameters) without saving it as a pickle, I just want to use the model somewhere else in my notebook. <\/p>\n\n<pre><code>\nif __name__ == \"__main__\":\n    study = optuna.create_study(\n        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"maximize\"\n    )\n    study.optimize(objective, n_trials=100)\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n\n<\/code><\/pre>\n\n<p>desired output would be <\/p>\n\n<pre><code>best_model = ~model from above~\nnew_target_pred = best_model.predict(new_data_test)\nmetrics.accuracy_score(new_target_test, new__target_pred)\n\n<\/code><\/pre>",
        "Challenge_closed_time":1591153873300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591072505577,
        "Challenge_favorite_count":9,
        "Challenge_last_edit_time":1591072893872,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62144904",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":13.2,
        "Challenge_reading_time":33.18,
        "Challenge_score_count":14,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":22.6021452778,
        "Challenge_title":"Python: How to retrive the best model from Optuna LightGBM study?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":8921,
        "Challenge_word_count":229,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529932143432,
        "Poster_location":"Melbourne VIC, Australia",
        "Poster_reputation_count":525.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>I think you can use the <code>callback<\/code> argument of <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/reference\/study.html#optuna.study.Study.optimize\" rel=\"noreferrer\"><code>Study.optimize<\/code><\/a> to save the best model. In the following code example, the callback checks if a given trial is corresponding to the best trial and saves the model as a global variable <code>best_booster<\/code>.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>best_booster = None\ngbm = None\n\ndef objective(trial):\n    global gbm\n    # ...\n\ndef callback(study, trial):\n    global best_booster\n    if study.best_trial == trial:\n        best_booster = gbm\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(\n        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"maximize\"\n    )\n    study.optimize(objective, n_trials=100, callbacks=[callback])\n\n<\/code><\/pre>\n\n<p>If you define your objective function as a class, you can remove the global variables. I created a notebook as a code example. Please take a look at it:\n<a href=\"https:\/\/colab.research.google.com\/drive\/1ssjXp74bJ8bCAbvXFOC4EIycBto_ONp_?usp=sharing\" rel=\"noreferrer\">https:\/\/colab.research.google.com\/drive\/1ssjXp74bJ8bCAbvXFOC4EIycBto_ONp_?usp=sharing<\/a><\/p>\n\n<blockquote>\n  <p>I would like to somehow retrieve the best model from the study (not just the parameters) without saving it as a pickle<\/p>\n<\/blockquote>\n\n<p>FYI, if you can pickle the boosters, I think you can make the code simple by following <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/faq.html#how-to-save-machine-learning-models-trained-in-objective-functions\" rel=\"noreferrer\">this FAQ<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"callback argument studi optim save model callback trial trial save model global variabl booster defin object function class remov global variabl pickl booster faq",
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_original_content":"callback argument studi optim save model callback trial trial save model global variabl booster booster gbm object trial global gbm callback studi trial global booster studi trial trial booster gbm studi creat studi pruner pruner medianprun warmup step direct maxim studi optim object trial callback callback defin object function class remov global variabl creat notebook http colab research com drive ssjxpbjbcabvxfoceiycbto onp usp share retriev model studi paramet save pickl fyi pickl booster faq",
        "Solution_preprocessed_content":"argument save model callback trial trial save model global variabl defin object function class remov global variabl creat notebook retriev model studi save pickl fyi pickl booster faq",
        "Solution_readability":15.3,
        "Solution_reading_time":21.14,
        "Solution_score_count":8.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":152.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1333532422647,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":10340.0,
        "Answerer_view_count":1243.0,
        "Challenge_adjusted_solved_time":7.9249297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking to run a pre-trained object detection model onto a folder of ~400k images which is about 1.5GB. When I've tried running locally, it was estimated to take ~8 days to complete (with keras yolov3). Thus, I am looking to use AWS SageMaker and S3.<\/p>\n<p>When I have uploaded the zip folder of my images in the SageMaker jupyter notebook and tried to unzip by using bash command, an error pops ups saying that I have insufficient space. The volume assigned to my notebook is 5GB EBS, I do have other heavy datasets in my jupyter notebook space which could be causing this issue.<\/p>\n<p>To tackle that, I am looking for a way where I can upload my data to S3 and run SageMaker to read the images hosted and run an object detection model over. However, it does not look like there's a method to unzip folders on S3 without using an additional service (read that AWS Lambda may help) as these services are paid by my school.<\/p>\n<p>I could possibly re-run my code to extract my images from URL. In this case, how can I save these images to S3 directly in this case? Also, does anyone know if I am able to run yolov3 on SageMaker or if there is a better model I can look to use. Appreciate any advice that may help.<\/p>",
        "Challenge_closed_time":1604629425800,
        "Challenge_comment_count":5,
        "Challenge_created_time":1604600896053,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64703268",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":8.7,
        "Challenge_reading_time":15.32,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":7.9249297222,
        "Challenge_title":"How to use AWS SageMaker and S3 for Object Detection?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":256,
        "Challenge_word_count":238,
        "Platform":"Stack Overflow",
        "Poster_created_time":1507880230152,
        "Poster_location":"Singapore",
        "Poster_reputation_count":180.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>yes u are right u can upload thousands of images using aws cli using $aws s3 cp  ; or $aws s3 sync<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"cli upload imag sync insuffici space unzip upload zip folder imag jupyt notebook run extract imag url save directli rais run yolov model",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"upload thousand imag cli sync",
        "Solution_preprocessed_content":"upload thousand imag cli sync",
        "Solution_readability":6.8,
        "Solution_reading_time":1.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1523192621643,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":1229.0,
        "Answerer_view_count":175.0,
        "Challenge_adjusted_solved_time":4.4849386111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to create a dataset on my azure ML workspace from a GitHub action<\/p>\n<p>I've created a datastore and uploaded data to that datastore\nwhen I try to create a dataset using the cli, I get this error:<\/p>\n<p><code>'create' is misspelled or not recognized by the system.<\/code><\/p>\n<p>this is the command i use:<\/p>\n<pre><code>&gt; az ml dataset create \n          -n insurance_dataset \n          --resource-group rg-name \n          --workspace-name ml-ws-name \n          -p 'file:azureml\/datastore\/$(az ml datastore show-default -w ml-ws-name -g rg-name --query name -o tsv)\/insurance\/insurance.csv'\n<\/code><\/pre>\n<p>any idea what am I doing wrong?<\/p>",
        "Challenge_closed_time":1658774288592,
        "Challenge_comment_count":3,
        "Challenge_created_time":1658758142813,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73110661",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":13.0,
        "Challenge_reading_time":8.8,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.4849386111,
        "Challenge_title":"'create' is misspelled or not recognized by the system on az ml dataset create",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":237,
        "Challenge_word_count":99,
        "Platform":"Stack Overflow",
        "Poster_created_time":1523192621643,
        "Poster_location":"Israel",
        "Poster_reputation_count":1229.0,
        "Poster_view_count":175.0,
        "Solution_body":"<p>in my case, the issue was solved by upgrading the ml extension to <code>azure-cli-ml v2<\/code><\/p>\n<p>Remove any existing installation of the of <code>ml<\/code> extension and also the CLI v1 <code>azure-cli-ml<\/code> extension:<\/p>\n<pre><code>az extension remove -n azure-cli-ml\naz extension remove -n ml\n<\/code><\/pre>\n<p>Now, install the ml extension:<\/p>\n<pre><code>az extension add -n ml -y\n<\/code><\/pre>\n<p>which still doesn't explain why the <code>create<\/code> command wasn't recognized, but the v2 behavior works fine for me.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"upgrad extens cli remov instal extens cli cli extens instal extens extens add clear creat recogn",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"upgrad extens cli remov instal extens cli cli extens extens remov cli extens remov instal extens extens add explain creat wasn recogn",
        "Solution_preprocessed_content":"upgrad extens remov instal extens cli extens instal extens explain wasn recogn",
        "Solution_readability":32.6,
        "Solution_reading_time":6.87,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":17.5084694445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to build and push a custom ML model with docker to Amazon SageMaker. I know things are supposed to follow the general structure of being in opt\/ml. But there's no such bucket in Amazon S3??? Am I supposed to create this directory within my container before I build and push the image to AWS? I just have no idea where to put my training data, etc.<\/p>",
        "Challenge_closed_time":1565167576867,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565104546377,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57379173",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":5.9,
        "Challenge_reading_time":5.11,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":17.5084694445,
        "Challenge_title":"Where do I store my model's training data, artifacts, etc?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1022,
        "Challenge_word_count":77,
        "Platform":"Stack Overflow",
        "Poster_created_time":1399251405187,
        "Poster_location":null,
        "Poster_reputation_count":145.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>SageMaker is automating the deployment of the Docker image with your code using the convention of channel->local-folder. Everything that you define with a channel in your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"nofollow noreferrer\">input data configuration<\/a>, will be copied to the local Docker file system under <em>\/opt\/ml\/<\/em> folder, using the name of the channel as the name of the sub-folder.<\/p>\n\n<pre><code>{\n\"train\" : {\"ContentType\":  \"trainingContentType\", \n           \"TrainingInputMode\": \"File\", \n           \"S3DistributionType\": \"FullyReplicated\", \n           \"RecordWrapperType\": \"None\"},\n\"evaluation\" : {\"ContentType\":  \"evalContentType\", \n                \"TrainingInputMode\": \"File\", \n                \"S3DistributionType\": \"FullyReplicated\", \n                \"RecordWrapperType\": \"None\"},\n\"validation\" : {\"TrainingInputMode\": \"File\", \n                \"S3DistributionType\": \"FullyReplicated\", \n                \"RecordWrapperType\": \"None\"}\n} \n<\/code><\/pre>\n\n<p>to:<\/p>\n\n<pre><code>\/opt\/ml\/input\/data\/training\n\/opt\/ml\/input\/data\/validation\n\/opt\/ml\/input\/data\/testing\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"defin input data configur channel local folder copi data local docker file opt folder channel sub folder creat directori build push imag",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"autom deploy docker imag convent channel local folder defin channel input data configur copi local docker file opt folder channel sub folder train contenttyp trainingcontenttyp traininginputmod file sdistributiontyp fullyrepl recordwrappertyp evalu contenttyp evalcontenttyp traininginputmod file sdistributiontyp fullyrepl recordwrappertyp traininginputmod file sdistributiontyp fullyrepl recordwrappertyp opt input data train opt input data opt input data test",
        "Solution_preprocessed_content":"autom deploy docker imag convent defin channel input data configur copi local docker file folder channel",
        "Solution_readability":26.0,
        "Solution_reading_time":15.0,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":100.7332322222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to implement the best estimator from a hyperparameter tuning job into a pipeline object to deploy an endpoint.<\/p>\n\n<p>I've read the docs in a best effort to include the results from the tuning job in the pipeline, but I'm having trouble creating the Model() class object.<\/p>\n\n<pre><code># This is the hyperparameter tuning job\ntuner.fit({'train': s3_train, 'validation': s3_val}, \ninclude_cls_metadata=False)\n\n\n#With a standard Model (Not from the tuner) the process was as follows:\nscikit_learn_inferencee_model_name = sklearn_preprocessor.create_model()\nxgb_model_name = Model(model_data=xgb_model.model_data, image=xgb_image)\n\n\nmodel_name = 'xgb-inference-pipeline-' + timestamp_prefix\nendpoint_name = 'xgb-inference-pipeline-ep-' + timestamp_prefix\nsm_model = PipelineModel(\n    name=model_name, \n    role=role, \n    models=[\n        scikit_learn_inferencee_model_name, \n        xgb_model_name])\n\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', \nendpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I would like to be able to cleanly instantiate a model object using my results from the tuning job and pass it into the PipelineModel object. Any guidance is appreciated.<\/p>",
        "Challenge_closed_time":1558879357103,
        "Challenge_comment_count":0,
        "Challenge_created_time":1558813444520,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56308169",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":16.14,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":18.3090508334,
        "Challenge_title":"Creating a model for use in a pipeline from a hyperparameter tuning job",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":455,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558812981692,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>I think you are on the right track. Do you get any error? Refer this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/e9c295c8538d29cc9fea2f73a29649126628064a\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a>  for instantiating the model from the tuner and use in inference pipeline.<\/p>\n\n<p>Editing previous response based on the comment. To create model from the best training job of the hyperparameter tuning job, you can use below snippet<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tuner import HyperparameterTuner\nfrom sagemaker.estimator import Estimator\nfrom sagemaker.model import Model\n\n# Attach to an existing hyperparameter tuning job.\nxgb_tuning_job_name = 'my_xgb_hpo_tuning_job_name'\nxgb_tuner = HyperparameterTuner.attach(xgb_tuning_job_name)\n\n# Get the best XGBoost training job name from the HPO job\nxgb_best_training_job = xgb_tuner.best_training_job()\nprint(xgb_best_training_job)\n\n# Attach estimator to the best training job name\nxgb_best_estimator = Estimator.attach(xgb_best_training_job)\n\n# Create model to be passed to the inference pipeline\nxgb_model = Model(model_data=xgb_best_estimator.model_data,\n                  role=sagemaker.get_execution_role(),\n                  image=xgb_best_estimator.image_name)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_gpt_summary":"creat model object train job hyperparamet tune job hyperparametertun estim class attach train job creat estim object model class creat model object pass infer pipelin",
        "Solution_last_edit_time":1559176084156,
        "Solution_link_count":1.0,
        "Solution_original_content":"track notebook instanti model tuner infer pipelin edit previou respons base comment creat model train job hyperparamet tune job tuner import hyperparametertun estim import estim model import model attach hyperparamet tune job xgb tune job xgb hpo tune job xgb tuner hyperparametertun attach xgb tune job train job hpo job xgb train job xgb tuner train job print xgb train job attach estim train job xgb estim estim attach xgb train job creat model pass infer pipelin xgb model model model data xgb estim model data role execut role imag xgb estim imag",
        "Solution_preprocessed_content":"track notebook instanti model tuner infer pipelin edit previou respons base comment creat model train job hyperparamet tune job",
        "Solution_readability":18.7,
        "Solution_reading_time":18.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":119.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":142.8473586111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a pipeline in Azure Machine Learning that includes a <strong>Math Operation<\/strong> (natural logarithm of a column named <em>charges<\/em>). The next pill to the <strong>Math Operatio<\/strong>n is <strong>Select Column in Dataset<\/strong>. Since the pipeline has not ben submitted and run I cannot access the column <em>ln(charges)<\/em> in the pill <strong>Select Column in Dataset<\/strong>.\nMy problem is that if I submit it I am able to run it and see the results in the pipeline once completed, but I have found no way of accessing those results (and thus the <em>ln(charges)<\/em> column in Designer.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DOddA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DOddA.png\" alt=\"Pipeline Job after submitting and running\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Hp6Dc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Hp6Dc.png\" alt=\"Pipeline in designer after submitting and running the job\" \/><\/a><\/p>\n<p><strong>UPDATE:<\/strong><\/p>\n<p><strong>I have found a workaround. Still in designer the column ln(charges) is not selectable but if I manually enter Ln(charges) in the select column fields it works.<\/strong><\/p>",
        "Challenge_closed_time":1663747263323,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663174586247,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1663233012832,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73720626",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":16.82,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":159.0769655556,
        "Challenge_title":"How can I select a column product of a math operation in Azure Machine Learning Designer?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":55,
        "Challenge_word_count":165,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526397625168,
        "Poster_location":"Spain",
        "Poster_reputation_count":47.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>The following is the procedure of the math operation in Azure ML designer to select the column to be implemented. The following procedure will help to give the column name as well as we can also give the index number of the column. This answer contains both the procedures.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9YV8f.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9YV8f.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>We can click on edit column.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bAnfq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bAnfq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/cZFfF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cZFfF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Based on the dataset which the experiment was running, both are options are mentioned in the above screen. We can choose either of the options.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PblH7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PblH7.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To access the data, right click and go to access data and click on result_dataset<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8jHz5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8jHz5.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The following page will open and click on any file mentioned in the box<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4jWxT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4jWxT.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on download and open in the editor according to your wish.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wfGPV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wfGPV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It looks like the above result screen.\nThe below screens are the designer created.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bTWGR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bTWGR.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6kUAw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6kUAw.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/I2Ej1.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I2Ej1.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To check the final model result. Go to evaluate model and get the results in visualization manner.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"procedur access math oper design procedur involv click edit column option select desir column index access data click access data click dataset download open file editor final model evalu model visual",
        "Solution_last_edit_time":null,
        "Solution_link_count":20.0,
        "Solution_original_content":"procedur math oper design select column implement procedur column index column procedur click edit column base dataset run option screen choos option access data click access data click dataset page open click file box click download open editor accord wish screen screen design creat final model evalu model visual",
        "Solution_preprocessed_content":"procedur math oper design select column implement procedur column index column procedur click edit column base dataset run option screen choos option access data click access data click page open click file box click download open editor accord wish screen screen design creat final model evalu model visual",
        "Solution_readability":11.2,
        "Solution_reading_time":34.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":31.0,
        "Solution_word_count":252.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1508520702036,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":21.2055166667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>when running an AML pipeline on AML compute, I get this kind of error : <\/p>\n\n<p>I can try rebooting the cluster, but that may not fix the problem (if storage gets accumulated no the nodes, that should be cleaned.<\/p>\n\n<pre><code>Session ID: 933fc468-7a22-425d-aa1b-94eba5784faa\n{\"error\":{\"code\":\"ServiceError\",\"message\":\"Job preparation failed: [Errno 28] No space left on device\",\"detailsUri\":null,\"target\":null,\"details\":[],\"innerError\":null,\"debugInfo\":{\"type\":\"OSError\",\"message\":\"[Errno 28] No space left on device\",\"stackTrace\":\" File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1-setup\/job_prep.py\\\", line 126, in &lt;module&gt;\\n invoke()\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1-setup\/job_prep.py\\\", line 97, in invoke\\n extract_project(project_dir, options.project_zip, options.snapshots)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1-setup\/job_prep.py\\\", line 60, in extract_project\\n project_fetcher.fetch_project_snapshot(snapshot[\\\"Id\\\"], snapshot[\\\"PathStack\\\"])\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 72, in fetch_project_snapshot\\n _download_tree(sas_tree, path_stack)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 106, in _download_tree\\n _download_tree(child, path_stack)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 106, in _download_tree\\n _download_tree(child, path_stack)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 98, in _download_tree\\n fh.write(response.read())\\n\",\"innerException\":null,\"data\":null,\"errorResponse\":null}},\"correlation\":null,\"environment\":null,\"location\":null,\"time\":\"0001-01-01T00:00:00+00:00\"}\n<\/code><\/pre>\n\n<p>I would expect the job to run as it should. And in fact, I've checked on the node and the node do have lots of available harddrive space :<\/p>\n\n<pre><code>root@4f57957ac829466a86bad4d4dc51fadd000001:~# df -kh                                                                                               Filesystem      Size  Used Avail Use% Mounted on\nudev             28G     0   28G   0% \/dev\ntmpfs           5.6G  9.0M  5.5G   1% \/run\n\/dev\/sda1       125G  2.8G  122G   3% \/\ntmpfs            28G     0   28G   0% \/dev\/shm\ntmpfs           5.0M     0  5.0M   0% \/run\/lock\ntmpfs            28G     0   28G   0% \/sys\/fs\/cgroup\n\/dev\/sdb1       335G  6.7G  311G   3% \/mnt\ntmpfs           5.6G     0  5.6G   0% \/run\/user\/1002\n<\/code><\/pre>\n\n<p>Suggestions on what I should check?<\/p>",
        "Challenge_closed_time":1568309206067,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568232866207,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57896195",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":15.5,
        "Challenge_reading_time":42.81,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":21.2055166667,
        "Challenge_title":"Out of disk space",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":738,
        "Challenge_word_count":210,
        "Platform":"Stack Overflow",
        "Poster_created_time":1538275960603,
        "Poster_location":"Montreal, QC, Canada",
        "Poster_reputation_count":381.0,
        "Poster_view_count":50.0,
        "Solution_body":"<p>Seems like you've run into Azure file share constraints. You can use the following sample code to change your runs to use blob storage which can scale to large number of jobs running in parallel:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data#accessing-source-code-during-training\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data#accessing-source-code-during-training<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"sampl run blob storag file share scale larg job run parallel",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"run file share constraint sampl run blob storag scale larg job run parallel http doc com servic access data access sourc train",
        "Solution_preprocessed_content":"run file share constraint sampl run blob storag scale larg job run parallel",
        "Solution_readability":19.0,
        "Solution_reading_time":6.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1576813179640,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":39.3623194445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a data set with about 7999 attributes and 39 labels, with 3339 total observations (resulting in 3339x8038 data set), and I'm trying to upload id to Azure ML platform.\nI've selected the 'type' as 'tabular', encoding as 'utf-8', no row skipping, and use header from first file.\nThe problem is, that the headers are still not included and the data is interpreted as string with 0s, 1s, and commas (see pic <a href=\"https:\/\/imgur.com\/a\/QdQNt1y\" rel=\"nofollow noreferrer\">https:\/\/imgur.com\/a\/QdQNt1y<\/a>)<\/p>\n\n<p>Am I missing something? For smaller data sets it seemed to work. My headers are A1, ... A7999 for the attributes, and L1, ... L39 for the labels.<\/p>\n\n<p>Thanks for help in advance.<\/p>",
        "Challenge_closed_time":1576813812910,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576672108560,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59392060",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":6.2,
        "Challenge_reading_time":9.4,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":39.3623194445,
        "Challenge_title":"Azure ML platform wrongly interprets uploaded dataset",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":49,
        "Challenge_word_count":114,
        "Platform":"Stack Overflow",
        "Poster_created_time":1527091507808,
        "Poster_location":"Stockholm, Sweden",
        "Poster_reputation_count":235.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>our system does our best guess over file settings when you try to create a dataset, but cannot guarantee perfect guesses in all cases. <\/p>\n\n<p>In such scenarios, you should be able to adjust the settings. We had a bug with the ability to change those settings, but rolled out a fix. Can you try to change those now?  <\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"adjust set manual perfect guess set upload dataset abil set",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"guess file set creat dataset perfect guess adjust set abil set roll",
        "Solution_preprocessed_content":"guess file set creat dataset perfect guess adjust set abil set roll",
        "Solution_readability":5.6,
        "Solution_reading_time":3.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405882600928,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":552.0,
        "Answerer_view_count":115.0,
        "Challenge_adjusted_solved_time":34.4314577778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>To train tensorflow keras models on AI Platform using Docker containers, we convert our raw images stored on GCS to a tfrecord dataset using <code>tf.data.Dataset<\/code>. Thereby the data is never stored locally. Instead the raw images are transformed directly to tfrecords to another bucket. Is it possible to make use of <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\" rel=\"nofollow noreferrer\">kedro<\/a> with a tfrecord dataset and the streaming capability of <code>tf.data.Dataset<\/code>? According to the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.extras.datasets.html\" rel=\"nofollow noreferrer\">docs<\/a> kedro doesn't seem to support tfrecord datasets.<\/p>",
        "Challenge_closed_time":1596184916008,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596148880400,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63182406",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":9.23,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":10.0098911111,
        "Challenge_title":"Does kedro support tfrecord?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":107,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1362514672823,
        "Poster_location":"Vorarlberg, Austria",
        "Poster_reputation_count":1570.0,
        "Poster_view_count":159.0,
        "Solution_body":"<p>Only TF related dataset we have at the moment is <code>TensorFlowModelDataset<\/code> (<a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/_modules\/kedro\/extras\/datasets\/tensorflow\/tensorflow_model_dataset.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/latest\/_modules\/kedro\/extras\/datasets\/tensorflow\/tensorflow_model_dataset.html<\/a>), but you can easily <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/07_extend_kedro\/01_custom_datasets.html#custom-datasets\" rel=\"nofollow noreferrer\">add your own custom dataset<\/a>, or please add a feature request\/your contribution in <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/issues\" rel=\"nofollow noreferrer\">the repo<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"creat dataset tfrecord dataset request featur contribut repositori add tfrecord dataset bias summari",
        "Solution_last_edit_time":1596272833648,
        "Solution_link_count":4.0,
        "Solution_original_content":"relat dataset moment tensorflowmodeldataset http readthedoc latest modul extra dataset tensorflow tensorflow model dataset html easili add dataset add featur request contribut repo",
        "Solution_preprocessed_content":"relat dataset moment easili add dataset add featur contribut repo",
        "Solution_readability":30.0,
        "Solution_reading_time":9.56,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":40.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1426093220648,
        "Answerer_location":"Bengaluru, India",
        "Answerer_reputation_count":1861.0,
        "Answerer_view_count":294.0,
        "Challenge_adjusted_solved_time":0.3459813889,
        "Challenge_answer_count":1,
        "Challenge_body":"<ol>\n<li><p>I want to write pytest unit test in <strong>Kedro 0.17.5<\/strong>. They need to perform integrity checks on dataframes created by the pipeline.\nThese dataframes are specified in the <code>catalog.yml<\/code> and already persisted successfully using <code>kedro run<\/code>. The <code>catalog.yml<\/code> is in <code>conf\/base<\/code>.<\/p>\n<\/li>\n<li><p>I have a test module <code>test_my_dataframe.py<\/code> in <code>src\/tests\/pipelines\/my_pipeline\/<\/code>.<\/p>\n<\/li>\n<\/ol>\n<p>How can I load the data catalog based on my <code>catalog.yml<\/code> programmatically from within <code>test_my_dataframe.py<\/code> in order to properly access my specified dataframes?<\/p>\n<p>Or, for that matter, how can I programmatically load the whole project context (including the data catalog) in order to also execute nodes etc.?<\/p>",
        "Challenge_closed_time":1656143487270,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656142241737,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1656265981920,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72752043",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":11.24,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.3459813889,
        "Challenge_title":"Load existing data catalog programmatically",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":107,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1258185382660,
        "Poster_location":null,
        "Poster_reputation_count":333.0,
        "Poster_view_count":33.0,
        "Solution_body":"<ol>\n<li><p>For unit testing, we test just the function which we are testing, and everything external to the function we should mock\/patch. Check if you really need kedro project context while writing the unit test.<\/p>\n<\/li>\n<li><p>If you really need project context in test, you can do something like following<\/p>\n<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from kedro.framework.project import configure_project\nfrom kedro.framework.session import KedroSession\n\nwith KedroSession.create(package_name=&quot;demo&quot;, project_path=Path.cwd()) as session:\n    context = session.load_context()\n    catalog = context.catalog\n<\/code><\/pre>\n<p>or you can also create pytest fixture to use it again and again with scope of your choice.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@pytest.fixture\ndef get_project_context():\n    session = KedroSession.create(\n        package_name=&quot;demo&quot;,\n        project_path=Path.cwd()\n    )\n    _activate_session(session, force=True)\n    context = session.load_context()\n    return context\n<\/code><\/pre>\n<p>Different args supported by KedroSession create you can check it here <a href=\"https:\/\/kedro.readthedocs.io\/en\/0.17.5\/kedro.framework.session.session.KedroSession.html#kedro.framework.session.session.KedroSession.create\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/0.17.5\/kedro.framework.session.session.KedroSession.html#kedro.framework.session.session.KedroSession.create<\/a><\/p>\n<p>To read more about pytest fixture you can refer to <a href=\"https:\/\/docs.pytest.org\/en\/6.2.x\/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session\" rel=\"nofollow noreferrer\">https:\/\/docs.pytest.org\/en\/6.2.x\/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"unit test test function test mock patch extern function context write unit test context creat pytest fixtur repeatedli desir scope session load context programmat test file bias summari",
        "Solution_last_edit_time":1656143791536,
        "Solution_link_count":4.0,
        "Solution_original_content":"unit test test function test extern function mock patch context write unit test context test framework import configur framework session import session session creat packag path path cwd session context session load context catalog context catalog creat pytest fixtur scope choic pytest fixtur context session session creat packag path path cwd activ session session forc context session load context return context arg session creat http readthedoc framework session session session html framework session session session creat read pytest fixtur http doc pytest org fixtur html scope share fixtur class modul packag session",
        "Solution_preprocessed_content":"unit test test function test extern function context write unit test context test creat pytest fixtur scope choic arg session creat read pytest fixtur",
        "Solution_readability":19.5,
        "Solution_reading_time":23.58,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":135.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1456986606312,
        "Answerer_location":null,
        "Answerer_reputation_count":757.0,
        "Answerer_view_count":80.0,
        "Challenge_adjusted_solved_time":7933.4568986111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>There's probably something very obvious I'm missing or Sagemaker just doesn't support these kinds of extensions, but I've been trying to enable toc2 (Table of Contents) jupyter extension for my Sagemaker notebook via lifecycle configurations, but for whatever reason it still isn't showing up.<\/p>\n\n<p>I built my script out combining a sample AWS script and a quick article on the usual ways of enabling extensions:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh<\/a><\/p>\n\n<p><a href=\"https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231<\/a><\/p>\n\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;EOF\n\n--Activate notebook environment\nsource activate JupyterSystemEnv\n\n--Install extensions\npip install jupyter_contrib_nbextensions &amp;&amp; jupyter contrib\nnbextension install\njupyter nbextension enable toc2 --py --sys-prefix\n\nsource deactivate\n\n\nEOF\n<\/code><\/pre>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1610748471692,
        "Challenge_comment_count":2,
        "Challenge_created_time":1586792396013,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61191412",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":20.5,
        "Challenge_reading_time":18.54,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6654.4654663889,
        "Challenge_title":"Unable to install toc2 notebook extension for AWS Sagemaker Instance (Lifecycle Configurations)",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1027,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456986606312,
        "Poster_location":null,
        "Poster_reputation_count":757.0,
        "Poster_view_count":80.0,
        "Solution_body":"<p>Answering my question, looks like I was just missing the line <code>jupyter contrib nbextension install --user<\/code> to copy the JS\/CSS files into Jupyter's search directory and some config updates (<a href=\"https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions\" rel=\"nofollow noreferrer\">https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions<\/a>).<\/p>\n<p>Corrected statement<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\nsource \/home\/ec2-user\/anaconda3\/bin\/activate JupyterSystemEnv\n\npip install jupyter_contrib_nbextensions\njupyter contrib nbextension install --user\njupyter nbextension enable toc2\/main\n\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\n\n\nEOF\n\n##Below may be unnecessary, but other user needed to run to see success\ninitctl restart jupyter-server --no-wait\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"miss line jupyt contrib nbextens instal copi css file jupyt search directori config updat line run toc jupyt extens successfulli enabl notebook",
        "Solution_last_edit_time":1615352840848,
        "Solution_link_count":2.0,
        "Solution_original_content":"miss line jupyt contrib nbextens instal copi css file jupyt search directori config updat http github com ipython contrib jupyt contrib nbextens statement bin bash set sudo eof sourc home anaconda bin activ jupytersystemenv pip instal jupyt contrib nbextens jupyt contrib nbextens instal jupyt nbextens enabl toc sourc home anaconda bin deactiv eof unnecessari run initctl restart jupyt server wait",
        "Solution_preprocessed_content":"miss line copi file jupyt search directori config updat statement",
        "Solution_readability":18.1,
        "Solution_reading_time":11.02,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":6.1298294445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a way to deploy a xgboost model trained locally using amazon sagemaker? I only saw tutorial talking about both training and deploying model with amazon sagemaker.\nThanks.<\/p>",
        "Challenge_closed_time":1531778753543,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531756686157,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51365850",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":2.95,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":6.1298294445,
        "Challenge_title":"how to deploy a xgboost model on amazon sagemaker?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1641,
        "Challenge_word_count":37,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530193663836,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>This <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/d5681a07611ae29567355b60b2f22500b561218b\/advanced_functionality\/xgboost_bring_your_own_model\/xgboost_bring_your_own_model.ipynb\" rel=\"nofollow noreferrer\">example notebook<\/a> is good starting point showing how to use a pre-existing scikit-learn xgboost model with the Amazon SageMaker to create a hosted endpoint for that model.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"link notebook demonstr pre scikit model creat host endpoint model",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"notebook start pre scikit model creat host endpoint model",
        "Solution_preprocessed_content":"notebook start model creat host endpoint model",
        "Solution_readability":21.1,
        "Solution_reading_time":5.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1505653015243,
        "Answerer_location":null,
        "Answerer_reputation_count":1128.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":0.1140905556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to export a machine learning model I created in Azure Machine Learning studio. One of the required input is \"Path to blob beginning with container\"<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/xvSDd.png\" alt=\"Here is the screenshoot from azure export\"><\/p>\n\n<p>How do I find this path? I have already created a blob storage but I have no idea how to find the path to the blob storage. <\/p>",
        "Challenge_closed_time":1538035308943,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538034898217,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1538038508827,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52532078",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":5.24,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1140905556,
        "Challenge_title":"How to find the path to blob?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":15096,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>you should be able to find this from the Azure portal. Open the storage account, drill down into blobs, then your container. Use properties for the context menu, the URL should be the path ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"path blob storag portal open storag account drill blob properti context menu url path",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"portal open storag account drill blob properti context menu url path",
        "Solution_preprocessed_content":"portal open storag account drill blob properti context menu url path",
        "Solution_readability":7.3,
        "Solution_reading_time":4.7,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":50.0496855556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was wondering whether there is some size limit for storing a folder in sagemaker studio?\nI have this dataset stored in s3 but I want to download that dataset in my sagemaker studio environment to train my model. Is there some kind of limit in the size of a file i can download?<\/p>",
        "Challenge_closed_time":1638800978928,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638620800060,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70225564",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.5,
        "Challenge_reading_time":3.78,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":50.0496855556,
        "Challenge_title":"Sagemaker file size limit?",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":398,
        "Challenge_word_count":57,
        "Platform":"Stack Overflow",
        "Poster_created_time":1638359312992,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tasks-manage-storage.html\" rel=\"nofollow noreferrer\">home directory in Amazon SageMaker Studio is stored in Amazon EFS file system<\/a>. The file system grows and shrink as you add and remove files.<br \/>\nAccording to <a href=\"https:\/\/docs.aws.amazon.com\/efs\/latest\/ug\/limits.html#limits-fs-specific\" rel=\"nofollow noreferrer\">Amazon EFS limits<\/a> the <strong>maximum size for a single file is 47.9TiB.<\/strong><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"maximum size singl file studio tib accord ef limit",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"home directori studio store ef file file grow shrink add remov file accord ef limit maximum size singl file tib",
        "Solution_preprocessed_content":"home directori studio store ef file file grow shrink add remov file accord ef limit maximum size singl file",
        "Solution_readability":17.5,
        "Solution_reading_time":6.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":46.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1500629225150,
        "Answerer_location":null,
        "Answerer_reputation_count":5939.0,
        "Answerer_view_count":886.0,
        "Challenge_adjusted_solved_time":2.1350308333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am taking my first steps into the Terraform world so please be gentle with me. I have a user with AmazonSageMakerFullAccess, which I stored via AWS CLI in a profile called terraform. I can create an S3 bucket as follows no problem referring this user in Windows in VSC:<\/p>\n<pre><code>provider &quot;aws&quot; {\n    region = &quot;eu-west-2&quot;\n    shared_credentials_files = [&quot;C:\\\\Users\\\\amazinguser\\\\.aws\\\\credentials&quot;]\n    profile = &quot;terraform&quot;\n}\n\nresource &quot;aws_s3_bucket&quot; &quot;b&quot; {\n  bucket = &quot;blabla-test-bucket&quot;\n\n  tags = {\n    Name        = &quot;amazing_tag&quot;\n    Environment = &quot;dev&quot;\n  }\n}\n<\/code><\/pre>\n<p>I try to implement <a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/tree\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\" rel=\"nofollow noreferrer\">this<\/a> documented <a href=\"https:\/\/towardsdatascience.com\/terraform-sagemaker-part-2a-creating-a-custom-sagemaker-notebook-instance-1d68c90b192b\" rel=\"nofollow noreferrer\">here<\/a> and try to this:<\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n  name = &quot;titanic-sagemaker-byoc-notebook&quot;\n  role_arn = aws_iam_role.notebook_iam_role.arn\n  instance_type = &quot;ml.t2.medium&quot;\n  #lifecycle_config_name = aws_sagemaker_notebook_instance_lifecycle_configuration.notebook_config.name\n  #default_code_repository = aws_sagemaker_code_repository.git_repo.code_repository_name\n}\n<\/code><\/pre>\n<p>I am a bit confused about the role_arn which is defined here:<\/p>\n<p><a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/blob\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\/terraform\/iam.tf\" rel=\"nofollow noreferrer\">https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/blob\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\/terraform\/iam.tf<\/a><\/p>\n<p>Can I not use the above user? Thanks!<\/p>",
        "Challenge_closed_time":1649687933128,
        "Challenge_comment_count":9,
        "Challenge_created_time":1649680247017,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71827884",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":10,
        "Challenge_readability":22.5,
        "Challenge_reading_time":26.44,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2.1350308333,
        "Challenge_title":"create sagemaker notebook instance via Terraform",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":338,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>AWS services trying to call other AWS services and perform actions are not allowed to do so by default. For example, SageMaker Notebooks are basically EC2 instances. In order for SageMaker to create EC2 instances, it has to have a policy which allows e.g., injecting ENIs to a VPC. Since you probably do not want to do all that by yourself (it is a managed Notebook service after all), you have to give SageMaker permissions to perform actions on your behalf. Enter <strong>execution roles<\/strong>. For SageMaker, you can read more in [1]. Other services that you will commonly find using execution roles are Lambda, ECS and many others. An IAM role usually consists of two parts:<\/p>\n<ol>\n<li>Trust relationship (I like to call it trust policy)<\/li>\n<li>Permissions policy<\/li>\n<\/ol>\n<p>The first one decides which principal (AWS identifier, Service etc. [2]) will be able to assume the role. In your example, that is:<\/p>\n<pre><code>data &quot;aws_iam_policy_document&quot; &quot;sm_assume_role_policy&quot; {\n  statement {\n    actions = [&quot;sts:AssumeRole&quot;]\n    \n    principals {\n      type = &quot;Service&quot;\n      identifiers = [&quot;sagemaker.amazonaws.com&quot;]\n    }\n  }\n}\n<\/code><\/pre>\n<p>What this policy says is &quot;I am going to allow SageMaker (which is of type <code>Service<\/code>) to assume any role to which this policy is attached and perform actions that are defined in the permissions policy&quot;. The permissions policy is:<\/p>\n<pre><code># Attaching the AWS default policy, &quot;AmazonSageMakerFullAccess&quot;\nresource &quot;aws_iam_policy_attachment&quot; &quot;sm_full_access_attach&quot; {\n  name = &quot;sm-full-access-attachment&quot;\n  roles = [aws_iam_role.notebook_iam_role.name]\n  policy_arn = &quot;arn:aws:iam::aws:policy\/AmazonSageMakerFullAccess&quot;\n}\n<\/code><\/pre>\n<p>Without going into too much details about what the AWS managed policy for SageMaker does, it is enough to see the <code>FullAccess<\/code> part for it to be clear. What you could do if you want to be extra careful is to define a customer managed policy [3] for SageMaker notebooks. This permissions policy will be attached to the IAM role(s) defined in the <code>roles<\/code> argument. Note that it is a list, so multiple roles can have the same permissions policy attached.<\/p>\n<p>Last, but not the least, the glue between the trust and permissions policy is the role itself:<\/p>\n<pre><code>resource &quot;aws_iam_role&quot; &quot;notebook_iam_role&quot; {\n  name = &quot;sm_notebook_role&quot;\n  assume_role_policy = data.aws_iam_policy_document.sm_assume_role_policy.json\n}\n<\/code><\/pre>\n<p>As you can see, the <code>assume_role_policy<\/code> is the policy which will allow SageMaker to perform actions in the AWS account based on the permissions defined in the permissions policy.<\/p>\n<p>This topic is much more complex than in this answer, but it should give you a fair amount of information.<\/p>\n<p>NOTE: In theory, the same role accessing information in AWS and running the AWS API actions when using Terraform could be used for SageMaker, but I would strongly advise against it. Always keep in mind separation of concerns and principle of least privilege.<\/p>\n<hr \/>\n<p>[1] <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html#sagemaker-roles-create-execution-role\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html#sagemaker-roles-create-execution-role<\/a><\/p>\n<p>[2] <a href=\"https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/reference_policies_elements_principal.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/reference_policies_elements_principal.html<\/a><\/p>\n<p>[3] <a href=\"https:\/\/docs.aws.amazon.com\/acm\/latest\/userguide\/authen-custmanagedpolicies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/acm\/latest\/userguide\/authen-custmanagedpolicies.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"creat execut role perform action behalf iam role consist trust relationship permiss polici trust relationship decid princip role permiss polici defin action perform defin polici notebook attach iam role defin role argument role access run api action terraform separ principl privileg",
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_original_content":"servic servic perform action allow default notebook instanc order creat instanc polici allow inject eni vpc probabl notebook servic permiss perform action behalf enter execut role read servic commonli execut role lambda ec iam role consist trust relationship trust polici permiss polici decid princip identifi servic role data iam polici document role polici statement action st assumerol princip type servic identifi amazonaw com polici sai allow type servic role polici attach perform action defin permiss polici permiss polici attach default polici amazonfullaccess resourc iam polici attach access attach access attach role iam role notebook iam role polici arn arn iam polici amazonfullaccess polici fullaccess clear extra care defin polici notebook permiss polici attach iam role defin role argument note list multipl role permiss polici attach glue trust permiss polici role resourc iam role notebook iam role notebook role role polici data iam polici document role polici json role polici polici allow perform action account base permiss defin permiss polici complex fair note theori role access run api action terraform strongli separ principl privileg http doc com latest role html role creat execut role http doc com iam latest userguid polici element princip html http doc com acm latest userguid authen custmanagedpolici html",
        "Solution_preprocessed_content":"servic servic perform action allow default notebook instanc order creat instanc polici allow inject eni vpc probabl permiss perform action behalf enter execut role read servic commonli execut role lambda ec iam role consist trust relationship permiss polici decid princip role polici sai allow role polici attach perform action defin permiss polici permiss polici polici clear extra care defin polici notebook permiss polici attach iam role defin argument note list multipl role permiss polici attach glue trust permiss polici role polici allow perform action account base permiss defin permiss polici complex fair note theori role access run api action terraform strongli separ principl privileg",
        "Solution_readability":14.1,
        "Solution_reading_time":50.21,
        "Solution_score_count":2.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":435.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1340895239552,
        "Answerer_location":"Cambridge, MA, United States",
        "Answerer_reputation_count":1401.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":0.1411905556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have a dataframe like bellow, where <code>ID<\/code> is numeric value, and <code>comment1<\/code> and <code>comment2<\/code> string that I am importing as a csv. But the data frame is giving result something like this bellow, where <code>fifth comment<\/code> should be in the <code>comment2<\/code> and the original <code>ID<\/code> value is replaced by this. This is happening randomly for only few rows. Moreover, this problem is only occurring when I am importing my <strong>R<\/strong> code in <strong>Azure ML<\/strong> studio, in <strong>RStudio<\/strong> no data misplace is occurring. So what I was thinking, just delete the entire row where the first column <code>ID<\/code> is not a numeric value. As the misplace string value is random long sentence, I can not do string matching to delete the row. And the dataframe is big enough that I just cannot delete the rows manually. Suggestion please. <\/p>\n\n<pre><code>  ID                 Comment1                  comment2\n 123             This is first comment        this is second\n 234              third comment               fourth comment\nfifth comment                                                  \n 345               sixth comment              seventh comment\n<\/code><\/pre>\n\n<p>You will find a sample of the dataframe here,<\/p>\n\n<pre><code>    df &lt;-\n  read.csv(\n    \"https:\/\/docs.google.com\/spreadsheets\/d\/171YXjzm3FsapXSkqgOSos6UGXNRcd1yxmLyvaRnCX5E\/pub?output=csv\"\n  )\ndf &lt;- df[-1,]\ndf &lt;- df[, 1:12]\ncolnames(df) &lt;-\n  c(\n    \"ID\",\"Created\",\"Comments\",\"Liked_By\",\"Disliked_By\", \"Recipient_Number\",\n    \"Sender\",\"Recipients\",\"Read_By\", \"Subject\",\"Introduction\",\"Body\"\n  )\n<\/code><\/pre>",
        "Challenge_closed_time":1457377932163,
        "Challenge_comment_count":5,
        "Challenge_created_time":1457377423877,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1483518911967,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35851851",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":8,
        "Challenge_readability":12.3,
        "Challenge_reading_time":19.3,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.1411905556,
        "Challenge_title":"How to delete all non-numeric rows in R?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":5339,
        "Challenge_word_count":200,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455672951607,
        "Poster_location":"Germany",
        "Poster_reputation_count":473.0,
        "Poster_view_count":67.0,
        "Solution_body":"<p>Subset to numeric IDs:<\/p>\n\n<pre><code>subset(df, grepl('^\\\\d+$', df$ID))\n<\/code><\/pre>\n\n<p>The pattern should match values of ID that start and end with digits, and only contain digits.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"subset datafram row numer valu column subset grepl regular express pattern match valu start end digit digit",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"subset numer id subset grepl pattern match valu start end digit digit",
        "Solution_preprocessed_content":"subset numer id pattern match valu start end digit digit",
        "Solution_readability":13.0,
        "Solution_reading_time":2.45,
        "Solution_score_count":3.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1373375969332,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":273.5959513889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We have a huge set of data in CSV format, containing a few numeric elements, like this:<\/p>\n\n<pre><code>Year,BinaryDigit,NumberToPredict,JustANumber, ...other stuff\n1954,1,762,16, ...other stuff\n1965,0,142,16, ...other stuff\n1977,1,172,16, ...other stuff\n<\/code><\/pre>\n\n<p>The thing here is that there is a strong correlation between the third column and the columns before that. So I have pre-processed the data and it's now available in a format I think is perfect:<\/p>\n\n<pre><code>1954,1,762\n1965,0,142\n1977,1,172\n<\/code><\/pre>\n\n<p>What I want is a predicition on the value in the third column, using the first two as input. So in the case above, I want the input 1965,0 to return 142. In real life this file is thousands of rows, but since there's a pattern, I'd like to retrieve the most possible value.<\/p>\n\n<p>So far I've setup a train job on the CSV file using the L<em>inear Learner<\/em> algorithm, with the following settings:<\/p>\n\n<pre><code>label_size = 1\nfeature_dim = 2\npredictor_type = regression\n<\/code><\/pre>\n\n<p>I've also created a model from it, and setup an endpoint. When I invoke it, I get a score in return.<\/p>\n\n<pre><code>    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                   ContentType='text\/csv',\n                                   Body=payload)\n<\/code><\/pre>\n\n<p>My goal here is to get the third column prediction instead. How can I achieve that? I have read a lot of the documentation regarding this, but since I'm not very familiar with AWS, I might as well have used the wrong algorithms for what I am trying to do.<\/p>\n\n<p>(Please feel free to edit this question to better suit AWS terminology)<\/p>",
        "Challenge_closed_time":1553880598532,
        "Challenge_comment_count":0,
        "Challenge_created_time":1552553455710,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1552895653107,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55158307",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":21.27,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":368.6507838889,
        "Challenge_title":"Can I make Amazon SageMaker deliver a recommendation based on historic data instead of a probability score?",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":271,
        "Challenge_word_count":256,
        "Platform":"Stack Overflow",
        "Poster_created_time":1411464641600,
        "Poster_location":"\u00d6rebro, Sverige",
        "Poster_reputation_count":205.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>For csv input, the label should be in the first column, as mentioned <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-training.html\" rel=\"nofollow noreferrer\">here<\/a>:  So you should preprocess your data to put the label (the column you want to predict) on the left.<\/p>\n\n<p>Next, you need to decide whether this is a regression problem or a classification problem. <\/p>\n\n<p>If you want to predict a number that's as close as possible to the true number, that's regression. For example, the truth might be 4, and the model might predict 4.15. If you need an integer prediction, you could round the model's output.<\/p>\n\n<p>If you want the prediction to be one of a few categories, then you have a classification problem. For example, we might encode 'North America' = 0, 'Europe' = 1, 'Africa' = 2, and so on. In this case, a fractional prediction wouldn't make sense. <\/p>\n\n<p>For regression, use <code>'predictor_type' = 'regressor'<\/code> and for classification with more than 2 classes, use <code>'predictor_type' = 'multiclass_classifier'<\/code> as documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The output of regression will contain only a <code>'score'<\/code> field, which is the model's prediction. The output of multiclass classification will contain a <code>'predicted_label'<\/code> field, which is the model's prediction, as well as a <code>'score'<\/code> field, which is a vector of probabilities representing the model's confidence. The index with the highest probability will be the one that's predicted as the <code>'predicted_label'<\/code>. The output formats are documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/LL-in-formats.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"preprocess data label column predict left decid regress classif regress predictor type regressor classif class predictor type multiclass classifi output regress score field model predict output multiclass classif predict label field model predict score field vector probabl repres model confid",
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"csv input label column preprocess data label column predict left decid regress classif predict close regress model predict integ predict round model output predict classif encod america europ africa fraction predict sens regress predictor type regressor classif class predictor type multiclass classifi document output regress score field model predict output multiclass classif predict label field model predict score field vector probabl repres model confid index highest probabl predict predict label output format document",
        "Solution_preprocessed_content":"csv input label column preprocess data label left decid regress classif predict close regress model predict integ predict round model output predict classif encod america europ africa fraction predict sens regress classif class document output regress field model predict output multiclass classif field model predict field vector probabl repres model confid index highest probabl predict output format document",
        "Solution_readability":11.3,
        "Solution_reading_time":23.09,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":230.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1526732158023,
        "Answerer_location":"\u0130stanbul, T\u00fcrkiye",
        "Answerer_reputation_count":888.0,
        "Answerer_view_count":89.0,
        "Challenge_adjusted_solved_time":178.8386513889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am getting an error on my modeling of lightgbm searching for optimal auc. Any help would be appreciated.<\/p>\n<pre><code>import optuna  \nfrom sklearn.model_selection import StratifiedKFold\nfrom optuna.integration import LightGBMPruningCallback\ndef objective(trial, X, y):\n    param = {\n        &quot;objective&quot;: &quot;binary&quot;,\n        &quot;metric&quot;: &quot;auc&quot;,\n        &quot;verbosity&quot;: -1,\n        &quot;boosting_type&quot;: &quot;gbdt&quot;,\n        &quot;lambda_l1&quot;: trial.suggest_loguniform(&quot;lambda_l1&quot;, 1e-8, 10.0),\n        &quot;lambda_l2&quot;: trial.suggest_loguniform(&quot;lambda_l2&quot;, 1e-8, 10.0),\n        &quot;num_leaves&quot;: trial.suggest_int(&quot;num_leaves&quot;, 2, 256),\n        &quot;feature_fraction&quot;: trial.suggest_uniform(&quot;feature_fraction&quot;, 0.4, 1.0),\n        &quot;bagging_fraction&quot;: trial.suggest_uniform(&quot;bagging_fraction&quot;, 0.4, 1.0),\n        &quot;bagging_freq&quot;: trial.suggest_int(&quot;bagging_freq&quot;, 1, 7),\n        &quot;min_child_samples&quot;: trial.suggest_int(&quot;min_child_samples&quot;, 5, 100),\n    }\n\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1121218)\n\n    cv_scores = np.empty(5)\n    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        pruning_callback = optuna.integration.LightGBMPruningCallback(trial, &quot;auc&quot;)\n        \n        model = lgb.LGBMClassifier(**param)\n        \n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_test, y_test)],\n            early_stopping_rounds=100,\n            callbacks=[pruning_callback])\n        \n        preds = model.predict_proba(X_test)\n        cv_scores[idx] = log_loss(y_test, preds)\n        auc_scores[idx] = roc_auc_score(y_test, preds)\n        \n    return np.mean(cv_scores), np.mean(auc_scores)\n    \n\n\nstudy = optuna.create_study(direction=&quot;minimize&quot;, study_name=&quot;LGBM Classifier&quot;)\nfunc = lambda trial: objective(trial, sample_df[cols_to_keep], sample_df[target])\n\nstudy.optimize(func, n_trials=1)\n<\/code><\/pre>\n<blockquote>\n<p>Trial 0 failed because of the following error: ValueError('The\nintermediate values are inconsistent with the objective values in\nterms of study directions. Please specify a metric to be minimized for\nLightGBMPruningCallback.',)*<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1649404346652,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648760527507,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71699098",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":18.8,
        "Challenge_reading_time":30.1,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":178.8386513889,
        "Challenge_title":"Optuna LightGBM LightGBMPruningCallback",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":806,
        "Challenge_word_count":162,
        "Platform":"Stack Overflow",
        "Poster_created_time":1380052343127,
        "Poster_location":null,
        "Poster_reputation_count":419.0,
        "Poster_view_count":97.0,
        "Solution_body":"<p>Your objective function returns two values but you specify only one direction when creating the study. Try this:<\/p>\n<pre><code>study = optuna.create_study(directions=[&quot;minimize&quot;, &quot;maximize&quot;], study_name=&quot;LGBM Classifier&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"specifi direct studi creat pass list direct direct paramet creat studi function",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"object function return valu specifi direct creat studi studi creat studi direct minim maxim studi lgbm classifi",
        "Solution_preprocessed_content":"object function return valu specifi direct creat studi",
        "Solution_readability":17.0,
        "Solution_reading_time":3.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1460436951967,
        "Answerer_location":null,
        "Answerer_reputation_count":156.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":0.2497888889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an experiment in azure machine learning studio, and I would like to the see entire scored dataset.<\/p>\n\n<p>Naturally I used the 'visualise' option on the scored dataset but these yields only 100 rows (the test dataset is around 500 rows)<\/p>\n\n<p>I also tired the 'save as dataset' option, but then file does not open well with excel or text editor (special character encoding)<\/p>\n\n<p>Basically, I want to see the entire test data with scored labels as table or download as .csv maybe<\/p>",
        "Challenge_closed_time":1460437058280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1460436159040,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36563769",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":13.1,
        "Challenge_reading_time":6.92,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.2497888889,
        "Challenge_title":"How to download the entire scored dataset from Azure machine studio?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":5296,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Poster_created_time":1406266059940,
        "Poster_location":"Link\u00f6ping, Sweden",
        "Poster_reputation_count":1677.0,
        "Poster_view_count":221.0,
        "Solution_body":"<p>Please try the Convert to CSV module: <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/faa6ba63-383c-4086-ba58-7abf26b85814\" rel=\"noreferrer\">https:\/\/msdn.microsoft.com\/library\/azure\/faa6ba63-383c-4086-ba58-7abf26b85814<\/a><\/p>\n\n<p>After you run the experiment, right click on the output of the module to download the CSV file.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"convert csv modul studio download entir score dataset csv file run click output modul download csv file",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"convert csv modul http msdn com librari faaba abfb run click output modul download csv file",
        "Solution_preprocessed_content":"convert csv modul run click output modul download csv file",
        "Solution_readability":13.5,
        "Solution_reading_time":4.51,
        "Solution_score_count":14.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":28.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1488575811772,
        "Answerer_location":"Bothell, WA, United States",
        "Answerer_reputation_count":51.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":303.7152472222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a model in AzureML that scores incoming values from a csv.<\/p>\n\n<p>The flow is ...->(Score Model using one-class SVM)->(Normalize Data)->(Convert to CSV)->(Convert to Dataset)->(Web Service Output)<\/p>\n\n<p>When the experiment is run I can download the csv from the (Convert to CSV) module output and it will contain Scored Probabilities column.<\/p>\n\n<p>But when I'm using a streaming job I don't know how to access the Scored Probabilities column using Query SQL. How do I do it?<\/p>",
        "Challenge_closed_time":1488576068267,
        "Challenge_comment_count":0,
        "Challenge_created_time":1487482693377,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42324035",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":6.89,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":303.7152472222,
        "Challenge_title":"How to select Scored Probabilities from azure prediction model",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":576,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1300047702248,
        "Poster_location":null,
        "Poster_reputation_count":586.0,
        "Poster_view_count":108.0,
        "Solution_body":"<p>You can access the response using the amlresult.[Scored Probabilities] notation, where amlresult is an alias for the return value from your AzureML call.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"access score probabl column amlresult score probabl notat amlresult alia return valu",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"access respons amlresult score probabl notat amlresult alia return valu",
        "Solution_preprocessed_content":"access respons probabl notat amlresult alia return valu",
        "Solution_readability":9.0,
        "Solution_reading_time":2.03,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1615216681047,
        "Answerer_location":null,
        "Answerer_reputation_count":43.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":0.174105,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a local environment for the ML Studio using the Python SDK, following\n<a href=\"https:\/\/azure.github.io\/azureml-cheatsheets\/docs\/cheatsheets\/python\/v1\/environment\/\" rel=\"nofollow noreferrer\">this official cheatsheet<\/a>. The result should be a conda-like environment that can be used for local testing. However, I am running into an error when importing the Numpy package with the <code>add_conda_package()<\/code> method of the <code>CondaDependencies()<\/code> class. Where I've tried not specifying, as well as specifying package versions, like:\n<code>add_conda_package('numpy')<\/code> or <code>add_conda_package('numpy=1.21.2')<\/code>, but it does not seem to make a difference.<\/p>\n<p>Numpy's error message is extensive, and I've tried many of the suggestions, without success nonetheless. I'm grateful for any tips on what might resolve my issues!<\/p>\n<hr \/>\n<h2>Full code<\/h2>\n<pre><code>from azureml.core import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n\ndef get_env() -&gt; Environment:\n    conda = CondaDependencies()\n\n    # add channels\n    conda.add_channel('defaults')\n    conda.add_channel('conda-forge')\n    conda.add_channel('pytorch')\n\n    # Python\n    conda.add_conda_package('python=3.8')\n\n    # Other conda packages\n    conda.add_conda_package('cudatoolkit=11.3')\n    conda.add_conda_package('pip')\n    conda.add_conda_package('python-dateutil')\n    conda.add_conda_package('python-dotenv')\n    conda.add_conda_package('pytorch=1.10')\n    conda.add_conda_package('torchaudio')\n    conda.add_conda_package('torchvision')\n    conda.add_conda_package('wheel')\n    conda.add_conda_package('numpy=1.21.2') # &lt;--- Error with this import \n\n    # create environment\n    env = Environment('test_env')\n    env.python.conda_dependencies = conda\n\n    return env\n<\/code><\/pre>\n<hr \/>\n<h2>Detailed error message:<\/h2>\n<p>User program failed with ImportError:<\/p>\n<p>IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!<\/p>\n<p>Importing the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.<\/p>\n<p>We have compiled some common reasons and troubleshooting tips at:<\/p>\n<pre><code>https:\/\/numpy.org\/devdocs\/user\/troubleshooting-importerror.html\n<\/code><\/pre>\n<p>Please note and check the following:<\/p>\n<ul>\n<li>The Python version is: Python3.8 from &quot;&lt;LOCAL_DIR&gt;.azureml\\envs\\azureml_&gt;\\python.exe&quot;<\/li>\n<li>The NumPy version is: &quot;1.19.1&quot;<\/li>\n<\/ul>\n<p>and make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.<\/p>\n<p>Original error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.<\/p>\n<hr \/>\n<h2>System specifications:<\/h2>\n<ul>\n<li>Local OS: Windows 10<\/li>\n<li>ML studio OS: Linux Ubuntu 18<\/li>\n<li>Python version: 3.8<\/li>\n<\/ul>",
        "Challenge_closed_time":1637743033888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637742407110,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70092793",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":38.52,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":0.174105,
        "Challenge_title":"Azure ML Studio Local Environment \u2014 Numpy package import failure using the Azure ML Python SDK",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":135,
        "Challenge_word_count":302,
        "Platform":"Stack Overflow",
        "Poster_created_time":1615216681047,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>I was finally able to resolve the issue by using the pip method instead of the conda method:\n<code>add_pip_package('numpy')<\/code> instead of <code>add_conda_package('numpy')<\/code>\nI can imagine this being the reason for other packages as well.<\/p>\n<hr \/>\n<h2>Full solution<\/h2>\n<pre><code>from azureml.core import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n\ndef get_env() -&gt; Environment:\n    conda = CondaDependencies()\n\n    # add channels\n    conda.add_channel('defaults')\n    conda.add_channel('conda-forge')\n    conda.add_channel('pytorch')\n\n    # Python\n    conda.add_conda_package('python=3.8')\n\n    # Other conda packages\n    conda.add_conda_package('cudatoolkit=11.3')\n    conda.add_conda_package('pip')\n    conda.add_conda_package('python-dateutil')\n    conda.add_conda_package('python-dotenv')\n    conda.add_conda_package('pytorch=1.10')\n    conda.add_conda_package('torchaudio')\n    conda.add_conda_package('torchvision')\n    conda.add_conda_package('wheel')\n    #conda.add_conda_package('numpy=1.21.2') # &lt;--- Error with this import \n\n    # Add pip packages\n    conda.add_pip_package('numpy') # &lt;--- Fixes import error\n\n    # create environment\n    env = Environment('test_env')\n    env.python.conda_dependencies = conda\n\n    return env\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"pip conda import numpi packag involv replac line add conda packag numpi add pip packag numpi",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"final pip conda add pip packag numpi add conda packag numpi imagin reason packag core import environ core conda depend import condadepend env environ conda condadepend add channel conda add channel default conda add channel conda forg conda add channel pytorch conda add conda packag conda packag conda add conda packag cudatoolkit conda add conda packag pip conda add conda packag dateutil conda add conda packag dotenv conda add conda packag pytorch conda add conda packag torchaudio conda add conda packag torchvis conda add conda packag wheel conda add conda packag numpi import add pip packag conda add pip packag numpi import creat environ env environ test env env conda depend conda return env",
        "Solution_preprocessed_content":"final pip conda imagin reason packag",
        "Solution_readability":22.6,
        "Solution_reading_time":16.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":92.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.6372286111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've followed the documentation pretty well as outlined <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-custom-docker-image\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>I've setup my azure machine learning environment the following way:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace\n\n# Connect to the workspace\nws = Workspace.from_config()\n\nfrom azureml.core import Environment\nfrom azureml.core import ContainerRegistry\n\nmyenv = Environment(name = &quot;myenv&quot;)\n\nmyenv.inferencing_stack_version = &quot;latest&quot;  # This will install the inference specific apt packages.\n\n# Docker\nmyenv.docker.enabled = True\nmyenv.docker.base_image_registry.address = &quot;myazureregistry.azurecr.io&quot;\nmyenv.docker.base_image_registry.username = &quot;myusername&quot;\nmyenv.docker.base_image_registry.password = &quot;mypassword&quot;\nmyenv.docker.base_image = &quot;4fb3...&quot; \nmyenv.docker.arguments = None\n\n# Environment variable (I need python to look at folders \nmyenv.environment_variables = {&quot;PYTHONPATH&quot;:&quot;\/root&quot;}\n\n# python\nmyenv.python.user_managed_dependencies = True\nmyenv.python.interpreter_path = &quot;\/opt\/miniconda\/envs\/myenv\/bin\/python&quot; \n\nfrom azureml.core.conda_dependencies import CondaDependencies\nconda_dep = CondaDependencies()\nconda_dep.add_pip_package(&quot;azureml-defaults&quot;)\nmyenv.python.conda_dependencies=conda_dep\n\nmyenv.register(workspace=ws) # works!\n<\/code><\/pre>\n<p>I have a score.py file configured for inference (not relevant to the problem I'm having)...<\/p>\n<p>I then setup inference configuration<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.model import InferenceConfig\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=myenv)\n<\/code><\/pre>\n<p>I setup my compute cluster:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.compute import ComputeTarget, AksCompute\nfrom azureml.exceptions import ComputeTargetException\n\n# Choose a name for your cluster\naks_name = &quot;theclustername&quot; \n\n# Check to see if the cluster already exists\ntry:\n    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n    print('Found existing compute target')\nexcept ComputeTargetException:\n    print('Creating a new compute target...')\n    prov_config = AksCompute.provisioning_configuration(vm_size=&quot;Standard_NC6_Promo&quot;)\n\n    aks_target = ComputeTarget.create(workspace=ws, name=aks_name, provisioning_configuration=prov_config)\n\n    aks_target.wait_for_completion(show_output=True)\n\nfrom azureml.core.webservice import AksWebservice\n\n# Example\ngpu_aks_config = AksWebservice.deploy_configuration(autoscale_enabled=False,\n                                                    num_replicas=3,\n                                                    cpu_cores=4,\n                                                    memory_gb=10)\n<\/code><\/pre>\n<p>Everything succeeds; then I try and deploy the model for inference:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.model import Model\n\nmodel = Model(ws, name=&quot;thenameofmymodel&quot;)\n\n# Name of the web service that is deployed\naks_service_name = 'tryingtodeply'\n\n# Deploy the model\naks_service = Model.deploy(ws,\n                           aks_service_name,\n                           models=[model],\n                           inference_config=inference_config,\n                           deployment_config=gpu_aks_config,\n                           deployment_target=aks_target,\n                           overwrite=True)\n\naks_service.wait_for_deployment(show_output=True)\nprint(aks_service.state)\n<\/code><\/pre>\n<p>And it fails saying that it can't find the environment. More specifically, my environment version is <strong>version 11<\/strong>, but it keeps trying to find an environment with a version number that is 1 higher (i.e., <strong>version 12<\/strong>) than the current environment:<\/p>\n<pre><code>FailedERROR - Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: 0f03a025-3407-4dc1-9922-a53cc27267d4\nMore information can be found here: \nError:\n{\n  &quot;code&quot;: &quot;BadRequest&quot;,\n  &quot;statusCode&quot;: 400,\n  &quot;message&quot;: &quot;The request is invalid&quot;,\n  &quot;details&quot;: [\n    {\n      &quot;code&quot;: &quot;EnvironmentDetailsFetchFailedUserError&quot;,\n      &quot;message&quot;: &quot;Failed to fetch details for Environment with Name: myenv Version: 12.&quot;\n    }\n  ]\n}\n\n<\/code><\/pre>\n<p>I have tried to manually edit the environment JSON to match the version that azureml is trying to fetch, but nothing works. Can anyone see anything wrong with this code?<\/p>\n<h1>Update<\/h1>\n<p>Changing the name of the environment (e.g., <code>my_inference_env<\/code>) and passing it to <code>InferenceConfig<\/code> seems to be on the right track. However, the error now changes to the following<\/p>\n<pre><code>Running..........\nFailed\nERROR - Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: f0dfc13b-6fb6-494b-91a7-de42b9384692\nMore information can be found here: https:\/\/some_long_http_address_that_leads_to_nothing\nError:\n{\n  &quot;code&quot;: &quot;DeploymentFailed&quot;,\n  &quot;statusCode&quot;: 404,\n  &quot;message&quot;: &quot;Deployment not found&quot;\n}\n<\/code><\/pre>\n<h1>Solution<\/h1>\n<p>The answer from Anders below is <strong>indeed correct<\/strong> regarding the use of azure ML environments. However, the last error I was getting was because I was setting the <em>container image<\/em> using the digest value (a sha) and NOT the image name and tag (e.g., <code>imagename:tag<\/code>). Note the line of code in the first block:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv.docker.base_image = &quot;4fb3...&quot; \n<\/code><\/pre>\n<p>I reference the digest value, but it should be changed to<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv.docker.base_image = &quot;imagename:tag&quot;\n<\/code><\/pre>\n<p>Once I made that change, the deployment succeeded! :)<\/p>",
        "Challenge_closed_time":1597702121696,
        "Challenge_comment_count":5,
        "Challenge_created_time":1597699827673,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1599771558392,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63458904",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":17.3,
        "Challenge_reading_time":77.15,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":0.6372286111,
        "Challenge_title":"Azure-ML Deployment does NOT see AzureML Environment (wrong version number)",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1768,
        "Challenge_word_count":509,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432406490590,
        "Poster_location":"Milwaukee, WI",
        "Poster_reputation_count":381.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>One concept that took me a while to get was the bifurcation of registering and using an Azure ML <code>Environment<\/code>. If you have already registered your env, <code>myenv<\/code>, and none of the details of the your environment have changed, there is no need re-register it with <code>myenv.register()<\/code>. You can simply get the already register env using <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.environment?view=azure-ml-py#get-workspace--name--version-none-\" rel=\"nofollow noreferrer\"><code>Environment.get()<\/code><\/a> like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv = Environment.get(ws, name='myenv', version=11)\n<\/code><\/pre>\n<p>My recommendation would be to name your environment something new: like <code>&quot;model_scoring_env&quot;<\/code>. Register it once, then pass it to the <code>InferenceConfig<\/code>.<\/p>",
        "Solution_comment_count":9.0,
        "Solution_gpt_summary":"set imag imag tag digest valu environ regist pass inferenceconfig",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"concept took bifurc regist environ regist env myenv environ regist myenv regist simpli regist env environ myenv environ myenv version environ model score env regist pass inferenceconfig",
        "Solution_preprocessed_content":"concept took bifurc regist regist env environ simpli regist env environ regist pass",
        "Solution_readability":14.2,
        "Solution_reading_time":11.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":11.5363191667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm Stumped.  <\/p>\n\n<p>I took my TensorFlow model and moved it up into SageMaker to try it out.  I put my own data up into an s3 bucket, set all the IAM roles\/access (or so I think).  I can read a file from s3.  I can push a new file to s3. I can read local directories from my SageMaker local directories.<\/p>\n\n<p><strong>I cannot traverse my s3 bucket directories.<\/strong>  I turned on logging and I get AccessDenied messages whenever I try access a URI of this format <strong>'s3:\/\/my_bucketName_here\/Directory_of_my_data\/'<\/strong>.<\/p>\n\n<p>Here is what I've done:\nI've confirmed that my notebook uses the AmazonSageMaker-ExecutionRole-***\nI've added AmazonSageMakerFullAccess Policy to that default role\nI've subsequently added AmazonS3FullAccess Policy as well<\/p>\n\n<p>I then created a bucket policy specifically granting s3:* access on the specific bucket to that specific role.<\/p>\n\n<p>Heck, I eventually made the bucket public with ListObjects = Yes.<\/p>\n\n<p>os.listdir() simply fails with file or directory not found and a lot message is created with AccessDenied. (TensorFlow libraries just didn't work, so I went with os.listdir() to simplify things.<\/p>\n\n<p>Finally, I test my access from the Policy Simulator - I selected the Role mentioned above, selected to test s3 and selected all 69 items and they all passed.<\/p>\n\n<p>But I continue to log AccessDenied and cannot actually list the contents of a directory from my SageMaker jupyter notebook.<\/p>\n\n<p>I'm at a loss.  Thoughts?<\/p>\n\n<p>EDIT:\nPer suggestion below, I have the following:\nbucket name contains sagemaker: '[redacted]-test-sagemaker'\nPublic access is off, and the only account is my root account.\n<code>\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::[redacted]-test-sagemaker\"\n            ]\n        },\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::[redacted]-test-sagemaker\/*\"\n            ]\n        }\n    ]\n}<\/code>\nand\narn:aws:iam::aws:policy\/AmazonSageMakerFullAccess<\/p>\n\n<p>Finally the bucket policy after the above failed:\n<code>{\n  \"Id\": \"Policy1534116031672\",\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Stmt1534116026409\",\n      \"Action\": \"s3:*\",\n      \"Effect\": \"Allow\",\n      \"Resource\": \"arn:aws:s3:::[redacted]-test-sagemaker\",\n      \"Principal\": {\n        \"AWS\": [\n          \"arn:aws:iam::[id]:role\/service-role\/AmazonSageMaker-ExecutionRole-***\"\n        ]\n      }\n    }\n  ]\n}<\/code><\/p>",
        "Challenge_closed_time":1534057177852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1534015647103,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1534116796512,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51803032",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":31.2,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":11.5363191667,
        "Challenge_title":"AWS SageMaker S3 os.listdir() Access denied",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":2961,
        "Challenge_word_count":304,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449682589303,
        "Poster_location":"Phoenix, AZ, United States",
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>So you need to troubleshoot. Here are a few things to check:   <\/p>\n\n<p>0) Make sure the bucket is in the SageMaker region.<\/p>\n\n<p>1) Include the string \"sagemaker\" in your bucket name (e.g., <em>my_bucketName_here-sagemaker<\/em>, SageMaker has out of the box access to buckets named this way.<\/p>\n\n<p>2) Try using the SageMaker S3 <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py\" rel=\"nofollow noreferrer\">default_bucket()<\/a>:<\/p>\n\n<pre><code>import sagemaker\ns = sagemaker.Session()\ns.upload_data(path='somefile.csv', bucket=s.default_bucket(), key_prefix='data\/train')\n<\/code><\/pre>\n\n<p>3) Open terminal on the Notebook instance, to try to list your bucket using AWS CLI in bash:<\/p>\n\n<pre><code>aws iam get-user\naws s3 ls my_bucketName_here\n<\/code><\/pre>\n\n<p>Finally, pasting the bucket's access and resource policy in your question could help others to answer you.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_gpt_summary":"bucket region bucket default bucket open termin notebook instanc list bucket cli bash past bucket access resourc polici",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"troubleshoot bucket region bucket bucketnam box access bucket default bucket import session upload data path somefil csv bucket default bucket kei prefix data train open termin notebook instanc list bucket cli bash iam bucketnam final past bucket access resourc polici",
        "Solution_preprocessed_content":"troubleshoot bucket region bucket open termin notebook instanc list bucket cli bash final past bucket access resourc polici",
        "Solution_readability":12.8,
        "Solution_reading_time":11.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":107.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":3.1229155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What could be the point I am missing here. All datasets and training is done on gcp and my setup is basic.<\/p>\n<ol>\n<li>Training, validation and test done on JupyterLab<\/li>\n<li>Model pushed to a gcp storage bucket<\/li>\n<li>Create and endpoint<\/li>\n<li>Deploy model to endpoint.<\/li>\n<\/ol>\n<p>All steps looked fine until the last (4). Tried other pre-built pytorch images recommended by google but the error is persisting. The error long is as shown below:<\/p>\n<pre><code>---------------------------------------------------------------------------\n_InactiveRpcError                         Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     65         try:\n---&gt; 66             return callable_(*args, **kwargs)\n     67         except grpc.RpcError as exc:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\n    945                                       wait_for_ready, compression)\n--&gt; 946         return _end_unary_response_blocking(state, call, False, None)\n    947 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\n    848     else:\n--&gt; 849         raise _InactiveRpcError(state)\n    850 \n\n_InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\n    status = StatusCode.INVALID_ARGUMENT\n    details = &quot;Invalid image &quot;us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest&quot; for deployment. Please use a Model with a valid image.&quot;\n    debug_error_string = &quot;{&quot;created&quot;:&quot;@1652032269.328842405&quot;,&quot;description&quot;:&quot;Error received from peer ipv4:142.250.148.95:443&quot;,&quot;file&quot;:&quot;src\/core\/lib\/surface\/call.cc&quot;,&quot;file_line&quot;:903,&quot;grpc_message&quot;:&quot;Invalid image &quot;us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest&quot; for deployment. Please use a Model with a valid image.&quot;,&quot;grpc_status&quot;:3}&quot;\n&gt;\n\nThe above exception was the direct cause of the following exception:\n\nInvalidArgument                           Traceback (most recent call last)\n\/tmp\/ipykernel_2924\/2180059764.py in &lt;module&gt;\n      5     machine_type = DEPLOY_COMPUTE,\n      6     min_replica_count = 1,\n----&gt; 7     max_replica_count = 1\n      8 )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in deploy(self, model, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, sync)\n    697             explanation_parameters=explanation_parameters,\n    698             metadata=metadata,\n--&gt; 699             sync=sync,\n    700         )\n    701 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/base.py in wrapper(*args, **kwargs)\n    728                 if self:\n    729                     VertexAiResourceNounWithFutureManager.wait(self)\n--&gt; 730                 return method(*args, **kwargs)\n    731 \n    732             # callbacks to call within the Future (in same Thread)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in _deploy(self, model, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, sync)\n    812             explanation_metadata=explanation_metadata,\n    813             explanation_parameters=explanation_parameters,\n--&gt; 814             metadata=metadata,\n    815         )\n    816 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in _deploy_call(cls, api_client, endpoint_resource_name, model_resource_name, endpoint_resource_traffic_split, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata)\n    979             deployed_model=deployed_model,\n    980             traffic_split=traffic_split,\n--&gt; 981             metadata=metadata,\n    982         )\n    983 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1\/services\/endpoint_service\/client.py in deploy_model(self, request, endpoint, deployed_model, traffic_split, retry, timeout, metadata)\n   1155 \n   1156         # Send the request.\n-&gt; 1157         response = rpc(request, retry=retry, timeout=timeout, metadata=metadata,)\n   1158 \n   1159         # Wrap the response in an operation future.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method.py in __call__(self, timeout, retry, *args, **kwargs)\n    152             kwargs[&quot;metadata&quot;] = metadata\n    153 \n--&gt; 154         return wrapped_func(*args, **kwargs)\n    155 \n    156 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     66             return callable_(*args, **kwargs)\n     67         except grpc.RpcError as exc:\n---&gt; 68             raise exceptions.from_grpc_error(exc) from exc\n     69 \n     70     return error_remapped_callable\n\nInvalidArgument: 400 Invalid image &quot;us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest&quot; for deployment. Please use a Model with a valid image.\n<\/code><\/pre>\n<p>Below are some details where I create model, endpoint and deploy to endpoint.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>DEPLOY_COMPUTE = 'n1-standard-4'\nDEPLOY_IMAGE='us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest'\n\nmodel = aip.Model.upload(\n    display_name = f'{NOTEBOOK}_{TIMESTAMP}',\n    serving_container_image_uri = DEPLOY_IMAGE,\n    artifact_uri = URI,\n    labels = {'notebook':f'{NOTEBOOK}'}\n)\n\nendpoint = aip.Endpoint.create(\n    display_name = f'{NOTEBOOK}_{TIMESTAMP}',\n    labels = {'notebook':f'{NOTEBOOK}'}\n)\n\nendpoint.deploy(\n    model = model,\n    deployed_model_display_name = f'{NOTEBOOK}_{TIMESTAMP}',\n    traffic_percentage = 100,\n    machine_type = DEPLOY_COMPUTE,\n    min_replica_count = 1,\n    max_replica_count = 1\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1652252560712,
        "Challenge_comment_count":2,
        "Challenge_created_time":1652033668977,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1652241318216,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72163917",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":18.2,
        "Challenge_reading_time":78.74,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":42,
        "Challenge_solved_time":60.8032597222,
        "Challenge_title":"400 Invalid image \"us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest\" for deployment. Please use a Model with a valid image",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":208,
        "Challenge_word_count":456,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426920929352,
        "Poster_location":"Bangkok",
        "Poster_reputation_count":194.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>You are using <code>us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-11:latest<\/code> <strong>container image<\/strong> for importing models. However, models trained in <code>pytorch<\/code> cannot use pre-built containers when importing models since as mentioned in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/model-registry\/import-model#container-type\" rel=\"nofollow noreferrer\">documentation<\/a>,<\/p>\n<blockquote>\n<p>You can use a pre-built container if your model meets the following\nrequirements:<\/p>\n<ul>\n<li>Trained in Python 3.7 or later<\/li>\n<li>Trained using TensorFlow, scikit-learn, or XGBoost<\/li>\n<li>Exported to meet framework-specific requirements for one of the pre-built prediction containers<\/li>\n<\/ul>\n<\/blockquote>\n<p>I suggest 2 workaround options for your use case:<\/p>\n<ol>\n<li><p>You can create a custom prediction container image for your <code>pytorch<\/code> trained model by referring to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-container\" rel=\"nofollow noreferrer\">documentation<\/a> .<\/p>\n<p>or<\/p>\n<\/li>\n<li><p>Re-train your model that meets the above requirements so that you can use the pre-bult container.<\/p>\n<\/li>\n<\/ol>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"creat predict imag pytorch train model document retrain model meet document pre built bias summari",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"docker pkg dev vertex train pytorch gpu latest imag import model model train pytorch pre built import model document pre built model meet train later train tensorflow scikit export meet framework pre built predict workaround option creat predict imag pytorch train model document train model meet pre bult",
        "Solution_preprocessed_content":"imag import model model train import model document model meet train later train tensorflow export meet predict workaround option creat predict imag train model document model meet",
        "Solution_readability":15.2,
        "Solution_reading_time":15.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":123.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1362230894870,
        "Answerer_location":null,
        "Answerer_reputation_count":165.0,
        "Answerer_view_count":33.0,
        "Challenge_adjusted_solved_time":0.5939086111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Im running the keras examples from <a href=\"https:\/\/github.com\/comet-ml\/comet-keras-example\/blob\/master\/comet_keras_example.py\" rel=\"nofollow noreferrer\">Comet github project<\/a> .<\/p>\n\n<p>I add the import and create a new experiment:<\/p>\n\n<pre><code>def train(x_train,y_train,x_test,y_test):\nmodel = build_model_graph()\n\nfrom comet_ml import Experiment\n\nexperiment = Experiment(api_key=\"XXXX\", log_code=True)\n\nmodel.fit(x_train, y_train, batch_size=128, epochs=50, validation_data=(x_test, y_test))\n\nscore = model.evaluate(x_test, y_test, verbose=0)\n<\/code><\/pre>\n\n<p>and when i run my training code it fails.<\/p>\n\n<p>error:<\/p>\n\n<pre><code>Using TensorFlow backend.\nTraceback (most recent call last):\n  File \"\/Users\/nimrodlahav\/Code\/semantica\/experiment-logger-client\/train-examples\/keras-example.py\", line 21, in &lt;module&gt;\n    from comet_ml import Experiment\n  File \"..\/.\/comet-client-lib\/comet_ml\/__init__.py\", line 3, in &lt;module&gt;\n    from .comet import Experiment\n  File \"..\/.\/comet-client-lib\/comet_ml\/comet.py\", line 29, in &lt;module&gt;\n    from comet_ml import keras_logger\n  File \"..\/.\/comet-client-lib\/comet_ml\/keras_logger.py\", line 31, in &lt;module&gt;\n    raise SyntaxError(\"Please import Comet before importing any keras modules\")\nSyntaxError: Please import Comet before importing any keras modules\n<\/code><\/pre>\n\n<p>what am I missing?<\/p>",
        "Challenge_closed_time":1506026797448,
        "Challenge_comment_count":0,
        "Challenge_created_time":1506024659377,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1506491876827,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46352435",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":18.33,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.5939086111,
        "Challenge_title":"comet (comet-ml) fails to run with Keras",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":601,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Poster_created_time":1505841491572,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I don't see start of the code but it looks like you have imported Keras before you have imported Comet.<\/p>\n\n<p>From the error message it looks like just need to switch the import lines (Comet first Keras second), like in your example:<\/p>\n\n<pre><code>from comet_ml import Experiment\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import RMSprop \n<\/code><\/pre>\n\n<p>view the full source code <a href=\"https:\/\/github.com\/comet-ml\/comet-keras-example\" rel=\"nofollow noreferrer\">example<\/a> .<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"messag import import kera modul switch import line import kera import",
        "Solution_last_edit_time":1506066589407,
        "Solution_link_count":1.0,
        "Solution_original_content":"start import kera import messag switch import line kera import import kera kera dataset import mnist kera model import sequenti kera layer import dens dropout kera optim import rmsprop sourc",
        "Solution_preprocessed_content":"start import kera import messag switch import line sourc",
        "Solution_readability":7.5,
        "Solution_reading_time":7.54,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":76.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":3.3754333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run custom python\/sklearn sagemaker script on AWS, basically learning from these examples: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>\n<p>All works fine, if define the arguments, train the model and output the file:<\/p>\n<pre><code>parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\nparser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\nparser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n# train the model...\n\njoblib.dump(model, os.path.join(args.model_dir, &quot;model.joblib&quot;))\n<\/code><\/pre>\n<p>And call the job with:<\/p>\n<pre><code>aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test'}, wait=False)\n<\/code><\/pre>\n<p>In this case model gets stored on different auto-generated bucket, which I do not want. I want to get the output (.joblib file) in the same s3 bucket I took data from. So I add the parameter <code>model-dir<\/code>:<\/p>\n<pre><code>aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test', `model-dir`: 's3:\/\/path\/to\/model'}, wait=False)\n<\/code><\/pre>\n<p>But it results in error:\n<code>FileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/path\/to\/model\/model.joblib'<\/code><\/p>\n<p>Same happens if I hardcode the output path inside the training script.<\/p>\n<p>So the main question, how can I get the output file in the bucket of my choice?<\/p>",
        "Challenge_closed_time":1610545645387,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610533493827,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65699980",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":23.75,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":3.3754333334,
        "Challenge_title":"Change model file save location on AWS SageMaker Training Job",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1244,
        "Challenge_word_count":159,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572957474856,
        "Poster_location":null,
        "Poster_reputation_count":123.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>You can use parameter <code>output_path<\/code> when you define the estimator. If you use the\n<code>model_dir<\/code> I guess you have to create that bucket beforehand, but you have the advantage that artifacts can be saved in real time during the training (if the instance has rights on S3). You can take a look at my <a href=\"https:\/\/github.com\/roccopietrini\/TFSagemakerDetection\" rel=\"nofollow noreferrer\">repo<\/a> for this specific case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"output path paramet defin estim save output file desir bucket model dir paramet creat bucket instanc save artifact time train repositori",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"paramet output path defin estim model dir guess creat bucket advantag artifact save time train instanc repo",
        "Solution_preprocessed_content":"paramet defin estim guess creat bucket advantag artifact save time train repo",
        "Solution_readability":10.5,
        "Solution_reading_time":5.66,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1497960178323,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":12088.0,
        "Answerer_view_count":3630.0,
        "Challenge_adjusted_solved_time":16.0933286111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to creating similar machine learning experiment as on found on github at below link.<\/p>\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-orange-juice-sales\/auto-ml-forecasting-orange-juice-sales.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-orange-juice-sales\/auto-ml-forecasting-orange-juice-sales.ipynb<\/a><\/p>\n<p>what could I do to resolve the ImportEror:?<\/p>",
        "Challenge_closed_time":1606803664676,
        "Challenge_comment_count":1,
        "Challenge_created_time":1606745728693,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65075228",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":26.7,
        "Challenge_reading_time":9.17,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":16.0933286111,
        "Challenge_title":"How could I resolve ImportError: no module named 'azureml', I am using Azure databricks notebook",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1063,
        "Challenge_word_count":43,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483978710216,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":17.0,
        "Solution_body":"<blockquote>\n<p>To resolve this issue, I would request you to install azureml library from PyPi packages.<\/p>\n<\/blockquote>\n<p>To make third-party or locally-built code available to notebooks and jobs running on your clusters, you can install a library. Libraries can be written in Python, Java, Scala, and R. You can upload Java, Scala, and Python libraries and point to external packages in PyPI, Maven, and CRAN repositories.<\/p>\n<p><strong>Steps to install third-party libraries:<\/strong><\/p>\n<p><strong>Step1:<\/strong> Create Databricks Cluster.<\/p>\n<p><strong>Step2:<\/strong> Select the cluster created.<\/p>\n<p><strong>Step3:<\/strong> Select Libraries =&gt; Install New =&gt; Select Library Source = &quot;PYPI&quot; =&gt; Package = &quot;azureml-sdk[databricks]&quot;.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DafqR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DafqR.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wCjUG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wCjUG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/HD6VC.gif\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/HD6VC.gif\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Reference:<\/strong> <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-databricks-automl-environment\" rel=\"nofollow noreferrer\">Set up a development environment with Azure Databricks and autoML in Azure Machine Learning<\/a><\/p>\n<p>For different methods to install packages in Azure Databricks: <a href=\"https:\/\/stackoverflow.com\/questions\/60543850\/how-to-install-a-library-on-a-databricks-cluster-using-some-command-in-the-noteb\/60557852#60557852\">How to install a library on a databricks cluster using some command in the notebook?<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"instal librari pypi packag step instal librari creat cluster select cluster instal librari pypi packag link link stack overflow instal packag",
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_original_content":"request instal librari pypi packag parti local built notebook job run cluster instal librari librari written java scala upload java scala librari extern packag pypi maven cran repositori step instal parti librari step creat cluster step select cluster creat step select librari instal select librari sourc pypi packag sdk set environ automl instal packag instal librari cluster notebook",
        "Solution_preprocessed_content":"request instal librari pypi packag notebook job run cluster instal librari librari written java scala upload java scala librari extern packag pypi maven cran repositori step instal librari step creat cluster step select cluster creat step select librari instal select librari sourc pypi packag set environ automl instal packag instal librari cluster notebook",
        "Solution_readability":12.1,
        "Solution_reading_time":24.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":167.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1351154914716,
        "Answerer_location":null,
        "Answerer_reputation_count":2564.0,
        "Answerer_view_count":451.0,
        "Challenge_adjusted_solved_time":50.1200916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a Vetex AI dataset in <code>us-central1<\/code> and confirm it exists using:<\/p>\n<pre><code>vertex_ai.TabularDataset.list()\n<\/code><\/pre>\n<p>When I look at the UI I don't see any datsets, but I see a region drop-down, but no <code>us-central1<\/code>. Why is that? (The project is the correct one).<\/p>",
        "Challenge_closed_time":1652357677267,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652177244937,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72184371",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":4.5,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":50.1200916667,
        "Challenge_title":"Why is my Vertex AI dataset not displayed?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":89,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>It <strong>is<\/strong> there but at the beginning of the list, not with the other US ones.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"dataset present displai dataset begin list dataset",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":6.0,
        "Solution_reading_time":1.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1320061998252,
        "Answerer_location":null,
        "Answerer_reputation_count":778.0,
        "Answerer_view_count":89.0,
        "Challenge_adjusted_solved_time":52.7947772223,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I'm trying to create a '<strong>Reader<\/strong>' alternative to read data from Azure SQL Database using the 'Execute python script' module in <strong>Azure ML<\/strong>.\nwhile doing so, I'm trying to connect to Azure Sql using pyodbc library.\nhere's my code:<\/p>\n\n<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):\n    import pyodbc   \n    import pandas as pd\n\n    conn = pyodbc.connect('DRIVER={SQL Server}; SERVER=server.database.windows.net; DATABASE=db_name; UID=user; PWD=Password')\n    SQLCommand = ('''select * from table1 ''')\n    data_frame = pd.read_sql(SQLCommand, conn)\n    return data_frame,\n<\/code><\/pre>\n\n<p>also tried to use a different driver name: {SQL Server Native Client 11.0}<\/p>\n\n<p>Here is the error i'm getting:<\/p>\n\n<pre><code>Error: ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')\n<\/code><\/pre>\n\n<p>Does anybody know which driver should I use?<\/p>\n\n<p>just to make sure, I tried  \"{SQL Server}\", \"{SQL Server Native Client 11.0}\" and \"{SQL Server Native Client 10.0}\" and got the same error<\/p>\n\n<p>I also tried a different format: <\/p>\n\n<pre><code>conn = pyodbc.connect('DRIVER={SQL Server}; SERVER=server.database.windows.net; DATABASE=db_name; user=user@server; password=Password')\n<\/code><\/pre>\n\n<p>and <\/p>\n\n<pre><code>conn = pyodbc.connect('DRIVER={SQL Server Native Client 11.0}; SERVER=server.database.windows.net; DATABASE=db_name; user=user@server; password=Password')\n<\/code><\/pre>",
        "Challenge_closed_time":1456990625532,
        "Challenge_comment_count":1,
        "Challenge_created_time":1456664661510,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":1456812850912,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35682879",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":10.4,
        "Challenge_reading_time":20.35,
        "Challenge_score_count":6,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":90.5455616667,
        "Challenge_title":"What is the name of the driver to connect to Azure SQL Database from pyodbc in Azure ML?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2168,
        "Challenge_word_count":182,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320061998252,
        "Poster_location":null,
        "Poster_reputation_count":778.0,
        "Poster_view_count":89.0,
        "Solution_body":"<p>I got an answer from azure support:<\/p>\n\n<blockquote>\n  <p>Currently it is not possible to access sql azure dbs from within  an\n  \u201cexecute python script\u201d module. As you suspected this is due to\n  missing odbc drivers in the execution environment.   Suggested\n  workarounds are to  a) use reader module   or   b) export to blobs\n  and use the Azure Python SDK for accessing those blobs\n  <a href=\"http:\/\/blogs.msdn.com\/b\/bigdatasupport\/archive\/2015\/10\/02\/using-azure-sdk-for-python.aspx\" rel=\"nofollow\">http:\/\/blogs.msdn.com\/b\/bigdatasupport\/archive\/2015\/10\/02\/using-azure-sdk-for-python.aspx<\/a><\/p>\n<\/blockquote>\n\n<p>So currently it it <strong>impossible<\/strong> to connect to SQL server from \u201cexecute python script\u201d module in Azure-ML. If you like to change it, please vote <a href=\"https:\/\/feedback.azure.com\/forums\/257792-machine-learning\/suggestions\/12589266-enable-odbc-connection-from-excute-python-script\" rel=\"nofollow\">here<\/a> <\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"reader modul pyodbc librari access sql databas export data blob sdk access blob messag",
        "Solution_last_edit_time":1457002912110,
        "Solution_link_count":3.0,
        "Solution_original_content":"access sql db execut modul miss odbc driver execut environ workaround reader modul export blob sdk access blob http blog msdn com bigdatasupport archiv sdk aspx imposs connect sql server execut modul vote",
        "Solution_preprocessed_content":"access sql db execut modul miss odbc driver execut environ workaround reader modul export blob sdk access blob imposs connect sql server execut modul vote",
        "Solution_readability":14.2,
        "Solution_reading_time":12.19,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":95.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":1.3060847222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have a DataSet defined in my AzureML workspace that is linked to an Azure Blob Storage csv file of 1.6Gb.  This file contains timeseries information of around 10000 devices.  So, I could've also created 10000 smaller files (since I use ADF for the transmission pipeline).<\/p>\n\n<p>My question now is: is it possible to load a part of the AzureML DataSet in my python notebook or script instead of loading the entire file?<br>\nThe only code I have now load the full file:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>dataset = Dataset.get_by_name(workspace, name='devicetelemetry')\ndf = dataset.to_pandas_dataframe()\n<\/code><\/pre>\n\n<p>The only concept of partitions I found with regards to the AzureML datasets was around time series and partitioning of timestamps &amp; dates.  However, here I would love to partition per device, so I can very easily just do a load of all telemetry of a specific device.<\/p>\n\n<p>Any pointers to docs or any suggestions? (I couldn't find any so far)<\/p>\n\n<p>Thanks already<\/p>",
        "Challenge_closed_time":1585761136672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585756434767,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60975078",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":6.9,
        "Challenge_reading_time":13.71,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.3060847222,
        "Challenge_title":"How to only load one portion of an AzureML tabular dataset (linked to Azure Blob Storage)",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":560,
        "Challenge_word_count":169,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>You're right there are the <code>.time_*()<\/code> filtering methods available with a <code>TabularDataset<\/code>.<\/p>\n\n<p>I'm not aware of anyway to do filtering as you suggest (but I agree it would be a useful feature). To get per-device partitioning, my recommendation would be to structure your container like so:<\/p>\n\n<pre><code>- device1\n    - 2020\n        - 2020-03-31.csv\n        - 2020-04-01.csv\n- device2\n   - 2020\n        - 2020-03-31.csv\n        - 2020-04-01.csv\n<\/code><\/pre>\n\n<p>In this way you can define an all-up Dataset, but also per-device Datasets by passing folder of the device to the DataPath<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># all up dataset\nds_all = Dataset.Tabular.from_delimited_files(\n    path=DataPath(datastore, '*')\n)\n# device 1 dataset\nds_d1 = Dataset.Tabular.from_delimited_files(\n    path=DataPath(datastore, 'device1\/*')\n)\n<\/code><\/pre>\n\n<p><strong>CAVEAT<\/strong><\/p>\n\n<p>dataprep SDK is optimized for blobs around 200MB in size. So you can work with many small files, but sometimes it can be slower than expected, especially considering the overhead of enumerating all blobs in a container.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"time filter tabulardataset filter data structur allow devic partit achiev creat folder devic creat subfold year csv file dai allow defin dataset devic dataset pass folder devic datapath dataprep sdk optim blob size small file slower overhead enumer blob",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"time filter tabulardataset awar filter agre featur devic partit structur devic csv csv devic csv csv defin dataset devic dataset pass folder devic datapath dataset dataset tabular delimit file path datapath datastor devic dataset dataset tabular delimit file path datapath datastor devic caveat dataprep sdk optim blob size small file slower overhead enumer blob",
        "Solution_preprocessed_content":"filter awar filter partit structur defin dataset dataset pass folder devic datapath caveat dataprep sdk optim blob size small file slower overhead enumer blob",
        "Solution_readability":11.4,
        "Solution_reading_time":14.03,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":133.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":0.3159602778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am designing a learning management system and inflow for the website is more in some cases and  less in another time. I would like to know about the getting the vCPU's which are scaled up to make it down after the stipulated time. I found a document regarding <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/best-practices\/auto-scaling\" rel=\"nofollow noreferrer\">scaling up<\/a> but didn't find a way to scale it down.<\/p>\n<p>Any help is appreciated.<\/p>",
        "Challenge_closed_time":1655984305567,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655983168110,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72729267",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":6.92,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.3159602778,
        "Challenge_title":"Degrading the services automatically by autoscaling in azure services - vCPU",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":49,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1652123310643,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>There is a chance of auto scaling for the normal services in azure cloud services, that means for stipulated time you can increase or decrease as mentioned in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cloud-services\/cloud-services-how-to-scale-portal\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n<p>When it comes for vCPU which is cannot be performed automatically. vCPU can be scaled up based on the request criteria and in the same manner we need to request the support team to scale those down to the normal.<\/p>\n<p><strong>There is no specific procedure to make the auto scaling for vCPU operations. We can increase the capacity of core, but to reduce to the normal, we need to approach the support system for manual changing. You can change it from 10 cores to next level 16 cores, but cannot be performed automatic scaling down from 16 cores to 10 cores.<\/strong><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"autosc vcpu servic autosc normal servic vcpu scale base request criteria scale vcpu team manual increas capac core automat scale",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"chanc auto scale normal servic cloud servic stipul time increas decreas link come vcpu perform automat vcpu scale base request criteria request team scale normal procedur auto scale vcpu oper increas capac core reduc normal manual core level core perform automat scale core core",
        "Solution_preprocessed_content":"chanc auto scale normal servic cloud servic stipul time increas decreas link come vcpu perform automat vcpu scale base request criteria request team scale normal procedur auto scale vcpu oper increas capac core reduc normal manual core level core perform automat scale core core",
        "Solution_readability":9.6,
        "Solution_reading_time":11.0,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1598441874396,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":16.4161730556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Currently, I'm using kedro and kedro-viz.<\/p>\n<p>I can specify a layer of dataset from catalog.yml.<\/p>\n<pre><code>hoge:\n  type: MemoryDataSet\n  layer: raw\n<\/code><\/pre>\n<p>but I don't know how to do it with parameters.yml<\/p>\n<pre><code>step_size: 1\nlearning_rate: 0.01\n<\/code><\/pre>\n<p>if it can be done not in parameters.yml but in run.py, I want to see example code.<\/p>",
        "Challenge_closed_time":1598442196703,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598383098480,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63585717",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":5.35,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":16.4161730556,
        "Challenge_title":"In Kedro, How to specify layer to parameters.yml?",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":225,
        "Challenge_word_count":60,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545209959320,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>At the moment layers can only be specified for datasets, not for nodes or parameters.<\/p>\n<p>If you have a specific use case for adding layers to nodes\/parameters, please let us know by opening a feature request in the Kedro repo: <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/issues\" rel=\"nofollow noreferrer\">https:\/\/github.com\/quantumblacklabs\/kedro\/issues<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"moment layer specifi dataset node paramet layer node paramet open featur request repo http github com quantumblacklab",
        "Solution_preprocessed_content":"moment layer specifi dataset node paramet layer open featur request repo",
        "Solution_readability":13.7,
        "Solution_reading_time":4.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":44.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1410333105327,
        "Answerer_location":"Turin, Metropolitan City of Turin, Italy",
        "Answerer_reputation_count":477.0,
        "Answerer_view_count":130.0,
        "Challenge_adjusted_solved_time":0.0448425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using MLflow to log the metrics but I want to change the default saving logs directory. So, instead of writing log files besides my main file, I want to store them to <code>\/path\/outputs\/lg <\/code>. I don't know how to change it. I use it without in the <code>Model<\/code>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom time import time\n\nimport mlflow\nimport numpy as np\nimport torch\nimport tqdm\n\n# from segmentation_models_pytorch.utils import metrics\nfrom AICore.emergency_landing.metrics import IoU, F1\nfrom AICore.emergency_landing.utils import AverageMeter\nfrom AICore.emergency_landing.utils import TBLogger\n\n\nclass Model:\n    def __init__(self, model, num_classes=5, ignore_index=0, optimizer=None, scheduler=None, criterion=None,\n                 device=None, epochs=30, train_loader=None, val_loader=None, tb_logger: TBLogger = None,\n                 logger=None,\n                 best_model_path=None,\n                 model_check_point_path=None,\n                 load_from_best_model=None,\n                 load_from_model_checkpoint=None,\n                 early_stopping=None,\n                 debug=False):\n\n        self.debug = debug\n\n        self.early_stopping = {\n            'init': early_stopping,\n            'changed': 0\n        }\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.device = device\n        self.epochs = epochs\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n        self.model = model.to(device)\n\n        self.tb_logger = tb_logger\n        self.logger = logger\n\n        self.best_loss = np.Inf\n\n        if not os.path.exists(best_model_path):\n            os.makedirs(best_model_path)\n        self.best_model_path = best_model_path\n\n        if not os.path.exists(model_check_point_path):\n            os.makedirs(model_check_point_path)\n        self.model_check_point_path = model_check_point_path\n\n        self.load_from_best_model = load_from_best_model\n        self.load_from_model_checkpoint = load_from_model_checkpoint\n\n        if self.load_from_best_model is not None:\n            self.load_model(path=self.load_from_best_model)\n        if self.load_from_model_checkpoint is not None:\n            self.load_model_checkpoint(path=self.load_from_model_checkpoint)\n\n        self.train_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.val_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.test_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n\n        self.train_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.val_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.test_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n\n    def metrics(self, is_train=True):\n        if is_train:\n            train_losses = AverageMeter('Training Loss', ':.4e')\n            train_iou = AverageMeter('Training iou', ':6.2f')\n            train_f_score = AverageMeter('Training F_score', ':6.2f')\n\n            return train_losses, train_iou, train_f_score\n        else:\n            val_losses = AverageMeter('Validation Loss', ':.4e')\n            val_iou = AverageMeter('Validation mean iou', ':6.2f')\n            val_f_score = AverageMeter('Validation F_score', ':6.2f')\n\n            return val_losses, val_iou, val_f_score\n\n    def fit(self):\n\n        self.logger.info(&quot;\\nStart training\\n\\n&quot;)\n        start_training_time = time()\n\n        with mlflow.start_run():\n            for e in range(self.epochs):\n                start_training_epoch_time = time()\n                self.model.train()\n                train_losses_avg, train_iou_avg, train_f_score_avg = self.metrics(is_train=True)\n                with tqdm.tqdm(self.train_loader, unit=&quot;batch&quot;) as tepoch:\n                    tepoch.set_description(f&quot;Epoch {e}&quot;)\n                    for image, target in tepoch:\n                        # Transfer Data to GPU if available\n                        image = image.to(self.device)\n                        target = target.to(self.device)\n                        # Clear the gradients\n                        self.optimizer.zero_grad()\n                        # Forward Pass\n                        # out = self.model(image)['out']\n                        # if unet == true =&gt; remove ['out']\n                        out = self.model(image)\n                        # Find the Loss\n                        loss = self.criterion(out, target)\n                        # Calculate Loss\n                        train_losses_avg.update(loss.item(), image.size(0))\n                        # Calculate gradients\n                        loss.backward()\n                        # Update Weights\n                        self.optimizer.step()\n\n                        iou = self.train_iou(out.cpu(), target.cpu()).item()\n                        train_iou_avg.update(iou)\n\n                        f1_score = self.train_f1(out.cpu(), target.cpu()).item()\n                        train_f_score_avg.update(f1_score)\n\n                        tepoch.set_postfix(loss=train_losses_avg.avg,\n                                           iou=train_iou_avg.avg,\n                                           f_score=train_f_score_avg.avg)\n                        if self.debug:\n                            break\n\n                self.tb_logger.log(log_type='criterion\/training', value=train_losses_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='iou\/training', value=train_iou_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='f_score\/training', value=train_f_score_avg.avg, epoch=e)\n\n                mlflow.log_metric('criterion\/training', train_losses_avg.avg, step=e)\n                mlflow.log_metric('iou\/training', train_iou_avg.avg, step=e)\n                mlflow.log_metric('f_score\/training', train_f_score_avg.avg, step=e)\n\n                end_training_epoch_time = time() - start_training_epoch_time\n                print('\\n')\n                self.logger.info(\n                    f'Training Results - [{end_training_epoch_time:.3f}s] Epoch: {e}:'\n                    f' f_score: {train_f_score_avg.avg:.3f},'\n                    f' IoU: {train_iou_avg.avg:.3f},'\n                    f' Loss: {train_losses_avg.avg:.3f}')\n\n                # validation step\n                val_loss = self.evaluation(e)\n                # apply scheduler\n                if self.scheduler:\n                    self.scheduler.step()\n                # early stopping\n                if self.early_stopping['init'] &gt;= self.early_stopping['changed']:\n                    self._early_stopping_model(val_loss=val_loss)\n                else:\n                    print(f'The model can not learn more, Early Stopping at epoch[{e}]')\n                    break\n\n                # save best model\n                if self.best_model_path is not None:\n                    self._best_model(val_loss=val_loss, path=self.best_model_path)\n\n                # model check points\n                if self.model_check_point_path is not None:\n                    self.save_model_check_points(path=self.model_check_point_path, epoch=e, net=self.model,\n                                                 optimizer=self.optimizer, loss=self.criterion,\n                                                 avg_loss=train_losses_avg.avg)\n\n                # log mlflow\n                if self.scheduler:\n                    mlflow.log_param(&quot;get_last_lr&quot;, self.scheduler.get_last_lr())\n                    mlflow.log_param(&quot;scheduler&quot;, self.scheduler.state_dict())\n\n                self.tb_logger.flush()\n                if self.debug:\n                    break\n\n            end_training_time = time() - start_training_time\n            print(f'Finished Training after {end_training_time:.3f}s')\n            self.tb_logger.close()\n\n    def evaluation(self, epoch):\n        print('Validating...')\n        start_validation_epoch_time = time()\n        self.model.eval()  # Optional when not using Model Specific layer\n        with torch.no_grad():\n            val_losses_avg, val_iou_avg, val_f_score_avg = self.metrics(is_train=False)\n            with tqdm.tqdm(self.val_loader, unit=&quot;batch&quot;) as tepoch:\n                for image, target in tepoch:\n                    # Transfer Data to GPU if available\n                    image = image.to(self.device)\n                    target = target.to(self.device)\n                    # out = self.model(image)['out']\n                    # if unet == true =&gt; remove ['out']\n                    out = self.model(image)\n                    # Find the Loss\n                    loss = self.criterion(out, target)\n                    # Calculate Loss\n                    val_losses_avg.update(loss.item(), image.size(0))\n\n                    iou = self.val_iou(out.cpu(), target.cpu()).item()\n                    val_iou_avg.update(iou)\n\n                    f1_score = self.val_f1(out.cpu(), target.cpu()).item()\n                    val_f_score_avg.update(f1_score)\n\n                    tepoch.set_postfix(loss=val_losses_avg.avg,\n                                       iou=val_iou_avg.avg,\n                                       f_score=val_f_score_avg.avg)\n                    if self.debug:\n                        break\n            print('\\n')\n            self.tb_logger.log(log_type='criterion\/validation', value=val_losses_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='iou\/validation', value=val_iou_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='f_score\/validation', value=val_f_score_avg.avg, epoch=epoch)\n\n            mlflow.log_metric('criterion\/validation', val_losses_avg.avg, step=epoch)\n            mlflow.log_metric('iou\/validation', val_iou_avg.avg, step=epoch)\n            mlflow.log_metric('f_score\/validation', val_f_score_avg.avg, step=epoch)\n\n            end_validation_epoch_time = time() - start_validation_epoch_time\n            self.logger.info(\n                f'validation Results - [{end_validation_epoch_time:.3f}s] Epoch: {epoch}:'\n                f' f_score: {val_f_score_avg.avg:.3f},'\n                f' IoU: {val_iou_avg.avg:.3f},'\n                f' Loss: {val_losses_avg.avg:.3f}')\n            print('\\n')\n            return val_losses_avg.avg\n\n    def _save_model(self, name, path, params):\n        torch.save(params, path)\n\n    def _early_stopping_model(self, val_loss):\n        if self.best_loss &lt; val_loss:\n            self.early_stopping['changed'] += 1\n        else:\n            self.early_stopping['changed'] = 0\n\n    def _best_model(self, val_loss, path):\n        if self.best_loss &gt; val_loss:\n            self.best_loss = val_loss\n            name = f'\/best_model_loss_{self.best_loss:.2f}'.replace('.', '_')\n            self._save_model(name, path=f'{path}\/{name}.pt', params={\n                'model_state_dict': self.model.state_dict(),\n            })\n\n            print(f'The best model is saved with criterion: {self.best_loss:.2f}')\n\n    def save_model_check_points(self, path, epoch, net, optimizer, loss, avg_loss):\n        name = f'\/model_epoch_{epoch}_loss_{avg_loss:.2f}'.replace('.', '_')\n        self._save_model(name, path=f'{path}\/{name}.pt', params={\n            'epoch': epoch,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'criterion': loss,\n        })\n        print(f'model checkpoint is saved at model_epoch_{epoch}_loss_{avg_loss:.2f}')\n\n    def load_model_checkpoint(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        epoch = checkpoint['epoch']\n        self.criterion = checkpoint['criterion']\n\n        return epoch\n\n    def load_model(self, path):\n        best_model = torch.load(path)\n        self.model.load_state_dict(best_model['model_state_dict'])\n<\/code><\/pre>",
        "Challenge_closed_time":1636732235590,
        "Challenge_comment_count":1,
        "Challenge_created_time":1636727119830,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1636755379243,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69944447",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":18.3,
        "Challenge_reading_time":121.09,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":124,
        "Challenge_solved_time":1.4210444445,
        "Challenge_title":"How to change the directory of mlflow logs?",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":436,
        "Challenge_word_count":636,
        "Platform":"Stack Overflow",
        "Poster_created_time":1410333105327,
        "Poster_location":"Turin, Metropolitan City of Turin, Italy",
        "Poster_reputation_count":477.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>The solution is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.set_tracking_uri(uri=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nexp = mlflow.get_experiment_by_name(name='Emegency_landing')\nif not exp:\n    experiment_id = mlflow.create_experiment(name='Emegency_landing',\n                                                 artifact_location=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nelse:\n    experiment_id = exp.experiment_id\n<\/code><\/pre>\n<p>And then you should pass the experiment Id to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run(experiment_id=experiment_id):\n     pass \n<\/code><\/pre>\n<p>If you don't mention the <code>\/path\/mlruns<\/code>, when you run the command of <code>mlflow ui<\/code>, it will create another folder automatically named <code>mlruns<\/code>. so, pay attention to this point to have the same name as <code>mlruns<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":1636755540676,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":20.1,
        "Solution_reading_time":12.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":68.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":22.3896641667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have to deploy a custom keras model in AWS Sagemaker. I have a created a notebook instance and I have the following files:<\/p>\n\n<pre><code>AmazonSagemaker-Codeset16\n   -ann\n      -nginx.conf\n      -predictor.py\n      -serve\n      -train.py\n      -wsgi.py\n   -Dockerfile\n<\/code><\/pre>\n\n<p>I now open the AWS terminal and build the docker image and push the image in the ECR repository. Then I open a new jupyter python notebook and try to fit the model and deploy the same. The training is done correctly but while deploying I get the following error:<\/p>\n\n<blockquote>\n  <p>\"Error hosting endpoint sagemaker-example-2019-10-25-06-11-22-366: Failed. >Reason: The primary container for production variant AllTraffic did not pass >the ping health check. Please check CloudWatch logs for this endpoint...\"<\/p>\n<\/blockquote>\n\n<p>When I check the logs, I find the following:<\/p>\n\n<blockquote>\n  <p>2019\/11\/11 11:53:32 [crit] 19#19: *3 connect() to unix:\/tmp\/gunicorn.sock >failed (2: No such file or directory) while connecting to upstream, client: >10.32.0.4, server: , request: \"GET \/ping HTTP\/1.1\", upstream: >\"<a href=\"http:\/\/unix:\/tmp\/gunicorn.sock:\/ping\" rel=\"nofollow noreferrer\">http:\/\/unix:\/tmp\/gunicorn.sock:\/ping<\/a>\", host: \"model.aws.local:8080\"<\/p>\n<\/blockquote>\n\n<p>and <\/p>\n\n<blockquote>\n  <p>Traceback (most recent call last):\n   File \"\/usr\/local\/bin\/serve\", line 8, in \n     sys.exit(main())\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/cli\/serve.py\", line 19, in main\n     server.start(env.ServingEnv().framework_module)\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/_server.py\", line 107, in start\n     module_app,\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 711, in <strong>init<\/strong>\n     errread, errwrite)\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 1343, in _execute_child\n     raise child_exception<\/p>\n<\/blockquote>\n\n<p>I tried to deploy the same model in AWS Sagemaker with these files in my local computer and the model was deployed successfully but inside AWS, I am facing this problem.<\/p>\n\n<p>Here is my serve file code:<\/p>\n\n<pre><code>from __future__ import print_function\nimport multiprocessing\nimport os\nimport signal\nimport subprocess\nimport sys\n\ncpu_count = multiprocessing.cpu_count()\n\nmodel_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\nmodel_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\n\n\ndef sigterm_handler(nginx_pid, gunicorn_pid):\n    try:\n        os.kill(nginx_pid, signal.SIGQUIT)\n    except OSError:\n        pass\n    try:\n        os.kill(gunicorn_pid, signal.SIGTERM)\n    except OSError:\n        pass\n\n    sys.exit(0)\n\n\ndef start_server():\n    print('Starting the inference server with {} workers.'.format(model_server_workers))\n\n\n    # link the log streams to stdout\/err so they will be logged to the container logs\n    subprocess.check_call(['ln', '-sf', '\/dev\/stdout', '\/var\/log\/nginx\/access.log'])\n    subprocess.check_call(['ln', '-sf', '\/dev\/stderr', '\/var\/log\/nginx\/error.log'])\n\n    nginx = subprocess.Popen(['nginx', '-c', '\/opt\/ml\/code\/nginx.conf'])\n    gunicorn = subprocess.Popen(['gunicorn',\n                                 '--timeout', str(model_server_timeout),\n                                 '-b', 'unix:\/tmp\/gunicorn.sock',\n                                 '-w', str(model_server_workers),\n                                 'wsgi:app'])\n\n    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n\n    # If either subprocess exits, so do we.\n    pids = set([nginx.pid, gunicorn.pid])\n    while True:\n        pid, _ = os.wait()\n        if pid in pids:\n            break\n\n    sigterm_handler(nginx.pid, gunicorn.pid)\n    print('Inference server exiting')\n\n\n# The main routine just invokes the start function.\nif __name__ == '__main__':\n    start_server()\n<\/code><\/pre>\n\n<p>I deploy the model using the following:<\/p>\n\n<blockquote>\n  <p>predictor = classifier.deploy(1, 'ml.t2.medium', serializer=csv_serializer)<\/p>\n<\/blockquote>\n\n<p>Kindly let me know the mistake I am doing while deploying.<\/p>",
        "Challenge_closed_time":1573630807523,
        "Challenge_comment_count":1,
        "Challenge_created_time":1573549904240,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1573550204732,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58815367",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":11.6,
        "Challenge_reading_time":49.48,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":22.4731341667,
        "Challenge_title":"How to solve the error with deploying a model in aws sagemaker?",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":4605,
        "Challenge_word_count":399,
        "Platform":"Stack Overflow",
        "Poster_created_time":1550756471932,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Using Sagemaker script mode can be much simpler than dealing with container and nginx low-level stuff like you're trying to do, have you considered that?<br>\nYou only need to provide the keras script:   <\/p>\n\n<blockquote>\n  <p>With Script Mode, you can use training scripts similar to those you would use outside SageMaker with SageMaker's prebuilt containers for various deep learning frameworks such TensorFlow, PyTorch, and Apache MXNet.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"mode deal nginx low level stuff mode train outsid prebuilt framework tensorflow pytorch apach mxnet link mode",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"mode simpler deal nginx low level stuff kera mode train outsid prebuilt framework tensorflow pytorch apach mxnet http github com sampl mode blob master sentiment mode sentiment analysi ipynb",
        "Solution_preprocessed_content":"mode simpler deal nginx stuff kera mode train outsid prebuilt framework tensorflow pytorch apach mxnet",
        "Solution_readability":18.5,
        "Solution_reading_time":9.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1425426748316,
        "Answerer_location":null,
        "Answerer_reputation_count":91.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":1787.9299130556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am doing the following steps to install R Hash_2.2.6.zip package on to Azure ML<\/p>\n\n<ol>\n<li>Upload the .zip file as a dataset<\/li>\n<li>Create a new experiment and Add \"Execute R Script\" to experiment<\/li>\n<li>Drag and drop .zip file dataset to experiment.<\/li>\n<li>Connect the Dataset in step3 to \"Execute R Script\" of step2<\/li>\n<li>Run the experiment to install the package<\/li>\n<\/ol>\n\n<p>However I am getting this error: <code>zip file src\/hash_2.2.6.zip not found<\/code><\/p>\n\n<p>Just so that its very clear, I am following steps mentioned in this article: <a href=\"http:\/\/blogs.technet.com\/b\/saketbi\/archive\/2014\/08\/20\/microsoft-azure-ml-amp-r-language-extensibility.aspx\" rel=\"nofollow\">http:\/\/blogs.technet.com\/b\/saketbi\/archive\/2014\/08\/20\/microsoft-azure-ml-amp-r-language-extensibility.aspx<\/a>.<\/p>\n\n<p>Any help in this regard is greatly appreciated.<\/p>",
        "Challenge_closed_time":1425437860360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1419001312673,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1483481029407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/27568624",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":11.87,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1787.9299130556,
        "Challenge_title":"Installing additional R Package on Azure ML",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":2765,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419000630800,
        "Poster_location":null,
        "Poster_reputation_count":159.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>To install a package this way, you have to create a .zip of a .zip. The outer layer of packaging will get unzipped into the src\/ folder when the dataset is passed in to the module, and you'll be able to install the inner package from there.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"creat zip zip instal hash zip packag outer layer packag unzip src folder dataset pass modul instal packag",
        "Solution_last_edit_time":1425439161243,
        "Solution_link_count":0.0,
        "Solution_original_content":"instal packag creat zip zip outer layer packag unzip src folder dataset pass modul instal packag",
        "Solution_preprocessed_content":"instal packag creat zip zip outer layer packag unzip src folder dataset pass modul instal packag",
        "Solution_readability":5.9,
        "Solution_reading_time":2.95,
        "Solution_score_count":5.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":47.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1582301202872,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":44.77405,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When deploying a real-time inferencing pipeline in Azure ML (as per <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy#deploy-the-real-time-endpoint\" rel=\"nofollow noreferrer\">this<\/a> tutorial), I receive the below error. I've tried forcibly logging out using OAuth. Tried creating a new Azure workspace but continue to receive the same error.<\/p>\n\n<p>It looks like the tenant id causing the problem is example.onmicrosoft.com (72f988bf-86f1-41af-91ab-2d7cd011db47)<\/p>\n\n<hr>\n\n<p><em>Deploy: Failed on step CreateServiceFromModels. Details: AzureML service API error. Error calling ServiceCreate: {\"code\":\"Unauthorized\",\"statusCode\":401,\"message\":\"Unauthorized\",\"details\":[{\"code\":\"EmptyOrInvalidToken\",\"message\":\"Error: Service invocation failed!\\r\\nRequest: GET <a href=\"https:\/\/management.azure.com\/subscriptions\/subscription_id\/resourceGroups\/dev-rg\/providers\/Microsoft.MachineLearningServices\/workspaces\/dev-ws\/providers\/Microsoft.Authorization\/permissions?api-version=2015-07-01\" rel=\"nofollow noreferrer\">https:\/\/management.azure.com\/subscriptions\/subscription_id\/resourceGroups\/dev-rg\/providers\/Microsoft.MachineLearningServices\/workspaces\/dev-ws\/providers\/Microsoft.Authorization\/permissions?api-version=2015-07-01<\/a>\\r\\nStatus Code: 401 Unauthorized\\r\\nReason Phrase: Unauthorized\\r\\nResponse Body: {\\\"error\\\":{\\\"code\\\":\\\"InvalidAuthenticationTokenTenant\\\",\\\"message\\\":\\\"The access token is from the wrong issuer '<a href=\"https:\/\/sts.windows.net\/72f988bf-86f1-41af-91ab-2d7cd011db47\/\" rel=\"nofollow noreferrer\">https:\/\/sts.windows.net\/72f988bf-86f1-41af-91ab-2d7cd011db47\/<\/a>'. It must match the tenant '<a href=\"https:\/\/sts.windows.net\/correct_tenant_id\/\" rel=\"nofollow noreferrer\">https:\/\/sts.windows.net\/correct_tenant_id\/<\/a>' associated with this subscription. Please use the authority (URL) '<a href=\"https:\/\/login.windows.net\/correct_tenant_id\" rel=\"nofollow noreferrer\">https:\/\/login.windows.net\/correct_tenant_id<\/a>' to get the token. Note, if the subscription is transferred to another tenant there i<\/em><\/p>",
        "Challenge_closed_time":1582556597823,
        "Challenge_comment_count":2,
        "Challenge_created_time":1582302156893,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1582395411243,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60342645",
        "Challenge_link_count":9,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":23.7,
        "Challenge_reading_time":29.64,
        "Challenge_score_count":2,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":70.6780361111,
        "Challenge_title":"Azure ML inference pipeline deployment authorization token error",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":481,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582301202872,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>I appear to have had User Access Administrator role only (in addition to Classic Service Administrator). As soon as I added myself to the Owner role in the Access Control (IAM) section of the Azure Portal, the deployment succeeded.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"owner role access control iam section portal",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"access administr role addit classic servic administr soon owner role access control iam section portal deploy succeed",
        "Solution_preprocessed_content":"access administr role soon owner role access control section portal deploy succeed",
        "Solution_readability":12.1,
        "Solution_reading_time":2.94,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1463515446747,
        "Answerer_location":"India",
        "Answerer_reputation_count":15335.0,
        "Answerer_view_count":1991.0,
        "Challenge_adjusted_solved_time":4.5067230556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>please be easy on me, I am new to ML. I am sure somebody will request to close this as subjective but I cannot find my specific answer and don't know how else to ask. <\/p>\n\n<p>If I have a shop, with three areas of the shop. I have sensors to detect when people come in or out of each area. This happens every 15 seconds. So, in my db, I have a count of the occupancy, per room, every 15 seconds. <\/p>\n\n<p>Using this data, I want to predict the occupancy, per room, in the future but also, if somebody comes in the door, predict most likely room they will go to. <\/p>\n\n<p>Is it possible to predict future occupancy per room and also probability of where people will go when the walk in using a dataset that simply lists the rooms and the occupancy of each room every 15 seconds? Is this a regression model?<\/p>\n\n<p>Thanks!<\/p>\n\n<p>Mike<\/p>",
        "Challenge_closed_time":1544846023056,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544840911773,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53789057",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.6,
        "Challenge_reading_time":10.77,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.4198008333,
        "Challenge_title":"Is it possible to get two different types of results from dataset",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":67,
        "Challenge_word_count":170,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455012473430,
        "Poster_location":"Temecula, CA, United States",
        "Poster_reputation_count":1652.0,
        "Poster_view_count":198.0,
        "Solution_body":"<p>Predicting the most likely room, which they would walk in. :<\/p>\n\n<p>This falls under the classification problem. The output falls under a set of categories, in this case it is different rooms.<\/p>\n\n<p>Predicting the Occpancy of each room :\nAs mentioned by @poorna is a regression problem. <\/p>\n\n<p>Two ways you can look at this problem, <\/p>\n\n<ol>\n<li><p>Multi- target regression problem with occupancy of each room as one target and past occupancies of all rooms as input. <\/p><\/li>\n<li><p>Independent forecast problem for each room with past occupancies of corresponding room as input.<\/p><\/li>\n<\/ol>\n\n<p>For learning the basics of machine learning, you can go through this <a href=\"https:\/\/scikit-learn.org\/stable\/tutorial\/basic\/tutorial.html\" rel=\"nofollow noreferrer\">link<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_gpt_summary":"predict room walk fall classif predict occup room regress occup predict multi target regress past occup room input independ forecast room past occup room input resourc link bias summari",
        "Solution_last_edit_time":1544857135976,
        "Solution_link_count":1.0,
        "Solution_original_content":"predict room walk fall classif output fall set room predict occpanc room poorna regress multi target regress occup room target past occup room input independ forecast room past occup room input link",
        "Solution_preprocessed_content":"predict room walk fall classif output fall set room predict occpanc room regress multi target regress occup room target past occup room input independ forecast room past occup room input link",
        "Solution_readability":9.8,
        "Solution_reading_time":9.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1589293508567,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was trying to register a model using the <code>Run<\/code> Class like this:<\/p>\n<pre><code>model = run.register_model(\n    model_name=model_name,\n    model_path=model_path)\n<\/code><\/pre>\n<p>Errors with message: <code>Could not locate the provided model_path ... in the set of files uploaded to the run...<\/code><\/p>",
        "Challenge_closed_time":1643643837027,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643643837027,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70928761",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.9,
        "Challenge_reading_time":4.33,
        "Challenge_score_count":2,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.0,
        "Challenge_title":"AzureML Model Register",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":319,
        "Challenge_word_count":38,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589293508567,
        "Poster_location":null,
        "Poster_reputation_count":833.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>The only way I found to fix the issue was to use the <code>Model<\/code> Class instead:<\/p>\n<pre><code>        model = Model.register(\n            workspace=ws,\n            model_name=model_name,\n            model_path=model_path,\n            model_framework=Model.Framework.SCIKITLEARN,\n            model_framework_version=sklearn.__version__,\n            description='Model Deescription',\n            tags={'Name' : 'ModelName', 'Type' : 'Production'},\n            model_framework=Model.Framework.SCIKITLEARN,\n            model_framework_version='1.0'\n            )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"model class run class regist model model class regist model",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"model class model model regist workspac model model model path model path model framework model framework scikitlearn model framework version sklearn version descript model deescript tag modelnam type model framework model framework scikitlearn model framework version",
        "Solution_preprocessed_content":null,
        "Solution_readability":20.6,
        "Solution_reading_time":6.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1564790214540,
        "Answerer_location":null,
        "Answerer_reputation_count":71.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":113.7935575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a Pipeline with DatabricksSteps each containing:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\n#do stuff\nrun.log(name, val, desc)\nrun.log_list(name, vals, desc)\nrun.log_image(title, fig, desc)\n<\/code><\/pre>\n\n<p>Only <code>log_image()<\/code> seems to work.  The image appears in the \"images\" section of the AML experiment workspace as expected, but the \"tracked metrics\" and \"charts\" areas are blank.  In an interactive job, <code>run.log()<\/code> and <code>run.log_list()<\/code> work as expected.  I tested that there is no problem with the arguments by using <code>print()<\/code> instead of <code>run.log()<\/code>.<\/p>",
        "Challenge_closed_time":1569427861030,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569018204223,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58035744",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":9.06,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":113.7935575,
        "Challenge_title":"AML run.log() and run.log_list() fail without error",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":121,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1465320834943,
        "Poster_location":"Redmond, WA, USA",
        "Poster_reputation_count":677.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>Add run.flush() at the end of the script.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"add run flush end",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"add run flush end",
        "Solution_preprocessed_content":null,
        "Solution_readability":0.5,
        "Solution_reading_time":0.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":8.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405882600928,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":552.0,
        "Answerer_view_count":115.0,
        "Challenge_adjusted_solved_time":0.4960861111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>i have added \"versioned: true\" in the \"catalog.yml\" file of the \"hello_world\" tutorial.<\/p>\n\n<pre><code>example_iris_data:\n  type: pandas.CSVDataSet\n  filepath: data\/01_raw\/iris.csv\n  versioned: true\n<\/code><\/pre>\n\n<p>Then when I used \n\"kedro run\" to run the tutorial, it has error as below:\n\"VersionNotFoundError: Did not find any versions for CSVDataSet\".<\/p>\n\n<p>May i know what is the right way for me to do versioning for the \"iris.csv\" file? thanks!<\/p>",
        "Challenge_closed_time":1592218684390,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592216898480,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62386291",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.3,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.4960861111,
        "Challenge_title":"Data versioning of \"Hello_World\" tutorial",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":338,
        "Challenge_word_count":66,
        "Platform":"Stack Overflow",
        "Poster_created_time":1517455831447,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Try versioning one of the downstream outputs. For example, add this entry in your <code>catalog.yml<\/code>, and run <code>kedro run<\/code><\/p>\n\n<pre><code>example_train_x:\n  type: pandas.CSVDataSet\n  filepath: data\/02_intermediate\/example_iris_data.csv\n  versioned: true\n<\/code><\/pre>\n\n<p>And you will see <code>example_iris.data.csv<\/code> directory (not a file) under <code>data\/02_intermediate<\/code>. The reason <code>example_iris_data<\/code> gives you an error is that it's the starting data and there's already <code>iris.csv<\/code> in <code>data\/01_raw<\/code> so, Kedro cannot create <code>data\/01_raw\/iris.csv\/<\/code> directory because of the name conflict with the existing <code>iris.csv<\/code> file. <\/p>\n\n<p>Hope this helps :) <\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"version downstream output entri catalog yml file train dataset run run creat iri data csv directori data intermedi reason messag iri csv data raw creat directori",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"version downstream output add entri catalog yml run run train type panda csvdataset filepath data intermedi iri data csv version iri data csv directori file data intermedi reason iri data start data iri csv data raw creat data raw iri csv directori conflict iri csv file hope",
        "Solution_preprocessed_content":"version downstream output add entri run directori reason start data creat directori conflict file hope",
        "Solution_readability":11.1,
        "Solution_reading_time":9.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":78.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1537550036732,
        "Answerer_location":null,
        "Answerer_reputation_count":83.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":257.7548786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Why we need to ML Batch Execution and ML Update resource option in Data factory ? How this can be used to retrain machine learning when updating a blob file ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/8KWH0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8KWH0.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1539774604663,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538738198597,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1538846687100,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52664415",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":5.27,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":287.8905738889,
        "Challenge_title":"Why we need ML batch execution and update resource option in azure data factory",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":141,
        "Challenge_word_count":52,
        "Platform":"Stack Overflow",
        "Poster_created_time":1538737895716,
        "Poster_location":null,
        "Poster_reputation_count":78.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>Ml Batch Execution- to call retraining experiment and get a .ilearner file as output.\nML Update Resource- Use the above .ilearner as input and call patch endpoint of predictive web service to Update resource.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"batch execut retrain ilearn file output updat resourc ilearn file input patch endpoint predict web servic updat resourc",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"batch execut retrain ilearn file output updat resourc ilearn input patch endpoint predict web servic updat resourc",
        "Solution_preprocessed_content":"batch execut retrain ilearn file output updat resourc ilearn input patch endpoint predict web servic updat resourc",
        "Solution_readability":6.6,
        "Solution_reading_time":2.67,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1492048364132,
        "Answerer_location":"Cambridge, MA, USA",
        "Answerer_reputation_count":4436.0,
        "Answerer_view_count":713.0,
        "Challenge_adjusted_solved_time":351.9080963889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a large number of JSON requests for a model split across multiple files in an S3 bucket. I would like to use Sagemaker's Batch Transform feature to process all of these requests (I have done a couple of test runs using small amounts of data and the transform job succeeds). My main issue is here (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors<\/a>), specifically:<\/p>\n<blockquote>\n<p>If a batch transform job fails to process an input file because of a problem with the dataset, SageMaker marks the job as failed. If an input file contains a bad record, the transform job doesn't create an output file for that input file because doing so prevents it from maintaining the same order in the transformed data as in the input file. When your dataset has multiple input files, a transform job continues to process input files even if it fails to process one. The processed files still generate useable results.<\/p>\n<\/blockquote>\n<p>This is not preferable mainly because if 1 request fails (whether its a transient error, a malformmated request, or something wrong with the model container) in a file with a large number of requests, all of those requests will get discarded (even if all of them succeeded and the last one failed). I would ideally prefer Sagemaker to just write the output of the failed response to the file and keep going, rather than discarding the entire file.<\/p>\n<p>My question is, are there any suggestions to mitigating this issue? I was thinking about storing 1 request per file in S3, but this seems somewhat ridiculous? Even if I did this, is there a good way of seeing which requests specifically failed after the transform job finishes?<\/p>",
        "Challenge_closed_time":1644424499008,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643261540133,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70873792",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":24.08,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":323.0441319444,
        "Challenge_title":"How to handle Sagemaker Batch Transform discarding a file with a failed model request",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":357,
        "Challenge_word_count":295,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597858315076,
        "Poster_location":null,
        "Poster_reputation_count":84.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>You've got the right idea: the fewer datapoints are in each file, the less likely a given file is to fail. The issue is that while you can pass a prefix with many files to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">CreateTransformJob<\/a>, partitioning one datapoint per file at least requires an S3 read per datapoint, plus a model invocation per datapoint, which is probably not great. Be aware also that <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=1000415&amp;tstart=0\" rel=\"nofollow noreferrer\">apparently there are hidden rate limits<\/a>.<\/p>\n<p>Here are a couple options:<\/p>\n<ol>\n<li><p>Partition into small-ish files, and plan on failures being rare. Hopefully, not many of your datapoints would actually fail. If you partition your dataset into e.g. 100 files, then a single failure only requires reprocessing 1% of your data. Note that Sagemaker has built-in retries, too, so most of the time failures should be caused by your data\/logic, not randomness on Sagemaker's side.<\/p>\n<\/li>\n<li><p>Deal with failures directly in your model. The same doc you quoted in your question also says:<\/p>\n<\/li>\n<\/ol>\n<blockquote>\n<p>If you are using your own algorithms, you can use placeholder text, such as ERROR, when the algorithm finds a bad record in an input file. For example, if the last record in a dataset is bad, the algorithm places the placeholder text for that record in the output file.<\/p>\n<\/blockquote>\n<p>Note that the reason Batch Transform does this whole-file failure is to maintain a 1-1 mapping between rows in the input and the output. If you can substitute the output for failed datapoints with an error message from inside your model, without actually causing the model itself to fail processing, Batch Transform will be happy.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"partit dataset small file reduc likelihood built retri data logic random deal directli model placehold text algorithm record input file allow batch transform maintain map row input output note hidden rate limit partit datapoint file practic increas read model invoc datapoint",
        "Solution_last_edit_time":1644528409280,
        "Solution_link_count":2.0,
        "Solution_original_content":"idea fewer datapoint file file pass prefix file createtransformjob partit datapoint file read datapoint plu model invoc datapoint probabl great awar appar hidden rate limit coupl option partit small ish file plan rare hopefulli datapoint partit dataset file singl reprocess data note built retri time data logic random deal directli model doc quot sai algorithm placehold text algorithm record input file record dataset algorithm placehold text record output file note reason batch transform file maintain map row input output substitut output datapoint messag insid model model process batch transform happi",
        "Solution_preprocessed_content":"idea fewer datapoint file file pass prefix file createtransformjob partit datapoint file read datapoint plu model invoc datapoint probabl great awar appar hidden rate limit coupl option partit file plan rare hopefulli datapoint partit dataset file singl reprocess data note retri time random deal directli model doc quot sai algorithm placehold text algorithm record input file record dataset algorithm placehold text record output file note reason batch transform maintain map row input output substitut output datapoint messag insid model model process batch transform happi",
        "Solution_readability":11.5,
        "Solution_reading_time":23.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":272.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":35.0730002778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own%20example\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own example<\/a> for product recommendations.<\/p>\n\n<p>I want to use the SVD from <a href=\"https:\/\/pypi.org\/project\/scikit-surprise\/\" rel=\"nofollow noreferrer\">scikit-surprise<\/a> library on Sagemaker.<\/p>\n\n<pre><code>from surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\n<\/code><\/pre>\n\n<p>I added the scikit-surprise package in the Dockerfile, but i am getting the following errors:<\/p>\n\n<h1>Dockerfile:<\/h1>\n\n<pre><code># Build an image that can do training and inference in SageMaker\n# This is a Python 2 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:16.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Here we get all python packages.\n# There's substantial overlap between scipy and numpy that we eliminate by\n# linking them together. Likewise, pip leaves the install caches populated which uses\n# a significant amount of space. These optimizations save a fair amount of space in the\n# image, which reduces start up time.\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp; \\\n    pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp; \\\n        (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nRUN pip install scikit-surprise\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=\"\/opt\/program:${PATH}\"\n\n# Set up the program in the image\nCOPY products_recommender \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n\n<h1>Docker build and deploy :<\/h1>\n\n<pre><code>fullname:XXXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender:latest\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\nSending build context to Docker daemon  67.58kB\nStep 1\/10 : FROM ubuntu:16.04\n ---&gt; 13c9f1285025\nStep 2\/10 : MAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n ---&gt; Using cache\n ---&gt; 44baf3286201\nStep 3\/10 : RUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends          wget          python          nginx          ca-certificates     &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n ---&gt; Using cache\n ---&gt; 8983fa906515\nStep 4\/10 : RUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp;     pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp;         (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp;         rm -rf \/root\/.cache\n ---&gt; Using cache\n ---&gt; 9dbfedf02b57\nStep 5\/10 : RUN pip install scikit-surprise\n ---&gt; Running in 82295cb0affe\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\nCollecting scikit-surprise\n  Downloading https:\/\/files.pythonhosted.org\/packages\/f5\/da\/b5700d96495fb4f092be497f02492768a3d96a3f4fa2ae7dea46d4081cfa\/scikit-surprise-1.1.0.tar.gz (6.4MB)\nCollecting joblib&gt;=0.11 (from scikit-surprise)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/28\/5c\/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf\/joblib-0.14.1-py2.py3-none-any.whl (294kB)\nRequirement already satisfied: numpy&gt;=1.11.2 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.16.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.2.1)\nRequirement already satisfied: six&gt;=1.10.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.12.0)\nBuilding wheels for collected packages: scikit-surprise\n  Building wheel for scikit-surprise (setup.py): started\n  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n  ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-Bb1_iT --python-tag cp27:\n  ERROR: running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build\/lib.linux-x86_64-2.7\n  creating build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running egg_info\n  writing requirements to scikit_surprise.egg-info\/requires.txt\n  writing scikit_surprise.egg-info\/PKG-INFO\n  writing top-level names to scikit_surprise.egg-info\/top_level.txt\n  writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n  writing entry points to scikit_surprise.egg-info\/entry_points.txt\n  reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running build_ext\n  building 'surprise.similarities' extension\n  creating build\/temp.linux-x86_64-2.7\n  creating build\/temp.linux-x86_64-2.7\/surprise\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n  ----------------------------------------\n  ERROR: Failed building wheel for scikit-surprise\n  Running setup.py clean for scikit-surprise\nFailed to build scikit-surprise\nInstalling collected packages: joblib, scikit-surprise\n  Running setup.py install for scikit-surprise: started\n    Running setup.py install for scikit-surprise: finished with status 'error'\n    ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile:\n    ERROR: running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib.linux-x86_64-2.7\n    creating build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running egg_info\n    writing requirements to scikit_surprise.egg-info\/requires.txt\n    writing scikit_surprise.egg-info\/PKG-INFO\n    writing top-level names to scikit_surprise.egg-info\/top_level.txt\n    writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n    writing entry points to scikit_surprise.egg-info\/entry_points.txt\n    reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    reading manifest template 'MANIFEST.in'\n    writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running build_ext\n    building 'surprise.similarities' extension\n    creating build\/temp.linux-x86_64-2.7\n    creating build\/temp.linux-x86_64-2.7\/surprise\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n    unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n    ----------------------------------------\nERROR: Command \"\/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in \/tmp\/pip-install-VsuzGr\/scikit-surprise\/\nWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nThe command '\/bin\/sh -c pip install scikit-surprise' returned a non-zero code: 1\nThe push refers to repository [XXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender]\n89c1adca7d35: Layer already exists \nddcb6879486f: Layer already exists \n4a02efecad74: Layer already exists \n92d3f22d44f3: Layer already exists \n10e46f329a25: Layer already exists \n24ab7de5faec: Layer already exists \n1ea5a27b0484: Layer already exists \nlatest: digest: sha256:5ed35f1964d10f13bc8a05d379913c24195ea31ec848157016381fbd1bb12f28 size: 1782\n<\/code><\/pre>",
        "Challenge_closed_time":1579273412888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579147150087,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59762829",
        "Challenge_link_count":7,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":20.5,
        "Challenge_reading_time":212.31,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":223,
        "Challenge_solved_time":35.0730002778,
        "Challenge_title":"AWS Sagemaker scikit_bring_your_own example",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":341,
        "Challenge_word_count":1082,
        "Platform":"Stack Overflow",
        "Poster_created_time":1241005356852,
        "Poster_location":"Kuala Lumpur, Malaysia",
        "Poster_reputation_count":15794.0,
        "Poster_view_count":1032.0,
        "Solution_body":"<p>The 'x86_64-linux-gnu-gcc' binary can't be found in environment where you're building the container. Make sure that gcc is installed, and that you use the right name (gcc?).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"gcc instal gcc build",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"linux gnu gcc binari environ build gcc instal gcc",
        "Solution_preprocessed_content":"binari environ build gcc instal",
        "Solution_readability":6.2,
        "Solution_reading_time":2.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":27.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":1.0276408334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Created an azure ml dataset. how do I delete the dataset if it already exists?<\/p>\n<pre><code>#register dataset\npath='path'\nfile_ds=Dataset.File.from_files(path=path)\nfile_ds=file_ds.register(workspace=ws,name=&quot;Dataset&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1658911127743,
        "Challenge_comment_count":2,
        "Challenge_created_time":1658908144600,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73134073",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":9.7,
        "Challenge_reading_time":3.97,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.8286508334,
        "Challenge_title":"How to delete azureml dataset if it already exists",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":114,
        "Challenge_word_count":29,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606756004663,
        "Poster_location":null,
        "Poster_reputation_count":49.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>AFAIK, as of now, deleting the dataset using <a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-python\/search?q=delete+datasets\" rel=\"nofollow noreferrer\">AzureML Python SDK<\/a> is not possible via <code>delete.datasets()<\/code>. But it might be possible via <a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-python\/blob\/396853110f5c15463e5a531ee759446d3389d441\/sdk\/ml\/azure-ai-ml\/azure\/ai\/ml\/_restclient\/dataset_dataplane\/operations\/_delete_operations.py\" rel=\"nofollow noreferrer\">delete_operations.py<\/a><\/p>\n<p>As suggested by <a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/567611\/is-there-a-way-to-delete-datasets-on-azureml.html\" rel=\"nofollow noreferrer\">YutongTie<\/a>, you can delete the dataset using the Azure Machine Learning Studio.<\/p>\n<p>References: <a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/379022\/how-to-delete-data-backing-a-dataset.html\" rel=\"nofollow noreferrer\">How to Delete Data Backing a Dataset<\/a>, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-export-delete-data\" rel=\"nofollow noreferrer\">Export or delete your Machine Learning service workspace data<\/a> and <a href=\"https:\/\/rdrr.io\/cran\/AzureML\/man\/delete.datasets.html\" rel=\"nofollow noreferrer\">R interface to AzureML - delete dataset<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"delet dataset sdk delet dataset delet oper dataset delet studio bias summari",
        "Solution_last_edit_time":1658911844107,
        "Solution_link_count":6.0,
        "Solution_original_content":"afaik delet dataset sdk delet dataset delet oper yutongti delet dataset studio delet data back dataset export delet servic workspac data interfac delet dataset",
        "Solution_preprocessed_content":"afaik delet dataset sdk yutongti delet dataset studio delet data back dataset export delet servic workspac data interfac delet dataset",
        "Solution_readability":20.6,
        "Solution_reading_time":17.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1367741264647,
        "Answerer_location":null,
        "Answerer_reputation_count":169.0,
        "Answerer_view_count":30.0,
        "Challenge_adjusted_solved_time":1.3759711111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have used estimator for a pytorch model and have saved the artifacts in s3 bucket. using below code<\/p>\n<pre><code>estimator = PyTorch(\n    entry_point=&quot;train_deploy.py&quot;,\n    source_dir=&quot;code&quot;,\n    role=role,\n    framework_version=&quot;1.3.1&quot;,\n    py_version=&quot;py3&quot;,\n    instance_count=1,  # this script only support distributed training for GPU instances.\n    instance_type=&quot;ml.m5.12xlarge&quot;,\n    output_path=output_path,\n    hyperparameters={\n        &quot;epochs&quot;: 1,\n        &quot;num_labels&quot;: 7,\n        &quot;backend&quot;: &quot;gloo&quot;,\n    },\n    disable_profiler=False, # disable debugger\n)\nestimator.fit({&quot;training&quot;: inputs_train, &quot;testing&quot;: inputs_test})\n<\/code><\/pre>\n<p>The model works well and there are no issues with it. However i would like to re use this model later for inference, how do i do that. i am looking for something like below<\/p>\n<pre><code>estimator = PyTorch.load(input_path = &quot;&lt;xyz&gt;&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1659511740763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659506787267,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73216926",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":13.32,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.3759711111,
        "Challenge_title":"Load estimator from model artifact in s3 bucket aws",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":28,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1367741264647,
        "Poster_location":null,
        "Poster_reputation_count":169.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p>I was able to solve this by the following steps<\/p>\n<pre><code>model_data=output_path\nfrom sagemaker.pytorch.model import PyTorchModel \n\npytorch_model = PyTorchModel(model_data=model_data,\n                             role=role,\n                             framework_version=&quot;1.3.1&quot;,\n                             source_dir=&quot;code&quot;,\n                             py_version=&quot;py3&quot;,\n                             entry_point=&quot;train_deploy.py&quot;)\n\npredictor = pytorch_model.deploy(initial_instance_count=1, instance_type=&quot;ml.m4.2xlarge&quot;)\npredictor.serializer = sagemaker.serializers.JSONSerializer()\npredictor.deserializer = sagemaker.deserializers.JSONDeserializer()\nresult = predictor.predict(&quot;&lt;text that needs to be predicted&gt;&quot;)\nprint(&quot;predicted class: &quot;, np.argmax(result, axis=1))\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"load model save artifact bucket pytorchmodel class pytorch model modul model data role framework version sourc dir version entri paramet pytorchmodel constructor deploi model deploi pytorchmodel class set serial deseri predictor object jsonseri jsondeseri respect final predict predictor object predict",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"step model data output path pytorch model import pytorchmodel pytorch model pytorchmodel model data model data role role framework version sourc dir version entri train deploi predictor pytorch model deploi initi instanc count instanc type xlarg predictor serial serial jsonseri predictor deseri deseri jsondeseri predictor predict print predict class argmax axi",
        "Solution_preprocessed_content":null,
        "Solution_readability":31.6,
        "Solution_reading_time":9.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":42.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":41.0340352778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I use a Jupyter Notebook instance on Sagemaker to run a code that took around 3 hours to complete. Since I pay for hour use, I would like to automatically \"Close and Halt\" the Notebook as well as stop the \"Notebook instance\" after running that code. Is this possible?<\/p>",
        "Challenge_closed_time":1568364157240,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568216434713,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57892580",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":4.63,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":41.0340352778,
        "Challenge_title":"Is there a Jupyter code that I can use \"to stop\" a Notebook Instances on Sagemaker, after running any code?",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1393,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509012479112,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>Jupyter Notebook are predominantly designed for exploration and\n   development. If you want to launch long-running or scheduled jobs on\n   ephemeral hardware, it will be a much better experience to use the\n   training API, such as the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_training_job\" rel=\"nofollow noreferrer\"><code>create_training_job<\/code><\/a> in boto3 or the\n   <code>estimator.fit()<\/code> of the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">Python SDK<\/a>. The code passed to training jobs\n   can be completely arbitrary - not necessarily ML code - so whatever\n   you write in jupyter could likely be scheduled and ran in those\n   training jobs. See the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">random forest sklearn demo here<\/a> for an\n   example. That being said, if you still want to programmatically shut\n   down a SageMaker notebook instance, you can use that boto3 call:<\/p>\n\n<pre><code>import boto3\n\nsm = boto3.client('sagemaker')\nsm.stop_notebook_instance(NotebookInstanceName='string')\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"jupyt notebook train api creat train job boto estim fit sdk launch run schedul job ephemer hardwar programmat shut notebook instanc stop notebook instanc boto",
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"jupyt notebook predominantli design explor launch run schedul job ephemer hardwar train api creat train job boto estim fit sdk pass train job complet arbitrari necessarili write jupyt schedul ran train job random forest sklearn said programmat shut notebook instanc boto import boto boto client stop notebook instanc notebookinstancenam",
        "Solution_preprocessed_content":"jupyt notebook predominantli design explor launch schedul job ephemer hardwar train api boto sdk pass train job complet arbitrari necessarili write jupyt schedul ran train job random forest sklearn said programmat shut notebook instanc boto",
        "Solution_readability":16.9,
        "Solution_reading_time":16.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":122.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1341441916656,
        "Answerer_location":null,
        "Answerer_reputation_count":5985.0,
        "Answerer_view_count":161.0,
        "Challenge_adjusted_solved_time":21.6879675,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm learning image classification with Amazon SageMaker. I was trying to follow their  learning demo <strong>Image classification transfer learning demo<\/strong> (<code>Image-classification-transfer-learning-highlevel.ipynb<\/code>)<\/p>\n\n<p>I got up to Start the training. Executed below.<\/p>\n\n<pre><code>ic.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>Set the hyper parameters as given in the demo<\/p>\n\n<pre><code>ic.set_hyperparameters(num_layers=18,\n                             use_pretrained_model=1,\n                             image_shape = \"3,224,224\",\n                             num_classes=257,\n                             num_training_samples=15420,\n                             mini_batch_size=128,\n                             epochs=1,\n                             learning_rate=0.01,\n                             precission_dtype='float32')\n<\/code><\/pre>\n\n<p>Got the client error<\/p>\n\n<pre><code>ERROR 140291262150464] Customer Error: Additional hyperparameters are not allowed (u'precission_dtype' was unexpected) (caused by ValidationError)\n\nCaused by: Additional properties are not allowed (u'precission_dtype' was unexpected)\n<\/code><\/pre>\n\n<p>Does anyone know how to overcome this? I'm also reporting this to aws support. Posting here for sharing and get a fix. Thanks !<\/p>",
        "Challenge_closed_time":1539831993640,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539753916957,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52847777",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":15.47,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":21.6879675,
        "Challenge_title":"Customer Error: Additional hyperparameters are not allowed - Image classification training- Sagemaker",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":146,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403185902747,
        "Poster_location":"Colombo, Sri Lanka",
        "Poster_reputation_count":169.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>Assuming you are referring to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb<\/a> - you have a typo, it's <code>precision_dtype<\/code>, not <code>precission_dtype<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"messag typo hyperparamet paramet precis dtype preciss dtype",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"http github com awslab blob master introduct algorithm imageclassif caltech imag classif transfer highlevel ipynb typo precis dtype preciss dtype",
        "Solution_preprocessed_content":null,
        "Solution_readability":71.6,
        "Solution_reading_time":7.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":24.1372647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am passing the values of lables as below to create a featurestore with labels. But after creation of the featurestore, I do not see the featurestore created with labels. Is it still not supported in VertexAI<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>    fs = aiplatform.Featurestore.create(\n        featurestore_id=featurestore_id,\n        labels=dict(project='retail', env='prod'),\n        online_store_fixed_node_count=online_store_fixed_node_count,\n        sync=sync\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/viOSu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/viOSu.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1651709813300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651616413553,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1651623411107,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72106030",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":9.21,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":25.9443741667,
        "Challenge_title":"I am not able to create a feature store in vertexAI using labels",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":83,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530457174832,
        "Poster_location":null,
        "Poster_reputation_count":1043.0,
        "Poster_view_count":212.0,
        "Solution_body":"<p>As mentioned in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-featurestores\" rel=\"nofollow noreferrer\">featurestore documentation<\/a>:<\/p>\n<blockquote>\n<p>A <strong>featurestore<\/strong> is a top-level container for entity types, features,\nand feature values.<\/p>\n<\/blockquote>\n<p>With this, the GCP console UI &quot;labels&quot; are the &quot;labels&quot; at the <strong>Feature<\/strong> level.<\/p>\n<p>Once a <strong>featurestore<\/strong> is created, you will need to create an <strong>entity<\/strong> and then create a <strong>Feature<\/strong> that has the <em>labels<\/em> parameter as shown on the below sample python code.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from google.cloud import aiplatform\n\ntest_label = {'key1' : 'value1'}\n\ndef create_feature_sample(\n    project: str,\n    location: str,\n    feature_id: str,\n    value_type: str,\n    entity_type_id: str,\n    featurestore_id: str,\n):\n\n    aiplatform.init(project=project, location=location)\n\n    my_feature = aiplatform.Feature.create(\n        feature_id=feature_id,\n        value_type=value_type,\n        entity_type_name=entity_type_id,\n        featurestore_id=featurestore_id,\n        labels=test_label,\n    )\n\n    my_feature.wait()\n\n    return my_feature\n\ncreate_feature_sample('your-project','us-central1','test_feature3','STRING','test_entity3','test_fs3')\n<\/code><\/pre>\n<p>Below is the screenshot of the GCP console which shows that <em>labels<\/em> for <strong>test_feature3<\/strong> feature has the values defined in the above sample python code.\n<a href=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You may refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-features#create-feature\" rel=\"nofollow noreferrer\">creation of feature documentation<\/a> using python for more details.<\/p>\n<p>On the other hand, you may still view the <em>labels<\/em> you defined for your featurestore using the REST API as shown on the below sample.<\/p>\n<pre><code>curl -X GET \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n&quot;https:\/\/&lt;your-location&gt;-aiplatform.googleapis.com\/v1\/projects\/&lt;your-project&gt;\/locations\/&lt;your-location&gt;\/featurestores&quot;\n<\/code><\/pre>\n<p>Below is the result of the REST API which also shows the value of the <em>labels<\/em> I defined for my &quot;test_fs3&quot; featurestore.\n<a href=\"https:\/\/i.stack.imgur.com\/gW45X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gW45X.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"creat entiti creat featur label paramet label featur store rest api",
        "Solution_last_edit_time":1651710305260,
        "Solution_link_count":7.0,
        "Solution_original_content":"featurestor document featurestor level entiti type featur featur valu consol label label featur level featurestor creat creat entiti creat featur label paramet shown sampl cloud import aiplatform test label kei valu creat featur sampl str locat str featur str valu type str entiti type str featurestor str aiplatform init locat locat featur aiplatform featur creat featur featur valu type valu type entiti type entiti type featurestor featurestor label test label featur wait return featur creat featur sampl central test featur test entiti test screenshot consol label test featur featur valu defin sampl creation featur document hand label defin featurestor rest api shown sampl curl author bearer gcloud auth default print access token http aiplatform googleapi com locat featurestor rest api valu label defin test featurestor",
        "Solution_preprocessed_content":"featurestor document featurestor entiti type featur featur valu consol label label featur level featurestor creat creat entiti creat featur label paramet shown sampl screenshot consol label featur valu defin sampl creation featur document hand label defin featurestor rest api shown sampl rest api valu label defin featurestor",
        "Solution_readability":16.7,
        "Solution_reading_time":34.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":226.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1522794798772,
        "Answerer_location":null,
        "Answerer_reputation_count":157.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":7.1225861111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>when I create a run using <code>mlflow.start_run()<\/code> ,even if my script is interrupted before executing <code>mlflow.end_run()<\/code>, the run gets tagged as finished instead of unfinished in Status?<\/p>",
        "Challenge_closed_time":1618223603527,
        "Challenge_comment_count":2,
        "Challenge_created_time":1618197962217,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67052295",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":8.8,
        "Challenge_reading_time":3.33,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":7.1225861111,
        "Challenge_title":"MLflow unfinished experiment saved as finished",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":332,
        "Challenge_word_count":32,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578750761196,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>When your notebook stops the run gets the status finished. However, if you want to continue logging metrics or artifacts to that run, you just need to use <code>mlflow.start_run(run_id=&quot;YourRunIDYouCanGetItFromUI&quot;)<\/code>. This is explained in the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":11.7,
        "Solution_reading_time":5.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":38.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1511812251067,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":98.2814944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I just started with using the Azure Machine Learning Services and ran into this problem. Creating a local environment and deploying my model to localhost works perfectly fine. \nCan anyone identify what could have caused this error, because i do not know where to start..<\/p>\n\n<p>I tried to create a cluster for Location \"eastus2\" aswell, which caused the same error.\nThank you very much in advance!<\/p>\n\n<p>Btw, the ressource group and ressources are being created into my azure account.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wYwJD.png\" rel=\"nofollow noreferrer\">Image of error<\/a><\/p>",
        "Challenge_closed_time":1511812251067,
        "Challenge_comment_count":1,
        "Challenge_created_time":1511458774783,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47460981",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.5,
        "Challenge_reading_time":8.59,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":98.1878566667,
        "Challenge_title":"Error when creating cluster environment for azureML: \"Failed to get scoring front-end info\"",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":381,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1511458091352,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Ashvin [MSFT]<\/p>\n\n<p>Sorry to hear that you were facing issues. We checked logs on our side using the info you provided in the screenshot. The cluster setup failed because there weren't enough cores to fit AzureML and system components in the cluster. You specified agent-vm-size of D1v2 which has 1 CPU core. By default we create 2 agents so total cores were 2. To resolve, can you please try creating a new cluster without specifying agent size? Then AzureML will create 2 agents of D3v2 which is 8 cores total. This should fit the AzureML and system components and leave some room for you to deploy your services. <\/p>\n\n<p>If you wish a bigger cluster you could specify agent-count along with agent-vm-size to appropriately size your cluster but please have minimum total of 8 cores with each individual VM >= 2 cores to ensure cluster works smoothly. Hope this helps.<\/p>\n\n<p>We are working on our side to add error handling to ensure request fails with clear error message. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"core fit compon cluster creat cluster specifi agent size creat agent core wish bigger cluster specifi agent count agent size size cluster minimum core individu core cluster smoothli team request clear messag",
        "Solution_last_edit_time":1511812588163,
        "Solution_link_count":0.0,
        "Solution_original_content":"ashvin msft sorri log screenshot cluster setup weren core fit compon cluster specifi agent size cpu core default creat agent core creat cluster specifi agent size creat agent core fit compon leav room deploi servic wish bigger cluster specifi agent count agent size size cluster minimum core individu core cluster smoothli hope add request clear messag",
        "Solution_preprocessed_content":"ashvin sorri log screenshot cluster setup weren core fit compon cluster specifi cpu core default creat agent core creat cluster specifi agent size creat agent core fit compon leav room deploi servic wish bigger cluster specifi size cluster minimum core individu core cluster smoothli hope add request clear messag",
        "Solution_readability":6.9,
        "Solution_reading_time":11.99,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":168.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1395737095150,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":468.8686019445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If I instantiate a SageMaker <code>PyTorchModel<\/code> object like this:<\/p>\n\n<pre><code>from sagemaker.pytorch import PyTorchModel\n\nmodel = PyTorchModel(name=name_from_base('model-name'),\n                     model_data=model_data,\n                     role=role,\n                     framework_version='1.0.0',\n                     entry_point='serve.py',\n                     source_dir='src',\n                     sagemaker_session=sagemaker_session,\n                     predictor_cls=ImagePredictor)\n\n#model.create_without_deploying??\n<\/code><\/pre>\n\n<p>Is there a way that I can create this model using the sagemaker python SDK so that the model shows up in the SageMaker console, but <em>without<\/em> actually deploying it to an endpoint?<\/p>",
        "Challenge_closed_time":1561626837347,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559938343560,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1559938910380,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56500704",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":17.5,
        "Challenge_reading_time":8.64,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":469.0260519445,
        "Challenge_title":"SageMaker create PyTorchModel without deploying",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":434,
        "Challenge_word_count":61,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>I don't think it is possible to do so using the high-level SageMaker Pyhton SDK. However, you should be able to do it by calling the CreateModel API using the low-level boto3 <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model<\/a>. For your reference, below is an example snippet code on how to do it.<\/p>\n\n<pre><code>%%time\nimport boto3\nimport time\n\nsage = boto3.Session().client(service_name='sagemaker')\n\nimage_uri = '520713654638.dkr.ecr.us-east-1.amazonaws.com\/sagemaker-pytorch:1.0.0-cpu-py3'\nmodel_data ='s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/output\/model.tar.gz'\nsource = 's3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/sourcedir.tar.gz'\nrole = 'arn:aws:iam::xxxxxxxx:role\/service-role\/AmazonSageMaker-ExecutionRole-xxxxxx'\n\ntimestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\nmodel_name = 'my-pytorch-model' + timestamp\n\nresponse = sage.create_model(\n    ModelName=model_name,\n    PrimaryContainer={\n        'Image': image_uri,\n        'ModelDataUrl': model_data,\n        'Environment': { 'SAGEMAKER_CONTAINER_LOG_LEVEL':'20', 'SAGEMAKER_ENABLE_CLOUDWATCH_METRICS': 'False', \n                   'SAGEMAKER_PROGRAM': 'generate.py','SAGEMAKER_REGION': 'us-east-1','SAGEMAKER_SUBMIT_DIRECTORY': source}\n         },\n         ExecutionRoleArn=role\n}\nprint(response)\n<\/code><\/pre>\n\n<p>If you get no error message, then the model will shows up in the SageMaker console<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"creat pytorchmodel object high level sdk deploi endpoint call createmodel api low level boto boto",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"high level pyhton sdk call createmodel api low level boto http boto amazonaw com document api latest servic html client creat model time import boto import time sage boto session client servic imag uri dkr ecr amazonaw com pytorch cpu model data output model tar sourc sourcedir tar role arn iam role servic role executionrol timestamp time strftime time gmtime model pytorch model timestamp respons sage creat model modelnam model primarycontain imag imag uri modeldataurl model data environ log level enabl cloudwatch metric program gener region submit directori sourc executionrolearn role print respons messag model consol",
        "Solution_preprocessed_content":"pyhton sdk call createmodel api boto messag model consol",
        "Solution_readability":23.2,
        "Solution_reading_time":20.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":106.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1553882107003,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":294.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":87.5641388889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I spin up a Sagemaker notebook using the <code>conda_python3<\/code> kernel, and follow the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"noreferrer\">example<\/a> Notebook for <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/randomcutforest.html\" rel=\"noreferrer\">Random Cut Forest<\/a>.<\/p>\n<p>As of this writing, the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"noreferrer\">Sagemaker SDK<\/a> that comes with <code>conda_python3<\/code> is version 1.72.0, but I want to use new features, so I update my notebook to use the latest<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>%%bash\npip install -U sagemaker\n<\/code><\/pre>\n<p>And I see it updates.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>print(sagemaker.__version__)\n\n# 2.4.1\n<\/code><\/pre>\n<p>A change from version 1.x to 2.x was the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html#serializer-and-deserializer-classes\" rel=\"noreferrer\">serializer\/deserializer classes<\/a><\/p>\n<p>Previously (in version 1.72.0) I'd update my predictor to use the proper serializer\/deserializer, and could run inference on my model<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.predictor import csv_serializer, json_deserializer\n\n\nrcf_inference = rcf.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m4.xlarge',\n)\n\nrcf_inference.content_type = 'text\/csv'\nrcf_inference.serializer = csv_serializer\nrcf_inference.accept = 'application\/json'\nrcf_inference.deserializer = json_deserializer\n\nresults = rcf_inference.predict(some_numpy_array)\n<\/code><\/pre>\n<p>(Note this all comes from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"noreferrer\">example<\/a><\/p>\n<p>I try and replicate this using sagemaker 2.4.1 like so<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.deserializers import JSONDeserializer\nfrom sagemaker.serializers import CSVSerializer\n\nrcf_inference = rcf.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge',\n    serializer=CSVSerializer,\n    deserializer=JSONDeserializer\n)\n\nresults = rcf_inference.predict(some_numpy_array)\n<\/code><\/pre>\n<p>And I receive an error of<\/p>\n<pre><code>TypeError: serialize() missing 1 required positional argument: 'data'\n<\/code><\/pre>\n<p>I know I'm using the serliaizer\/deserializer incorrectly, but can't find good documentation on how this should be used<\/p>",
        "Challenge_closed_time":1598503719772,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598300762780,
        "Challenge_favorite_count":4,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63568274",
        "Challenge_link_count":5,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":21.6,
        "Challenge_reading_time":35.2,
        "Challenge_score_count":8,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":56.3769422222,
        "Challenge_title":"How to use Serializer and Deserializer in Sagemaker 2",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":5910,
        "Challenge_word_count":207,
        "Platform":"Stack Overflow",
        "Poster_created_time":1348841548500,
        "Poster_location":null,
        "Poster_reputation_count":5182.0,
        "Poster_view_count":315.0,
        "Solution_body":"<p>in order to use the new serializers\/deserializers, you will need to init them, for example:<\/p>\n<pre><code>from sagemaker.deserializers import JSONDeserializer\nfrom sagemaker.serializers import CSVSerializer\n\nrcf_inference = rcf.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge',\n    serializer=CSVSerializer(),\n    deserializer=JSONDeserializer()\n)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"initi serial deseri class",
        "Solution_last_edit_time":1598615993680,
        "Solution_link_count":0.0,
        "Solution_original_content":"order serial deseri init deseri import jsondeseri serial import csvserial rcf infer rcf deploi initi instanc count instanc type xlarg serial csvserial deseri jsondeseri",
        "Solution_preprocessed_content":null,
        "Solution_readability":26.9,
        "Solution_reading_time":4.98,
        "Solution_score_count":16.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":30.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":0.1185691666,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to log the git_sha parameter on Mlflow as shown in the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/02_hooks.html?highlight=run_params#add-metrics-tracking-to-your-model\" rel=\"nofollow noreferrer\">documentation<\/a>. What appears to me here, is that simply running the following portion of code should be enough to get git_sha logged in the Mlflow UI. Am I right ?<\/p>\n<pre><code>@hook_impl\n    def before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:\n        &quot;&quot;&quot;Hook implementation to start an MLflow run\n        with the same run_id as the Kedro pipeline run.\n        &quot;&quot;&quot;\n        mlflow.start_run(run_name=run_params[&quot;run_id&quot;])\n        mlflow.log_params(run_params)\n<\/code><\/pre>\n<p>But this does not work as I get all but the git_sha parameter. And when I look at the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/_modules\/kedro\/framework\/hooks\/specs.html?highlight=run_params#\" rel=\"nofollow noreferrer\">hooks specs<\/a>, it seems that this param is not part of run_params (anymore?)<\/p>\n<p>Is there a way I could get the git sha (maybe from the context journal ?) and add it to the logged parameters ?<\/p>\n<p>Thank you in advance !<\/p>",
        "Challenge_closed_time":1637159193836,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637158421443,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1637158766987,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70005957",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":16.14,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.2145536111,
        "Challenge_title":"Logging the git_sha as a parameter on Mlflow using Kedro hooks",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":172,
        "Challenge_word_count":148,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586517832390,
        "Poster_location":null,
        "Poster_reputation_count":127.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>Whilst it's heavily encouraged to use git with Kedro it's not required and as such no part of Kedro (except <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro-starters\" rel=\"nofollow noreferrer\">kedro-starters<\/a> if we're being pedantic) is 'aware' of git.<\/p>\n<p>In your <code>before_pipeline_hook<\/code> there it is pretty easy for you to retrieve the info <a href=\"https:\/\/stackoverflow.com\/questions\/14989858\/get-the-current-git-hash-in-a-python-script\">via the techniques documented here<\/a>. It seems trivial for the whole codebase, a bit more involved if you want to say provide pipeline specific hashes.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"retriev git sha pipelin hook techniqu document stack overflow awar git run param default",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"whilst heavili encourag git starter pedant awar git pipelin hook pretti retriev techniqu document trivial codebas bit involv pipelin hash",
        "Solution_preprocessed_content":"whilst heavili encourag git awar git pretti retriev techniqu document trivial codebas bit involv pipelin hash",
        "Solution_readability":10.1,
        "Solution_reading_time":8.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":72.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":1.1966977778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We developed a Jupyter Notebook in a local machine to train models with the Python (V3) libraries <code>sklearn<\/code> and <code>gensim<\/code>.\nAs we set the <code>random_state<\/code> variable to a fixed integer, the results were always the same.<\/p>\n\n<p>After this, we tried moving the notebook to a workspace in Azure Machine Learning Studio (classic), but the results differ even if we leave the <code>random_state<\/code> the same.<\/p>\n\n<p>As suggested in the following links, we installed the same libraries versions and checked the <code>MKL<\/code> version was the same and the <code>MKL_CBWR<\/code> variable was set to <code>AUTO<\/code>.<\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/46766714\/t-sne-generates-different-results-on-different-machines\">t-SNE generates different results on different machines<\/a><\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/38228088\/same-python-code-same-data-different-results-on-different-machines\">Same Python code, same data, different results on different machines<\/a><\/p>\n\n<p>Still, we are not able to get the same results.<\/p>\n\n<p>What else should we check or why is this happening?<\/p>\n\n<p><strong>Update<\/strong><\/p>\n\n<p>If we generate a <code>pkl<\/code> file in the local machine and import it in AML, the results are the same (as the intention of the pkl file is).<\/p>\n\n<p>Still, we are looking to get the same results (if possible) without importing the pkl file.<\/p>\n\n<p><strong>Library versions<\/strong><\/p>\n\n<pre><code>gensim 3.8.3.\nsklearn 0.19.2.\nmatplotlib 2.2.3.\nnumpy 1.17.2.\nscipy 1.1.0.\n<\/code><\/pre>\n\n<p><strong>Code<\/strong><\/p>\n\n<p>Full code can be found <a href=\"https:\/\/t.ly\/YlCi\" rel=\"nofollow noreferrer\">here<\/a>, sample data link inside.<\/p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\nfrom gensim.models import KeyedVectors\n%matplotlib inline\n\nimport time\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\n\nwordvectors_file_vec = '..\/libraries\/embeddings-new_large-general_3B_fasttext.vec'\nwordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec)\n\nmath_quests = # some transformations using wordvectors\n\ndf_subset = pd.DataFrame()\n\npca = PCA(n_components=3, random_state = 42)\npca_result = pca.fit_transform(mat_quests)\ndf_subset['pca-one'] = pca_result[:,0]\ndf_subset['pca-two'] = pca_result[:,1] \n\ntime_start = time.time()\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300, random_state = 42)\ntsne_results = tsne.fit_transform(mat_quests)\n\ndf_subset['tsne-2d-one'] = tsne_results[:,0]\ndf_subset['tsne-2d-two'] = tsne_results[:,1]\n\npca_50 = PCA(n_components=50, random_state = 42)\npca_result_50 = pca_50.fit_transform(mat_quests)\nprint('Cumulative explained variation for 50 principal components: {}'.format(np.sum(pca_50.explained_variance_ratio_)))\n\ntime_start = time.time()\ntsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300, random_state = 42)\ntsne_pca_results = tsne.fit_transform(pca_result_50)\nprint('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n<\/code><\/pre>",
        "Challenge_closed_time":1591493823768,
        "Challenge_comment_count":5,
        "Challenge_created_time":1591464199347,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1591489515656,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62235365",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":12.2,
        "Challenge_reading_time":42.22,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":8.2290058333,
        "Challenge_title":"Models generate different results when moving to Azure Machine Learning Studio",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":201,
        "Challenge_word_count":320,
        "Platform":"Stack Overflow",
        "Poster_created_time":1585590244876,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Definitely empathize with the issue you're having. Every data scientist has struggled with this at some point.<\/p>\n\n<p>The hard truth I have for you is that Azure ML Studio (classic) isn't really capable of  solving this \"works on my machine\" problem. However, the good news is that Azure ML Service is incredible at it. Studio classic doesn't let you define custom environments deterministically, only add and remove packages (and not so well even at that) <\/p>\n\n<p>Because ML Service's execution is built on top of <code>Docker<\/code> containers and <code>conda<\/code> environments, you can feel more confident in repeated results. I highly recommend you take the time to learn it (and I'm also happy to debug any issues that come up). Azure's <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\" rel=\"nofollow noreferrer\">MachineLearningNotebooks repo<\/a> has a lot of great tutorials for getting started.<\/p>\n\n<p>I spent two hours making <a href=\"https:\/\/github.com\/swanderz\/MachineLearningNotebooks\/blob\/SO_CPR\/how-to-use-azureml\/training\/train-on-amlcompute\/train-on-amlcompute.ipynb\" rel=\"nofollow noreferrer\">a proof of concept<\/a> that demonstrate how ML Service solves the problem you're having by synthesizing:<\/p>\n\n<ul>\n<li>your code sample (before you shared your notebook),<\/li>\n<li><a href=\"https:\/\/scikit-learn.org\/stable\/auto_examples\/manifold\/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py\" rel=\"nofollow noreferrer\">Jake Vanderplas's sklearn example<\/a>, and<\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-amlcompute\/train-on-amlcompute.ipynb\" rel=\"nofollow noreferrer\">this Azure ML tutorial<\/a> on remote training.<\/li>\n<\/ul>\n\n<p>I'm no T-SNE expert, but from the screenshot below, you can see that the t-sne outputs are the same when I run the script locally and remotely. This might be possible with Studio classic, but it would be hard to guarantee that it will always work.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/mhlg6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mhlg6.png\" alt=\"Azure ML Experiment Results Page\"><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"servic docker conda environ confid repeat servic studio classic machinelearningnotebook repo tutori start servic proof concept creat demonstr servic local aml workspac sne output run local remot servic",
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_original_content":"definit empath data scientist studio classic isn capabl servic studio classic defin environ determinist add remov packag servic execut built docker conda environ confid repeat highli time happi debug come machinelearningnotebook repo great tutori start spent hour proof concept demonstr servic synthes sampl share notebook jake vanderpla sklearn tutori remot train sne expert screenshot sne output run local remot studio classic",
        "Solution_preprocessed_content":"definit empath data scientist studio isn capabl servic studio classic defin environ determinist add remov packag servic execut built environ confid repeat highli time machinelearningnotebook repo great tutori start spent hour proof concept demonstr servic synthes sampl jake vanderpla sklearn tutori remot train expert screenshot output run local remot studio classic",
        "Solution_readability":13.5,
        "Solution_reading_time":28.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":242.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1508517418056,
        "Answerer_location":null,
        "Answerer_reputation_count":156.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":3.9598897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to access the Workspace object in my <code>train.py<\/code> script, when running in an Estimator.  <\/p>\n\n<p>I currently can access the Run object, using the following code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>run = Run.get_context()\n<\/code><\/pre>\n\n<p>But I cannot seem to get my hands on the Workspace object in my training script.  I would use this mostly to get access to the Datastores and Datasets (as I would hope to keep all data set references inside the training script, instead of passing them as input datasets)<\/p>\n\n<p>Any idea if\/how this is possible ?<\/p>",
        "Challenge_closed_time":1591197955080,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591183699477,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62171724",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":8.21,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":3.9598897222,
        "Challenge_title":"How can I access the Workspace object from a training script in AzureML?",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1148,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>Sure, try this:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\nws = run.experiment.workspace\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"access workspac object train import run object workspac achiev core run import run run run context run workspac",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"core run import run run run context run workspac",
        "Solution_preprocessed_content":null,
        "Solution_readability":10.8,
        "Solution_reading_time":1.78,
        "Solution_score_count":12.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":12.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":1014.9057013889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a training job yesterday, same as usual, just adding few more training data. I didn't have any problem with this in the last 2 years (the same exact procedure and code). This time after 14 hours more or less simply stalled.\nTraining job is still &quot;in processing&quot;, but cloudwatch is not logging anything since then. Right now 8 more hours passed and no new entry is in the logs, no errors no crash.\nCan someone explain this ? Unfortunately I don't have any AWS support plan.\nAs you can see from the picture below after 11am there is nothing..<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hswD7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hswD7.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The training job is supposed to complete in the next couple of hours, but now I'm not sure if is actually running (in this case would be a cloudwatch problem) or not..<\/p>\n<p><strong>UPDATE<\/strong><\/p>\n<p>Suddenly the training job failed, without any further log. The reason is<\/p>\n<blockquote>\n<p>ClientError: Artifact upload failed:Error 7: The credentials received\nhave been expired<\/p>\n<\/blockquote>\n<p>But there is still nothing in the logs after 11am. Very weird.<\/p>",
        "Challenge_closed_time":1616684677672,
        "Challenge_comment_count":5,
        "Challenge_created_time":1612979073253,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1613031017147,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66142193",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":7.3,
        "Challenge_reading_time":15.88,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1029.3345608333,
        "Challenge_title":"AWS Sagemaker training job stuck in progress state",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":755,
        "Challenge_word_count":188,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416346350292,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation_count":2302.0,
        "Poster_view_count":227.0,
        "Solution_body":"<p>For future readers I can confirm that is something that can happen very rarely (I' haven't experienced it anymore since then), but it's AWS fault. Same data, same algorithm.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"futur reader rare haven experienc anymor fault data algorithm",
        "Solution_preprocessed_content":"futur reader rare fault data algorithm",
        "Solution_readability":6.6,
        "Solution_reading_time":2.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1454593215100,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":17600.8984933333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So my question is this, <\/p>\n\n<p>When creating a notebook in <code>Sagemaker<\/code> <code>AWS<\/code> I need to help the devEngineer keep his secret key in <code>.ssh\/id_rsa<\/code> as the file after every instance reboot becomes empty. \nHe requires a <code>github<\/code> repo to be downloaded and he has to work on the code and then push the updates as needed. \nPlease let me know what details I need to provide to help you help me. \nThanks. <\/p>",
        "Challenge_closed_time":1612305167163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548941458327,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1548941932587,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54461730",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.8,
        "Challenge_reading_time":6.51,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":17601.0302322222,
        "Challenge_title":"How to create a permanent login from jupyter notebook to github with ssh_rsa key pair",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1856,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_created_time":1541484812983,
        "Poster_location":"London, UK",
        "Poster_reputation_count":33.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>This is the filesystems for my notebook instance:<\/p>\n<pre><code>sh-4.2$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\ndevtmpfs         16G   76K   16G   1% \/dev\ntmpfs            16G     0   16G   0% \/dev\/shm\n\/dev\/nvme0n1p1   94G   76G   19G  81% \/\n\/dev\/nvme1n1     99G   40G   55G  43% \/home\/ec2-user\/SageMaker\n<\/code><\/pre>\n<p>Note that one pointing to <code>\/home\/ec2-user\/SageMaker<\/code> is the only one which is saved between reboots. Since ssh keys are stored in <code>\/home\/ec2-user\/.ssh<\/code>, they are lost after reboot.<\/p>\n<p>The way I make it work is:<\/p>\n<ol>\n<li>Create the folder <code>\/home\/ec2-user\/SageMaker\/.ssh<\/code><\/li>\n<li>Run <code>ssh-keygen<\/code> and set the location <code>\/home\/ec2-user\/SageMaker\/.ssh\/id_rsa<\/code><\/li>\n<li>Clone repo with <code>GIT_SSH_COMMAND=&quot;ssh -i ~\/SageMaker\/.ssh\/id_rsa -F \/dev\/null&quot; git clone git@domain:account\/repo.git<\/code><\/li>\n<li>cd repo<\/li>\n<li>Set your repo to use the custom location with <code>git config core.sshCommand &quot;ssh -i ~\/SageMaker\/.ssh\/id_rsa -F \/dev\/null&quot;<\/code><\/li>\n<\/ol>\n<p>Based on <a href=\"https:\/\/superuser.com\/a\/912281\">https:\/\/superuser.com\/a\/912281<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"creat folder home ssh run ssh keygen set locat home ssh rsa clone repo git ssh ssh ssh rsa dev null git clone git domain account repo git repo set repo locat git config core sshcommand ssh ssh rsa dev null",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"filesystem notebook instanc filesystem size mount devtmpf dev tmpf dev shm dev nvmenp dev nvmen home note home save reboot ssh kei store home ssh lost reboot creat folder home ssh run ssh keygen set locat home ssh rsa clone repo git ssh ssh ssh rsa dev null git clone git domain account repo git repo set repo locat git config core sshcommand ssh ssh rsa dev null base http superus com",
        "Solution_preprocessed_content":"filesystem notebook instanc note save reboot ssh kei store lost reboot creat folder run set locat clone repo repo set repo locat base",
        "Solution_readability":10.0,
        "Solution_reading_time":14.69,
        "Solution_score_count":3.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":123.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1547398724312,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":667.1184413889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have completed a labelling job in AWS ground truth and started working on the notebook template for object detection.<\/p>\n\n<p>I have 2 manifests which has 293 labeled images for birds in a train and validation set like this:<\/p>\n\n<pre><code>{\"source-ref\":\"s3:\/\/XXXXXXX\/Train\/Blackbird_1.JPG\",\"Bird-Label-Train\":{\"workerId\":XXXXXXXX,\"imageSource\":{\"s3Uri\":\"s3:\/\/XXXXXXX\/Train\/Blackbird_1.JPG\"},\"boxesInfo\":{\"annotatedResult\":{\"boundingBoxes\":[{\"width\":1612,\"top\":841,\"label\":\"Blackbird\",\"left\":1276,\"height\":757}],\"inputImageProperties\":{\"width\":3872,\"height\":2592}}}},\"Bird-Label-Train-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"bird-label-train\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-16T17:28:23+0000\"}}\n<\/code><\/pre>\n\n<p>Below are the parameters I am using for the notebook instance:<\/p>\n\n<pre><code>training_params = \\\n{\n    \"AlgorithmSpecification\": {\n        \"TrainingImage\": training_image, # NB. This is one of the named constants defined in the first cell.\n        \"TrainingInputMode\": \"Pipe\"\n    },\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": s3_output_path\n    },\n    \"ResourceConfig\": {\n        \"InstanceCount\": 1,   \n        \"InstanceType\": \"ml.p3.2xlarge\",\n        \"VolumeSizeInGB\": 5\n    },\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": { # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n         \"base_network\": \"resnet-50\",\n         \"use_pretrained_model\": \"1\",\n         \"num_classes\": \"1\",\n         \"mini_batch_size\": \"16\",\n         \"epochs\": \"5\",\n         \"learning_rate\": \"0.001\",\n         \"lr_scheduler_step\": \"3,6\",\n         \"lr_scheduler_factor\": \"0.1\",\n         \"optimizer\": \"rmsprop\",\n         \"momentum\": \"0.9\",\n         \"weight_decay\": \"0.0005\",\n         \"overlap_threshold\": \"0.5\",\n         \"nms_threshold\": \"0.45\",\n         \"image_shape\": \"300\",\n         \"label_width\": \"350\",\n         \"num_training_samples\": str(num_training_samples)\n    },\n    \"StoppingCondition\": {\n        \"MaxRuntimeInSeconds\": 86400\n    },\n \"InputDataConfig\": [\n    {\n        \"ChannelName\": \"train\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                \"S3Uri\": s3_train_data_path,\n                \"S3DataDistributionType\": \"FullyReplicated\",\n                \"AttributeNames\": [\"source-ref\",\"Bird-Label-Train\"] # NB. This must correspond to the JSON field names in your augmented manifest.\n            }\n        },\n        \"ContentType\": \"image\/jpeg\",\n        \"RecordWrapperType\": \"None\",\n        \"CompressionType\": \"None\"\n    },\n    {\n        \"ChannelName\": \"validation\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                \"S3Uri\": s3_validation_data_path,\n                \"S3DataDistributionType\": \"FullyReplicated\",\n                \"AttributeNames\": [\"source-ref\",\"Bird-Label\"] # NB. This must correspond to the JSON field names in your augmented manifest.\n            }\n        },\n        \"ContentType\": \"image\/jpeg\",\n        \"RecordWrapperType\": \"None\",\n        \"CompressionType\": \"None\"\n    }\n]\n<\/code><\/pre>\n\n<p>I would end up with this being printed after running my ml.p3.2xlarge instance:<\/p>\n\n<pre><code>InProgress Starting\nInProgress Starting\nInProgress Starting\nInProgress Training\nFailed Failed\n<\/code><\/pre>\n\n<p>Followed by this error message: \n<strong>'ClientError: train channel is not specified.'<\/strong><\/p>\n\n<p>Does anyone have any thoughts for how I can get this running with no errors? Any help is much apreciated!<\/p>\n\n<p><strong>Successful run:<\/strong> Below is the paramaters that were used, along with the Augmented Manifest JSON Objects for a successful run.<\/p>\n\n<pre><code>training_params = \\\n{\n    \"AlgorithmSpecification\": {\n        \"TrainingImage\": training_image, # NB. This is one of the named constants defined in the first cell.\n        \"TrainingInputMode\": \"Pipe\"\n    },\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": s3_output_path\n    },\n    \"ResourceConfig\": {\n        \"InstanceCount\": 1,   \n        \"InstanceType\": \"ml.p3.2xlarge\",\n        \"VolumeSizeInGB\": 50\n    },\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": { # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n         \"base_network\": \"resnet-50\",\n         \"use_pretrained_model\": \"1\",\n         \"num_classes\": \"3\",\n         \"mini_batch_size\": \"1\",\n         \"epochs\": \"5\",\n         \"learning_rate\": \"0.001\",\n         \"lr_scheduler_step\": \"3,6\",\n         \"lr_scheduler_factor\": \"0.1\",\n         \"optimizer\": \"rmsprop\",\n         \"momentum\": \"0.9\",\n         \"weight_decay\": \"0.0005\",\n         \"overlap_threshold\": \"0.5\",\n         \"nms_threshold\": \"0.45\",\n         \"image_shape\": \"300\",\n         \"label_width\": \"350\",\n         \"num_training_samples\": str(num_training_samples)\n    },\n    \"StoppingCondition\": {\n        \"MaxRuntimeInSeconds\": 86400\n    },\n    \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                    \"S3Uri\": s3_train_data_path,\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"AttributeNames\": attribute_names # NB. This must correspond to the JSON field names in your **TRAIN** augmented manifest.\n                }\n            },\n            \"ContentType\": \"application\/x-recordio\",\n            \"RecordWrapperType\": \"RecordIO\",\n            \"CompressionType\": \"None\"\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                    \"S3Uri\": s3_validation_data_path,\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"AttributeNames\": [\"source-ref\",\"ValidateBird\"] # NB. This must correspond to the JSON field names in your **VALIDATION** augmented manifest.\n                }\n            },\n            \"ContentType\": \"application\/x-recordio\",\n            \"RecordWrapperType\": \"RecordIO\",\n            \"CompressionType\": \"None\"\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>Training Augmented Manifest File generated during the running of the training job<\/p>\n\n<pre><code>Line 1\n{\"source-ref\":\"s3:\/\/XXXXX\/Train\/Blackbird_1.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":1613,\"top\":840,\"height\":766,\"left\":1293}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:21:29.829003\",\"type\":\"groundtruth\/object-detection\"}}\n\n\nLine 2\n{\"source-ref\":\"s3:\/\/xxxxx\/Train\/Blackbird_2.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":897,\"top\":665,\"height\":1601,\"left\":1598}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:22:34.502274\",\"type\":\"groundtruth\/object-detection\"}}\n\n\nLine 3\n{\"source-ref\":\"s3:\/\/XXXXX\/Train\/Blackbird_3.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":1040,\"top\":509,\"height\":1695,\"left\":1548}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:20:26.660164\",\"type\":\"groundtruth\/object-detection\"}}\n<\/code><\/pre>\n\n<p>I then unzip the model.tar file to get the following files:hyperparams.JSON, model_algo_1-0000.params and model_algo_1-symbol<\/p>\n\n<p>hyperparams.JSON looks like this:<\/p>\n\n<pre><code>{\"label_width\": \"350\", \"early_stopping_min_epochs\": \"10\", \"epochs\": \"5\", \"overlap_threshold\": \"0.5\", \"lr_scheduler_factor\": \"0.1\", \"_num_kv_servers\": \"auto\", \"weight_decay\": \"0.0005\", \"mini_batch_size\": \"1\", \"use_pretrained_model\": \"1\", \"freeze_layer_pattern\": \"\", \"lr_scheduler_step\": \"3,6\", \"early_stopping\": \"False\", \"early_stopping_patience\": \"5\", \"momentum\": \"0.9\", \"num_training_samples\": \"11\", \"optimizer\": \"rmsprop\", \"_tuning_objective_metric\": \"\", \"early_stopping_tolerance\": \"0.0\", \"learning_rate\": \"0.001\", \"kv_store\": \"device\", \"nms_threshold\": \"0.45\", \"num_classes\": \"1\", \"base_network\": \"resnet-50\", \"nms_topk\": \"400\", \"_kvstore\": \"device\", \"image_shape\": \"300\"}\n<\/code><\/pre>",
        "Challenge_closed_time":1549801330896,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547399704507,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1551005141750,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54171261",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":102.2,
        "Challenge_score_count":3,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":56,
        "Challenge_solved_time":667.1184413889,
        "Challenge_title":"ClientError: train channel is not specified with AWS object_detection_augmented_manifest_training using ground truth images",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":1312,
        "Challenge_word_count":542,
        "Platform":"Stack Overflow",
        "Poster_created_time":1547398724312,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Thank you again for your help. All of which were valid in helping me get further. Having received a response on the AWS forum pages, I finally got it working.<\/p>\n\n<p>I understood that my JSON was slightly different to the augmented manifest training guide. Having gone back to basics, I created another labelling job, but used the 'Bounding Box' type as opposed to the 'Custom - Bounding box template'. My output matched what was expected. This ran with no errors!<\/p>\n\n<p>As my purpose was to have multiple labels, I was able to edit the files and mapping of my output manifests, which also worked!<\/p>\n\n<p>i.e.<\/p>\n\n<pre><code>{\"source-ref\":\"s3:\/\/xxxxx\/Blackbird_15.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":0,\"width\":2023,\"top\":665,\"height\":1421,\"left\":1312}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.174131\",\"type\":\"groundtruth\/object-detection\"}}\n{\"source-ref\":\"s3:\/\/xxxx\/Pigeon_19.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":2,\"width\":784,\"top\":634,\"height\":1657,\"left\":1306}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"2\":\"Pigeon\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.074809\",\"type\":\"groundtruth\/object-detection\"}} \n<\/code><\/pre>\n\n<p>The original mapping was 0:'Bird' for all images through the labelling job.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_gpt_summary":"creat label job bound box type bound box templat edit file map output manifest multipl label",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"receiv respons forum page final understood json slightli augment manifest train guid gone creat label job bound box type oppos bound box templat output match ran purpos multipl label edit file map output manifest sourc ref blackbird jpg validatebird annot class width height left imag size width depth height validatebird metadata job label job validatebird class map blackbird human annot object confid creation date type groundtruth object detect sourc ref pigeon jpg validatebird annot class width height left imag size width depth height validatebird metadata job label job validatebird class map pigeon human annot object confid creation date type groundtruth object detect origin map bird imag label job",
        "Solution_preprocessed_content":"receiv respons forum page final understood json slightli augment manifest train guid gone creat label job bound box type oppos bound box templat output match ran purpos multipl label edit file map output manifest origin map bird imag label job",
        "Solution_readability":16.7,
        "Solution_reading_time":21.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":119.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":0.2689961111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to organize the node functions by Classes in the nodes.py file. For example, functions related to cleaning data are in the \"CleanData\" Class, with a @staticmethod decorator, while other functions will stay in the \"Other\" Class, without any decorator (the names of these classes are merely representative). In the pipeline file, I tried importing the names of the classes, the names of the nodes and the following way: CleanData.function1 (which gave an error) and none of them worked. How can I call the nodes from the classes, if possible, please?<\/p>",
        "Challenge_closed_time":1573209953536,
        "Challenge_comment_count":2,
        "Challenge_created_time":1573208985150,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58764792",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":8.7,
        "Challenge_reading_time":7.59,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.2689961111,
        "Challenge_title":"How to run functions from a Class in the nodes.py file?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":289,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572973807452,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>I'm not entirely certain what the error you're getting is. If you're literally trying to do <code>from .nodes import CleanData.function1<\/code> that won't work. Imports don't work like that in Python. If you do something like this:<\/p>\n\n<p><code>nodes.py<\/code> has:<\/p>\n\n<pre><code>class CleanData:\n    def clean(arg1):\n        pass\n<\/code><\/pre>\n\n<p>and <code>pipeline.py<\/code> has:<\/p>\n\n<pre><code>from kedro.pipeline import Pipeline, node\nfrom .nodes import CleanData\n\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                CleanData.clean,\n                \"example_iris_data\",\n                None,\n            )\n        ]\n    )\n<\/code><\/pre>\n\n<p>that should work.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"defin class function node file import pipelin file function call class function separ dot",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"entir liter node import cleandata function import node class cleandata clean arg pass pipelin pipelin import pipelin node node import cleandata creat pipelin kwarg return pipelin node cleandata clean iri data",
        "Solution_preprocessed_content":null,
        "Solution_readability":8.1,
        "Solution_reading_time":7.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":68.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":23.0099397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running local unsupervised learning (predominantly clustering) on a large, single node with GPU.<\/p>\n<p>Does SageMaker support <strong>distributed unsupervised learning<\/strong> using <strong>clustering<\/strong>?<\/p>\n<p>If yes, please provide the relevant example (preferably non-TensorFlow).<\/p>",
        "Challenge_closed_time":1663485400223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663402564440,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73753271",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":4.64,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":23.0099397222,
        "Challenge_title":"Distributed Unsupervised Learning in SageMaker",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":14,
        "Challenge_word_count":36,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>SageMaker Training allow you to bring your own training scripts, and supports various forms of distributed training, like data\/model parallel, and frameworks like PyTorch DDP, Horovod, DeepSpeed, etc.\nAdditionally, if you want to bring your data, but not code, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algorithms-unsupervised.html\" rel=\"nofollow noreferrer\">SageMaker training offers various unsupervised built-in algorithms<\/a>, some of which are parallelizable.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"train allow train distribut train data model parallel framework pytorch ddp horovod deepspe train unsupervis built algorithm paralleliz data distribut unsupervis cluster train built algorithm",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"train allow train distribut train data model parallel framework pytorch ddp horovod deepspe data train unsupervis built algorithm paralleliz",
        "Solution_preprocessed_content":"train allow train distribut train parallel framework pytorch ddp horovod deepspe data train unsupervis algorithm paralleliz",
        "Solution_readability":16.2,
        "Solution_reading_time":6.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":16.1130741667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Similar question as <a href=\"https:\/\/stackoverflow.com\/questions\/43176442\/install-r-packages-in-azure-ml\">here<\/a> but now on Python packages. Currently, the CVXPY is missing in Azure ML. I am also trying to get other solvers such as GLPK, CLP and COINMP working in Azure ML.<\/p>\n<p><strong>How can I install Python packages in Azure ML?<\/strong><\/p>\n<hr \/>\n<p><em>Update about trying to install the Python packages not found in Azure ML.<\/em><\/p>\n<blockquote>\n<p>I did as instructed by Peter Pan but I think the 32bits CVXPY files are wrong for the Anaconda 4 and Python 3.5 in Azure ML, logs and errors are <a href=\"https:\/\/pastebin.com\/zN5QrPtL\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<pre><code>[Information]         Running with Python 3.5.1 |Anaconda 4.0.0 (64-bit)| (default, Feb 16 2016, 09:49:46) [MSC v.1900 64 bit (AMD64)]\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rS0Us.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rS0Us.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6qz3p.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6qz3p.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9glSm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9glSm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/blockquote>\n<p><em>Update 2 with win_amd64 files (paste <a href=\"https:\/\/pastebin.com\/tisWuP5C\" rel=\"nofollow noreferrer\">here<\/a>)<\/em><\/p>\n<blockquote>\n<pre><code>[Information]         Extracting Script Bundle.zip to .\\Script Bundle\n[Information]         File Name                                             Modified             Size\n[Information]         cvxopt-1.1.9-cp35-cp35m-win_amd64.whl          2017-06-07 01:03:34      1972074\n[Information]         __MACOSX\/                                      2017-06-07 01:26:28            0\n[Information]         __MACOSX\/._cvxopt-1.1.9-cp35-cp35m-win_amd64.whl 2017-06-07 01:03:34          452\n[Information]         cvxpy-0.4.10-py3-none-any.whl                  2017-06-07 00:25:36       300880\n[Information]         __MACOSX\/._cvxpy-0.4.10-py3-none-any.whl       2017-06-07 00:25:36          444\n[Information]         ecos-2.0.4-cp35-cp35m-win_amd64.whl            2017-06-07 01:03:40        56522\n[Information]         __MACOSX\/._ecos-2.0.4-cp35-cp35m-win_amd64.whl 2017-06-07 01:03:40          450\n[Information]         numpy-1.13.0rc2+mkl-cp35-cp35m-win_amd64.whl   2017-06-07 01:25:02    127909457\n[Information]         __MACOSX\/._numpy-1.13.0rc2+mkl-cp35-cp35m-win_amd64.whl 2017-06-07 01:25:02          459\n[Information]         scipy-0.19.0-cp35-cp35m-win_amd64.whl          2017-06-07 01:05:12     12178932\n[Information]         __MACOSX\/._scipy-0.19.0-cp35-cp35m-win_amd64.whl 2017-06-07 01:05:12          452\n[Information]         scs-1.2.6-cp35-cp35m-win_amd64.whl             2017-06-07 01:03:34        78653\n[Information]         __MACOSX\/._scs-1.2.6-cp35-cp35m-win_amd64.whl  2017-06-07 01:03:34          449\n[Information]         [ READING ] 0:00:00\n[Information]         Input pandas.DataFrame #1:\n[Information]         Empty DataFrame\n[Information]         Columns: [1]\n[Information]         Index: []\n[Information]         [ EXECUTING ] 0:00:00\n[Information]         [ WRITING ] 0:00:00\n<\/code><\/pre>\n<p>where <code>import cvxpy<\/code>, <code>import cvxpy-0.4.10-py3-none-any.whl<\/code> or <code>cvxpy-0.4.10-py3-none-any<\/code> do not work so<\/p>\n<p><strong>How can I use the following wheel files downloaded from <a href=\"http:\/\/www.lfd.uci.edu\/%7Egohlke\/pythonlibs\/#cvxpy\" rel=\"nofollow noreferrer\">here<\/a> to use the external Python packages not found in Azure ML?<\/strong><\/p>\n<\/blockquote>\n<p><em>Update about permission problem about importing cvxpy (paste <a href=\"https:\/\/pastebin.com\/3kTKgLfc\" rel=\"nofollow noreferrer\">here<\/a>)<\/em><\/p>\n<blockquote>\n<pre><code> [Error]         ImportError: No module named 'canonInterface'\n<\/code><\/pre>\n<p>where the ZIP Bundle is organised a bit differently, the content of each wheel downloaded to a folder and the content having all zipped as a ZIP Bundle.<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1496732346280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1496674339213,
        "Challenge_favorite_count":3,
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44371692",
        "Challenge_link_count":11,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":10.2,
        "Challenge_reading_time":49.77,
        "Challenge_score_count":8,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":16.1130741667,
        "Challenge_title":"Install Python Packages in Azure ML?",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":12625,
        "Challenge_word_count":346,
        "Platform":"Stack Overflow",
        "Poster_created_time":1251372839052,
        "Poster_location":null,
        "Poster_reputation_count":48616.0,
        "Poster_view_count":3348.0,
        "Solution_body":"<p>According to the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts#limitations\" rel=\"nofollow noreferrer\"><code>Limitations<\/code><\/a> and <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn955437.aspx#Anchor_3\" rel=\"nofollow noreferrer\"><code>Technical Notes<\/code><\/a> of <code>Execute Python Script<\/code> tutorial, the only way to add custom Python modules is via the zip file mechanism to package the modules and all dependencies.<\/p>\n\n<p>For example to install <code>CVXPY<\/code>, as below.<\/p>\n\n<ol>\n<li>Download the wheel file of <a href=\"http:\/\/www.lfd.uci.edu\/~gohlke\/pythonlibs\/#cvxpy\" rel=\"nofollow noreferrer\"><code>CVXPY<\/code><\/a> and its dependencies like <a href=\"http:\/\/www.lfd.uci.edu\/~gohlke\/pythonlibs\/#cvxopt\" rel=\"nofollow noreferrer\"><code>CVXOPT<\/code><\/a>.<\/li>\n<li>Decompress these wheel files, and package these files in the path <code>cvxpy<\/code> and <code>cvxopt<\/code>, etc as a zipped file with your script.<\/li>\n<li>Upload the zip file as a dataset and use it as the script bundle.<\/li>\n<\/ol>\n\n<p>If you were using IPython, you also can try to install the Python Package via the code <code>!pip install cvxpy<\/code>.<\/p>\n\n<p>And there are some similar SO threads which may be helpful for you, as below.<\/p>\n\n<ol>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/44285641\/azure-ml-python-with-script-bundle-cannot-import-module\">Azure ML Python with Script Bundle cannot import module<\/a><\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/8663046\/how-to-install-a-python-package-from-within-ipython\">How to install a Python package from within IPython?<\/a><\/li>\n<\/ol>\n\n<p>Hope it helps.<\/p>\n\n<hr>\n\n<p>Update:<\/p>\n\n<p>For IPython interface of Azure ML, you move to the <code>NOTEBOOKS<\/code> tab to create a notebook via <code>ADD TO PROJECT<\/code> button at the bottom of the page, as the figure below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/X2Asv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/X2Asv.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Or you can directly login to the website <code>https:\/\/notebooks.azure.com<\/code> to use it.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_gpt_summary":"download wheel file packag depend decompress packag zip file upload zip file dataset bundl ipython instal packag pip instal cvxpy link stack overflow thread addit ipython interfac notebook tab creat notebook add button page directli login websit http notebook com",
        "Solution_last_edit_time":1496760284030,
        "Solution_link_count":9.0,
        "Solution_original_content":"accord limit technic note execut tutori add modul zip file mechan packag modul depend instal cvxpy download wheel file cvxpy depend cvxopt decompress wheel file packag file path cvxpy cvxopt zip file upload zip file dataset bundl ipython instal packag pip instal cvxpy thread bundl import modul instal packag ipython hope updat ipython interfac notebook tab creat notebook add button page figur directli login websit http notebook com",
        "Solution_preprocessed_content":"accord tutori add modul zip file mechan packag modul depend instal download wheel file depend decompress wheel file packag file path zip file upload zip file dataset bundl ipython instal packag thread bundl import modul instal packag ipython hope updat ipython interfac tab creat notebook button page figur directli login websit",
        "Solution_readability":10.8,
        "Solution_reading_time":28.66,
        "Solution_score_count":2.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":215.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1306085731476,
        "Answerer_location":"Cleveland, TN",
        "Answerer_reputation_count":7737.0,
        "Answerer_view_count":454.0,
        "Challenge_adjusted_solved_time":3.9352741667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to follow a Python tutorial and I have been able to execute almost everything, until the point of Deploying an endpoint to Azure with python.<\/p>\n\n<p>In order to give some context I have uploaded the scripts to my git account:\n<a href=\"https:\/\/github.com\/levalencia\/MLTutorial\" rel=\"nofollow noreferrer\">https:\/\/github.com\/levalencia\/MLTutorial<\/a><\/p>\n\n<p>File 1 and 2 Work perfectly fine<\/p>\n\n<p>However the following section in File 3 fails:<\/p>\n\n<pre><code>%%time\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\n\ninference_config = InferenceConfig(runtime= \"python\", \n                                   entry_script=\"score.py\",\n                                   conda_file=\"myenv.yml\")\n\nservice = Model.deploy(workspace=ws, \n                       name='keras-mnist-svc2', \n                       models=[amlModel], \n                       inference_config=inference_config, \n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n\n<p>with below error:<\/p>\n\n<pre><code>ERROR - Service deployment polling reached non-successful terminal state, current service state: Transitioning\nOperation ID: 8353cad2-4218-450a-a03b-df418725acb1\nMore information can be found here: https:\/\/machinelearnin1143382465.blob.core.windows.net\/azureml\/ImageLogs\/8353cad2-4218-450a-a03b-df418725acb1\/build.log?sv=2018-03-28&amp;sr=b&amp;sig=UKzefxIrm3l7OsXxj%2FT4RsvUfAuhuaBwaz2P4mJu7vY%3D&amp;st=2020-03-11T12%3A23%3A33Z&amp;se=2020-03-11T20%3A28%3A33Z&amp;sp=r\nError:\n{\n  \"code\": \"EnvironmentBuildFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Failed Building the Environment.\"\n}\n\nERROR - Service deployment polling reached non-successful terminal state, current service state: Transitioning\nOperation ID: 8353cad2-4218-450a-a03b-df418725acb1\nMore information can be found here: https:\/\/machinelearnin1143382465.blob.core.windows.net\/azureml\/ImageLogs\/8353cad2-4218-450a-a03b-df418725acb1\/build.log?sv=2018-03-28&amp;sr=b&amp;sig=UKzefxIrm3l7OsXxj%2FT4RsvUfAuhuaBwaz2P4mJu7vY%3D&amp;st=2020-03-11T12%3A23%3A33Z&amp;se=2020-03-11T20%3A28%3A33Z&amp;sp=r\nError:\n{\n  \"code\": \"EnvironmentBuildFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Failed Building the Environment.\"\n}\n<\/code><\/pre>\n\n<p>When I download the logs, I got this:<\/p>\n\n<pre><code>wheel-0.34.2         | 24 KB     |            |   0% [0m[91m\nwheel-0.34.2         | 24 KB     | ########## | 100% [0m\nDownloading and Extracting Packages\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... failed\n[91m\nERROR conda.core.link:_execute(502): An error occurred while installing package 'conda-forge::astor-0.7.1-py_0'.\nFileNotFoundError(2, \"No such file or directory: '\/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56\/bin\/python3.6'\")\nAttempting to roll back.\n\n[0mRolling back transaction: ...working... done\n[91m\nFileNotFoundError(2, \"No such file or directory: '\/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56\/bin\/python3.6'\")\n\n\n[0mThe command '\/bin\/sh -c ldconfig \/usr\/local\/cuda\/lib64\/stubs &amp;&amp; conda env create -p \/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56 -f azureml-environment-setup\/mutated_conda_dependencies.yml &amp;&amp; rm -rf \"$HOME\/.cache\/pip\" &amp;&amp; conda clean -aqy &amp;&amp; CONDA_ROOT_DIR=$(conda info --root) &amp;&amp; rm -rf \"$CONDA_ROOT_DIR\/pkgs\" &amp;&amp; find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + &amp;&amp; ldconfig' returned a non-zero code: 1\n2020\/03\/11 12:28:11 Container failed during run: acb_step_0. No retries remaining.\nfailed to run step ID: acb_step_0: exit status 1\n\nRun ID: cb3 failed after 2m21s. Error: failed during run, err: exit status 1\n<\/code><\/pre>\n\n<p>Update 1:<\/p>\n\n<p>I tried to run:\nconda list    --name base  conda<\/p>\n\n<p>inside the notebook and I got this:<\/p>\n\n<pre><code> # packages in environment at \/anaconda:\n    #\n    # Name                    Version                   Build  Channel\n    _anaconda_depends         2019.03                  py37_0  \n    anaconda                  custom                   py37_1  \n    anaconda-client           1.7.2                    py37_0  \n    anaconda-navigator        1.9.6                    py37_0  \n    anaconda-project          0.8.4                      py_0  \n    conda                     4.8.2                    py37_0  \n    conda-build               3.17.6                   py37_0  \n    conda-env                 2.6.0                         1  \n    conda-package-handling    1.6.0            py37h7b6447c_0  \n    conda-verify              3.1.1                    py37_0  \n\n    Note: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n\n<p>However in the deployment log I got this:<\/p>\n\n<pre><code>Solving environment: ...working... \ndone\n[91m\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 4.5.11\n  latest version: 4.8.2\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n<\/code><\/pre>",
        "Challenge_closed_time":1584015256823,
        "Challenge_comment_count":6,
        "Challenge_created_time":1583931341127,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1584001089836,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60636558",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":12.9,
        "Challenge_reading_time":59.69,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":23.3099155556,
        "Challenge_title":"ERROR conda.core.link:_execute(502): An error occurred while installing package 'conda-forge::astor-0.7.1-py_0'",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":5656,
        "Challenge_word_count":421,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>Unfortunately there seems to an issue with this version of Conda (4.5.11). To complete this task in the tutorial, you can just update the dependency for Tensorflow and Keras to be from <code>pip<\/code> and not <code>conda<\/code>. There are reasons why this is less than ideal for a production environment. The Azure ML <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py\" rel=\"nofollow noreferrer\">documentation states<\/a>:<\/p>\n\n<blockquote>\n  <p>\"If your dependency is available through both Conda and pip (from\n  PyPi), use the Conda version, as Conda packages typically come with\n  pre-built binaries that make installation more reliable.\"<\/p>\n<\/blockquote>\n\n<p>In this case though, if you update the following code block:<\/p>\n\n<pre><code>from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_conda_package(\"tensorflow\")\nmyenv.add_conda_package(\"keras\")\n\nwith open(\"myenv.yml\",\"w\") as f:\n    f.write(myenv.serialize_to_string())\n\n# Review environment file\nwith open(\"myenv.yml\",\"r\") as f:\n    print(f.read())\n<\/code><\/pre>\n\n<p>To be the following:<\/p>\n\n<pre><code>from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_pip_package(\"tensorflow==2.0.0\")\nmyenv.add_pip_package(\"azureml-defaults\")\nmyenv.add_pip_package(\"keras\")\n\nwith open(\"myenv.yml\", \"w\") as f:\n    f.write(myenv.serialize_to_string())\n\nwith open(\"myenv.yml\", \"r\") as f:\n    print(f.read())\n<\/code><\/pre>\n\n<p>The tutorial should be able to be completed.  Let me know if any of this does not work for you once this update has been made.<\/p>\n\n<p>I have also reported this issue to Microsoft (in regards to the Conda version).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"updat depend tensorflow kera pip conda updat block add pip packag conda packag updat block tutori complet updat conda version report",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"unfortun version conda complet task tutori updat depend tensorflow kera pip conda reason ideal environ document state depend conda pip pypi conda version conda packag typic come pre built binari instal reliabl updat block core conda depend import condadepend myenv condadepend myenv add conda packag tensorflow myenv add conda packag kera open myenv yml write myenv serial review environ file open myenv yml print read core conda depend import condadepend myenv condadepend myenv add pip packag tensorflow myenv add pip packag default myenv add pip packag kera open myenv yml write myenv serial open myenv yml print read tutori complet updat report conda version",
        "Solution_preprocessed_content":"unfortun version conda complet task tutori updat depend tensorflow kera reason ideal environ document state depend conda pip conda version conda packag typic come binari instal updat block tutori complet updat report",
        "Solution_readability":11.7,
        "Solution_reading_time":22.73,
        "Solution_score_count":2.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":187.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1620426047396,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":386.5795511111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using the library weights and  biases. My model outputs a curve (a time series). I'd like to see how this curve changes throughout training. So, I'd need some kind of slider where I can select epoch and it shows me the curve for that epoch. It could be something very similar to what it's done with histograms (it shows an image of the histograms across epochs and when you hover it display the histogram corresponding to that epoch). Is there a way to do this or something similar using <code>wandb<\/code>?<\/p>\n<p>Currently my code looks like this:<\/p>\n<pre><code>for epoch in range(epochs):\n   output = model(input)\n   #output is shape (37,40) (lenght 40 and I have 37 samples)\n   #it's enough to plot the first sample\n   xs = torch.arange(40).unsqueeze(dim=1)\n   ys = output[0,:].unsqueeze(dim=1)\n   wandb.log({&quot;line&quot;: wandb.plot.line_series(xs=xs, ys=ys,title=&quot;Out&quot;)}, step=epoch)\n<\/code><\/pre>\n<p>I'd appreciate any help! Thanks!<\/p>",
        "Challenge_closed_time":1620426900467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619035214083,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67202711",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":12.43,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":386.5795511111,
        "Challenge_title":"How to get multiple lines exported to wandb",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":840,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1577734207070,
        "Poster_location":null,
        "Poster_reputation_count":422.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>You can use <code>wandb.log()<\/code> with matplotlib. Create your plot using matplotlib:<\/p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(0, 50)\nfor i in range(1, 4):\n    fig, ax = plt.subplots()\n    y = x ** i\n    ax.plot(x, y)\n    wandb.log({'chart': ax})\n<\/code><\/pre>\n<p>Then when you look on your wandb dashboard for the run, you will see the plot rendered as plotly plot. Click the gear in the upper left hand corner to see a slider that lets you slide over training steps and see the plot at each step.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"log matplotlib creat plot epoch dashboard plot render plotli plot slider allow slide train step plot step",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"log matplotlib creat plot matplotlib import matplotlib pyplot plt import numpi linspac rang fig plt subplot plot log chart dashboard run plot render plotli plot click gear upper left hand corner slider slide train step plot step",
        "Solution_preprocessed_content":"matplotlib creat plot matplotlib dashboard run plot render plotli plot click gear upper left hand corner slider slide train step plot step",
        "Solution_readability":3.4,
        "Solution_reading_time":6.54,
        "Solution_score_count":3.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":85.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":336.0223277778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running a <strong>Sagemaker pipeline<\/strong> with 2 steps, tuning and then training. The purpose is the get the best hyperparameter with tuning, and then use those hyperparameters in the next training step.\nI am aware that I can use <code>HyperparameterTuningJobAnalytics<\/code> to retrieve the tuning job specs after the tuning. However, I want to be able to use the hyperparameters like dependency and pass them directly to next trainingStep's estimator, see code below:\n<code>hyperparameters=step_tuning.properties.BestTrainingJob.TunedHyperParameters,<\/code>\nBut this doesn't work with this error msg: <code>AttributeError: 'PropertiesMap' object has no attribute 'update'<\/code><\/p>\n<pre><code>tf_estimator_final = TensorFlow(entry_point='.\/train.py',\n                          role=role,\n                          sagemaker_session=sagemaker_session,\n                          code_location=code_location,\n                          instance_count=1,\n                          instance_type=&quot;ml.p3.16xlarge&quot;,\n                          framework_version='2.4',\n                          py_version=&quot;py37&quot;,\n                          base_job_name=base_job_name,\n                          output_path=model_path, # if output_path not specified,\nhyperparameters=step_tuning.properties.BestTrainingJob.TunedHyperParameters,\n                          model_dir=&quot;\/opt\/ml\/model&quot;,\n                          script_mode=True\n                          )\n\nstep_train = TrainingStep(\n    name=base_job_name,\n    estimator=tf_estimator_final,\n    inputs={\n        &quot;train&quot;: TrainingInput(\n            s3_data=train_s3\n        )\n    },\n    depends_on = [step_tuning]\n)\n\npipeline = Pipeline(\n    name=jobname,\n    steps=[\n        step_tuning,\n        step_train\n    ],\n    sagemaker_session=sagemaker_session\n)\n\njson.loads(pipeline.definition())\n<\/code><\/pre>\n<p>Any suggestions?<\/p>",
        "Challenge_closed_time":1661377323147,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660154954560,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1660167642767,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73310895",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":21.6,
        "Challenge_reading_time":21.4,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":339.5468297222,
        "Challenge_title":"Sagemaker how to pass tuning step's best hyperparameter into another estimator?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":79,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1542606952710,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>This can't be done in SageMaker Pipelines at the moment.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":4.8,
        "Solution_reading_time":0.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1369156710532,
        "Answerer_location":"Japan",
        "Answerer_reputation_count":189.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":2.8227241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a project in which I do several optuna studies each having around 50 trials.<\/p>\n<p>The optuna documentation suggests saving each model to a file for later use on <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/faq.html#how-to-define-objective-functions-that-have-own-arguments\" rel=\"nofollow noreferrer\">this FAQ section<\/a><\/p>\n<p>What I want is to have all the best models of different studies in a python list. How is that possible?<\/p>\n<p>This is somewhat similar to my code:<\/p>\n<pre><code>def objective(trial, n_epochs, batch_size):\n     params = {\n              'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n              'optimizer': trial.suggest_categorical(&quot;optimizer&quot;, [&quot;Adam&quot;, &quot;RMSprop&quot;, &quot;SGD&quot;]),\n              'n_epochs': n_epochs,\n              'batch_size': batch_size\n              }\n     model = clp_network(trial)\n     accuracy, model = train_and_evaluate(params, model, trial) # THIS LINE\n     return accuracy\n<\/code><\/pre>\n<pre><code>     for i in range(50):\n           study = optuna.create_study(direction=&quot;maximize&quot;, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n           study.optimize(lambda trial: objective(trial, e, b), n_trials=50, show_progress_bar=True)\n<\/code><\/pre>\n<p>I would like to either save the <code>model<\/code> variable in the line marked <strong>THIS LINE<\/strong>, or somehow get the best model as a variable from the study.<\/p>",
        "Challenge_closed_time":1661860813547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661850651740,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73539873",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":18.69,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":2.8227241667,
        "Challenge_title":"saving trained models in optuna to a variable",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":23,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1612517804323,
        "Poster_location":"Babol, Mazandaran, Iran",
        "Poster_reputation_count":125.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>The easiest way is to define a global variable to store a model for each trial as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import optuna\nfrom collections import defaultdict\n\n\nmodels = defaultdict(dict)\n\ndef objective(t):\n    model = t.suggest_int(&quot;x&quot;, 0, 100)\n    models[t.study.study_name][t.number] = model\n    \n    return model\n\nfor _ in range(10):\n    s = optuna.create_study()\n    s.optimize(objective, n_trials=10)\n\n<\/code><\/pre>\n<p>However I reckon this approach is not scalable in terms of memory space, so I'd suggest removing non-best models after each <code>optimize<\/code> call or saving models on an external file as mentioned in Optuna's FAQ.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"defin global variabl store model trial remov model optim save model extern file faq note scalabl term memori space",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"easiest defin global variabl store model trial import collect import defaultdict model defaultdict dict object model model studi studi model return model rang creat studi optim object trial reckon scalabl term memori space remov model optim save model extern file faq",
        "Solution_preprocessed_content":"easiest defin global variabl store model trial reckon scalabl term memori space remov model save model extern file faq",
        "Solution_readability":14.8,
        "Solution_reading_time":8.49,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":82.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1520413126203,
        "Answerer_location":null,
        "Answerer_reputation_count":37123.0,
        "Answerer_view_count":4058.0,
        "Challenge_adjusted_solved_time":16.5961252778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am getting consistent error \"Your session has expired\" (screenshot below), after logging in to machine learning studio. <\/p>\n\n<p>I have tried chrome incognito and guest windows, but no difference. <\/p>\n\n<p>I am using a new account and have signed up for Free workspace. Any suggestion to get past this or delete workspace, to start again?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/fsqtw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fsqtw.png\" alt=\"Error screenshot\"><\/a><\/p>",
        "Challenge_closed_time":1557300039456,
        "Challenge_comment_count":1,
        "Challenge_created_time":1557237613340,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1557240710416,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56024354",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":7.18,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":17.3405877778,
        "Challenge_title":"\"Session has expired\" message with Machine Learning Studio",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":482,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1255194026492,
        "Poster_location":"Bedford, MA, USA",
        "Poster_reputation_count":300.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>I can reproduce your issue, I sign out and log in <a href=\"https:\/\/studio.azureml.net\/\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/<\/a> again, it solved my problem. Or you can try to clear the browsing data or change a browser. Anyway, the issue should be caused by the browser, not azure. Even if your account is not the owner of the workspace, when you click <code>Sign In<\/code> in <a href=\"https:\/\/studio.azureml.net\/\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/<\/a> , it will create a free workspace(with a different workspace id) for you automatically.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/MXDJC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/MXDJC.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If you want to delete the workspace, you need to let the owner of the workspace delete it, navigate to the <code>SETTINGS<\/code> on the left of the studio -> <code>NAME<\/code> -> <code>DELETE WORKSPACE<\/code>. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/E6aUl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E6aUl.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"sign log clear brows data browser browser delet workspac owner workspac delet navig set delet workspac",
        "Solution_last_edit_time":1557300456467,
        "Solution_link_count":8.0,
        "Solution_original_content":"reproduc sign log http studio net clear brows data browser browser account owner workspac click sign http studio net creat free workspac workspac automat delet workspac owner workspac delet navig set left studio delet workspac",
        "Solution_preprocessed_content":"reproduc sign log clear brows data browser browser account owner workspac click creat free workspac automat delet workspac owner workspac delet navig left studio",
        "Solution_readability":8.0,
        "Solution_reading_time":14.76,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":128.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1294730361590,
        "Answerer_location":"New York, NY",
        "Answerer_reputation_count":57082.0,
        "Answerer_view_count":4597.0,
        "Challenge_adjusted_solved_time":4.4930697222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm quite new using Kedro and after installing kedro in my conda environment, I'm getting the following error when trying to list my catalog:<\/p>\n<p>Command performed: <code>kedro catalog list<\/code><\/p>\n<p>Error:<\/p>\n<blockquote>\n<p>kedro.io.core.DataSetError: An exception occurred when parsing config\nfor DataSet <code>df_medinfo_raw<\/code>: Object <code>ParquetDataSet<\/code> cannot be loaded\nfrom <code>kedro.extras.datasets.pandas<\/code>. Please see the documentation on\nhow to install relevant dependencies for\nkedro.extras.datasets.pandas.ParquetDataSet:<\/p>\n<\/blockquote>\n<p>I installed kedro trough conda-forge: <code>conda install -c conda-forge &quot;kedro[pandas]&quot;<\/code>. As far as I understand, this way to install kedro also installs the pandas dependencies.<\/p>\n<p>I tried to read the kedro documentation for dependencies, but it's not really clear how to solve this kind of issue.<\/p>\n<p>My kedro version is <strong>0.17.6<\/strong>.<\/p>",
        "Challenge_closed_time":1642280220676,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642224358493,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1642264400916,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70719080",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":12.6,
        "Challenge_reading_time":13.71,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":15.5172730556,
        "Challenge_title":"AttributeError: Object ParquetDataSet cannot be loaded from kedro.extras.datasets.pandas",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":863,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Poster_created_time":1492098397316,
        "Poster_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Poster_reputation_count":135.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>Kedro uses Pandas to load <code>ParquetDataSet<\/code> objects, and Pandas requires additional dependencies to accomplish this (see <a href=\"https:\/\/pandas.pydata.org\/docs\/getting_started\/install.html#other-data-sources\" rel=\"nofollow noreferrer\">&quot;Installation: Other data sources&quot;<\/a>). That is, in addition to Pandas, one must also install either <code>fastparquet<\/code> or <code>pyarrow<\/code>.<\/p>\n<p>For Conda you either want:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>## use pyarrow for parquet\nconda install -c conda-forge kedro pandas pyarrow\n<\/code><\/pre>\n<p>or<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>## or use fastparquet for parquet\nconda install -c conda-forge kedro pandas fastparquet\n<\/code><\/pre>\n<p>Note that the syntax used in the question <code>kedro[pandas]<\/code> is meaningless to Conda (i.e., it ultimately parses to just <code>kedro<\/code>). Conda package specification uses <a href=\"https:\/\/stackoverflow.com\/a\/57734390\/570918\">a custom grammar called <code>MatchSpec<\/code><\/a>, where anything inside a <code>[...]<\/code> is parsed for a <code>[key1=value1;key2=value2;...]<\/code> syntax. Essentially, the <code>[pandas]<\/code> is treated as an unknown key, which is ignored.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"instal fastparquet pyarrow addit panda load parquetdataset object conda instal depend conda instal conda forg panda pyarrow conda instal conda forg panda fastparquet syntax panda conda",
        "Solution_last_edit_time":1642280575967,
        "Solution_link_count":2.0,
        "Solution_original_content":"panda load parquetdataset object panda addit depend accomplish instal data sourc addit panda instal fastparquet pyarrow conda pyarrow parquet conda instal conda forg panda pyarrow fastparquet parquet conda instal conda forg panda fastparquet note syntax panda conda ultim pars conda packag grammar call matchspec insid pars kei valu kei valu syntax essenti panda treat unknown kei ignor",
        "Solution_preprocessed_content":"panda load object panda addit depend accomplish addit panda instal conda note syntax conda conda packag grammar call insid pars syntax essenti treat unknown kei ignor",
        "Solution_readability":13.5,
        "Solution_reading_time":16.42,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":127.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1645519217332,
        "Answerer_location":null,
        "Answerer_reputation_count":336.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":166.7237425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How can I store additional information in an <code>optuna trial<\/code> when using it via the Hydra sweep plugin?<\/p>\n<p>My use case is as follows:\nI want to optimize a bunch of hyperparameters. I am storing all reproducibility information of all experiments (i.e., trials) in a separate database.\nI know I can get the best values via <code>optuna.load_study().best_params<\/code> or even <code>best_trial<\/code>. However, that only allows me to replicate the experiment - potentially this takes quite some time. To overcome this issue, I need to somehow link it to my own database. I would like to store the ID of my own database somewhere in the <code>trial<\/code> object.<\/p>\n<p>Without using Hydra, I suppose I'd set <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/003_attributes.html#sphx-glr-tutorial-20-recipes-003-attributes-py\" rel=\"nofollow noreferrer\">User Attributes<\/a>. However, with Hydra <a href=\"https:\/\/github.com\/facebookresearch\/hydra\/blob\/535dc7aacfe607e25848b2c4b8068317095a730b\/plugins\/hydra_optuna_sweeper\/hydra_plugins\/hydra_optuna_sweeper\/_impl.py#L183\" rel=\"nofollow noreferrer\">abstracting all that away<\/a>, there seems no option to do so.<\/p>\n<p>I know that I can just query my own database for the exact combination of best params that optuna found, but that just seems like a difficult solution to a simple problem.<\/p>\n<p>Some minimal code:<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>from dataclasses import dataclass\n\nimport hydra\nfrom hydra.core.config_store import ConfigStore\nfrom omegaconf import MISSING\n\n\n@dataclass\nclass TrainConfig:\n    x: float | int = MISSING\n    y: int = MISSING\n    z: int | None = None\n\n\nConfigStore.instance().store(name=&quot;config&quot;, node=TrainConfig)\n\n\n@hydra.main(version_base=None, config_path=&quot;conf&quot;, config_name=&quot;sweep&quot;)\ndef sphere(cfg: TrainConfig) -&gt; float:\n    x: float = cfg.x\n    y: float = cfg.y\n    return x**2 + y**2\n\n\nif __name__ == &quot;__main__&quot;:\n    sphere()\n<\/code><\/pre>\n<pre class=\"lang-yaml prettyprint-override\"><code>defaults:\n  - override hydra\/sweeper: optuna\n  - override hydra\/sweeper\/sampler: tpe\n\nhydra:\n  sweeper:\n    sampler:\n      seed: 123\n    direction: minimize\n    study_name: sphere\n    storage: sqlite:\/\/\/trials.db\n    n_trials: 20\n    n_jobs: 1\n    params:\n      x: range(-5.5, 5.5, step=0.5)\n      y: choice(-5 ,0 ,5)\n      z: choice(0, 3, 5)\n\nx: 1\ny: 1\nz: 1\n<\/code><\/pre>",
        "Challenge_closed_time":1657793209687,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657194655610,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72897321",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":31.17,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":166.2650213889,
        "Challenge_title":"Store user attributes in Optuna Sweeper plugin for Hydra",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":83,
        "Challenge_word_count":275,
        "Platform":"Stack Overflow",
        "Poster_created_time":1645519217332,
        "Poster_location":null,
        "Poster_reputation_count":336.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>A hacky solution via the <a href=\"https:\/\/hydra.cc\/docs\/plugins\/optuna_sweeper\/#experimental--custom-search-space-optimization\" rel=\"nofollow noreferrer\"><code>custom_search_space<\/code><\/a>.<\/p>\n<pre><code>hydra:\n  sweeper:\n    sampler:\n      seed: 123\n    direction: minimize\n    study_name: sphere\n    storage: sqlite:\/\/\/trials.db\n    n_trials: 20\n    n_jobs: 1\n    params:\n      x: range(-5.5, 5.5, step=0.5)\n      y: choice(-5 ,0 ,5)\n      z: choice([0, 1], [2, 3], [2, 5])\n    custom_search_space: package.run.configure\n<\/code><\/pre>\n<pre><code>def configure(_, trial: Trial) -&gt; None:\n    trial.set_user_attr(&quot;experiment_db_id&quot;, 123456)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"hacki search space option hydra sweep plugin defin function set attribut trial object pass search space option",
        "Solution_last_edit_time":1657794861083,
        "Solution_link_count":1.0,
        "Solution_original_content":"hacki search space hydra sweeper sampler seed direct minim studi sphere storag sqlite trial trial job param rang step choic choic search space packag run configur configur trial trial trial set attr",
        "Solution_preprocessed_content":null,
        "Solution_readability":15.6,
        "Solution_reading_time":8.21,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":52.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1656670919183,
        "Answerer_location":null,
        "Answerer_reputation_count":1.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":76.5733497222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new with ruby and I want to use GCP AIPlatform but I'm struggeling with the payload.<\/p>\n<p>So far, I have :<\/p>\n<pre class=\"lang-rb prettyprint-override\"><code>client = ::Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\n  config.endpoint = &quot;#{location}-aiplatform.googleapis.com&quot;\nend\n\nimg = File.open(imgPath, 'rb') do |img|\n  'data:image\/png;base64,' + Base64.strict_encode64(img.read)\nend\n\ninstance = Instance.new(:content =&gt; img)\n\nrequest = Google::Cloud::AIPlatform::V1::PredictRequest.new(\n  endpoint: &quot;projects\/#{project}\/locations\/#{location}\/endpoints\/#{endpoint}&quot;,\n  instances: [instance]\n)\n\nresult = client.predict request\np result\n<\/code><\/pre>\n<p>Here is my proto<\/p>\n<pre><code>message Instance {\n  required bytes content = 1;\n};\n<\/code><\/pre>\n<p>But I have the following error : <code>Invalid type Instance to assign to submessage field 'instances'<\/code><\/p>\n<p>I read the documentation but for ruby SDK it's a bit light.\nThe parameters are OK, the JS example here : <a href=\"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-image-object-detection.js\" rel=\"nofollow noreferrer\">https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-image-object-detection.js<\/a> is working with those parameters<\/p>\n<p>What am I doing wrong ?<\/p>",
        "Challenge_closed_time":1656947266916,
        "Challenge_comment_count":1,
        "Challenge_created_time":1656671602857,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72827960",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":15.3,
        "Challenge_reading_time":18.6,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":76.5733497222,
        "Challenge_title":"How to get image classification prediction from GCP AIPlatform in ruby?",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":54,
        "Challenge_word_count":126,
        "Platform":"Stack Overflow",
        "Poster_created_time":1656670919183,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I managed it<\/p>\n<pre class=\"lang-rb prettyprint-override\"><code>client = Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\n  config.endpoint = &quot;#{location}-aiplatform.googleapis.com&quot;\nend\n\nimg = File.open(imgPath, 'rb') do |img|\n  Base64.strict_encode64(img.read)\nend\n\ninstance = Google::Protobuf::Value.new(:struct_value =&gt; {:fields =&gt; {\n  :content =&gt; {:string_value =&gt; img}\n}})\nendpoint = &quot;projects\/#{project}\/locations\/#{location}\/endpoints\/#{endpoint}&quot;\n\n\nrequest = Google::Cloud::AIPlatform::V1::PredictRequest.new(\n  endpoint: endpoint,\n  instances: [instance]\n)\n\nresult = client.predict request\np result\n<\/code><\/pre>\n<p>The use of the Google::Protobuf::Value looks ugly to me but it works<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"share protobuf valu encod imag file pass instanc predict request aesthet pleas",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"client cloud aiplatform predictionservic client config config endpoint locat aiplatform googleapi com end img file open imgpath img base strict encod img read end instanc protobuf valu struct valu field valu img endpoint locat locat endpoint endpoint request cloud aiplatform predictrequest endpoint endpoint instanc instanc client predict request protobuf valu ugli",
        "Solution_preprocessed_content":null,
        "Solution_readability":16.6,
        "Solution_reading_time":9.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":55.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1355002392776,
        "Answerer_location":"Bolzano, Italia",
        "Answerer_reputation_count":530.0,
        "Answerer_view_count":91.0,
        "Challenge_adjusted_solved_time":1320.8344933333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After running <code>dvc push data.csv<\/code> (to ssh-remote), when i try to dvc-pull the same file on another machine from the same remote, it won't get pulled. Below are the logs and the error:<\/p>\n<pre><code>2021-01-21 22:17:26,643 DEBUG: checking if 'data.csv'('HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)') has changed.\n2021-01-21 22:17:26,643 DEBUG: 'data.csv' doesn't exist.\n2021-01-21 22:17:26,644 WARNING: Cache 'HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)' not found. File 'data.csv' won't be created.\n2021-01-21 22:17:26,644 DEBUG: cache '\/usr\/src\/bohr\/.dvc\/cache\/27\/9936268f488e1e613f81a537f29055' expected 'HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)' actual 'None'\n...\n2021-01-21 22:17:26,660 ERROR: failed to pull data from the cloud - Checkout failed for following targets:\ndata.csv\n<\/code><\/pre>\n<p>However, the file is present on the remote:<\/p>\n<pre><code>$ ls -la ~\/.dvcstorage\/bohr\/27\/9936268f488e1e613f81a537f29055\n-rw-rw-r-- 1 hbabii hbabii 1458311 Jan 22 00:19 \/home\/hbabii\/.dvcstorage\/bohr\/27\/9936268f488e1e613f81a537f29055\n<\/code><\/pre>\n<p>I double-checked that I am pulling from and pushing to the same remote. I am using DVC v1.11.11.<\/p>\n<p>Could you please give me any hints on what could be wrong?<\/p>\n<p>Cheers, Hlib<\/p>",
        "Challenge_closed_time":1616082756536,
        "Challenge_comment_count":4,
        "Challenge_created_time":1611327752360,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65847574",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":5.5,
        "Challenge_reading_time":19.49,
        "Challenge_score_count":3,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1320.8344933333,
        "Challenge_title":"Failed to pull existing files from SSH DVC Remote",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1715,
        "Challenge_word_count":159,
        "Platform":"Stack Overflow",
        "Poster_created_time":1355002392776,
        "Poster_location":"Bolzano, Italia",
        "Poster_reputation_count":530.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p>In the end, the problem was that I indeed was pulling from the wrong remote (I had multiple remotes, their configuration was tricky, and local configurations differed on different machines).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":16.2,
        "Solution_reading_time":2.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":30.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1608747030727,
        "Answerer_location":null,
        "Answerer_reputation_count":105.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":15.1535161111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So I'm a beginner in docker and containers and I've been getting this error for days now.\nI get this error when my lambda function runs a sagemaker processing job.\nMy core python file resides in an s3 bucket.\nMy docker image resides in ECR.\nBut I dont understand why I dont get a similar error when I run the same processing job with a python docker image.\nPFB the python docker file that didnt throw any errors.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>FROM python:latest\n#installing dependencies\nRUN pip3 install argparse\nRUN pip3 install boto3\nRUN pip3 install numpy\nRUN pip3 install scipy\nRUN pip3 install pandas\nRUN pip3 install scikit-learn\nRUN pip3 install matplotlib\n<\/code><\/pre>\n<p>I only get this error when i run this with a an ubunutu docker image with python3 installed.\nPFB the dockerfile which throws the error mentioned.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>FROM ubuntu:20.04\n\nRUN apt-get update -y\nRUN apt-get install -y python3\nRUN apt-get install -y python3-pip\n\nRUN pip3 install argparse\nRUN pip3 install boto3\nRUN pip3 install numpy==1.19.1\nRUN pip3 install scipy\nRUN pip3 install pandas\nRUN pip3 install scikit-learn\n\nENTRYPOINT [ &quot;python3&quot; ]\n<\/code><\/pre>\n<p>How do I fix this?<\/p>",
        "Challenge_closed_time":1622294378528,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622233871527,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1622239825870,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67745141",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.3,
        "Challenge_reading_time":17.23,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":16.8075002778,
        "Challenge_title":"Facing this error : container_linux.go:367: starting container process caused: exec: \"python\": executable file not found in $PATH: unknown",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1123,
        "Challenge_word_count":205,
        "Platform":"Stack Overflow",
        "Poster_created_time":1608747030727,
        "Poster_location":null,
        "Poster_reputation_count":105.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>Fixed this error by changing the entry point to<\/p>\n<p><strong>ENTRYPOINT [ &quot;\/usr\/bin\/python3.8&quot;]<\/strong><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"entri entrypoint usr bin",
        "Solution_preprocessed_content":null,
        "Solution_readability":11.1,
        "Solution_reading_time":1.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1401922736652,
        "Answerer_location":"Brisbane",
        "Answerer_reputation_count":748.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":510.2445766667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I'm looking to make some hyper parameters available to the serving endpoint in SageMaker. The training instances is given access to input parameters using hyperparameters in:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='autocat.py',\n                       role=role,\n                       output_path=params['output_path'],\n                       code_location=params['code_location'],\n                       train_instance_count=1,\n                       train_instance_type='ml.c4.xlarge',\n                       training_steps=10000,\n                       evaluation_steps=None,\n                       hyperparameters=params)\n<\/code><\/pre>\n\n<p>However, when the endpoint is deployed, there is no way to pass in parameters that are used to control the data processing in the <code>input_fn(serialized_input, content_type)<\/code> function.<\/p>\n\n<p>What would be the best way to pass parameters to the serving instance?? Is the <code>source_dir<\/code> parameter defined in the <code>sagemaker.tensorflow.TensorFlow<\/code> class copied to the serving instance? If so, I could use a config.yml or similar.<\/p>",
        "Challenge_closed_time":1523591814356,
        "Challenge_comment_count":0,
        "Challenge_created_time":1521754623920,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1521754933880,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49438903",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":13.4,
        "Challenge_reading_time":13.07,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":510.3306766667,
        "Challenge_title":"How to make parameters available to SageMaker Tensorflow Endpoint",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1426,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443201378360,
        "Poster_location":null,
        "Poster_reputation_count":749.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>Ah i have had a similar problem to you where I needed to download something off S3 to use in the input_fn for inference. In my case it was a dictionary.<\/p>\n\n<p>Three options:<\/p>\n\n<ol>\n<li>use your config.yml approach, and download and import the s3 file from within your entrypoint file before any function declarations. This would make it available to the input_fn <\/li>\n<li>Keep using the hyperparameter approach, download and import the vectorizer in <code>serving_input_fn<\/code> and make it available via a global variable so that <code>input_fn<\/code> has access to it.<\/li>\n<li>Download the file from s3 before training and include it in the source_dir directly.<\/li>\n<\/ol>\n\n<p>Option 3 would only work if you didnt need to make changes to the vectorizer seperately after initial training.<\/p>\n\n<p>Whatever you do, don't download the file directly in input_fn. I made that mistake and the performance is terrible as each invoking of the endpoint would result in the s3 file being downloaded.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"config yml download import file entrypoint file function declar input hyperparamet download import vector serv input global variabl input access download file train sourc dir directli option vector separ initi train download file directli input poor perform",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"download input infer dictionari option config yml download import file entrypoint file function declar input hyperparamet download import vector serv input global variabl input access download file train sourc dir directli option didnt vector seper initi train download file directli input perform terribl invok endpoint file download",
        "Solution_preprocessed_content":"download infer dictionari option download import file entrypoint file function declar hyperparamet download import vector global variabl access download file train directli option didnt vector seper initi train download file directli perform terribl invok endpoint file download",
        "Solution_readability":10.1,
        "Solution_reading_time":12.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":157.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1535382420716,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":1.9537672222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used data wrangler for maybe 3h a week ago, and I open my account today to see that Ive been charged for 6 days worth of data wrangler usage. Basically it was running in the background the whole time. The first 25h were part of free tier then I got charged for the rest of the time. I dont have any endpoints to close so whats the issue? I dont care about the costs, I know I can talk to support to get the charges reversed but they dont seem to know whats going on because they havent helped me at all.<\/p>",
        "Challenge_closed_time":1641216477460,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641209554070,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70565147",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.96,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.9231638889,
        "Challenge_title":"AWS sagemaker datawrangler continues to be used after closing everything",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":36,
        "Challenge_word_count":112,
        "Platform":"Stack Overflow",
        "Poster_created_time":1535382420716,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>After going over <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler-shut-down.html\" rel=\"nofollow noreferrer\">the docs<\/a>, I found that I needed to shut down the wrangler instance under Running Instances and Kernels button.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"shut data wrangler instanc run instanc kernel button",
        "Solution_last_edit_time":1641216587632,
        "Solution_link_count":1.0,
        "Solution_original_content":"doc shut wrangler instanc run instanc kernel button",
        "Solution_preprocessed_content":"doc shut wrangler instanc run instanc kernel button",
        "Solution_readability":15.2,
        "Solution_reading_time":3.33,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1305851487736,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Challenge_adjusted_solved_time":5.7160508333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a couple of projects that are using and updating the same data sources. I recently learned about <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\" rel=\"nofollow noreferrer\">dvc's data registries<\/a>, which sound like a great way of versioning data across these different projects (e.g. scrapers, computational pipelines).<\/p>\n<p>I have put all of the relevant data into <code>data-registry<\/code> and then I imported the relevant files into the scraper project with:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ poetry run dvc import https:\/\/github.com\/username\/data-registry raw\n<\/code><\/pre>\n<p>where <code>raw<\/code> is a directory that stores the scraped data. This seems to have worked properly, but then when I went to build <a href=\"https:\/\/dvc.org\/doc\/start\/data-pipelines\" rel=\"nofollow noreferrer\">a dvc pipeline<\/a> that <em>outputted<\/em> data into a file that was already tracked by dvc, I got an error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc run -n menu_items -d src\/ -o raw\/menu_items\/restaurant.jsonl scrapy crawl restaurant\nERROR: Paths for outs:                                                \n'raw'('raw.dvc')\n'raw\/menu_items\/restaurant.jsonl'('menu_items')\noverlap. To avoid unpredictable behaviour, rerun command with non overlapping outs paths.\n<\/code><\/pre>\n<p>Can someone help me understand what is going on here? <strong>What is the best way to use data registries to share and update data across projects?<\/strong><\/p>\n<p>I would ideally like to update the data-registry with new data from the scraper project and then allow other dependent projects to update their data when they are ready to do so.<\/p>",
        "Challenge_closed_time":1614537291720,
        "Challenge_comment_count":1,
        "Challenge_created_time":1614516713937,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1614699218992,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66409283",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":21.45,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":5.7160508333,
        "Challenge_title":"updating data in dvc registry from other projects",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":388,
        "Challenge_word_count":217,
        "Platform":"Stack Overflow",
        "Poster_created_time":1294268936687,
        "Poster_location":"Chicago, IL",
        "Poster_reputation_count":2893.0,
        "Poster_view_count":168.0,
        "Solution_body":"<p>When you <code>import<\/code> (or <code>add<\/code>) something into your project, a .dvc file is created with that lists that something (in this case the <code>raw\/<\/code> dir) as an &quot;output&quot;.<\/p>\n<p>DVC doesn't allow overlapping outputs among .dvc files or dvc.yaml stages, meaning that your &quot;menu_items&quot; stage shouldn't write to <code>raw\/<\/code> since it's already under the control of <code>raw.dvc<\/code>.<\/p>\n<p>Can you make a separate directory for the pipeline outputs? E.g. use <code>processed\/menu_items\/restaurant.jsonl<\/code><\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":1614698988012,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":9.0,
        "Solution_reading_time":7.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":69.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1336984204220,
        "Answerer_location":"Wien, \u00d6sterreich",
        "Answerer_reputation_count":166.0,
        "Answerer_view_count":31.0,
        "Challenge_adjusted_solved_time":1.9699208333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am using sacred package in python, this allows to keep track of computational experiments i'm running. sacred allows to add observer (<code>mongodb<\/code>) which stores all sorts of information regarding the experiment (<code>configuration<\/code>, <code>source files<\/code> etc).\n<code>sacred<\/code> allows to add artifacts to the db bt using <code>sacred.Experiment.add_artifact(PATH_TO_FILE).<\/code><\/p>\n\n<p>This command essentially adds the file to the DB.<\/p>\n\n<p>I'm using MongoDB compass, I can access the experiment information and see that an artifact has been added. it contains two fields:\n'<code>name<\/code>' and '<code>file_id<\/code>' which contains an <code>ObjectId<\/code>. (see image)<\/p>\n\n<p>I am attempting to access the stored file itself. i have noticed that under my db there is an additional <code>sub-db<\/code> called <code>fs.files<\/code> in it i can filter to find my <code>ObjectId<\/code> but it does not seem to allow me to access to content of the file itself.<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/SBg8m.png\" alt=\"object id under .files\"><\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/B7ymG.png\" alt=\"file_id under artifact\/object\"><\/p>",
        "Challenge_closed_time":1513855746928,
        "Challenge_comment_count":0,
        "Challenge_created_time":1513848655213,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1525602466710,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47921875",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":11.0,
        "Challenge_reading_time":15.38,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1.9699208333,
        "Challenge_title":"Accessing files in Mongodb",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2457,
        "Challenge_word_count":149,
        "Platform":"Stack Overflow",
        "Poster_created_time":1468845236196,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>MongoDB file storage is handled by \"GridFS\" which basically splits up files in chunks and stores them in a collection (fs.files).<\/p>\n\n<p>Tutorial to access: <a href=\"http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html\" rel=\"nofollow noreferrer\">http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"access file store mongodb gridf packag gridf split file chunk store collect call file tutori access file gridf",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"mongodb file storag gridf split file chunk store collect file tutori access http api mongodb com gridf html",
        "Solution_preprocessed_content":"mongodb file storag gridf split file chunk store collect tutori access",
        "Solution_readability":17.5,
        "Solution_reading_time":4.32,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":1331550180963,
        "Answerer_location":"Spain",
        "Answerer_reputation_count":1697.0,
        "Answerer_view_count":97.0,
        "Challenge_adjusted_solved_time":0.7546861111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When creating a notebook instance via GCP console, I can check a box &quot;Turn on Secure Boot&quot;. Is it possible to turn on Secure Boot when creating the notebook using gcloud command?<\/p>",
        "Challenge_closed_time":1664037589950,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664034657440,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1664034873080,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73838593",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":2.95,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.8145861111,
        "Challenge_title":"CGP Vertex AI notebook - turn on Secure Boot",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":13,
        "Challenge_word_count":39,
        "Platform":"Stack Overflow",
        "Poster_created_time":1592856629630,
        "Poster_location":null,
        "Poster_reputation_count":5.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>It's a little involved, but <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/shielded-vm\" rel=\"nofollow noreferrer\">yes you can<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"turn secur boot process creat notebook instanc gcloud process littl involv link document explain",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":17.2,
        "Solution_reading_time":2.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1468179475927,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation_count":795.0,
        "Answerer_view_count":210.0,
        "Challenge_adjusted_solved_time":459.2678666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to build an experiment to create recommendations (using the Movie Ratings sample database), but without using the ratings. I simply consider that if a user has rated certain movies, then he would be interested by other movies that have been rated by users that have also rated his movies.<\/p>\n\n<p>I can consider, for instance, that ratings are 1 (exists in the database) or 0 (does not exist), but in that case, how do I transform the initial data to reflect this?<\/p>\n\n<p>I couldn't find any kind of examples or tutorials about this kind of scenario, and I don't really know how to proceed. Should I transform the data before injecting it into an algorithm? And\/or is there any kind of specific algorithm that I should use?<\/p>",
        "Challenge_closed_time":1471354453808,
        "Challenge_comment_count":4,
        "Challenge_created_time":1469698571557,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1469701089488,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38632533",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":9.5,
        "Challenge_reading_time":9.58,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":459.9672919444,
        "Challenge_title":"Recommendations without ratings (Azure ML)",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":910,
        "Challenge_word_count":132,
        "Platform":"Stack Overflow",
        "Poster_created_time":1461857379436,
        "Poster_location":"Lille, France",
        "Poster_reputation_count":133.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>If you're hoping to use the Matchbox Recommender in AML, you're correct that you need to identify some user-movie pairs that <em>are<\/em> not present in the raw dataset, and add these in with a rating of zero. (I'll assume that you have already set all of the real user-movie pairs to have a rating of one, as you described above.)<\/p>\n\n<p>I would recommend generating some random candidate pairs and confirming their absence from the training data in an Execute R (or Python) Script module. I don't know the names of your dataset's features, but here is some pseudocode in R to do that:<\/p>\n\n<pre><code>library(dplyr)\ndf &lt;- maml.mapInputPort(1)  # input dataset of observed user-movie pairs\nall_movies &lt;- unique(df[['movie']])\nall_users &lt;- unique(df[['user']])\nn &lt;- 30  # number of random pairs to start with\n\nnegative_observations &lt;- data.frame(movie = sample(all_movies, n, replace=TRUE),\n                                    user = sample(all_users, n, replace=TRUE),\n                                    rating = rep(0, n))          \nacceptable_negative_observations &lt;- anti_join(unique(negative_observations), df, by=c('movie', 'user'))\ndf &lt;- rbind(df, acceptable_negative_observations)\nmaml.mapOutputPort(\"df\");\n<\/code><\/pre>\n\n<p>Alternatively, you could try a method like <a href=\"https:\/\/en.wikipedia.org\/wiki\/Association_rule_learning\" rel=\"nofollow\">association rule learning<\/a> which would not require you to add in the fake zero ratings. Martin Machac has posted a <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Frequently-bought-together-market-basket-analyses-using-ARULES-1\" rel=\"nofollow\">nice example<\/a> of how to do this in R\/AML in the Cortana Intelligence Gallery.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"convers involv gener random candid pair absenc train data execut modul involv associ rule fake rate nice aml cortana intellig galleri",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"hope matchbox aml identifi movi pair present raw dataset add rate set movi pair rate gener random candid pair absenc train data execut modul dataset featur pseudocod librari dplyr maml mapinputport input dataset observ movi pair movi uniqu movi uniqu random pair start neg observ data frame movi sampl movi replac sampl replac rate rep accept neg observ anti uniqu neg observ movi rbind accept neg observ maml mapoutputport associ rule add fake rate martin machac nice aml cortana intellig galleri",
        "Solution_preprocessed_content":"hope matchbox aml identifi pair present raw dataset add rate gener random candid pair absenc train data execut modul dataset featur pseudocod associ rule add fake rate martin machac nice cortana intellig galleri",
        "Solution_readability":13.4,
        "Solution_reading_time":21.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":199.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":1.5416555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have around 10000 images in my S3 bucket. I need to cut each of these images to 12 smaller images and save them in another folder in the S3 bucket. I want to do this through the AWS Sagemaker. I am not able to read the image from the S3 bucket from my Sagemaker Jupter notebook. I have the code for cutting the images. <\/p>\n\n<p>Need help in reading images and storing them back into S3 from Sagemaker.Is it possible to do this, and also efficiently?<\/p>",
        "Challenge_closed_time":1559476844132,
        "Challenge_comment_count":2,
        "Challenge_created_time":1559410433010,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1559471294172,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56408976",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":5.6,
        "Challenge_reading_time":6.14,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":18.4475338889,
        "Challenge_title":"How to read AWS S3 images from Sagemaker for processing",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":641,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495175078600,
        "Poster_location":"Coimbatore, Tamil Nadu, India",
        "Poster_reputation_count":126.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>You can bring images to a local repo of your SageMaker instance (eg \/home\/ec2-user\/SageMaker\/Pics\/ with the following command:<\/p>\n\n<pre><code>aws s3 sync s3:\/\/pic_folder_in_s3 \/home\/ec2-user\/SageMaker\/Pics\n<\/code><\/pre>\n\n<p>or in python:<\/p>\n\n<pre><code>import subprocess as sb\n\nsb.call('aws s3 sync s3:\/\/pic_folder_in_s3 \/home\/ec2-user\/SageMaker\/Pics'.split())\n<\/code><\/pre>\n\n<p>Note that in order for the transfer to happen, the role carried by your SageMaker instance must have the right to read from this S3 location<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"imag local repo instanc cli sync subprocess modul role carri instanc read locat",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"imag local repo instanc home pic sync pic folder home pic import subprocess sync pic folder home pic split note order transfer role carri instanc read locat",
        "Solution_preprocessed_content":"imag local repo instanc note order transfer role carri instanc read locat",
        "Solution_readability":13.8,
        "Solution_reading_time":6.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.3237788889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to register a data set via the Azure Machine Learning Studio designer but keep getting an error. Here is my code, used in a &quot;Execute Python Script&quot; module:<\/p>\n<pre><code>import pandas as pd\nfrom azureml.core.dataset import Dataset\nfrom azureml.core import Workspace\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    ws = Workspace.get(name = &lt;my_workspace_name&gt;, subscription_id = &lt;my_id&gt;, resource_group = &lt;my_RG&gt;)\n    ds = Dataset.from_pandas_dataframe(dataframe1)\n    ds.register(workspace = ws,\n                name = &quot;data set name&quot;,\n                description = &quot;example description&quot;,\n                create_new_version = True)\n    return dataframe1, \n<\/code><\/pre>\n<p>But I get the following error in the Workspace.get line:<\/p>\n<pre><code>Authentication Exception: Unknown error occurred during authentication. Error detail: Unexpected polling state code_expired.\n<\/code><\/pre>\n<p>Since I am inside the workspace and in the designer, I do not usually need to do any kind of authentication (or even reference the workspace). Can anybody offer some direction? Thanks!<\/p>",
        "Challenge_closed_time":1628038438487,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628037272883,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1628038626927,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68644137",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":14.73,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.3237788889,
        "Challenge_title":"Azure Machine Learning Studio Designer Error: code_expired",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":321,
        "Challenge_word_count":133,
        "Platform":"Stack Overflow",
        "Poster_created_time":1371499229816,
        "Poster_location":null,
        "Poster_reputation_count":1111.0,
        "Poster_view_count":191.0,
        "Solution_body":"<p>when you're inside a &quot;Execute Python Script&quot; module or <code>PythonScriptStep<\/code>, the authentication for fetching the workspace is already done for you (unless you're trying to authenticate to different Azure ML workspace.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Run\nrun = Run.get_context()\n\nws = run.experiment.workspace\n<\/code><\/pre>\n<p>You should be able to use that <code>ws<\/code> object to register a Dataset.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"execut modul pythonscriptstep authent fetch workspac object regist dataset",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"insid execut modul pythonscriptstep authent fetch workspac authent workspac core import run run run context run workspac object regist dataset",
        "Solution_preprocessed_content":"insid execut modul authent fetch workspac authent workspac object regist dataset",
        "Solution_readability":9.9,
        "Solution_reading_time":6.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1526004205792,
        "Answerer_location":"China",
        "Answerer_reputation_count":28087.0,
        "Answerer_view_count":3298.0,
        "Challenge_adjusted_solved_time":2.4671630556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After having solved <a href=\"https:\/\/stackoverflow.com\/questions\/55347910\/\">Why does my ML model deployment in Azure Container Instance still fail?<\/a> and having deployed on ACI, I am using Azure Machine Learning Service to deploy a ML model as web service on AKS.<\/p>\n\n<p>My current (working) ACI-deployment code is<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice, AciWebservice\nfrom azureml.core.image import ContainerImage\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                      memory_gb=8, \n                      tags={\"data\": \"text\",  \"method\" : \"NB\"}, \n                      description='Predict something')\n\n\nimage_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n                      docker_file=\"Dockerfile\",\n                      runtime=\"python\", \n                      conda_file=\"myenv.yml\")\n\nimage = ContainerImage.create(name = \"scorer-image\",\n                      models = [model],\n                      image_config = image_config,\n                      workspace = ws\n                      )\n\nservice_name = 'scorer-svc'\nservice = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                        image = image,\n                                        name = service_name,\n                                        workspace = ws)\n<\/code><\/pre>\n\n<p>I would like to modify it so to deploy on AKS, but looks more convoluted than I expected, as I imagined moving from ACI to AKS (i.e. from test to production) to be a routine operation. Still, it seems to need a bit more of changes in the code than I thought: <\/p>\n\n<ul>\n<li>AKS seems to require an <code>InferenceConfig<\/code> object (?) <\/li>\n<li>with AKS there's no method like <code>deploy_from_image<\/code> for deployment from my existing Docker <code>image<\/code> (?)<\/li>\n<\/ul>\n\n<p>Can deployment be done on AKS by performing minimal changes to the ACI code instead?  <\/p>",
        "Challenge_closed_time":1557660771803,
        "Challenge_comment_count":2,
        "Challenge_created_time":1557392548383,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1557651890016,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56055868",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":11.5,
        "Challenge_reading_time":21.26,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":74.5065055556,
        "Challenge_title":"What's the easiest way to move from ACI to AKS deployment?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":460,
        "Challenge_word_count":192,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>From the code that you have provided, when you deploy the application in the ACI using the method <code>Webservice.deploy_from_image<\/code> with the parameters <code>deployment_config<\/code> and container image. The deployment_config makes by the <code>AciWebservice.deploy_configuration<\/code>.<\/p>\n\n<p>When you take a look at the ML about AKS, you can also find the method <code>AksWebservice.deploy_configuration<\/code>. So you just need to change the method <code>AciWebservice.deploy_configuration<\/code> into <code>AksWebservice.deploy_configuration<\/code>, then the application can be deployed from ACI into AKS. And it's the minimal changes. Also, it can deploy from the docker image.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"modifi aci deploy deploi ak minim aciwebservic deploi configur akswebservic deploi configur webservic deploi imag paramet deploy config imag deploi docker imag",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"deploi aci webservic deploi imag paramet deploy config imag deploy config aciwebservic deploi configur ak akswebservic deploi configur aciwebservic deploi configur akswebservic deploi configur deploi aci ak minim deploi docker imag",
        "Solution_preprocessed_content":"deploi aci paramet imag ak deploi aci ak minim deploi docker imag",
        "Solution_readability":11.6,
        "Solution_reading_time":9.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1505194585676,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":44.2415530556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to deploy a model to Kubernetes in Azure Machine Learning Studio, it was working for a while, but now, it fails during deployment, the error message is as follows:<\/p>\n<pre><code>Deploy: Failed on step WaitServiceCreating. Details: AzureML service API error. \nYour container application crashed. This may be caused by errors in your scoring file's init() function.\nPlease check the logs for your container instance: pipeline-created-on-07-28-2020-r.\nFrom the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\nYou can also try to run image viennaglobal.azurecr.io\/azureml\/azureml_6ae744633f749472feb283065055dc2c:latest locally.\nPlease refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\n<\/code><\/pre>\n<pre class=\"lang-js prettyprint-override\"><code>{\n    &quot;code&quot;: &quot;KubernetesDeploymentFailed&quot;,\n    &quot;statusCode&quot;: 400,\n    &quot;message&quot;: &quot;Kubernetes Deployment failed&quot;,\n    &quot;details&quot;: [\n        {\n            &quot;code&quot;: &quot;CrashLoopBackOff&quot;,\n            &quot;message&quot;: &quot;Your container application crashed. This may be caused by errors in your scoring file's init() function.\nPlease check the logs for your container instance: pipeline-created-on-07-28-2020-r. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image viennaglobal.azurecr.io\/azureml\/azureml_6ae744633f749472feb283065055dc2c:latest locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.&quot;\n        }\n    ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1596104635683,
        "Challenge_comment_count":2,
        "Challenge_created_time":1595913837627,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1595945366092,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63127521",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":10.5,
        "Challenge_reading_time":21.34,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":52.99946,
        "Challenge_title":"Deploying Model to Kubernetes",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":186,
        "Challenge_word_count":178,
        "Platform":"Stack Overflow",
        "Poster_created_time":1505194585676,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>It seems it was a bug, got corrected by itself today. Closing this question now<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":3.9,
        "Solution_reading_time":1.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":15.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1532464254552,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":14.0691794444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using the AWS SageMaker \"built in\" object detection algorithm (SSD) and we've trained it on a series of annotated 512x512 images (image_shape=512).  We've deployed an endpoint and when using it for prediction we're getting mixed results.  <\/p>\n\n<p>If the image we use for prediciton is around that 512x512 size we're getting great accuracy and good results.  If the image is significantly larger (e.g. 8000x10000) we get either wildly inaccurate, or no results.  If I manually resize those large images to 512x512pixels the features we're looking for are no longer discernable to the eye.  Which suggests that if my endpoint is resizing images, then that would explain why the model is struggling.<\/p>\n\n<p><strong>Note:<\/strong> Although the size in pexels is large, my images are basically line drawings on a white background. They have very little color and large patches of solid white, so they compress very well.  I'm mot running into the 6Mb request size limit.<\/p>\n\n<p>So, my questions are:<\/p>\n\n<ol>\n<li>Does training the model at image_shape=512 mean my prediction images should also be that same size?<\/li>\n<li>Is there a generally accepted method for doing object detection on very large images?  I can envisage how I might chop the image into smaller tiles then feed each tile to my model, but if there's something \"out of the box\" that will do it for me, then that'd save some effort.<\/li>\n<\/ol>",
        "Challenge_closed_time":1547584358316,
        "Challenge_comment_count":1,
        "Challenge_created_time":1547533709270,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54193723",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":18.07,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":14.0691794444,
        "Challenge_title":"Size of image for prediction with SageMaker object detection?",
        "Challenge_topic":"Database Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":771,
        "Challenge_word_count":237,
        "Platform":"Stack Overflow",
        "Poster_created_time":1226984969400,
        "Poster_location":"Adelaide, Australia",
        "Poster_reputation_count":5789.0,
        "Poster_view_count":464.0,
        "Solution_body":"<p>Your understanding is correct. The endpoint resizes images based on the parameter <code>image_shape<\/code>. To answer your questions:<\/p>\n\n<ol>\n<li>As long as the scale of objects (i.e., expansion of pixels) in the resized images are similar between training and prediction data, the trained model should work.<\/li>\n<li>Cropping is one option. Another method is to train separate models for large and small images as David suggested.<\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"train model scale object resiz imag train predict data crop imag smaller tile feed tile model option train separ model larg small imag",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"endpoint resiz imag base paramet imag shape scale object expans pixel resiz imag train predict data train model crop option train separ model larg small imag david",
        "Solution_preprocessed_content":"endpoint resiz imag base paramet scale object resiz imag train predict data train model crop option train separ model larg small imag david",
        "Solution_readability":7.5,
        "Solution_reading_time":5.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":13.3565036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to use this <a href=\"https:\/\/github.com\/amogh147\/binance_takeHome_gemini_amogh\" rel=\"nofollow noreferrer\">repo<\/a> and I have created and activated a virtualenv and installed the required dependencies.<\/p>\n<p>I get an error when I run pytest.<\/p>\n<p>And under the file binance_cdk\/app.py it describes the following tasks:<\/p>\n<h1>App (PSVM method) entry point of the program.<\/h1>\n<h1>Note:<\/h1>\n<p>Steps tp setup CDK:<\/p>\n<ol>\n<li>install npm<\/li>\n<li>cdk -init (creates an empty project)<\/li>\n<li>Add in your infrastructure code.<\/li>\n<li>Run CDK synth<\/li>\n<li>CDK bootstrap &lt;aws_account&gt;\/<\/li>\n<li>Run CDK deploy ---&gt; This creates a cloudformation .yml file and the aws resources will be created as per the mentioned stack.<\/li>\n<\/ol>\n<p>I'm stuck on step 3, what do I add in this infrastructure code, and if I want to use this on amazon sagemaker which I am not familiar with, do I even bother doing this on my local terminal, or do I do the whole process regardless on sagemaker?<\/p>\n<p>Thank you in advance for your time and answers !<\/p>",
        "Challenge_closed_time":1655849768796,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655801685383,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72697889",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":13.87,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":13.3565036111,
        "Challenge_title":"How to deploy AWS using CDK, sagemaker?",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":100,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604312740476,
        "Poster_location":null,
        "Poster_reputation_count":141.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>The infrastructure code is the Python code that you want to write for the resources you want to provision with SageMaker. In the example you provided for example the infra code they have is creating a Lambda function. You can do this locally on your machine, the question is what do you want to achieve with SageMaker? If you want to create an endpoint then following the CDK Python docs with SageMaker to identify the steps for creating an endpoint. Here's two guides, the first is an introduction to the AWS CDK and getting started. The second is an example of using the CDK with SageMaker to create an endpoint for  inference.<\/p>\n<p>CDK Python Starter: <a href=\"https:\/\/towardsdatascience.com\/build-your-first-aws-cdk-project-18b1fee2ed2d\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/build-your-first-aws-cdk-project-18b1fee2ed2d<\/a>\nCDK SageMaker Example: <a href=\"https:\/\/github.com\/philschmid\/cdk-samples\/tree\/master\/sagemaker-serverless-huggingface-endpoint\" rel=\"nofollow noreferrer\">https:\/\/github.com\/philschmid\/cdk-samples\/tree\/master\/sagemaker-serverless-huggingface-endpoint<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"creat infrastructur local cdk doc identifi step creat endpoint guid cdk starter cdk bias summari",
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_original_content":"infrastructur write resourc provis infra creat lambda function local achiev creat endpoint cdk doc identifi step creat endpoint guid introduct cdk start cdk creat endpoint infer cdk starter http towardsdatasci com build cdk bfeeedd cdk http github com philschmid cdk sampl tree master serverless huggingfac endpoint",
        "Solution_preprocessed_content":"infrastructur write resourc provis infra creat lambda function local achiev creat endpoint cdk doc identifi step creat endpoint guid introduct cdk start cdk creat endpoint infer cdk starter cdk",
        "Solution_readability":11.7,
        "Solution_reading_time":14.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":126.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1572812561640,
        "Answerer_location":null,
        "Answerer_reputation_count":3502.0,
        "Answerer_view_count":120.0,
        "Challenge_adjusted_solved_time":0.1742652778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using LightGBM in Azure ML Jupyter notebooks, it works fine and I also installed graphviz.<\/p>\n<p>However this line:<\/p>\n<pre><code>lgb.plot_tree(clf, tree_index = 1, figsize=(20,12))\n<\/code><\/pre>\n<p>throws this error:<\/p>\n<pre><code>ExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n<\/code><\/pre>",
        "Challenge_closed_time":1660922530248,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660921902893,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73418843",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":6.45,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1742652778,
        "Challenge_title":"Azure ML ExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":94,
        "Challenge_word_count":60,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>Common problem (very common).  There are two systems named Graphviz, and you need both!\nsee <a href=\"https:\/\/stackoverflow.com\/questions\/73040021\/im-getting-this-issue-when-trying-to-run-the-code-i-found-on-github-pydot-and\/73041302#73041302\">I&#39;m getting this issue when trying to run the code I found on GitHub. Pydot and graphivz are installed but still getting this error<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"common instal system graphviz",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"common common system graphviz run github pydot graphivz instal",
        "Solution_preprocessed_content":"common system graphviz run github pydot graphivz instal",
        "Solution_readability":8.8,
        "Solution_reading_time":5.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1447151270223,
        "Answerer_location":null,
        "Answerer_reputation_count":141.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":215.0267602778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I created a docker image for training. In the dockerfile I have an entrypoint defined such that when <code>docker run<\/code> is executed, it will start running my python code.\nTo use this on aws sagemaker in my understanding I need to create a pytorch estimator in a jupyter notebook in sagemaker. I tried something like this:<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nsagemaker_session = sagemaker.Session()\n\nrole = sagemaker.get_execution_role()\n\nestimator = PyTorch(entry_point='train.py',\n                    role=role,\n                    framework_version='1.3.1',\n                    image_name='xxx.ecr.eu-west-1.amazonaws.com\/xxx:latest',\n                    train_instance_count=1,\n                    train_instance_type='ml.p3.xlarge',\n                    hyperparameters={})\n\nestimator.fit({})\n\n<\/code><\/pre>\n\n<p>In the documentation I found that as image name I can specify the link the my docker image on aws ecr. When I try to execute this it keeps complaining<\/p>\n\n<pre><code>[Errno 2] No such file or directory: 'train.py'\n<\/code><\/pre>\n\n<p>It complains immidiatly, so surely I am doing something completely wrong. I would expect that first my docker image should run, and than it could find out that the entry point does not exist.<\/p>\n\n<p>But besides this, why do I need to specify an entry point, as in, should it not be clear that the entry to my training is simply <code>docker run<\/code>?<\/p>\n\n<p>For maybe better understanding. The entrypoint python file in my docker image looks like this:<\/p>\n\n<pre><code>if __name__=='__main__':\n    parser = argparse.ArgumentParser()\n\n    # Hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=5)\n    parser.add_argument('--batch_size', type=int, default=16)\n    parser.add_argument('--learning_rate', type=float, default=0.0001)\n\n    # Data and output directories\n    parser.add_argument('--output_data_dir', type=str, default=os.environ['OUTPUT_DATA_DIR'])\n    parser.add_argument('--train_data_path', type=str, default=os.environ['CHANNEL_TRAIN'])\n    parser.add_argument('--valid_data_path', type=str, default=os.environ['CHANNEL_VALID'])\n\n    # Start training\n    ...\n<\/code><\/pre>\n\n<p>Later I would like to specify the hyperparameters and data channels. But for now I simply do not understand what to put as entry point. In the documentation it says that the entrypoint is required and it should be a local\/global path to the entrypoint...<\/p>",
        "Challenge_closed_time":1579270087087,
        "Challenge_comment_count":3,
        "Challenge_created_time":1578494679740,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1578495990750,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59648275",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":10.7,
        "Challenge_reading_time":31.92,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":215.3909297222,
        "Challenge_title":"What to define as entrypoint when initializing a pytorch estimator with a custom docker image for training on AWS Sagemaker?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":768,
        "Challenge_word_count":300,
        "Platform":"Stack Overflow",
        "Poster_created_time":1447151270223,
        "Poster_location":null,
        "Poster_reputation_count":141.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>If you really would like to use a complete separate by yourself build docker image, you should create an Amazon Sagemaker algorithm (which is one of the options in the Sagemaker menu). Here you have to specify a link to your docker image on amazon ECR as well as the input parameters and data channels etc. When choosing this options, you should <strong>not<\/strong> use the PyTorch estimater but the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/algorithm.html\" rel=\"nofollow noreferrer\">Algoritm estimater<\/a>. This way you indeed don't have to specify an entrypoint because it simple runs the docker when training and the default entrypoint can be defined in your docker file.<\/p>\n\n<p>The Pytorch estimator can be used when having you own model code, but you would like to run this code in an off-the-shelf Sagemaker PyTorch docker image. This is why you have to for example specify the PyTorch framework version. In this case the entrypoint file by default should be placed next to where your jupyter notebook is stored (just upload the file by clicking on the upload button). The PyTorch estimator inherits all options from the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html#sagemaker.estimator.Framework\" rel=\"nofollow noreferrer\">framework estimator<\/a> where options can be found where to place the entrypoint and model, for example <em>source_dir<\/em>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"creat algorithm specifi link docker imag ecr pytorch estim algorithm estim specifi entri run docker train default entri defin docker file pytorch estim upload entri file jupyt notebook store specifi entri locat option inherit framework estim",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"complet separ build docker imag creat algorithm option menu specifi link docker imag ecr input paramet data channel choos option pytorch estimat algoritm estimat specifi entrypoint run docker train default entrypoint defin docker file pytorch estim model run shelf pytorch docker imag specifi pytorch framework version entrypoint file default jupyt notebook store upload file click upload button pytorch estim inherit option framework estim option entrypoint model sourc dir",
        "Solution_preprocessed_content":"complet separ build docker imag creat algorithm specifi link docker imag ecr input paramet data channel choos option pytorch estimat algoritm estimat specifi entrypoint run docker train default entrypoint defin docker file pytorch estim model run pytorch docker imag specifi pytorch framework version entrypoint file default jupyt notebook store pytorch estim inherit option framework estim option entrypoint model",
        "Solution_readability":12.3,
        "Solution_reading_time":17.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":200.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1653511725307,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":26.0,
        "Challenge_adjusted_solved_time":165.2682675,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I created a sagemaker project with a terraform template which successfully created with a stack successfully created and associated with it. However, there is no repository associated or pipeline associated with the sagemaker project despite there being both in the cloudformation template I used. Can someone help with this?<\/p>\n<p>Is there a way to manually link a sagemaker project with a code commit repository? I see that succesfully linked repositories have the tag: <code>sagemaker:project-name<\/code> with the correct project name.<\/p>",
        "Challenge_closed_time":1657906025030,
        "Challenge_comment_count":2,
        "Challenge_created_time":1657295938437,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1657311059267,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72914046",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":11.5,
        "Challenge_reading_time":8.02,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":169.4684980556,
        "Challenge_title":"Sagemaker Project successfully creates but there are no linked pipelines or repositories",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":124,
        "Challenge_word_count":90,
        "Platform":"Stack Overflow",
        "Poster_created_time":1653511725307,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>Using a different cloudformation template fixed the issue. Not sure why.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":6.6,
        "Solution_reading_time":1.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1575516017430,
        "Answerer_location":"Mexico City, CDMX, M\u00e9xico",
        "Answerer_reputation_count":4882.0,
        "Answerer_view_count":260.0,
        "Challenge_adjusted_solved_time":48.8901825,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am very new to AWS and the cloud environment. I am a machine learning engineer, I am planning to build a custom CNN into the AWS environment to predict a given image has an iPhone present or not.<\/p>\n<p><strong>What I have done:<\/strong><\/p>\n<p><em><strong>Step 1:<\/strong><\/em><\/p>\n<p>I have created a S3 bucket for iPhone classifier with the below folder structure :<\/p>\n<pre><code> Iphone_Classifier &gt; Train &gt; Yes_iphone_images &gt; 1000 images\n                           &gt; No_iphone_images  &gt; 1000 images\n\n                   &gt; Dev   &gt; Yes_iphone_images &gt; 100 images\n                           &gt; No_iphone_images  &gt; 100 images\n\n                   &gt; Test  &gt; 30 random images\n<\/code><\/pre>\n<p>Permission - &gt; <strong>Block all public access<\/strong><\/p>\n<p><em><strong>Step 2:<\/strong><\/em><\/p>\n<p>Then I go to Amazon Sagemaker, and create an instance:<\/p>\n<p>I select the following<\/p>\n<pre><code> Name: some-xyz,\n Type: ml.t2.medium\n IAM : created new IAM role ( root access was enabled.)\n others: All others were in default\n<\/code><\/pre>\n<p>Then the notebook instance was created and opened.<\/p>\n<p><em><strong>Step 3:<\/strong><\/em><\/p>\n<p>Once I had the instance opened,<\/p>\n<pre><code>1. I used to prefer - conda_tensorflow2_p36 as interpreter\n2. Created a new Jupyter notebook and stated.\n3. I checked image classification examples but was confused, and most others used CSV files, but I want to retrieve images from S3 buckets. \n<\/code><\/pre>\n<p><em><strong>Question:<\/strong><\/em><\/p>\n<pre><code>1. How simply can we access the S3 bucket image dataset from the Jupiter Instances of Sagemaker? \n2. I exactly need the reference code to access the S3 bucket images. \n3. Is it a good approach to copy the data to the notebook or is it better to work from the S3 bucket.\n<\/code><\/pre>\n<p><em><strong>What I have tried was:<\/strong><\/em><\/p>\n<pre><code>import boto3\nclient = boto3.client('s3')\n\n# I tried this one and failed\n#path = 's3:\/\/iphone\/Train\/Yes_iphone_images\/100.png'\n\n# I tried this one and failed\npath = 's3:\/\/iphone\/Test\/10.png'\n\n# I uploaded to the notebook instance an image file and when I try to read it works\n#path = 'thiyaga.jpg'\nprint(path)\n\nimport cv2\nfrom matplotlib import pyplot as plt\nprint(cv2.__version__)\nplt.imshow(img)\n<\/code><\/pre>",
        "Challenge_closed_time":1597165677847,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596987527557,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1596989673190,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63328246",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":8.6,
        "Challenge_reading_time":28.51,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":49.4861916667,
        "Challenge_title":"How to read bucket image from AWS S3 into Sagemaker Jupyter Instance",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2065,
        "Challenge_word_count":315,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324277062387,
        "Poster_location":"Finland",
        "Poster_reputation_count":677.0,
        "Poster_view_count":161.0,
        "Solution_body":"<p>If your image is binary-encoded, you could try this:<\/p>\n<pre><code>import boto3 \nimport matplotlib.pyplot as plt \n\n# Define Bucket and Key \ns3_bucket, s3_key = 'YOUR_BUCKET', 'YOUR_IMAGE_KEY'\n\nwith BytesIO() as f:\n    boto3.client(&quot;s3&quot;).download_fileobj(Bucket=s3_bucket, Key=s3_key, Fileobj=f)\n    f.seek(0)\n    img = plt.imread(f, format='png')\n<\/code><\/pre>\n<p>in other case, the following code works out (based on the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/1.9.42\/guide\/s3-example-download-file.html\" rel=\"nofollow noreferrer\">documentation<\/a>):<\/p>\n<pre><code>s3 = boto3.resource('s3')\n\nimg = s3.Bucket(s3_bucket).download_file(s3_key, 'local_image.jpg')\n<\/code><\/pre>\n<p>In both cases, you can visualize the image with <code>plt.imshow(img)<\/code>.<\/p>\n<p>In your path example <code>path = 's3:\/\/iphone\/Test\/10.png'<\/code>, the bucket and key will be <code>s3_bucket = 'iphone'<\/code> and <code>s3_key=Test\/10.png<\/code><\/p>\n<p>Additional Resources: <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/guide\/s3-example-download-file.html\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/guide\/s3-example-download-file.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"access bucket imag dataset jupit instanc copi data notebook addit resourc access bucket boto bias summari",
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"imag binari encod import boto import matplotlib pyplot plt defin bucket kei bucket kei bucket imag kei bytesio boto client download fileobj bucket bucket kei kei fileobj img plt imread format png base document boto resourc img bucket bucket download file kei local imag jpg visual imag plt imshow img path path iphon test png bucket kei bucket iphon kei test png addit resourc http boto amazonaw com document api latest guid download file html",
        "Solution_preprocessed_content":"imag visual imag path bucket kei addit resourc",
        "Solution_readability":17.9,
        "Solution_reading_time":16.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1590520808976,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":1231.9309869445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using SageMaker python SDK I've created an hyper-param tuning job, which runs many jobs in parallel to search for the optimal HP values.<\/p>\n<p>The jobs complete and I get the best training job name as a string &quot;Job...&quot;.\nI've found the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTrainingJob.html\" rel=\"nofollow noreferrer\">following article<\/a> about how to describe a job using the AWS-CLI or http request.<\/p>\n<p>Is there a way of doing it using the python SageMaker SDK, in order to avoid the complexity of an authenticated request to AWS?<\/p>",
        "Challenge_closed_time":1608656242863,
        "Challenge_comment_count":0,
        "Challenge_created_time":1604221291310,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64630198",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":8.37,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1231.9309869445,
        "Challenge_title":"AWS SageMaker, describe a specific training job using python SDK",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":686,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324808381143,
        "Poster_location":null,
        "Poster_reputation_count":9050.0,
        "Poster_view_count":1750.0,
        "Solution_body":"<p>With a <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py#L70\" rel=\"nofollow noreferrer\"><code>sagemaker.session.Session<\/code><\/a> instance, you can <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py#L1519\" rel=\"nofollow noreferrer\">describe training jobs<\/a>:<\/p>\n<pre><code>import sagemaker\n\n\nsagemaker_session = sagemaker.session.Session()\nsagemaker_session.describe_training_job(&quot;Job...&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"session session instanc train job sdk complex authent request train job achiev",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"session session instanc train job import session session session session train job job",
        "Solution_preprocessed_content":null,
        "Solution_readability":41.9,
        "Solution_reading_time":6.98,
        "Solution_score_count":4.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1459917054448,
        "Answerer_location":"Frankfurt, Germany",
        "Answerer_reputation_count":9168.0,
        "Answerer_view_count":675.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a time series predictor with Keras and  Dockerized the model with with Flash and Gunicorn as per AWS docs. I am loading the serialized model with this code.<\/p>\n\n<pre><code>@classmethod\ndef get_model(cls):\n    if cls.model == None:\n        cls.model = load_model('\/opt\/ml\/bitcoin_model.h5')\n    return cls.model\n<\/code><\/pre>\n\n<p>Then I used the predict method to produce the results , the dockerized container is working perfectly in the local environment , but when I try to host the model in sagemaker it produces this error.<\/p>\n\n<pre><code>ValueError: Tensor Tensor(\"dense_1\/BiasAdd:0\", shape=(?, 1), dtype=float32) is not an element of this graph.\n<\/code><\/pre>\n\n<p>So how can I resolve this issue ?<\/p>",
        "Challenge_closed_time":1541480239328,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541480239330,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1541480860787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53165953",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":10.3,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"ValueError: Tensor is not an element of this graph, when hosting a model in Sagemaker with Gunicorn and Flask and Keras",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":223,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459917054448,
        "Poster_location":"Frankfurt, Germany",
        "Poster_reputation_count":9168.0,
        "Poster_view_count":675.0,
        "Solution_body":"<p>The issue was resolved by calling _make_predict_function() method in the model load phase.<\/p>\n\n<pre><code>@classmethod\ndef get_model(cls):\n    if cls.model == None:\n        cls.model = load_model('\/opt\/ml\/bitcoin_model.h5')\n        cls.model._make_predict_function()\n    return cls.model\n<\/code><\/pre>\n\n<p>Bug Reference : <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/6462\" rel=\"nofollow noreferrer\">https:\/\/github.com\/keras-team\/keras\/issues\/6462<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"call predict function model load phase implement",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"call predict function model load phase classmethod model cl cl model cl model load model opt bitcoin model cl model predict function return cl model http github com kera team kera",
        "Solution_preprocessed_content":"call model load phase",
        "Solution_readability":18.3,
        "Solution_reading_time":5.96,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1544390307847,
        "Answerer_location":"Palo Alto, CA, USA",
        "Answerer_reputation_count":151.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":270.3311944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've seen examples of labeling data using SageMaker Ground Truth and then using that data to train off-the-shelf SageMaker models. However, am I able to use this same annotation format with TensorFlow Script Mode? <\/p>\n\n<p>More specifically, I have a tensorflow.keras model I'm training using TF Script Mode, and I'd like to take data labeled with Ground Truth and convert my script from File mode to Pipe mode.<\/p>",
        "Challenge_closed_time":1549915976767,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548942784467,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54462105",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":5.66,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":270.3311944445,
        "Challenge_title":"SageMaker Ground Truth with TensorFlow",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":454,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>I am from Amazon SageMaker Ground Truth team and happy to assist you in your experiment. Just to be clear our understanding, are you running TF model in SageMaker using TF estimator in your own container (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst<\/a>)? <\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"ground team happi assist clear run model estim http github com sdk blob master src tensorflow readm rst",
        "Solution_preprocessed_content":"ground team happi assist clear run model estim",
        "Solution_readability":19.3,
        "Solution_reading_time":5.85,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":1.2687819444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose I have a server where many users run different experiments, possibly with different Trains Servers.<\/p>\n<p>I know about the <code>TRAINS_CONFIG_FILE<\/code> environment variable, but I wonder if this can be made more flexible in one of the following ways:<\/p>\n<ol>\n<li>Specifying the Trains config file dynamically, i.e. during runtime of the training script?<\/li>\n<li>Storing a config file in each of the training repos and specifying its path relatively to the running script path (instead of relatively to <code>~\/<\/code>)?<\/li>\n<\/ol>",
        "Challenge_closed_time":1592997291032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592992723417,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1609531417760,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62552414",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":7.99,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.2687819444,
        "Challenge_title":"Can Trains config file be specified dynamically or relative to the running script path?",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":129,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>Disclaimer: I'm a member of Allegro Trains team<\/p>\n<ol>\n<li>Loading of the configuration is done at import time. This means that if you set the os environment before importing the package, you should be fine:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>os.environ['TRAINS_CONFIG_FILE']='~\/repo\/trains.conf'\nfrom trains import Task\n<\/code><\/pre>\n<ol start=\"2\">\n<li>The configuration file is loaded based on the current working directory, this means that if you have <code>os.environ['TRAINS_CONFIG_FILE']='trains.conf'<\/code> the trains.conf file will be loaded from the running directory at the time the import happens (which usually is the folder where your script is executed from). This means you can have it as part of the repository, and always set the <code>TRAINS_CONFIG_FILE<\/code> to point to it.<\/li>\n<\/ol>\n<p>A few notes:<\/p>\n<ul>\n<li>What is the use case for different configuration files ?<\/li>\n<li>Notice that when running with <a href=\"https:\/\/github.com\/allegroai\/trains-agent\" rel=\"nofollow noreferrer\">trains-agent<\/a> , this method will override the configuration that the <em>trains-agent<\/em> passes to the code.<\/li>\n<\/ul>",
        "Solution_comment_count":7.0,
        "Solution_gpt_summary":"set environ import packag load configur dynam runtim store configur file train repo specifi path rel run path bias summari",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"disclaim member allegro train team load configur import time set environ import packag environ train config file repo train conf train import task configur file load base directori environ train config file train conf train conf file load run directori time import folder execut repositori set train config file note configur file notic run train agent overrid configur train agent pass",
        "Solution_preprocessed_content":"disclaim member allegro train team load configur import time set environ import packag configur file load base directori file load run directori time import repositori set note configur file notic run overrid configur pass",
        "Solution_readability":12.1,
        "Solution_reading_time":14.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":149.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1403392071732,
        "Answerer_location":null,
        "Answerer_reputation_count":91.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":21.2274555556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>This command:<\/p>\n\n<pre><code>BUCKET_TO_READ='my-bucket'\nFILE_TO_READ='myFile'\ndata_location = 's3:\/\/{}\/{}'.format(BUCKET_TO_READ, FILE_TO_READ)\ndf=pd.read_csv(data_location)\n<\/code><\/pre>\n\n<p>is failing with a <\/p>\n\n<pre><code>ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n\n<p>Error and I'm unable to figure out why.  That should work according to <a href=\"https:\/\/stackoverflow.com\/a\/50244897\/3763782\">https:\/\/stackoverflow.com\/a\/50244897\/3763782<\/a> <\/p>\n\n<p>Here are my permissions on the bucket:<\/p>\n\n<pre><code>            \"Action\": [\n                \"s3:ListMultipartUploadParts\",\n                \"s3:ListBucket\",\n                \"s3:GetObjectVersionTorrent\",\n                \"s3:GetObjectVersionTagging\",\n                \"s3:GetObjectVersionAcl\",\n                \"s3:GetObjectVersion\",\n                \"s3:GetObjectTorrent\",\n                \"s3:GetObjectTagging\",\n                \"s3:GetObjectAcl\",\n                \"s3:GetObject\"\n<\/code><\/pre>\n\n<p>And these commands work as expected: <\/p>\n\n<pre><code>role = get_execution_role()\nregion = boto3.Session().region_name\nprint(role)\nprint(region)\n\ns3 = boto3.resource('s3')\nbucket = s3.Bucket(BUCKET_TO_READ)\nprint(bucket.creation_date)\n\nfor my_bucket_object in bucket.objects.all():\n    print(my_bucket_object)\n    FILE_TO_READ = my_bucket_object.key\n    break\n\nobj = s3.Object(BUCKET_TO_READ, FILE_TO_READ)\nprint(obj)\n\n<\/code><\/pre>\n\n<p>All of those print statements worked just fine.  <\/p>\n\n<p>I'm not sure if it matters, but each file is within a folder, so my FILE_TO_READ looks like <code>folder\/file<\/code>.<\/p>\n\n<p>This command which should download the file to sagemaker also falied with a 403:<\/p>\n\n<pre><code>import boto3\ns3 = boto3.resource('s3')\ns3.Object(BUCKET_TO_READ, FILE_TO_READ).download_file(FILE_TO_READ)\n<\/code><\/pre>\n\n<p>This is also happening when I open a terminal and use <\/p>\n\n<pre><code>aws s3 cp AWSURI local_file_name\n<\/code><\/pre>",
        "Challenge_closed_time":1582382844412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582298932137,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1582306425572,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60341782",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":15.0,
        "Challenge_reading_time":24.84,
        "Challenge_score_count":4,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":23.3089652778,
        "Challenge_title":"Reading a file from s3 to sagemaker on AWS gives 403 forbidden error, but other operations work on the file",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2322,
        "Challenge_word_count":179,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403392071732,
        "Poster_location":null,
        "Poster_reputation_count":91.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>The reason was that we granted permission to the bucket not the objects.  That would be granting <code>\"Resource\": \"arn:aws:s3:::bucket-name\/\"<\/code> but not <code>\"Resource\": \"arn:aws:s3:::bucket-name\/*\"<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"grant permiss object bucket resourc arn bucket permiss resourc arn bucket",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"reason grant permiss bucket object grant resourc arn bucket resourc arn bucket",
        "Solution_preprocessed_content":"reason grant permiss bucket object grant",
        "Solution_readability":9.0,
        "Solution_reading_time":2.86,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":23.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1601729162436,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":887.0,
        "Answerer_view_count":130.0,
        "Challenge_adjusted_solved_time":406.6022325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>To submit a parameter in an az ml cli <code>run submit-pipeline<\/code> command we use the syntax:<\/p>\n<pre><code>az ml run submit-pipeline \u2013datapaths [DataPATHS Name=datastore\/datapath] --experiment-name [Experiment_Name] --parameters [String_parameters Name=Value] --pipeline-id [ID]--resource-group [RGP] --subscription-id [SUB_ID] --workspace-name [AML_WS_NAME]\n<\/code><\/pre>\n<p>This will submit Datapaths and some string parameters with the pipeline. How do we submit Dataset references using az ml cli <code>run submit-pipeline<\/code> command?<\/p>\n<p>For example, the Documentation Notebook: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb\" rel=\"nofollow noreferrer\">aml-pipelines-showcasing-dataset-and-pipelineparameter<\/a><\/p>\n<p>To submit a Dataset Class reference we do:<\/p>\n<pre><code>iris_tabular_ds = Dataset.Tabular.from_delimited_files('link\/iris.csv')\npipeline_run_with_params = experiment.submit(pipeline, pipeline_parameters={'tabular_ds_param': iris_tabular_ds})\n<\/code><\/pre>\n<p>Using REST Call the syntax is:<\/p>\n<pre><code>response = requests.post(rest_endpoint, \n                         headers=aad_token, \n                         json={&quot;ExperimentName&quot;: &quot;MyRestPipeline&quot;,\n                               &quot;RunSource&quot;: &quot;SDK&quot;,\n                               &quot;DataSetDefinitionValueAssignments&quot;: { &quot;tabular_ds_param&quot;: {&quot;SavedDataSetReference&quot;: {&quot;Id&quot;: iris_tabular_ds.id}}}\n                              }\n                        )\n<\/code><\/pre>\n<p>What is the syntax to achieve this using <code>az ml cli<\/code>?<\/p>",
        "Challenge_closed_time":1617345815328,
        "Challenge_comment_count":1,
        "Challenge_created_time":1616413555030,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66745404",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":20.6,
        "Challenge_reading_time":22.81,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":258.9611938889,
        "Challenge_title":"How to submit Dataset Input as a Parameter to AZ ML CLI run submit-pipeline command?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":268,
        "Challenge_word_count":128,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>To consume this from the AZ ML CLI we use the following syntax:<\/p>\n<pre><code>    curl -X POST [Pipeline_REST_Endpoint] -H &quot;Authorization: Bearer $(az account get-access-token --query accessToken -o tsv)&quot; -H &quot;Content-Type: application\/json&quot; --data-binary @- &lt;&lt;DATA\n{&quot;ExperimentName&quot;: &quot;[ExperimentName]&quot;,\n                               &quot;RunSource&quot;: &quot;SDK&quot;,\n                               &quot;DataSetDefinitionValueAssignments&quot;: {&quot;tabular_ds_param&quot;: \n                                                                     {&quot;SavedDataSetReference&quot;: \n                                                                      {&quot;Id&quot;:&quot;[Dataset_ID]&quot;}\n                                                                     }\n                                                                    }\n                              }\nDATA\n<\/code><\/pre>\n<p>We use the simple REST call because <code>az ml run submit-pipeline<\/code> does not have the dataset parameter and datapath does not achieve the desired result.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"submit dataset cli run submit pipelin rest",
        "Solution_last_edit_time":1617877323067,
        "Solution_link_count":0.0,
        "Solution_original_content":"consum cli syntax curl pipelin rest endpoint author bearer account access token queri accesstoken tsv type json data binari data experimentnam experimentnam runsourc sdk datasetdefinitionvalueassign tabular param saveddatasetrefer dataset data rest run submit pipelin dataset paramet datapath achiev desir",
        "Solution_preprocessed_content":"consum cli syntax rest dataset paramet datapath achiev desir",
        "Solution_readability":36.9,
        "Solution_reading_time":9.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":68.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554060427012,
        "Answerer_location":null,
        "Answerer_reputation_count":2433.0,
        "Answerer_view_count":228.0,
        "Challenge_adjusted_solved_time":68.9593552778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been using a jupyter notebook instance to spin up a training job (on separate instance) and deploy the endpoint (on another instance). I am using sagemaker tensorflow APIs for this as shown below:<\/p>\n<pre><code># create Tensorflow object and provide and entry point script\ntf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',\n                      train_instance_count=1, train_instance_type='ml.p2.xlarge',\n                      framework_version='1.12', py_version='py3')\n\n# train model on data on s3 and save model artifacts to s3\ntf_estimator.fit('s3:\/\/bucket\/path\/to\/training\/data')\n\n# deploy model on another instance using checkpoints saved on S3\npredictor = estimator.deploy(initial_instance_count=1,\n                         instance_type='ml.c5.xlarge',\n                         endpoint_type='tensorflow-serving')\n<\/code><\/pre>\n<p>I have been doing all of these steps through a jupyter notebook instance. What AWS services I can use to get rid off the dependency of jupyter notebook instance and automate these tasks of training and deploying the model in serverless fashion?<\/p>",
        "Challenge_closed_time":1595624761856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595377179513,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63024900",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":14.66,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":68.7728730556,
        "Challenge_title":"How to train and deploy model in script mode on Sagemaker without using jupyter notebook instance (serverless)?",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":758,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378268842847,
        "Poster_location":"Pune, India",
        "Poster_reputation_count":4616.0,
        "Poster_view_count":592.0,
        "Solution_body":"<p>I recommend <code>AWS Step Functions<\/code>.  Been using it to schedule <code>SageMaker Batch Transform<\/code> and preprocessing jobs since it integrates with <code>CloudWatch<\/code> event rules.  It can also train models, perform hpo tuning, and integrates with <code>lambda<\/code>.  There is a SageMaker\/Step Functions SDK as well as you can use Step Functions directly by creating state machines. Some examples and documentation:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/11\/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/11\/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker\/<\/a><\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-sagemaker.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-sagemaker.html<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"step function autom task train deploi model tensorflow api step function schedul batch transform preprocess job train model perform hpo tune integr lambda step function sdk step function directli creat state document link share",
        "Solution_last_edit_time":1595625433192,
        "Solution_link_count":4.0,
        "Solution_original_content":"step function schedul batch transform preprocess job integr cloudwatch event rule train model perform hpo tune integr lambda step function sdk step function directli creat state document http com what introduc step function data scienc sdk http doc com step function latest connect html",
        "Solution_preprocessed_content":"schedul preprocess job integr event rule train model perform hpo tune integr step function sdk step function directli creat state document",
        "Solution_readability":21.9,
        "Solution_reading_time":12.5,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":17.6780297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am roughly following this script <a href=\"https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/blob\/master\/keras\/05-keras-blog-post\/Fashion%20MNIST-SageMaker.ipynb\" rel=\"nofollow noreferrer\">fashion-MNIST-sagemaker<\/a>.<\/p>\n\n<p>I see that in the notebook <\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='mnist_keras_tf.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='local',\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1}\n                         )\n<\/code><\/pre>\n\n<p>I am wondering to what extent I can and should use the <code>train_instance_count<\/code> parameter. Will it distribute training along some dimension automatically, if yes - what is the dimension?<\/p>\n\n<p>Further, does it generally make sense to distribute training horizontally in a keras (with tensorflow) based setting?<\/p>",
        "Challenge_closed_time":1577100115740,
        "Challenge_comment_count":0,
        "Challenge_created_time":1577037857400,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59446807",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":17.5,
        "Challenge_reading_time":12.32,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":17.2939833333,
        "Challenge_title":"sagemaker horizontally scaling tensorflow (keras) model",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":295,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456487654208,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":1464.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>distributed training is model and framework specific. Not all models are easy to distribute, and from ML framework to ML framework things are not equally easy. <strong>It is rarely automatic, even less so with TensorFlow and Keras.<\/strong><\/p>\n\n<p>Neural nets are conceptually easy to distribute under the data-parallel paradigm, whereby the gradient computation of a given mini-batch is split among workers, which could be multiple devices in the same host (multi-device) or multiple hosts with each multiple devices (multi-device multi-host). The D2L.ai course provides an in-depth view of how neural nets are distributed <a href=\"http:\/\/d2l.ai\/chapter_computational-performance\/multiple-gpus.html\" rel=\"nofollow noreferrer\">here<\/a> and <a href=\"http:\/\/d2l.ai\/chapter_computational-performance\/parameterserver.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Keras used to be trivial to distribute in multi-device, single host fashion with the <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/multi_gpu_model\" rel=\"nofollow noreferrer\"><code>multi_gpu_model<\/code>, which will sadly get deprecated in 4 months<\/a>. In your case, you seem to refer to multi-host model (more than one machine), and that requires writing ad-hoc synchronization code such as the one seen in <a href=\"https:\/\/www.tensorflow.org\/tutorials\/distribute\/multi_worker_with_keras\" rel=\"nofollow noreferrer\">this official tutorial<\/a>. <\/p>\n\n<p>Now let's look at how does this relate to SageMaker.<\/p>\n\n<p>SageMaker comes with 3 options for algorithm development. Using distributed training may require a varying amount of custom work depending on the option you choose:<\/p>\n\n<ol>\n<li><p>The <strong>built-in algorithms<\/strong> is a library of 18 pre-written algorithms. Many of them are written to be distributed in single-host multi-GPU or multi-GPU multi-host. With that first option, you don't have anything to do apart from setting <code>train_instance_count<\/code> > 1 to distribute over multiple instances<\/p><\/li>\n<li><p>The <strong>Framework containers<\/strong> (the option you are using) are containers developed for popular frameworks (TensorFlow, PyTorch, Sklearn, MXNet) and provide pre-written docker environment in which you can write arbitrary code. In this options, some container will support one-click creation of ephemeral training clusters to do distributed training, <strong>however using <code>train_instance_count<\/code> greater than one is not enough to distribute the training of your model. It will just run your script on multiple machines. In order to distribute your training, you must write appropriate distribution and synchronization code in your <code>mnist_keras_tf.py<\/code> script.<\/strong> For some frameworks such code modification will be very simple, for example for TensorFlow and Keras, SageMaker comes with Horovod pre-installed. Horovod is a peer-to-peer ring-style communication mechanism that requires very little code modification and is highly scalable (<a href=\"https:\/\/eng.uber.com\/horovod\/\" rel=\"nofollow noreferrer\">initial annoucement from Uber<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html#training-with-horovod\" rel=\"nofollow noreferrer\">SageMaker doc<\/a>, <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb\" rel=\"nofollow noreferrer\">SageMaker example<\/a>, <a href=\"https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/launching-tensorflow-distributed-training-easily-with-horovod-or-parameter-servers-in-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">SageMaker blog post<\/a>). My recommendation would be to try using Horovod to distribute your code. Similarly, in Apache MXNet you can easily create Parameter Stores to host model parameters in a distributed fashion and sync with them from multiple nodes. MXNet scalability and ease of distribution is one of the reason Amazon loves it.<\/p><\/li>\n<li><p>The <strong>Bring-Your-Own Container<\/strong> requires you to write both docker container and algorithm code. In this situation, you can of course distribute your training over multiple machines but you also have to write machine-to-machine communication code<\/p><\/li>\n<\/ol>\n\n<p>For your specific situation my recommendation would be to scale horizontally first in a single node with multiple GPUs over bigger and bigger machine types, because latency and complexity increase drastically as you switch from single-host to multi-host context. If truly necessary, use multi-node context and things maybe easier if that's done with Horovod.\nIn any case, things are still much easier to do with SageMaker since it manages creation of ephemeral, billed-per-second clusters with built-in, logging and metadata and artifacts persistence and also handles fast training data loading from s3, sharded over training nodes. <\/p>\n\n<p><strong>Note on the relevancy of distributed training<\/strong>: Keep in mind that when you distribute over N devices a model that was running fine on one device, you usually grow the batch size by N so that the per-device batch size stays constant and each device keeps being busy. This will disturb your model convergence, because bigger batches means a less noisy SGD. A common heuristic is to grow the learning rate by N (more info in <a href=\"https:\/\/arxiv.org\/abs\/1706.02677\" rel=\"nofollow noreferrer\">this great paper from Priya Goyal et al<\/a>), but this on the other hand induces instability at the first couple epochs, so it is sometimes associated with a learning rate warmup. Scaling SGD to work well with very large batches is still an active research problem, with new ideas coming up frequently. Reaching good model performance with very large batches sometimes require ad-hoc research and a fair amount of parameter tuning, occasionally to the extent where the extra money spent on finding how to distribute well overcome the benefits of the faster training you eventually manage to run. A situation where distributed training makes sense is when an individual record represent too much compute to form a big enough physical batch on a device, a situation seen on big input sizes (eg vision over HD pictures) or big parameter counts (eg BERT). That being said, for those models requiring very big logical batch you don't necessarily have to distribute things physically: you can run sequentially N batches through your single GPU and wait N per-device batches before doing the gradient averaging and parameter update to simulate having an N times bigger GPU. (a clever hack sometimes called <em>gradient accumulation<\/em>)<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"built algorithm set train instanc count paramet valu greater distribut multipl instanc framework train instanc count greater distribut train model distribut synchron written horovod tensorflow kera distribut commun written scale horizont singl node multipl gpu bigger bigger type switch multi host context",
        "Solution_last_edit_time":1577101498307,
        "Solution_link_count":9.0,
        "Solution_original_content":"distribut train model framework model distribut framework framework equal rare automat tensorflow kera neural net conceptu distribut data parallel paradigm gradient comput mini batch split worker multipl devic host multi devic multipl host multipl devic multi devic multi host cours depth neural net distribut kera trivial distribut multi devic singl host fashion multi gpu model sadli deprec month multi host model write hoc synchron offici tutori relat come option algorithm distribut train vari depend option choos built algorithm librari pre written algorithm written distribut singl host multi gpu multi gpu multi host option apart set train instanc count distribut multipl instanc framework option popular framework tensorflow pytorch sklearn mxnet pre written docker environ write arbitrari option click creation ephemer train cluster distribut train train instanc count greater distribut train model run multipl order distribut train write distribut synchron mnist kera framework modif tensorflow kera come horovod pre instal horovod peer peer ring style commun mechan littl modif highli scalabl initi annouc uber doc blog horovod distribut similarli apach mxnet easili creat paramet store host model paramet distribut fashion sync multipl node mxnet scalabl eas distribut reason love write docker algorithm cours distribut train multipl write commun scale horizont singl node multipl gpu bigger bigger type latenc complex increas drastic switch singl host multi host context truli multi node context mayb easier horovod easier creation ephemer bill cluster built log metadata artifact persist fast train data load shard train node note relev distribut train distribut devic model run devic grow batch size devic batch size stai constant devic keep busi disturb model converg bigger batch noisi sgd common heurist grow rate great paper priya goyal hand induc instabl coupl epoch associ rate warmup scale sgd larg batch activ research idea come frequent reach model perform larg batch hoc research fair paramet tune extent extra monei spent distribut overcom benefit faster train run distribut train sens individu record repres comput big physic batch devic big input size vision pictur big paramet count bert said model big logic batch necessarili distribut physic run sequenti batch singl gpu wait devic batch gradient averag paramet updat simul time bigger gpu clever hack call gradient accumul",
        "Solution_preprocessed_content":"distribut train model framework model distribut framework framework equal rare automat tensorflow kera neural net conceptu distribut paradigm gradient comput split worker multipl devic host multipl host multipl devic cours neural net distribut kera trivial distribut singl host fashion sadli deprec month model write synchron offici tutori relat come option algorithm distribut train vari depend option choos algorithm librari algorithm written distribut option apart set distribut multipl instanc framework popular framework docker environ write arbitrari option creation ephemer train cluster distribut train greater distribut train model run multipl order distribut train write distribut synchron framework modif tensorflow kera come horovod horovod commun mechan littl modif highli scalabl horovod distribut similarli apach mxnet easili creat paramet store host model paramet distribut fashion sync multipl node mxnet scalabl eas distribut reason love write docker algorithm cours distribut train multipl write commun scale horizont singl node multipl gpu bigger bigger type latenc complex increas drastic switch context truli context mayb easier horovod easier creation ephemer cluster log metadata artifact persist fast train data load shard train node note relev distribut train distribut devic model run devic grow batch size batch size stai constant devic keep busi disturb model converg bigger batch noisi sgd common heurist grow rate hand induc instabl coupl epoch associ rate warmup scale sgd larg batch activ research idea come frequent reach model perform larg batch research fair paramet tune extent extra monei spent distribut overcom benefit faster train run distribut train sens individu record repres comput big physic batch devic big input size big paramet count said model big logic batch necessarili distribut physic run sequenti batch singl gpu wait batch gradient averag paramet updat simul time bigger gpu",
        "Solution_readability":13.8,
        "Solution_reading_time":85.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":41.0,
        "Solution_word_count":853.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1522634496792,
        "Answerer_location":"Rio de Janeiro, RJ, Brasil",
        "Answerer_reputation_count":264.0,
        "Answerer_view_count":23.0,
        "Challenge_adjusted_solved_time":26.5148555556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have many experiment, like:<\/p>\n<p><img src=\"https:\/\/user-images.githubusercontent.com\/40580910\/95883598-82a07d00-0d51-11eb-847d-872452f6caa4.png\" alt=\"image\" \/><\/p>\n<p>and now, i want load an experiment<\/p>\n<pre><code>#%% sumonando os pacotes e verificando azureml.core\nimport azureml.core\nimport pandas as pd\nimport numpy as np\nimport logging\n\nprint(&quot;AzureML SDK Version: &quot;, azureml.core.VERSION)\n\n#%% Conectando ao azure e crinado o exparimento\n\nfrom azureml.core import Workspace, Experiment\n\nws = Workspace.from_config() \nprint(Experiment.list(ws))\n#%%\nExperiment = Experiment.from_directory('teste2-Monitor-Runs') `\n<\/code><\/pre>\n<p>but<\/p>\n<pre><code>&quot;error&quot;: {\n    &quot;message&quot;: &quot;No cache found for current project, try providing resource group and workspace \narguments&quot;\n}`\n<\/code><\/pre>\n<hr \/>\n<p>Content: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.experiment(class)?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.core.Experiment class - Azure Machine Learning Python<\/a><\/p>",
        "Challenge_closed_time":1602700300720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602604847240,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64338898",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":14.8,
        "Challenge_reading_time":14.62,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":26.5148555556,
        "Challenge_title":"How to load an experiment in azureml?",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":254,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522634496792,
        "Poster_location":"Rio de Janeiro, RJ, Brasil",
        "Poster_reputation_count":264.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>I believe it is that way.<\/p>\n<pre><code>from azureml.core import Experiment, Workspace\nExperiment = ws.experiments[&quot;teste2-Monitor-Runs&quot;]\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"believ core import workspac test monitor run",
        "Solution_preprocessed_content":null,
        "Solution_readability":15.7,
        "Solution_reading_time":2.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":14.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1428454496052,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":61.6148286111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have created a docker container that is using Sagemaker via the java sdk. This container is deployed on a k8s cluster with several replicas. <\/p>\n\n<p>The container is doing simple requests to Sagemaker to list some models that we have trained and deployed. However we are now having issues with some java certificate. I am quite novice with k8s and certificates so I will appreciate if you could provide some help to fix the issue.<\/p>\n\n<p>Here are some traces from the log when it tries to list the endpoints:<\/p>\n\n<pre><code>org.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:394)\n    at org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:353)\n    at com.amazonaws.http.conn.ssl.SdkTLSSocketFactory.connectSocket(SdkTLSSocketFactory.java:132)\n    at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:141)\n    at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:353)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)\n    at com.amazonaws.http.conn.$Proxy67.connect(Unknown Source)\n    at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:380)\n    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)\n    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184)\n    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184)\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)\n    at com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1236)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1056)\n    ... 70 common frames omitted\nCaused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n    at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:397)\n    at sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:302)\n    at sun.security.validator.Validator.validate(Validator.java:262)\n    at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:324)\n    at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:229)\n    at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:124)\n    at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1621)\n    ... 97 common frames omitted\nCaused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n    at sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:141)\n    at sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:126)\n    at java.security.cert.CertPathBuilder.build(CertPathBuilder.java:280)\n    at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:392)\n    ... 103 common frames omitted \n<\/code><\/pre>",
        "Challenge_closed_time":1544024416203,
        "Challenge_comment_count":2,
        "Challenge_created_time":1543802602820,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53586515",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":37.1,
        "Challenge_reading_time":51.22,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":61.6148286111,
        "Challenge_title":"Sagemaker certificate issue with Kubernetes",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":488,
        "Challenge_word_count":199,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428454496052,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>I think I have found the answer to my problem. I have set up another k8s cluster and deployed the container there as well. They are working fine and the certificate issues does not happen. When investigating more I have noticed that they were some issues with DNS resolution on the first k8s cluster. In fact the containers with certificate issues could not ping google.com for example.\nI fixed the DNS issue by not relying on core-dns and setting the DNS configuration in the deployment.yaml file. I am not sure to understand why exactly but this seems to have fixed the certificate issue.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"set dn configur deploy yaml file reli core dn certif dn resolut kubernet cluster",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"set cluster deploi certif notic dn resolut cluster certif ping com dn reli core dn set dn configur deploy yaml file exactli certif",
        "Solution_preprocessed_content":"set cluster deploi certif notic dn resolut cluster certif ping dn reli set dn configur file exactli certif",
        "Solution_readability":7.1,
        "Solution_reading_time":7.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":103.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":3520.4706275,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have pretrained model artifacts stored in S3 buckets. I want to create a service that loads this model and uses it for inference.<\/p>\n\n<p>I am working in AWS ecosystem and confused between using ECS vs Sagemaker for model deployment?\nWhat are some pros\/cons for choosing one over other?<\/p>",
        "Challenge_closed_time":1578553162123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578049726853,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59577521",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":4.1,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":139.8431305556,
        "Challenge_title":"AWS Sagemaker vs ECS for model hosting",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":3288,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1390885171883,
        "Poster_location":"College Station, TX, United States",
        "Poster_reputation_count":426.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>SageMaker has a higher price mark but it is taking a lot of the heavy lifting of deploying a machine learning model, such as wiring the pieces (load balancer, gunicorn, CloudWatch, Auto-Scaling...) and it is easier to automate the processes such as A\/B testing.<\/p>\n\n<p>If you have a strong team of DevOps that have nothing more important to do, you can build a flow that will be cheaper than the SageMaker option. ECS and EKS are doing at the same time a lot of work to make it very easy for you to automate the machine learning model deployments. However, they will always be more general purpose and SageMaker with its focus on machine learning will be easier for these use cases. <\/p>\n\n<p>The usual pattern of using the cloud is to use the managed services early on as you want to move fast and you don't really know where are your future problems. Once the system is growing and you start feeling some pains here and there, you can decide to spend the time and improve that part of the system. Therefore, if you don't know the pros\/cons, start with using the simpler options. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"servic simplifi deploy model autom process involv load balanc scale monitor expens option save time effort run team strong devop expertis ec ek gener purpos orchestr servic deploi model setup configur cost team strong devop skill resourc start simpler option option choos start simpler",
        "Solution_last_edit_time":1590723421112,
        "Solution_link_count":0.0,
        "Solution_original_content":"higher price mark heavi lift deploi model wire piec load balanc gunicorn cloudwatch auto scale easier autom process test strong team devop import build flow cheaper option ec ek time autom model deploy gener purpos focu easier pattern cloud servic earli fast futur grow start pain decid spend time improv pro con start simpler option",
        "Solution_preprocessed_content":"higher price mark heavi lift deploi model wire piec easier autom process test strong team devop import build flow cheaper option ec ek time autom model deploy gener purpos focu easier pattern cloud servic earli fast futur grow start pain decid spend time improv start simpler option",
        "Solution_readability":10.5,
        "Solution_reading_time":13.09,
        "Solution_score_count":7.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":196.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1589205020747,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":163.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":76.5197022222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to upgrade the default Ubuntu version that comes with the Compute Instance in Azure ML.<\/p>\n\n<p>Anyone has any guide on safely upgrading to the latest LTS?<\/p>",
        "Challenge_closed_time":1591531320408,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591255849480,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62189103",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":3.36,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":76.5197022222,
        "Challenge_title":"Azure ML Compute Instance: How can I safely upgrade the default Azure Ubuntu 16.04 LTS to the latest LTS?",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":312,
        "Challenge_word_count":46,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449542867716,
        "Poster_location":null,
        "Poster_reputation_count":1302.0,
        "Poster_view_count":157.0,
        "Solution_body":"<p>Any specific reason you want to do this?<\/p>\n\n<p>Since there are some heavy dependencies (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#contents\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#contents<\/a>), my guess is you have to try it yourself.<\/p>\n\n<p>Create a new one and run:<\/p>\n\n<pre><code>$ sudo apt update \n$ sudo apt upgrade\n$ sudo apt dist-upgrade\n<\/code><\/pre>\n\n<p>Let us know what happened.<\/p>\n\n<p>BTW: Are Compute Instance also Docker images? If so, the upgrade might be working, if not, there might be many drivers that need to be upgraded too. The ones from the GPU would be the easiest...<\/p>",
        "Solution_comment_count":4.0,
        "Solution_gpt_summary":"creat comput instanc run upgrad ubuntu version sudo apt updat sudo apt upgrad sudo apt dist upgrad",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"reason heavi depend http doc com concept comput instanc guess creat run sudo apt updat sudo apt upgrad sudo apt dist upgrad btw comput instanc docker imag upgrad driver upgrad on gpu easiest",
        "Solution_preprocessed_content":"reason heavi depend guess creat run btw comput instanc docker imag upgrad driver upgrad on gpu",
        "Solution_readability":9.8,
        "Solution_reading_time":9.21,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1391261341596,
        "Answerer_location":null,
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":5196.6227847222,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I wants to create azure machine learning workspace using terraform scripts.Is there any terraform provider to achieve this.<\/p>",
        "Challenge_closed_time":1600285333648,
        "Challenge_comment_count":1,
        "Challenge_created_time":1581577491623,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60202189",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":5.7,
        "Challenge_reading_time":2.72,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":5196.6227847222,
        "Challenge_title":"How to create azure machine learning resource using terraform resource providers?",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":1152,
        "Challenge_word_count":28,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565633099383,
        "Poster_location":null,
        "Poster_reputation_count":110.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>In the meantime Microsoft has added a Terraform resource for ML Workspace in the Azure Provider. This should make any custom scripting obsolete.<\/p>\n<p><a href=\"https:\/\/www.terraform.io\/docs\/providers\/azurerm\/r\/machine_learning_workspace.html\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/docs\/providers\/azurerm\/r\/machine_learning_workspace.html<\/a><\/p>\n<pre><code>resource &quot;azurerm_machine_learning_workspace&quot; &quot;example&quot; {\n  name                    = &quot;example-workspace&quot;\n  location                = azurerm_resource_group.example.location\n  resource_group_name     = azurerm_resource_group.example.name\n  application_insights_id = azurerm_application_insights.example.id\n  key_vault_id            = azurerm_key_vault.example.id\n  storage_account_id      = azurerm_storage_account.example.id\n\n  identity {\n    type = &quot;SystemAssigned&quot;\n  }\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"terraform resourc workspac creat workspac terraform resourc http terraform doc azurerm workspac html",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"terraform resourc workspac obsolet http terraform doc azurerm workspac html resourc azurerm workspac workspac locat azurerm resourc group locat resourc group azurerm resourc group azurerm kei vault azurerm kei vault storag account azurerm storag account ident type systemassign",
        "Solution_preprocessed_content":"terraform resourc workspac obsolet",
        "Solution_readability":25.1,
        "Solution_reading_time":11.31,
        "Solution_score_count":4.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1352206833663,
        "Answerer_location":null,
        "Answerer_reputation_count":893.0,
        "Answerer_view_count":185.0,
        "Challenge_adjusted_solved_time":28.8670294445,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I'm trying to define a Sagemaker Training Job with an existing Python class. To my understanding, I could create my own container but would rather not deal with container management.<\/p>\n\n<p>When choosing \"Algorithm Source\" there is the option of \"Your own algorithm source\" but nothing is listed under resources. Where does this come from?<\/p>\n\n<p>I know I could do this through a notebook, but I really want this defined in a job that can be invoked through an endpoint.<\/p>",
        "Challenge_closed_time":1550361262523,
        "Challenge_comment_count":0,
        "Challenge_created_time":1550257341217,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54715601",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":8.4,
        "Challenge_reading_time":7.15,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":28.8670294445,
        "Challenge_title":"How do I create a Sagemaker training job with my own Tensorflow code without having to build a container?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":781,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1366768533200,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>As Bruno has said you will have to use a container somewhere, but you can use an existing container to run your own custom tensorflow code.<\/p>\n\n<p>There is a good example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_quickstart\/tensorflow_script_mode_quickstart.ipynb\" rel=\"nofollow noreferrer\">in the sagemaker github<\/a> for how to do this.<\/p>\n\n<p>The way this works is you modify your code to have an entry point which takes argparse command line arguments, and then you point a 'Sagemaker Tensorflow estimator' to the entry point. Then when you call fit on the sagemaker estimator it will download the tensorflow container and run your custom code in there.<\/p>\n\n<p>So you start off with your own custom code that looks something like this<\/p>\n\n<pre><code># my_custom_code.py\nimport tensorflow as tf\nimport numpy as np\n\ndef build_net():\n    # single fully connected\n    image_place = tf.placeholder(tf.float32, [None, 28*28])\n    label_place = tf.placeholder(tf.int32, [None,])\n    net = tf.layers.dense(image_place, units=1024, activation=tf.nn.relu)\n    net = tf.layers.dense(net, units=10, activation=None)\n    return image_place, label_place, net\n\n\ndef process_data():\n    # load\n    (x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n\n    # center\n    x_train = x_train \/ 255.0\n    m = x_train.mean()\n    x_train = x_train - m\n\n    # convert to right types\n    x_train = x_train.astype(np.float32)\n    y_train = y_train.astype(np.int32)\n\n    # reshape so flat\n    x_train = np.reshape(x_train, [-1, 28*28])\n    return x_train, y_train\n\n\ndef train_model(init_learn, epochs):\n    image_p, label_p, logit = build_net()\n    x_train, y_train = process_data()\n\n    loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n        logits=logit,\n        labels=label_p)\n    optimiser = tf.train.AdamOptimizer(init_learn)\n    train_step = optimiser.minimize(loss)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for _ in range(epochs):\n            sess.run(train_step, feed_dict={image_p: x_train, label_p: y_train})\n\n\nif __name__ == '__main__':\n    train_model(0.001, 10)\n<\/code><\/pre>\n\n<p>To make it work with sagemaker we need to create a command line entry point, which will allow sagemaker to run it in the container it will download for us eventually.<\/p>\n\n<pre><code># entry.py\n\nimport argparse\nfrom my_custom_code import train_model\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        '--model_dir',\n        type=str)\n    parser.add_argument(\n        '--init_learn',\n        type=float)\n    parser.add_argument(\n        '--epochs',\n        type=int)\n    args = parser.parse_args()\n    train_model(args.init_learn, args.epochs)\n<\/code><\/pre>\n\n<p>Apart from specifying the arguments my function needs to take, we also need to provide a <code>model_dir<\/code> argument. This is always required, and is an S3 location which is where an model artifacts will be saved when the training job completes. Note that you don't need to specify what this value is (though you can) as Sagemaker will provide a default location in S3 for you.<\/p>\n\n<p>So we have modified our code, now we need to actually run it on Sagemaker. Go to the AWS console and fire up a small instance from Sagemaker. Download your custom code to the instance, and then create a jupyter notebook as follows:<\/p>\n\n<pre><code># sagemaker_run.ipyb\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\nhyperparameters = {\n    'epochs': 10,\n    'init_learn': 0.001}\n\nrole = sagemaker.get_execution_role()\nsource_dir = '\/path\/to\/folder\/with\/my\/code\/on\/instance'\nestimator = TensorFlow(\n    entry_point='entry.py',\n    source_dir=source_dir,\n    train_instance_type='ml.t2.medium',\n    train_instance_count=1,\n    hyperparameters=hyperparameters,\n    role=role,\n    py_version='py3',\n    framework_version='1.12.0',\n    script_mode=True)\n\nestimator.fit()\n<\/code><\/pre>\n\n<p>Running the above will:<\/p>\n\n<ul>\n<li>Spin up an ml.t2.medium instance<\/li>\n<li>Download the tensorflow 1.12.0 container to the instance<\/li>\n<li>Download any data we specify in fit to the newly created instance in fit (in this case nothing)<\/li>\n<li>Run our code on the instance<\/li>\n<li>upload the model artifacts to model_dir<\/li>\n<\/ul>\n\n<p>And that is pretty much it. There is of course a lot not mentioned here but you can:<\/p>\n\n<ul>\n<li>Download training\/testing data from s3<\/li>\n<li>Save checkpoint files, and tensorboard files during training and upload them to s3<\/li>\n<\/ul>\n\n<p>The best resource I found was the example I shared but here are all the things I was looking at to get this working:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_quickstart\/tensorflow_script_mode_quickstart.ipynb\" rel=\"nofollow noreferrer\">example code again<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">documentation<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers\" rel=\"nofollow noreferrer\">explanation of environment variables<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"run tensorflow modifi entri argpars line argument tensorflow estim entri fit estim download tensorflow run creat line entri allow run download model dir argument locat model artifact save train job complet download train test data save checkpoint file tensorboard file",
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_original_content":"bruno said run tensorflow github modifi entri argpars line argument tensorflow estim entri fit estim download tensorflow run start import tensorflow import numpi build net singl fulli connect imag placehold label placehold net layer dens imag unit activ relu net layer dens net unit activ return imag label net process data load train train kera dataset mnist load data center train train train train train convert type train train astyp train train astyp reshap flat train reshap train return train train train model init epoch imag label logit build net train train process data loss softmax cross entropi logit logit logit label label optimis train adamoptim init train step optimis minim loss session sess sess run global variabl initi rang epoch sess run train step feed dict imag train label train train model creat line entri allow run download entri import argpars import train model parser argpars argumentpars formatt class argpars argumentdefaultshelpformatt parser add argument model dir type str parser add argument init type parser add argument epoch type arg parser pars arg train model arg init arg epoch apart specifi argument function model dir argument locat model artifact save train job complet note specifi valu default locat modifi run consol small instanc download instanc creat jupyt notebook run ipyb import tensorflow import tensorflow hyperparamet epoch init role execut role sourc dir path folder instanc estim tensorflow entri entri sourc dir sourc dir train instanc type medium train instanc count hyperparamet hyperparamet role role version framework version mode estim fit run spin medium instanc download tensorflow instanc download data specifi fit newli creat instanc fit run instanc upload model artifact model dir pretti cours download train test data save checkpoint file tensorboard file train upload resourc share document explan environ variabl",
        "Solution_preprocessed_content":"bruno said run tensorflow github modifi entri argpars line argument tensorflow estim entri fit estim download tensorflow run start creat line entri allow run download apart specifi argument function argument locat model artifact save train job complet note specifi valu default locat modifi run consol small instanc download instanc creat jupyt notebook run spin instanc download tensorflow instanc download data specifi fit newli creat instanc fit run instanc upload model artifact pretti cours download data save checkpoint file tensorboard file train upload resourc share document explan environ variabl",
        "Solution_readability":12.3,
        "Solution_reading_time":66.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":50.0,
        "Solution_word_count":546.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":4.3897661111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained and deployed a model in Pytorch with Sagemaker. I am able to call the endpoint and get a prediction. I am using the default input_fn() function (i.e. not defined in my serve.py).<\/p>\n\n<pre><code>model = PyTorchModel(model_data=trained_model_location,\n                     role=role,\n                     framework_version='1.0.0',\n                     entry_point='serve.py',\n                     source_dir='source')\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>A prediction can be made as follows:<\/p>\n\n<pre><code>input =\"0.12787057,  1.0612601,  -1.1081504\"\npredictor.predict(np.genfromtxt(StringIO(input), delimiter=\",\").reshape(1,3) )\n<\/code><\/pre>\n\n<p>I want to be able to serve the model with REST API and am HTTP POST using lambda and API gateway. I was able to use invoke_endpoint() for this with an XGBOOST model in Sagemaker this way. I am not sure what to send into the body for Pytorch.<\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(EndpointName=ENDPOINT  ,\nContentType='text\/csv',\nBody=???)\n<\/code><\/pre>\n\n<p>I believe I need to understand how to write the customer input_fn to accept and process the type of data I am able to send through invoke_client. Am I on the right track and if so, how could the input_fn be written to accept a csv from invoke_endpoint?<\/p>",
        "Challenge_closed_time":1559781766688,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559765963530,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56467434",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":17.2,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":4.3897661111,
        "Challenge_title":"Making a Prediction Sagemaker Pytorch",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":2346,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1294628108596,
        "Poster_location":null,
        "Poster_reputation_count":1748.0,
        "Poster_view_count":393.0,
        "Solution_body":"<p>Yes you are on the right track. You can send csv-serialized input to the endpoint without using the <code>predictor<\/code> from the SageMaker SDK, and using other SDKs such as <code>boto3<\/code> which is installed in lambda:<\/p>\n\n<pre><code>import boto3\nruntime = boto3.client('sagemaker-runtime')\n\npayload = '0.12787057,  1.0612601,  -1.1081504'\n\nresponse = runtime.invoke_endpoint(\n    EndpointName=ENDPOINT_NAME,\n    ContentType='text\/csv',\n    Body=payload.encode('utf-8'))\n\nresult = json.loads(response['Body'].read().decode()) \n<\/code><\/pre>\n\n<p>This will pass to the endpoint a csv-formatted input, that you may need to reshape back in the <code>input_fn<\/code> to put in the appropriate dimension expected by the model.<\/p>\n\n<p>for example:<\/p>\n\n<pre><code>def input_fn(request_body, request_content_type):\n    if request_content_type == 'text\/csv':\n        return torch.from_numpy(\n            np.genfromtxt(StringIO(request_body), delimiter=',').reshape(1,3))\n<\/code><\/pre>\n\n<p><strong>Note<\/strong>: I wasn't able to test the specific <code>input_fn<\/code> above with your input content and shape but I used the approach on Sklearn RandomForest couple times, and looking at the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html#model-serving\" rel=\"nofollow noreferrer\">Pytorch SageMaker serving doc<\/a> the above rationale should work.<\/p>\n\n<p>Don't hesitate to use endpoint logs in Cloudwatch to diagnose any inference error (available from the endpoint UI in the console), those logs are usually <strong>much more verbose<\/strong> that the high-level logs returned by the inference SDKs<\/p>",
        "Solution_comment_count":4.0,
        "Solution_gpt_summary":"send csv serial input endpoint predictor sdk sdk boto instal lambda pass endpoint csv format input reshap input dimens model endpoint log cloudwatch diagnos infer log verbos high level log return infer sdk",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"track send csv serial input endpoint predictor sdk sdk boto instal lambda import boto runtim boto client runtim payload respons runtim invok endpoint endpointnam endpoint contenttyp text csv bodi payload encod utf json load respons bodi read decod pass endpoint csv format input reshap input dimens model input request bodi request type request type text csv return torch numpi genfromtxt stringio request bodi delimit reshap note wasn test input input shape sklearn randomforest coupl time pytorch serv doc rational hesit endpoint log cloudwatch diagnos infer endpoint consol log verbos high level log return infer sdk",
        "Solution_preprocessed_content":"track send input endpoint sdk sdk instal lambda pass endpoint input reshap dimens model note wasn test input shape sklearn randomforest coupl time pytorch serv doc rational hesit endpoint log cloudwatch diagnos infer log verbos log return infer sdk",
        "Solution_readability":12.9,
        "Solution_reading_time":20.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":172.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1515931819516,
        "Answerer_location":null,
        "Answerer_reputation_count":177.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":92.4561386111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm not very familiar with parallelization in Python and I'm getting an error when trying to train a model on multiple training folds in parallel. Here's a simplified version of my code:<\/p>\n<pre><code>def train_test_model(fold):\n    # here I train the model etc...\n    \n    # now I want to save the parameters and metrics\n    with mlflow.start_run():\n        mlflow.log_param(&quot;run_name&quot;, run_name)\n        mlflow.log_param(&quot;modeltype&quot;, modeltype)\n        # and so on...\n\nif __name__==&quot;__main__&quot;:\n    pool = ThreadPool(processes = num_trials)\n    # run folds in parallel\n    pool.map(lambda fold:train_test_model(fold), folds)\n<\/code><\/pre>\n<p>I'm getting the following error:<\/p>\n<pre><code>Exception: Run with UUID 23e9bb6d22674a518e48af9c51252860 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True\n<\/code><\/pre>\n<p>The <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">documentation<\/a> says that <code>mlflow.start_run()<\/code> starts a new run and makes it active which is the root of my problem. Every thread starts a MLFlow run for its corresponding fold and makes it active while I need the runs to run in parallel i.e. all be active(?) and save parameters\/metrics of the corresponding fold. How can I solve that issue?<\/p>",
        "Challenge_closed_time":1603877162672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603544320573,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64513552",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":17.98,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":92.4561386111,
        "Challenge_title":"How to have multiple MLFlow runs in parallel?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2275,
        "Challenge_word_count":176,
        "Platform":"Stack Overflow",
        "Poster_created_time":1515931819516,
        "Poster_location":null,
        "Poster_reputation_count":177.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>I found a solution, maybe it will be useful for someone else. You can see details with code examples here: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/3592\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/3592<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":8.8,
        "Solution_reading_time":3.23,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":3.4555330556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to connect a VM I have in AzureML studio.  I keep getting the following:  Connection attempt timed out for ''. Verify that server is accessible and SSH service is accepting connections.<\/p>",
        "Challenge_closed_time":1644985009903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644973069297,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71135228",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":2.97,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":3.316835,
        "Challenge_title":"Connection timeout in AzureML studio",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":208,
        "Challenge_word_count":36,
        "Platform":"Stack Overflow",
        "Poster_created_time":1555914279856,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Go to your VM config and test your connection through the 'connect' tab.  Is your test successful?  If not, check if port 22 is blocked.  Watch for automated blocking rules applied to your VM.<\/p>\n<p>we have DSVM attach in preview - might be interesting for you: <a href=\"https:\/\/github.com\/Azure\/azureml-previews\/tree\/main\/previews\/dsvm-attach\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azureml-previews\/tree\/main\/previews\/dsvm-attach<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"config test connect connect tab test unsuccess port block watch autom block rule appli dsvm attach preview",
        "Solution_last_edit_time":1644985509216,
        "Solution_link_count":2.0,
        "Solution_original_content":"config test connect connect tab test port block watch autom block rule appli dsvm attach preview http github com preview tree preview dsvm attach",
        "Solution_preprocessed_content":"config test connect connect tab test port block watch autom block rule appli dsvm attach preview",
        "Solution_readability":10.0,
        "Solution_reading_time":5.93,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":49.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":24.4510758334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to get all the files that are a specified size within a folder of an s3 bucket. How do I go about iterating through the bucket and filtering the files by the specified size? I also want to return the file names of those with the correct size.<\/p>\n\n<pre><code>s3 = boto3.client('s3')\ns3.list_objects_v2(Bucket = 'my-images')\n<\/code><\/pre>\n\n<p>A sample output is <\/p>\n\n<pre><code> u'Key': u'detail\/01018535.jpg',\n   u'LastModified': datetime.datetime(2019, 1, 23, 0, 48, 41, tzinfo=tzlocal()),\n   u'Size': 13535,\n   u'StorageClass': 'STANDARD'},\n  {u'ETag': '\"cd65991a1c6f118e8b036208a30028a7\"',\n   u'Key': u'detail\/0119AF2.jpg',\n   u'LastModified': datetime.datetime(2019, 1, 10, 17, 17, tzinfo=tzlocal()),\n   u'Size': 12984,\n   u'StorageClass': 'STANDARD'}\n<\/code><\/pre>\n\n<p>for instance lets say that I would want a search for a size of 12984.\nThen it would return the 'Key'<\/p>",
        "Challenge_closed_time":1555701934872,
        "Challenge_comment_count":0,
        "Challenge_created_time":1555614779330,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55752427",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":6.6,
        "Challenge_reading_time":12.0,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":24.2098727778,
        "Challenge_title":"How to filter and list all objects in s3 folder by a certain size using python",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":4330,
        "Challenge_word_count":128,
        "Platform":"Stack Overflow",
        "Poster_created_time":1549637130876,
        "Poster_location":"El Paso, TX, USA",
        "Poster_reputation_count":17.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>If you are looking to use boto3, I use this function to find zero byte objects. You can tweak it to your needs by filtering on specific size<\/p>\n\n<pre><code>import boto3\n\ndef get_empty_objects(bucket_name, prefixes):\n    \"\"\"\n    get list of objects from a given s3 prefix recursively\n    \"\"\"\n    results = []\n    for prefix in prefixes:\n        s3client = boto3.client('s3')\n        paginator = s3client.get_paginator(\"list_objects_v2\")\n        paginator_result = paginator.paginate(\n            Bucket=bucket_name, Prefix=prefix)\n        try:\n            for object in paginator_result.search('Contents'):\n                if object['Size'] == 0:\n                    results.append(\"s3:\/\/\" + bucket_name + \"\/\" + object['Key'])\n        except Exception as err:\n            print(\"&gt;&gt;&gt; Error processing objects of [s3:\/\/\" + bucket_name +\n                  \"\/\" + prefix + \"] - \" + str(err))\n        print(\"&gt;&gt;&gt; Returning \" + str(len(results)) + \" objects for [s3:\/\/\" + bucket_name + \"\/\" + prefix + \"]\")\n    return results\n<\/code><\/pre>\n\n<p>Usage:<\/p>\n\n<pre><code>get_empty_objects(\"mybucket\", [\"prefix1\/\", \"prefix2\/\"])\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"function filter list object folder size modifi function filter size condit statement",
        "Solution_last_edit_time":1555702803203,
        "Solution_link_count":0.0,
        "Solution_original_content":"boto function byte object tweak filter size import boto object bucket prefix list object prefix recurs prefix prefix sclient boto client pagin sclient pagin list object pagin pagin pagin bucket bucket prefix prefix object pagin search object size append bucket object kei except err print process object bucket prefix str err print return str len object bucket prefix return usag object mybucket prefix prefix",
        "Solution_preprocessed_content":"boto function byte object tweak filter size usag",
        "Solution_readability":15.7,
        "Solution_reading_time":12.57,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":96.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":334.3928986111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am developing an Android application that uses raw accelerometer data and I want to classify the data by using machine learning, i.e. Azure ML service. For example when device moves like a 1 in space, it should generate number 1 in text field specified in application. I decided to use Machine Learning to classify movements but I couldn't decide how to store data and send it to the machine learning service for training. For now, I am creating an SQLite table in application and add the X,Y,Z value of sensor each time sensor data gets changed. After that I am sending data to machine learning service but I have problem. The data only includes one movement for 1. How can I store multiple data for same movement and data for other movements -that will represent different numbers like 2, 3- and send them to the machine learning service?<\/p>",
        "Challenge_closed_time":1473766291168,
        "Challenge_comment_count":0,
        "Challenge_created_time":1472562476733,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1483518837827,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39228377",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":11.28,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":334.3928986111,
        "Challenge_title":"How to Store Accelerometer Data for Classification by using Machine Learning",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":730,
        "Challenge_word_count":159,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428256167643,
        "Poster_location":"\u0130stanbul, T\u00fcrkiye",
        "Poster_reputation_count":576.0,
        "Poster_view_count":68.0,
        "Solution_body":"<p>@MuhammedKadirY\u00fccel, Based on my understanding, I think you want to send raw accelerometer data to Azure and store into some storage service for importing on Machine Learning service.<\/p>\n\n<p>Per my experience, I think the best practice is that create a <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/services\/event-hubs\/\" rel=\"nofollow\">EventHub<\/a> or <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/services\/iot-hub\/\" rel=\"nofollow\">IoTHub<\/a> instance for receiving these raw accelerometer data. <\/p>\n\n<p>Then create a <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/stream-analytics-get-started-with-azure-stream-analytics-to-process-data-from-iot-devices\/\" rel=\"nofollow\">Stream Analytics<\/a> to transfer sensor data from EventHub or IoTHub to Azure Blob Storage. <\/p>\n\n<p>Finally, you can import these data of the blob storage on Machine Learning service, please see <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-import-data-from-online-sources\/\" rel=\"nofollow\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-import-data-from-online-sources\/<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"creat eventhub iothub instanc receiv raw acceleromet data creat stream analyt transfer data blob storag final data import blob storag servic",
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_original_content":"muhammedkadirycel base send raw acceleromet data store storag servic import servic practic creat eventhub iothub instanc receiv raw acceleromet data creat stream analyt transfer sensor data eventhub iothub blob storag final import data blob storag servic http com document articl import data onlin sourc",
        "Solution_preprocessed_content":"base send raw acceleromet data store storag servic import servic practic creat eventhub iothub instanc receiv raw acceleromet data creat stream analyt transfer sensor data eventhub iothub blob storag final import data blob storag servic",
        "Solution_readability":22.7,
        "Solution_reading_time":15.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1300961991836,
        "Answerer_location":"Prague, Czechia",
        "Answerer_reputation_count":31015.0,
        "Answerer_view_count":2353.0,
        "Challenge_adjusted_solved_time":18426.1498944444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In this talk <a href=\"https:\/\/channel9.msdn.com\/Events\/Ignite\/2015\/BRK3550\" rel=\"nofollow\">https:\/\/channel9.msdn.com\/Events\/Ignite\/2015\/BRK3550<\/a> the speaker mentions that the Azure ML service originally had a different name.<\/p>\n\n<p>My stack and google searches have shown no results so far.\nDoes anyone know it by chance?<\/p>\n\n<p>Edit: The fragment i'm talking about starts at 56:00 and is a couple of seconds long.<\/p>",
        "Challenge_closed_time":1512165883856,
        "Challenge_comment_count":3,
        "Challenge_created_time":1438888294060,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1445831744236,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/31863977",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":7.2,
        "Challenge_reading_time":6.27,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":20354.8860544444,
        "Challenge_title":"What's the original name of Microsoft Azure Machine Learning?",
        "Challenge_topic":"Columnar Data Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":220,
        "Challenge_word_count":60,
        "Platform":"Stack Overflow",
        "Poster_created_time":1437760185463,
        "Poster_location":"Belgium",
        "Poster_reputation_count":33.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>Look for <strong>Project Passau<\/strong>. Here's one of the references, in <a href=\"https:\/\/visualstudiomagazine.com\/articles\/2014\/09\/01\/azure-machine-learning-studio.aspx\" rel=\"nofollow noreferrer\">Visual Studio Magazine<\/a> or another one <a href=\"https:\/\/blogs.msdn.microsoft.com\/mspowerutilities\/2014\/07\/08\/harnessing-the-power-of-big-data-with-cloud-predictive-analytics-introducing-azure-machine-learning\/\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"speaker passau visual studio magazin power util blog",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"passau visual studio magazin",
        "Solution_preprocessed_content":"passau visual studio magazin",
        "Solution_readability":32.7,
        "Solution_reading_time":6.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1621658973823,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":3213.0,
        "Answerer_view_count":1896.0,
        "Challenge_adjusted_solved_time":5.4727108333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been following the learning path for <a href=\"https:\/\/docs.microsoft.com\/en-us\/learn\/certifications\/exams\/ai-900\" rel=\"nofollow noreferrer\">Microsoft Azure AI 900<\/a>. In the second module, I have deployed my model as an endpoint. It says Container instances for compute type. How much will this cost me. Azure doesn't seem to show any pricing for this. Is this endpoint always active? If yes how much does it cost?<\/p>",
        "Challenge_closed_time":1635505501916,
        "Challenge_comment_count":1,
        "Challenge_created_time":1635485800157,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69764100",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":6.1,
        "Challenge_reading_time":5.95,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":5.4727108333,
        "Challenge_title":"Endpoints cost on Azure Machine Learning",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":633,
        "Challenge_word_count":66,
        "Platform":"Stack Overflow",
        "Poster_created_time":1566078293736,
        "Poster_location":"Mumbai, Maharashtra, India",
        "Poster_reputation_count":449.0,
        "Poster_view_count":77.0,
        "Solution_body":"<p>The price depends on the number of <strong>vCPU<\/strong> and <strong>GBs<\/strong> of memory requested for the container group. You are charged based on the <strong>vCPU request<\/strong> for your container group rounded up to the nearest whole number for the duration (measured in seconds) <strong>your instance is running<\/strong>. You are also charged for the <strong>GB request<\/strong> for your container group rounded up to the nearest tenths place for the duration (measured in seconds) your <strong>container group is running<\/strong>. There is an additional charge of $0.000012 per vCPU second for Windows software duration on Windows container groups. Check here <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/container-instances\/\" rel=\"nofollow noreferrer\">Pricing - Container Instances | Microsoft Azure<\/a> for details<\/p>\n<ul>\n<li>After Deployed the Azure Machine Learning managed online endpoint (preview).<\/li>\n<li>Have at least <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/role-based-access-control\/role-assignments-portal.md\" rel=\"nofollow noreferrer\">Billing Reader<\/a> access on the subscription where the endpoint is deployed<\/li>\n<\/ul>\n<p>To know the costs estimation<\/p>\n<ol>\n<li><p>In the <a href=\"https:\/\/portal.azure.com\/\" rel=\"nofollow noreferrer\">Azure portal<\/a>, Go to your subscription<\/p>\n<\/li>\n<li><p>Select <strong>Cost Analysis<\/strong> for your subscription.<\/p>\n<\/li>\n<\/ol>\n<p><img src=\"https:\/\/i.imgur.com\/W2eaRIO.png\" alt=\"enter image description here\" \/><\/p>\n<p>Create a filter to scope data to your Azure Machine learning workspace resource:<\/p>\n<ol>\n<li><p>At the top navigation bar, select <strong>Add filter<\/strong>.<\/p>\n<\/li>\n<li><p>In the first filter dropdown, select <strong>Resource<\/strong> for the filter type.<\/p>\n<\/li>\n<li><p>In the second filter dropdown, select your Azure Machine Learning workspace.<\/p>\n<\/li>\n<\/ol>\n<p><img src=\"https:\/\/i.imgur.com\/HEvprph.png\" alt=\"enter image description here\" \/><\/p>\n<p>Create a tag filter to show your managed online endpoint and\/or managed online deployment:<\/p>\n<ol>\n<li><p>Select <strong>Add filter<\/strong> &gt; <strong>Tag<\/strong> &gt; <strong>azuremlendpoint<\/strong>: &quot;&lt; your endpoint name&gt;&quot;<\/p>\n<\/li>\n<li><p>Select <strong>Add filter<\/strong> &gt; <strong>Tag<\/strong> &gt; <strong>azuremldeployment<\/strong>: &quot;&lt; your deployment name&gt;&quot;.<\/p>\n<\/li>\n<\/ol>\n<p><img src=\"https:\/\/i.imgur.com\/1aapYGB.png\" alt=\"enter image description here\" \/><\/p>\n<p>Refer  <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/how-to-view-online-endpoints-costs.md\" rel=\"nofollow noreferrer\">here <\/a> for more detailed steps<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"cost deploi model endpoint depend vcpu gb memori request group charg base vcpu request request durat group run addit charg vcpu window softwar durat window group estim cost portal select cost analysi subscript creat filter scope data workspac resourc creat tag filter onlin endpoint onlin deploy bill reader access",
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_original_content":"price depend vcpu gb memori request group charg base vcpu request group round nearest durat measur instanc run charg request group round nearest tenth durat measur group run addit charg vcpu window softwar durat window group price instanc deploi onlin endpoint preview bill reader access subscript endpoint deploi cost estim portal subscript select cost analysi subscript creat filter scope data workspac resourc navig bar select add filter filter dropdown select resourc filter type filter dropdown select workspac creat tag filter onlin endpoint onlin deploy select add filter tag endpoint select add filter tag deploy step",
        "Solution_preprocessed_content":"price depend vcpu gb memori request group charg base vcpu request group round nearest durat instanc run charg request group round nearest tenth durat group run addit charg vcpu window softwar durat window group price instanc deploi onlin endpoint bill reader access subscript endpoint deploi cost estim portal subscript select cost analysi subscript creat filter scope data workspac resourc navig bar select add filter filter dropdown select resourc filter type filter dropdown select workspac creat tag filter onlin endpoint onlin deploy select add filter tag endpoint endpoint select add filter tag deploy deploy step",
        "Solution_readability":12.6,
        "Solution_reading_time":35.84,
        "Solution_score_count":2.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":280.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1227171471292,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":17500.0,
        "Answerer_view_count":1561.0,
        "Challenge_adjusted_solved_time":233.4898177778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Invoking a multimodel Sagemaker Endpoint, I get an error that it is not multimodel. I create it like this.<\/p>\n<pre><code>create_endpoint_config_response = client.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[\n        {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialVariantWeight&quot;: 0.5,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;ModelName&quot;: model_name1,\n            &quot;VariantName&quot;: model_name1,\n        },\n         {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialVariantWeight&quot;: 0.5,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;ModelName&quot;: model_name2,\n            &quot;VariantName&quot;: model_name2,\n        }\n    ]\n)\n<\/code><\/pre>\n<p>I confirm in the GUI that it in fact has multiple models. I invoke it like this:<\/p>\n<pre><code>response = client.invoke_endpoint(\n    EndpointName=endpoint_name, \n    TargetModel=model_name1,\n    ContentType=&quot;text\/x-libsvm&quot;, \n    Body=payload\n)\n<\/code><\/pre>\n<p>and get this error:<\/p>\n<blockquote>\n<p>ValidationError: An error occurred (ValidationError) when calling the\nInvokeEndpoint operation: Endpoint\nmy-endpoint1 is not a multi-model endpoint\nand does not support target model header.<\/p>\n<\/blockquote>\n<p>The same problem was discussed <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026\" rel=\"nofollow noreferrer\">here<\/a> with no resolution.<\/p>\n<p>How can I invoke a multimodel endpoint?<\/p>",
        "Challenge_closed_time":1629283623048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629114933853,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1629119102992,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68802388",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":19.77,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":46.8581097222,
        "Challenge_title":"Why do I get an error that Sagemaker Endpoint does not have multiple models when it does?",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":247,
        "Challenge_word_count":133,
        "Platform":"Stack Overflow",
        "Poster_created_time":1227171471292,
        "Poster_location":"Israel",
        "Poster_reputation_count":17500.0,
        "Poster_view_count":1561.0,
        "Solution_body":"<p>The answer (see <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026\" rel=\"nofollow noreferrer\">GitHub<\/a> discussion) is that this error message is simply false.<\/p>\n<p>To avoid this error, the model's local filename (usually for the form <code>model_filename.tar.gz<\/code>) must be used, not the model name.<\/p>\n<p>The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/invoke-multi-model-endpoint.html\" rel=\"nofollow noreferrer\">documentation<\/a> does say this, though it lacks essential detail.<\/p>\n<p>I found <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb\" rel=\"nofollow noreferrer\">this to be the best example<\/a>.  See the last part  of that Notebook, in which <code>invoke_endpoint<\/code> is used (rather than a predictor as used earlier in the Notebook).<\/p>\n<p>As to the location of that model file: This <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">Notebook<\/a> says:<\/p>\n<blockquote>\n<p>When creating the Model entity for multi-model endpoints, the container's ModelDataUrl is the S3 prefix where the model\nartifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when invoking the model.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"messag avoid model local filenam model document lack essenti invok endpoint function predictor locat model file specifi model entiti modeldataurl prefix invok model artifact locat",
        "Solution_last_edit_time":1629959666336,
        "Solution_link_count":4.0,
        "Solution_original_content":"github messag simpli avoid model local filenam model filenam tar model document lack essenti notebook invok endpoint predictor earlier notebook locat model file notebook sai creat model entiti multi model endpoint modeldataurl prefix model artifact invok endpoint locat rest path specifi invok model",
        "Solution_preprocessed_content":"messag simpli avoid model local filenam model document lack essenti notebook locat model file notebook sai creat model entiti endpoint modeldataurl prefix model artifact invok endpoint locat rest path specifi invok model",
        "Solution_readability":17.2,
        "Solution_reading_time":19.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":138.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1577353307072,
        "Answerer_location":null,
        "Answerer_reputation_count":491.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":2.0195838889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>after download a dataset, convert to dataframe and manipulate it.. how can I upload again as new dataset in Azure Machine Learning? <\/p>",
        "Challenge_closed_time":1582569006872,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582561736370,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60380154",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":4.8,
        "Challenge_reading_time":2.41,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2.0195838889,
        "Challenge_title":"Upload dataframe as dataset in Azure Machine Learning",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":3736,
        "Challenge_word_count":30,
        "Platform":"Stack Overflow",
        "Poster_created_time":1348126905516,
        "Poster_location":"Barcelona",
        "Poster_reputation_count":327.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>You can follow the steps below: <br>\n1. write dataframe to a local file (e.g. csv, parquet)<\/p>\n\n<pre><code>local_path = 'data\/prepared.csv'\ndf.to_csv(local_path)\n<\/code><\/pre>\n\n<ol start=\"2\">\n<li>upload the local file to a datastore on the cloud<\/li>\n<\/ol>\n\n<pre><code># azureml-core of version 1.0.72 or higher is required\n# azureml-dataprep[pandas] of version 1.1.34 or higher is required\nfrom azureml.core import Workspace, Dataset\n\nsubscription_id = 'xxxxxxxxxxxxxxxxxxxxx'\nresource_group = 'xxxxxx'\nworkspace_name = 'xxxxxxxxxxxxxxxx'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n# get the datastore to upload prepared data\ndatastore = workspace.get_default_datastore()\n# upload the local file from src_dir to the target_path in datastore\ndatastore.upload(src_dir='data', target_path='data')\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li>create a dataset referencing the cloud location<\/li>\n<\/ol>\n\n<pre><code>ds = Dataset.Tabular.from_delimited_files(datastore.path('data\/prepared.csv'))\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"step upload manipul datafram dataset download manipul dataset write datafram local file csv parquet upload local file datastor cloud creat dataset referenc cloud locat",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"step write datafram local file csv parquet local path data prepar csv csv local path upload local file datastor cloud core version higher dataprep panda version higher core import workspac dataset subscript resourc group workspac workspac workspac subscript resourc group workspac datastor upload prepar data datastor workspac default datastor upload local file src dir target path datastor datastor upload src dir data target path data creat dataset referenc cloud locat dataset tabular delimit file datastor path data prepar csv",
        "Solution_preprocessed_content":"step write datafram local file upload local file datastor cloud creat dataset referenc cloud locat",
        "Solution_readability":12.4,
        "Solution_reading_time":13.32,
        "Solution_score_count":7.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1353403873547,
        "Answerer_location":null,
        "Answerer_reputation_count":4909.0,
        "Answerer_view_count":583.0,
        "Challenge_adjusted_solved_time":370.54581,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Optuna's FAQ has a <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#id10\" rel=\"nofollow noreferrer\">clear answer<\/a> when it comes to dynamically adjusting the range of parameter during a study: it poses no problem since each sampler is defined individually.<\/p>\n<p>But what about adding and\/or removing parameters? Is Optuna able to handle such adjustments?<\/p>\n<p>One thing I noticed when doing this is that in the results dataframe these parameters get <code>nan<\/code> entries for other trials. Would there be any benefit to being able to set these <code>nan<\/code>s to their (default) value that they had when not being sampled? Is the study still sound with all these unknown values?<\/p>",
        "Challenge_closed_time":1609649865303,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608315900387,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65362133",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":9.9,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":370.54581,
        "Challenge_title":"What happens when I add\/remove parameters dynamically during an Optuna study?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":227,
        "Challenge_word_count":110,
        "Platform":"Stack Overflow",
        "Poster_created_time":1353403873547,
        "Poster_location":null,
        "Poster_reputation_count":4909.0,
        "Poster_view_count":583.0,
        "Solution_body":"<p>Question was answered <a href=\"https:\/\/github.com\/optuna\/optuna\/issues\/2141\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<blockquote>\n<p>Thanks for the question. Optuna internally supports two types of sampling: <code>optuna.samplers.BaseSampler.sample_independent<\/code> and <code>optuna.samplers.BaseSampler.sample_relative<\/code>.<\/p>\n<p>The former <code>optuna.samplers.BaseSampler.sample_independent<\/code> is a method that samples independently on each parameter, and is not affected by the addition or removal of parameters. The added parameters are taken into account from the timing when they are added.<\/p>\n<p>The latter <code>optuna.samplers.BaseSampler.sample_relative<\/code> is a method that samples by considering the correlation of parameters and is affected by the addition or removal of parameters. Optuna's default search space for correlation is the product set of the domains of the parameters that exist from the beginning of the hyperparameter tuning to the present. Developers who implement samplers can implement their own search space calculation method <code>optuna.samplers.BaseSampler.infer_relative_search_space<\/code>. This may allow correlations to be considered for hyperparameters that have been added or removed, but this depends on the sampling algorithm, so there is no API for normal users to modify.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"intern type sampl sampler basesampl sampl independ sampler basesampl sampl rel sampl independ paramet affect addit remov paramet paramet taken account time sampl correl paramet affect addit remov paramet default search space correl set domain paramet begin hyperparamet tune present implement sampler implement search space calcul sampler basesampl infer",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"intern type sampl sampler basesampl sampl independ sampler basesampl sampl rel sampler basesampl sampl independ sampl independ paramet affect addit remov paramet paramet taken account time sampler basesampl sampl rel sampl correl paramet affect addit remov paramet default search space correl set domain paramet begin hyperparamet tune present implement sampler implement search space calcul sampler basesampl infer rel search space allow correl hyperparamet remov depend sampl algorithm api normal modifi",
        "Solution_preprocessed_content":"intern type sampl sampl independ paramet affect addit remov paramet paramet taken account time sampl correl paramet affect addit remov paramet default search space correl set domain paramet begin hyperparamet tune present implement sampler implement search space calcul allow correl hyperparamet remov depend sampl algorithm api normal modifi",
        "Solution_readability":14.3,
        "Solution_reading_time":17.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":157.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1369787017728,
        "Answerer_location":"Atlanta, Georgia",
        "Answerer_reputation_count":55.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":115.0280841667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is what I use to deploy an Auto-ML model:<\/p>\n<pre><code>            MachineSpec machineSpec = MachineSpec.newBuilder().setMachineType(&quot;n1-standard-2&quot;).build();\n            DedicatedResources dedicatedResources =\n                    DedicatedResources.newBuilder().setMinReplicaCount(1).setMachineSpec(machineSpec).build();            \n            String model = ModelName.of(project, location, modelId).toString();\n            DeployedModel deployedModel =\n                    DeployedModel.newBuilder()\n                            .setModel(model)\n                            .setDisplayName(deployedModelDisplayName)\n                            .setDedicatedResources(dedicatedResources)\n                            .build();\n            Map&lt;String, Integer&gt; trafficSplit = new HashMap&lt;&gt;();\n            trafficSplit.put(&quot;0&quot;, 100);\n            EndpointName endpoint = EndpointName.of(project, location, endpointId);\n            OperationFuture&lt;DeployModelResponse, DeployModelOperationMetadata&gt; response =\n                    client.deployModelAsync(endpoint, deployedModel, trafficSplit);\n            response.getInitialFuture().get().getName());\n<\/code><\/pre>\n<p>The error appears when I hit this line <code>response.getInitialFuture().get().getName());<\/code><\/p>\n<p>Here is the error:\n<code>INVALID_ARGUMENT: 'dedicated_resources' is not supported for Model projects\/***\/locations\/us-central1\/models\/***<\/code><\/p>\n<p>I can deploy the model using cloud console but not programmatically using java 8. It is a new model and the endpoint is also new without any assigned model to it.<\/p>",
        "Challenge_closed_time":1640820603630,
        "Challenge_comment_count":8,
        "Challenge_created_time":1640406502527,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70477987",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":9,
        "Challenge_readability":21.9,
        "Challenge_reading_time":18.8,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":115.0280841667,
        "Challenge_title":"Vertex Ai issue when deploying a model using Java",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":244,
        "Challenge_word_count":109,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369787017728,
        "Poster_location":"Atlanta, Georgia",
        "Poster_reputation_count":55.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>I am sorry everyone, I was implementing the wrong section of the documentation. I had to follow AutoML image, not Custom-trained one.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/a4xPR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/a4xPR.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"realiz implement section document automl imag train",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"sorri implement section document automl imag train",
        "Solution_preprocessed_content":"sorri implement section document automl imag",
        "Solution_readability":12.9,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":32.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1593662684510,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":119.8228480555,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am fairly new to TensorFlow (and SageMaker) and am stuck in the process of deploying a SageMaker endpoint. I have just recently succeeded in creating a Saved Model type model, which is currently being used to service a sample endpoint (the model was created externally). However, when I checked the image I am using for the endpoint, it says '...\/tensorflow-inference', which is not the direction I want to go in because I want to use a SageMaker TensorFlow serving container (I followed tutorials from the official TensorFlow serving GitHub repo-using sample models, and they are deployed correcting using the TensorFlow serving framework).<\/p>\n<p>Am I encountering this issue because my Saved Model does not have the correct 'serving' tag? I have not checked my tag sets yet but wanted to know if this would be the core reason to the problem. Also, most importantly, <strong>what are the differences between the two container types<\/strong>-I think having a better understanding of these two concepts would show me why I am unable to produce the correct image.<\/p>\n<hr \/>\n<p>This is how I deployed the sample endpoint:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model = Model(model_data =...)\n\npredictor = model.deploy(initial_instance_count=...)\n<\/code><\/pre>\n<p>When I run the code, I get a model, an endpoint configuration, and an endpoint. I got the container type by clicking on model details within the AWS SageMaker console.<\/p>",
        "Challenge_closed_time":1595219623720,
        "Challenge_comment_count":3,
        "Challenge_created_time":1594709403670,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1594788261467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62889537",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":5,
        "Challenge_readability":11.9,
        "Challenge_reading_time":19.08,
        "Challenge_score_count":2,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":141.7277916667,
        "Challenge_title":"TensorFlow Serving vs. TensorFlow Inference (container type for SageMaker model)",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1223,
        "Challenge_word_count":229,
        "Platform":"Stack Overflow",
        "Poster_created_time":1593662684510,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>There are different versions for the framework containers. Since the framework version I'm using is 1.15, the image I got had to be in a tensorflow-inference container. If I used versions &lt;= 1.13, then I would get sagemaker-tensorflow-serving images. The two aren't the same, but there's no 'correct' container type.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"type framework version type depend framework version",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"version framework framework version imag tensorflow infer version tensorflow serv imag aren type",
        "Solution_preprocessed_content":"version framework framework version imag version imag aren type",
        "Solution_readability":6.5,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1588516515763,
        "Answerer_location":"UK",
        "Answerer_reputation_count":29087.0,
        "Answerer_view_count":3080.0,
        "Challenge_adjusted_solved_time":0.0890694444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using SageMaker JupyterLab, but I found pandas is out of date, what's the process of updating it?<\/p>\n<p>I tried this:\nIn terminal:<\/p>\n<pre><code>cd SageMaker\nconda update pandas\n<\/code><\/pre>\n<p>The package has been updated to 1.0.5\nbut when I use this command in SageMaker instance:<\/p>\n<pre><code>import pandas\nprint(pandas,__version__)\n\nreturn:\n0.24.2\n<\/code><\/pre>\n<p>It didn't work at all, can someone help me? Thanks.<\/p>",
        "Challenge_closed_time":1593895412640,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593894159730,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1593895091990,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62734059",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":6.29,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.3480305556,
        "Challenge_title":"How to update pandas version in SageMaker notebook terminal?",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1158,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>If you want to perform any kind of upgrades or modification to the kernel of the notebook you can do this at launch by using <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">lifecycle configuration<\/a>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_gpt_summary":"lifecycl configur modifi kernel notebook launch",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"perform upgrad modif kernel notebook launch lifecycl configur",
        "Solution_preprocessed_content":"perform upgrad modif kernel notebook launch lifecycl configur",
        "Solution_readability":17.4,
        "Solution_reading_time":3.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":30.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":143.1018663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I need to use the function tsCV on azure machine learning studio to evaluate models of forecast, but i got the error <\/p>\n\n<pre><code>could not find function \"tsCV\n<\/code><\/pre>\n\n<p>I'm trying to update the forecast package, but no package are loaded.\nI followed this tutorial\n<a href=\"http:\/\/blog.revolutionanalytics.com\/2015\/10\/using-minicran-in-azure-ml.html\" rel=\"noreferrer\">http:\/\/blog.revolutionanalytics.com\/2015\/10\/using-minicran-in-azure-ml.html<\/a>\nand \n<a href=\"https:\/\/blog.tallan.com\/2016\/12\/27\/adding-r-packages-in-azure-ml\/\" rel=\"noreferrer\">https:\/\/blog.tallan.com\/2016\/12\/27\/adding-r-packages-in-azure-ml\/<\/a>\nbut i dont get the same result.\nNo packages are load.<\/p>\n\n<p>I need an example of a package with R code that works o Azure ML or an update of forecast package to use tsCV function.<\/p>",
        "Challenge_closed_time":1537192338792,
        "Challenge_comment_count":0,
        "Challenge_created_time":1536677172073,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52278613",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":11.31,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":143.1018663889,
        "Challenge_title":"Add custom packages to Azure Machine Learing Studio",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":404,
        "Challenge_word_count":95,
        "Platform":"Stack Overflow",
        "Poster_created_time":1515518171123,
        "Poster_location":null,
        "Poster_reputation_count":1108.0,
        "Poster_view_count":183.0,
        "Solution_body":"<p>I have installed the latest version of the forecast package and here are the steps I followed during the installation. <\/p>\n\n<ol>\n<li>Download latest version of CRAN<\/li>\n<li>Be sure that tsCV is working locally<\/li>\n<li>Zip all the dependencies + forecast package<\/li>\n<li>Zip all the generated zips together and upload it to the AMLStudio<\/li>\n<li>Run the following code:<\/li>\n<\/ol>\n\n<blockquote>\n<pre><code>install.packages(\"src\/glue.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/stringi.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/assertthat.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/fansi.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/utf8.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/stringr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/labeling.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/munsell.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/R6.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RColorBrewer.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/cli.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/crayon.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/pillar.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/xts.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/TTR.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/curl.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/digest.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/gtable.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/lazyeval.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/plyr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/reshape2.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/rlang.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/scales.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/tibble.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/viridisLite.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/withr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/quadprog.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/quantmod.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/colorspace.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/fracdiff.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/ggplot2.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/lmtest.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/magrittr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/Rcpp.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/timeDate.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/tseries.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/urca.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/uroot.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/zoo.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RcppArmadillo.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/forecast.zip\", lib = \".\", repos = NULL, verbose = TRUE)\n\nlibrary(forecast, lib.loc=\".\", verbose=TRUE)\nfar2 &lt;- function(x, h){forecast(Arima(x, order=c(2,0,0)), h=h)}\ne &lt;- tsCV(lynx, far2, h=1)\n<\/code><\/pre>\n<\/blockquote>\n\n<p><a href=\"https:\/\/drive.google.com\/open?id=10Bj0RGCmRFrRECLQrVc26nbx3T-bNSL6\" rel=\"nofollow noreferrer\">Here is the zip I have generated:<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/bbowH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bbowH.png\" alt=\"My experiment\"><\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_gpt_summary":"involv download latest version cran zip depend forecast packag upload amlstudio run instal load packag final tscv function evalu forecast model link zip file gener",
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"instal latest version forecast packag step instal download latest version cran tscv local zip depend forecast packag zip gener zip upload amlstudio run instal packag src glue zip lib repo null verbos instal packag src stringi zip lib repo null verbos instal packag src assertthat zip lib repo null verbos instal packag src fansi zip lib repo null verbos instal packag src utf zip lib repo null verbos instal packag src stringr zip lib repo null verbos instal packag src label zip lib repo null verbos instal packag src munsel zip lib repo null verbos instal packag src zip lib repo null verbos instal packag src rcolorbrew zip lib repo null verbos instal packag src cli zip lib repo null verbos instal packag src crayon zip lib repo null verbos instal packag src pillar zip lib repo null verbos instal packag src xt zip lib repo null verbos instal packag src ttr zip lib repo null verbos instal packag src curl zip lib repo null verbos instal packag src digest zip lib repo null verbos instal packag src gtabl zip lib repo null verbos instal packag src lazyev zip lib repo null verbos instal packag src plyr zip lib repo null verbos instal packag src reshap zip lib repo null verbos instal packag src rlang zip lib repo null verbos instal packag src scale zip lib repo null verbos instal packag src tibbl zip lib repo null verbos instal packag src viridislit zip lib repo null verbos instal packag src withr zip lib repo null verbos instal packag src quadprog zip lib repo null verbos instal packag src quantmod zip lib repo null verbos instal packag src colorspac zip lib repo null verbos instal packag src fracdiff zip lib repo null verbos instal packag src ggplot zip lib repo null verbos instal packag src lmtest zip lib repo null verbos instal packag src magrittr zip lib repo null verbos instal packag src rcpp zip lib repo null verbos instal packag src timed zip lib repo null verbos instal packag src tseri zip lib repo null verbos instal packag src urca zip lib repo null verbos instal packag src uroot zip lib repo null verbos instal packag src zoo zip lib repo null verbos instal packag src rcpparmadillo zip lib repo null verbos instal packag src forecast zip lib repo null verbos librari forecast lib loc verbos function forecast arima order tscv lynx zip gener",
        "Solution_preprocessed_content":"instal latest version forecast packag step instal download latest version cran tscv local zip depend forecast packag zip gener zip upload amlstudio run zip gener",
        "Solution_readability":14.4,
        "Solution_reading_time":51.58,
        "Solution_score_count":2.0,
        "Solution_sentence_count":47.0,
        "Solution_word_count":337.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1378136257732,
        "Answerer_location":"Budapest, Hungary",
        "Answerer_reputation_count":8162.0,
        "Answerer_view_count":283.0,
        "Challenge_adjusted_solved_time":0.0478905556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on Azure ML implementation on text analytics with NLTK, the following execution is throwing <\/p>\n\n<pre><code>AssertionError: 1 columns passed, passed data had 2 columns\\r\\nProcess returned with non-zero exit code 1\n<\/code><\/pre>\n\n<p>Below is the code <\/p>\n\n<pre><code># The script MUST include the following function,\n# which is the entry point for this module:\n# Param&lt;dataframe1&gt;: a pandas.DataFrame\n# Param&lt;dataframe2&gt;: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    # import required packages\n    import pandas as pd\n    import nltk\n    import numpy as np\n    # tokenize the review text and store the word corpus\n    word_dict = {}\n    token_list = []\n    nltk.download(info_or_id='punkt', download_dir='C:\/users\/client\/nltk_data')\n    nltk.download(info_or_id='maxent_treebank_pos_tagger', download_dir='C:\/users\/client\/nltk_data')\n    for text in dataframe1[\"tweet_text\"]:\n        tokens = nltk.word_tokenize(text.decode('utf8'))\n        tagged = nltk.pos_tag(tokens)\n\n\n      # convert feature vector to dataframe object\n    dataframe_output = pd.DataFrame(tagged, columns=['Output'])\n    return [dataframe_output]\n<\/code><\/pre>\n\n<p>Error is throwing here <\/p>\n\n<pre><code> dataframe_output = pd.DataFrame(tagged, columns=['Output'])\n<\/code><\/pre>\n\n<p>I suspect this to be the tagged data type passed to dataframe, can some one let me know the right approach to add this to dataframe.<\/p>",
        "Challenge_closed_time":1471040769603,
        "Challenge_comment_count":0,
        "Challenge_created_time":1471040597197,
        "Challenge_favorite_count":3,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38927230",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":18.6,
        "Challenge_score_count":7,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.0478905556,
        "Challenge_title":"Panda AssertionError columns passed, passed data had 2 columns",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":48200,
        "Challenge_word_count":158,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370924418390,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":1748.0,
        "Poster_view_count":339.0,
        "Solution_body":"<p>Try this:<\/p>\n\n<pre><code>dataframe_output = pd.DataFrame(tagged, columns=['Output', 'temp'])\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"creat datafram object tag data explicitli specifi column output temp achiev datafram output datafram tag column output temp",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"datafram output datafram tag column output temp",
        "Solution_preprocessed_content":null,
        "Solution_readability":9.4,
        "Solution_reading_time":1.5,
        "Solution_score_count":13.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":7.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1297232105368,
        "Answerer_location":null,
        "Answerer_reputation_count":27164.0,
        "Answerer_view_count":3016.0,
        "Challenge_adjusted_solved_time":0.6067841667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I have this code to write out a json file to be connected to via endpoint. The file is pretty standard in that it has the location of how to connect to the endpoint as well as some data.<\/p>\n<pre><code>%%writefile default-pred.json\n{   PROJECT_ID:&quot;msds434-gcp&quot;,\n    REGION:&quot;us-central1&quot;,\n    ENDPOINT_ID:&quot;2857701089334001664&quot;,\n    INPUT_DATA_FILE:&quot;INPUT-JSON&quot;,\n    &quot;instances&quot;: [\n    {&quot;age&quot;: 39,\n    &quot;bill_amt_1&quot;: 47174,\n    &quot;bill_amt_2&quot;: 47974,\n    &quot;bill_amt_3&quot;: 48630,\n    &quot;bill_amt_4&quot;: 50803,\n    &quot;bill_amt_5&quot;: 30789,\n    &quot;bill_amt_6&quot;: 15874,\n    &quot;education_level&quot;: &quot;1&quot;,\n    &quot;limit_balance&quot;: 50000,\n    &quot;marital_status&quot;: &quot;2&quot;,\n    &quot;pay_0&quot;: 0,\n    &quot;pay_2&quot;:0,\n    &quot;pay_3&quot;: 0,\n    &quot;pay_4&quot;: 0,\n    &quot;pay_5&quot;: &quot;0&quot;,\n    &quot;pay_6&quot;: &quot;0&quot;,\n    &quot;pay_amt_1&quot;: 1800,\n    &quot;pay_amt_2&quot;: 2000,\n    &quot;pay_amt_3&quot;: 3000,\n    &quot;pay_amt_4&quot;: 2000,\n    &quot;pay_amt_5&quot;: 2000,\n    &quot;pay_amt_6&quot;: 2000,\n    &quot;sex&quot;: &quot;1&quot;\n    }\n  ]\n    }\n<\/code><\/pre>\n<p>Then I have this trying to connect to the file and then taking the information to connect to the end point in question. I know the information is right as it's the exact code from GCP.<\/p>\n<pre><code>!curl \\\n-X POST \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-prediction-aiplatform.googleapis.com\/v1alpha1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID:predict \\\n-d &quot;@default-pred.json&quot;\n<\/code><\/pre>\n<p>So from the information I have I would expect it to parse the information I have and connect to the endpoint, but obviously I have my file wrong somehow. Any idea what it is?<\/p>\n<pre><code>{\n  &quot;error&quot;: {\n    &quot;code&quot;: 400,\n    &quot;message&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;PROJECT_ID\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;REGION\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;ENDPOINT_ID\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;INPUT_DATA_FILE\\&quot;: Cannot find field.&quot;,\n    &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;,\n    &quot;details&quot;: [\n      {\n        &quot;@type&quot;: &quot;type.googleapis.com\/google.rpc.BadRequest&quot;,\n        &quot;fieldViolations&quot;: [\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;PROJECT_ID\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;REGION\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;ENDPOINT_ID\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;INPUT_DATA_FILE\\&quot;: Cannot find field.&quot;\n          }\n        ]\n      }\n    ]\n  }\n}\n<\/code><\/pre>\n<p>What am I missing here?<\/p>",
        "Challenge_closed_time":1653863593763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653862009460,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72427594",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":40.65,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":0.4400841667,
        "Challenge_title":"Questions on json and GCP",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":77,
        "Challenge_word_count":281,
        "Platform":"Stack Overflow",
        "Poster_created_time":1632597456472,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>The data file should only include the data.<\/p>\n<p>You've included <code>PROJECT_ID<\/code>, <code>REGION<\/code>, <code>ENDPOINT<\/code> and should not.<\/p>\n<p>These need to be set in the (bash) environment <strong>before<\/strong> you issue the <code>curl<\/code> command:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>PROJECT_ID=&quot;msds434-gcp&quot;\nREGION=&quot;us-central1&quot;\nENDPOINT_ID=&quot;2857701089334001664&quot;\n\ncurl \\\n--request POST \\\n--header &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n--header &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-prediction-aiplatform.googleapis.com\/v1alpha1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID:predict \\\n--data &quot;@default-pred.json&quot;\n<\/code><\/pre>\n<p>The file <code>default-pred.json<\/code> should <strike>probably (I can never find this service's methods in <a href=\"https:\/\/developers.google.com\/apis-explorer\" rel=\"nofollow noreferrer\">APIs Explorer<\/a>!<\/strike>) just be:<\/p>\n<pre><code>{\n  instances&quot;: [\n    { &quot;age&quot;: 39,\n      &quot;bill_amt_1&quot;: 47174,\n      &quot;bill_amt_2&quot;: 47974,\n      &quot;bill_amt_3&quot;: 48630,\n      &quot;bill_amt_4&quot;: 50803,\n      &quot;bill_amt_5&quot;: 30789,\n      &quot;bill_amt_6&quot;: 15874,\n      &quot;education_level&quot;: &quot;1&quot;,\n      &quot;limit_balance&quot;: 50000,\n      &quot;marital_status&quot;: &quot;2&quot;,\n      &quot;pay_0&quot;: 0,\n      &quot;pay_2&quot;:0,\n      &quot;pay_3&quot;: 0,\n      &quot;pay_4&quot;: 0,\n      &quot;pay_5&quot;: &quot;0&quot;,\n      &quot;pay_6&quot;: &quot;0&quot;,\n      &quot;pay_amt_1&quot;: 1800,\n      &quot;pay_amt_2&quot;: 2000,\n      &quot;pay_amt_3&quot;: 3000,\n      &quot;pay_amt_4&quot;: 2000,\n      &quot;pay_amt_5&quot;: 2000,\n      &quot;pay_amt_6&quot;: 2000,\n      &quot;sex&quot;: &quot;1&quot;\n    }\n  ]\n}\n<\/code><\/pre>\n<p>See the documentation for the <code>aiplatform<\/code> <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/reference\/rest\/v1\/projects\/predict\" rel=\"nofollow noreferrer\"><code>predict<\/code><\/a> method as this explains this.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"file data region endpoint region endpoint set bash environ curl file default pred json instanc data",
        "Solution_last_edit_time":1653864193883,
        "Solution_link_count":3.0,
        "Solution_original_content":"data file data region endpoint set bash environ curl msd region central endpoint curl request header author bearer gcloud auth print access token header type json http central predict aiplatform googleapi com valpha locat region endpoint endpoint predict data default pred json file default pred json probabl servic api explor instanc ag amt amt amt amt amt amt educ level limit balanc marit statu pai pai pai pai pai pai pai amt pai amt pai amt pai amt pai amt pai amt sex document aiplatform predict explain",
        "Solution_preprocessed_content":"data file data set environ file probabl document explain",
        "Solution_readability":19.8,
        "Solution_reading_time":27.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":135.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1464811778510,
        "Answerer_location":null,
        "Answerer_reputation_count":196.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":7.7506297223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><a href=\"https:\/\/developer.nvidia.com\/nvidia-triton-inference-server\" rel=\"nofollow noreferrer\">NVIDIA Triton<\/a>\u00a0vs\u00a0<a href=\"https:\/\/pytorch.org\/serve\/\" rel=\"nofollow noreferrer\">TorchServe<\/a>\u00a0for SageMaker inference? When to recommend each?<\/p>\n<p>Both are modern, production grade inference servers. TorchServe is the DLC default inference server for PyTorch models. Triton is also supported for PyTorch inference on SageMaker.<\/p>\n<p>Anyone has a good comparison matrix for both?<\/p>",
        "Challenge_closed_time":1663971240670,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663943338403,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73829280",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":7.15,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":7.7506297223,
        "Challenge_title":"NVIDIA Triton vs TorchServe for SageMaker Inference",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":12,
        "Challenge_word_count":57,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>Important notes to add here where both serving stacks differ:<\/p>\n<p>TorchServe does not provide the Instance Groups feature that Triton does (that is, stacking many copies of the same model or even different models onto the same GPU). This is a major advantage for both realtime and batch use-cases, as the performance increase is almost proportional to the model replication count (i.e. 2 copies of the model get you almost twice the throughput and half the latency; check out a BERT benchmark of this here). Hard to match a feature that is almost like having 2+ GPU's for the price of one.\nif you are deploying PyTorch DL models, odds are you often want to accelerate them with GPU's. TensorRT (TRT) is a compiler developed by NVIDIA that automatically quantizes and optimizes your model graph, which represents another huge speed up, depending on GPU architecture and model. It is understandably so probably the best way of automatically optimizing your model to run efficiently on GPU's and make good use of TensorCores. Triton has native integration to run TensorRT engines as they're called (even automatically converting your model to a TRT engine via config file), while TorchServe does not (even though you can use TRT engines with it).\nThere is more parity between both when it comes to other important serving features: both have dynamic batching support, you can define inference DAG's with both (not sure if the latter works with TorchServe on SageMaker without a big hassle), and both support custom code\/handlers instead of just being able to serve a model's forward function.<\/p>\n<p>Finally, MME on GPU (coming shortly) will be based on Triton, which is a valid argument for customers to get familiar with it so that they can quickly leverage this new feature for cost-optimization.<\/p>\n<p>Bottom line I think that Triton is just as easy (if not easier) ot use, a lot more optimized\/integrated for taking full advantage of the underlying hardware (and will be updated to keep being that way as newer GPU architectures are released, enabling an easy move to them), and in general blows TorchServe out of the water performance-wise when its optimization features are used in combination.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"nvidia triton instanc group featur torchserv advantag time batch triton nativ integr run tensorrt engin torchserv triton torchserv dynam batch defin infer dag unclear torchserv big hassl handler serv model forward function mme gpu come shortli base triton argument familiar quickli leverag",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"import note add serv stack torchserv instanc group featur triton stack copi model model gpu advantag realtim batch perform increas proport model replic count copi model twice throughput half latenc bert benchmark match featur gpu price deploi pytorch model odd acceler gpu tensorrt trt compil nvidia automat quantiz optim model graph repres huge speed depend gpu architectur model probabl automat optim model run effici gpu tensorcor triton nativ integr run tensorrt engin call automat convert model trt engin config file torchserv trt engin pariti come import serv featur dynam batch defin infer dag torchserv big hassl handler serv model forward function final mme gpu come shortli base triton argument familiar quickli leverag featur cost optim line triton easier optim integr advantag underli hardwar updat newer gpu architectur releas enabl gener blow torchserv water perform wise optim featur combin",
        "Solution_preprocessed_content":"import note add serv stack torchserv instanc group featur triton advantag realtim batch perform increas proport model replic count match featur gpu price deploi pytorch model odd acceler gpu tensorrt compil nvidia automat quantiz optim model graph repres huge speed depend gpu architectur model probabl automat optim model run effici gpu tensorcor triton nativ integr run tensorrt engin call torchserv pariti come import serv featur dynam batch defin infer dag serv model forward function final mme gpu base triton argument familiar quickli leverag featur line triton advantag underli hardwar gener blow torchserv water optim featur combin",
        "Solution_readability":15.0,
        "Solution_reading_time":27.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":363.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1337868050812,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":483.0,
        "Answerer_view_count":105.0,
        "Challenge_adjusted_solved_time":6.9429577778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I can't find the proper way to add dependencies to my Azure Container Instance for ML Inference.<\/p>\n<p>I basically started by following this tutorial : <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-train-deploy-notebook\" rel=\"nofollow noreferrer\">Train and deploy an image classification model with an example Jupyter Notebook<\/a><\/p>\n<p>It works fine.<\/p>\n<p>Now I want to deploy my trained TensorFlow model for inference. I tried many ways, but I was never able to add python dependencies to the Environment.<\/p>\n<h1>From the TensorFlow curated environment<\/h1>\n<p>Using <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/resource-curated-environments#inference-curated-environments-and-prebuilt-docker-images\" rel=\"nofollow noreferrer\">AzureML-tensorflow-2.4-ubuntu18.04-py37-cpu-inference<\/a> :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace\n\n\n# connect to your workspace\nws = Workspace.from_config()\n\n# names\nexperiment_name = &quot;my-experiment&quot;\nmodel_name = &quot;my-model&quot;\nenv_version=&quot;1&quot;\nenv_name=&quot;my-env-&quot;+env_version\nservice_name = str.lower(model_name + &quot;-service-&quot; + env_version)\n\n\n# create environment for the deploy\nfrom azureml.core.environment import Environment, DEFAULT_CPU_IMAGE\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.webservice import AciWebservice\n\n# get a curated environment\nenv = Environment.get(\n    workspace=ws, \n    name=&quot;AzureML-tensorflow-2.4-ubuntu18.04-py37-cpu-inference&quot;,\n# )\ncustom_env = env.clone(env_name)\ncustom_env.inferencing_stack_version='latest'\n\n# add packages\nconda_dep = CondaDependencies()\npython_packages = ['joblib', 'numpy', 'os', 'json', 'tensorflow']\nfor package in python_packages:\n    conda_dep.add_pip_package(package)\n    conda_dep.add_conda_package(package)\n\n# Adds dependencies to PythonSection of env\ncustom_env.python.user_managed_dependencies=True\ncustom_env.python.conda_dependencies=conda_dep\n\ncustom_env.register(workspace=ws)\n\n# create deployment config i.e. compute resources\naciconfig = AciWebservice.deploy_configuration(\n    cpu_cores=1,\n    memory_gb=1,\n    tags={&quot;experiment&quot;: experiment_name, &quot;model&quot;: model_name},\n)\n\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.model import Model\n\n# get the registered model\nmodel = Model(ws, model_name)\n\n# create an inference config i.e. the scoring script and environment\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=custom_env)\n\n# deploy the service\nservice = Model.deploy(\n    workspace=ws,\n    name=service_name,\n    models=[model],\n    inference_config=inference_config,\n    deployment_config=aciconfig,\n)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n<p>I get the following log :<\/p>\n<pre><code>\nAzureML image information: tensorflow-2.4-ubuntu18.04-py37-cpu-inference:20220110.v1\n\n\nPATH environment variable: \/opt\/miniconda\/envs\/amlenv\/bin:\/opt\/miniconda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\nPYTHONPATH environment variable: \n\nPip Dependencies\n---------------\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2022-01-24T10:21:09,855130300+00:00 - iot-server\/finish 1 0\n2022-01-24T10:21:09,856870100+00:00 - Exit code 1 is normal. Not restarting iot-server.\nabsl-py==0.15.0\napplicationinsights==0.11.10\nastunparse==1.6.3\nazureml-inference-server-http==0.4.2\ncachetools==4.2.4\ncertifi==2021.10.8\ncharset-normalizer==2.0.10\nclick==8.0.3\nFlask==1.0.3\nflatbuffers==1.12\ngast==0.3.3\ngoogle-auth==2.3.3\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngrpcio==1.32.0\ngunicorn==20.1.0\nh5py==2.10.0\nidna==3.3\nimportlib-metadata==4.10.0\ninference-schema==1.3.0\nitsdangerous==2.0.1\nJinja2==3.0.3\nKeras-Preprocessing==1.1.2\nMarkdown==3.3.6\nMarkupSafe==2.0.1\nnumpy==1.19.5\noauthlib==3.1.1\nopt-einsum==3.3.0\npandas==1.1.5\nprotobuf==3.19.1\npyasn1==0.4.8\npyasn1-modules==0.2.8\npython-dateutil==2.8.2\npytz==2021.3\nrequests==2.27.1\nrequests-oauthlib==1.3.0\nrsa==4.8\nsix==1.15.0\ntensorboard==2.7.0\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.4.0\ntensorflow-estimator==2.4.0\ntermcolor==1.1.0\ntyping-extensions==3.7.4.3\nurllib3==1.26.8\nWerkzeug==2.0.2\nwrapt==1.12.1\nzipp==3.7.0\n\n\nEntry script directory: \/var\/azureml-app\/.\n\nDynamic Python package installation is disabled.\nStarting AzureML Inference Server HTTP.\n\nAzure ML Inferencing HTTP server v0.4.2\n\n\nServer Settings\n---------------\nEntry Script Name: score.py\nModel Directory: \/var\/azureml-app\/azureml-models\/my-model\/1\nWorker Count: 1\nWorker Timeout (seconds): 300\nServer Port: 31311\nApplication Insights Enabled: false\nApplication Insights Key: None\n\n\nServer Routes\n---------------\nLiveness Probe: GET   127.0.0.1:31311\/\nScore:          POST  127.0.0.1:31311\/score\n\nStarting gunicorn 20.1.0\nListening at: http:\/\/0.0.0.0:31311 (69)\nUsing worker: sync\nBooting worker with pid: 100\nException in worker process\nTraceback (most recent call last):\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker\n    worker.init_process()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process\n    self.load_wsgi()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load\n    return self.load_wsgiapp()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 359, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/entry.py&quot;, line 1, in &lt;module&gt;\n    import create_app\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/create_app.py&quot;, line 4, in &lt;module&gt;\n    from routes_common import main\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/routes_common.py&quot;, line 32, in &lt;module&gt;\n    from aml_blueprint import AMLBlueprint\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/aml_blueprint.py&quot;, line 28, in &lt;module&gt;\n    main_module_spec.loader.exec_module(main)\n  File &quot;\/var\/azureml-app\/score.py&quot;, line 4, in &lt;module&gt;\n    import joblib\nModuleNotFoundError: No module named 'joblib'\nWorker exiting (pid: 100)\nShutting down: Master\nReason: Worker failed to boot.\n2022-01-24T10:21:13,851467800+00:00 - gunicorn\/finish 3 0\n2022-01-24T10:21:13,853259700+00:00 - Exit code 3 is not normal. Killing image.\n<\/code><\/pre>\n<h1>From a Conda specification<\/h1>\n<p>Same as before, but with a fresh environment from Conda specification and changing the <code>env_version<\/code> number :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># ...\n\n\nenv_version=&quot;2&quot;\n\n# ...\n\ncustom_env = Environment.from_conda_specification(name=env_name, file_path=&quot;my-env.yml&quot;)\ncustom_env.docker.base_image = DEFAULT_CPU_IMAGE\n\n# ...\n\n<\/code><\/pre>\n<p>with <code>my-env.yml<\/code> :<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: my-env\ndependencies:\n- python\n\n- pip:\n  - azureml-defaults\n  - azureml-sdk\n  - sklearn\n  - numpy\n  - matplotlib\n  - joblib\n  - uuid\n  - requests\n  - tensorflow\n\n<\/code><\/pre>\n<p>I get this log :<\/p>\n<pre><code>2022-01-24T11:06:54,887886931+00:00 - iot-server\/run \n2022-01-24T11:06:54,891839877+00:00 - rsyslog\/run \n2022-01-24T11:06:54,893640998+00:00 - gunicorn\/run \n2022-01-24T11:06:54,912032812+00:00 - nginx\/run \nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2022-01-24T11:06:55,398420960+00:00 - iot-server\/finish 1 0\n2022-01-24T11:06:55,414425146+00:00 - Exit code 1 is normal. Not restarting iot-server.\n\nPATH environment variable: \/opt\/miniconda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\nPYTHONPATH environment variable: \n\nPip Dependencies\n---------------\nbrotlipy==0.7.0\ncertifi==2020.6.20\ncffi @ file:\/\/\/tmp\/build\/80754af9\/cffi_1605538037615\/work\nchardet @ file:\/\/\/tmp\/build\/80754af9\/chardet_1605303159953\/work\nconda==4.9.2\nconda-package-handling @ file:\/\/\/tmp\/build\/80754af9\/conda-package-handling_1603018138503\/work\ncryptography @ file:\/\/\/tmp\/build\/80754af9\/cryptography_1605544449973\/work\nidna @ file:\/\/\/tmp\/build\/80754af9\/idna_1593446292537\/work\npycosat==0.6.3\npycparser @ file:\/\/\/tmp\/build\/80754af9\/pycparser_1594388511720\/work\npyOpenSSL @ file:\/\/\/tmp\/build\/80754af9\/pyopenssl_1605545627475\/work\nPySocks @ file:\/\/\/tmp\/build\/80754af9\/pysocks_1594394576006\/work\nrequests @ file:\/\/\/tmp\/build\/80754af9\/requests_1592841827918\/work\nruamel-yaml==0.15.87\nsix @ file:\/\/\/tmp\/build\/80754af9\/six_1605205313296\/work\ntqdm @ file:\/\/\/tmp\/build\/80754af9\/tqdm_1605303662894\/work\nurllib3 @ file:\/\/\/tmp\/build\/80754af9\/urllib3_1603305693037\/work\n\nStarting HTTP server\n2022-01-24T11:06:59,701365128+00:00 - gunicorn\/finish 127 0\n.\/run: line 127: exec: gunicorn: not found\n2022-01-24T11:06:59,706177784+00:00 - Exit code 127 is not normal. Killing image.\n    \n<\/code><\/pre>\n<p>I really don't know what I'm missing, and I've been searching for too long already (Azure docs, SO, ...).<\/p>\n<p>Thanks for your help !<\/p>\n<p>Edit : Non-exhaustive list of solutions I tried :<\/p>\n<ul>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/65159778\/how-to-create-azureml-environement-and-add-required-packages\">How to create AzureML environement and add required packages<\/a><\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/65159308\/how-to-use-existing-conda-environment-as-a-azureml-environment\">how to use existing conda environment as a AzureML environment<\/a><\/li>\n<li>...<\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-environments#environment-building-caching-and-reuse\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-environments#environment-building-caching-and-reuse<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments#add-packages-to-an-environment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments#add-packages-to-an-environment<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-inferencing-gpus\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-inferencing-gpus<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-a-deployment-configuration\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-a-deployment-configuration<\/a><\/li>\n<li>...<\/li>\n<\/ul>",
        "Challenge_closed_time":1643188448820,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643025448957,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1643163454172,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70833499",
        "Challenge_link_count":13,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":19.5,
        "Challenge_reading_time":162.47,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":94,
        "Challenge_solved_time":45.2777397222,
        "Challenge_title":"AzureML Environment for Inference : can't add pip packages to dependencies",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":902,
        "Challenge_word_count":789,
        "Platform":"Stack Overflow",
        "Poster_created_time":1337868050812,
        "Poster_location":"Paris, France",
        "Poster_reputation_count":483.0,
        "Poster_view_count":105.0,
        "Solution_body":"<p>OK, I got it working : I started over from scratch and it worked.<\/p>\n<p>I have no idea what was wrong in all my preceding tries, and that is terrible.<\/p>\n<p>Multiple problems and how I (think I) solved them :<\/p>\n<ul>\n<li><code>joblib<\/code> : I actually didn't need it to load my Keras model. But the problem was not with this specific library, rather that I couldn't add dependencies to the inference environment.<\/li>\n<li><code>Environment<\/code> : finally, I was only able to make things work with a custom env : <code>Environment.from_conda_specification(name=version, file_path=&quot;conda_dependencies.yml&quot;)<\/code> . I haven't been able to add my libraries (or specify a specific package version) to a &quot;currated environment&quot;. I don't know why though...<\/li>\n<li><code>TensorFlow<\/code> : last problem I had was that I trained and registered my model in AzureML Notebook's <code>azureml_py38_PT_TF<\/code> kernel (<code>tensorflow==2.7.0<\/code>), and tried to load it in the inference Docker image (<code>tensorflow==2.4.0<\/code>). So I had to specify the version of TensorFlow I wanted to use in the inference image (which required the previous point to be solved).<\/li>\n<\/ul>\n<p>What finally worked :<\/p>\n<ul>\n<li>notebook.ipynb<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>import uuid\nfrom azureml.core import Workspace, Environment, Model\nfrom azureml.core.webservice import AciWebservice\nfrom azureml.core.model import InferenceConfig\n\n\nversion = &quot;test-&quot;+str(uuid.uuid4())[:8]\n\nenv = Environment.from_conda_specification(name=version, file_path=&quot;conda_dependencies.yml&quot;)\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=env)\n\nws = Workspace.from_config()\nmodel = Model(ws, model_name)\n\naci_config = AciWebservice.deploy_configuration(\n    cpu_cores=1,\n    memory_gb=1,\n)\n\nservice = Model.deploy(\n    workspace=ws,\n    name=version,\n    models=[model],\n    inference_config=inference_config,\n    deployment_config=aci_config,\n    overwrite=True,\n)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n<ul>\n<li>conda_dependencies.yml<\/li>\n<\/ul>\n<pre class=\"lang-yaml prettyprint-override\"><code>channels:\n- conda-forge\ndependencies:\n- python=3.8\n- pip:\n  - azureml-defaults\n  - azureml-sdk\n  - numpy\n  - tensorflow==2.7.0\n\n<\/code><\/pre>\n<ul>\n<li>score.py<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nimport numpy as np\nimport tensorflow as tf\n\n\ndef init():\n    global model\n\n    model_path = os.path.join(os.getenv(&quot;AZUREML_MODEL_DIR&quot;), &quot;model\/data\/model&quot;)\n    model = tf.keras.models.load_model(model_path)\n\n\n\ndef run(raw_data):\n    data = np.array(json.loads(raw_data)[&quot;data&quot;])\n    y_hat = model.predict(data)\n\n    return y_hat.tolist()\n\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"creat environ conda file specifi version tensorflow infer imag add librari specifi packag version curat environ share final import librari creat environ defin infer configur deploi model specifi score",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"start scratch idea preced tri terribl multipl joblib load kera model librari add depend infer environ environ final env environ conda version file path conda depend yml haven add librari specifi packag version currat environ tensorflow train regist model notebook kernel tensorflow tri load infer docker imag tensorflow specifi version tensorflow infer imag previou final notebook ipynb import uuid core import workspac environ model core webservic import aciwebservic core model import inferenceconfig version test str uuid uuid env environ conda version file path conda depend yml infer config inferenceconfig entri score environ env workspac config model model model aci config aciwebservic deploi configur cpu core memori servic model deploi workspac version model model infer config infer config deploy config aci config overwrit servic wait deploy output conda depend yml channel conda forg depend pip default sdk numpi tensorflow score import import json import numpi import tensorflow init global model model path path getenv model dir model data model model kera model load model model path run raw data data arrai json load raw data data hat model predict data return hat tolist",
        "Solution_preprocessed_content":"start scratch idea preced tri terribl multipl load kera model librari add depend infer environ final env haven add librari currat environ train regist model notebook kernel tri load infer docker imag specifi version tensorflow infer imag final",
        "Solution_readability":13.8,
        "Solution_reading_time":35.99,
        "Solution_score_count":1.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":267.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1406731060412,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":139.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":3215.9254886111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to set up my first SageMaker Studio so my team and myself can run some post processing scripts in a shared environment but I'm having issues.<\/p>\n<p>I've followed the steps in this video(<a href=\"https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices<\/a>) which are:<\/p>\n<ol>\n<li>Select Standard setup<\/li>\n<li>Select AWS Identity and Access Management (IAM)<\/li>\n<li>Under permissions - Create and select new execution role<\/li>\n<li>Under Network and storage - Select VPC, Subnet and Security group<\/li>\n<li>Hit the submit button at the bottom of the page.<\/li>\n<\/ol>\n<p>In the video, he clicks submit and is taken to the control panel where he starts the next phase of adding users, however I'm greeted with this error.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/g4k2g.png\" rel=\"nofollow noreferrer\"> Resource limit Error<\/a><\/p>\n<p>I've checked my Registered domains under route 53 and it says No domains to display, I've also checked my S2 and I have no instances so I have no idea where the 2 domains being utilized are.<\/p>\n<p>My dashboard, image and Notebooks are all empty so as far as I know there's nothing setup on this Sage Maker account.<\/p>\n<p>Could anyone tell me how to resolve this error?<\/p>",
        "Challenge_closed_time":1626975605176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615398273417,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66570138",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":17.72,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":3215.9254886111,
        "Challenge_title":"How to setup AWS sagemaker - Resource limit Error",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":237,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530468231707,
        "Poster_location":null,
        "Poster_reputation_count":357.0,
        "Poster_view_count":72.0,
        "Solution_body":"<p>You can have maximum 1 studio domain per region, by the default limits. Though, it seems like you have two domains already provisioned. Try to delete all the domains through the AWS cli and recreate with the AWS Management Console.<\/p>\n<p>Unfortunately, AWS Management Console cannot visualize more than one Studio domain.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"delet domain cli recreat consol note consol visual studio domain",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"maximum studio domain region default limit domain provis delet domain cli recreat consol unfortun consol visual studio domain",
        "Solution_preprocessed_content":"maximum studio domain region default limit domain provis delet domain cli recreat consol unfortun consol visual studio domain",
        "Solution_readability":8.3,
        "Solution_reading_time":4.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":51.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1476987111723,
        "Answerer_location":"Canada",
        "Answerer_reputation_count":2021.0,
        "Answerer_view_count":134.0,
        "Challenge_adjusted_solved_time":1.4678261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have 50TB of uncompressed data (images) that is in dozens of tar.gz files in S3. I'm training tensorflow models with a dozen of these tar.gz files at a time. I would like to use a Sagemaker training job to pull this data and unpack it before training. Is this possible? Do I have to change the way that the data is stored before running training?<\/p>",
        "Challenge_closed_time":1611858171047,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611852886873,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65941675",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":3.5,
        "Challenge_reading_time":4.98,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.4678261111,
        "Challenge_title":"Can gzip tar files be used for training data in Sagemaker?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":433,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1476987111723,
        "Poster_location":"Canada",
        "Poster_reputation_count":2021.0,
        "Poster_view_count":134.0,
        "Solution_body":"<p><strong>Short Answer<\/strong> : No<\/p>\n<p><strong>Long Answer<\/strong>:\nThe recommended way to use Sagemaker with very large datasets is to use the Pipe API (as opposed to the File Api) which streams data to the training image rather than downloading the data. To take advantage of the Pipe API the data will need to be in one of the supported file types: <strong>text records, TFRecord or Protobuf<\/strong><\/p>\n<p>The benefits are<\/p>\n<ol>\n<li>reducing delay when the container is launched<\/li>\n<li>not needing to scale the instance storage to the size of the training data<\/li>\n<li>increasing throughput by moving most preprocessing before model training<\/li>\n<\/ol>\n<p>References:<\/p>\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/<\/a><\/li>\n<li><a href=\"https:\/\/julsimon.medium.com\/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233\" rel=\"nofollow noreferrer\">https:\/\/julsimon.medium.com\/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233<\/a> (This is a fantastic resource which answers a lot of questions regarding using Sagemaker on very large datasets)<\/li>\n<li><a href=\"https:\/\/julsimon.medium.com\/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c\" rel=\"nofollow noreferrer\">https:\/\/julsimon.medium.com\/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c<\/a><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"gzip tar file train data larg dataset pipe api stream data train imag download data advantag pipe api data file type text record tfrecord protobuf",
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_original_content":"short larg dataset pipe api oppos file api stream data train imag download data advantag pipe api data file type text record tfrecord protobuf benefit reduc delai launch scale instanc storag size train data increas throughput move preprocess model train http com blog pipe input mode algorithm http julsimon medium com tensorflow fantast resourc larg dataset http julsimon medium com dive tensorflow train",
        "Solution_preprocessed_content":"short larg dataset pipe api stream data train imag download data advantag pipe api data file type text record tfrecord protobuf benefit reduc delai launch scale instanc storag size train data increas throughput move preprocess model train",
        "Solution_readability":20.1,
        "Solution_reading_time":21.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":132.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1592673441292,
        "Answerer_location":null,
        "Answerer_reputation_count":38.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":0.1245758334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been working with ML Studio (classic) and facing a problem with &quot;Execute Python&quot; scripts. I have noticed that it takes additional time to perform some internal tasks after which it starts executing the actual Python code in ML Studio. This delay has caused an increased time of 40-60 seconds per module which is aggregating and causing a delay of 400-500 seconds per execution when consumed through Batch Execution System or on running the experiments manually. (I've multiple Modules of &quot;Execute Python&quot; scripts)<\/p>\n<p>For instance - If I run a code in my local system, suppose it takes 2-3 seconds. The same would consume 50-60 seconds in Azure ML Studio.<\/p>\n<p>Can you please help understand the reason behind this or any optimization that can be done?<\/p>\n<p>Regards,\nAnant<\/p>",
        "Challenge_closed_time":1593695267950,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593694819477,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62696966",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":11.11,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.1245758334,
        "Challenge_title":"Why does Azure ML Studio (classic) take additional time to execute Python Scripts?",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":166,
        "Challenge_word_count":140,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582179684312,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":601.0,
        "Poster_view_count":94.0,
        "Solution_body":"<p>The known limitations of Machine Learning Studio (classic) are:<\/p>\n<p>The Python runtime is sandboxed and does not allow access to the network or to the local file system in a persistent manner.<\/p>\n<p>All files saved locally are isolated and deleted once the module finishes. The Python code cannot access most directories on the machine it runs on, the exception being the current directory and its subdirectories.<\/p>\n<p>When you provide a zipped file as a resource, the files are copied from your workspace to the experiment execution space, unpacked, and then used. Copying and unpacking resources can consume memory.<\/p>\n<p>The module can output a single data frame. It's not possible to return arbitrary Python objects such as trained models directly back to the Studio (classic) runtime. However, you can write objects to storage or to the workspace. Another option is to use pickle to serialize multiple objects into a byte array and then return the array inside a data frame.<\/p>\n<p>Hope this helps!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"limit studio classic runtim sandbox allow access network local file persist file save local isol delet modul finish access directori run except directori subdirectori zip file resourc file copi workspac execut space unpack copi unpack resourc consum memori modul output singl data frame return arbitrari object train model directli studio classic runtim write object storag workspac option pickl serial multipl object byte arrai return arrai insid data frame hope",
        "Solution_preprocessed_content":"limit studio runtim sandbox allow access network local file persist file save local isol delet modul finish access directori run except directori subdirectori zip file resourc file copi workspac execut space unpack copi unpack resourc consum memori modul output singl data frame return arbitrari object train model directli studio runtim write object storag workspac option pickl serial multipl object byte arrai return arrai insid data frame hope",
        "Solution_readability":9.6,
        "Solution_reading_time":12.57,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":162.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":0.5330241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I followed the aws documentation ( <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config<\/a>) to create a model and to use that model, i coded for a serverless endpoint config (sample code below) ,I have all the required values  but this throws an error below and i'm not sure why<\/p>\n<p>parameter validation failed unknown parameter inProductVariants [ 0 ]: &quot;ServerlessConfig&quot;, must be one of : VairantName, ModelName, InitialInstanceCount , Instancetype...<\/p>\n<pre><code>response = client.create_endpoint_config(\n   EndpointConfigName=&quot;abc&quot;,\n   ProductionVariants=[\n        {\n            &quot;ModelName&quot;: &quot;foo&quot;,\n            &quot;VariantName&quot;: &quot;variant-1&quot;,\n            &quot;ServerlessConfig&quot;: {\n                &quot;MemorySizeInMB&quot;: 1024,\n                &quot;MaxConcurrency&quot;: 2\n            }\n        } \n    ]\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1645069539320,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645067620433,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1645073711768,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71152047",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":21.6,
        "Challenge_reading_time":14.15,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.5330241667,
        "Challenge_title":"how to create a serverless endpoint in sagemaker?",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":161,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1590797441983,
        "Poster_location":null,
        "Poster_reputation_count":525.0,
        "Poster_view_count":98.0,
        "Solution_body":"<p>You are probably using <strong>old boto3<\/strong> version. <code>ServerlessConfig<\/code> is a very new configuration option. You need to upgrade to the latest version (1.21.1) if possible.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_gpt_summary":"upgrad latest version boto messag",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"probabl boto version serverlessconfig configur option upgrad latest version",
        "Solution_preprocessed_content":"probabl boto version configur option upgrad latest version",
        "Solution_readability":8.1,
        "Solution_reading_time":2.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1523298968403,
        "Answerer_location":null,
        "Answerer_reputation_count":1754.0,
        "Answerer_view_count":197.0,
        "Challenge_adjusted_solved_time":140.9113833333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to train a model using the sagemaker library. So far, my code is the following:<\/p>\n\n<pre><code>container = get_image_uri(boto3.Session().region_name,\n                      'xgboost', \n                      repo_version='0.90-1')\n\nestimator = sagemaker.estimator.Estimator(container, \n                                          role = 'AmazonSageMaker-ExecutionRole-20190305TXXX',\n                                          train_instance_count = 1,\n                                          train_instance_type = 'ml.m4.2xlarge',\n                                          output_path = 's3:\/\/antifraud\/production\/',\n                                          hyperparameters = {'num_rounds':'400',\n                                                             'objective':'binary:logistic',\n                                                             'eval_metric':'error@0.1'})\n\ntrain_config = training_config(estimator=estimator,\n                               inputs = {'train':'s3:\/\/antifraud\/production\/train',\n                                         'validation':'s3:\/\/-antifraud\/production\/validation'})\n<\/code><\/pre>\n\n<p>And I get an error parsing the hyperparameters. This commands gives me a configuration JSON output in the console. I have been able to run a training job using boto3 with the configuration as Json, so I have figured out that the thing I am missing in my json configuration generated by my code is the content_type parameter, which should be there as follow:<\/p>\n\n<pre><code>\"InputDataConfig\": [\n    {\n        \"ChannelName\": \"train\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"S3Prefix\",\n                \"S3Uri\": \"s3:\/\/antifraud\/production\/data\/train\",\n                \"S3DataDistributionType\": \"FullyReplicated\" \n            }\n        },\n        \"ContentType\": \"text\/csv\",\n        \"CompressionType\": \"None\"\n    },\n    {\n        \"ChannelName\": \"validation\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"S3Prefix\",\n                \"S3Uri\": \"s3:\/\/antifraud\/production\/validation\",\n                \"S3DataDistributionType\": \"FullyReplicated\"\n            }\n        },\n        \"ContentType\": \"text\/csv\",\n        \"CompressionType\": \"None\"\n    }\n]\n<\/code><\/pre>\n\n<p>I have tried coding content_type = 'text\/csv' in container, estimator and train_config as parameter and also inside inputs as another key of the dictionary, with no success. How could I make this work?<\/p>",
        "Challenge_closed_time":1568804066440,
        "Challenge_comment_count":1,
        "Challenge_created_time":1568296785460,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57908395",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":18.0,
        "Challenge_reading_time":24.56,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":140.9113833333,
        "Challenge_title":"How can I specify content_type in a training job of XGBoost from Sagemaker in Python?",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":644,
        "Challenge_word_count":182,
        "Platform":"Stack Overflow",
        "Poster_created_time":1523298968403,
        "Poster_location":null,
        "Poster_reputation_count":1754.0,
        "Poster_view_count":197.0,
        "Solution_body":"<p>I have solved it using s3_input objects:<\/p>\n\n<pre><code>s3_input_train = sagemaker.s3_input(s3_data='s3:\/\/antifraud\/production\/data\/{domain}-{product}-{today}\/train_data.csv',\ncontent_type='text\/csv')\ns3_input_validation = sagemaker.s3_input(s3_data='s3:\/\/antifraud\/production\/data\/{domain}-{product}-{today}\/validation_data.csv',\ncontent_type='text\/csv')\n\ntrain_config = training_config(estimator=estimator,\ninputs = {'train':s3_input_train,\n          'validation':s3_input_validation})\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"input object add type paramet configur json gener input object creat train data type paramet set text csv object pass input train config function estim gener configur json",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"input object input train input data antifraud data domain todai train data csv type text csv input input data antifraud data domain todai data csv type text csv train config train config estim estim input train input train input",
        "Solution_preprocessed_content":null,
        "Solution_readability":51.8,
        "Solution_reading_time":6.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1598380609848,
        "Answerer_location":"Oxford, UK",
        "Answerer_reputation_count":38531.0,
        "Answerer_view_count":4137.0,
        "Challenge_adjusted_solved_time":0.0715297222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using DataBricks and Spark 7.4ML,<\/p>\n<p>The following code successfully logs the params and metrics, and I can see the ROCcurve.png in the MLFLOW gui (just the item in the tree below the model). But the actually plot is blank. Why?<\/p>\n<pre><code>with mlflow.start_run(run_name=&quot;logistic-regression&quot;) as run:\n  pipeModel = pipe.fit(trainDF)\n  mlflow.spark.log_model(pipeModel, &quot;model&quot;)\n  predTest = pipeModel.transform(testDF)\n  predTrain = pipeModel.transform(trainDF)\n  evaluator=BinaryClassificationEvaluator(labelCol=&quot;arrivedLate&quot;)\n  trainROC = evaluator.evaluate(predTrain)\n  testROC = evaluator.evaluate(predTest)\n  print(f&quot;Train ROC: {trainROC}&quot;)\n  print(f&quot;Test ROC: {testROC}&quot;)\n  mlflow.log_param(&quot;Dataset Name&quot;, &quot;Flights &quot; + datasetName)\n  mlflow.log_metric(key=&quot;Train ROC&quot;, value=trainROC)\n  mlflow.log_metric(key=&quot;Test ROC&quot;, value=testROC)\n\n  lrModel = pipeModel.stages[3]\n  trainingSummary = lrModel.summary\n  roc = trainingSummary.roc.toPandas()\n  plt.plot(roc['FPR'],roc['TPR'])\n  plt.ylabel('False Positive Rate')\n  plt.xlabel('True Positive Rate')\n  plt.title('ROC Curve')\n  plt.show()\n  plt.savefig(&quot;ROCcurve.png&quot;)\n  mlflow.log_artifact(&quot;ROCcurve.png&quot;)\n  plt.close()\n  \n  display(predTest.select(stringCols + [&quot;arrivedLate&quot;, &quot;prediction&quot;]))\n<\/code><\/pre>\n<p>What the notebook shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What the MLFlow shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1607094854147,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607094596640,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1607191847983,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65145994",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":17.1,
        "Challenge_reading_time":24.11,
        "Challenge_score_count":8,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":0.0715297222,
        "Challenge_title":"Saving an Matlabplot as an MLFlow artifact",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":5219,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Poster_created_time":1316705139196,
        "Poster_location":"Boston, MA",
        "Poster_reputation_count":6711.0,
        "Poster_view_count":819.0,
        "Solution_body":"<p>Put <code>plt.show()<\/code> after <code>plt.savefig()<\/code> - <code>plt.show()<\/code> will remove your plot because it is shown already.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":10.7,
        "Solution_reading_time":1.91,
        "Solution_score_count":7.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":14.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":92.1736986111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have some data with very particular format (e.g., tdms files generated by NI systems) and I stored them in a S3 bucket. Typically, for reading this data in python if the data was stored in my local computer, I would use npTDMS package. But, how should is read this tdms files when they are stored in a S3 bucket? One solution is to download the data for instance to the EC2 instance and then use npTDMS package for reading the data into python. But it does not seem to be a perfect solution. Is there any way that I can read the data similar to reading CSV files from S3? <\/p>",
        "Challenge_closed_time":1577181091056,
        "Challenge_comment_count":3,
        "Challenge_created_time":1576870865157,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59430560",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":6.6,
        "Challenge_reading_time":7.21,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":86.1738608333,
        "Challenge_title":"Reading Data from AWS S3",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":4393,
        "Challenge_word_count":116,
        "Platform":"Stack Overflow",
        "Poster_created_time":1534965197292,
        "Poster_location":"Seattle, WA",
        "Poster_reputation_count":320.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>Some Python packages (such as Pandas) support reading data directly from S3, as it is the most popular location for data. See <a href=\"https:\/\/stackoverflow.com\/questions\/37703634\/how-to-import-a-text-file-on-aws-s3-into-pandas-without-writing-to-disk\">this question<\/a> for example on the way to do that with Pandas.<\/p>\n\n<p>If the package (npTDMS) doesn't support reading directly from S3, you should copy the data to the local disk of the notebook instance.<\/p>\n\n<p>The simplest way to copy is to run the AWS CLI in a cell in your notebook<\/p>\n\n<pre><code>!aws s3 cp s3:\/\/bucket_name\/path_to_your_data\/ data\/\n<\/code><\/pre>\n\n<p>This command will copy all the files under the \"folder\" in S3 to the local folder <code>data<\/code><\/p>\n\n<p>You can use more fine-grained copy using the filtering of the files and other specific requirements using the boto3 rich capabilities. For example:<\/p>\n\n<pre><code>s3 = boto3.resource('s3')\nbucket = s3.Bucket('my-bucket')\nobjs = bucket.objects.filter(Prefix='myprefix')\nfor obj in objs:\n   obj.download_file(obj.key)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"packag panda read data directli packag nptdm read directli copi data local disk notebook instanc cli bucket path data data grain copi filter file boto rich capabl",
        "Solution_last_edit_time":1577202690472,
        "Solution_link_count":1.0,
        "Solution_original_content":"packag panda read data directli popular locat data panda packag nptdm read directli copi data local disk notebook instanc simplest copi run cli cell notebook bucket path data data copi file folder local folder data grain copi filter file boto rich capabl boto resourc bucket bucket bucket obj bucket object filter prefix myprefix obj obj obj download file obj kei",
        "Solution_preprocessed_content":"packag read data directli popular locat data panda packag read directli copi data local disk notebook instanc simplest copi run cli cell notebook copi file folder local folder copi filter file boto rich capabl",
        "Solution_readability":10.8,
        "Solution_reading_time":13.62,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":133.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.1954663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to write the output of batch scoring into datalake:<\/p>\n<pre><code>    parallel_step_name = &quot;batchscoring-&quot; + datetime.now().strftime(&quot;%Y%m%d%H%M&quot;)\n    \n    output_dir = PipelineData(name=&quot;scores&quot;, \n                              datastore=def_ADL_store,\n                              output_mode=&quot;upload&quot;,\n                              output_path_on_compute=&quot;path in data lake&quot;)\n\nparallel_run_config = ParallelRunConfig(\n    environment=curated_environment,\n    entry_script=&quot;use_model.py&quot;,\n    source_directory=&quot;.\/&quot;,\n    output_action=&quot;append_row&quot;,\n    mini_batch_size=&quot;20&quot;,\n    error_threshold=1,\n    compute_target=compute_target,\n    process_count_per_node=2,\n    node_count=2\n)\n    \n    batch_score_step = ParallelRunStep(\n        name=parallel_step_name,\n        inputs=[test_data.as_named_input(&quot;test_data&quot;)],\n        output=output_dir,\n        parallel_run_config=parallel_run_config,\n        allow_reuse=False\n    )\n<\/code><\/pre>\n<p>However I meet the error: &quot;code&quot;: &quot;UserError&quot;,\n&quot;message&quot;: &quot;User program failed with Exception: Missing argument --output or its value is empty.&quot;<\/p>\n<p>How can I write results of batch score to data lake?<\/p>",
        "Challenge_closed_time":1596781453976,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596780750297,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63296185",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":21.8,
        "Challenge_reading_time":16.06,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.1954663889,
        "Challenge_title":"How to write Azure machine learning batch scoring results to data lake?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":277,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1513841518107,
        "Poster_location":"China",
        "Poster_reputation_count":71.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>I don\u2019t think ADLS is supported for <code>PipelineData<\/code>. My suggestion is to use the workspace\u2019s default blob store for the <code>PipelineData<\/code>, then use a <code>DataTransferStep<\/code> for after the <code>ParallelRunStep<\/code> is completed.<\/p>",
        "Solution_comment_count":6.0,
        "Solution_gpt_summary":"workspac default blob store pipelinedata datatransferstep parallelrunstep complet adl pipelinedata",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"adl pipelinedata workspac default blob store pipelinedata datatransferstep parallelrunstep complet",
        "Solution_preprocessed_content":"adl workspac default blob store complet",
        "Solution_readability":12.9,
        "Solution_reading_time":3.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":27.9929452778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to create a prediction model using the VertexAI class AutoMLtabularTrainingJob, and I'm having problems with two parameters listed in the <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform.AutoMLTabularTrainingJob\" rel=\"nofollow noreferrer\">documentation<\/a>.<\/p>\n<p>First, the parameter column_specs is a dictionary in the documentation, and the parameter export_evaluated_data_items is a bool.<\/p>\n<p>I created the function below, and I called it inside a loop.<\/p>\n<pre><code>def create_training_pipeline_tabular_regression_sample(\ndisplay_name:str,\ndataset_id:int,\ncolumn_specs:dict,\ntarget_column:str = None,\noptimization_prediction_type:str = 'regression',\noptimization_objective:str = 'minimize-rmse',\nmodel_display_name:str = None,\nbudget_milli_node_hours:int = 1000,\ndisable_early_stopping:bool = False,\nexport_evaluated_data:bool = True,\nsync:bool = True,\n**kwargs\n):\n\ntabular_regression_job = aiplatform.AutoMLTabularTrainingJob(\n    display_name=display_name,\n    column_specs=column_specs,\n    optimization_prediction_type=optimization_prediction_type,\n    optimization_objective=optimization_objective\n)\n\nmy_tabular_dataset = aiplatform.TabularDataset(dataset_id)\n\nmodel = tabular_regression_job.run(\n    dataset=my_tabular_dataset,\n    target_column=target_column,\n    budget_milli_node_hours=budget_milli_node_hours,\n    model_display_name=model_display_name,\n    disable_early_stopping=disable_early_stopping,\n    export_evaluated_data_items=True,\n    sync=sync,\n    **kwargs\n)\n\nmodel.wait()\n\nprint(model.display_name)\nprint(model.resource_name)\nprint(model.uri)\nreturn model\n<\/code><\/pre>\n<p>The error is that the class is not accepting these parameters. The error message:<\/p>\n<pre><code>    ---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n\/tmp\/ipykernel_118\/330955058.py in &lt;module&gt;\n     60                     optimization_objective=optimization,\n     61                     budget_milli_node_hours= BUDGET_MILLI_NODE_HOURS,\n---&gt; 62                     export_evaluated_data_items_bigquery_destination_uri=export_evaluated_data_items_bigquery_destination_uri\n     63                 )\n     64 \n\n\/tmp\/ipykernel_118\/2971171495.py in create_training_pipeline_tabular_regression_sample(display_name, dataset_id, target_column, optimization_prediction_type, optimization_objective, model_display_name, budget_milli_node_hours, disable_early_stopping, export_evaluated_data, sync, **kwargs)\n     31         export_evaluated_data_items=True,\n     32         sync=sync,\n---&gt; 33         **kwargs\n     34     )\n     35 \n\nTypeError: run() got an unexpected keyword argument 'export_evaluated_data_items'\n<\/code><\/pre>\n<p>Does anyone know if the documentation is updated? In the page's footer the update date is recent, but these errors make me have doubts. And there's other information in the documentation that does not match with the API's use.<\/p>",
        "Challenge_closed_time":1653620941596,
        "Challenge_comment_count":2,
        "Challenge_created_time":1653520166993,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72385022",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":23.0,
        "Challenge_reading_time":39.22,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":27.9929452778,
        "Challenge_title":"VertexAI's class AutoMLtabularTrainingJob doesn't recognize parameters listed in the documentation",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":61,
        "Challenge_word_count":215,
        "Platform":"Stack Overflow",
        "Poster_created_time":1563027261236,
        "Poster_location":null,
        "Poster_reputation_count":315.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>I was able to reproduce your error when I downgraded to google-cloud-aiplatform version <code>0.7.1<\/code>. To resolve this, you must update your version to the latest <strong>google-cloud-aiplatform<\/strong> package by using the below command.<\/p>\n<pre><code>pip install google-cloud-aiplatform --upgrade\n<\/code><\/pre>\n<p>You will now have <strong>google-cloud-aiplatform<\/strong> version <code>1.13.1<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/81afC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/81afC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Once upgraded to the latest version, you can now proceed and finish your training.\n<a href=\"https:\/\/i.stack.imgur.com\/RUzE3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RUzE3.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"updat version cloud aiplatform packag latest version pip instal cloud aiplatform upgrad updat proce train",
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_original_content":"reproduc downgrad cloud aiplatform version updat version latest cloud aiplatform packag pip instal cloud aiplatform upgrad cloud aiplatform version upgrad latest version proce finish train",
        "Solution_preprocessed_content":"reproduc downgrad version updat version latest packag version upgrad latest version proce finish train",
        "Solution_readability":11.8,
        "Solution_reading_time":11.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":78.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1613062428296,
        "Answerer_location":null,
        "Answerer_reputation_count":141.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":1.9716805556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am looking at Kedro Library as my team are looking into using it for our data pipeline.<\/p>\n<p>While going to the offical tutorial - Spaceflight.<\/p>\n<p>I came across this function:<\/p>\n<pre><code>def preprocess_companies(companies: pd.DataFrame) -&gt; pd.DataFrame:\n&quot;&quot;&quot;Preprocess the data for companies.\n\n    Args:\n        companies: Source data.\n    Returns:\n        Preprocessed data.\n\n&quot;&quot;&quot;\n\ncompanies[&quot;iata_approved&quot;] = companies[&quot;iata_approved&quot;].apply(_is_true)\n\ncompanies[&quot;company_rating&quot;] = companies[&quot;company_rating&quot;].apply(_parse_percentage)\n\nreturn companies\n<\/code><\/pre>\n<ul>\n<li>companies is the name of the csv file containing the data<\/li>\n<\/ul>\n<p>Looking at the function, my assumption is that <code>(companies: pd.Dafarame)<\/code> is the shorthand to read the &quot;companies&quot; dataset as a dataframe. If so, I do not understand what does <code>-&gt; pd.Dataframe<\/code> at the end means<\/p>\n<p>I tried looking at python documentation regarding such style of code but I did not managed to find any<\/p>\n<p>Much help is appreciated to assist me in understanding this.<\/p>\n<p>Thank you<\/p>",
        "Challenge_closed_time":1613062868172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613060367020,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66158536",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":9.4,
        "Challenge_reading_time":15.78,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":0.6947644444,
        "Challenge_title":"What does this python function signature means in Kedro Tutorial?",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":135,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1504515330836,
        "Poster_location":null,
        "Poster_reputation_count":5.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>The <code>-&gt;<\/code> notation is <a href=\"https:\/\/docs.python.org\/3\/library\/typing.html\" rel=\"nofollow noreferrer\">type hinting<\/a>, as is the <code>:<\/code> part in the <code>companies: pd.DataFrame<\/code> function definition. This is not essential to do in Python but many people like to include it. The function definition would work exactly the same if it didn't contain this but instead read:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def preprocess_companies(companies):\n<\/code><\/pre>\n<p>This is a general Python thing rather than anything kedro-specific.<\/p>\n<p>The way that kedro registers <code>companies<\/code> as a kedro dataset is completely separate from this function definition and is done through the catalog.yml file:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>companies:\n  type: pandas.CSVDataSet\n  filepath: data\/01_raw\/companies.csv\n<\/code><\/pre>\n<p>There will then a <em>node<\/em> defined (in pipeline.py) to specify that the <code>preprocess_companies<\/code> function should take as input the kedro dataset <code>companies<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>node(\n    func=preprocess_companies,\n    inputs=&quot;companies&quot;,  # THIS LINE REFERS TO THE DATASET NAME\n    outputs=&quot;preprocessed_companies&quot;,\n    name=&quot;preprocessing_companies&quot;,\n),\n<\/code><\/pre>\n<p>In theory the name of the parameter in the function itself could be completely different, e.g.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def preprocess_companies(anything_you_want):\n<\/code><\/pre>\n<p>... although it is very common to give it the same name as the dataset.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"datafram end function signatur type hint notat essenti peopl function definit exactli notat regist dataset complet separ function definit catalog yml file node defin pipelin specifi function input dataset theori paramet function complet common dataset",
        "Solution_last_edit_time":1613067465070,
        "Solution_link_count":1.0,
        "Solution_original_content":"notat type hint compani datafram function definit essenti peopl function definit exactli read preprocess compani compani gener regist compani dataset complet separ function definit catalog yml file compani type panda csvdataset filepath data raw compani csv node defin pipelin specifi preprocess compani function input dataset compani node func preprocess compani input compani line dataset output preprocess compani preprocess compani theori paramet function complet preprocess compani common dataset",
        "Solution_preprocessed_content":"notat type hint function definit essenti peopl function definit exactli read gener regist dataset complet separ function definit file node defin specifi function input dataset theori paramet function complet common dataset",
        "Solution_readability":13.6,
        "Solution_reading_time":21.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":171.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1408356046196,
        "Answerer_location":"Bonn, Deutschland",
        "Answerer_reputation_count":594.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":0.2372688889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to test my service and to do so I deploy it locally and until now everything worked fine. However, for some unrelated reason I was forced to delete all my docker images and since then I'm unable to deploy the service locally. Upon deployment I receive the following error:<\/p>\n\n<blockquote>\n  <p>404 Client Error: Not Found for url:\n  http+docker:\/\/localnpipe\/v1.39\/images\/471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814\/json<\/p>\n<\/blockquote>\n\n<p>And also:<\/p>\n\n<blockquote>\n  <p>ImageNotFound: 404 Client Error: Not Found (\"no such image: \n  471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814: No\n  such image:\n  sha256:471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814\")<\/p>\n<\/blockquote>\n\n<p>What I did to deploy the model:<\/p>\n\n<pre><code>from azureml.core.model import Model\nfrom azureml.core import Workspace\nfrom azureml.core.webservice import LocalWebservice\nfrom azureml.core.model import InferenceConfig\n\nws = Workspace.from_config(\"config.json\")\n\ndeployment_config = LocalWebservice.deploy_configuration(port=8890)\n\ninference_config = InferenceConfig(runtime= \"python\", \n                               entry_script=\"score.py\",\n                               conda_file=\"env.yml\")\n\nmodel_box = Model(ws, \"box\")\nmodel_view = Model(ws, \"view_crop\")\nmodel_damage = Model(ws, \"damage_crop\")\n\nservice = Model.deploy(ws, \"test-service\", [model_box, model_view, model_damage], inference_config, deployment_config)\n\nservice.wait_for_deployment(True)\n<\/code><\/pre>\n\n<p>I understand why there is no image present, but I would expect that it is downloaded in that case.<\/p>\n\n<p>Is there a way to force the build process to re-download the docker base image?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1559043123528,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559042269360,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56341012",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":22.58,
        "Challenge_score_count":2,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":0.2372688889,
        "Challenge_title":"Docker image not found during local deployment (\"no such image\")",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":3685,
        "Challenge_word_count":176,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408356046196,
        "Poster_location":"Bonn, Deutschland",
        "Poster_reputation_count":594.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>I just found the problem and corresponding solution:<\/p>\n\n<p>I deleted all images but there where still some containers based on deleted images present. Deleting the corresponding container had the desired effect that the docker image is reloaded from the server.<\/p>\n\n<p>You can delete all containers with <code>docker kill $(docker ps -q)<\/code>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"delet docker kill docker forc build process download docker base imag",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"delet imag base delet imag present delet desir docker imag reload server delet docker kill docker",
        "Solution_preprocessed_content":"delet imag base delet imag present delet desir docker imag reload server delet",
        "Solution_readability":9.9,
        "Solution_reading_time":4.45,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":188.0286397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using one of the images listed here <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md<\/a>, to create an sagemaker endpoint, but I keep getting &quot;failed reason: Image size 15136109518 is greater that suppported size 1073741824&quot; .<\/p>\n<p>is there a way to find out the size of images provided <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md<\/a> or any aws managed images?<\/p>",
        "Challenge_closed_time":1645569051340,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644892148237,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71120471",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":18.6,
        "Challenge_reading_time":9.87,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":188.0286397222,
        "Challenge_title":"How to determine size of images available in aws?",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":153,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1590797441983,
        "Poster_location":null,
        "Poster_reputation_count":525.0,
        "Poster_view_count":98.0,
        "Solution_body":"<p>I suspect you are trying to deploy a serverless endpoint provisioned with 1GB of memory. As discussed <a href=\"https:\/\/repost.aws\/questions\/QU35dVp2D9SKKUnnVYGw9Z7A\/how-to-check-determine-image-container-size-for-aws-managed-images\" rel=\"nofollow noreferrer\">here<\/a> &quot;You can increase the memory size of your endpoint with the MemorySizeInMB parameter, more info in this documentation: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config%22\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config&quot;<\/a><\/p>\n<p>In order to view the uncompressed size of an image you can use the following example command:<\/p>\n<pre><code>$ docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-training:1.15.2-cpu-py27-ubuntu18.04\n\n$ docker inspect -f &quot;{{ .Size }}&quot; 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-training:1.15.2-cpu-py27-ubuntu18.04\n<\/code><\/pre>\n<p>Kindly also note that you will need to provision enough memory to accommodate your model as well. Please see this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html#serverless-endpoints-how-it-works-memory\" rel=\"nofollow noreferrer\">link<\/a> for more information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"increas memori size endpoint memorysizeinmb paramet uncompress size imag docker inspect provis memori accommod model",
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_original_content":"deploi serverless endpoint provis memori increas memori size endpoint memorysizeinmb paramet document http doc com latest serverless endpoint creat html serverless endpoint creat config order uncompress size imag docker pull dkr ecr amazonaw com tensorflow train cpu ubuntu docker inspect size dkr ecr amazonaw com tensorflow train cpu ubuntu kindli note provis memori accommod model link",
        "Solution_preprocessed_content":"deploi serverless endpoint provis memori increas memori size endpoint memorysizeinmb paramet document order uncompress size imag kindli note provis memori accommod model link",
        "Solution_readability":20.1,
        "Solution_reading_time":18.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":99.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":8.3113277778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a program in python to detect a language and translate that to English using azure machine learning studio. The code block mentioned below throwing error when trying to detect the language.<\/p>\n<blockquote>\n<p>Error 0002: Failed to parse parameter.<\/p>\n<\/blockquote>\n<pre><code>def sample_detect_language():\n    print(\n        &quot;This sample statement will be translated to english from any other foreign language&quot;\n       \n    )\n    \n    from azure.core.credentials import AzureKeyCredential\n    from azure.ai.textanalytics import TextAnalyticsClient\n\n    endpoint = os.environ[&quot;AZURE_LANGUAGE_ENDPOINT&quot;]\n    key = os.environ[&quot;AZURE_LANGUAGE_KEY&quot;]\n\n    text_analytics_client = TextAnalyticsClient(endpoint=endpoint)\n    documents = [\n        &quot;&quot;&quot;\n        The feedback was awesome\n        &quot;&quot;&quot;,\n        &quot;&quot;&quot;\n        la recensione \u00e8 stata fantastica\n        &quot;&quot;&quot;\n    ]\n\n    result = text_analytics_client.detect_language(documents)\n    reviewed_docs = [doc for doc in result if not doc.is_error]\n\n    print(&quot;Check the languages we got review&quot;)\n\n    for idx, doc in enumerate(reviewed_docs):\n        print(&quot;Number#{} is in '{}', which has ISO639-1 name '{}'\\n&quot;.format(\n            idx, doc.primary_language.name, doc.primary_language.iso6391_name\n        ))\n        if doc.is_error:\n            print(doc.id, doc.error)\n    \n    print(\n        &quot;Storing reviews and mapping to their respective ISO639-1 name &quot;\n        \n    )\n\n    review_to_language = {}\n    for idx, doc in enumerate(reviewed_docs):\n        review_to_language[documents[idx]] = doc.primary_language.iso6391_name\n\n\nif __name__ == '__main__':\n    sample_detect_language()\n<\/code><\/pre>\n<p>Any help to solve the issue is appreciated.<\/p>",
        "Challenge_closed_time":1659007284270,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658977363490,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73146779",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":21.81,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":8.3113277778,
        "Challenge_title":"ML Studio language studio failing to detect the source language",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":50,
        "Challenge_word_count":164,
        "Platform":"Stack Overflow",
        "Poster_created_time":1652123310643,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>The issue was raised because of missing the called parameters in the function. While doing language detection in machine learning studio, we need to assign end point and key credentials. In the code mentioned above, endpoint details were mentioned, but missed <strong>AzureKeyCredential.<\/strong><\/p>\n<pre><code>endpoint = os.environ[&quot;AZURE_LANGUAGE_ENDPOINT&quot;]\nkey = os.environ[&quot;AZURE_LANGUAGE_KEY&quot;]\ntext_analytics_client = TextAnalyticsClient(endpoint=endpoint)\n<\/code><\/pre>\n<p>replace the above line with the code block mentioned below<\/p>\n<pre><code>text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential= AzureKeyCredential(key))\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"miss azurekeycredenti replac block block block azurekeycredenti kei languag detect studio",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"rais miss call paramet function languag detect studio assign end kei credenti endpoint miss azurekeycredenti endpoint environ languag endpoint kei environ languag kei text analyt client textanalyticscli endpoint endpoint replac line block text analyt client textanalyticscli endpoint endpoint credenti azurekeycredenti kei",
        "Solution_preprocessed_content":"rais miss call paramet function languag detect studio assign end kei credenti endpoint miss azurekeycredenti replac line block",
        "Solution_readability":16.9,
        "Solution_reading_time":9.15,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1467943515392,
        "Answerer_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Answerer_reputation_count":173.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":67.8829583333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to integrate MLFlow to my project. Because I'm using <code>tf.keras.fit_generator()<\/code> for my training so I take advantage of <code>mlflow.tensorflow.autolog()<\/code>(<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a> here) to enable automatic logging of metrics and parameters:<\/p>\n<pre><code>    model = Unet()\n    optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n\n    metrics = [IOUScore(threshold=0.5), FScore(threshold=0.5)]\n    model.compile(optimizer, customized_loss, metrics)\n\n    callbacks = [\n        tf.keras.callbacks.ModelCheckpoint(&quot;model.h5&quot;, save_weights_only=True, save_best_only=True, mode='min'),\n        tf.keras.callbacks.TensorBoard(log_dir='.\/logs', profile_batch=0, update_freq='batch'),\n    ]\n\n\n    train_dataset = Dataset(src_dir=SOURCE_DIR)\n\n    train_data_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n\n   \n    with mlflow.start_run():\n        mlflow.tensorflow.autolog()\n        mlflow.log_param(&quot;batch_size&quot;, BATCH_SIZE)\n\n        model.fit_generator(\n            train_data_loader,\n            steps_per_epoch=len(train_data_loader),\n            epochs=EPOCHS,\n            callbacks=callbacks   \n            )\n<\/code><\/pre>\n<p>I expected something like this (just a demonstration taken from the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#visualizing-metrics\" rel=\"nofollow noreferrer\">docs<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" alt=\"Visualization on the docs\" \/><\/a><\/p>\n<p>However, after the training finished, this is what I got:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" alt=\"f1_score visualization\" \/><\/a><\/p>\n<p>How can I configure so that the metric plot will update and display its value at each epoch instead of just showing the latest value?<\/p>",
        "Challenge_closed_time":1594008525280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593764146630,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1594008626392,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62711259",
        "Challenge_link_count":6,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":17.5,
        "Challenge_reading_time":26.5,
        "Challenge_score_count":2,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":67.8829583333,
        "Challenge_title":"Customize metric visualization in MLFlow UI when using mlflow.tensorflow.autolog()",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1035,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1467943515392,
        "Poster_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Poster_reputation_count":173.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>After searching around, I found <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/2390\" rel=\"nofollow noreferrer\">this issue<\/a> related to my problem above. Actually, all my metrics just logged once each training (instead of each epoch as my intuitive thought). The reason is I didn't specify the <code>every_n_iter<\/code> parameter in <code>mlflow.tensorflow.autolog()<\/code>, which indicates how many 'iterations' must pass before MLflow logs metric executed (see the <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a>). So, changing my code to:<\/p>\n<p><code>mlflow.tensorflow.autolog(every_n_iter=1)<\/code><\/p>\n<p>fixed the problem.<\/p>\n<p>P\/s: Remember that in TF 2.x, an 'iteration' is an epoch (in TF 1.x it's a batch).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":8.7,
        "Solution_reading_time":10.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":87.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1359884693920,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":9637.0,
        "Answerer_view_count":609.0,
        "Challenge_adjusted_solved_time":2068.9864736111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>im trying to setup <a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a> to log my experiment details <\/p>\n\n<p>getting strange error:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"train.py\", line 7, in &lt;module&gt;\n    from comet_ml import Experiment\nImportError: No module named comet_ml\n<\/code><\/pre>\n\n<p>trying in python 2 and python3<\/p>",
        "Challenge_closed_time":1506066553020,
        "Challenge_comment_count":0,
        "Challenge_created_time":1506066265147,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1506066568087,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46359436",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":5.55,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.0799647222,
        "Challenge_title":"How to configure comet (comet.ml) to track Keras?",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1208,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1505841491572,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>it seems like comet isn't installed on your machine.<\/p>\n\n<p>you can use :<\/p>\n\n<pre><code>pip3 install comet_ml\npip install comet_ml\n<\/code><\/pre>\n\n<p>take a look at the example projects at: <\/p>\n\n<p><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"instal run pip instal pip instal termin quickstart guid github",
        "Solution_last_edit_time":1513514919392,
        "Solution_link_count":2.0,
        "Solution_original_content":"isn instal pip instal pip instal http github com quickstart guid",
        "Solution_preprocessed_content":null,
        "Solution_readability":9.9,
        "Solution_reading_time":4.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1524670343368,
        "Answerer_location":"Z\u00fcrich, Schweiz",
        "Answerer_reputation_count":460.0,
        "Answerer_view_count":52.0,
        "Challenge_adjusted_solved_time":415.3991708334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I would like to run a test script on an existing compute instance of Azure using the Azure Machine Learning extension to the Azure CLI:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>az ml run submit-script test.py --target compute-instance-test --experiment-name test_example --resource-group ex-test-rg\n<\/code><\/pre>\n<p>I get a Service Error with the following error message:<\/p>\n<pre><code>Unable to run conda package manager. AzureML uses conda to provision python\\nenvironments from a dependency specification. To manage the python environment\\nmanually instead, set userManagedDependencies to True in the python environment\\nconfiguration. To use system managed python environments, install conda from:\\nhttps:\/\/conda.io\/miniconda.html\n<\/code><\/pre>\n<p>But when I connect to the compute instance through the Azure portal and select the default Python kernel, <code>conda --version<\/code> prints 4.5.12. So conda is effectively already installed on the compute instance. This is why I do not understand the error message.<\/p>\n<p>Further information on the azure versions:<\/p>\n<pre><code>  &quot;azure-cli&quot;: &quot;2.12.1&quot;,\n  &quot;azure-cli-core&quot;: &quot;2.12.1&quot;,\n  &quot;azure-cli-telemetry&quot;: &quot;1.0.6&quot;,\n  &quot;extensions&quot;: {\n    &quot;azure-cli-ml&quot;: &quot;1.15.0&quot;\n  }\n<\/code><\/pre>\n<p>The image I use is:<\/p>\n<pre><code>mcr.microsoft.com\/azure-cli:latest\n<\/code><\/pre>\n<p>Can somebody please explain as to why I am getting this error and help me resolve the error? Thank you!<\/p>\n<p>EDIT: I tried to update the environment in which the <code>az ml run<\/code>-command is run.\nEssentially this is my GitLab job. The installation of miniconda is a bit complicated as the azure-cli uses an alpine Linux image (reference: <a href=\"https:\/\/stackoverflow.com\/questions\/47177538\/installing-miniconda-on-alpine-linux-fails\">Installing miniconda on alpine linux fails<\/a>). I replaced some names with ... and cut out some irrelevant pieces of code.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>test:\n  image: 'mcr.microsoft.com\/azure-cli:latest'\n  script:\n    - echo &quot;Download conda&quot;\n    - apk --update add bash curl wget ca-certificates libstdc++ glib\n    - wget -q -O \/etc\/apk\/keys\/sgerrand.rsa.pub https:\/\/raw.githubusercontent.com\/sgerrand\/alpine-pkg-node-bower\/master\/sgerrand.rsa.pub\n    - curl -L &quot;https:\/\/github.com\/sgerrand\/alpine-pkg-glibc\/releases\/download\/2.23-r3\/glibc-2.23-r3.apk&quot; -o glibc.apk\n    - apk del libc6-compat\n    - apk add glibc.apk\n    - curl -L &quot;https:\/\/github.com\/sgerrand\/alpine-pkg-glibc\/releases\/download\/2.23-r3\/glibc-bin-2.23-r3.apk&quot; -o glibc-bin.apk \n    - apk add glibc-bin.apk \n    - curl -L &quot;https:\/\/github.com\/andyshinn\/alpine-pkg-glibc\/releases\/download\/2.25-r0\/glibc-i18n-2.25-r0.apk&quot; -o glibc-i18n.apk\n    - apk add --allow-untrusted glibc-i18n.apk \n    - \/usr\/glibc-compat\/bin\/localedef -i en_US -f UTF-8 en_US.UTF-8 \n    - \/usr\/glibc-compat\/sbin\/ldconfig \/lib \/usr\/glibc\/usr\/lib\n    - rm -rf glibc*apk \/var\/cache\/apk\/*\n    - echo &quot;yes&quot; | curl -sSL https:\/\/repo.continuum.io\/miniconda\/Miniconda3-latest-Linux-x86_64.sh -o miniconda.sh\n    - echo &quot;Install conda&quot;\n    - (echo -e &quot;\\n&quot;; echo &quot;yes&quot;; echo -e &quot;\\n&quot;; echo &quot;yes&quot;) | bash -bfp miniconda.sh\n    - echo &quot;Installing Azure Machine Learning Extension&quot;\n    - az extension add -n azure-cli-ml\n    - echo &quot;Azure Login&quot;\n    - az login\n    - az account set --subscription ...\n    - az configure --defaults group=...\n    - az ml folder attach -w ... \n    - az ml run submit-script test.py --target ... --experiment-name hello_world --resource-group ...\n<\/code><\/pre>",
        "Challenge_closed_time":1603466200947,
        "Challenge_comment_count":6,
        "Challenge_created_time":1601897074350,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1601970763932,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64207678",
        "Challenge_link_count":7,
        "Challenge_open_time":null,
        "Challenge_participation_count":9,
        "Challenge_readability":11.6,
        "Challenge_reading_time":48.54,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":435.8684991667,
        "Challenge_title":"How to avoid error \"conda --version: conda not found\" in az ml run --submit-script command?",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1089,
        "Challenge_word_count":373,
        "Platform":"Stack Overflow",
        "Poster_created_time":1524670343368,
        "Poster_location":"Z\u00fcrich, Schweiz",
        "Poster_reputation_count":460.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>One needs to pass the <code>--workspace-name<\/code> argument to be able to run it on Azure's compute target and not on the local compute target:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>az ml run submit-script test.py --target compute-instance-test --experiment-name test_example --resource-group ex-test-rg --workspace-name test-ws\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"pass workspac argument run test comput target local comput target run submit test target comput instanc test test resourc group test workspac test",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"pass workspac argument run comput target local comput target run submit test target comput instanc test test resourc group test workspac test",
        "Solution_preprocessed_content":"pass argument run comput target local comput target",
        "Solution_readability":14.6,
        "Solution_reading_time":4.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1407449881432,
        "Answerer_location":null,
        "Answerer_reputation_count":228.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":732.1807,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I am attempting to run the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">example code<\/a> for Amazon Sagemaker on a local GPU.  I have copied the code from the Jupyter notebook to the following Python script:<\/p>\n\n<pre><code>import boto3\nimport subprocess\nimport sagemaker\nfrom sagemaker.mxnet import MXNet\nfrom mxnet import gluon\nfrom sagemaker import get_execution_role\nimport os\n\nsagemaker_session = sagemaker.Session()\ninstance_type = 'local'\nif subprocess.call('nvidia-smi') == 0:\n    # Set type to GPU if one is present\n    instance_type = 'local_gpu'\n# role = get_execution_role()\n\ngluon.data.vision.MNIST('.\/data\/train', train=True)\ngluon.data.vision.MNIST('.\/data\/test', train=False)\n\n# successfully connects and uploads data\ninputs = sagemaker_session.upload_data(path='data', key_prefix='data\/mnist')\n\nhyperparameters = {\n    'batch_size': 100,\n    'epochs': 20,\n    'learning_rate': 0.1,\n    'momentum': 0.9,\n    'log_interval': 100\n}\n\nm = MXNet(\"mnist.py\",\n          role=role,\n          train_instance_count=1,\n          train_instance_type=instance_type,\n          framework_version=\"1.1.0\",\n          hyperparameters=hyperparameters)\n\n# fails in Docker container\nm.fit(inputs)\npredictor = m.deploy(initial_instance_count=1, instance_type=instance_type)\nm.delete_endpoint()\n<\/code><\/pre>\n\n<p>where the referenced <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">mnist.py<\/a> file is exactly as specified on Github. The script fails on <code>m.fit<\/code> in Docker container with the following error: <\/p>\n\n<pre><code>algo-1-1DUU4_1  | Downloading s3:\/\/&lt;S3-BUCKET&gt;\/sagemaker-mxnet-2018-10-07-00-47-10-435\/source\/sourcedir.tar.gz to \/tmp\/script.tar.gz\nalgo-1-1DUU4_1  | 2018-10-07 00:47:29,219 ERROR - container_support.training - uncaught exception during training: Unable to locate credentials\nalgo-1-1DUU4_1  | Traceback (most recent call last):\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/training.py\", line 36, in start\nalgo-1-1DUU4_1  |     fw.train()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/mxnet_container\/train.py\", line 169, in train\nalgo-1-1DUU4_1  |     mxnet_env.download_user_module()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/environment.py\", line 89, in download_user_module\nalgo-1-1DUU4_1  |     cs.download_s3_resource(self.user_script_archive, tmp)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/utils.py\", line 37, in download_s3_resource\nalgo-1-1DUU4_1  |     script_bucket.download_file(script_key_name, target)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 246, in bucket_download_file\nalgo-1-1DUU4_1  |     ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 172, in download_file\nalgo-1-1DUU4_1  |     extra_args=ExtraArgs, callback=Callback)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/transfer.py\", line 307, in download_file\nalgo-1-1DUU4_1  |     future.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 73, in result\nalgo-1-1DUU4_1  |     return self._coordinator.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 233, in result\nalgo-1-1DUU4_1  |     raise self._exception\nalgo-1-1DUU4_1  | NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n\n<p>I am confused that I can authenticate to S3 outside of the container (to pload the training\/test data) but I cannot within the Docker container.  So I am guessing the issues has to do with passing the AWS credentials to the Docker container.  Here is the generated Docker-compose file:<\/p>\n\n<pre><code>networks:\n  sagemaker-local:\n    name: sagemaker-local\nservices:\n  algo-1-1DUU4:\n    command: train\n    environment:\n    - AWS_REGION=us-west-2\n    - TRAINING_JOB_NAME=sagemaker-mxnet-2018-10-07-00-47-10-435\n    image: 123456789012.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.1.0-gpu-py2\n    networks:\n      sagemaker-local:\n        aliases:\n        - algo-1-1DUU4\n    stdin_open: true\n    tty: true\n    volumes:\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/input:\/opt\/ml\/input\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output:\/opt\/ml\/output\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output\/data:\/opt\/ml\/output\/data\n    - \/tmp\/tmpSkaR3x\/model:\/opt\/ml\/model\nversion: '2.1'\n<\/code><\/pre>\n\n<p>Should the AWS credentials be passed in as enviromental variables?<\/p>\n\n<p>I upgraded my <code>sagemaker<\/code> install to after reading <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/403\" rel=\"nofollow noreferrer\">Using boto3 in install local mode?<\/a>, but that had no effect.  I checked the credentials that are being fetched in the Sagemaker session (outside the container) and they appear to be blank, even though I have an <code>~\/.aws\/config<\/code> and <code>~\/.aws\/credentials<\/code> file:<\/p>\n\n<pre><code>{'_token': None, '_time_fetcher': &lt;function _local_now at 0x7f4dbbe75230&gt;, '_access_key': None, '_frozen_credentials': None, '_refresh_using': &lt;bound method AssumeRoleCredentialFetcher.fetch_credentials of &lt;botocore.credentials.AssumeRoleCredentialFetcher object at 0x7f4d2de48bd0&gt;&gt;, '_secret_key': None, '_expiry_time': None, 'method': 'assume-role', '_refresh_lock': &lt;thread.lock object at 0x7f4d9f2aafd0&gt;}\n<\/code><\/pre>\n\n<p>I am new to AWS so I do not know how to diagnose the issue regarding AWS credentials.  My <code>.aws\/config<\/code> file has the following information (with placeholder values):<\/p>\n\n<pre><code>[default]\noutput = json\nregion = us-west-2\nrole_arn = arn:aws:iam::123456789012:role\/SageMakers\nsource_profile = sagemaker-test\n\n[profile sagemaker-test]\noutput = json\nregion = us-west-2\n<\/code><\/pre>\n\n<p>Where the <code>sagemaker-test<\/code> profile has <code>AmazonSageMakerFullAccess<\/code> in the IAM Management Console.<\/p>\n\n<p>The <code>.aws\/credentials<\/code> file has the following information (represented by placeholder values):<\/p>\n\n<pre><code>[default]\naws_access_key_id = 1234567890\naws_secret_access_key = zyxwvutsrqponmlkjihgfedcba\n[sagemaker-test]\naws_access_key_id = 0987654321\naws_secret_access_key = abcdefghijklmopqrstuvwxyz\n<\/code><\/pre>\n\n<p>Lastly, these are versions of the applicable libraries from a <code>pip freeze<\/code>:<\/p>\n\n<pre><code>awscli==1.16.19\nboto==2.48.0\nboto3==1.9.18\nbotocore==1.12.18\ndocker==3.5.0\ndocker-compose==1.22.0\nmxnet-cu91==1.1.0.post0\nsagemaker==1.11.1\n<\/code><\/pre>\n\n<p>Please let me know if I left out any relevant information and thanks for any help\/feedback that you can provide.<\/p>\n\n<p><strong>UPDATE<\/strong>: Thanks for your help, everyone! While attempting some of your suggested fixes, I noticed that <code>boto3<\/code> was out of date, and update it (to <code>boto3-1.9.26<\/code> and <code>botocore-1.12.26<\/code>) which appeared to resolve the issue.  I was not able to find any documentation on that being an issue with <code>boto3==1.9.18<\/code>.  If someone could help me understand what the issue was with <code>boto3<\/code>, I would happy to make mark their answer as correct.<\/p>",
        "Challenge_closed_time":1539818808056,
        "Challenge_comment_count":3,
        "Challenge_created_time":1538879533133,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1539893383687,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52684987",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":16.6,
        "Challenge_reading_time":96.04,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":63,
        "Challenge_solved_time":260.9097008333,
        "Challenge_title":"AWS NoCredentials in training",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1374,
        "Challenge_word_count":621,
        "Platform":"Stack Overflow",
        "Poster_created_time":1380496984176,
        "Poster_location":"Gloucester, VA, USA",
        "Poster_reputation_count":544.0,
        "Poster_view_count":85.0,
        "Solution_body":"<p>SageMaker local mode is designed to pick up whatever credentials are available in your boto3 session, and pass them into the docker container as environment variables. <\/p>\n\n<p>However, the version of the sagemaker sdk that you are using (1.11.1 and earlier) will ignore the credentials if they include a token, because that usually indicates short-lived credentials that won't remain valid long enough for a training job to complete or endpoint to be useful.<\/p>\n\n<p>If you are using temporary credentials, try replacing them with permanent ones, or running from an ec2 instance (or SageMaker notebook!) that has an appropriate instance role assigned.<\/p>\n\n<p>Also, the sagemaker sdk's handling of credentials changed in v1.11.2 and later -- temporary credentials will be passed to local mode containers, but with a warning message. So you could just upgrade to a newer version and try again (<code>pip install -U sagemaker<\/code>). <\/p>\n\n<p>Also, try upgrading <code>boto3<\/code> can change, so try using the latest version.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"replac temporari credenti perman on run instanc notebook instanc role assign upgrad newer version sdk later pass temporari credenti local mode warn messag upgrad boto latest version",
        "Solution_last_edit_time":1542529234207,
        "Solution_link_count":0.0,
        "Solution_original_content":"local mode design pick credenti boto session pass docker environ variabl version sdk earlier ignor credenti token short live credenti remain train job complet endpoint temporari credenti replac perman on run instanc notebook instanc role assign sdk credenti later temporari credenti pass local mode warn messag upgrad newer version pip instal upgrad boto latest version",
        "Solution_preprocessed_content":"local mode design pick credenti boto session pass docker environ variabl version sdk ignor credenti token credenti remain train job complet endpoint temporari credenti replac perman on run instanc instanc role assign sdk credenti later temporari credenti pass local mode warn messag upgrad newer version upgrad latest version",
        "Solution_readability":10.0,
        "Solution_reading_time":12.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":156.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":7.9073555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I trained multiple models with different configuration for a custom hyperparameter search. I use pytorch_lightning and its logging (TensorboardLogger).\nWhen running my training script after Task.init() ClearML auto-creates a Task and connects the logger output to the server.<\/p>\n<p>I log for each straining stage <code>train<\/code>, <code>val<\/code> and <code>test<\/code> the following scalars at each epoch: <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code><\/p>\n<p>When I have multiple configuration, e.g. <code>networkA<\/code> and <code>networkB<\/code> the first training log its values to <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code>, but the second to <code>networkB:loss<\/code>, <code>networkB:acc<\/code> and <code>networkB:iou<\/code>. This makes values umcomparable.<\/p>\n<p>My training loop with Task initalization looks like this:<\/p>\n<pre><code>names = ['networkA', networkB']\nfor name in names:\n     task = Task.init(project_name=&quot;NetworkProject&quot;, task_name=name)\n     pl_train(name)\n     task.close()\n<\/code><\/pre>\n<p>method pl_train is a wrapper for whole training with Pytorch Ligtning. No ClearML code is inside this method.<\/p>\n<p>Do you have any hint, how to properly use the usage of a loop in a script using completly separated tasks?<\/p>\n<hr \/>\n<p>Edit: ClearML version was 0.17.4. Issue is fixed in main branch.<\/p>",
        "Challenge_closed_time":1613773903383,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613745436903,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1614004159640,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66279581",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":18.29,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":7.9073555556,
        "Challenge_title":"ClearML multiple tasks in single script changes logged value names",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":279,
        "Challenge_word_count":172,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604391794420,
        "Poster_location":null,
        "Poster_reputation_count":89.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p><code>pytorch_lightning<\/code> is creating a new Tensorboard for each experiment. When ClearML logs the TB scalars, and it captures the same scalar being re-sent again, it adds a prefix so if you are reporting the same metric it will not overwrite the previous one. A good example would be reporting <code>loss<\/code> scalar in the training phase vs validation phase (producing &quot;loss&quot; and &quot;validation:loss&quot;). It might be the <code>task.close()<\/code> call does not clear the previous logs, so it &quot;thinks&quot; this is the same experiment, hence adding the prefix <code>networkB<\/code> to the <code>loss<\/code>. As long as you are closing the Task after training is completed you should have all experiments log with the same metric\/variant (title\/series). I suggest opening a GitHub issue, this should probably be considered a bug.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"task close clear previou log add prefix metric task close clear previou log persist open github",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"disclaim train team pytorch lightn creat tensorboard log scalar captur scalar sent add prefix report metric overwrit previou report loss scalar train phase phase produc loss loss task close clear previou log prefix networkb loss close task train complet log metric variant titl seri open github probabl",
        "Solution_preprocessed_content":"disclaim team creat tensorboard log scalar captur scalar add prefix report metric overwrit previou report scalar train phase phase clear previou log prefix close task train complet log open github probabl",
        "Solution_readability":9.9,
        "Solution_reading_time":11.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":135.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":36.6310980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a pandas's DataFrame created by:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>TB_HISTORICO_MODELO = pd.read_sql(&quot;&quot;&quot;select DAT_INICIO_SEMANA_PLAN\n,COD_NEGOCIO\n,VENDA\n,LUCRO\n,MODULADO\n,RUPTURA\n,QTD_ESTOQUE_MEDIO\n,PECAS from TB&quot;&quot;&quot;, cursor)\n\nTB_HISTORICO_MODELO[&quot;DAT_INICIO_SEMANA_PLAN&quot;] = pd.to_datetime(TB_HISTORICO_MODELO[&quot;DAT_INICIO_SEMANA_PLAN&quot;])\n\ndataset = TB_HISTORICO_MODELO[TB_HISTORICO_MODELO['COD_NEGOCIO']=='A101'].drop(columns=['COD_NEGOCIO']) .reset_index(drop=True)\n<\/code><\/pre>\n<p>Everything look like right.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; dataset.dtypes\nDAT_INICIO_SEMANA_PLAN    datetime64[ns]\nVENDA                            float64\nLUCRO                            float64\nMODULADO                           int64\nRUPTURA                            int64\nQTD_ESTOQUE_MEDIO                  int64\nPECAS                            float64\ndtype: object\n<\/code><\/pre>\n<p>But when I rum this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>#%% Create the AutoML Config file and run the experiment on Azure\n\nfrom azureml.train.automl import AutoMLConfig\n\ntime_series_settings = {\n   'time_column_name': 'DAT_INICIO_SEMANA_PLAN',\n   'max_horizon': 14,\n   'country_or_region': 'BR',\n   'target_lags': 'auto'\n}\n\nautoml_config = AutoMLConfig(task='forecasting',\n                            primary_metric='normalized_root_mean_squared_error',\n                            blocked_models=['ExtremeRandomTrees'],\n                            experiment_timeout_minutes=30,\n                            training_data=dataset,\n                            label_column_name='VENDA',\n                            compute_target = compute_cluster,\n                            enable_early_stopping=True,\n                            n_cross_validations=3,\n                            # max_concurrent_iterations=4,\n                            # max_cores_per_iteration=-1,\n                            verbosity=logging.INFO,\n                            **time_series_settings)\n\nremote_run = Experimento.submit(automl_config, show_output=True)\n<\/code><\/pre>\n<p>I get the message<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; remote_run = Experimento.submit(automl_config, show_output=True)\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;\/home\/fnord\/venv\/lib64\/python3.6\/site-packages\/azureml\/core\/experiment.py&quot;, line 219, in submit\n    run = submit_func(config, self.workspace, self.name, **kwargs)\n  File &quot;\/home\/fnord\/venv\/lib64\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py&quot;, line 92, in _automl_static_submit\n    automl_config_object._validate_config_settings(workspace)\n  File &quot;\/home\/fnord\/venv\/lib64\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py&quot;, line 1775, in _validate_config_settings\n    supported_types=&quot;, &quot;.join(SupportedInputDatatypes.REMOTE_RUN_SCENARIO)\nazureml.train.automl.exceptions.ConfigException: ConfigException:\n        Message: Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset]\n        InnerException: None\n        ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset]&quot;,\n        &quot;details_uri&quot;: &quot;https:\/\/aka.ms\/AutoMLConfig&quot;,\n        &quot;target&quot;: &quot;training_data&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;BadArgument&quot;,\n            &quot;inner_error&quot;: {\n                &quot;code&quot;: &quot;ArgumentInvalid&quot;,\n                &quot;inner_error&quot;: {\n                    &quot;code&quot;: &quot;InvalidInputDatatype&quot;\n                }\n            }\n        }\n    }\n}\n\n<\/code><\/pre>\n<p>Where is wrong?<\/p>\n<p>documentation:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train<\/a>\n<a href=\"https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig<\/a><\/p>",
        "Challenge_closed_time":1603004866532,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602873387197,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64394661",
        "Challenge_link_count":5,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":30.2,
        "Challenge_reading_time":55.9,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":36.5220375,
        "Challenge_title":"Erro InvalidInputDatatype: Input of type 'Unknown' is not supported in azure (azureml.train.automl)",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":382,
        "Challenge_word_count":220,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522634496792,
        "Poster_location":"Rio de Janeiro, RJ, Brasil",
        "Poster_reputation_count":264.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train#data-source-and-format?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">Configure AutoML Doc<\/a> says:<\/p>\n<blockquote>\n<p>For remote experiments, training data must be accessible from the remote compute. AutoML only accepts Azure Machine Learning TabularDatasets when working on a remote compute.<\/p>\n<\/blockquote>\n<p>It looks as if your <code>dataset<\/code> object is a Pandas DataFrame, when it should really be an Azure ML <code>Dataset<\/code>. Check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-datasets?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">this doc<\/a> on creating Datasets.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"messag panda datafram dataset object dataset creat dataset input document creat regist dataset guidanc",
        "Solution_last_edit_time":1603005259150,
        "Solution_link_count":2.0,
        "Solution_original_content":"configur automl doc sai remot train data access remot comput automl accept tabulardataset remot comput dataset object panda datafram dataset doc creat dataset",
        "Solution_preprocessed_content":"configur automl doc sai remot train data access remot comput automl accept tabulardataset remot comput object panda datafram doc creat dataset",
        "Solution_readability":16.5,
        "Solution_reading_time":9.99,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":8.3539944444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am getting memory error while creating simple dataframe read from CSV file on Azure Machine Learning using notebook VM as compute instance. The VM has config of DS 13 56gb RAM, 8vcpu, 112gb storage on Ubuntu (Linux (ubuntu 16.04). CSV file is 5gb file. <\/p>\n\n<pre><code>blob_service = BlockBlobService(account_name,account_key)\nblobstring = blob_service.get_blob_to_text(container,filepath).content\ndffinaldata = pd.read_csv(StringIO(blobstring), sep=',')\n<\/code><\/pre>\n\n<p>What I am doing wrong here ?<\/p>",
        "Challenge_closed_time":1579583849896,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579544749467,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1579556126092,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59829017",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":7.36,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":10.8612302778,
        "Challenge_title":"Azure Machine Learning - Memory Error while creating dataframe",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":507,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1500744375327,
        "Poster_location":null,
        "Poster_reputation_count":255.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>you need to provide the right encoding when calling get_blob_to_text, please refer to the <a href=\"https:\/\/github.com\/Azure\/azure-storage-python\/blob\/master\/samples\/blob\/block_blob_usage.py#L390\" rel=\"nofollow noreferrer\">sample<\/a>.<\/p>\n\n<p>The code below is what  normally use for reading data file in blob storages. Basically, you can use blob\u2019s url along with sas token and use a request method. However, You might want to edit the \u2018for loop\u2019 depending what types of data you have (e.g. csv, jpg, and etc).<\/p>\n\n<p>-- Python code below --<\/p>\n\n<pre><code>import requests\nfrom azure.storage.blob import BlockBlobService, BlobPermissions\nfrom azure.storage.blob.baseblobservice import BaseBlobService\nfrom datetime import datetime, timedelta\n\naccount_name = '&lt;account_name&gt;'\naccount_key = '&lt;account_key&gt;'\ncontainer_name = '&lt;container_name&gt;'\n\nblob_service=BlockBlobService(account_name,account_key)\ngenerator = blob_service.list_blobs(container_name)\n\nfor blob in generator:\n    url = f\"https:\/\/{account_name}.blob.core.windows.net\/{container_name}\"\n    service = BaseBlobService(account_name=account_name, account_key=account_key)\n    token = service.generate_blob_shared_access_signature(container_name, img_name, permission=BlobPermissions.READ, expiry=datetime.utcnow() + timedelta(hours=1),)\n    url_with_sas = f\"{url}?{token}\"\n    response = requests.get(url_with_sas)\n<\/code><\/pre>\n\n<p>Please follow the below link to read data on Azure Blob Storage.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"encod call blob text read data file blob storag blob url sa token request edit loop depend type data link document access data blob storag",
        "Solution_last_edit_time":1579586200472,
        "Solution_link_count":4.0,
        "Solution_original_content":"encod call blob text sampl normal read data file blob storag blob url sa token request edit loop depend type data csv jpg import request storag blob import blockblobservic blobpermiss storag blob baseblobservic import baseblobservic datetim import datetim timedelta account account kei blob servic blockblobservic account account kei gener blob servic list blob blob gener url http account blob core window net servic baseblobservic account account account kei account kei token servic gener blob share access signatur img permiss blobpermiss read expiri datetim utcnow timedelta hour url sa url token respons request url sa link read data blob storag http doc com access data",
        "Solution_preprocessed_content":"encod call sampl normal read data file blob storag blob url sa token request edit loop depend type data link read data blob storag",
        "Solution_readability":17.2,
        "Solution_reading_time":22.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1565697423932,
        "Answerer_location":"Uzbekistan",
        "Answerer_reputation_count":602.0,
        "Answerer_view_count":117.0,
        "Challenge_adjusted_solved_time":8037.6370116666,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How do I optimize for multiple metrics simultaneously inside the <code>objective<\/code> function of Optuna. For example, I am training an LGBM classifier and want to find the best hyperparameter set for all common classification metrics like F1, precision, recall, accuracy, AUC, etc.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    # Train\n    gbm = lgb.train(param, dtrain)\n\n    preds = gbm.predict(X_test)\n    pred_labels = np.rint(preds)\n    # Calculate metrics\n    accuracy = sklearn.metrics.accuracy_score(y_test, pred_labels)\n    recall = metrics.recall_score(pred_labels, y_test)\n    precision = metrics.precision_score(pred_labels, y_test)\n    f1 = metrics.f1_score(pred_labels, y_test, pos_label=1)\n\n    ...\n<\/code><\/pre>\n<p>How do I do it?<\/p>",
        "Challenge_closed_time":1630917852487,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630917852487,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1630917952870,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69071684",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":10.3,
        "Challenge_score_count":2,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.0,
        "Challenge_title":"How to optimize for multiple metrics in Optuna",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1887,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565697423932,
        "Poster_location":"Uzbekistan",
        "Poster_reputation_count":602.0,
        "Poster_view_count":117.0,
        "Solution_body":"<p>After defining the grid and fitting the model with these params and generate predictions, calculate all metrics you want to optimize for:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    param_grid = {&quot;n_estimators&quot;: trial.suggest_int(&quot;n_estimators&quot;, 2000, 10000, step=200)}\n    clf = lgbm.LGBMClassifier(objective='binary', **param_grid)\n    clf.fit(X_train, y_train)\n    preds = clf.predict(X_valid)\n    probs = clf.predict_proba(X_valid)\n \n    # Metrics\n    f1 = sklearn.metrics.f1_score(y_valid, press)\n    accuracy = ...\n    precision = ...\n    recall = ...\n    logloss = ...\n<\/code><\/pre>\n<p>and return them in the order you want:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    ...\n\n    return f1, logloss, accuracy, precision, recall\n<\/code><\/pre>\n<p>Then, in the study object, specify whether you want to minimize or maximize each metric to <code>directions<\/code> like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>study = optuna.create_study(directions=['maximize', 'minimize', 'maximize', 'maximize', 'maximize'])\n\nstudy.optimize(objective, n_trials=100)\n<\/code><\/pre>\n<p>For more details, see <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/002_multi_objective.html#sphx-glr-tutorial-20-recipes-002-multi-objective-py\" rel=\"nofollow noreferrer\">Multi-objective Optimization with Optuna<\/a> in the documentation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"defin grid fit model paramet gener predict calcul metric optim return metric order optim specifi minim maxim metric studi object direct paramet document link multi object optim",
        "Solution_last_edit_time":1659853446112,
        "Solution_link_count":1.0,
        "Solution_original_content":"defin grid fit model param gener predict calcul metric optim object trial param grid estim trial estim step clf lgbm lgbmclassifi object binari param grid clf fit train train pred clf predict prob clf predict proba metric sklearn metric score press accuraci precis recal logloss return order object trial return logloss accuraci precis recal studi object specifi minim maxim metric direct studi creat studi direct maxim minim maxim maxim maxim studi optim object trial multi object optim document",
        "Solution_preprocessed_content":"defin grid fit model param gener predict calcul metric optim return order studi object specifi minim maxim metric optim document",
        "Solution_readability":18.8,
        "Solution_reading_time":18.47,
        "Solution_score_count":6.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":113.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":12.2421333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am a newbie in this, and I am facing some problems with the Azure ML workspace. I ran a python code from the terminal, and then I opened another terminal to check the process. I got the following message in the terminal that checked the process:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9XLPw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9XLPw.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What does this mean? It keeps running, but I don't know if it is a bad message. It takes soo long, and I don't want to lose the processing time.<\/p>\n<p>I appreciate any tips.<\/p>",
        "Challenge_closed_time":1651825622423,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651781550743,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72133111",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":8.12,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":12.2421333334,
        "Challenge_title":"Azure ML: What means reconnecting terminal?",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":48,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1575137776887,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":21.0,
        "Solution_body":"<blockquote>\n<p>What does this mean? It keeps running, but I don't know if it is a bad message. It takes soo long, and I don't want to lose the processing time.<\/p>\n<\/blockquote>\n<ul>\n<li><code>Reconnecting terminal<\/code> message can appear for multiple reasons like intermittent connectivity issues, unused active terminal sessions, processing of different size\/format of data.<\/li>\n<li>Make sure you close any unused terminal sessions to preserve your compute instance's resources. Idle terminals may impact the performance of compute instances.<\/li>\n<\/ul>\n<p>You can refer to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#manage-terminal-sessions\" rel=\"nofollow noreferrer\">Access a compute instance terminal in your workspace<\/a>, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-optimize-data-processing\" rel=\"nofollow noreferrer\">Optimize data processing with Azure Machine Learning<\/a> and <a href=\"https:\/\/www.youtube.com\/watch?v=kiScfw9i4FM\" rel=\"nofollow noreferrer\">Azure ML: Speed up processing time<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"reconnect termin messag multipl reason intermitt connect unus activ termin session process size format data close unus termin session preserv comput instanc resourc idl termin impact perform comput instanc document termin session optim data process speed process time",
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"keep run messag soo lose process time reconnect termin messag multipl reason intermitt connect unus activ termin session process size format data close unus termin session preserv comput instanc resourc idl termin impact perform comput instanc access comput instanc termin workspac optim data process speed process time",
        "Solution_preprocessed_content":"keep run messag soo lose process time messag multipl reason intermitt connect unus activ termin session process data close unus termin session preserv comput instanc resourc idl termin impact perform comput instanc access comput instanc termin workspac optim data process speed process time",
        "Solution_readability":12.5,
        "Solution_reading_time":14.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":114.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":45.3307377778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have 3 main process to perform using Amazon SageMaker.<\/p>\n\n<ol>\n<li>Using own training python script, (not using sagemaker container, inbuilt algorithm) [Train.py]<\/li>\n<\/ol>\n\n<p>-> For this, I have referred to this link:<br>\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/train-and-host-scikit-learn-models-in-amazon-sagemaker-by-building-a-scikit-docker-container\/\" rel=\"nofollow noreferrer\">Bring own algorithm to AWS sagemaker<\/a>\nand it seems that we can bring our own training script to sagemaker managed training setup, and model artifacts can be uploaded to s3 etc.\nNote: I am using Light GBM model for training.<\/p>\n\n<ol start=\"2\">\n<li>Writing forecast to AWS RDS DB:<\/li>\n<\/ol>\n\n<p>-> There is no need to deploy model and create endpoint, because training will happen everyday, and will create forecast as soon as training completes. (Need to generate forecast in train.py itself)<\/p>\n\n<p>-> <strong>Challenge is how can I write forecast in AWS RDS DB from train.py script. (Given that script is running in Private VPC)<\/strong><\/p>\n\n<ol start=\"3\">\n<li>Scheduling this process as daily job:<\/li>\n<\/ol>\n\n<p>--> I have gone through AWS step functions and seems to be the way to trigger daily training and write forecast to RDS.<\/p>\n\n<p>--> <strong>Challenge is how to use step function for time based trigger and not event based.<\/strong><\/p>\n\n<p>Any suggestions on how to do this? Any best practices to follow? Thank you in advance.<\/p>",
        "Challenge_closed_time":1566713256896,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566547602673,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1566550066240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57622122",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":19.38,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":46.0150619444,
        "Challenge_title":"Custom sagemaker container for training, write forecast to AWS RDS, on a daily basis",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":466,
        "Challenge_word_count":217,
        "Platform":"Stack Overflow",
        "Poster_created_time":1469183608840,
        "Poster_location":"Gurugram, Haryana, India",
        "Poster_reputation_count":624.0,
        "Poster_view_count":67.0,
        "Solution_body":"<p>The way to trigger Step Functions on schedule is by using CloudWatch Events (sort of cron). Check out this tutorial: <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/tutorial-cloudwatch-events-target.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/tutorial-cloudwatch-events-target.html<\/a><\/p>\n\n<p>Don't write to the RDS from your Python code! It is better to write the output to S3 and then \"copy\" the files from S3 into the RDS. Decoupling these batches will make a more reliable and scalable process. You can trigger the bulk copy into the RDS when the files are written to S3 or to a later time when your DB is not too busy. <\/p>",
        "Solution_comment_count":5.0,
        "Solution_gpt_summary":"cloudwatch event trigger step function schedul write output copi file rd write rd directli process reliabl scalabl",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"trigger step function schedul cloudwatch event sort cron tutori http doc com step function latest tutori cloudwatch event target html write rd write output copi file rd decoupl batch reliabl scalabl process trigger bulk copi rd file written later time busi",
        "Solution_preprocessed_content":"trigger step function schedul cloudwatch event tutori write rd write output copi file rd decoupl batch reliabl scalabl process trigger bulk copi rd file written later time busi",
        "Solution_readability":9.6,
        "Solution_reading_time":8.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":92.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1451214281323,
        "Answerer_location":null,
        "Answerer_reputation_count":602.0,
        "Answerer_view_count":214.0,
        "Challenge_adjusted_solved_time":9321.2005063889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is it possible to connect a notebook running in premises to an mlflow Tracking server that is part of an Azure Databricks workspace? Have all the local logging and tracking saved in Azure?<\/p>",
        "Challenge_closed_time":1559118227527,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559088053237,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56351452",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":3.26,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":8.3817472222,
        "Challenge_title":"Connect on-prem jypyter notebook to mlflow tracking server in Azure",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":291,
        "Challenge_word_count":42,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343948494332,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation_count":1390.0,
        "Poster_view_count":121.0,
        "Solution_body":"<p>I had a similar problem, used python and solved it with the following steps:<\/p>\n<ol>\n<li>Install mlflow and datbricks-cli libraries.<\/li>\n<li>Define the following env variables : DATABRICKS_HOST (databricks workspace url: <a href=\"https:\/\/region.azuredatabricks.net\" rel=\"nofollow noreferrer\">https:\/\/region.azuredatabricks.net<\/a>) and DATABRICKS_TOKEN<\/li>\n<li>Define mlflow client:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow_client = mlflow.tracking.MlflowClient(tracking_uri='databricks')\n<\/code><\/pre>\n<ol start=\"5\">\n<li>Use mlflow_client client for logging, saving and etc..<\/li>\n<\/ol>\n<p>for more reference you can look at the &quot;Log to a tracking server from a notebook&quot; section <a href=\"https:\/\/docs.azuredatabricks.net\/applications\/mlflow\/tracking.html#log-to-a-tracking-server-from-a-notebook\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":1592644375060,
        "Solution_link_count":3.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":14.6,
        "Solution_reading_time":11.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":76.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1351534968768,
        "Answerer_location":null,
        "Answerer_reputation_count":823.0,
        "Answerer_view_count":114.0,
        "Challenge_adjusted_solved_time":43.0164288889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I know how to access packages in <code>R<\/code> scripts in <code>Azure machine<\/code> learning by either using the <code>Azure<\/code> supported ones or by zipping up the packages.<\/p>\n\n<p>My problem now is that <code>Azure<\/code> machine learning does not support the <code>h2o package<\/code> and when I tried using the zipped file - it gave an <code>error<\/code>. <\/p>\n\n<p>Has anyone figured out how to use <code>h2o<\/code> in <code>R<\/code> in <code>Azure machine<\/code> learning?<\/p>",
        "Challenge_closed_time":1487237805672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1487082559403,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1487082946528,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42228715",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":6.79,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":43.1239636111,
        "Challenge_title":"Running h2o in R script in Azure machine learning",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":361,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351534968768,
        "Poster_location":null,
        "Poster_reputation_count":823.0,
        "Poster_view_count":114.0,
        "Solution_body":"<p>So since there was no reply to my question, I made some research and came up with the following:<\/p>\n\n<p>H2O cannot be ran in a straightforward manner in Azure machine learning embedded R scripts. A workaround the problem is to consider using an Azure created environment - specially for H2O. The options available are:<\/p>\n\n<ol>\n<li>Spinning up an H2O Artificial Intelligence Virtual Machine solution<\/li>\n<li>Using an H2O application for HDInsight<\/li>\n<\/ol>\n\n<p>For more reading, you can go to: <a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/azure.html\" rel=\"nofollow noreferrer\">http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/azure.html<\/a> <\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"run directli embed workaround creat environ option spin artifici intellig virtual hdinsight http doc latest stabl doc html",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"repli research came ran straightforward embed workaround creat environ option spin artifici intellig virtual hdinsight read http doc latest stabl doc html",
        "Solution_preprocessed_content":"repli research came ran straightforward embed workaround creat environ option spin artifici intellig virtual hdinsight read",
        "Solution_readability":14.8,
        "Solution_reading_time":8.43,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":1803.6491758334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to execute ClearML task remotely. According to docs there are 2 options: 1) execute single python file; 2) ClearML would identify that script is part of repo, that repo will be cloned and installed into docker and executed on the worker.<\/p>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<p>I think it is somewhat extending scenario 1, where not a single file is passed for execution but whole directory with file in it.<\/p>\n<p>PS: i understand reproducibility concerns that arise, but repo is really not accessible from worker :(<\/p>\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1659992618003,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653499480970,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72381916",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.62,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1803.6491758334,
        "Challenge_title":"Remotely execute ClearML task using local-only repo",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":25,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Poster_created_time":1653498830776,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<blockquote>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<\/blockquote>\n<p>well, no :( if your code is a single script, then yes ClearML would store the entire script, then the worker will reproduce it on the remote machine. But if your code base is composed of more than a single file, then why not use git? it is free hosted by GitHub, Bitbucket, GitLab etc.<\/p>\n<p>In theory this is doable and if you feel the need, I urge you to PR this feature. Basically you would store the entire folder as an artifact (ClearML will auto zip it for you), then the agent needs to unzip the artifact and run it. The main issue would be that cloning the Task will not clone the artifact...<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"execut singl file clone instal repo docker execut worker option git store entir base free host github bitbucket gitlab pack local repo send remot execut theori doabl submit featur request store entir folder artifact auto zip agent unzip artifact",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"disclaim team member repo remot url access worker isn pack local repo send remot execut singl store entir worker reproduc remot base compos singl file git free host github bitbucket gitlab theori doabl urg featur store entir folder artifact auto zip agent unzip artifact run clone task clone artifact",
        "Solution_preprocessed_content":"disclaim team member repo remot url access worker isn pack local repo send remot execut singl store entir worker reproduc remot base compos singl file git free host github bitbucket gitlab theori doabl urg featur store entir folder artifact agent unzip artifact run clone task clone",
        "Solution_readability":6.8,
        "Solution_reading_time":10.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":162.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1574678086832,
        "Answerer_location":"Amstelveen, Netherlands",
        "Answerer_reputation_count":3917.0,
        "Answerer_view_count":640.0,
        "Challenge_adjusted_solved_time":6.4474319444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have installed mlflow on GCP VM instance, \nnow I want to access mlflow UI with external IP.\nI tried setting up a firewall rule and opening the default port for mlflow, but not able to access it.\nCan someone give step by step process for just running mlflow on VM instance?<\/p>",
        "Challenge_closed_time":1583767598368,
        "Challenge_comment_count":3,
        "Challenge_created_time":1583744387613,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1583832420660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60597319",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":5.2,
        "Challenge_reading_time":3.67,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":6.4474319444,
        "Challenge_title":"Running MLFlow on GCP VM",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":1537,
        "Challenge_word_count":56,
        "Platform":"Stack Overflow",
        "Poster_created_time":1451124057623,
        "Poster_location":"India",
        "Poster_reputation_count":736.0,
        "Poster_view_count":234.0,
        "Solution_body":"<p>I've decided to check on my test VM and run mlflow server on GCE VM. Have a look at my steps below:<\/p>\n\n<ol>\n<li>create VM instance based on Ubuntu Linux 18.04 LTS<\/li>\n<li><p><a href=\"https:\/\/www.mlflow.org\/docs\/latest\/quickstart.html\" rel=\"noreferrer\">install MLflow<\/a>:<\/p>\n\n<pre><code>$ sudo apt update\n$ sudo apt upgrade\n$ cd ~\n$ git clone https:\/\/github.com\/mlflow\/mlflow\n$ cd mlflow\n$ sudo apt install python3-pip\n$ pip3 install mlflow\n$ python3 setup.py build\n$ sudo python3 setup.py install\n$ mlflow --version\nmlflow, version 1.7.1.dev0\n<\/code><\/pre><\/li>\n<li><p>run mlflow server on internal IP of VM instance (default 127.0.0.1):<\/p>\n\n<pre><code>$ ifconfig \nens4: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1460\ninet 10.XXX.15.XXX  netmask 255.255.255.255  broadcast 0.0.0.0\n...\n\n$ mlflow server --host 10.XXX.15.XXX\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Starting gunicorn 20.0.4\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Listening at: http:\/\/10.128.15.211:5000 (8631)\n[2020-03-09 15:05:50 +0000] [8631] [INFO] Using worker: sync\n[2020-03-09 15:05:50 +0000] [8634] [INFO] Booting worker with pid: 8634\n[2020-03-09 15:05:51 +0000] [8635] [INFO] Booting worker with pid: 8635\n[2020-03-09 15:05:51 +0000] [8636] [INFO] Booting worker with pid: 8636\n[2020-03-09 15:05:51 +0000] [8638] [INFO] Booting worker with pid: 8638\n<\/code><\/pre><\/li>\n<li><p>check from VM instance (from second connection):<\/p>\n\n<pre><code>$ curl -I http:\/\/10.XXX.15.XXX:5000\nHTTP\/1.1 200 OK\nServer: gunicorn\/20.0.4\nDate: Mon, 09 Mar 2020 15:06:08 GMT\nConnection: close\nContent-Length: 853\nContent-Type: text\/html; charset=utf-8\nLast-Modified: Mon, 09 Mar 2020 14:57:11 GMT\nCache-Control: public, max-age=43200\nExpires: Tue, 10 Mar 2020 03:06:08 GMT\nETag: \"1583765831.3202355-853-3764264575\"\n<\/code><\/pre><\/li>\n<li><p><a href=\"https:\/\/cloud.google.com\/vpc\/docs\/add-remove-network-tags\" rel=\"noreferrer\">set network tag<\/a> <code>mlflow-server<\/code> <\/p><\/li>\n<li><p><a href=\"https:\/\/cloud.google.com\/vpc\/docs\/using-firewalls#creating_firewall_rules\" rel=\"noreferrer\">create firewall rule<\/a> to allow access on port 5000<\/p>\n\n<pre><code>$ gcloud compute --project=test-prj firewall-rules create mlflow-server --direction=INGRESS --priority=999 --network=default --action=ALLOW --rules=tcp:5000 --source-ranges=0.0.0.0\/0 --target-tags=mlflow-server\n<\/code><\/pre><\/li>\n<li><p>check from on-premises Linux machine <code>nmap -Pn 35.225.XXX.XXX<\/code><\/p>\n\n<pre><code>Starting Nmap 7.80 ( https:\/\/nmap.org ) at 2020-03-09 16:20 CET\nNmap scan report for 74.123.225.35.bc.googleusercontent.com (35.225.XXX.XXX)\nHost is up (0.20s latency).\nNot shown: 993 filtered ports\nPORT     STATE  SERVICE\n...\n5000\/tcp open   upnp\n...\n<\/code><\/pre><\/li>\n<li><p>go to web browser <a href=\"http:\/\/35.225.XXX.XXX:5000\/\" rel=\"noreferrer\">http:\/\/35.225.XXX.XXX:5000\/<\/a><\/p><\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/u2aFt.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/u2aFt.png\" alt=\"mlflow\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":11.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":8.4,
        "Solution_reading_time":39.18,
        "Solution_score_count":5.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":297.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1564118772683,
        "Answerer_location":null,
        "Answerer_reputation_count":4730.0,
        "Answerer_view_count":167.0,
        "Challenge_adjusted_solved_time":156.0215711111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I wrote a simple Keras code, in which I use CNN for fashion mnist dataset. Everything works great. I implemented my own class and classification is OK.<\/p>\n<p>However, I wanted to use Optuna, as OptKeras (Optuna wrapper for Keras), you can see an example here: <a href=\"https:\/\/medium.com\/@Minyus86\/optkeras-112bcc34ec73\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@Minyus86\/optkeras-112bcc34ec73<\/a>.<\/p>\n<p>However, something is wrong with my code. When I try to use optKeras inside my own class. Here's the code: (ordinary <code>run<\/code> method works, but <code>optuna_run<\/code> gives an error: <code>AttributeError: type object 'FrozenTrial' has no attribute '_field_types'<\/code>.<\/p>\n<pre><code>! pip install optkeras\n# -*- coding: utf-8 -*- \n#!\/usr\/bin\/env python3\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN \nfrom keras.callbacks import ModelCheckpoint\nfrom keras import backend as K\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics\n\nimport optuna\nfrom optkeras.optkeras import OptKeras\n\nimport sys\nimport math\nimport numpy\nimport scipy.io as sio   \nimport matplotlib.pyplot as plt\n\nclass OptunaTest():\n\n  def __init__(self):\n    self.fashion_mnist = keras.datasets.fashion_mnist\n    (self.train_images, self.train_labels), (self.test_images, self.test_labels) = self.fashion_mnist.load_data()\n    self.class_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n    self.train_images = self.train_images \/ 255.0\n    self.test_images = self.test_images \/ 255.0\n    self.model = None \n    self.study_name = 'FashionMnist' + '_Simple'\n    self.ok = OptKeras(study_name=self.study_name)\n\n  def run(self):\n    self.model = keras.Sequential()\n    self.model.add(keras.layers.Flatten(input_shape=(28, 28)))\n    self.model.add(keras.layers.Dense(128, activation='relu'))\n    self.model.add(keras.layers.Dense(10))\n    self.model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n    self.model.fit(self.train_images, self.train_labels, epochs=5)\n    test_loss, test_acc = self.model.evaluate(self.test_images, self.test_labels, verbose=0)\n    predictions = self.model.predict(self.test_images)\n\n    INDEX = 10\n    print(&quot;\\nPREDICTION: &quot; + str(predictions[INDEX]))\n    print(&quot;\\nMAX PREDICTION VAL: &quot; + str(numpy.argmax(predictions[INDEX])))\n    print(&quot;\\nLABEL: &quot; + str(self.test_labels[INDEX]))\n\n  def optuna_run(self, trial):\n    K.clear_session() \n    \n    self.model = keras.Sequential()\n    self.model.add(keras.layers.Flatten(input_shape=(28, 28)))\n    self.model.add(keras.layers.Dense(units = trial.suggest_categorical('units', [32, 64, 128]), activation = trial.suggest_categorical('activation', ['relu', 'linear'])))\n    self.model.add(keras.layers.Dense(units = trial.suggest_categorical('units', [32, 64, 128]), activation = trial.suggest_categorical('activation', ['relu', 'linear'])))\n\n    self.model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n    self.model.fit(self.train_images, self.train_labels, epochs=5, callbacks = self.ok.callbacks(trial), verbose = self.ok.keras_verbose)\n    test_loss, test_acc = self.model.evaluate(self.test_images, self.test_labels, verbose=0)\n    predictions = self.model.predict(self.test_images)\n    print(ok.trial_best_value)\n    \n    INDEX = 10\n    print(&quot;\\nPREDICTION: &quot; + str(predictions[INDEX]))\n    print(&quot;\\nMAX PREDICTION VAL: &quot; + str(numpy.argmax(predictions[INDEX])))\n    print(&quot;\\nLABEL: &quot; + str(self.test_labels[INDEX]))\n\n\nif __name__ == &quot;__main__&quot;:\n  ot = OptunaTest()\n  ot.run()\n\n  ot.ok.optimize(ot.optuna_run,  timeout = 60)\n<\/code><\/pre>\n<p>A code can also be found here: <a href=\"https:\/\/colab.research.google.com\/drive\/1uibWa80BdjatA5Kcw27eMUsS7SmwxaDk?usp=sharing\" rel=\"nofollow noreferrer\">https:\/\/colab.research.google.com\/drive\/1uibWa80BdjatA5Kcw27eMUsS7SmwxaDk?usp=sharing<\/a>.<\/p>\n<p>The full error message:<\/p>\n<pre><code>[W 2020-06-30 11:09:26,959] Setting status of trial#0 as TrialState.FAIL because of the following error: AttributeError(&quot;type object 'FrozenTrial' has no attribute '_field_types'&quot;,)\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py&quot;, line 230, in synch_with_optuna\n    self.best_trial = self.study.best_trial\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optuna\/study.py&quot;, line 97, in best_trial\n    return copy.deepcopy(self._storage.get_best_trial(self._study_id))\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optuna\/storages\/in_memory.py&quot;, line 293, in get_best_trial\n    raise ValueError(&quot;No trials are completed yet.&quot;)\nValueError: No trials are completed yet.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optuna\/study.py&quot;, line 734, in _run_trial\n    result = func(trial)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py&quot;, line 130, in fun_tf\n    return fun(trial)\n  File &quot;&lt;ipython-input-11-45495c9f2ae9&gt;&quot;, line 65, in optima_run\n    self.model.fit(self.train_images, self.train_labels, epochs=10, callbacks = self.ok.callbacks(trial), verbose = self.ok.keras_verbose)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py&quot;, line 172, in callbacks\n    self.synch_with_optuna()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py&quot;, line 232, in synch_with_optuna\n    self.best_trial = get_trial_default()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py&quot;, line 367, in get_trial_default\n    num_fields = optuna.structs.FrozenTrial._field_types.__len__()\nAttributeError: type object 'FrozenTrial' has no attribute '_field_types'\n\n---------------------------------------------------------------------------\n\nValueError                                Traceback (most recent call last)\n\n\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py in synch_with_optuna(self)\n    229         try:\n--&gt; 230             self.best_trial = self.study.best_trial\n    231         except:\n\n12 frames\n\nValueError: No trials are completed yet.\n\n\nDuring handling of the above exception, another exception occurred:\n\nAttributeError                            Traceback (most recent call last)\n\n\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py in get_trial_default()\n    365 \n    366 def get_trial_default():\n--&gt; 367     num_fields = optuna.structs.FrozenTrial._field_types.__len__()\n    368     assert num_fields in (10, 11, 12)\n    369     if num_fields == 12: # possible future version\n\nAttributeError: type object 'FrozenTrial' has no attribute '_field_types'\n<\/code><\/pre>",
        "Challenge_closed_time":1593917427550,
        "Challenge_comment_count":1,
        "Challenge_created_time":1593516495007,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62656411",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":17.9,
        "Challenge_reading_time":93.74,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":75,
        "Challenge_solved_time":111.3701508334,
        "Challenge_title":"OptKeras (Keras Optuna Wrapper) - use optkeras inside my own class, AttributeError: type object 'FrozenTrial' has no attribute '_field_types'",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":888,
        "Challenge_word_count":541,
        "Platform":"Stack Overflow",
        "Poster_created_time":1344355535128,
        "Poster_location":null,
        "Poster_reputation_count":3580.0,
        "Poster_view_count":962.0,
        "Solution_body":"<p>It seems that optkeras (version I got was 0.0.7) being not quite up-to-date with optuna library is the reason for the issue. I was able to make it work with optuna 1.5.0 by doing the following changes:<\/p>\n<p>First, you'll need to monkey-patch <code>get_default_trial<\/code> like this before running your code:<\/p>\n<pre><code>import optkeras\noptkeras.optkeras.get_trial_default = lambda: optuna.trial.FrozenTrial(\n            None, None, None, None, None, None, None, None, None, None, None)\n<\/code><\/pre>\n<p>After doing so I'm getting an error with <code>Callback<\/code> saying:<\/p>\n<pre><code>AttributeError: 'OptKeras' object has no attribute '_implements_train_batch_hooks'\n<\/code><\/pre>\n<p>To solve this you'll have to manually edit optkeras.py, but not too much - just add <code>tensorflow.<\/code> to first two lines imports, i.e. make them:<\/p>\n<pre><code>import tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import Callback, CSVLogger, ModelCheckpoint\n<\/code><\/pre>\n<p>instead of:<\/p>\n<pre><code>import keras.backend as K\nfrom keras.callbacks import Callback, CSVLogger, ModelCheckpoint\n<\/code><\/pre>\n<p>If you can't change the code after installation it might be a bit of a problem - I would probably just recommend to copy full code of optkeras library (it's just one file optkeras.py) and use fixed version of that in your script or something like that. Unfortunately I don't see a nice way of monkey-patching this import issue. That said I think it can be fairly easy to either change that on-fly even from python (i.e. change <code>optkeras.py<\/code> lines from within python before importing optkeras) or copying the optkeras.py (also from withing python script) + replacing the strings on fly, then importing from the new location.<\/p>\n<p>After that is done I just had to:<\/p>\n<ul>\n<li>fix typo in your code (<code>print(ok.trial_best_value)<\/code> should really be <code>print(self.ok.trial_best_value)<\/code>)<\/li>\n<li>add <code>validation_split=0.1<\/code> to <code>self.model.fit<\/code> call (or you may use something else for your tuning - just with existing code example callback won't get <code>val_loss<\/code> value because there is no validation set and optkeras is using <code>val_loss<\/code> by default - see <code>monitor<\/code> argument for <code>OptKeras<\/code> constructor). My guess would be that you probably will either want to create a fixed validation set instead or monitor training loss <code>loss<\/code> instead of <code>val_loss<\/code>.<\/li>\n<li>add <code>return test_loss<\/code> at the end of <code>optuna_run<\/code> method.<\/li>\n<\/ul>\n<p>After all of these changes everything seems be working.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_gpt_summary":"optkera date librari monkei patch default trial run callback manual edit optkera instal copi optkera librari version typo trial valu add split model fit add return test loss end run",
        "Solution_last_edit_time":1594078172663,
        "Solution_link_count":0.0,
        "Solution_original_content":"optkera version date librari reason monkei patch default trial run import optkera optkera optkera trial default lambda trial frozentri callback sai attributeerror optkera object attribut implement train batch hook manual edit optkera add tensorflow line import import tensorflow kera backend tensorflow kera callback import callback csvlogger modelcheckpoint import kera backend kera callback import callback csvlogger modelcheckpoint instal bit probabl copi optkera librari file optkera version unfortun nice monkei patch import said fly optkera line import optkera copi optkera with replac fly import locat typo print trial valu print trial valu add split model fit tune callback val loss valu set optkera val loss default monitor argument optkera constructor guess probabl creat set monitor train loss loss val loss add return test loss end run",
        "Solution_preprocessed_content":"optkera librari reason run sai manual edit add line import instal bit probabl copi optkera librari version unfortun nice import said copi replac fly import locat typo add guess probabl creat set monitor train loss add end",
        "Solution_readability":9.6,
        "Solution_reading_time":33.73,
        "Solution_score_count":3.0,
        "Solution_sentence_count":26.0,
        "Solution_word_count":343.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1547628024160,
        "Answerer_location":"Brussels, Belgium",
        "Answerer_reputation_count":3378.0,
        "Answerer_view_count":475.0,
        "Challenge_adjusted_solved_time":0.3972655556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm migrating from Tensorflow 1.12 to Tensorflow 1.10 (Collaboratory -> AWS sagemaker), the code seems to be working fine in Tensorflow 1.12 but in 1.10 i get an error <code>ValueError: Error when checking target: expected dense to have 2 dimensions, but got array with shape (52692,)<\/code><\/p>\n\n<p>Input example - strings with no whitespaces: <\/p>\n\n<pre><code>[\"testAbc\", \"aaDD\", \"roam\"]\n<\/code><\/pre>\n\n<p>which I preprocess by changing small letters into 1, capital letters 2, digits - 3, '-' - 4, '_' - 5 and padding so they are equal length with 0s<\/p>\n\n<p>and 4 labels a - 0, b - 1, c - 2, d - 3<\/p>\n\n<p>Assuming max length for each word is 10 (in my code it's 20):<\/p>\n\n<p>features - [[1 1 1 1 2 1 1 0 0 0][1 1 2 2 0 0 0 0 0 0][1 1 1 1 0 0 0 0 0 0]]<\/p>\n\n<p>labels - [1, 1, 2, 3]<\/p>\n\n<p>expected output: [a: 0%, b: 0%, c: 1%, d: 99%] (example)<\/p>\n\n<pre><code>model = keras.Sequential()\nmodel.add(\n    keras.layers.Embedding(6, 8, input_length=maxFeatureLen))\nmodel.add(keras.layers.LSTM(12))\nmodel.add(keras.layers.Dense(4, activation=tf.nn.softmax))\nmodel.compile(tf.train.AdamOptimizer(0.001), loss=\"sparse_categorical_crossentropy\")\nmodel.fit(train[\"featuresVec\"],\n            train[\"labelsVec\"],\n            epochs=1,\n            verbose=1,\n            callbacks=[],\n            validation_data=(evale[\"featuresVec\"], evale[\"labelsVec\"],),\n            validation_steps=evale[\"count\"],\n            steps_per_epoch=train[\"count\"])\n<\/code><\/pre>\n\n<p>Shapes of train and evale - 2D arrays<\/p>\n\n<pre><code>train[\"featuresVec\"]=\n[[1 2 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n [1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [2 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n\nevale[\"featuresVec\"]=\n[[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n [1 1 1 1 1 1 2 1 1 1 1 1 0 0 0 0 0 0 0 0]\n [1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 0]\n [1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 0 0]\n [1 1 1 1 1 2 1 1 1 1 1 1 0 0 0 0 0 0 0 0]]\n\ntrain[\"labelsVec\"] = [1 0 0 0 2]\nevale[\"labelsVec\"] = [0 1 1 1 1]\n<\/code><\/pre>\n\n<p>Shapes:<\/p>\n\n<pre><code>train[\"featuresVec\"] = [52692, 20]\nevale[\"featuresVec\"] = [28916, 20]\ntrain[\"labelsVec\"] = [52692]\nevale[\"labelsVec\"] = [28916]\n<\/code><\/pre>",
        "Challenge_closed_time":1547917365403,
        "Challenge_comment_count":9,
        "Challenge_created_time":1547914327393,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1547915935247,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54268970",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":10,
        "Challenge_readability":17.2,
        "Challenge_reading_time":25.88,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.8438916667,
        "Challenge_title":"Wrong dense layer output shape after moving from TF 1.12 to 1.10",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":120,
        "Challenge_word_count":417,
        "Platform":"Stack Overflow",
        "Poster_created_time":1542146075152,
        "Poster_location":null,
        "Poster_reputation_count":91.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>Probably your labels vector needs to be of shape <code>(batch_size, 1)<\/code> instead of just <code>(batch_size,)<\/code>. <\/p>\n\n<p><strong>Note:<\/strong> Since you are using <code>sparse_categorical_crossentropy<\/code> as loss function instead of <code>categorical_crossentropy<\/code>, it is correct to not one-hot encode the labels. <\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"shape label vector batch size batch size spars categor crossentropi loss function categor crossentropi hot encod label",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"probabl label vector shape batch size batch size note spars categor crossentropi loss function categor crossentropi hot encod label",
        "Solution_preprocessed_content":"probabl label vector shape note loss function encod label",
        "Solution_readability":14.2,
        "Solution_reading_time":4.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1553882107003,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":294.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":2777.7798802778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am deploying a model onto AWS via Sagemaker:<\/p>\n\n<p>I set up my JSON schema as follow:<\/p>\n\n<pre><code>import json\nschema = {\n    \"input\": [\n        {\n            \"name\": \"V1\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V2\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V3\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V4\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V5\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V6\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V7\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V8\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V9\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V10\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V11\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V12\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V13\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V14\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V15\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V16\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V17\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V18\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V19\",\n            \"type\": \"double\"\n        }, \n                {\n            \"name\": \"V20\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V21\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V22\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V23\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V24\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V25\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V26\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V27\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V28\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"Amount\",\n            \"type\": \"double\"\n        },         \n    ],\n    \"output\": \n        {\n            \"name\": \"features\",\n            \"type\": \"double\",\n            \"struct\": \"vector\"\n        }\n}\nschema_json = json.dumps(schema)\nprint(schema_json)\n<\/code><\/pre>\n\n<p>And deployed as:<\/p>\n\n<pre><code>from sagemaker.model import Model\nfrom sagemaker.pipeline import PipelineModel\nfrom sagemaker.sparkml.model import SparkMLModel\n\nsparkml_data = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\n# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\nsparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\nxgb_model = Model(model_data=xgb_model.model_data, image=training_image)\n\nmodel_name = 'inference-pipeline-' + timestamp_prefix\nsm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])\n\n    endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I got the error as below:<\/p>\n\n<p>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: 1 validation error detected: Value '{SAGEMAKER_SPARKML_SCHEMA={\"input\": [{\"type\": \"double\", \"name\": \"V1\"}, {\"type\": \"double\", \"name\": \"V2\"}, {\"type\": \"double\", \"name\": \"V3\"}, {\"type\": \"double\", \"name\": \"V4\"}, {\"type\": \"double\", \"name\": \"V5\"}, {\"type\": \"double\", \"name\": \"V6\"}, {\"type\": \"double\", \"name\": \"V7\"}, {\"type\": \"double\", \"name\": \"V8\"}, {\"type\": \"double\", \"name\": \"V9\"}, {\"type\": \"double\", \"name\": \"V10\"}, {\"type\": \"double\", \"name\": \"V11\"}, {\"type\": \"double\", \"name\": \"V12\"}, {\"type\": \"double\", \"name\": \"V13\"}, {\"type\": \"double\", \"name\": \"V14\"}, {\"type\": \"double\", \"name\": \"V15\"}, {\"type\": \"double\", \"name\": \"V16\"}, {\"type\": \"double\", \"name\": \"V17\"}, {\"type\": \"double\", \"name\": \"V18\"}, {\"type\": \"double\", \"name\": \"V19\"}, {\"type\": \"double\", \"name\": \"V20\"}, {\"type\": \"double\", \"name\": \"V21\"}, {\"type\": \"double\", \"name\": \"V22\"}, {\"type\": \"double\", \"name\": \"V23\"}, {\"type\": \"double\", \"name\": \"V24\"}, {\"type\": \"double\", \"name\": \"V25\"}, {\"type\": \"double\", \"name\": \"V26\"}, {\"type\": \"double\", \"name\": \"V27\"}, {\"type\": \"double\", \"name\": \"V28\"}, {\"type\": \"double\", \"name\": \"Amount\"}], \"output\": {\"type\": \"double\", \"name\": \"features\", \"struct\": \"vector\"}}}' at 'containers.1**.<strong>member.environment' failed to satisfy constraint: Map value must satisfy constraint: [Member must have length less than or equal to 1024<\/strong>,** Member must have length greater than or equal to 0, Member must satisfy regular expression pattern: [\\S\\s]*]<\/p>\n\n<p>I try to reduce my features to 20 and it able to deploy. Just wondering how can I Pass the schema with 29 attributes?<\/p>",
        "Challenge_closed_time":1600448991412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590448983843,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62012264",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":17.0,
        "Challenge_reading_time":51.37,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2777.7798802778,
        "Challenge_title":"AWS SageMaker SparkML Schema Eroor: member.environment' failed to satisfy constraint",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":226,
        "Challenge_word_count":418,
        "Platform":"Stack Overflow",
        "Poster_created_time":1359061977540,
        "Poster_location":"New York, NY, USA",
        "Poster_reputation_count":427.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>I do not think the environment length of 1024 limit will be increased in a short time. To work around this, you could try to rebuild the spark ml container with the <code>SAGEMAKER_SPARKML_SCHEMA<\/code> env var:<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"rebuild spark sparkml schema env var",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"environ length limit increas short time rebuild spark sparkml schema env var http github com sparkml serv blob master readm run imag local",
        "Solution_preprocessed_content":"environ length limit increas short time rebuild spark env var",
        "Solution_readability":17.8,
        "Solution_reading_time":6.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":24.0739286111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Training a ml model with mlflow in azure environment.<\/p>\n<pre><code>import mlflow\nfrom mlflow import MlflowClient\nfrom azureml.core import Experiment, Workspace\n\nexperiment_name = 'housing-lin-mlflow'\n\nexperiment = Experiment(ws, experiment_name)\n\nruns = mlflow.search_runs(experiment_ids=[ experiment.id ])\n\n<\/code><\/pre>\n<p>While fetching runs from search_runs getting this error :<\/p>\n<pre><code>RestException: BAD_REQUEST: For input string: &quot;5b649b3c-3b8f-497a-bb4f&quot;\n<\/code><\/pre>\n<p>MLflow version : 1.28.0\nIn Azure studio jobs have been created and successfully run.<\/p>",
        "Challenge_closed_time":1661603882123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661517215980,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1661625379892,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73501103",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":8.33,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":24.0739286111,
        "Challenge_title":"Getting Bad request while searching run in mlflow",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":56,
        "Challenge_word_count":65,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582101477803,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":171.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>The bad request in MLFlow after successful running the job is because of not giving proper API permissions for the application.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Search for <strong>MLFLOW<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Scroll down<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s50AL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s50AL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on View API Permissions<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Under API permissions, assign the permissions according to the application running region and requirements. Checkout the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models-mlflow\" rel=\"nofollow noreferrer\">document<\/a> for further information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"request api permiss search scroll api permiss assign permiss accord run region document",
        "Solution_last_edit_time":null,
        "Solution_link_count":9.0,
        "Solution_original_content":"request run job api permiss search scroll click api permiss api permiss assign permiss accord run region checkout document",
        "Solution_preprocessed_content":"request run job api permiss search scroll click api permiss api permiss assign permiss accord run region checkout document",
        "Solution_readability":17.0,
        "Solution_reading_time":16.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":328.67735,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently deploying a model trained using AzureML to an AKS cluster as follows:<\/p>\n<pre><code>deployment_config_aks = AksWebservice.deploy_configuration(\n    cpu_cores = 1, \n    memory_gb = 1)\n\nservice = Model.deploy(ws, &quot;test&quot;, [model], inference_config, deployment_config_aks, aks_target)\n\n<\/code><\/pre>\n<p>I would like this service to be scheduled on a specific nodepool. With normal Kubernetes deployment, I can specify a <code>nodeSelector<\/code> like:<\/p>\n<pre><code>spec:\n  nodeSelector:\n    myNodeName: alpha\n<\/code><\/pre>\n<p>How do I specify a <code>nodeSelector<\/code> while deploying an Azure ML model to an AKS Cluster? Or in general, is there a way to merge my custom pod spec with the one generated by Azure ML library?<\/p>",
        "Challenge_closed_time":1650522075520,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649338837060,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71783228",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":15.0,
        "Challenge_reading_time":10.46,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":328.67735,
        "Challenge_title":"How do I specify nodeSelector while deploying an Azure ML model to an AKS Cluster?",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":289,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1338974093052,
        "Poster_location":"Beijing, China",
        "Poster_reputation_count":1830.0,
        "Poster_view_count":155.0,
        "Solution_body":"<blockquote>\n<p>How do I specify a nodeSelector while deploying an Azure ML model to an AKS Cluster? Or in general, is there a way to merge my custom pod spec with the one generated by Azure ML library?<\/p>\n<\/blockquote>\n<p>As per <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-attach-arc-kubernetes?tabs=studio\" rel=\"nofollow noreferrer\">Configure Kubernetes clusters for machine learning<\/a>:<\/p>\n<p><code>nodeSelector<\/code> : Set the node selector so the extension components and the training\/inference workloads will only be deployed to the nodes with all specified selectors.<\/p>\n<p>For example:<\/p>\n<p><code>nodeSelector.key=value<\/code> , <code>nodeSelector.node-purpose=worker<\/code> and <code>nodeSelector.node-region=eastus<\/code><\/p>\n<p>You can refer to <a href=\"https:\/\/kubernetes.io\/docs\/concepts\/scheduling-eviction\/assign-pod-node\/#built-in-node-labels\" rel=\"nofollow noreferrer\">Assigning Pods to Nodes<\/a> and <a href=\"https:\/\/github.com\/Azure\/AKS\/issues\/2866\" rel=\"nofollow noreferrer\">Cannot create nodepool with node-restriction.kubernetes.io\/ prefix label<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"specifi nodeselector deploi model ak cluster set node selector extens compon train infer workload deploi node specifi selector document",
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"specifi nodeselector deploi model ak cluster gener merg pod spec gener librari configur kubernet cluster nodeselector set node selector extens compon train infer workload deploi node specifi selector nodeselector kei valu nodeselector node purpos worker nodeselector node region eastu assign pod node creat nodepool node restrict kubernet prefix label",
        "Solution_preprocessed_content":"specifi nodeselector deploi model ak cluster gener merg pod spec gener librari configur kubernet cluster set node selector extens compon workload deploi node specifi selector assign pod node creat nodepool prefix label",
        "Solution_readability":17.9,
        "Solution_reading_time":14.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1381858437316,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":2720.0,
        "Answerer_view_count":482.0,
        "Challenge_adjusted_solved_time":0.8205025,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to query a MS Access Web App (SQL Azure) using the Azure ML platform. The field I'm trying to capture is type <code>Fixed-point number (6 decimal places)<\/code>, the default numeric field type in Azure SQL. When I try to query this field, I get the error:<\/p>\n\n<p><code>Error 1000: AFx Library library exception: Type Decimal is not supported<\/code><\/p>\n\n<p>I have tried casting it to another form like follows:<\/p>\n\n<p><code>select cast(a) FROM b<\/code><\/p>\n\n<p>And got the error:<\/p>\n\n<p><code>Error 0069: SQL query \"select cast(\"a\" as float) from \"b\"\" is not correct:\nColumn names cannot be null or empty.<\/code><\/p>\n\n<p>What gives?<\/p>\n\n<p>Furthermore, how isn't the default on Azure SQL supported in Azure ML???<\/p>",
        "Challenge_closed_time":1471362745296,
        "Challenge_comment_count":3,
        "Challenge_created_time":1471359791487,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38978361",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.99,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.8205025,
        "Challenge_title":"Error 1000: AFx Library library exception: Type Decimal is not supported",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":2105,
        "Challenge_word_count":122,
        "Platform":"Stack Overflow",
        "Poster_created_time":1381858437316,
        "Poster_location":"London, UK",
        "Poster_reputation_count":2720.0,
        "Poster_view_count":482.0,
        "Solution_body":"<p>As per serhiyb's answer, the win was to assign it to another variable:<\/p>\n\n<p><code>Select cast(\"field\" as float) as 'someAlias' FROM \"Table\"<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"assign queri field variabl sql queri select cast field somealia tabl",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"serhiyb win assign variabl select cast field somealia tabl",
        "Solution_preprocessed_content":"serhiyb win assign variabl",
        "Solution_readability":10.3,
        "Solution_reading_time":1.98,
        "Solution_score_count":4.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":21.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":0.4832136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to classify ~1m+ documents and have a Version Control System for in- and Output of the corresponding model. <\/p>\n\n<p>The data changes over time:<\/p>\n\n<ul>\n<li>sample size increases over time<\/li>\n<li>new Features might appear<\/li>\n<li>anonymization procedure might Change over time<\/li>\n<\/ul>\n\n<p>So basically \"everything\" might change: amount of observations, Features and the values.\nWe are interested in making the ml model Building reproducible without using 10\/100+ GB \nof disk volume, because we save all updated versions of Input data. Currently the volume size of the data is ~700mb.<\/p>\n\n<p>The most promising tool i found is: <a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"noreferrer\">https:\/\/github.com\/iterative\/dvc<\/a>. Currently the data\nis stored in a database in loaded in R\/Python from there.<\/p>\n\n<p><strong>Question:<\/strong><\/p>\n\n<p>How much disk volume can be (very approx.) saved by using dvc? <\/p>\n\n<p>If one can roughly estimate that. I tried to find out if only the \"diffs\" of the data are saved. I didnt find much info by reading through: <a href=\"https:\/\/github.com\/iterative\/dvc#how-dvc-works\" rel=\"noreferrer\">https:\/\/github.com\/iterative\/dvc#how-dvc-works<\/a> or other documentation. <\/p>\n\n<p><strong>I am aware that this is a very vague question. And it will highly depend on the dataset. However, i would still be interested in getting a very approximate idea.<\/strong><\/p>",
        "Challenge_closed_time":1582487867856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582482701247,
        "Challenge_favorite_count":3,
        "Challenge_last_edit_time":1582486128287,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60365473",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":18.55,
        "Challenge_score_count":7,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":1.4351691667,
        "Challenge_title":"By how much can i approx. reduce disk volume by using dvc?",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":689,
        "Challenge_word_count":204,
        "Platform":"Stack Overflow",
        "Poster_created_time":1504097190907,
        "Poster_location":null,
        "Poster_reputation_count":1365.0,
        "Poster_view_count":193.0,
        "Solution_body":"<p>Let me try to summarize how does DVC store data and I hope you'll be able to figure our from this how much space will be saved\/consumed in your specific scenario.<\/p>\n\n<p><strong>DVC is storing and deduplicating data on the individual <em>file level<\/em>.<\/strong> So, what does it usually mean from a practical perspective.<\/p>\n\n<p>I will use <code>dvc add<\/code> as an example, but the same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add<\/code>, <code>dvc run<\/code>, etc.<\/p>\n\n<h2>Scenario 1: Modifying file<\/h2>\n\n<p>Let's imagine I have a single 1GB XML file. I start tracking it with DVC:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc add data.xml\n<\/code><\/pre>\n\n<p>On the modern file system (or if <code>hardlinks<\/code>, <code>symlinks<\/code> are enabled, see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"noreferrer\">this<\/a> for more details) after this command we still consume 1GB (even though file is moved into DVC cache and is still present in the workspace).<\/p>\n\n<p>Now, let's change it a bit and save it again:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ echo \"&lt;test\/&gt;\" &gt;&gt; data.xml\n$ dvc add data.xml\n<\/code><\/pre>\n\n<p>In this case we will have 2GB consumed. <strong>DVC does not do diff between two versions of the same file<\/strong>, neither it splits files into chunks or blocks to understand that only small portion of data has changed.<\/p>\n\n<blockquote>\n  <p>To be precise, it calculates <code>md5<\/code> of each file and save it in the content addressable key-value storage. <code>md5<\/code> of the files serves as a key (path of the file in cache) and value is the file itself:<\/p>\n  \n  <pre class=\"lang-sh prettyprint-override\"><code>(.env) [ivan@ivan ~\/Projects\/test]$ md5 data.xml\n0c12dce03223117e423606e92650192c\n\n(.env) [ivan@ivan ~\/Projects\/test]$ tree .dvc\/cache\n.dvc\/cache\n\u2514\u2500\u2500 0c\n   \u2514\u2500\u2500 12dce03223117e423606e92650192c\n\n1 directory, 1 file\n\n(.env) [ivan@ivan ~\/Projects\/test]$ ls -lh data.xml\ndata.xml ----&gt; .dvc\/cache\/0c\/12dce03223117e423606e92650192c (some type of link)\n<\/code><\/pre>\n<\/blockquote>\n\n<h2>Scenario 2: Modifying directory<\/h2>\n\n<p>Let's now imagine we have a single large 1GB directory <code>images<\/code> with a lot of files:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ du -hs images\n1GB\n\n$ ls -l images | wc -l\n1001\n\n$ dvc add images\n<\/code><\/pre>\n\n<p>At this point we still consume 1GB. Nothing has changed. But if we modify the directory by adding more files (or removing some of them):<\/p>\n\n<pre><code>$ cp \/tmp\/new-image.png images\n\n$ ls -l images | wc -l\n1002\n\n$ dvc add images\n<\/code><\/pre>\n\n<p>In this case, after saving the new version we <strong>still close to 1GB<\/strong> consumption. <strong>DVC calculates diff on the directory level.<\/strong> It won't be saving all the files that were existing before in the directory.<\/p>\n\n<p>The same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add<\/code>, <code>dvc run<\/code>, etc.<\/p>\n\n<p>Please, let me know if it's clear or we need to add more details, clarifications.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":8.5,
        "Solution_reading_time":39.4,
        "Solution_score_count":12.0,
        "Solution_sentence_count":26.0,
        "Solution_word_count":428.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1253167241888,
        "Answerer_location":"Sydney, Australia",
        "Answerer_reputation_count":213670.0,
        "Answerer_view_count":23384.0,
        "Challenge_adjusted_solved_time":9.8921491666,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm currently trying to do distributed GPU training with 2 instances of ml.p3.8xlarge and after 4 attempts I have not been able to start a training job with the spot instances since AWS did not have any available instances in my region.<\/p>\n<p>How do I increase the number of regions I'm willing to choose from in SageMaker? At the moment I'm only using:<\/p>\n<p><code>sess.boto_region_name = us-east-1<\/code> (sagemaker session region)<\/p>\n<p>But I'm assuming if I allow SageMaker to choose from other regions, I will be able to start a training job with spot instances.<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1628913109040,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628872633507,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1628877497303,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68775733",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":8.26,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":11.2432036111,
        "Challenge_title":"How to choose from a list of Regions for GPU instances in Amazon SageMaker?",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":120,
        "Challenge_word_count":106,
        "Platform":"Stack Overflow",
        "Poster_created_time":1468889301236,
        "Poster_location":null,
        "Poster_reputation_count":819.0,
        "Poster_view_count":87.0,
        "Solution_body":"<p>Most AWS services are regional-based, meaning they run in a given region and do not spread <em>beyond<\/em> one region.<\/p>\n<p>If you wish to run SageMaker in multiple regions, you would need to launch it <em>separately<\/em> in each region. So, you would only be 'choosing' <em>one<\/em> region when requesting SageMaker to perform some work.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"servic region base run region spread region wish run multipl region launch separ region choos region request perform",
        "Solution_preprocessed_content":"servic run region spread region wish run multipl region launch separ region choos region request perform",
        "Solution_readability":9.0,
        "Solution_reading_time":4.33,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":53.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1619177157943,
        "Answerer_location":null,
        "Answerer_reputation_count":835.0,
        "Answerer_view_count":516.0,
        "Challenge_adjusted_solved_time":396.3547583334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Why <code> kfp.v2.dsl.Output<\/code> as function argument works without being provided?<\/p>\n<p>I am following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/training-data-analyst\/blob\/master\/courses\/machine_learning\/deepdive2\/machine_learning_in_the_enterprise\/solutions\/create_run_vertex_pipeline.ipynb\" rel=\"nofollow noreferrer\">Create and run ML pipelines with Vertex Pipelines!<\/a> Jupyter notebook example from GCP.<\/p>\n<p>The function <code>classif_model_eval_metrics<\/code> takes <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> which have no default values.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],                # No default value set, hence must be mandatory\n    metricsc: Output[ClassificationMetrics], # No default value set, hence must be mandatory\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]): \n    # Full code at the bottom.\n<\/code><\/pre>\n<p>Hence those arguments should be mandatory, but the function is called without those arguments.<\/p>\n<pre><code>    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p>The entire pipeline code is below.<\/p>\n<pre><code>@kfp.dsl.pipeline(name=&quot;automl-tab-beans-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = &quot;bq:\/\/aju-dev-demos.beans.beans1&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auRoc&quot;: 0.95}',\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=display_name, bq_source=bq_source\n    )\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=COLUMNS,\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;Class&quot;,\n    )\n    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Why does it work and what are <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> of type <code>kfp.v2.dsl.Output<\/code>?<\/p>\n<hr \/>\n<h2>classif_model_eval_metrics function code<\/h2>\n<pre><code>from kfp.v2.dsl import (\n    Dataset, Model, Output, Input, \n    OutputPath, ClassificationMetrics, Metrics, component\n)\n\n@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]):  # Return parameter.\n    &quot;&quot;&quot;Renders evaluation metrics for an AutoML Tabular classification model.\n    Retrieves the classification model evaluation and render the ROC and confusion matrix\n    for the model. Determine whether the model is sufficiently accurate to deploy.\n    &quot;&quot;&quot;\n    import json\n    import logging\n    from google.cloud import aiplatform\n\n    # Fetch model eval info\n    def get_eval_info(client, model_name):\n        from google.protobuf.json_format import MessageToDict\n        response = client.list_model_evaluations(parent=model_name)\n        metrics_list = []\n        metrics_string_list = []\n        for evaluation in response:\n            metrics = MessageToDict(evaluation._pb.metrics)\n            metrics_str = json.dumps(metrics)\n            metrics_list.append(metrics)\n            metrics_string_list.append(metrics_str)\n        return (\n            evaluation.name,\n            metrics_list,\n            metrics_string_list,\n        )\n\n    def classification_thresholds_check(metrics_dict, thresholds_dict):\n        for k, v in thresholds_dict.items():\n            if k in [&quot;auRoc&quot;, &quot;auPrc&quot;]:  # higher is better\n                if metrics_dict[k] &lt; v:  # if under threshold, don't deploy\n                    return False\n        return True\n\n    def log_metrics(metrics_list, metricsc):\n        test_confusion_matrix = metrics_list[0][&quot;confusionMatrix&quot;]\n        logging.info(&quot;rows: %s&quot;, test_confusion_matrix[&quot;rows&quot;])\n        # log the ROC curve\n        fpr = [], tpr = [], thresholds = []\n        for item in metrics_list[0][&quot;confidenceMetrics&quot;]:\n            fpr.append(item.get(&quot;falsePositiveRate&quot;, 0.0))\n            tpr.append(item.get(&quot;recall&quot;, 0.0))\n            thresholds.append(item.get(&quot;confidenceThreshold&quot;, 0.0))\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        # log the confusion matrix\n        annotations = []\n        for item in test_confusion_matrix[&quot;annotationSpecs&quot;]:\n            annotations.append(item[&quot;displayName&quot;])\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n        # log textual metrics info as well\n        for metric in metrics_list[0].keys():\n            if metric != &quot;confidenceMetrics&quot;:\n                val_string = json.dumps(metrics_list[0][metric])\n                metrics.log_metric(metric, val_string)\n        # metrics.metadata[&quot;model_type&quot;] = &quot;AutoML Tabular classification&quot;\n\n    aiplatform.init(project=project)\n\n    client = aiplatform.gapic.ModelServiceClient(client_options={&quot;api_endpoint&quot;: api_endpoint})\n    eval_name, metrics_list, metrics_str_list = get_eval_info(\n        client, model.uri.replace(&quot;aiplatform:\/\/v1\/&quot;, &quot;&quot;)\n    )\n    log_metrics(metrics_list, metricsc)\n    thresholds_dict = json.loads(thresholds_dict_str)\n\n    return (&quot;true&quot;,) if classification_thresholds_check(metrics_list[0], thresholds_dict) else (&quot;false&quot;, )\n<\/code><\/pre>",
        "Challenge_closed_time":1652802212843,
        "Challenge_comment_count":5,
        "Challenge_created_time":1651375335713,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72073763",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":23.4,
        "Challenge_reading_time":91.59,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":50,
        "Challenge_solved_time":396.3547583334,
        "Challenge_title":"GCP Vertex Pipeline - Why kfp.v2.dsl.Output as function arguments work without being provided?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":358,
        "Challenge_word_count":467,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416648155470,
        "Poster_location":null,
        "Poster_reputation_count":14749.0,
        "Poster_view_count":968.0,
        "Solution_body":"<p>The custom component is defined as a Python function with a <code>@kfp.v2.dsl.component<\/code> decorator.<\/p>\n<p>The <code>@component<\/code> decorator specifies three optional arguments: the base container image to use; any packages to install; and the yaml file to which to write the component specification.<\/p>\n<p>The component function, <code>classif_model_eval_metrics<\/code>, has some input parameters.  The model parameter is an input <code>kfp.v2.dsl.Model artifact<\/code>.<\/p>\n<p>The two function args, <code>metrics<\/code> and <code>metricsc<\/code>, are component Outputs, in this case of types Metrics and ClassificationMetrics. They\u2019re not explicitly passed as inputs to the component step, but rather are automatically instantiated and can be used in the component.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;,\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,  # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n)\n<\/code><\/pre>\n<p>For example, in the function below, we\u2019re calling <code>metricsc.log_roc_curve()<\/code> and <code>metricsc.log_confusion_matrix()<\/code> to render these visualizations in the Pipelines UI. These Output params become component outputs when the component is compiled, and can be consumed by other pipeline steps.<\/p>\n<pre><code>def log_metrics(metrics_list, metricsc):\n        ...\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        ...\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n<\/code><\/pre>\n<p>For more information you can refer to this <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/use-vertex-pipelines-build-automl-classification-end-end-workflow\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"focus explain purpos usag metric metricsc argument classif model eval metric function vertex pipelin",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"compon defin function kfp dsl compon decor compon decor specifi option argument base imag packag instal yaml file write compon compon function classif model eval metric input paramet model paramet input kfp dsl model artifact function arg metric metricsc compon output type metric classificationmetr theyr explicitli pass input compon step automat instanti compon compon base imag gcr deeplearn platform releas cpu latest output compon file tabl eval compon yaml packag instal cloud aiplatform classif model eval metric str locat str central api endpoint str central aiplatform googleapi com threshold dict str str model input model metric output metric metricsc output classificationmetr function call metricsc log roc curv metricsc log matrix render visual pipelin output param compon output compon compil consum pipelin step log metric metric list metricsc metricsc log roc curv fpr tpr threshold metricsc log matrix annot test matrix row document",
        "Solution_preprocessed_content":"compon defin function decor decor specifi option argument base imag packag instal yaml file write compon compon function input paramet model paramet input function arg compon output type metric classificationmetr theyr explicitli pass input compon step automat instanti compon function call render visual pipelin output param compon output compon compil consum pipelin step document",
        "Solution_readability":19.5,
        "Solution_reading_time":27.41,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":180.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":160.3563522222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running custom training jobs in google's Vertex AI. A simple <code>gcloud<\/code> command to execute a custom job would use something like the following syntax (complete documentation for the command can be seen <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/beta\/ai\/custom-jobs\/create#--config\" rel=\"nofollow noreferrer\">here<\/a>):<\/p>\n<pre><code>gcloud beta ai custom-jobs create --region=us-central1 \\\n--display-name=test \\\n--config=config.yaml\n<\/code><\/pre>\n<p>In the <code>config.yaml<\/code> file, it is possible to specify the machine and accelerator (GPU) types, etc., and in my case, point to a custom container living in the Google Artifact Registry that executes the training code (specified in the <code>imageUri<\/code> part of the <code>containerSpec<\/code>). An example config file may look like this:<\/p>\n<pre><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n    acceleratorType: NVIDIA_TESLA_P100\n    acceleratorCount: 2\n  replicaCount: 1\n  containerSpec:\n    imageUri: {URI_FOR_CUSTOM_CONATINER}\n    args:\n    - {ARGS TO PASS TO CONTAINER ENTRYPOINT COMMAND}\n<\/code><\/pre>\n<p>The code we're running needs some runtime environment variables (that need to be secure) passed to the container. In the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">API documentation<\/a> for the <code>containerSpec<\/code>, it says it is possible to set environment variables as follows:<\/p>\n<pre><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n    acceleratorType: NVIDIA_TESLA_P100\n    acceleratorCount: 2\n  replicaCount: 1\n  containerSpec:\n    imageUri: {URI_FOR_CUSTOM_CONATINER}\n    args:\n    - {ARGS TO PASS TO CONTAINER ENTRYPOINT COMMAND}\n    env:\n    - name: SECRET_ONE\n      value: $SECRET_ONE\n    - name: SECRET_TWO\n      value: $SECRET_TWO\n<\/code><\/pre>\n<p>When I try and add the <code>env<\/code> flag to the <code>containerSpec<\/code>, I get an error saying it's not part of the container spec:<\/p>\n<pre><code>ERROR: (gcloud.beta.ai.custom-jobs.create) INVALID_ARGUMENT: Invalid JSON payload received. Unknown name &quot;env&quot; at 'custom_job.job_spec.worker_pool_specs[0].container_spec': Cannot find field.\n- '@type': type.googleapis.com\/google.rpc.BadRequest\n  fieldViolations:\n  - description: &quot;Invalid JSON payload received. Unknown name \\&quot;env\\&quot; at 'custom_job.job_spec.worker_pool_specs[0].container_spec':\\\n      \\ Cannot find field.&quot;\n    field: custom_job.job_spec.worker_pool_specs[0].container_spec\n<\/code><\/pre>\n<p>Any idea how to securely set runtime environment variables in Vertex AI custom jobs using custom containers?<\/p>",
        "Challenge_closed_time":1632987814700,
        "Challenge_comment_count":7,
        "Challenge_created_time":1632409414673,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1632410531832,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69302528",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":8,
        "Challenge_readability":14.7,
        "Challenge_reading_time":35.84,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":160.6666741667,
        "Challenge_title":"How to pass environment variables to gcloud beta ai custom-jobs create with custom container (Vertex AI)",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":1033,
        "Challenge_word_count":283,
        "Platform":"Stack Overflow",
        "Poster_created_time":1464106929608,
        "Poster_location":null,
        "Poster_reputation_count":457.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>There are two versions of the REST API - \u201c<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">v1<\/a>\u201d and \u201c<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1beta1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">v1beta1<\/a>\u201d where &quot;v1beta1&quot; does not have the <code>env<\/code> option in <code>ContainerSpec<\/code> but &quot;v1&quot; does. The <code>gcloud ai custom-jobs create<\/code> command without the <code>beta<\/code> parameter doesn\u2019t throw the error as it uses version \u201cv1\u201d to make the API calls.<\/p>\n<p>The environment variables from the yaml file can be passed to the custom container in the following way:<\/p>\n<p>This is the docker file of the sample custom training application I used to test the requirement. Please refer to this <a href=\"https:\/\/codelabs.developers.google.com\/vertex_custom_training_prediction\" rel=\"nofollow noreferrer\">codelab<\/a> for more information about the training application.<\/p>\n<pre class=\"lang-docker prettyprint-override\"><code>FROM gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3\nWORKDIR \/root\n\nWORKDIR \/\n\n# Copies the trainer code to the docker image.\nCOPY trainer \/trainer\n\n\n# Copies the bash script to the docker image.\nCOPY commands.sh \/scripts\/commands.sh\n\n# Bash command to make the script file an executable\nRUN [&quot;chmod&quot;, &quot;+x&quot;, &quot;\/scripts\/commands.sh&quot;]\n\n\n# Command to execute the file\nENTRYPOINT [&quot;\/scripts\/commands.sh&quot;]\n\n# Sets up the entry point to invoke the trainer.\n# ENTRYPOINT &quot;python&quot; &quot;-m&quot; $SECRET_TWO \u21d2 To use the environment variable  \n# directly in the docker ENTRYPOINT. In case you are not using a bash script, \n# the trainer can be invoked directly from the docker ENTRYPOINT.\n<\/code><\/pre>\n<br \/>\n<p>Below is the <code>commands.sh<\/code> file used in the docker container to test whether the environment variables are passed to the container.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>#!\/bin\/bash\nmkdir \/root\/.ssh\necho $SECRET_ONE\npython -m $SECRET_TWO\n<\/code><\/pre>\n<br \/>\n<p>The example <code>config.yaml<\/code> file<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n  replicaCount: 1\n  containerSpec:\n    imageUri: gcr.io\/infosys-kabilan\/mpg:v1\n    env:\n    - name: SECRET_ONE\n      value: &quot;Passing the environment variables&quot;\n    - name: SECRET_TWO\n      value: &quot;trainer.train&quot;\n<\/code><\/pre>\n<p>As the next step, I built and pushed the container to Google Container Repository. Now, the <code>gcloud ai custom-jobs create --region=us-central1  --display-name=test --config=config.yaml<\/code> can be run to create the custom training job and the output of the <code>commands.sh<\/code> file can be seen in the job logs as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qRHV7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qRHV7.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"version rest api vbeta vbeta env option containerspec gcloud job creat beta paramet doesnt throw version api call environ variabl yaml file pass env flag containerspec version rest api config yaml file file demonstr pass environ",
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_original_content":"version rest api vbeta vbeta env option containerspec gcloud job creat beta paramet doesnt throw version api call environ variabl yaml file pass docker file sampl train test codelab train gcr deeplearn platform releas cpu workdir root workdir copi trainer docker imag copi trainer trainer copi bash docker imag copi bash file execut run chmod execut file entrypoint set entri invok trainer entrypoint secret environ variabl directli docker entrypoint bash trainer invok directli docker entrypoint file docker test environ variabl pass bin bash mkdir root ssh echo secret secret config yaml file config yaml workerpoolspec machinespec machinetyp highmem replicacount containerspec imageuri gcr infosi kabilan mpg env secret valu pass environ variabl secret valu trainer train step built push repositori gcloud job creat region central displai test config config yaml run creat train job output file job log shown",
        "Solution_preprocessed_content":"version rest api beta beta option paramet doesnt throw version api call environ variabl yaml file pass docker file sampl train test codelab train file docker test environ variabl pass file step built push repositori run creat train job output file job log shown",
        "Solution_readability":12.1,
        "Solution_reading_time":39.15,
        "Solution_score_count":4.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":319.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1623779095963,
        "Answerer_location":"Germany, Hesse",
        "Answerer_reputation_count":1341.0,
        "Answerer_view_count":128.0,
        "Challenge_adjusted_solved_time":4.1247302778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to understand what an RFormula is in MLflow or spark.<\/p>\n<p>I have found these:<\/p>\n<p><a href=\"https:\/\/george-jen.gitbook.io\/data-science-and-apache-spark\/rformula\" rel=\"nofollow noreferrer\">https:\/\/george-jen.gitbook.io\/data-science-and-apache-spark\/rformula<\/a>\n<a href=\"https:\/\/spark.apache.org\/docs\/latest\/api\/python\/reference\/api\/pyspark.ml.feature.RFormula.html\" rel=\"nofollow noreferrer\">https:\/\/spark.apache.org\/docs\/latest\/api\/python\/reference\/api\/pyspark.ml.feature.RFormula.html<\/a><\/p>\n<p>but still cannot understand how to interpret an RFormula fully. I am not sure how to interpret the below table\n<a href=\"https:\/\/i.stack.imgur.com\/guLyU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/guLyU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>based on the formula &quot;y ~ x+ s&quot;, y is related to x and s, but in the table when y=0 and x=0 and s =a (i.e. third row), then the features is [0,1] and label is 0, so how shall I interpret this.<\/p>\n<p>I have found <a href=\"https:\/\/stackoverflow.com\/questions\/61290042\/spark-rformula-interpretation\">this<\/a> but still cannot understand my way through this problem.<\/p>",
        "Challenge_closed_time":1627334164712,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627319315683,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1627371880352,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68533916",
        "Challenge_link_count":7,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":14.0,
        "Challenge_reading_time":16.22,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":4.1247302778,
        "Challenge_title":"what is features and how to interpret in RFormula",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":78,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1375186444008,
        "Poster_location":"Auckland, New Zealand",
        "Poster_reputation_count":912.0,
        "Poster_view_count":288.0,
        "Solution_body":"<p>So your label is y. You parse x and s in rformula.<\/p>\n<p>x stays the same:<\/p>\n<pre><code>+-----------+---+\n|      x    | x |\n+-----------+---+\n|     1.0   |1.0|\n|     2.0   |2.0|\n|     0.0   |0.0|\n+-----------+---+\n<\/code><\/pre>\n<p>s:<\/p>\n<pre><code>+-----------+---+\n|       s   | s |\n+-----------+---+\n|       a   |1.0|\n|       b   |0.0|\n|       a   |1.0|\n+-----------+---+\n<\/code><\/pre>\n<p>I hope I could answer you question.\nRformula just converts the strings, standarize them and parse them into a vector.<\/p>",
        "Solution_comment_count":6.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":1627334932660,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":2.5,
        "Solution_reading_time":5.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":57.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":3.5946425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.<\/p>\n\n<p>I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.<\/p>",
        "Challenge_closed_time":1549313202943,
        "Challenge_comment_count":0,
        "Challenge_created_time":1549300262230,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54521080",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":5.9,
        "Challenge_reading_time":7.35,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":3.5946425,
        "Challenge_title":"aws sagemaker for detecting text in an image",
        "Challenge_topic":"Database Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2255,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1412300825550,
        "Poster_location":"West Lafayette, IN, United States",
        "Poster_reputation_count":463.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>The different services will all provide different levels of abstraction for Optical Character Recognition (OCR) depending on what parts of the pipeline you are most comfortable with working with, and what you prefer to have abstracted.<\/p>\n\n<p>Here are a few options:<\/p>\n\n<ul>\n<li><p><strong>Rekognition<\/strong> will provide out of the box OCR with the <a href=\"https:\/\/docs.aws.amazon.com\/rekognition\/latest\/dg\/text-detecting-text-procedure.html\" rel=\"nofollow noreferrer\">DetectText<\/a> feature. However, it seems you will need to perform some sort of pre-processing on your images in your current case in order to get better results. This can be done through any method of your choice (Lambda, EC2, etc).<\/p><\/li>\n<li><p><strong>SageMaker<\/strong> is a tool that will enable you to easily train and deploy your own models (of any type). You have two primary options with SageMaker:<\/p>\n\n<ol>\n<li><p><em>Do-it-yourself option<\/em>: If you're looking to go the route of labeling your own data, gathering a sizable training set, and training your own OCR model, this is possible by training and deploying your own model via SageMaker.<\/p><\/li>\n<li><p><em>Existing OCR algorithm<\/em>: There are many algorithms out there that all have different potential tradeoffs for OCR. One example would be <a href=\"https:\/\/github.com\/tesseract-ocr\/tesseract\" rel=\"nofollow noreferrer\">Tesseract<\/a>. Using this, you can more closely couple your pre-processing step to the text detection.<\/p><\/li>\n<\/ol><\/li>\n<li><p><a href=\"https:\/\/aws.amazon.com\/textract\/\" rel=\"nofollow noreferrer\"><strong>Amazon Textract<\/strong><\/a> (In preview) is a purpose-built dedicated OCR service that may offer better performance depending on what your images look like and the settings you choose. <\/p><\/li>\n<\/ul>\n\n<p>I would personally recommend looking into <a href=\"https:\/\/docparser.com\/blog\/improve-ocr-accuracy\/\" rel=\"nofollow noreferrer\">pre-processing for OCR<\/a> to see if it improves Rekognition accuracy before moving onto the other options. Even if it doesn't improve Rekognition's accuracy, it will still be valuable for most of the other options!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"pre process imag rekognit detecttext featur improv accuraci train deploi ocr model ocr algorithm tesseract textract dedic ocr servic perform pre process ocr improv rekognit accuraci move option",
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_original_content":"servic level abstract optic charact recognit ocr depend pipelin comfort prefer abstract option rekognit box ocr detecttext featur perform sort pre process imag order choic lambda enabl easili train deploi model type primari option option rout label data gather sizabl train set train ocr model train deploi model ocr algorithm algorithm potenti tradeoff ocr tesseract close coupl pre process step text detect textract preview purpos built dedic ocr servic perform depend imag set choos pre process ocr improv rekognit accuraci move option improv rekognit accuraci valuabl option",
        "Solution_preprocessed_content":"servic level abstract optic charact recognit depend pipelin comfort prefer abstract option rekognit box ocr detecttext featur perform sort imag order choic enabl easili train deploi model primari option option rout label data gather sizabl train set train ocr model train deploi model ocr algorithm algorithm potenti tradeoff ocr tesseract close coupl step text detect textract dedic ocr servic perform depend imag set choos ocr improv rekognit accuraci move option improv rekognit accuraci valuabl option",
        "Solution_readability":12.3,
        "Solution_reading_time":27.32,
        "Solution_score_count":4.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":274.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1412515367427,
        "Answerer_location":null,
        "Answerer_reputation_count":161.0,
        "Answerer_view_count":24.0,
        "Challenge_adjusted_solved_time":682.3236119445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have followed an Amazon tutorial for using SageMaker and have used it to create the model in the tutorial (<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a>).<\/p>\n\n<p>This is my first time using SageMaker, so my question may be stupid.<\/p>\n\n<p>How do you actually view the model that it has created? I want to be able to see a) the final formula created with the parameters etc. b) graphs of plotted factors etc. as if I was reviewing a GLM for example.<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1559693969323,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557237604320,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56024351",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":9.18,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":682.3236119445,
        "Challenge_title":"Beginners guide to Sagemaker",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":748,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554397763220,
        "Poster_location":null,
        "Poster_reputation_count":327.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>If you followed the SageMaker tutorial you must have trained an XGBoost model. SageMaker places the model artifacts in a bucket that you own, check the output S3 location in the AWS SageMaker console. <\/p>\n\n<p>For more information about XGBoost you can check the AWS SageMaker documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#xgboost-sample-notebooks\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#xgboost-sample-notebooks<\/a> and the example notebooks, e.g. <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone.ipynb<\/a><\/p>\n\n<p>To consume the XGBoost artifact generated by SageMaker, check out the official documentation, which contains the following code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># SageMaker XGBoost uses the Python pickle module to serialize\/deserialize \n# the model, which can be used for saving\/loading the model.\n# To use a model trained with SageMaker XGBoost in open source XGBoost\n# Use the following Python code:\n\nimport pickle as pkl \nmodel = pkl.load(open(model_file_path, 'rb'))\n# prediction with test data\npred = model.predict(dtest)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"output locat consol model artifact document notebook consum artifact gener pickl modul serial deseri model",
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_original_content":"tutori train model model artifact bucket output locat consol document http doc com latest html sampl notebook notebook http github com awslab blob master introduct algorithm abalon abalon ipynb consum artifact gener offici document pickl modul serial deseri model save load model model train open sourc import pickl pkl model pkl load open model file path predict test data pred model predict dtest",
        "Solution_preprocessed_content":"tutori train model model artifact bucket output locat consol document notebook consum artifact gener offici document",
        "Solution_readability":19.6,
        "Solution_reading_time":18.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":131.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":1.4294077778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to convert a csv file from s3 into a table in Athena. When I run the query on Athena console it works but when I run it on Sagemaker Jupyter notebook with boto3 client it returns: <\/p>\n\n<pre><code>\"**InvalidRequestException**: An error occurred (InvalidRequestException) when calling the StartQueryExecution operation: line 1:8: no viable alternative at input 'CREATE EXTERNAL'\"\n<\/code><\/pre>\n\n<p>Here is my code <\/p>\n\n<pre><code>def run_query(query):\n    client = boto3.client('athena')\n    response = client.start_query_execution(\n        QueryString=query,\n        ResultConfiguration={\n            'OutputLocation': 's3:\/\/path\/to\/s3output',\n            }\n        )\n    print('Execution ID: ' + response['QueryExecutionId'])\n    return response\n\ncreateTable = \\\n\"\"\"CREATE EXTERNAL TABLE TestTable (\n    ID string,\n    CustomerId string, \n    Ip string,\n    MessageFilename string\n\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES (\n   'separatorChar' = ',',\n   'quoteChar' = '\\\"',\n   'escapeChar' = '\\\\'\n )\nSTORED AS TEXTFILE\nLOCATION 's3:\/\/bucket_name\/results\/csv\/'\nTBLPROPERTIES (\"skip.header.line.count\"=\"1\")\"\"\"\n\nresponse = run_query(createTable, s3_output)\nprint(response)\n<\/code><\/pre>\n\n<p>I have run queries through boto3 client in json format (so, using  ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe') which have worked well but somehow this doesn't. I have tried changing names, syntax, quotes but that doesn't seem to work. <\/p>\n\n<p>Any suggestion would be very appreciated, \nThank you! <\/p>",
        "Challenge_closed_time":1532127104847,
        "Challenge_comment_count":0,
        "Challenge_created_time":1532122235887,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1532122372580,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51450610",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":19.99,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.3524888889,
        "Challenge_title":"Athena query works in console but not with boto3 client in sagemaker (convert csv into table)",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":1635,
        "Challenge_word_count":176,
        "Platform":"Stack Overflow",
        "Poster_created_time":1395796123356,
        "Poster_location":"Atlanta, GA, USA",
        "Poster_reputation_count":23.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Thanks for sharing complete example. The issue is with the escaping in <code>SERDEPROPERTIES<\/code>. After modifying <code>createTable<\/code> as below it works<\/p>\n\n<pre><code>createTable = \\\n\"\"\"CREATE EXTERNAL TABLE testtable (\n    `id` string,\n    `customerid` string, \n    `ip` string,\n    `messagefilename` string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES (\n  'separatorChar' = ',', \n  'quoteChar' = '\\\\\\\"', \n  'escapeChar' = '\\\\\\\\' )\nSTORED AS TEXTFILE\nLOCATION 's3:\/\/bucket_name\/results\/csv\/'\nTBLPROPERTIES (\"skip.header.line.count\"=\"1\");\"\"\"\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"escap serdeproperti modifi createt involv modifi createt escap charact serdeproperti",
        "Solution_last_edit_time":1532127518448,
        "Solution_link_count":0.0,
        "Solution_original_content":"share complet escap serdeproperti modifi createt createt creat extern tabl testtabl customerid messagefilenam row format serd org apach hadoop hive serd opencsvserd serdeproperti separatorchar quotechar escapechar store textfil locat bucket csv tblproperti skip header line count",
        "Solution_preprocessed_content":"share complet escap modifi",
        "Solution_readability":14.1,
        "Solution_reading_time":7.64,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1492048364132,
        "Answerer_location":"Cambridge, MA, USA",
        "Answerer_reputation_count":4436.0,
        "Answerer_view_count":713.0,
        "Challenge_adjusted_solved_time":2.9043861111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've been reading some articles regarding this topic and have preliminary thoughts as what I should do with it, but still want to see if anyone can share comments if you have more experience with running machine learning on AWS. I was doing a project for a professor at school, and we decided to use AWS. I need to find a cost-effective and efficient way to deploy a forecasting model on it. <\/p>\n\n<p>What we want to achieve is:<\/p>\n\n<ul>\n<li>read the data from S3 bucket monthly (there will be new data coming in every month), <\/li>\n<li>run a few python files (.py) for custom-built packages and install dependencies (including the files, no more than 30kb), <\/li>\n<li>produce predicted results into a file back in S3 (JSON or CSV works), or push to other endpoints (most likely to be some BI tools - tableau etc.) - but really this step can be flexible (not web for sure) <\/li>\n<\/ul>\n\n<p><strong>First thought I have is AWS sagemaker<\/strong>. However, we'll be using \"fb prophet\" model to predict the results, and we built a customized package to use in the model, therefore, I don't think the notebook instance is gonna help us. (Please correct me if I'm wrong) My understanding is that sagemaker is a environment to build and train the model, but we already built and trained the model. Plus, we won't be using AWS pre-built models anyways.<\/p>\n\n<p>Another thing is if we want to use custom-built package, we will need to create container image, and I've never done that before, not sure about the efforts to do that.<\/p>\n\n<p><strong>2nd option is to create multiple lambda functions<\/strong><\/p>\n\n<ul>\n<li><p>one that triggers to run the python scripts from S3 bucket (2-3 .py files) every time a new file is imported into S3 bucket, which will happen monthly.<\/p><\/li>\n<li><p>one that trigger after the python scripts are done running and produce results and save into S3 bucket.<\/p><\/li>\n<\/ul>\n\n<p>3rd option will combine both options:\n - Use lambda function to trigger the implementation on the python scripts in S3 bucket when the new file comes in.\n - Push the result using sagemaker endpoint, which means we host the model on sagemaker and deploy from there.<\/p>\n\n<p>I am still not entirely sure how to put pre-built model and python scripts onto sagemaker instance and host from there.<\/p>\n\n<p>I'm hoping whoever has more experience with AWS service can help give me some guidance, in terms of more cost-effective and efficient way to run model.<\/p>\n\n<p>Thank you!! <\/p>",
        "Challenge_closed_time":1586317398727,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586306942937,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61091659",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":9.7,
        "Challenge_reading_time":31.07,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":2.9043861111,
        "Challenge_title":"Should I run forecast predictive model with AWS lambda or sagemaker?",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2675,
        "Challenge_word_count":425,
        "Platform":"Stack Overflow",
        "Poster_created_time":1579188091243,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>I would say it all depends on how heavy your model is \/ how much data you're running through it. You're right to identify that Lambda will likely be less work. It's quite easy to get a lambda up and running to do the things that you need, and <a href=\"https:\/\/aws.amazon.com\/lambda\/pricing\/\" rel=\"nofollow noreferrer\">Lambda has a very generous free tier<\/a>. The problem is:<\/p>\n\n<ol>\n<li><p>Lambda functions are fundamentally limited in their processing capacity (they timeout after <em>max<\/em> 15 minutes).<\/p><\/li>\n<li><p>Your model might be expensive to load.<\/p><\/li>\n<\/ol>\n\n<p>If you have a lot of data to run through your model, you will need multiple lambdas. Multiple lambdas means you have to load your model multiple times, and that's wasted work. If you're working with \"big data\" this will get expensive once you get through the free tier.<\/p>\n\n<p>If you don't have much data, Lambda will work just fine. I would eyeball it as follows: assuming your data processing step is dominated by your model step, and if all your model interactions (loading the model + evaluating all your data) take less than 15min, you're definitely fine. If they take more, you'll need to do a back-of-the-envelope calculation to figure out whether you'd leave the Lambda free tier.<\/p>\n\n<p>Regarding Lambda: You can literally copy-paste code in to setup a prototype. If your execution takes more than 15min for all your data, you'll need a method of splitting your data up between multiple Lambdas. Consider <a href=\"https:\/\/aws.amazon.com\/step-functions\/\" rel=\"nofollow noreferrer\">Step Functions<\/a> for this.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_gpt_summary":"lambda cost deploi forecast model limit process capac multipl lambda data process model interact minut lambda minut split data multipl lambda step function creat imag",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"depend heavi model data run identifi lambda lambda run lambda gener free tier lambda function fundament limit process capac timeout minut model expens load data run model multipl lambda multipl lambda load model multipl time wast big data expens free tier data lambda eyebal data process step domin model step model interact load model evalu data definit envelop calcul figur leav lambda free tier lambda liter copi past setup prototyp execut data split data multipl lambda step function",
        "Solution_preprocessed_content":"depend heavi model data run identifi lambda lambda run lambda gener free tier lambda function fundament limit process capac model expens load data run model multipl lambda multipl lambda load model multipl time wast big data expens free tier data lambda eyebal data process step domin model step model interact definit calcul figur leav lambda free tier lambda liter setup prototyp execut data split data multipl lambda step function",
        "Solution_readability":9.3,
        "Solution_reading_time":19.98,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":246.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1552934828727,
        "Answerer_location":null,
        "Answerer_reputation_count":254.0,
        "Answerer_view_count":62.0,
        "Challenge_adjusted_solved_time":0.5847786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a object detection code in Aws. Although opencv is listed in the requirement file, i have the error &quot;no module named cv2&quot;. I am not sure how to fix this error. could someone help me please.<\/p>\n<p>My requirement.txt file has<\/p>\n<ul>\n<li>opencv-python<\/li>\n<li>numpy&gt;=1.18.2<\/li>\n<li>scipy&gt;=1.4.1<\/li>\n<li>wget&gt;=3.2<\/li>\n<li>tensorflow==2.3.1<\/li>\n<li>tensorflow-gpu==2.3.1<\/li>\n<li>tqdm==4.43.0<\/li>\n<li>pandas<\/li>\n<li>boto3<\/li>\n<li>awscli<\/li>\n<li>urllib3<\/li>\n<li>mss<\/li>\n<\/ul>\n<p>I tried installing &quot;imgaug&quot; and &quot;opencv-python headless&quot; as well.. but still not able to get rid of this error.<\/p>\n<pre><code>sh-4.2$ python train_launch.py \n[INFO-ROLE] arn:aws:iam::021945294007:role\/service-role\/AmazonSageMaker-ExecutionRole-20200225T145269\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_count has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\n2021-04-14 13:29:58 Starting - Starting the training job...\n2021-04-14 13:30:03 Starting - Launching requested ML instances......\n2021-04-14 13:31:11 Starting - Preparing the instances for training......\n2021-04-14 13:32:17 Downloading - Downloading input data...\n2021-04-14 13:32:41 Training - Downloading the training image..WARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n2021-04-14 13:33:03,970 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n2021-04-14 13:33:05,030 sagemaker-containers INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {\n        &quot;training&quot;: &quot;\/opt\/ml\/input\/data\/training&quot;\n    },\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;unfreezed_epochs&quot;: 2,\n        &quot;freezed_batch_size&quot;: 8,\n        &quot;freezed_epochs&quot;: 1,\n        &quot;unfreezed_batch_size&quot;: 8,\n        &quot;model_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {\n        &quot;training&quot;: {\n            &quot;TrainingInputMode&quot;: &quot;File&quot;,\n            &quot;S3DistributionType&quot;: &quot;FullyReplicated&quot;,\n            &quot;RecordWrapperType&quot;: &quot;None&quot;\n        }\n    },\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;yolov4-2021-04-14-15-29&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;train_indu&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 8,\n    &quot;num_gpus&quot;: 1,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;train_indu.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2}\nSM_USER_ENTRY_POINT=train_indu.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[&quot;training&quot;]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=train_indu\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=8\nSM_NUM_GPUS=1\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{&quot;training&quot;:&quot;\/opt\/ml\/input\/data\/training&quot;},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;yolov4-2021-04-14-15-29&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;train_indu&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:8,&quot;num_gpus&quot;:1,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;train_indu.py&quot;}\nSM_USER_ARGS=[&quot;--freezed_batch_size&quot;,&quot;8&quot;,&quot;--freezed_epochs&quot;,&quot;1&quot;,&quot;--model_dir&quot;,&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;--unfreezed_batch_size&quot;,&quot;8&quot;,&quot;--unfreezed_epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_CHANNEL_TRAINING=\/opt\/ml\/input\/data\/training\nSM_HP_UNFREEZED_EPOCHS=2\nSM_HP_FREEZED_BATCH_SIZE=8\nSM_HP_FREEZED_EPOCHS=1\nSM_HP_UNFREEZED_BATCH_SIZE=8\nSM_HP_MODEL_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\n\nInvoking script with the following command:\n\n\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2\n\n\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n[name: &quot;\/device:CPU:0&quot;\ndevice_type: &quot;CPU&quot;\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 4667030854237447206\n, name: &quot;\/device:XLA_CPU:0&quot;\ndevice_type: &quot;XLA_CPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 3059419181456814147\nphysical_device_desc: &quot;device: XLA_CPU device&quot;\n, name: &quot;\/device:XLA_GPU:0&quot;\ndevice_type: &quot;XLA_GPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 6024475084695919958\nphysical_device_desc: &quot;device: XLA_GPU device&quot;\n, name: &quot;\/device:GPU:0&quot;\ndevice_type: &quot;GPU&quot;\nmemory_limit: 14949928141\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 13034103301168381073\nphysical_device_desc: &quot;device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5&quot;\n]\nTraceback (most recent call last):\n  File &quot;train_indu.py&quot;, line 12, in &lt;module&gt;\n    from yolov3.dataset import Dataset\n  File &quot;\/opt\/ml\/code\/yolov3\/dataset.py&quot;, line 3, in &lt;module&gt;\n    import cv2\nModuleNotFoundError: No module named 'cv2'\n2021-04-14 13:33:08,453 sagemaker-containers ERROR    ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n\n2021-04-14 13:33:11 Uploading - Uploading generated training model\n2021-04-14 13:33:54 Failed - Training job failed\nTraceback (most recent call last):\n  File &quot;train_launch.py&quot;, line 41, in &lt;module&gt;\n    estimator.fit(s3_data_path, logs=True, job_name=job_name) #the argument logs is crucial if you want to see what happends\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 535, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 1210, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 3365, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 2957, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job yolov4-2021-04-14-15-29: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n<\/code><\/pre>",
        "Challenge_closed_time":1618410091900,
        "Challenge_comment_count":1,
        "Challenge_created_time":1618407986697,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67093041",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":30.4,
        "Challenge_reading_time":154.58,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":56,
        "Challenge_solved_time":0.5847786111,
        "Challenge_title":"Aws Sagemaker - ModuleNotFoundError: No module named 'cv2'",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1218,
        "Challenge_word_count":510,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558009697176,
        "Poster_location":"Cergy-Pontoise, Cergy, France",
        "Poster_reputation_count":53.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Make sure your estimator has<\/p>\n<ul>\n<li>framework_version = '2.3',<\/li>\n<li>py_version = 'py37',<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"modulenotfounderror run object detect estim paramet framework version version",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"estim framework version version",
        "Solution_preprocessed_content":null,
        "Solution_readability":4.3,
        "Solution_reading_time":1.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554425457572,
        "Answerer_location":null,
        "Answerer_reputation_count":63.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":294.0039433334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Before the AWS Sagemaker batch transform I need to do some transform. is it possible to have an custom script and associate as entry point to BatchTransformer?<\/p>",
        "Challenge_closed_time":1646173336163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645114921967,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71161777",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.3,
        "Challenge_reading_time":2.56,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":294.0039433334,
        "Challenge_title":"Sagemaker Batch Transform entry point",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":230,
        "Challenge_word_count":31,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554425457572,
        "Poster_location":null,
        "Poster_reputation_count":63.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The inference code and requirement.txt should be stored as part of model.gz while training.  They will be used in the batch transform!!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"store infer txt model train batch transform",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"infer txt store model train batch transform",
        "Solution_preprocessed_content":"infer store train batch transform",
        "Solution_readability":3.1,
        "Solution_reading_time":1.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1480786532470,
        "Answerer_location":"Nagpur, Sitabuldi, Nagpur, Maharashtra, India",
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":44.0,
        "Challenge_adjusted_solved_time":2013.1323233333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is the first time I am using amazon web services to deploy my machine learning pre-trained model. I want to deploy my pre-trained TensorFlow model to Aws-Sagemaker. I am somehow able to deploy the endpoints successfully But whenever I call the <code>predictor.predict(some_data)<\/code> method to make prediction to invoking the endpoints it's throwing an error.<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\". See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-tensorflow-2020-04-07-04-25-27-055 in account 453101909370 for more information.\n<\/code><\/pre>\n\n<p>After going through the cloud watch logs I found this error.<\/p>\n\n<pre><code>#011details = \"NodeDef mentions attr 'explicit_paddings' not in Op&lt;name=Conv2D; signature=input:T, filter:T -&gt; output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=dilations:list(int),default=[1, 1, 1, 1]&gt;; NodeDef: {{node conv1_conv\/convolution}} = Conv2D[T=DT_FLOAT, _output_shapes=[[?,112,112,64]], data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](conv1_pad\/Pad, conv1_conv\/kernel\/read). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n<\/code><\/pre>\n\n<p>I don't know where I am wrong and I have wasted 2 days already to solve this error and couldn't find out the information regarding this. The detailed logs I have shared <a href=\"https:\/\/docs.google.com\/document\/d\/1NXsLRd6cfbNE55xSVq5d63ETt-cBmwZsOaic8IyF1Qw\/edit?usp=sharing\" rel=\"nofollow noreferrer\">here<\/a>. <\/p>\n\n<p>Tensorflow version of my notebook instance is 1.15<\/p>",
        "Challenge_closed_time":1593503184160,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586244384423,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1586255907796,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61074798",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":15.9,
        "Challenge_reading_time":29.31,
        "Challenge_score_count":2,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2016.3332602778,
        "Challenge_title":"Deploy pre-trained tensorflow model on the aws sagemaker - ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":740,
        "Challenge_word_count":214,
        "Platform":"Stack Overflow",
        "Poster_created_time":1480786532470,
        "Poster_location":"Nagpur, Sitabuldi, Nagpur, Maharashtra, India",
        "Poster_reputation_count":111.0,
        "Poster_view_count":44.0,
        "Solution_body":"<p>After a lot of searching and try &amp; error, I was able to solve this problem. In many cases, the problem arises because of the TensorFlow and Python versions.<\/p>\n<p><strong>Cause of the problem:<\/strong>\nTo deploy the endpoints, I was using the <code>TensorflowModel<\/code> on TF 1.12 and python 3 and which exactly caused the problem.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = TensorFlowModel(model_data = model_data,\n                                  role = role,\n                                  framework_version = '1.12',\n                                  entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Apparently, <code>TensorFlowModel<\/code> only allows python 2 on TF version 1.11, 1.12. 2.1.0.<\/p>\n<p><strong>How I fixed it:<\/strong> There are two TensorFlow solutions that handle serving in the Python SDK. They have different class representations and documentation as shown here.<\/p>\n<ol>\n<li><strong>TensorFlowModel<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts<\/a><\/li>\n<li>Key difference: Uses a proxy GRPC client to send requests<\/li>\n<li>Container impl:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py<\/a><\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>Model<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst<\/a><\/li>\n<li>Key difference: Utilizes the TensorFlow serving rest API<\/li>\n<li>Container impl: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py<\/a><\/li>\n<\/ul>\n<p>Python 3 isn't supported using the <code>TensorFlowModel<\/code> object, as the container uses the TensorFlow serving API library in conjunction with the GRPC client to handle making inferences, however, the TensorFlow serving API isn't supported in Python 3 officially, so there are only Python 2 versions of the containers when using the <code>TensorFlowModel<\/code> object.\nIf you need Python 3 then you will need to use the <code>Model<\/code> object defined in #2 above.<\/p>\n<p>Finally, I used the <code>Model<\/code> with the TensorFlow version 1.15.1.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = Model(model_data = model_data,\n                        role = role,\n                        framework_version='1.15.2',\n                        entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Also, here are the successful results.\n<a href=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"tensorflow version tensorflowmodel appar tensorflowmodel allow version tensorflow serv sdk class represent document model object defin final model tensorflow version",
        "Solution_last_edit_time":null,
        "Solution_link_count":14.0,
        "Solution_original_content":"search tensorflow version deploi endpoint tensorflowmodel exactli model tensorflowmodel model data model data role role framework version entri train appar tensorflowmodel allow version tensorflow serv sdk class represent document shown tensorflowmodel http github com sdk blob master src tensorflow model doc http github com sdk tree src tensorflow deploi directli model artifact kei proxi grpc client send request impl http github com tensorflow blob master src serv model http github com sdk blob master src tensorflow serv doc http github com sdk blob master src tensorflow deploi tensorflow serv rst kei util tensorflow serv rest api impl http github com tensorflow serv blob master serv isn tensorflowmodel object tensorflow serv api librari conjunct grpc client infer tensorflow serv api isn offici version tensorflowmodel object model object defin final model tensorflow version model model model data model data role role framework version entri train",
        "Solution_preprocessed_content":"search tensorflow version deploi endpoint exactli appar allow version tensorflow serv sdk class represent document shown tensorflowmodel doc kei proxi grpc client send request impl model doc kei util tensorflow serv rest api impl isn object tensorflow serv api librari conjunct grpc client infer tensorflow serv api isn offici version object object defin final tensorflow version",
        "Solution_readability":21.1,
        "Solution_reading_time":48.42,
        "Solution_score_count":4.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":271.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1543010383463,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":404.8255119445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If I send a TensorFlow training job to a SageMaker instance, what is the typical way to view training progress? Can I access TensorBoard for this launched EC2 instance? Is there some other alternative? What I'm looking for specifically are things like graphs of current training epoch and mAP.<\/p>",
        "Challenge_closed_time":1543010856183,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541553484340,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53182436",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":4.29,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":404.8255119445,
        "Challenge_title":"SageMaker: visualizing training statistics",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":415,
        "Challenge_word_count":52,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>you can now specify metrics(metricName, Regex) that you want to track by using AWS management console or Amazon SageMaker Python SDK APIs. After the model training starts, Amazon SageMaker will automatically monitor and stream the specified metrics in real time to the Amazon CloudWatch console for visualizing time-series curves. <\/p>\n\n<p>Ref: \n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_MetricDefinition.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_MetricDefinition.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"specifi metric consol sdk api model train start automat monitor stream specifi metric time cloudwatch consol visual time seri curv",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"specifi metric metricnam regex track consol sdk api model train start automat monitor stream specifi metric time cloudwatch consol visual time seri curv ref http doc com latest api metricdefinit html",
        "Solution_preprocessed_content":"specifi metric track consol sdk api model train start automat monitor stream specifi metric time cloudwatch consol visual curv ref",
        "Solution_readability":18.1,
        "Solution_reading_time":7.11,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1418276189383,
        "Answerer_location":null,
        "Answerer_reputation_count":272.0,
        "Answerer_view_count":142.0,
        "Challenge_adjusted_solved_time":1.9387202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have started using SageMaker Studio Lab.<\/p>\n<p>When I run &quot;apt install xvfb&quot; in SageMaker Studio Lab Notebook, I get the following error.<\/p>\n<pre><code>!apt install xvfb\n\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\n<\/code><\/pre>\n<p>Then I tried with sudo, but the sudo command was not installed.<\/p>\n<pre><code>!sudo apt install xvfb\n\n\/usr\/bin\/sh: 1: sudo: not found\n<\/code><\/pre>\n<p>Can you please tell me how to solve this problem?<\/p>",
        "Challenge_closed_time":1638468298390,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638461318997,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70202848",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":8.08,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.9387202778,
        "Challenge_title":"Is it possible to \"apt install\" in SageMaker Studio Lab?",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1174,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1481861289276,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>To date Studio Lab doesn't support package installs that require root access. It does support packages installable via pip and conda. You can do that either in your notebook with the %, rather than the !, or you can do that via opening a terminal.<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lab-use-manage.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lab-use-manage.html<\/a><\/li>\n<\/ul>\n<p>If you'd like to open an issue you're welcome to do that on our repository right here:<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/.github\/ISSUE_TEMPLATE\/bug-report-for-sagemaker-studio-lab.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/.github\/ISSUE_TEMPLATE\/bug-report-for-sagemaker-studio-lab.md<\/a><\/li>\n<\/ul>\n<p>Thanks for trying out Studio Lab!<\/p>",
        "Solution_comment_count":3.0,
        "Solution_gpt_summary":"studio lab notebook packag instal root access packag instal pip conda notebook open termin open repositori",
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_original_content":"date studio lab packag instal root access packag instal pip conda notebook open termin http doc com latest studio lab html open welcom repositori http github com studio lab blob github templat report studio lab studio lab",
        "Solution_preprocessed_content":"date studio lab packag instal root access packag instal pip conda notebook open termin open welcom repositori studio lab",
        "Solution_readability":18.5,
        "Solution_reading_time":11.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":78.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":109.4256513889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was following the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-python-sdk\" rel=\"nofollow noreferrer\">SDK v2 Python tutorial<\/a> in order to create a pipeline job with my own assets. I notice that in this tutorial they let you use a csv file that can be downloaded but Im trying to use a registered dataset that I already registered by my own. The problem that I facing is that I dont know where I need to specify the dataset.<\/p>\n<p>The funny part is that at the beginning they create this dataset like this:<\/p>\n<pre><code>credit_data = ml_client.data.create_or_update(credit_data)\nprint(\n    f&quot;Dataset with name {credit_data.name} was registered to workspace, the dataset version is {credit_data.version}&quot;\n)\n<\/code><\/pre>\n<p>But the only part where they refer to this dataset is on the last part where they # the line:<\/p>\n<pre><code>registered_model_name = &quot;credit_defaults_model&quot;\n\n# Let's instantiate the pipeline with the parameters of our choice\npipeline = credit_defaults_pipeline(\n    # pipeline_job_data_input=credit_data,\n    pipeline_job_data_input=Input(type=&quot;uri_file&quot;, path=web_path),\n    pipeline_job_test_train_ratio=0.2,\n    pipeline_job_learning_rate=0.25,\n    pipeline_job_registered_model_name=registered_model_name,\n)\n<\/code><\/pre>\n<p>For me this means that I can use this data like this (a already registered dataset), the problem is that I don't know where I need to do the changes (I know that in the data_prep.py and in the code below but I don\u00b4t know where else) and I don't know how to set this:<\/p>\n<pre><code>%%writefile {data_prep_src_dir}\/data_prep.py\n...\n\ndef main():\n    &quot;&quot;&quot;Main function of the script.&quot;&quot;&quot;\n\n    # input and output arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(&quot;--data&quot;, type=str, help=&quot;path to input data&quot;) # &lt;=== Here, but I don\u00b4t know how\n    parser.add_argument(&quot;--test_train_ratio&quot;, type=float, required=False, default=0.25)\n    parser.add_argument(&quot;--train_data&quot;, type=str, help=&quot;path to train data&quot;)\n    parser.add_argument(&quot;--test_data&quot;, type=str, help=&quot;path to test data&quot;)\n    args = parser.parse_args()\n\n...\n<\/code><\/pre>\n<p>Does anyone have experience working as registered datasets?<\/p>",
        "Challenge_closed_time":1657082550892,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656688618547,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1657482591390,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72831360",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":30.28,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":109.4256513889,
        "Challenge_title":"Use dataset registed in on pipelines in AML",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":85,
        "Challenge_word_count":259,
        "Platform":"Stack Overflow",
        "Poster_created_time":1636569000947,
        "Poster_location":null,
        "Poster_reputation_count":9.0,
        "Poster_view_count":2.0,
        "Solution_body":"<blockquote>\n<p>parser.add_argument(&quot;--data&quot;, type=str, help=&quot;path to input data&quot;) # &lt;=== Here, but I don\u00b4t know how<\/p>\n<\/blockquote>\n<p>To get the path to input data, according to <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/how-to-train-with-datasets.md\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n<ul>\n<li><p>You can get <code>--input-data<\/code> by ID which you can access in your training script.<\/p>\n<\/li>\n<li><p>Use it as <code>argument<\/code> on <code>mounted_input_path<\/code><\/p>\n<\/li>\n<\/ul>\n<p>For example, try the following three code snippets taken from the <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/how-to-train-with-datasets.md\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n<p><strong>Access dataset in training script:<\/strong><\/p>\n<pre><code>parser = argparse.ArgumentParser()\nparser.add_argument(&quot;--input-data&quot;, type=str)\nargs = parser.parse_args()\n\nrun = Run.get_context()\nws = run.experiment.workspace\n\n# get the input dataset by ID\ndataset = Dataset.get_by_id(ws, id=args.input_data)\n<\/code><\/pre>\n<p><strong>Configure the training run:<\/strong><\/p>\n<pre><code>src = ScriptRunConfig(source_directory=script_folder,\n                      script='train_titanic.py',\n                      # pass dataset as an input with friendly name 'titanic'\n                      arguments=['--input-data', titanic_ds.as_named_input('titanic')],\n                      compute_target=compute_target,\n                      environment=myenv)\n<\/code><\/pre>\n<p><strong>Pass <code>mounted_input_path<\/code> as argument:<\/strong><\/p>\n<pre><code>mounted_input_path = sys.argv[1]\nmounted_output_path = sys.argv[2]\n\nprint(&quot;Argument 1: %s&quot; % mounted_input_path)\nprint(&quot;Argument 2: %s&quot; % mounted_output_path)\n<\/code><\/pre>\n<p>References: <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/v1\/how-to-create-register-datasets.md\" rel=\"nofollow noreferrer\">How to create register dataset<\/a> and <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/datasets-tutorial\/scriptrun-with-data-input-output\/how-to-use-scriptrun.ipynb\" rel=\"nofollow noreferrer\">How to use configure a training run with data input and output<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"regist dataset pipelin job sdk tutori access dataset train dataset configur train run pass dataset input friendli pass mount input path argument document creat regist dataset configur train run data input output",
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_original_content":"parser add argument data type str path input data path input data accord document input data access train argument mount input path taken document access dataset train parser argpars argumentpars parser add argument input data type str arg parser pars arg run run context run workspac input dataset dataset dataset arg input data configur train run src scriptrunconfig sourc directori folder train titan pass dataset input friendli titan argument input data titan input titan comput target comput target environ myenv pass mount input path argument mount input path sy argv mount output path sy argv print argument mount input path print argument mount output path creat regist dataset configur train run data input output",
        "Solution_preprocessed_content":"type str path input data path input data accord document access train taken document access dataset train configur train run pass argument creat regist dataset configur train run data input output",
        "Solution_readability":21.5,
        "Solution_reading_time":30.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":155.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1394547235287,
        "Answerer_location":"Warsaw, Poland",
        "Answerer_reputation_count":352.0,
        "Answerer_view_count":22.0,
        "Challenge_adjusted_solved_time":2.3974027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>dvc documentation for <code>dvc gc<\/code> command states, the <code>-r<\/code> option indicates \"Remote storage to collect garbage in\" but I'm not sure if I understand it correctly. For example I execute this command:<\/p>\n\n<pre><code>dvc gc -r myremote\n<\/code><\/pre>\n\n<p>What exactly happens if I execute this command? I have 2 possible answers:<\/p>\n\n<ol>\n<li>dvc checks which files should be deleted, then moves these files to \"myremote\" and then deletes all these files in local cache but not in remote.<\/li>\n<li>dvc checks which files should be deleted and deletes these files both in local cache and \"myremote\"<\/li>\n<\/ol>\n\n<p>Which one of them is correct?<\/p>",
        "Challenge_closed_time":1569833846627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569828438393,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58163305",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":8.64,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.5022872222,
        "Challenge_title":"dvc gc and files in remote cache",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1617,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522254698710,
        "Poster_location":"Russia",
        "Poster_reputation_count":784.0,
        "Poster_view_count":77.0,
        "Solution_body":"<p>one of DVC maintainers here.<\/p>\n\n<p>Short answer: 2. is correct.<\/p>\n\n<p>A bit of additional information:\nPlease be careful when using <code>dvc gc<\/code>. It will clear your cache from all dependencies that are not mentioned in the current HEAD of your git repository. \nWe are working on making <code>dvc gc<\/code> preserving whole history by default. <\/p>\n\n<p>So if you don't want to delete files from your history commits, it would be better to wait for completion of <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2325\" rel=\"nofollow noreferrer\">this<\/a> task.<\/p>\n\n<p>[EDIT]\nPlease see comment below.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":1569837069043,
        "Solution_link_count":1.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":7.5,
        "Solution_reading_time":7.71,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":86.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1370505440848,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":3.8717241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've created a docker image using AWS SageMaker and am now trying to push said image to ECR. When I do <code>docker push ${fullname}<\/code> it retries a couple of times and then errors.<\/p>\n<p>In CloudTrail I can see that I'm getting an access denied error with message:<\/p>\n<p>&quot;User: arn:aws:sts::xxxxxxxxxx:assumed-role\/AmazonSageMaker-ExecutionRole-xxxxxxxxxxxx\/SageMaker is not authorized to perform: ecr:InitiateLayerUpload on resource: arn:aws:ecr:us-east-x:xxxxxxxxxx:repository\/image because no identity-based policy allows the ecr:InitiateLayerUpload action&quot;<\/p>\n<p>I have full permissions, but from the error message above it thinks the user is SageMaker and not me.<\/p>\n<p>How do I change the user? I'm guessing that's the problem.<\/p>",
        "Challenge_closed_time":1658175998627,
        "Challenge_comment_count":2,
        "Challenge_created_time":1658162060420,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73025706",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":13.7,
        "Challenge_reading_time":10.61,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":3.8717241667,
        "Challenge_title":"AccessDenied error when pushing docker image from SageMaker to ECR",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":143,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1657050754840,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>When you're running commands from SageMaker, you're executing them as the SageMaker execution role, instead of your role. There are two options -<\/p>\n<ol>\n<li>[Straighforward solution] Add <em>ecr:InitiateLayerUpload<\/em> permissions to the <code>AmazonSageMaker-ExecutionRole-xxxxxxxxxxxx<\/code> role<\/li>\n<li>Assume a different role using sts (in that case, <code>AmazonSageMaker-ExecutionRole-xxxxxxxxxxxx<\/code> needs to have permissions to assume your Admin role) and then run <code>docker push<\/code> command.<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"access push docker imag ecr add ecr initiatelayerupload permiss executionrol role role st run docker push note executionrol role permiss admin role",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"run execut execut role role option straighforward add ecr initiatelayerupload permiss executionrol role role st executionrol permiss admin role run docker push",
        "Solution_preprocessed_content":"run execut execut role role option straighforward add ecr initiatelayerupload permiss role role st run",
        "Solution_readability":19.5,
        "Solution_reading_time":6.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":59.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1363541433296,
        "Answerer_location":"Twin Cities, MN, USA",
        "Answerer_reputation_count":348.0,
        "Answerer_view_count":24.0,
        "Challenge_adjusted_solved_time":7.8782044444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying in Amazon Sagemaker to deploy an existing Scikit-Learn model. So a model that wasn't trained on SageMaker, but locally on my machine.<\/p>\n<p>On my local (windows) machine I've saved my model as model.joblib and tarred the model to model.tar.gz.<\/p>\n<p>Next, I've uploaded this model to my S3 bucket ('my_bucket') in the following path s3:\/\/my_bucket\/models\/model.tar.gz. I can see the tar file in S3.<\/p>\n<p>But when I'm trying to deploy the model, it keeps giving the error message &quot;Failed to extract model data archive&quot;.<\/p>\n<p>The .tar.gz is generated on my local machine by running 'tar -czf model.tar.gz model.joblib' in a powershell command window.<\/p>\n<p>The code for uploading to S3<\/p>\n<pre><code>import boto3\ns3 = boto3.client(&quot;s3&quot;, \n              region_name='eu-central-1', \n              aws_access_key_id=AWS_KEY_ID, \n              aws_secret_access_key=AWS_SECRET)\ns3.upload_file(Filename='model.tar.gz', Bucket=my_bucket, Key='models\/model.tar.gz')\n<\/code><\/pre>\n<p>The code for creating the estimator and deploying:<\/p>\n<pre><code>import boto3\nfrom sagemaker.sklearn.estimator import SKLearnModel\n\n...\n\nmodel_data = 's3:\/\/my_bucket\/models\/model.tar.gz'\nsklearn_model = SKLearnModel(model_data=model_data,\n                             role=role,\n                             entry_point=&quot;my-script.py&quot;,\n                             framework_version=&quot;0.23-1&quot;)\npredictor = sklearn_model.deploy(instance_type=&quot;ml.t2.medium&quot;, initial_instance_count=1)                             \n<\/code><\/pre>\n<p>The error message:<\/p>\n<blockquote>\n<p>error message: UnexpectedStatusException: Error hosting endpoint\nsagemaker-scikit-learn-2021-01-24-17-24-42-204: Failed. Reason: Failed\nto extract model data archive for container &quot;container_1&quot; from URL\n&quot;s3:\/\/my_bucket\/models\/model.tar.gz&quot;. Please ensure that the object\nlocated at the URL is a valid tar.gz archive<\/p>\n<\/blockquote>\n<p>Is there a way to see why the archive is invalid?<\/p>",
        "Challenge_closed_time":1611852069143,
        "Challenge_comment_count":5,
        "Challenge_created_time":1611565411923,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":1611823707607,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65881699",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":6,
        "Challenge_readability":11.7,
        "Challenge_reading_time":25.46,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":79.6270055556,
        "Challenge_title":"SageMaker failed to extract model data archive tar.gz for container when deploying",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1859,
        "Challenge_word_count":208,
        "Platform":"Stack Overflow",
        "Poster_created_time":1472970520888,
        "Poster_location":"Amersfoort, Nederland",
        "Poster_reputation_count":424.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>I had a similar issue as well, along with a similar fix to Bas (per comment above).<\/p>\n<p>I was finding I wasn't necessarily having issues with the .tar.gz step, this command does work fine:<\/p>\n<p><code>tar -czf &lt;filename&gt; .\/&lt;directory-with-files&gt;<\/code><\/p>\n<p>but rather with the uploading step.<\/p>\n<p>Manually uploading to S3 should take care of this, however, if you're doing this step programmatically, you might need to double check the steps taken. Bas appears to have had filename issues, mine were around using boto properly. Here's some code that works (Python only here, but watch for similar issues with other libraries):<\/p>\n<pre><code>bucket = 'bucket-name'\nkey = 'directory-inside-bucket'\nfile = 'the file name of the .tar.gz'\n\ns3_client = boto3.client('s3')\ns3_client.upload_file(file, bucket, key)\n<\/code><\/pre>\n<p>Docs: <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"manual upload model tar file bucket upload step programmat doubl step taken upload file bucket boto librari",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"ba comment wasn necessarili tar step tar czf upload step manual upload care step programmat doubl step taken ba filenam boto properli watch librari bucket bucket kei directori insid bucket file file tar client boto client client upload file file bucket kei doc http boto amazonaw com document api latest servic html client upload file",
        "Solution_preprocessed_content":"ba wasn necessarili step upload step manual upload care step programmat doubl step taken ba filenam boto properli doc",
        "Solution_readability":13.9,
        "Solution_reading_time":14.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":120.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1313736279736,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":207794.0,
        "Answerer_view_count":16864.0,
        "Challenge_adjusted_solved_time":26.7536241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a data set with 16 columns and 100,000 rows which I'm trying to prepare for a matrix-factorization training. I'm using the following code to split it and turn it into a sparse matrix.<\/p>\n\n<pre><code>X=data.drop([data.columns[0]],axis='columns')\ny=data[[1]]\nX=lil_matrix(100000,15).astype('float32')\ny=np.array(y).astype('float32')\nX\n<\/code><\/pre>\n\n<p>But when I run it, I get this error:<\/p>\n\n<blockquote>\n  <p>&lt;1x1 sparse matrix of type ''  with 1 stored\n  elements in LInked List format> .<\/p>\n<\/blockquote>\n\n<p>When I try to plug it into a training\/testing split it gives me further errors:<\/p>\n\n<blockquote>\n  <p>Found input variables with inconsistent numbers of samples: [1,\n  100000]<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1562801273076,
        "Challenge_comment_count":6,
        "Challenge_created_time":1562690689400,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1562705343183,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56957206",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":7,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.72,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":30.7176877778,
        "Challenge_title":"how to turn a matrix into a sparse matrix and protobuf it",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":498,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545524391676,
        "Poster_location":"Kansas City, MO, USA",
        "Poster_reputation_count":91.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>Your linked <code>notebook<\/code> is creating a 'blank' sparse matrix, and setting selected elements from data it reads from a <code>csv<\/code>.<\/p>\n\n<p>A simple example of this:<\/p>\n\n<pre><code>In [565]: from scipy import sparse                                                                           \nIn [566]: M = sparse.lil_matrix((10,5), dtype=float)                                                         \nIn [567]: M                                                                                                  \nOut[567]: \n&lt;10x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 0 stored elements in LInked List format&gt;\n<\/code><\/pre>\n\n<p>Note that I use <code>(10,5)<\/code> to specify the matrix shape.  The () matter!  That's why I stressed reading the <code>docs<\/code>.  In the link the relevant line is:<\/p>\n\n<pre><code>X = lil_matrix((lines, columns)).astype('float32')\n<\/code><\/pre>\n\n<p>Now I can set a couple elements, just as I would an dense array:<\/p>\n\n<pre><code>In [568]: M[1,2] = 12.3                                                                                      \nIn [569]: M[3,1] = 1.1                                                                                       \nIn [570]: M                                                                                                  \nOut[570]: \n&lt;10x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 2 stored elements in LInked List format&gt;\n<\/code><\/pre>\n\n<p>I can use <code>toarray<\/code> to display the matrix as a dense array (don't try this with large dimensions).<\/p>\n\n<pre><code>In [571]: M.toarray()                                                                                        \nOut[571]: \narray([[ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. , 12.3,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  1.1,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ]])\n<\/code><\/pre>\n\n<hr>\n\n<p>If I omit the (), it makes a (1,1) matrix with just one element, the first number.<\/p>\n\n<pre><code>In [572]: sparse.lil_matrix(10,5)                                                                            \nOut[572]: \n&lt;1x1 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 1 stored elements in LInked List format&gt;\nIn [573]: _.A                                                                                                \nOut[573]: array([[10]], dtype=int64)\n<\/code><\/pre>\n\n<p>Look again at your code.  You set the <code>X<\/code> value twice, once it is a dataframe.  The second time is this bad <code>lil<\/code> initialization.  The second time does not make use of the first <code>X<\/code>.<\/p>\n\n<pre><code>X=data.drop([data.columns[0]],axis='columns')\n...\nX=lil_matrix(100000,15).astype('float32')\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_gpt_summary":"creat spars matrix scipi spars modul specifi shape matrix row column lil matrix function set element spars matrix syntax dens arrai shape initi spars matrix avoid overwrit variabl",
        "Solution_last_edit_time":1562801656230,
        "Solution_link_count":0.0,
        "Solution_original_content":"link notebook creat blank spars matrix set select element data read csv scipi import spars spars lil matrix dtype store element link list format note specifi matrix shape matter stress read doc link relev line lil matrix line column astyp set coupl element dens arrai store element link list format toarrai displai matrix dens arrai larg dimens toarrai arrai omit matrix element spars lil matrix store element link list format arrai dtype set valu twice datafram time lil initi time data drop data column axi column lil matrix astyp",
        "Solution_preprocessed_content":"link creat blank spars matrix set select element data read note specifi matrix shape matter stress read link relev line set coupl element dens arrai displai matrix dens arrai omit matrix element set valu twice datafram time initi time",
        "Solution_readability":6.5,
        "Solution_reading_time":25.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":283.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1533754693910,
        "Answerer_location":null,
        "Answerer_reputation_count":801.0,
        "Answerer_view_count":80.0,
        "Challenge_adjusted_solved_time":1373.19555,
        "Challenge_answer_count":1,
        "Challenge_body":"<ol>\n<li><p>task : object_detection<\/p>\n<\/li>\n<li><p>environment: AWS sagemaker<\/p>\n<\/li>\n<li><p>instance type: 'ml.p2.xlarge' | num_instances = 1<\/p>\n<\/li>\n<li><p>Main file to be run: <a href=\"https:\/\/github.com\/tensorflow\/models\/blob\/master\/research\/object_detection\/model_main_tf2.py\" rel=\"nofollow noreferrer\">original<\/a><\/p>\n<\/li>\n<li><p>Problematic code segment from the main file:<\/p>\n<pre><code>    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n    FLAGS.tpu_name)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n    elif FLAGS.num_workers &gt; 1:\n        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n    else:\n        strategy = tf.compat.v2.distribute.MirroredStrategy()\n<\/code><\/pre>\n<\/li>\n<li><p>Problem : Can't find the proper value to be given as <code>tpu_name<\/code> argument.<\/p>\n<\/li>\n<li><p>My research on the problem:<\/p>\n<\/li>\n<\/ol>\n<p>According to the tensorflow documentation in <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/cluster_resolver\/TPUClusterResolver\" rel=\"nofollow noreferrer\">tf.distribute.cluster_resolver.TPUClusterResolver<\/a>, it says that this resolver works only on Google Cloud platform.<\/p>\n<blockquote>\n<p>This is an implementation of cluster resolvers for the Google Cloud\nTPU service.<\/p>\n<p>TPUClusterResolver supports the following distinct environments:\nGoogle Compute Engine Google Kubernetes Engine Google internal<\/p>\n<p>It can be passed into tf.distribute.TPUStrategy to support TF2\ntraining on Cloud TPUs.<\/p>\n<\/blockquote>\n<p>But from <a href=\"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/39721\" rel=\"nofollow noreferrer\">this issue in github<\/a>, I found out that a similar code also works in Azure.<\/p>\n<ol start=\"8\">\n<li>My question :<\/li>\n<\/ol>\n<p>Is there a way I can bypass this resolver and initialize my tpu in <strong>sagemaker<\/strong> ?<\/p>\n<p>Even better, if I can find a way to insert the name or url of sagemaker gpu to the resolver and initiate it from there ?<\/p>",
        "Challenge_closed_time":1614007459500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609059338347,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1609063955520,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65464181",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":15.6,
        "Challenge_reading_time":28.92,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":1374.4780980556,
        "Challenge_title":"An alternative to tf.distribute.cluster_resolver.TPUClusterResolver( tpu_name) to be used in Sagemaker?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":355,
        "Challenge_word_count":210,
        "Platform":"Stack Overflow",
        "Poster_created_time":1517147266416,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>Let me clarify some confusion here. TPUs are only offered on Google Cloud and the <code>TPUClusterResolver<\/code> implementation queries GCP APIs to get the cluster config for the TPU node. Thus, no you can't use <code>TPUClusterResolver<\/code> with AWS sagemaker, but you should try it out with TPUs on GCP instead or try find some other documentation on Sagemaker's end on how they enable cluster resolving on their end (if they do).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"tpuclusterresolv tpu document end enabl cluster end bias respons",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"clarifi tpu cloud tpuclusterresolv implement queri api cluster config tpu node tpuclusterresolv tpu document end enabl cluster end",
        "Solution_preprocessed_content":"clarifi tpu cloud implement queri api cluster config tpu node tpu document end enabl cluster end",
        "Solution_readability":11.2,
        "Solution_reading_time":5.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1436184843608,
        "Answerer_location":null,
        "Answerer_reputation_count":305.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":1.8145494444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have seen on various examples (even in Azure ML) that you are able to create appealing charts using R in Visual Studio (not R Studio!), but I have no clue how they did it. I am experienced with R, but if someone could point me in the right direction of how to visualize data sets in Visual Studio and Azure ML; I would really appreciate it.\nHere is an example I would like to duplicate (in both Azure ML and Visual Studio): <a href=\"http:\/\/i.stack.imgur.com\/2aoGB.jpg\" rel=\"nofollow\">Visual studio chart<\/a><\/p>\n\n<p>Image source: <a href=\"https:\/\/regmedia.co.uk\/2016\/03\/09\/r_vis_studio_plot.jpg?x=648&amp;y=348&amp;crop=1\" rel=\"nofollow\">https:\/\/regmedia.co.uk\/2016\/03\/09\/r_vis_studio_plot.jpg?x=648&amp;y=348&amp;crop=1<\/a><\/p>",
        "Challenge_closed_time":1465396070168,
        "Challenge_comment_count":2,
        "Challenge_created_time":1465389537790,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":1485875676343,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37702759",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":11.3,
        "Challenge_reading_time":10.28,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.8145494444,
        "Challenge_title":"How to visualize charts in Visual Studio and Azure ML through R script?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1548,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1465381666536,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>You can install ggplot2 in your solution in the Visual Studio extension Open R (<a href=\"https:\/\/www.visualstudio.com\/en-us\/features\/rtvs-vs.aspx\" rel=\"nofollow noreferrer\">https:\/\/www.visualstudio.com\/en-us\/features\/rtvs-vs.aspx<\/a>) through this line of code and visualize it within the R Plot window in Visual Studio after creating your R-project: <\/p>\n\n<pre><code>install.packages('ggplot2', dep = TRUE)\n\nlibrary(ggplot2)\n<\/code><\/pre>\n\n<p>The reason I have \u00ablibrary(ggplot2)\u00bb is to check if the package got successfully installed, else you would get an error like this: <strong>Error in library(ggplot2) : there is no package called \u2018ggplot2\u2019<\/strong><\/p>\n\n<p>So if you don\u2019t get that error; you should be good to go.<\/p>\n\n<p>For your question about how to output charts; you simply have to populate the ggplot2 charts from a datasource, like in my example below (csv-file):<\/p>\n\n<pre><code>dataset1 &lt;- read.csv(\"Adult Census Income Binary Classification dataset.csv\", header = TRUE, sep = \",\", quote = \"\", fill = TRUE, comment.char = \"\")\n\nhead(dataset1)\n\ninstall.packages('ggplot2', dep = TRUE)\n\nlibrary(ggplot2)\n\nnames(dataset1) &lt;- sub(pattern = ',', replacement = '.', x = names(dataset1))\n\nfoo = qplot(age, data = dataset1, geom = \"histogram\", fill = income, position = \"dodge\");\n\nprint(foo)\n\nbar = qplot(age, data = dataset1, geom = \"density\", alpha = 1, fill = income);\n\nprint(bar)\n<\/code><\/pre>\n\n<p>Here you can see that I create two charts, one histogram and one density-chart.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sLxMN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sLxMN.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>In Azure ML, the same charts (this time I included a histogram for Relationships as well), would look like this:<\/p>\n\n<pre><code>\/\/ Map 1-based optional input ports to variables\n\ndataset1 &lt;- maml.mapInputPort(1) # class: data.frame\n\nlibrary(ggplot2)\n\nlibrary(data.table)\n\nnames(dataset1) &lt;- sub(pattern=',', replacement='.', x=names(dataset1))\n\n\/\/ This time we need to specify the X to be sex; which we didn\u2019t need in Visual Studio\n\nfoo = qplot(x=sex, data=dataset1, geom=\"histogram\", fill=income, position=\"dodge\");\n\nprint(foo)\n\nfoo = qplot(x=relationship, data=dataset1, geom=\"histogram\", fill=income, position=\"dodge\");\n\nprint(foo)\n\nfoo = qplot(x=age, data=dataset1, geom=\"density\", alpha=0.5, fill=income);\n\nprint(foo)\n\n\/\/ Select data.frame to be sent to the output Dataset port maml.mapOutputPort(\"dataset1\");\n<\/code><\/pre>\n\n<p>Remember to put all of this in a \u201cExecute R Script module\u201d in order to run it correctly. After that, you can right lick the module and visualize the result.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/2vTlD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2vTlD.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"instal ggplot visual studio instal packag ggplot dep visual data set plot window visual studio execut modul creat histogram densiti chart ggplot platform",
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_original_content":"instal ggplot visual studio extens open http visualstudio com featur rtv aspx line visual plot window visual studio creat instal packag ggplot dep librari ggplot reason librari ggplot packag successfulli instal librari ggplot packag call ggplot output chart simpli popul ggplot chart datasourc csv file dataset read csv adult censu incom binari classif dataset csv header sep quot comment char head dataset instal packag ggplot dep librari ggplot dataset sub pattern replac dataset foo qplot ag data dataset geom histogram incom posit dodg print foo bar qplot ag data dataset geom densiti alpha incom print bar creat chart histogram densiti chart chart time histogram relationship map base option input port variabl dataset maml mapinputport class data frame librari ggplot librari data tabl dataset sub pattern replac dataset time specifi sex didnt visual studio foo qplot sex data dataset geom histogram incom posit dodg print foo foo qplot relationship data dataset geom histogram incom posit dodg print foo foo qplot ag data dataset geom densiti alpha incom print foo select data frame sent output dataset port maml mapoutputport dataset rememb execut modul order run lick modul visual",
        "Solution_preprocessed_content":"instal ggplot visual studio extens open line visual plot window visual studio creat reason librari packag successfulli instal librari packag call ggplot output chart simpli popul ggplot chart datasourc creat chart histogram chart rememb execut modul order run lick modul visual",
        "Solution_readability":10.8,
        "Solution_reading_time":36.18,
        "Solution_score_count":3.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":315.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":571.4619775,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running the following code to create an endpoint with a preexisting model:<\/p>\n<pre><code>from sagemaker.tensorflow import serving\nsagemaker_session = sagemaker.Session()\nclf_sm_model = serving.Model(model_data='s3:\/\/mybucket\/mytrainedmodel\/model.tar.gz',\n                                     entry_point=&quot;inference.py&quot;,\n                                     source_dir=&quot;inf_source_dir&quot;,\n                                     role=get_execution_role(),\n                                     framework_version='1.14',\n                                     sagemaker_session=sagemaker_session)\n<\/code><\/pre>\n<p>However this create a copy of the model into the default sagemaker bucket. How can I pass a custom path? I've tried model_dir, and output_path but neither are accepted as parameters<\/p>",
        "Challenge_closed_time":1645727041932,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643669778813,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70933814",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.1,
        "Challenge_reading_time":9.46,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":571.4619775,
        "Challenge_title":"SageMaker custom model output path for tensorflow when creating from s3 artifacts",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":216,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421343783700,
        "Poster_location":null,
        "Poster_reputation_count":1387.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>The SageMaker Python SDK repackages your model to include your <code>entry_point<\/code> and <code>source_dir<\/code> files and uploads this &quot;new&quot; tar ball to the SageMaker default bucket.<\/p>\n<p>You can change this behavior by setting the <code>default_bucket<\/code> in your <code>sagemaker_session<\/code> as follows:<\/p>\n<pre><code>sagemaker_session = sagemaker.Session(default_bucket=&quot;&lt;mybucket&gt;&quot;)\n\nclf_sm_model = serving.Model(model_data='s3:\/\/mybucket\/mytrainedmodel\/model.tar.gz',         \n                    .\n                    .\n                    sagemaker_session=sagemaker_session)\n                    .\n                    )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"default bucket session pass path preexist model creat endpoint set default bucket session path access preexist model",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"sdk repackag model entri sourc dir file upload tar ball default bucket set default bucket session session session default bucket clf model serv model model data mybucket mytrainedmodel model tar session session",
        "Solution_preprocessed_content":"sdk repackag model file upload tar ball default bucket set",
        "Solution_readability":20.9,
        "Solution_reading_time":7.65,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":78.4313997222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to convert a numpy array to an amazon protobuf record using <code>sagemaker.amazon.amazon_estimator.AmazonAlgorithmEstimatorBase.record_set()<\/code> However, this is taking a really long time. <\/p>\n\n<p>I'm wondering how the function actually performs and how long it should take<\/p>\n\n<pre><code>from sagemaker import LinearLearner\nimport numpy as np\n\nmodel=LinearLearner(role=get_execution_role(),\n                             train_instance_count=len(train_features),\n                             train_instance_type='ml.t2.medium',\n                             predictor_type='binary_classifier',\n                                )\n<\/code><\/pre>\n\n<pre><code>numpy_array = np.array([[7.4727994e-01 9.5506465e-01 7.6940370e-01 8.2015032e-01 1.8113719e-01\n  7.8720862e-01 2.9677063e-01 2.6711187e-01 7.9498607e-01 4.4924998e-01\n  4.9533784e-01 2.6846960e-01 7.0506859e-01 4.1573554e-01 6.5843487e-01\n  3.2448095e-01 4.3870610e-01 7.2739214e-01 6.0914969e-01 5.5108833e-01\n  5.8835250e-01 5.5872935e-01 4.4392920e-01 6.8353373e-01 4.7664520e-01\n  5.6887656e-01 4.7034043e-01 4.1631639e-01 3.1357434e-01 5.5933639e-04]\n [5.7815754e-01 9.5828843e-01 7.7824914e-01 8.3188844e-01 2.3287645e-01\n  7.7196079e-01 2.5512937e-01 2.7032304e-01 7.8349811e-01 5.0130588e-01\n  4.8345023e-01 3.8397798e-01 5.9922373e-01 4.7720599e-01 6.7832541e-01\n  2.7788603e-01 4.6435007e-01 7.6100332e-01 7.7771670e-01 5.1536995e-01\n  5.8536130e-01 5.6407303e-01 5.0898582e-01 6.7815554e-01 3.0614817e-01\n  5.7353836e-01 3.8981739e-01 4.1474316e-01 3.1389123e-01 3.5031504e-04]]) \n<\/code><\/pre>\n\n<pre><code>record=model.record_set(numpy_array)\n<\/code><\/pre>\n\n<h2>Expected output<\/h2>\n\n<p>I expect the variable record to container a record ready for training with linearlearning model<\/p>",
        "Challenge_closed_time":1563444116136,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563161763097,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57032981",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":23.03,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":78.4313997222,
        "Challenge_title":"how long does converting to amazon protobuf record using .record_set() take to complete",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":119,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1515320453680,
        "Poster_location":"Bangkok",
        "Poster_reputation_count":1848.0,
        "Poster_view_count":206.0,
        "Solution_body":"<p>I believe this is the problem:<\/p>\n\n<pre><code>train_instance_count=len(train_features)\n<\/code><\/pre>\n\n<p>This parameter is about infrastructure (how many SageMaker instances you want to train on), not about features. You should set it to 1.<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker import LinearLearner\nimport numpy as np\n\nmodel=LinearLearner(role=sagemaker.get_execution_role(),\n                             train_instance_count=1,\n                             train_instance_type='ml.t2.medium',\n                             predictor_type='binary_classifier')\n\nnumpy_array = np.array(...)\n\nrecord=model.record_set(numpy_array)\n# This takes &lt;100 ms on my t3 notebook instance\n\nprint(record)\n\n(&lt;class 'sagemaker.amazon.amazon_estimator.RecordSet'&gt;, {'s3_data':\n's3:\/\/sagemaker-eu-west-1-123456789012\/sagemaker-record-sets\/LinearLearner-\n2019-07-18-09-48-21-639\/.amazon.manifest', 'feature_dim': 30, 'num_records': 2,\n's3_data_type': 'ManifestFile', 'channel': 'train'})\n<\/code><\/pre>\n\n<p>The manifest file lists the protobuf-encoded file(s):<\/p>\n\n<pre><code>[{\"prefix\": \"s3:\/\/sagemaker-eu-west-1-123456789012\/sagemaker-record-sets\/LinearLearner-2019-07-18-09-48-21-639\/\"}, \"matrix_0.pbr\"]\n<\/code><\/pre>\n\n<p>You can now use it for the training channel when you call fit(), re: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"set train instanc count paramet len train featur infrastructur featur record set function convert numpi arrai protobuf record manifest file train channel call fit",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"believ train instanc count len train featur paramet infrastructur instanc train featur set import import linearlearn import numpi model linearlearn role execut role train instanc count train instanc type medium predictor type binari classifi numpi arrai arrai record model record set numpi arrai data record set linearlearn manifest featur dim num record data type manifestfil channel train manifest file list protobuf encod file prefix record set linearlearn matrix pbr train channel fit http doc com latest api sdatasourc html",
        "Solution_preprocessed_content":"believ paramet infrastructur featur set manifest file list file train channel fit",
        "Solution_readability":19.0,
        "Solution_reading_time":18.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":101.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1657058369727,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":16.1952341667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using <code>Google-Colab<\/code> for creating a model by using FbProphet and i am try to use Apache Spark in the <code>Google-Colab<\/code> itself. Now can i upload this <code>Google-colab<\/code> notebook in <code>aws Sagemaker\/Lambda<\/code> for free <code>(without charge for Apache Spark and only charge for AWS SageMaker)<\/code>?<\/p>",
        "Challenge_closed_time":1658964743500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658906440657,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1660220920907,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73133746",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":5.13,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":16.1952341667,
        "Challenge_title":"Fb-Prophet, Apache Spark in Colab and AWS SageMaker\/ Lambda",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":51,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_created_time":1658906023852,
        "Poster_location":null,
        "Poster_reputation_count":152.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>In short, You can upload the notebook without any issue into SageMaker. Few things to keep in mind<\/p>\n<ol>\n<li>If you are using the pyspark library in colab and running spark locally,  you should be able to do the same by installing necessary pyspark libs in Sagemaker studio kernels. Here you will only pay for the underlying compute for the notebook instance. If you are experimenting then I would recommend you to use <a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a> to create a free account and try things out.<\/li>\n<li>If you had a separate spark cluster setup then you may need a similar setup in AWS using EMR so that you can connect to the cluster to execute the job.<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"pyspark librari colab run spark local instal pyspark librari studio kernel upload notebook pai underli comput notebook instanc separ spark cluster setup setup emr connect cluster execut job bias respons",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"short upload notebook pyspark librari colab run spark local instal pyspark lib studio kernel pai underli comput notebook instanc http studiolab creat free account separ spark cluster setup setup emr connect cluster execut job",
        "Solution_preprocessed_content":"short upload notebook pyspark librari colab run spark local instal pyspark lib studio kernel pai underli comput notebook instanc creat free account separ spark cluster setup setup emr connect cluster execut job",
        "Solution_readability":9.9,
        "Solution_reading_time":9.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":118.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554060427012,
        "Answerer_location":null,
        "Answerer_reputation_count":2433.0,
        "Answerer_view_count":228.0,
        "Challenge_adjusted_solved_time":3.8459444445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have <code>parquet<\/code> files that are generated via <code>spark<\/code> and the filename (key) in <code>s3<\/code> will always change post ETL job.  This is the code I use to read the <code>parquet<\/code> files via <code>boto3<\/code> in <code>sagemaker<\/code>.  Looking for a way to dynamically read the <code>S3<\/code> filename (key) since hard-coding the key will fail the read since it changes every time.  How can this be achieved?  Thanks.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>filename = \"datasets\/randomnumbergenerator.parquet\"\nbucketName = \"bucket-name\"\n\nbuffer = io.BytesIO()\nclient = boto3.resource(\"s3\")\nobj = client.Object(bucketName, filename)\nobj.download_fileobj(buffer)\ndf = pd.read_parquet(buffer)\n<\/code><\/pre>",
        "Challenge_closed_time":1583863596403,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583849751003,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1583863687520,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60619460",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":9.9,
        "Challenge_reading_time":10.14,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":3.8459444445,
        "Challenge_title":"Dynamically read changing filename key",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":280,
        "Challenge_word_count":87,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554060427012,
        "Poster_location":null,
        "Poster_reputation_count":2433.0,
        "Poster_view_count":228.0,
        "Solution_body":"<p>This solution is working for me.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\nimport pandas as pd\nimport io\nimport pyarrow\nimport fastparquet\n\ndef dynamically_read_filename_key(bucket, prefix='', suffix=''):\n    s3 = boto3\\\n    .client(\"s3\",\\\n            region_name=os.environ['AWS_DEFAULT_REGION'],\\\n            aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\\\n            aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'])\n    kwargs = {'Bucket': bucket}\n    if isinstance(prefix, str):\n        kwargs['Prefix'] = prefix\n    resp = s3\\\n    .list_objects_v2(**kwargs)\n    for obj in resp['Contents']:\n        key = obj['Key']\n    if key.startswith(prefix) and key.endswith(suffix):\n        return key\n\nfilename = \"\".join(i for i in dynamically_read_filename_key\\\n                   (bucket=\"my-bucket\",\\\n                    prefix=\"datasets\/\",\\\n                    suffix=\".parquet\"))\n\nbucket = \"my-bucket\"\n\ndef parquet_read_filename_key(bucket, filename):\n    client = boto3\\\n    .resource(\"s3\",\\\n            region_name=os.environ['AWS_DEFAULT_REGION'],\\\n            aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\\\n            aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'])\n    buffer = io.BytesIO()\n    obj = client.Object(bucket, filename)\n    obj.download_fileobj(buffer)\n    df = pd.read_parquet(buffer)\n    return df\n\ndf = parquet_read_filename_key(bucket, filename)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"boto dynam read filenam kei parquet file gener spark dynam read filenam kei function read filenam kei parquet read filenam kei function read parquet file dynam read filenam kei function list object bucket return filenam kei start prefix end suffix parquet read filenam kei function read parquet file filenam kei return panda datafram",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"import boto import panda import import pyarrow import fastparquet dynam read filenam kei bucket prefix suffix boto client region environ default region access kei environ access kei secret access kei environ secret access kei kwarg bucket bucket isinst prefix str kwarg prefix prefix resp list object kwarg obj resp kei obj kei kei startswith prefix kei endswith suffix return kei filenam dynam read filenam kei bucket bucket prefix dataset suffix parquet bucket bucket parquet read filenam kei bucket filenam client boto resourc region environ default region access kei environ access kei secret access kei environ secret access kei buffer bytesio obj client object bucket filenam obj download fileobj buffer read parquet buffer return parquet read filenam kei bucket filenam",
        "Solution_preprocessed_content":null,
        "Solution_readability":18.9,
        "Solution_reading_time":16.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1436432728608,
        "Answerer_location":"Colleferro, Italy",
        "Answerer_reputation_count":809.0,
        "Answerer_view_count":361.0,
        "Challenge_adjusted_solved_time":142.5591269445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an experiment in which a module R script uses functions defined in a zip source (Data Exploration). <a href=\"https:\/\/blogs.msdn.microsoft.com\/benjguin\/2014\/09\/24\/how-to-upload-an-r-package-to-azure-machine-learning\/\" rel=\"nofollow noreferrer\">Here<\/a> it's described how to do about the packages not already existing in the Azure environment.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/jKLlP.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jKLlP.png\" alt=\"enter image description here\"><\/a> <\/p>\n\n<p>The DataExploration module has been imported from a file Azure.zip containing all the packages and functions I need (as shown in the next picture).<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WlrVE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WlrVE.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I run the experiment nothing goes wrong. At the contrary, watching the log it seems clear that Azure is able to manage the source.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/AuJLD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AuJLD.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The problem is that, when I deploy the web service (classic), if I run the experiment I get the following error:<\/p>\n\n<blockquote>\n  <p>FailedToEvaluateRScript: The following error occurred during\n  evaluation of R script: R_tryEval: return error: Error in\n  .zip.unpack(pkg, tmpDir) : zip file 'src\/scales_0.4.0.zip' not found ,\n  Error code: LibraryExecutionError, Http status code: 400, Timestamp:\n  Thu, 21 Jul 2016 09:05:25 GMT<\/p>\n<\/blockquote>\n\n<p>It's like he cannot see the scales_0.4.0.zip into the 'src' folder.<\/p>\n\n<p>The strange fact is that all used to work until some days ago. Then I have copied the experiment on a second workspace and it gives me the above error. <\/p>\n\n<p>I have also tried to upload again the DataExploration module on the new workspace, but it's the same.<\/p>",
        "Challenge_closed_time":1469606718630,
        "Challenge_comment_count":1,
        "Challenge_created_time":1469093505773,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38500359",
        "Challenge_link_count":7,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":25.77,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":142.5591269445,
        "Challenge_title":"Azure: importing not already existing packages in 'src'",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":144,
        "Challenge_word_count":241,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436432728608,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation_count":809.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>I have \"solved\" thanks to the help of the AzureML support: it is a bug they are trying to solve right now.<\/p>\n\n<p>The bug shows up when you have <strong>more R script modules<\/strong>, and the <strong>first has no a zip<\/strong> input module while the following have. <\/p>\n\n<p><em>Workaround<\/em>: connect the zip input module to the first R script module too.\n<a href=\"https:\/\/i.stack.imgur.com\/C4fV9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/C4fV9.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"modul zip input modul on team workaround connect zip input modul modul",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"modul zip input modul workaround connect zip input modul modul",
        "Solution_preprocessed_content":"modul zip input modul workaround connect zip input modul modul",
        "Solution_readability":7.5,
        "Solution_reading_time":6.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":69.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1341655118270,
        "Answerer_location":"Egypt",
        "Answerer_reputation_count":505.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":19.6138405556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a deployed model on sagemaker with two production variants. I was wondering if you get charged for both variants even if I set all the traffic to just go through one of them.<\/p>\n<p>The docs on pricing are found below but I couldn't seem to find the answer to this.<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/<\/a><\/p>",
        "Challenge_closed_time":1622110871903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622040262077,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67707288",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.42,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":19.6138405556,
        "Challenge_title":"Do you get charged for production variants on sagemaker that have no traffic going through them?",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":40,
        "Challenge_word_count":71,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521737124656,
        "Poster_location":"Milton Keynes",
        "Poster_reputation_count":738.0,
        "Poster_view_count":69.0,
        "Solution_body":"<p>You will be charged as long as the model is running on an instance regardless of whether the traffic is going through it or not as still the model is running on an instance.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"charg variant deploi model model run instanc regardless traffic",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"charg model run instanc regardless traffic model run instanc",
        "Solution_preprocessed_content":"charg model run instanc regardless traffic model run instanc",
        "Solution_readability":14.2,
        "Solution_reading_time":2.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1341161196310,
        "Answerer_location":"Tokyo",
        "Answerer_reputation_count":1057.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When running <code>kedro install<\/code> I get the following error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>Attempting uninstall: terminado\n    Found existing installation: terminado 0.8.3\nERROR: Cannot uninstall 'terminado'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n<\/code><\/pre>\n<p>This github <a href=\"https:\/\/github.com\/jupyter\/notebook\/issues\/4543\" rel=\"nofollow noreferrer\">issue<\/a> suggests the following fix:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install terminado --user --ignore-installed\n<\/code><\/pre>\n<p>But it does not work for me as I keep having the same error.<\/p>\n<p><strong>Note:<\/strong>\nThis question is similar to <a href=\"https:\/\/stackoverflow.com\/questions\/61770369\/docker-ubuntu-20-04-cannot-uninstall-terminado-and-problems-with-pip\">this<\/a> but different enough that I think it is worth asking separately.<\/p>",
        "Challenge_closed_time":1605936149736,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605936149737,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64940102",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":14.3,
        "Challenge_reading_time":13.4,
        "Challenge_score_count":4,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Kedro install - Cannot uninstall `terminado`",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":2331,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1341161196310,
        "Poster_location":"Tokyo",
        "Poster_reputation_count":1057.0,
        "Poster_view_count":76.0,
        "Solution_body":"<p>The problem is the version that the kedro template project requires see <code>src\/requiremetns.txt<\/code><\/p>\n<p>In my project it is <code>terminado==0.9.1<\/code>, hence the following solves the problem:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install terminado==0.9.1  --user --ignore-installed\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"instal version terminado templat run pip instal terminado ignor instal",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"version templat src requiremetn txt terminado pip instal terminado ignor instal",
        "Solution_preprocessed_content":null,
        "Solution_readability":10.1,
        "Solution_reading_time":4.33,
        "Solution_score_count":10.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1501772252047,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":922.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":0.0029661111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Following the pandas documentation for visualization (<a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/visualization.html#visualization-hist\" rel=\"nofollow noreferrer\">https:\/\/pandas.pydata.org\/pandas-docs\/stable\/visualization.html#visualization-hist<\/a>) I am trying to create the following graphics:<\/p>\n\n<pre><code>import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n## A data set in my AzureML workplace experiment \ndf = ds.to_dataframe()\nplt.figure(); \ndf.plot.hist(stacked=True, bins=20) \nplt.figure();df.boxplot()\n<\/code><\/pre>\n\n<p>However, the output is limited to <code>\"&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd12e15dc18&gt;\"<\/code> (for the histogram(=) and <code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd12e0ce828&gt;\"<\/code> (to the box plot), but no image appearing. Can anyone help me to identify what I'm missing out? Thanks!<\/p>\n\n<p>I'm using Python 3 in Jupyter Notebook in AzureML. <\/p>\n\n<p>The <code>df.describe()<\/code> method works properly (there is a dataFrame)<\/p>",
        "Challenge_closed_time":1516035353070,
        "Challenge_comment_count":3,
        "Challenge_created_time":1516035090403,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1516035342392,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48267427",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":13.6,
        "Challenge_reading_time":14.47,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.0729630556,
        "Challenge_title":"Trouble in creating graphics with matplotlib in a Jupyter notebook",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":378,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1316565559336,
        "Poster_location":"Brazil",
        "Poster_reputation_count":2563.0,
        "Poster_view_count":503.0,
        "Solution_body":"<p>Have you set the backend?<\/p>\n\n<pre><code>%matplotlib inline\n<\/code><\/pre>\n\n<p>Worth reading about what this does for a notebook here too\n<a href=\"https:\/\/stackoverflow.com\/questions\/43027980\/purpose-of-matplotlib-inline\/43028034\">Purpose of &quot;%matplotlib inline&quot;<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"set backend matplotlib inlin enabl creat graphic matplotlib jupyt notebook",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"set backend matplotlib inlin worth read notebook purpos matplotlib inlin",
        "Solution_preprocessed_content":"set backend worth read notebook purpos matplotlib inlin",
        "Solution_readability":11.1,
        "Solution_reading_time":3.79,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1435524174732,
        "Answerer_location":"Bursa, Turkey",
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":19.1911183333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an aws sagemaker end-point which need to be called from .Net core client, I have used the AWS SDK that deals with SageMaker and provided the required credentials however, always it keeps saying : <\/p>\n\n<p>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.<\/p>\n\n<p>var requestBody = \"{'url':'\"+\"<a href=\"https:\/\/cdn.pixabay.com\/photo\/2018\/05\/28\/22\/11\/message-in-a-bottle-3437294_960_720.jpg\" rel=\"nofollow noreferrer\">https:\/\/cdn.pixabay.com\/photo\/2018\/05\/28\/22\/11\/message-in-a-bottle-3437294_960_720.jpg<\/a>\" + \"'}\";<\/p>\n\n<pre><code>        var request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest()\n        {\n            EndpointName = \"CG-model-v1-endpoint\",\n            ContentType = \"application\/json;utf-8\",\n            Body = new MemoryStream(Encoding.ASCII.GetBytes(JsonConvert.SerializeObject(requestBody)))\n\n        };\n\n\n        var awsClient = new AmazonSageMakerRuntimeClient(awsAccessKeyId: \"XXXX\", awsSecretAccessKey: \"XXX\", region: RegionEndpoint.EUCentral1);\n\n        try\n        {\n            var resposnse = await awsClient.InvokeEndpointAsync(request);\n\n        }\n        catch (Exception ex)\n        {\n\n            return ApiResponse&lt;bool&gt;.Create(false);\n        }\n<\/code><\/pre>",
        "Challenge_closed_time":1563872105436,
        "Challenge_comment_count":3,
        "Challenge_created_time":1563803017410,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57147396",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":16.4,
        "Challenge_reading_time":16.92,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":19.1911183333,
        "Challenge_title":"Amazon Sage Maker: How to authenticate AWS SageMaker End-Point Request",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":222,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1435524174732,
        "Poster_location":"Bursa, Turkey",
        "Poster_reputation_count":11.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>I found the error , it was simply because of the request content-type,it had to be application\/json instead of application\/json;utf-8<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"request type json utf json",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"simpli request type json json utf",
        "Solution_preprocessed_content":null,
        "Solution_readability":15.4,
        "Solution_reading_time":1.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":1.785035,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create my own custom Sagemaker Framework that runs a custom python script to train a ML model using the entry_point parameter.<\/p>\n\n<p>Following the Python SDK documentation (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a>), I wrote the simplest code to run a training job just to see how it behaves and how Sagemaker Framework works.<\/p>\n\n<p>My problem is that I don't know how to properly build my Docker container in order to run the entry_point script.<\/p>\n\n<p>I added the <code>train.py<\/code> script into the container that only logs the folders and files paths as well as the variables in the containers environment.<\/p>\n\n<p>I was able to run the training job, but I couldn't find any reference of the entry_point script neither in environment variable nor the files in the container.<\/p>\n\n<p>Here is the code I used:<\/p>\n\n<ul>\n<li><strong>Custom Sagemaker Framework Class:<\/strong><\/li>\n<\/ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.estimator import Framework\n\nclass Doc2VecEstimator(Framework):\n    def create_model():\n        pass\n<\/code><\/pre>\n\n<ul>\n<li><strong>train.py:<\/strong><\/li>\n<\/ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>import argparse\nimport os\nfrom datetime import datetime\n\n\ndef log(*_args):\n    print('[log-{}]'.format(datetime.now().isoformat()), *_args)\n\n\ndef listdir_rec(path):\n    ls = os.listdir(path)\n    print(path, ls)\n\n    for ls_path in ls:\n        if os.path.isdir(os.path.join(path, ls_path)):\n            listdir_rec(os.path.join(path, ls_path))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--epochs', type=int, default=5)\n    parser.add_argument('--debug_size', type=int, default=None)\n\n    # # I commented the lines bellow since I haven't configured the environment variables in my container\n    #     # Sagemaker specific arguments. Defaults are set in the environment variables.\n    #     parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n    #     parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    #     parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n\n    args, _ = parser.parse_known_args()\n\n    log('Received arguments {}'.format(args))\n\n    log(os.environ)\n\n    listdir_rec('.')\n\n<\/code><\/pre>\n\n<ul>\n<li><strong>Dockerfile:<\/strong><\/li>\n<\/ul>\n\n<pre class=\"lang-sh prettyprint-override\"><code>FROM ubuntu:18.04\n\nRUN apt-get -y update \\\n    &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n        wget \\\n        python3 \\\n        python3-pip \\\n        nginx \\\n        ca-certificates \\\n    &amp;&amp; \\\n    rm -rf \/var\/lib\/apt\/lists\/*\n\nRUN pip3 install --upgrade pip setuptools \\\n    &amp;&amp; \\\n    pip3 install \\\n        numpy \\\n        scipy \\\n        scikit-learn \\\n        pandas \\\n        flask \\\n        gevent \\\n        gunicorn \\\n        joblib \\\n        pyAthena \\\n        pandarallel \\\n        nltk \\\n        gensim \\\n    &amp;&amp; \\\n    rm -rf \/root\/.cache\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\n\nCOPY train.py \/train.py\n\nENTRYPOINT [\"python3\", \"-u\", \"train.py\"]\n<\/code><\/pre>\n\n<ul>\n<li><strong>Training Job Execution Script:<\/strong><\/li>\n<\/ul>\n\n<pre><code>framework = Doc2VecEstimator(\n    image_name=image,\n    entry_point='train_doc2vec_model.py',\n    output_path='s3:\/\/{bucket_prefix}'.format(bucket_prefix=bucket_prefix),\n\n    train_instance_count=1,\n    train_instance_type='ml.m5.xlarge',\n    train_volume_size=5,\n\n    role=role,\n    sagemaker_session=sagemaker_session,\n    base_job_name='gensim-doc2vec-train-100-epochs-test',\n\n    hyperparameters={\n        'epochs': '100',\n        'debug_size': '100',\n    },\n)\n\nframework.fit(s3_input_data_path, wait=True)\n<\/code><\/pre>\n\n<p>I haven't found a way to make the training job to run the <code>train_doc2vec_model.py<\/code>. So how do I create my own custom Framework class\/container?<\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1590435558776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590429132650,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1590437261656,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62007961",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.5,
        "Challenge_reading_time":49.84,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":1.785035,
        "Challenge_title":"Where does entry_point script is stored in custom Sagemaker Framework training job container?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":747,
        "Challenge_word_count":363,
        "Platform":"Stack Overflow",
        "Poster_created_time":1587648874872,
        "Poster_location":"S\u00e3o Paulo, SP, Brasil",
        "Poster_reputation_count":23.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>SageMaker team created a <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\" rel=\"nofollow noreferrer\">python package <code>sagemaker-training<\/code><\/a> to install in your docker so that your customer container will be able to handle external <code>entry_point<\/code> scripts.\nSee here for an example using Catboost that does what you want to do :)<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/sagemaker-byo-catboost-container-demo\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-byo-catboost-container-demo<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"team creat packag call train instal docker extern entri packag catboost github repositori http github com sampl byo catboost",
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"team creat packag train instal docker extern entri catboost http github com sampl byo catboost",
        "Solution_preprocessed_content":"team creat packag instal docker extern catboost",
        "Solution_readability":18.5,
        "Solution_reading_time":7.39,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":47.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":2.5114116667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a machine learning model with <a href=\"https:\/\/facebook.github.io\/prophet\/docs\/quick_start.html\" rel=\"nofollow noreferrer\">Prophet<\/a>:<\/p>\n\n<p><a href=\"https:\/\/www.kaggle.com\/marcmetz\/ticket-sales-prediction-facebook-prophet\" rel=\"nofollow noreferrer\">https:\/\/www.kaggle.com\/marcmetz\/ticket-sales-prediction-facebook-prophet<\/a><\/p>\n\n<p>I have a web application running with Django. From that application, I want to be able to lookup predictions from the model I created. I assume the best way to do is to deploy my model on Google Cloud Platform or AWS (?) and access forecasts through API calls from my web application to one of these services.<\/p>\n\n<p>My question now: Is that way I described it the right way to do so? I still struggle to decide if either AWS or Google Cloud is the better solution for my case, especially with Prophet. I could only find examples with <code>scikit-learn<\/code>. Any of you who has experience with that and can point me in the right direction?<\/p>",
        "Challenge_closed_time":1563019168512,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563010127430,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57017876",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":13.46,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":2.5114116667,
        "Challenge_title":"Where to deploy machine learning model for API predictions?",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":187,
        "Challenge_word_count":139,
        "Platform":"Stack Overflow",
        "Poster_created_time":1537976685540,
        "Poster_location":null,
        "Poster_reputation_count":2879.0,
        "Poster_view_count":5051.0,
        "Solution_body":"<p>It really depends on the type of model that you are using. In many cases, the model inference is getting a data point (similar to the data points you trained it with) and the model will generate a prediction to that requested data point. In such cases, you need to host the model somewhere in the cloud or on the edge. <\/p>\n\n<p>However, Prophet is often generating the predictions for the future as part of the training of the model. In this case, you only need to serve the predictions that were already calculated, and you can serve them as a CSV file from S3, or as lookup values from a DynamoDB or other lookup data stores. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"model infer data gener predict model host cloud edg choic cloud platform depend prefer prophet gener predict futur train model predict serv csv file lookup valu dynamodb lookup data store bias respons",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"depend type model model infer data data train model gener predict request data host model cloud edg prophet gener predict futur train model serv predict calcul serv csv file lookup valu dynamodb lookup data store",
        "Solution_preprocessed_content":"depend type model model infer data model gener predict request data host model cloud edg prophet gener predict futur train model serv predict calcul serv csv file lookup valu dynamodb lookup data store",
        "Solution_readability":10.1,
        "Solution_reading_time":7.59,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":118.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":5.2634725,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to determine how the Az ML model deployment works with AKS. If you have a single AKS cluster but attach from two separate workspaces, will models from both workspaces get deployed into different azureml-fe's with different IP addresses OR a single azureml-fe with a single IP address? Reason I ask is because I want to purchase a certificate but am unsure if all the models (regardless of workspace) will get exposed under the same IP Address OR multiple IP Addresses? If its the former, I can do it with one certificate...otherwise I have to do it with multiple certificates or SAN based certificates. So if anyone has experience with this, please let me know!<\/p>",
        "Challenge_closed_time":1655345708048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655326759547,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72637756",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":9.36,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":5.2634725,
        "Challenge_title":"Azure Machine Learning Model deployment as AKS web service from multiple workspaces",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":127,
        "Challenge_word_count":128,
        "Platform":"Stack Overflow",
        "Poster_created_time":1376577570772,
        "Poster_location":null,
        "Poster_reputation_count":301.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>By checking AKS webservice class, we can do the multiple services links to single AKS cluster. The endpoint management was described in <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aks.akswebservice?view=azure-ml-py\" rel=\"nofollow noreferrer\">document<\/a>, but this is representing one-to-one cluster and service. For multiple workspaces refer <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aksendpoint?view=azure-ml-py#azureml-core-webservice-aksendpoint-create-version\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>\n<p>Regarding <strong><code>azureml-fe<\/code><\/strong>. There will be one <strong>azureml-fe<\/strong> for one cluster. That means, when we are using different workspaces for deployment into one AKS, then only <strong>one<\/strong> <strong>azureml-fe<\/strong> and can be considered to take <strong>one certificate.<\/strong><\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-deploy-azure-kubernetes-service?tabs=python#azure-ml-router\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-deploy-azure-kubernetes-service?tabs=python#azure-ml-router<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"ak webservic class allow multipl servic link singl ak cluster endpoint multipl workspac document cluster workspac deploy ak certif",
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_original_content":"ak webservic class multipl servic link singl ak cluster endpoint document repres cluster servic multipl workspac document cluster workspac deploy ak certif http doc com deploi kubernet servic tab router",
        "Solution_preprocessed_content":"ak webservic class multipl servic link singl ak cluster endpoint document repres cluster servic multipl workspac document cluster workspac deploy ak certif",
        "Solution_readability":19.4,
        "Solution_reading_time":16.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":81.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1467005064900,
        "Answerer_location":null,
        "Answerer_reputation_count":16.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":310.5718647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've got the following data structure in an Azure DB Table:<\/p>\n\n<pre><code>Client_ID | Customer_ID | Item | Preference_Score\n<\/code><\/pre>\n\n<p>The table can contain different datasets from different clients but the data structure is always the same. Then, the table is imported in Azure ML.<\/p>\n\n<p>What I need is to repeat the same sequence of tasks in Azure ML for all the Client_ID in the above mentioned table.<\/p>\n\n<p>So that in the end I will train a single model for each client and score the data of each single client individually and append the scored data and store it again in Azure SQL.<\/p>\n\n<p>Is there any for each task in Azure ML like in SSIS? What's the best way to do this? <\/p>\n\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1467005967403,
        "Challenge_comment_count":0,
        "Challenge_created_time":1465887908690,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37805113",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":9.2,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":310.5718647222,
        "Challenge_title":"Azure ML Loops through the different tasks",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":575,
        "Challenge_word_count":126,
        "Platform":"Stack Overflow",
        "Poster_created_time":1427804300743,
        "Poster_location":null,
        "Poster_reputation_count":983.0,
        "Poster_view_count":259.0,
        "Solution_body":"<p>You can start from Azure Data Factory for automating batch scoring. In your model instead of web service output, you can use DataWriter exporter module to write the output directly into an Azure Table etc. You can check Microsoft MyDriving reference guide (<a href=\"http:\/\/aka.ms\/mydrivingdocs\" rel=\"nofollow\">http:\/\/aka.ms\/mydrivingdocs<\/a>) at page 107-8 where the machine learning section starts at page 100.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"data factori autom batch score web servic output datawrit export modul write output directli tabl mydriv guid",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"start data factori autom batch score model web servic output datawrit export modul write output directli tabl mydriv guid http aka mydrivingdoc page section start page",
        "Solution_preprocessed_content":"start data factori autom batch score model web servic output datawrit export modul write output directli tabl mydriv guid page section start page",
        "Solution_readability":10.0,
        "Solution_reading_time":5.32,
        "Solution_score_count":-1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":34.4616136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am from R background where we can use Plumber kind tool which provide visualization\/graph as Image via end points so we can integrate in our Java application.<\/p>\n\n<p>Now I want to integrate my Python\/Juypter visualization graph with my Java application but not sure how to host it and make it as endpoint. Right now I using AWS sagemaker to host Juypter notebook<\/p>",
        "Challenge_closed_time":1526073355692,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525949293883,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50271174",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":5.7,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":34.4616136111,
        "Challenge_title":"How to make a Python Visualization as service | Integrate with website | specially sagemaker",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":76,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501403168107,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>Amazon SageMaker is a set of different services for data scientists. You are using the notebook service that is used for developing ML models in an interactive way. The hosting service in SageMaker is creating an endpoint based on a trained model. You can call this endpoint with invoke-endpoint API call for real time inference. <\/p>\n\n<p>It seems that you are looking for a different type of hosting that is more suitable for serving HTML media rich pages, and doesn\u2019t fit into the hosting model of SageMaker. A combination of EC2 instances, with pre-built AMI or installation scripts, Congnito for authentication, S3 and EBS for object and block storage, and similar building blocks should give you a scalable and cost effective solution. <\/p>",
        "Solution_comment_count":4.0,
        "Solution_gpt_summary":"suitabl host visual graph endpoint integr java combin instanc pre built ami instal congnito authent eb object block storag build block scalabl cost",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"set servic data scientist notebook servic model interact host servic creat endpoint base train model endpoint invok endpoint api time infer type host suitabl serv html media rich page doesnt fit host model combin instanc pre built ami instal congnito authent eb object block storag build block scalabl cost",
        "Solution_preprocessed_content":"set servic data scientist notebook servic model interact host servic creat endpoint base train model endpoint api time infer type host suitabl serv html media rich page doesnt fit host model combin instanc ami instal congnito authent eb object block storag build block scalabl cost",
        "Solution_readability":11.3,
        "Solution_reading_time":9.18,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":123.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1340833876128,
        "Answerer_location":null,
        "Answerer_reputation_count":751.0,
        "Answerer_view_count":73.0,
        "Challenge_adjusted_solved_time":1.1074488889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to call an Azure Machine Learning Pipeline Endpoint I've set up using C# &amp; the Machine Learning REST api.<\/p>\n<p>I am certain that I have the Service Principal configured correctly, as I can successfully authenticate &amp; hit the endpoint using the <code>azureml-core<\/code> python sdk:<\/p>\n<pre><code>sp = ServicePrincipalAuthentication(\n    tenant_id=tenant_id,\n    service_principal_id=service_principal_id,\n    service_principal_password=service_principal_password)\nws =Workspace.get(\n    name=workspace_name, \n    resource_group=resource_group, \n    subscription_id=subscription_id, \n    auth=sp)\n\nendpoint = PipelineEndpoint.get(ws, name='MyEndpoint')\nendpoint.submit('Test_Experiment')\n<\/code><\/pre>\n<p>I'm using the following example in C# to attempt to run my endpoint: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-pipelines#run-a-published-pipeline-using-c\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-pipelines#run-a-published-pipeline-using-c<\/a><\/p>\n<p>I'm attempting to fill <code>auth_key<\/code> with the following code:<\/p>\n<pre><code>var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\nvar clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\nvar tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\nvar cred = new ClientSecretCredential(tenantId, clientId, clientSecret);\nvar auth_key = cred.GetToken(new Azure.Core.TokenRequestContext(new string[] {&quot;.default&quot; }));\n<\/code><\/pre>\n<p>I receive a 401 (unauthorized).<\/p>\n<p>What am I am doing wrong?<\/p>\n<ul>\n<li>UPDATE *<\/li>\n<\/ul>\n<p>I changed the 'scopes' param in the <code>TokenRequestContext<\/code> to look like:<\/p>\n<pre><code>var auth_key = cred.GetToken(new Azure.Core.TokenRequestContext(new string[] { &quot;http:\/\/DataTriggerApp\/.default&quot; }));\n<\/code><\/pre>\n<p><code>http:\/\/DataTriggerApp<\/code> is one of the <code>servicePrincipalNames<\/code> that shows up when i query my Service Principal from the azure CLI.<\/p>\n<p>Now, when I attempt to use the returned token to call the Machine Learning Pipeline Endpoint, I receive a 403 instead of a 401.  Maybe some progress?<\/p>",
        "Challenge_closed_time":1634160031172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634153827710,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1634156473112,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69561386",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":15.7,
        "Challenge_reading_time":30.97,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":1.7231838889,
        "Challenge_title":"How do I use Service Principal authentication with an Azure Machine Learning Pipeline Endpoint in C#?",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":752,
        "Challenge_word_count":204,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340833876128,
        "Poster_location":null,
        "Poster_reputation_count":751.0,
        "Poster_view_count":73.0,
        "Solution_body":"<p>Ok, through a lot of trial-and-error I was able to come up with two ways of acquiring a token that allows me to hit my Azure Machine Learning Pipeline Endpoint through the REST api.  One uses Microsoft.Identity.Client &amp; one uses Azure.Identity.<\/p>\n<pre><code>using Microsoft.Identity.Client;\n\n...\n\npublic static async Task&lt;string&gt; GetAccessToken()\n{\n      var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\n      var clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\n      var tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\n   \n      var app = ConfidentialClientApplicationBuilder.Create(clientId)\n                                                .WithClientSecret(clientSecret)                                                \n                                                .WithAuthority(AzureCloudInstance.AzurePublic, tenantId)\n                                                .Build();\n      var result = await app.AcquireTokenForClient(new string[] { &quot;https:\/\/ml.azure.com\/.default&quot; }).ExecuteAsync();\n      return result.AccessToken;\n}\n<\/code><\/pre>\n<p>Or:<\/p>\n<pre><code>using Azure.Identity;\n...\n\npublic static async Task&lt;string&gt; GetAccessToken()\n{\n      var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\n      var clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\n      var tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\n\n      var cred = new ClientSecretCredential(tenantId, clientId, clientSecret);\n      var token =  await cred.GetTokenAsync(new Azure.Core.TokenRequestContext(new string[] { &quot;https:\/\/ml.azure.com\/.default&quot; }));\n      return token.Token;\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"servic princip authent pipelin endpoint ident client ident involv acquir token allow hit pipelin endpoint rest api",
        "Solution_last_edit_time":1634160459928,
        "Solution_link_count":2.0,
        "Solution_original_content":"trial come acquir token allow hit pipelin endpoint rest api ident client ident ident client public static async task getaccesstoken var clientid environ getenvironmentvari client var clientsecret environ getenvironmentvari client secret var tenantid environ getenvironmentvari tenant var app confidentialclientapplicationbuild creat clientid withclientsecret clientsecret withauthor azurecloudinst azurepubl tenantid build var await app acquiretokenforcli http com default executeasync return accesstoken ident public static async task getaccesstoken var clientid environ getenvironmentvari client var clientsecret environ getenvironmentvari client secret var tenantid environ getenvironmentvari tenant var cred clientsecretcredenti tenantid clientid clientsecret var token await cred gettokenasync core tokenrequestcontext http com default return token token",
        "Solution_preprocessed_content":"come acquir token allow hit pipelin endpoint rest api",
        "Solution_readability":22.1,
        "Solution_reading_time":20.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":107.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1501163272143,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":1.6257427778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I wonder if it's possible to export my model to a json file, so I can do some kind of versioning.<\/p>\n\n<p>Building up a model with Azure Machine Learning Studio is easy, but I need to save the previous version anytime I do an update.<\/p>\n\n<p>It's possible to do this?<\/p>",
        "Challenge_closed_time":1544666744367,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544660891693,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53753367",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":3.94,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.6257427778,
        "Challenge_title":"How to export my azure machine learning model to json",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":445,
        "Challenge_word_count":59,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458558039430,
        "Poster_location":"Trieste, Province of Trieste, Italy",
        "Poster_reputation_count":1657.0,
        "Poster_view_count":164.0,
        "Solution_body":"<p>In Azure ML Studio, the versioning is available as Run History: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/manage-experiment-iterations\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/manage-experiment-iterations<\/a>\nRegards,\nJaya<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"version studio run histori save previou version time updat",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"studio version run histori http doc com studio iter jaya",
        "Solution_preprocessed_content":"studio version run histori jaya",
        "Solution_readability":30.3,
        "Solution_reading_time":4.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1513106638900,
        "Answerer_location":null,
        "Answerer_reputation_count":276.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":1546.0056783333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am building a time series usecase to automate the preprocess and retrain tasks.At first the data is preprocessed using numpy, pandas, statsmodels etc &amp; later a machine learning algorithm is applied to make predictions.\nThe reason for using inference pipeline is that it reuses the same preprocess code for training and inference. I have checked the examples given by AWS sagemaker team with spark and sci-kit learn. In both the examples they use a sci-kit learn container to fit &amp; transform their preprocess code. Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code? <\/p>\n\n<p>Can someone give me a custom example of using these pipelines? Any help is appreciated!<\/p>\n\n<p><strong>Sources looked into:<\/strong><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline<\/a>\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia<\/a><\/p>",
        "Challenge_closed_time":1575505485728,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574408498230,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58989610",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":19.05,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":304.7187494444,
        "Challenge_title":"How to custom code an inference pipeline in AWS sagemaker?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1186,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553712330910,
        "Poster_location":null,
        "Poster_reputation_count":103.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Apologies for the late response.<\/p>\n\n<p>Below is some documentation on inference pipelines:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html<\/a>\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-real-time.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-real-time.html<\/a><\/p>\n\n<blockquote>\n  <p>Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code?<\/p>\n<\/blockquote>\n\n<p>Your container is an encapsulation of the environment needed for your custom code needed to run properly. Based on the requirements listed above, <code>numpy, pandas, statsmodels etc &amp; later a machine learning algorithm<\/code>, I would create a container if you wish to isolate your dependencies or modify an existing predefined SageMaker container, such as the scikit-learn one, and add your dependencies into that.<\/p>\n\n<blockquote>\n  <p>Can someone give me a custom example of using these pipelines? Any help is appreciated!<\/p>\n<\/blockquote>\n\n<p>Unfortunately, the two example notebooks referenced above are the only examples utilizing inference pipelines. The biggest hurdle most likely is creating containers that fulfill the preprocessing and prediction task you are seeking and then combining those two together into the inference pipeline.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"document infer pipelin creat time seri creat encapsul environ run properli sci kit modifi predefin scikit add depend unfortun pipelin",
        "Solution_last_edit_time":1579974118672,
        "Solution_link_count":4.0,
        "Solution_original_content":"apolog late respons document infer pipelin http doc com latest infer pipelin html http doc com latest infer pipelin time html creat sci kit encapsul environ run properli base list numpi panda statsmodel later algorithm creat wish isol depend modifi predefin scikit add depend pipelin unfortun notebook referenc util infer pipelin biggest hurdl creat fulfil preprocess predict task combin infer pipelin",
        "Solution_preprocessed_content":"apolog late respons document infer pipelin creat encapsul environ run properli base list creat wish isol depend modifi predefin add depend pipelin unfortun notebook referenc util infer pipelin biggest hurdl creat fulfil preprocess predict task combin infer pipelin",
        "Solution_readability":15.8,
        "Solution_reading_time":19.52,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":168.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":18.5281647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to register a data set as a Python step with the Azure Machine Learning Studio designer. Here is my code:<\/p>\n<pre><code>import pandas as pd\nfrom azureml.core import Workspace, Run, Dataset\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    run = Run.get_context()\n    ws = run. experiment.workspace\n    ds = Dataset.from_pandas_dataframe(dataframe1)\n    ds.register(workspace = ws,\n                name = &quot;data set name&quot;,\n                description = &quot;example description&quot;,\n                create_new_version = True)\n    return dataframe1, \n<\/code><\/pre>\n<p>I get an error saying that &quot;create_new_version&quot; in the ds.register line was an unexpected keyword argument. However, this keyword appears in the documentation and I need it to keep track of new versions of the file.<\/p>\n<p>If I remove the argument, I get a different error: &quot;Local data source path not supported for this operation&quot;, so it still does not work. Any help is appreciated. Thanks!<\/p>",
        "Challenge_closed_time":1628119362627,
        "Challenge_comment_count":3,
        "Challenge_created_time":1628113771587,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68658385",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":9.3,
        "Challenge_reading_time":13.29,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.5530666667,
        "Challenge_title":"Azure Machine Learning Studio designer - \"create new version\" unexpected when registering a data set",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":565,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1371499229816,
        "Poster_location":null,
        "Poster_reputation_count":1111.0,
        "Poster_view_count":191.0,
        "Solution_body":"<h2>update<\/h2>\n<p>sharing OP's solution here for easier discovery<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom azureml.core import Workspace, Run, Dataset\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    run = Run.get_context()\n    ws = run. experiment.workspace\n    datastore = ws.get_default_datastore()\n    ds = Dataset.Tabular.register_pandas_dataframe(\n        dataframe1, datastore, 'data_set_name',\n        description = 'data set description.')\n    return dataframe1,\n<\/code><\/pre>\n<h2>original answer<\/h2>\n<p>Sorry you're struggling. You're very close!<\/p>\n<p>A few things may be the culprit here.<\/p>\n<ol>\n<li>It looks like you're using the <code>Dataset<\/code> class, which has been deprecated. I recommend trying <code>Dataset.Tabular.register_pandas_dataframe()<\/code> (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py#register-pandas-dataframe-dataframe--target--name--description-none--tags-none--show-progress-true-\" rel=\"nofollow noreferrer\">docs link<\/a>) instead of <code>Dataset.from_pandas_dataframe()<\/code>. (<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/dataset-api-change-notice.md\" rel=\"nofollow noreferrer\">more about the Dataset API deprecation<\/a>)<\/li>\n<li>More conjectire here, but another thing is there might be some limitations to using dataset registration within an &quot;Execute Python Script&quot; (EPS) module due to:\n<ol>\n<li>the workspace object might not have the right permissions<\/li>\n<li>you might not be able to use the <code>register_pandas_dataframe<\/code> method inside the EPS module, but might have better luck with save the dataframe first to parquet, then calling <code>Dataset.Tabular.from_parquet_files<\/code><\/li>\n<\/ol>\n<\/li>\n<\/ol>\n<p>Hopefully something works here!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"dataset tabular regist panda datafram dataset panda datafram limit dataset registr execut ep modul workspac object permiss inabl regist panda datafram insid ep modul luck save datafram parquet call dataset tabular parquet file",
        "Solution_last_edit_time":1628180472980,
        "Solution_link_count":2.0,
        "Solution_original_content":"updat share easier discoveri import panda core import workspac run dataset datafram datafram run run context run workspac datastor default datastor dataset tabular regist panda datafram datafram datastor data set descript data set descript return datafram origin sorri close culprit dataset class deprec dataset tabular regist panda datafram doc link dataset panda datafram dataset api deprec conjectir limit dataset registr execut ep modul workspac object permiss regist panda datafram insid ep modul luck save datafram parquet call dataset tabular parquet file hopefulli",
        "Solution_preprocessed_content":"updat share easier discoveri origin sorri close culprit class deprec conjectir limit dataset registr execut modul workspac object permiss insid ep modul luck save datafram parquet call hopefulli",
        "Solution_readability":14.7,
        "Solution_reading_time":25.08,
        "Solution_score_count":2.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":165.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1449207605092,
        "Answerer_location":"Manila, NCR, Philippines",
        "Answerer_reputation_count":185.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":92.0650375,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using the following lines of code to specify the desired machine type and accelerator\/GPU on a Kubeflow Pipeline (KFP) that I will be running on a serverless manner through Vertex AI\/Pipelines.<\/p>\n<pre><code>op().\nset_cpu_limit(8).\nset_memory_limit(50G).\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-k80').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>and it works for other GPUs as well i.e. Pascal, Tesla, Volta cards.<\/p>\n<p>However, I can't do the same with the latest accelerator type which is the <code>Tesla A100<\/code> as it requires a special machine type, which is as least an <code>a2-highgpu-1g<\/code>.<\/p>\n<p>How do I make sure that this particular component will run on top of <code>a2-highgpu-1g<\/code> when I run it on Vertex?<\/p>\n<p>If i simply follow the method for older GPUs:<\/p>\n<pre><code>op().\nset_cpu_limit(12). # max for A2-highgpu-1g\nset_memory_limit(85G). # max for A2-highgpu-1g\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>It throws an error when run\/deployed since the machine type that is being spawned is the general type i.e. N1-Highmem-*<\/p>\n<p>Same thing happened when I did not specify the cpu and memory limits, in hope that it will automatically select the right machnie type based on the accelerator constraint.<\/p>\n<pre><code>    op().\n    add_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\n    set_gpu_limit(1)\n<\/code><\/pre>\n<p>Error:\n<code>&quot;NVIDIA_TESLA_A100&quot; is not supported for machine type &quot;n1-highmem-2&quot;,<\/code><\/p>",
        "Challenge_closed_time":1632103988632,
        "Challenge_comment_count":2,
        "Challenge_created_time":1631772554497,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69203143",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":12.8,
        "Challenge_reading_time":21.65,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":92.0650375,
        "Challenge_title":"Using Tesla A100 GPU with Kubeflow Pipelines on Vertex AI",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":473,
        "Challenge_word_count":197,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449207605092,
        "Poster_location":"Manila, NCR, Philippines",
        "Poster_reputation_count":185.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>Currently, GCP don't support A2 Machine type for normal KF Components. A potential workaround right now is to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-job\" rel=\"nofollow noreferrer\"><strong>GCP custom job component<\/strong><\/a> that you can explicitly specify the machine type.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"potenti workaround job compon allow explicitli specifi type type normal compon",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"type normal compon potenti workaround job compon explicitli specifi type",
        "Solution_preprocessed_content":"type normal compon potenti workaround job compon explicitli specifi type",
        "Solution_readability":13.6,
        "Solution_reading_time":4.2,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1467799702463,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":550.0,
        "Answerer_view_count":50.0,
        "Challenge_adjusted_solved_time":42.7221963889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using AWS Sagemaker and trying to upload a data folder into S3 from Sagemaker. I am trying to do is to upload my data into the s3_train_data directory (the directory exists in S3). However, it wouldn't upload it in that bucket, but in a default Bucket that has been created, and in turn creates a new folder directory with the S3_train_data variables.<\/p>\n\n<p>code to input in directory<\/p>\n\n<pre><code>import os\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\nbucket = &lt;bucket name&gt;\nprefix = &lt;folders1\/folders2&gt;\nkey = &lt;input&gt;\n\n\ns3_train_data = 's3:\/\/{}\/{}\/{}\/'.format(bucket, prefix, key)\n\n\n#path 'data' is the folder in the Jupyter Instance, contains all the training data\ninputs = sagemaker_session.upload_data(path= 'data', key_prefix= s3_train_data)\n<\/code><\/pre>\n\n<p>Is the problem in the code or more in how I created the notebook?<\/p>",
        "Challenge_closed_time":1518097617247,
        "Challenge_comment_count":0,
        "Challenge_created_time":1517943817340,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1525621837032,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48650152",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":13.2,
        "Challenge_reading_time":12.32,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":42.7221963889,
        "Challenge_title":"AWS uploading file into wrong bucket",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":722,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509392519216,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>You could look at the Sample notebooks, how to upload the data S3 bucket \nThere have many ways. I am just giving you hints to answer. \nAnd you forgot create a boto3 session to access the S3 bucket <\/p>\n\n<p><strong>It is one of the ways to do it.<\/strong> <\/p>\n\n<pre><code>import os \nimport urllib.request\nimport boto3\n\ndef download(url):\n    filename = url.split(\"\/\")[-1]\n    if not os.path.exists(filename):\n        urllib.request.urlretrieve(url, filename)\n\n\ndef upload_to_s3(channel, file):\n    s3 = boto3.resource('s3')\n    data = open(file, \"rb\")\n    key = channel + '\/' + file\n    s3.Bucket(bucket).put_object(Key=key, Body=data)\n\n\n# caltech-256\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\nupload_to_s3('train', 'caltech-256-60-train.rec')\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-val.rec')\nupload_to_s3('validation', 'caltech-256-60-val.rec')\n<\/code><\/pre>\n\n<p>link : <a href=\"https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb\" rel=\"nofollow noreferrer\">https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb<\/a><\/p>\n\n<p><strong>Another way to do it.<\/strong> <\/p>\n\n<pre><code>bucket = '&lt;your_s3_bucket_name_here&gt;'# enter your s3 bucket where you will copy data and model artifacts\nprefix = 'sagemaker\/breast_cancer_prediction' # place to upload training files within the bucket\n# do some processing then prepare to push the data. \n\nf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\nf.seek(0)\n\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', train_file)).upload_fileobj(f)\n<\/code><\/pre>\n\n<p>Link : <a href=\"https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_applying_machine_learning\/breast_cancer_prediction\/Breast%20Cancer%20Prediction.ipynb\" rel=\"nofollow noreferrer\">https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_applying_machine_learning\/breast_cancer_prediction\/Breast%20Cancer%20Prediction.ipynb<\/a><\/p>\n\n<p>Youtube link : <a href=\"https:\/\/www.youtube.com\/watch?v=-YiHPIGyFGo\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=-YiHPIGyFGo<\/a> - how to pull the data in S3 bucket.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"involv creat boto session access bucket upload function upload data involv specifi bucket prefix data upload upload fileobj function upload data link sampl notebook youtub video",
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_original_content":"sampl notebook upload data bucket hint forgot creat boto session access bucket import import urllib request import boto download url filenam url split path filenam urllib request urlretriev url filenam upload channel file boto resourc data open file kei channel file bucket bucket object kei kei bodi data caltech download http data mxnet data caltech caltech train rec upload train caltech train rec download http data mxnet data caltech caltech val rec upload caltech val rec link http buildcustom notebook notebook sampl notebook introduct algorithm imageclassif caltech imag classif fulltrain ipynb bucket enter bucket copi data model artifact prefix breast cancer predict upload train file bucket process prepar push data bytesio smac write numpi dens tensor train astyp train astyp boto session resourc bucket bucket object path prefix train train file upload fileobj link http buildcustom notebook notebook sampl notebook introduct appli breast cancer predict breast cancer predict ipynb youtub link http youtub com watch yihpigyfgo pull data bucket",
        "Solution_preprocessed_content":"sampl notebook upload data bucket hint forgot creat boto session access bucket link link youtub link pull data bucket",
        "Solution_readability":23.9,
        "Solution_reading_time":33.63,
        "Solution_score_count":1.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":158.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":1.83106,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a computer vision application which connects with Azure cognitive services and runs facial detection algorithm in backend in loop. It connects with a camera in the car and also alarm. My error is with installation of <code>dlib<\/code> library which handles the facial parameters of the person.<\/p>\n<p>I tried to run the following command:<\/p>\n<pre><code>pip install dlib\n<\/code><\/pre>\n<p>But I am getting the following error<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/uZt3w.png\" rel=\"nofollow noreferrer\">dlib installation error<\/a><\/p>\n<p>I'm using <em>Anaconda Navigator Individual Edition, Python 3.9<\/em> as the programming language.<\/p>\n<p>Any help would be appreciated.<\/p>",
        "Challenge_closed_time":1651126783803,
        "Challenge_comment_count":2,
        "Challenge_created_time":1651120191987,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1651158458512,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72038060",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":11.5,
        "Challenge_reading_time":9.46,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.83106,
        "Challenge_title":"DLIB library installation error through pip",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":373,
        "Challenge_word_count":95,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651094469216,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p>As you are using Anaconda Navigator, the IDLE will be either Jupyter Notebook or Spyder. DLIB is a regular upgrading library for facial recognition. It cannot be installed on the base environment. @Avinash mentioned regarding two more libraries <em><strong>cmake<\/strong><\/em> and <em><strong>face_recognition<\/strong><\/em> the major issue for is DLIB.<\/p>\n<p>Follow the below steps to install DLIB using Anaconda Navigator<\/p>\n<ol>\n<li><p>Open <strong>Anaconda Navigator<\/strong> or <strong>Open Anaconda Prompt<\/strong><\/p>\n<\/li>\n<li><p>Click on <strong>Environments<\/strong> (For anaconda navigator UI)<\/p>\n<\/li>\n<li><p>Click on <strong>&quot;Create&quot;<\/strong> and to create a new environment (For anaconda navigator UI)<\/p>\n<\/li>\n<li><p>Select the <strong>python version<\/strong>. (For anaconda navigator UI)<\/p>\n<\/li>\n<li><p>Click on create. (For anaconda navigator UI)<\/p>\n<\/li>\n<li><p>Open anaconda navigator<\/p>\n<\/li>\n<li><p>Enter the following command to navigate from base environment to virtual environment just created<\/p>\n<\/li>\n<li><p>Enter the following command to install DLIB<\/p>\n<\/li>\n<\/ol>\n<p>If you are using normal <em><strong>Python IDLE<\/strong><\/em>, use the following procedure.<\/p>\n<ol>\n<li><strong>Virtual Environment creation using command prompt<\/strong><\/li>\n<\/ol>\n<p>Syntax: <code>python3 -m venv [Virtual Environment Name]<\/code><\/p>\n<p>Code: <code>python3 -m venv dlib<\/code><\/p>\n<ol start=\"2\">\n<li><strong>Activate Virtual Environment<\/strong><\/li>\n<\/ol>\n<p>Syntax: <code>.\\[Virtual Environment Folder Name]\\Scripts\\activate<\/code><\/p>\n<p>Code: <code>.\\dlib\\Scripts\\activate<\/code><\/p>\n<p>or you can directly mention as<\/p>\n<pre><code>activate dlib\n<\/code><\/pre>\n<ol start=\"3\">\n<li><p><strong>Finally you will be shifter from base environment to virtual environment<\/strong><\/p>\n<\/li>\n<li><p><strong>Deactivating virtual environment<\/strong><\/p>\n<p><code>deactivate<\/code><\/p>\n<\/li>\n<\/ol>\n<p>If you are not having proper UI access, then directly go to <strong>Anaconda Navigator<\/strong> and use the below command for virtual environment creation.<\/p>\n<ol>\n<li><p><strong>Open Anaconda Prompt<\/strong><\/p>\n<\/li>\n<li><p><strong>Check for conda installed in path<\/strong><\/p>\n<p><code>conda -V<\/code><\/p>\n<\/li>\n<li><p><strong>Check if conda is updated or not<\/strong><\/p>\n<p><code>conda update conda<\/code><\/p>\n<\/li>\n<li><p><strong>Create a virtual environment<\/strong><\/p>\n<\/li>\n<\/ol>\n<p>syntax: <code>conda create --name [Virtual Environment Name] python=[Version you want to install]<\/code><\/p>\n<p>code: <code>conda create --name dlib python=3.8<\/code><\/p>\n<p>Installing DLIB in virtual environment.<\/p>\n<pre><code>pip install dlib\n<\/code><\/pre>\n<p>remaining activating and deactivating are same.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"creat virtual environ anaconda navig instal dlib creat virtual environ prompt instal dlib anaconda prompt creat virtual environ instal dlib bias summari",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"anaconda navig idl jupyt notebook spyder dlib regular upgrad librari facial recognit instal base environ avinash librari cmake recognit dlib step instal dlib anaconda navig open anaconda navig open anaconda prompt click environ anaconda navig click creat creat environ anaconda navig select version anaconda navig click creat anaconda navig open anaconda navig enter navig base environ virtual environ creat enter instal dlib normal idl procedur virtual environ creation prompt syntax venv virtual environ venv dlib activ virtual environ syntax virtual environ folder activ dlib activ directli activ dlib final shifter base environ virtual environ deactiv virtual environ deactiv access directli anaconda navig virtual environ creation open anaconda prompt conda instal path conda conda updat conda updat conda creat virtual environ syntax conda creat virtual environ version instal conda creat dlib instal dlib virtual environ pip instal dlib remain activ deactiv",
        "Solution_preprocessed_content":"anaconda navig idl jupyt notebook spyder dlib regular upgrad librari facial recognit instal base environ librari cmake dlib step instal dlib anaconda navig open anaconda navig open anaconda prompt click environ click creat creat environ select version click creat open anaconda navig enter navig base environ virtual environ creat enter instal dlib normal idl procedur virtual environ creation prompt syntax activ virtual environ syntax directli final shifter base environ virtual environ deactiv virtual environ access directli anaconda navig virtual environ creation open anaconda prompt conda instal path conda updat creat virtual environ syntax instal dlib virtual environ remain activ deactiv",
        "Solution_readability":16.8,
        "Solution_reading_time":35.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":293.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":63.7779516667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>AWS Sagemaker's notebook comes with Scikit-Learn version 0.19.1<\/p>\n\n<p>I would like to use version 0.20.2. To avoid updating it every time in the notebook code, I tried using the lifecycle configurations. I created one with the following code :<\/p>\n\n<pre><code>#!\/bin\/bash\nset -e\n\/home\/ec2-user\/anaconda3\/bin\/conda install scikit-learn -y\n<\/code><\/pre>\n\n<p>When I run the attached notebook instance and go to the terminal, the version of scikit-learn found with <code>conda list<\/code> is correct (0.20.2). But when I run a notebook and import sklearn, the version is still 0.19.2.<\/p>\n\n<pre><code>import sklearn\nprint(sklearn.__version__)\n<\/code><\/pre>\n\n<p>Is there any virtual environment on the SageMaker instances where I should install the package ? How can I fix my notebook lifecycle configuration ?<\/p>",
        "Challenge_closed_time":1547708377156,
        "Challenge_comment_count":1,
        "Challenge_created_time":1547478776530,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54184145",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":10.77,
        "Challenge_score_count":5,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":63.7779516667,
        "Challenge_title":"AWS Sagemaker does not update the package",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1546,
        "Challenge_word_count":118,
        "Platform":"Stack Overflow",
        "Poster_created_time":1527781503483,
        "Poster_location":"Metz, France",
        "Poster_reputation_count":352.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>Your conda update does not refer to a specific virtualenv, while your notebook probably does. Therefore you dont see an update on the notebook virtualenv.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_gpt_summary":"specifi virtual environ conda updat packag updat environ",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"conda updat virtualenv notebook probabl updat notebook virtualenv",
        "Solution_preprocessed_content":"conda updat virtualenv notebook probabl updat notebook virtualenv",
        "Solution_readability":8.2,
        "Solution_reading_time":2.01,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":18.5603936111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>It was written in the Microsoft AzureML documentation, &quot;A run represents a single trial of an experiment. Runs are used to monitor the asynchronous execution of a trial&quot; and A Run object is also created when you submit or start_logging with the Experiment class.&quot;<\/p>\n<p>Related to <code>start_logging<\/code>, as far as I know, when we have simply started the run by executing this <code>start logging<\/code> method. We have to stop, or complete by <code>complete<\/code> method when the run is completed. This is because  <code>start_logging<\/code> is a synchronized way of creating an experiment. However, Run object created from <code>start_logging<\/code> is to monitor the asynchronous execution of a trial.<\/p>\n<p>Can anyone clarify whether <code>start_logging<\/code> will start asynchronous execution or synchronous execution?<\/p>",
        "Challenge_closed_time":1660048147607,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659981330190,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1660267366787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73282180",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":11.93,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":18.5603936111,
        "Challenge_title":"In AzureML, start_logging will start asynchronous execution or synchronous execution?",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":61,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525287643000,
        "Poster_location":"Ottawa, ON, Canada",
        "Poster_reputation_count":27.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p><strong>start_logging<\/strong> will be considered as <strong>asynchronous<\/strong> execution as this generates the multiple interactive run sessions. In a specific experiment, there is a chance of multiple interactive sessions, that work parallelly and there will be no scenario to be followed in sequential.<\/p>\n<p>The individual operation can be performed and recognized based on the parameters like <strong>args<\/strong>  and <strong>kwargs<\/strong>.<\/p>\n<p>When the start_logging is called, then an interactive run like <strong>jupyter notebook<\/strong> was created. The complete metrics and components which are created when the start_logging was called will be utilized. When the output directory was mentioned for each interactive run, based on the args value, the output folder will be called seamlessly.<\/p>\n<p>The following code block will help to define the operation of start_logging<\/p>\n<pre><code>experiment = Experiment(your_workspace, &quot;your_experiment_name&quot;)\n   run = experiment.start_logging(outputs=None, snapshot_directory=&quot;.&quot;, display_name=&quot;test&quot;)\n   ...\n   run.log_metric(&quot;Accuracy_Value&quot;, accuracy)\n   run.complete()\n<\/code><\/pre>\n<p>the below code block will be defining the basic syntax of start_logging<\/p>\n<pre><code>start_logging(*args, **kwargs)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"function start log initi asynchron execut gener multipl interact run session parallelli sequenti individu oper perform recogn base paramet arg kwarg start log call interact run jupyt notebook creat complet metric compon creat start log call util output directori interact run base arg valu output folder call seamlessli",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"start log asynchron execut gener multipl interact run session chanc multipl interact session parallelli sequenti individu oper perform recogn base paramet arg kwarg start log call interact run jupyt notebook creat complet metric compon creat start log call util output directori interact run base arg valu output folder call seamlessli block defin oper start log workspac run start log output snapshot directori displai test run log metric accuraci valu accuraci run complet block defin syntax start log start log arg kwarg",
        "Solution_preprocessed_content":"asynchron execut gener multipl interact run session chanc multipl interact session parallelli sequenti individu oper perform recogn base paramet arg kwarg call interact run jupyt notebook creat complet metric compon creat call util output directori interact run base arg valu output folder call seamlessli block defin oper block defin syntax",
        "Solution_readability":14.4,
        "Solution_reading_time":17.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":147.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1655446100500,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":161.0086202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Previously, using Kubeflow Pipelines SDK v1, the status of a pipeline could be inferred during pipeline execution by passing an Argo placeholder, <code>{{workflow.status}}<\/code>, to the component, as shown below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import kfp.dsl as dsl\n\ncomponent_1 = dsl.ContainerOp(\n    name='An example component',\n    image='eu.gcr.io\/...\/my-component-img',\n    arguments=[\n               'python3', 'main.py',\n               '--status', &quot;{{workflow.status}}&quot;\n              ]\n)\n<\/code><\/pre>\n<p>This placeholder would take the value <code>Succeeded<\/code> or <code>Failed<\/code> when passed to the component. One use-case for this would be to send a failure-warning to eg. Slack, in combination with <code>dsl.ExitHandler<\/code>.<\/p>\n<p>However, when using Pipeline SDK version 2, <code>kfp.v2<\/code>, together with Vertex AI to compile and run the pipeline the Argo placeholders no longer work, as described by <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/7614\" rel=\"nofollow noreferrer\">this open issue<\/a>. Because of this, I would need another way to check the status of the pipeline within the component. I was thinking I could use the <code>kfp.Client<\/code> <a href=\"https:\/\/kubeflow-pipelines.readthedocs.io\/en\/latest\/source\/kfp.client.html\" rel=\"nofollow noreferrer\">class<\/a>, but I'm assuming this won't work using Vertex AI, since there is no &quot;host&quot; really. Also, there seems to be supported placeholders for to pass the run id (<code>dsl.PIPELINE_JOB_ID_PLACEHOLDER<\/code>) as a placeholder, as per <a href=\"https:\/\/stackoverflow.com\/questions\/68348026\/run-id-in-kubeflow-pipelines-on-vertex-ai\">this SO post<\/a>, but I can't find anything around <code>status<\/code>.<\/p>\n<p>Any ideas how to get the status of a pipeline run within a component, running on Vertex AI?<\/p>",
        "Challenge_closed_time":1655447541936,
        "Challenge_comment_count":1,
        "Challenge_created_time":1651551486133,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1654867910903,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72094768",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.9,
        "Challenge_reading_time":24.36,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":1082.2377230556,
        "Challenge_title":"How to get the status of a pipeline run within a component, running on Vertex AI?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":520,
        "Challenge_word_count":219,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562750927332,
        "Poster_location":"Stockholm, Sverige",
        "Poster_reputation_count":803.0,
        "Poster_view_count":73.0,
        "Solution_body":"<p>Each pipeline run is automatically logged to Google Logging, and so are also the failed pipeline runs.\nThe error logs also contain information about the pipeline and the component that failed.<\/p>\n<p>We can use this information to monitor our logs and set up an alert via email for example.<\/p>\n<p>The logs for our Vertex AI Pipeline runs we get with the following filter<\/p>\n<p>resource.type=\u201daiplatform.googleapis.com\/PipelineJob\u201d\nseverity=(ERROR OR CRITICAL OR ALERT OR EMERGENCY)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/e4jFR.png\" rel=\"nofollow noreferrer\">Vertex AI Pipeline Logs<\/a><\/p>\n<p>Based on those logs you can set up log-based alerts <a href=\"https:\/\/cloud.google.com\/logging\/docs\/alerting\/log-based-alerts\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/logging\/docs\/alerting\/log-based-alerts<\/a>. Notifications via email, Slack, SMS, and many more are possible.<\/p>\n<p>source:\n<a href=\"https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"pipelin run automat log log pipelin run log pipelin compon monitor log set alert email base log set log base alert notif email slack sm",
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_original_content":"pipelin run automat log log pipelin run log pipelin compon monitor log set alert email log pipelin run filter resourc type aiplatform googleapi com pipelinejob sever critic alert pipelin log base log set log base alert http cloud com log doc alert log base alert notif email slack sm sourc http medium com cloud vertex easiest run pipelin ac",
        "Solution_preprocessed_content":"pipelin run automat log log pipelin run log pipelin compon monitor log set alert email log pipelin run filter sever critic alert pipelin log base log set alert notif email slack sm sourc",
        "Solution_readability":13.4,
        "Solution_reading_time":15.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":107.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":54.3866655556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Because of a faulty score.py file in my InferenceConfig, a Model.Deploy failed to Azure Machine Learning, using ACI.  I wanted to create the endpoint in the cloud, but the only state I can see in the portal is Unhealthy.  My local script to deploy the model (using ) keeps running, until it times out. (using the <code>service.wait_for_deployment(show_output=True)<\/code>statement).<\/p>\n\n<p>Is there an option to get more insights in the actual reason\/error message of the deployment turning \"Unhealthy\"?<\/p>",
        "Challenge_closed_time":1584133364176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583937572180,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1584215688008,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60638587",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":7.35,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":54.3866655556,
        "Challenge_title":"How to get insights in exceptions and logging of AzureML endpoint deployment",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":364,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>Usually the timeout is caused by an error in init() function in scoring script. You can get the detailed logs using <code>print(service.get_logs())<\/code> to find the Python error.<\/p>\n\n<p>For more comprehensive troubleshooting guide, see:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"print servic log log deploy troubleshoot guid link http doc com troubleshoot deploy comprehens troubleshoot",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"timeout init function score log print servic log comprehens troubleshoot guid http doc com troubleshoot deploy",
        "Solution_preprocessed_content":"timeout init function score log comprehens troubleshoot guid",
        "Solution_readability":13.3,
        "Solution_reading_time":6.32,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":37.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1317676233236,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation_count":2348.0,
        "Answerer_view_count":206.0,
        "Challenge_adjusted_solved_time":276.3017388889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created an S3 bucket 'testshivaproject' and uploaded an image in it. When I try to access it in sagemaker notebook, it throws an error 'No such file or directory'.<\/p>\n\n<pre><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                   \n\n# Define IAM role\nrole = get_execution_role()\n\nmy_region = boto3.session.Session().region_name # set the region of the instance\n\nprint(\"success :\"+my_region)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> success :us-east-2<\/p>\n\n<pre><code>role\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> 'arn:aws:iam::847047967498:role\/service-role\/AmazonSageMaker-ExecutionRole-20190825T121483'<\/p>\n\n<pre><code>bucket = 'testprojectshiva2' \ndata_key = 'ext_image6.jpg' \ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key) \nprint(data_location)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> s3:\/\/testprojectshiva2\/ext_image6.jpg<\/p>\n\n<pre><code>test = load_img(data_location)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> No such file or directory<\/p>\n\n<p>There are similar questions raised (<a href=\"https:\/\/stackoverflow.com\/questions\/48264656\/load-s3-data-into-aws-sagemaker-notebook\">Load S3 Data into AWS SageMaker Notebook<\/a>) but did not find any solution?<\/p>",
        "Challenge_closed_time":1567829268830,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566834582570,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57661142",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":15.1,
        "Challenge_reading_time":17.39,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":276.3017388889,
        "Challenge_title":"AWS S3 and Sagemaker: No such file or directory",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":4622,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442731325232,
        "Poster_location":"Hyderabad",
        "Poster_reputation_count":187.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>Thanks for using Amazon SageMaker!<\/p>\n\n<p>I sort of guessed from your description, but are you trying to use the Keras load_img function to load images directly from your S3 bucket?<\/p>\n\n<p>Unfortunately, <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/11684\" rel=\"nofollow noreferrer\">the load_img function is designed to only load files from disk<\/a>, so passing an s3:\/\/ URL to that function will always return a <code>FileNotFoundError<\/code>.<\/p>\n\n<p>It's common to first download images from S3 before using them, so you can use boto3 or the AWS CLI to download the file before calling load_img.<\/p>\n\n<p><strong>Alternatively<\/strong>, since the load_img function simply creates a <a href=\"https:\/\/en.wikipedia.org\/wiki\/Python_Imaging_Library\" rel=\"nofollow noreferrer\">PIL Image<\/a> object, you can create the PIL object directly from the data in S3 using boto3, and not use the load_img function at all.<\/p>\n\n<p>In other words, you could do something like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from PIL import Image\n\ns3 = boto3.client('s3')\ntest = Image.open(BytesIO(\n    s3.get_object(Bucket=bucket, Key=data_key)['Body'].read()\n    ))\n<\/code><\/pre>\n\n<p>Hope this helps you out in your project!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"download imag file boto cli call load img function creat pil imag object directli data boto load img function bias summari",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"sort guess descript kera load img function load imag directli bucket unfortun load img function design load file disk pass url function return filenotfounderror common download imag boto cli download file call load img load img function simpli creat pil imag object creat pil object directli data boto load img function pil import imag boto client test imag open bytesio object bucket bucket kei data kei bodi read hope",
        "Solution_preprocessed_content":"sort guess descript kera function load imag directli bucket unfortun function design load file disk pass url function return common download imag boto cli download file call function simpli creat pil imag object creat pil object directli data boto function hope",
        "Solution_readability":10.4,
        "Solution_reading_time":15.67,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":151.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1421596186347,
        "Answerer_location":null,
        "Answerer_reputation_count":812.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":59.8581247222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In the SageMaker documentation, both <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html\" rel=\"nofollow noreferrer\">Multi-Model Endpoints<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-container-direct.html\" rel=\"nofollow noreferrer\">Multi-Container Endpoints with Direct Invocation<\/a> are described as very similar methods to host multiple models on a single endpoint. The given use cases appear identical except that <strong>Multi-Model Endpoints<\/strong> include many more advanced features.<\/p>\n<p>For example, <strong>Multi-Model Endpoints<\/strong> can host <em>n<\/em> number of models and support features such as resource sharing and model caching while <strong>Multi-Container Endpoints with Direct Invocation<\/strong> are limited to hosting only 5 models and lack model caching.<\/p>\n<p>When does it make sense to use <strong>Multi-Container Endpoints with Direct Invocation<\/strong> instead of <strong>Multi-Model Endpoints<\/strong>?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/2EQAA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2EQAA.png\" alt=\"Multi-Model Endpoint\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/A6jyS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/A6jyS.png\" alt=\"Multi-Container Endpoint with Direct Invocation\" \/><\/a><\/p>",
        "Challenge_closed_time":1630054299816,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629838066340,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1629838810567,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68913914",
        "Challenge_link_count":6,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":18.0,
        "Challenge_reading_time":18.92,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":60.0648544444,
        "Challenge_title":"Why Use Multi-Container Endpoints instead of Multi-Model Endpoints?",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":224,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531231343652,
        "Poster_location":"St. Louis, MO, USA",
        "Poster_reputation_count":676.0,
        "Poster_view_count":70.0,
        "Solution_body":"<p>If you want to serve multiple models from the same framework using the same endpoint then you can use multi-model endpoints. Due to using the same framework (e.g. only sklearn models), multi-model endpoints make it to the endpoint when they are called. You can have thousands of those models under one endpoint. Multi-container endpoints on the other hand allow serving models from multiple frameworks, e.g. one TensorFlow, one XGBoost and so on, with direct invocation again. However in this case there's <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-container-direct.html\" rel=\"nofollow noreferrer\">limit of 5 different models<\/a> on a single endpoint.<\/p>\n<p>So depending on the problem you are working, if you need to use multiple frameworks on a single endpoint then you will need to use multi-container endpoint with direct invocation. Otherwise you can use the multi-model endpoint.<\/p>\n<p><a href=\"https:\/\/towardsdatascience.com\/deploy-thousands-of-models-on-sagemaker-real-time-endpoints-with-automatic-retraining-pipelines-4eef7521d5a3\" rel=\"nofollow noreferrer\">Reference<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"serv multipl model framework endpoint multi model endpoint unlimit model endpoint multipl framework singl endpoint multi endpoint direct invoc limit model singl endpoint",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"serv multipl model framework endpoint multi model endpoint framework sklearn model multi model endpoint endpoint call thousand model endpoint multi endpoint hand allow serv model multipl framework tensorflow direct invoc limit model singl endpoint depend multipl framework singl endpoint multi endpoint direct invoc multi model endpoint",
        "Solution_preprocessed_content":"serv multipl model framework endpoint endpoint framework endpoint endpoint call thousand model endpoint endpoint hand allow serv model multipl framework tensorflow direct invoc limit model singl endpoint depend multipl framework singl endpoint endpoint direct invoc endpoint",
        "Solution_readability":12.4,
        "Solution_reading_time":14.35,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":134.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1546882781360,
        "Answerer_location":null,
        "Answerer_reputation_count":106.0,
        "Answerer_view_count":41.0,
        "Challenge_adjusted_solved_time":463.0246583333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am creating annotation .json files to be used in AWS Sagemaker <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection.html\" rel=\"nofollow noreferrer\">object detection algo<\/a>. The format is as below:<\/p>\n\n<pre><code>{\n   \"file\": \"your_image_directory\/sample_image1.jpg\",\n   \"image_size\": [\n      {\n         \"width\": 500,\n         \"height\": 400,\n         \"depth\": 3\n      }\n   ],\n   \"annotations\": [\n      {\n         \"class_id\": 0,\n         \"left\": 111,\n         \"top\": 134,\n         \"width\": 61,\n         \"height\": 128\n      },\n      {\n         \"class_id\": 0,\n         \"left\": 161,\n         \"top\": 250,\n         \"width\": 79,\n         \"height\": 143\n      },\n      {\n         \"class_id\": 1,\n         \"left\": 101,\n         \"top\": 185,\n         \"width\": 42,\n         \"height\": 130\n      }\n   ],\n   \"categories\": [\n      {\n         \"class_id\": 0,\n         \"name\": \"dog\"\n      },\n      {\n         \"class_id\": 1,\n         \"name\": \"cat\"\n      }\n   ]\n}\n<\/code><\/pre>\n\n<p>I want to use Sagemaker ground truth to get the object co-ords(using bounding boxes) as well as the class_id. Can some one please provide some inputs on how I can achieve this for <strong>multiple objects<\/strong> in an image. I do not see the option to provide multiple labels(for multiple objects) in the bounding box task in ground truth.<\/p>\n\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1546883542080,
        "Challenge_comment_count":0,
        "Challenge_created_time":1545216653310,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53849650",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.3,
        "Challenge_reading_time":14.21,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":463.0246583333,
        "Challenge_title":"Bounding box for multiple objects in an image using Sagemaker ground truth",
        "Challenge_topic":"Database Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":637,
        "Challenge_word_count":144,
        "Platform":"Stack Overflow",
        "Poster_created_time":1490126578688,
        "Poster_location":"Atlanta, GA, United States",
        "Poster_reputation_count":23.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>I'm a member of the Ground Truth service team. We do not support multiple objects as part of the bounding boxes labeling task. We recognize this as a feature request and will look to prioritize for future releases.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"team member request featur multipl label multipl object bound box task ground state futur releas",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"member ground servic team multipl object bound box label task recogn featur request priorit futur releas",
        "Solution_preprocessed_content":"member ground servic team multipl object bound box label task recogn featur request priorit futur releas",
        "Solution_readability":7.1,
        "Solution_reading_time":2.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1388584380832,
        "Answerer_location":null,
        "Answerer_reputation_count":396.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":42.3742536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to integrate mlflow ui to our website by using an iframe, but with the header hidden if possible. I found there is an environment variable setting in the source code \/mlflow\/server\/js\/components\/HomeView.js:\n<code>const headerHeight = process.env.HIDE_HEADER === 'true' ? 0 : 60;<\/code> But how can I specify this environment by running the server with <code>mlflow server<\/code>? I tried with <code>HIDE_HEADER=true mlflow server<\/code>, but this doesn't work. Or is there any other way to solve this?<\/p>",
        "Challenge_closed_time":1572468752163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572316204850,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58600732",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.9,
        "Challenge_reading_time":7.46,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":42.3742536111,
        "Challenge_title":"Is there a way to hide mlflow ui header when start the server with mlflow server?",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":296,
        "Challenge_word_count":87,
        "Platform":"Stack Overflow",
        "Poster_created_time":1441151447408,
        "Poster_location":null,
        "Poster_reputation_count":626.0,
        "Poster_view_count":79.0,
        "Solution_body":"<p>@Jason good question, those environment variables are read at build-time for the MLflow UI's Javascript assets. Since the PyPI MLflow wheel comes with pre-built Javascript assets, it's difficult to achieve your use case using a PyPI installation of <code>mlflow<\/code>.<\/p>\n\n<p>However, you can build a custom MLflow wheel from source with the UI header hidden by following the instructions <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/branch-1.3\/CONTRIBUTING.rst#building-a-distributable-artifact\" rel=\"nofollow noreferrer\">here<\/a>, replacing the <code>npm run build<\/code> step with <code>HIDE_HEADER=true npm run build<\/code> (basically, the idea is to set the desired environment variables prior to building Javascript assets via <code>npm run build<\/code>). You can then pip-install that wheel on the node hosting your MLflow server &amp; launch the server via <code>mlflow server<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":12.0,
        "Solution_reading_time":11.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":112.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1477948823888,
        "Answerer_location":null,
        "Answerer_reputation_count":721.0,
        "Answerer_view_count":57.0,
        "Challenge_adjusted_solved_time":50.7645952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an AWS Sagemaker inference endpoint with autoscaling enabled with SageMakerVariantInvocationsPerInstance target metric. When I send a lot of requests to the endpoint the number of instances correctly scales out to the maximum instance count. But after I stop sending the requests the number of instances doesn't scale in to 1, minimum instance count. I waited for many hours. Is there a reason for this behaviour?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1608300540056,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608117787513,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65322286",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":6.36,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":50.7645952778,
        "Challenge_title":"AWS Sagemaker inference endpoint doesn't scale in with autoscaling",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1028,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1382978984190,
        "Poster_location":null,
        "Poster_reputation_count":1311.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>AutoScaling requires a cloudwatch alarm to trigger to scale in.  Sagemaker doesn't push 0 value metrics when there's no activity (it just doesn't push anything).  This leads to the alarm being put into insufficient data and not triggering the autoscaling scale in action when your workload suddenly ends.<\/p>\n<p>Workarounds are either:<\/p>\n<ol>\n<li>Have a step scaling policy using the cloudwatch metric math FILL() function for your scale in.  This way you can tell CloudWatch &quot;if there's no data, pretend this was the metric value when evaluating the alarm.  This is only possible with step scaling since target tracking creates the alarms for you (and AutoScaling will periodically recreate them, so if you make manual changes they'll get deleted)<\/li>\n<li>Have scheduled scaling set the size back down to 1 every evening<\/li>\n<li>Make sure traffic continues at a low level for some times<\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"step scale polici cloudwatch metric math function scale set schedul scale reduc size even traffic low level time reason push valu metric activ alarm insuffici data trigger autosc scale action workload suddenli end",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"autosc cloudwatch alarm trigger scale push valu metric activ push alarm insuffici data trigger autosc scale action workload suddenli end workaround step scale polici cloudwatch metric math function scale cloudwatch data pretend metric valu evalu alarm step scale target track creat alarm autosc period recreat manual delet schedul scale set size even traffic low level time",
        "Solution_preprocessed_content":"autosc cloudwatch alarm trigger scale push valu metric activ alarm insuffici data trigger autosc scale action workload suddenli end workaround step scale polici cloudwatch metric math function scale cloudwatch data pretend metric valu evalu alarm step scale target track creat alarm schedul scale set size even traffic low level time",
        "Solution_readability":11.4,
        "Solution_reading_time":11.24,
        "Solution_score_count":5.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":142.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":9.1737127778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to train a neural network (Tensorflow) on AWS. I have some AWS credits. From my understanding AWS SageMaker is the one best for the job. I managed to load the Jupyter Lab console on SageMaker and tried to find a GPU kernel since, I know it is the best for training neural networks. However, I could not find such kernel. <\/p>\n\n<p>Would anyone be able to help in this regard.<\/p>\n\n<p>Thanks &amp; Best Regards<\/p>\n\n<p>Michael<\/p>",
        "Challenge_closed_time":1585261947503,
        "Challenge_comment_count":1,
        "Challenge_created_time":1585228922137,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60868257",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":3.7,
        "Challenge_reading_time":5.6,
        "Challenge_score_count":11,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":9.1737127778,
        "Challenge_title":"AWS SageMaker on GPU",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":13664,
        "Challenge_word_count":81,
        "Platform":"Stack Overflow",
        "Poster_created_time":1484485648903,
        "Poster_location":"Melbourne Australia",
        "Poster_reputation_count":605.0,
        "Poster_view_count":133.0,
        "Solution_body":"<p>You train models on GPU in the SageMaker ecosystem via 2 different components:<\/p>\n\n<ol>\n<li><p>You can instantiate a GPU-powered <strong><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi.html\" rel=\"noreferrer\">SageMaker Notebook Instance<\/a><\/strong>, for example <code>p2.xlarge<\/code> (NVIDIA K80) or <code>p3.2xlarge<\/code> (NVIDIA V100). This is convenient for interactive development - you have the GPU right under your notebook and can run code on the GPU interactively and monitor the GPU via <code>nvidia-smi<\/code> in a terminal tab - a great development experience. However when you develop directly from a GPU-powered machine, there are times when you may not use the GPU. For example when you write code or browse some documentation. All that time you pay for a GPU that sits idle. In that regard, it may not be the most cost-effective option for your use-case. <\/p><\/li>\n<li><p>Another option is to use a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"noreferrer\"><strong>SageMaker Training Job<\/strong><\/a> running on a GPU instance. This is a preferred option for training, because training metadata (data and model path, hyperparameters, cluster specification, etc) is persisted in the SageMaker metadata store, logs and metrics stored in Cloudwatch and the instance automatically shuts down itself at the end of training. Developing on a small CPU instance and launching training tasks using SageMaker Training API will help you make the most of your budget, while helping you retain metadata and artifacts of all your experiments. You can see <a href=\"https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/using-tensorflow-eager-execution-with-amazon-sagemaker-script-mode\/\" rel=\"noreferrer\">here a well documented TensorFlow example<\/a><\/p><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"gpu kernel train neural network tensorflow gpu power notebook instanc conveni interact cost option train job run gpu instanc prefer option train retain metadata artifact budget friendli document tensorflow",
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"train model gpu ecosystem compon instanti gpu power notebook instanc xlarg nvidia xlarg nvidia conveni interact gpu notebook run gpu interact monitor gpu nvidia smi termin tab great directli gpu power time gpu write brows document time pai gpu sit idl cost option option train job run gpu instanc prefer option train train metadata data model path hyperparamet cluster persist metadata store log metric store cloudwatch instanc automat shut end train small cpu instanc launch train task train api budget retain metadata artifact document tensorflow",
        "Solution_preprocessed_content":"train model gpu ecosystem compon instanti notebook instanc conveni interact gpu notebook run gpu interact monitor gpu termin tab great directli time gpu write brows document time pai gpu sit idl option option train job run gpu instanc prefer option train train metadata persist metadata store log metric store cloudwatch instanc automat shut end train small cpu instanc launch train task train api budget retain metadata artifact document tensorflow",
        "Solution_readability":12.8,
        "Solution_reading_time":23.39,
        "Solution_score_count":20.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":229.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":5.6708125,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Sagemaker Object2Vec to train on data of size 2GB.<\/p>\n<p>ml.p2.xlarge instance took 12 hours to train the data on 4 epochs going at the speed of 5000 samples\/sec.<\/p>\n<p>Now, I am using a higher level instance ml.p2.16xlarge and it only trains at 400 samples\/sec with this in the logs<\/p>\n<pre><code>[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:739: only 114 out of 240 GPU pairs are enabled direct access. It may affect the performance. You can set MXNET_ENABLE_GPU_P2P=0 to turn it off\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .vvvvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: v.vvvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vv.vvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvv.vvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvv.vvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvv.vvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvv.vv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvvv.v.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvvvv........\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: ..........vvvvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........v.vvvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vv.vvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvv.vvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvv.vv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvvv.v\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvvvv.\n<\/code><\/pre>\n<p>There are about 50 million samples.<\/p>\n<p>What can I do to correct this?<\/p>",
        "Challenge_closed_time":1595937519408,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595917104483,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63128111",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":30.5,
        "Challenge_reading_time":56.41,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":5.6708125,
        "Challenge_title":"Sagemaker Object2Vec training samples per second",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":59,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1504123657672,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>2 ideas:<\/p>\n<ol>\n<li>Before increasing the GPU count, grow batch size so that a single\nGPU is as busy as possible<\/li>\n<li>Use P3 instances instead of P2. P3 is more recent, has more memory, more CUDA cores, faster memory bandwidth and NVLink inter-GPU connections. Though it's more\nexpensive by hour, your total training cost may be much smaller if\nproperly tuned<\/li>\n<\/ol>\n<p>Also, if your problem involves sparse updates, meaning if just a small fraction of all tokens appear in a given mini-batch, you can try using <code>token_embedding_storage_type = 'row_sparse'<\/code>, which I think refers to using sparse gradient updates like described here <a href=\"https:\/\/medium.com\/apache-mxnet\/learning-embeddings-for-music-recommendation-with-mxnets-sparse-api-5698f4d7d8\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/apache-mxnet\/learning-embeddings-for-music-recommendation-with-mxnets-sparse-api-5698f4d7d8<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"increas batch size increas gpu count singl gpu busi instanc instanc memori cuda core faster memori bandwidth nvlink inter gpu connect token embed storag type row spars involv spars updat",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"idea increas gpu count grow batch size singl gpu busi instanc memori cuda core faster memori bandwidth nvlink inter gpu connect expens hour train cost smaller properli tune involv spars updat small fraction token mini batch token embed storag type row spars spars gradient updat http medium com apach mxnet embed music mxnet spars api fdd",
        "Solution_preprocessed_content":"idea increas gpu count grow batch size singl gpu busi instanc memori cuda core faster memori bandwidth nvlink connect expens hour train cost smaller properli tune involv spars updat small fraction token spars gradient updat",
        "Solution_readability":15.9,
        "Solution_reading_time":12.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":105.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1476780098123,
        "Answerer_location":"Tokyo, Japan",
        "Answerer_reputation_count":1672.0,
        "Answerer_view_count":159.0,
        "Challenge_adjusted_solved_time":13.8667347222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a custom model on sagemaker inference endpoint (single instance) and while I was load testing, I have observed that CPU utilization metric is maxing out at 100% but according to <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-cpu-gpu-utilization-100\/\" rel=\"nofollow noreferrer\">this post<\/a> it should max out at #vCPU*100 %. I have confirmed that the inference endpoint is not using all cores in clowdwatch logs.<\/p>\n<p>So if one prediction call requires one second to be processed to give response, the deployed model is only able to handle one API call per second which could have been increased to 8 calls per second if all vCPUs would have been used.<\/p>\n<p>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/p>\n<p>Or could we use multiprocessing python package inside <code>inference.py<\/code> file while deploying such that each call comes to the default core and from there all calculations\/prediction is done in any other core whichever is empty at that instance?<\/p>",
        "Challenge_closed_time":1624416005592,
        "Challenge_comment_count":2,
        "Challenge_created_time":1624366085347,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1628443120180,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68083831",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":12.4,
        "Challenge_reading_time":14.07,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":13.8667347222,
        "Challenge_title":"AWS Sagemaker inference endpoint not utilizing all vCPUs",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":661,
        "Challenge_word_count":163,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473938483227,
        "Poster_location":"https:\/\/dzone.com\/articles\/machine-learning-provides-360-degree-view-of-the-c",
        "Poster_reputation_count":909.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>UPDATE<\/p>\n<ul>\n<li><p>Set three environment variables<\/p>\n<ol>\n<li>ENABLE_MULTI_MODEL as &quot;true&quot; (make sure it is string and not bool) and set <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L74\" rel=\"nofollow noreferrer\">SAGEMAKER_HANDLER<\/a> as custom model handler python module path if custom service else dont define it. Also make sure model name <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L94\" rel=\"nofollow noreferrer\">model.mar<\/a>, before compressing it as tar ball and storing in s3<\/li>\n<li>TS_DEFAULT_WORKERS_PER_MODEL as number of vcpus<\/li>\n<li>First environment variable makes sure torch serve env_vars are enabled and second one uses first setting and loads requested number of workers<\/li>\n<li>Setting can be done by passing env dictionary argument to <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">PyTorch function<\/a>. Below is explanation as to why it works<\/li>\n<\/ol>\n<\/li>\n<li><p>From the looks of it, sagemaker deployment for pytorch model as given in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">Sagemaker SDK guide<\/a>, uses <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu\" rel=\"nofollow noreferrer\">this dockerfile<\/a>. In this docker, entrypoint is <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> as in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu#L124\" rel=\"nofollow noreferrer\">Dockerfile line#124<\/a>.<\/p>\n<\/li>\n<li><p>This <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> calls <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">serving.main()<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py\" rel=\"nofollow noreferrer\">serving.py<\/a>. Which ends up calling <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py#L34\" rel=\"nofollow noreferrer\">torchserve.start_torchserve(handler_service=HANDLER_SERVICE)<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py\" rel=\"nofollow noreferrer\">torchserve.py<\/a>.<\/p>\n<\/li>\n<li><p><a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L34\" rel=\"nofollow noreferrer\">At line 34 in torchserve.py<\/a> it defines &quot;\/etc\/default-ts.properties&quot; as DEFAULT_TS_CONFIG_FILE. This file is located <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties\" rel=\"nofollow noreferrer\">here<\/a>. In this file <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties#L2\" rel=\"nofollow noreferrer\">enable_envvars_config=true<\/a> is set. It will use this file setting IFF Environment variable &quot;ENABLE_MULTI_MODEL&quot; is set to &quot;false&quot; as refered <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L167\" rel=\"nofollow noreferrer\">here<\/a>. If it is set to &quot;true&quot; then it will use \/etc\/mme-ts.properties<\/p>\n<\/li>\n<\/ul>\n<hr \/>\n<p>As for the question <code>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/code>\nThere are various settings you can use\nFor models you can set <code>default_workers_per_model<\/code> in config.properties <code>TS_DEFAULT_WORKERS_PER_MODEL=$(nproc --all)<\/code> in environment variables. Environment variables take top priority.<\/p>\n<p>Other than that for each model, you can set the number of workers by using management API, but sadly it is not possible to curl to management API in sagemaker. SO TS_DEFAULT_WORKERS_PER_MODEL is the best bet.\nSetting this should make sure all cores are used.<\/p>\n<p>But if you are using docker file then in entrypoint you can setup scripts which wait for model loading and curl to it to set number of workers<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code># load the model\ncurl -X POST localhost:8081\/models?url=model_1.mar&amp;batch_size=8&amp;max_batch_delay=50\n# after loading the model it is possible to set min_worker, etc\ncurl -v -X PUT http:\/\/localhost:8081\/models\/model_1?min_worker=1\n<\/code><\/pre>\n<p>About the other issue that logs confirm that not all cores are used, I face the same issue and believe that is a problem in the logging system. Please look at this issue <a href=\"https:\/\/github.com\/pytorch\/serve\/issues\/782\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pytorch\/serve\/issues\/782<\/a>. The community itself agrees that if threads are not set, then by default then it prints 0, even if by default it uses 2*num_cores.<\/p>\n<p><strong>For an exhaustive set of all configs possible<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Reference: https:\/\/github.com\/pytorch\/serve\/blob\/master\/docs\/configuration.md\n# Variables that can be configured through config.properties and Environment Variables\n# NOTE: Variables which can be configured through environment variables **SHOULD** have a\n# &quot;TS_&quot; prefix\n# debug\ninference_address=http:\/\/0.0.0.0:8080\nmanagement_address=http:\/\/0.0.0.0:8081\nmetrics_address=http:\/\/0.0.0.0:8082\nmodel_store=\/opt\/ml\/model\nload_models=model_1.mar\n# blacklist_env_vars\n# default_workers_per_model\n# default_response_timeout\n# unregister_model_timeout\n# number_of_netty_threads\n# netty_client_threads\n# job_queue_size\n# number_of_gpu\n# async_logging\n# cors_allowed_origin\n# cors_allowed_methods\n# cors_allowed_headers\n# decode_input_request\n# keystore\n# keystore_pass\n# keystore_type\n# certificate_file\n# private_key_file\n# max_request_size\n# max_response_size\n# default_service_handler\n# service_envelope\n# model_server_home\n# snapshot_store\n# prefer_direct_buffer\n# allowed_urls\n# install_py_dep_per_model\n# metrics_format\n# enable_metrics_api\n# initial_worker_port\n\n# Configuration which are not documented or enabled through environment variables\n\n# When below variable is set true, then the variables set in environment have higher precedence.\n# For example, the value of an environment variable overrides both command line arguments and a property in the configuration file. The value of a command line argument overrides a value in the configuration file.\n# When set to false, environment variables are not used at all\n# use_native_io=\n# io_ratio=\n# metric_time_interval=\nenable_envvars_config=true\n# model_snapshot=\n# version=\n<\/code><\/pre>",
        "Solution_comment_count":6.0,
        "Solution_gpt_summary":"set environ variabl enabl multi model default worker model vcpu handler model handler modul path servic defin set pass env dictionari argument pytorch function model set default worker model config properti default worker model nproc environ variabl model set worker api curl api docker file entrypoint",
        "Solution_last_edit_time":1624470584452,
        "Solution_link_count":23.0,
        "Solution_original_content":"updat set environ variabl enabl multi model bool set handler model handler modul path servic defin model model mar compress tar ball store default worker model vcpu environ variabl torch serv env var enabl set load request worker set pass env dictionari argument pytorch function explan deploy pytorch model sdk guid dockerfil docker entrypoint torchserv entrypoint dockerfil line torchserv entrypoint call serv serv end call torchserv start torchserv handler servic handler servic torchserv line torchserv defin default properti default config file file locat file enabl envvar config set file set iff environ variabl enabl multi model set set mme properti set deploy vcpu increas concurr set model set default worker model config properti default worker model nproc environ variabl environ variabl prioriti model set worker api sadli curl api default worker model bet set core docker file entrypoint setup wait model load curl set worker load model curl localhost model url model mar batch size batch delai load model set worker curl http localhost model model worker log core believ log http github com pytorch serv commun agre thread set default print default num core exhaust set config http github com pytorch serv blob master doc configur variabl configur config properti environ variabl note variabl configur environ variabl prefix debug infer address http address http metric address http model store opt model load model model mar blacklist env var default worker model default respons timeout unregist model timeout netti thread netti client thread job queue size gpu async log cor allow origin cor allow cor allow header decod input request keystor keystor pass keystor type certif file privat kei file request size respons size default servic handler servic envelop model server home snapshot store prefer direct buffer allow url instal dep model metric format enabl metric api initi worker port configur document enabl environ variabl variabl set variabl set environ higher preced valu environ variabl overrid line argument properti configur file valu line argument overrid valu configur file set environ variabl nativ ratio metric time interv enabl envvar config model snapshot version",
        "Solution_preprocessed_content":"updat set environ variabl set model handler modul path servic defin model compress tar ball store vcpu environ variabl torch serv enabl set load request worker set pass env dictionari argument pytorch function explan deploy pytorch model sdk guid dockerfil docker entrypoint dockerfil line call end call line defin file locat file set file set iff environ variabl set set set model set environ variabl environ variabl prioriti model set worker api sadli curl api bet set core docker file entrypoint setup wait model load curl set worker log core believ log commun agre thread set default print default exhaust set config",
        "Solution_readability":20.5,
        "Solution_reading_time":99.55,
        "Solution_score_count":5.0,
        "Solution_sentence_count":55.0,
        "Solution_word_count":599.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554424491243,
        "Answerer_location":null,
        "Answerer_reputation_count":51.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":18.6288588889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>From <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html#google.cloud.aiplatform.AutoMLTabularTrainingJob.run\" rel=\"nofollow noreferrer\">the docs<\/a> it says that<\/p>\n<blockquote>\n<p>The value of the key values of the key (the values in the column) must be in RFC 3339 date-time format, where time-offset = \u201cZ\u201d (e.g. 1985-04-12T23:20:50.52Z)<\/p>\n<\/blockquote>\n<p>The dataset that I'm pointing to is a CSV in cloud storage, where the data is in the format suggested by the docs:<\/p>\n<pre><code>$ gsutil cat gs:\/\/my-data.csv | head | xsv select TS_SPLIT_COL\nTS_SPLIT_COL\n2021-01-18T00:00:00.00Z\n2021-01-18T00:00:00.00Z\n2021-01-04T00:00:00.00Z\n2021-03-06T00:00:00.00Z\n2021-01-15T00:00:00.00Z\n2021-02-11T00:00:00.00Z\n2021-02-05T00:00:00.00Z\n2021-05-20T00:00:00.00Z\n2021-01-05T00:00:00.00Z\n<\/code><\/pre>\n<p>But I receive a <code>Training pipeline failed with error message: The timestamp column must have valid timestamp entries.<\/code> error when I try to run a training job<\/p>\n<p>EDIT: this should hopefully make it more reproducible<\/p>\n<p>data: <a href=\"https:\/\/pastebin.com\/qEDqvzX6\" rel=\"nofollow noreferrer\">https:\/\/pastebin.com\/qEDqvzX6<\/a><\/p>\n<p>Code I'm running:<\/p>\n<pre><code>from google.cloud import aiplatform\n\nPROJECT = &quot;my-project&quot;\nDATASET_ID = &quot;dataset-id&quot;  # points to CSV \n\naiplatform.init(project=PROJECT)\n\ndataset = aiplatform.TabularDataset(DATASET_ID)\n\njob = aiplatform.AutoMLTabularTrainingJob(\n    display_name=&quot;so-58454722&quot;,\n    optimization_prediction_type=&quot;classification&quot;,\n    optimization_objective=&quot;maximize-au-roc&quot;,\n)\n\nmodel = job.run(\n    dataset=dataset,\n    model_display_name=&quot;so-58454722&quot;,\n    target_column=&quot;Y&quot;,\n    training_fraction_split=0.8,\n    validation_fraction_split=0.1,\n    test_fraction_split=0.1,\n    timestamp_split_column_name=&quot;TS_SPLIT_COL&quot;,\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1647602028112,
        "Challenge_comment_count":3,
        "Challenge_created_time":1647473614997,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1647534964220,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71505415",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":13.2,
        "Challenge_reading_time":26.91,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":35.6703097222,
        "Challenge_title":"\"The timestamp column must have valid timestamp entries.\" error when using `timestamp_split_column_name` arg in `AutoMLTabularTrainingJob.run`",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":157,
        "Challenge_word_count":165,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519933306780,
        "Poster_location":null,
        "Poster_reputation_count":3576.0,
        "Poster_view_count":203.0,
        "Solution_body":"<p>Try this timestamp format instead:<\/p>\n<p><code>2022-03-18T01:23:45.123456+00:00<\/code><\/p>\n<p>It uses <code>+00:00<\/code> instead of <code>Z<\/code> to specify timezone.<\/p>\n<p>This change will eliminate the &quot;The timestamp column must have valid timestamp entries.&quot; error<\/p>",
        "Solution_comment_count":4.0,
        "Solution_gpt_summary":"timestamp format timezon offset format elimin messag allow run train job successfulli",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"timestamp format specifi timezon elimin timestamp column timestamp entri",
        "Solution_preprocessed_content":"timestamp format specifi timezon elimin timestamp column timestamp",
        "Solution_readability":8.3,
        "Solution_reading_time":3.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1361240380856,
        "Answerer_location":null,
        "Answerer_reputation_count":3349.0,
        "Answerer_view_count":465.0,
        "Challenge_adjusted_solved_time":46.4306352778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In data exploration phrase, I need some visualization to check the relationship between data columns. I have tried Azure ML built-in visualization and felt it's limited. R ggplot allows me to choose the chart I want and do some customization.<\/p>\n\n<p>I am very new in using Azure ML. When I am trying Azure ML R script and type:<\/p>\n\n<pre><code>library(\"ggplot2\")\n\np &lt;- ggplot(train, aes(x= Item_Visibility, y = Item_Outlet_Sales)) +\ngeom_point(size = 2.5, color = \"navy\") + xlab(\"Item Visibility\") + ylab(\"Item Outlet Sales\")\n<\/code><\/pre>\n\n<p>Cannot find where is my visualization... It is not in visualization result, neither in log ourput\nI have also tried Azure ML built-in plot(), it cannot find the column in my dataset... <\/p>\n\n<p>So, is there anyway, when I am using ggplot in Azure ML R Script, I can find the visualization results?<\/p>",
        "Challenge_closed_time":1461446546867,
        "Challenge_comment_count":2,
        "Challenge_created_time":1461279396580,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36781843",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":8.0,
        "Challenge_reading_time":11.0,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":46.4306352778,
        "Challenge_title":"how to use ggplot through Azure ML R Script",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":425,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361240380856,
        "Poster_location":null,
        "Poster_reputation_count":3349.0,
        "Poster_view_count":465.0,
        "Solution_body":"<p>Finally I found the visualization. Just put the code in AzureML Studio Execute R Script module will be fine.\nAfter the code has been executed successfully, right click Execute R Script module, and choose R device, the visualization is there<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/1aZ0y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1aZ0y.png\" alt=\"ggplot through AzureML R Script\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"locat visual ggplot put studio execut modul click modul choos devic visual",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"final visual studio execut modul execut successfulli click execut modul choos devic visual",
        "Solution_preprocessed_content":"final visual studio execut modul execut successfulli click execut modul choos devic visual",
        "Solution_readability":9.6,
        "Solution_reading_time":5.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":5.2579222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been exploring Azure ML Pipeline. I am referring to <a href=\"https:\/\/github.com\/MicrosoftLearning\/DP100\/blob\/master\/06A%20-%20Creating%20a%20Pipeline.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a> for the below code:<\/p>\n<p>Here is a small snippet from a MS Repo:<\/p>\n<pre><code>train_step = PythonScriptStep(name = &quot;Prepare Data&quot;,\nsource_directory = experiment_folder,\nscript_name = &quot;prep_diabetes.py&quot;,\narguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n'--prepped-data', prepped_data_folder],\noutputs=[prepped_data_folder],\ncompute_target = pipeline_cluster,\nrunconfig = pipeline_run_config,\nallow_reuse = True)\n<\/code><\/pre>\n<p>This suggests that while defining a pipeline, we must provide it a compute resource(pipeline_cluster). This obviously makes sense, since specific compute might be required for a specific step.<\/p>\n<p>But do we need to have this compute resource up and running always, so that whenever a pipeline is triggered, it can find the compute resource?<\/p>\n<p>Also, i figured we can probably keep a cluster with Zero minimum nodes, in which cases cluster is resized whenever pipeline is triggered. But i think there is a minimal cost incurrent in probably container registry regularly in such a setup. Is this the recommended way to deploy ML pipelines or some more efficient approach is possible?<\/p>",
        "Challenge_closed_time":1620823042460,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620804113940,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67498965",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":18.8,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":5.2579222222,
        "Challenge_title":"Pre-existing Compute Resource necessary for running a scheduled Azure ML pipeline?",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":29,
        "Challenge_word_count":171,
        "Platform":"Stack Overflow",
        "Poster_created_time":1315165259620,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":339.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>Yep you're right -- create a <code>ComputeTarget<\/code> with a minimum of zero nodes. The <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/container-registry\/#pricing\" rel=\"nofollow noreferrer\">container registry costs<\/a> are ~$0.16 USD\/day and, IIRC, that cost is bundled in with Azure Machine learning.<\/p>\n<p>This is what our team does for our published pipelines in production.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"creat computetarget minimum node run schedul pipelin cost registri approxim usd dai bundl team publish pipelin",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"yep creat computetarget minimum node registri cost usd dai iirc cost bundl team publish pipelin",
        "Solution_preprocessed_content":"yep creat minimum node registri cost iirc cost bundl team publish pipelin",
        "Solution_readability":9.2,
        "Solution_reading_time":5.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1436696527987,
        "Answerer_location":null,
        "Answerer_reputation_count":724.0,
        "Answerer_view_count":87.0,
        "Challenge_adjusted_solved_time":17.01314,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>As mentioned in step-3 of <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">this blog by AWS<\/a>, I have created a role to invoke sagemaker endpoint. But, when I deploy the API to a stage, I get &quot;AWS ARN for integration contains invalid action&quot; and I can't deploy the stage.\n<a href=\"https:\/\/i.stack.imgur.com\/maMdl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/maMdl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>blog suggested to select API Gateway under services and to keep on next, but didn't mention which policy will be attached. and also that another inline policy to invoke a specific sagemaker endpoint to be created and attached.\n<a href=\"https:\/\/i.stack.imgur.com\/uQwx0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uQwx0.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>and as mentioned in <a href=\"https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/integration-request-basic-setup.html\" rel=\"nofollow noreferrer\">AWS Docs<\/a>:<\/p>\n<blockquote>\n<p>It must also have API Gateway declared (in the role's trust\nrelationship) as a trusted entity to assume the role.<\/p>\n<\/blockquote>\n<p>my role also have the trust-relationshp:\n<a href=\"https:\/\/i.stack.imgur.com\/VJ9aU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VJ9aU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What's missing in my role that led to the error?<\/p>",
        "Challenge_closed_time":1645781417427,
        "Challenge_comment_count":3,
        "Challenge_created_time":1645719715780,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1645720170123,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71255132",
        "Challenge_link_count":8,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":14.4,
        "Challenge_reading_time":22.18,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":17.1393463889,
        "Challenge_title":"API Gateway + AWS SageMaker - AWS ARN for integration contains invalid action for integration with sagemaker",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":177,
        "Challenge_word_count":174,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559910246180,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":2046.0,
        "Poster_view_count":369.0,
        "Solution_body":"<p>Check in all your API methods that you haven't specified &quot;Use Action Name&quot; for any integration request, and then left the &quot;Action&quot; field blank. If you do the &quot;AWS ARN for integration contains invalid action&quot; error message will be shown.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EXEnQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EXEnQ.png\" alt=\"action type choice\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"api left action field blank specifi action integr request prevent arn integr action messag shown",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"api haven specifi action integr request left action field blank arn integr action messag shown",
        "Solution_preprocessed_content":"api haven specifi action integr request left action field blank arn integr action messag shown",
        "Solution_readability":9.5,
        "Solution_reading_time":5.63,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1370505440848,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":2.7925833333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to use the value of the <code>DOMAIN_ID<\/code> variable to filter the EFS to get a FileSystemId. I used the commands below. The first command works and it stores the domain ID. The second one returns an empty list, even though the <code>DOMAIN_ID<\/code> variable is present.<\/p>\n<pre><code>DOMAIN_ID=$(aws sagemaker list-domains --query 'Domains[0].DomainId')\naws efs describe-file-systems --query 'FileSystems[?CreationToken==`$DOMAIN_ID`].FileSystemId'\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>[]\n<\/code><\/pre>\n<p>Expected output:<\/p>\n<pre><code>&lt;Some EFS identifier&gt;\n<\/code><\/pre>",
        "Challenge_closed_time":1658177973060,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658155362930,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1658167919760,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73024189",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":8.33,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":6.2805916667,
        "Challenge_title":"AWS CLI: How to use variable to filter EFS",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":64,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Poster_created_time":1503840831940,
        "Poster_location":"Rotterdam, Netherlands",
        "Poster_reputation_count":33.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>This works (escaping backticks) -<\/p>\n<pre><code>aws efs describe-file-systems --query &quot;FileSystems[?CreationToken==\\`$DOMAIN_ID\\`].FileSystemId&quot;\n<\/code><\/pre>\n<p>You can also use describe-domain command instead -<\/p>\n<pre><code>$ DOMAIN_ID=$(aws sagemaker list-domains --query 'Domains[0].DomainId' | tr -d '&quot;')\n$ aws sagemaker describe-domain --domain-id $DOMAIN_ID --query 'HomeEfsFileSystemId'\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"involv ef file system queri filter ef filesystemid involv domain domain option homeefsfilesystemid",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"escap backtick ef file system queri filesystem creationtoken domain filesystemid domain domain list domain queri domain domainid domain domain domain queri homeefsfilesystemid",
        "Solution_preprocessed_content":null,
        "Solution_readability":15.1,
        "Solution_reading_time":5.76,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":4389.5575297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Sagemaker pipelines are rather unclear to me, I'm not experienced in the field of ML but I'm working on figuring out the pipeline definitions.<\/p>\n<p>I have a few questions:<\/p>\n<ul>\n<li><p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/li>\n<li><p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/li>\n<li><p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/li>\n<\/ul>\n<p>I can't seem to find any examples besides the Python SDK usage, how come?<\/p>\n<p>The docs and workshops seem only to properly describe the Python SDK usage,it would be really helpful if someone could clear this up for me!<\/p>",
        "Challenge_closed_time":1638396070903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638395443060,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70191668",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":10.68,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.1744008334,
        "Challenge_title":"What are SageMaker pipelines actually?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":716,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578250359256,
        "Poster_location":"Amsterdam",
        "Poster_reputation_count":197.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>SageMaker has two things called Pipelines: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pipelines.html\" rel=\"nofollow noreferrer\">Model Building Pipelines<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>. I believe you're referring to the former<\/p>\n<p>A model building pipeline defines steps in a machine learning workflow, such as pre-processing, hyperparameter tuning, batch transformations, and setting up endpoints<\/p>\n<p>A serial inference pipeline is two or more SageMaker models run one after the other<\/p>\n<p>A model building pipeline is defined in JSON, and is hosted\/run in some sort of proprietary, serverless fashion by SageMaker<\/p>\n<blockquote>\n<p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/blockquote>\n<p>You can create\/modify them using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePipeline.html\" rel=\"nofollow noreferrer\">API<\/a>, which can also be called via the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-pipeline.html\" rel=\"nofollow noreferrer\">CLI<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline.Pipeline.create\" rel=\"nofollow noreferrer\">Python SDK<\/a>, or <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html\" rel=\"nofollow noreferrer\">CloudFormation<\/a>. These all use the AWS API under the hood<\/p>\n<p>You can start\/stop\/view them in SageMaker Studio:<\/p>\n<pre><code>Left-side Navigation bar &gt; SageMaker resources &gt; Drop-down menu &gt; Pipelines\n<\/code><\/pre>\n<blockquote>\n<p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/blockquote>\n<p>Unlikely. CodePipeline is more for building and deploying code, not specific to SageMaker. There is no direct integration as far as I can tell, other than that you can start a SM pipeline with CP<\/p>\n<blockquote>\n<p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/blockquote>\n<p>The Python SDK is a stand-alone library to interact with SageMaker in a developer-friendly fashion. It's more dynamic than CloudFormation. Let's you build pipelines using code. Whereas CloudFormation takes a static JSON string<\/p>\n<p>A very simple example of Python SageMaker SDK usage:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>processor = SKLearnProcessor(\n    framework_version=&quot;0.23-1&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    role=&quot;role-arn&quot;,\n)\n\nprocessing_step = ProcessingStep(\n    name=&quot;processing&quot;,\n    processor=processor,\n    code=&quot;preprocessor.py&quot;\n)\n\npipeline = Pipeline(name=&quot;foo&quot;, steps=[processing_step])\npipeline.upsert(role_arn = ...)\npipeline.start()\n<\/code><\/pre>\n<p><code>pipeline.definition()<\/code> produces rather verbose JSON like this:<\/p>\n\n<pre class=\"lang-json prettyprint-override\"><code>{\n&quot;Version&quot;: &quot;2020-12-01&quot;,\n&quot;Metadata&quot;: {},\n&quot;Parameters&quot;: [],\n&quot;PipelineExperimentConfig&quot;: {\n    &quot;ExperimentName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineName&quot;\n    },\n    &quot;TrialName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineExecutionId&quot;\n    }\n},\n&quot;Steps&quot;: [\n    {\n        &quot;Name&quot;: &quot;processing&quot;,\n        &quot;Type&quot;: &quot;Processing&quot;,\n        &quot;Arguments&quot;: {\n            &quot;ProcessingResources&quot;: {\n                &quot;ClusterConfig&quot;: {\n                    &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n                    &quot;InstanceCount&quot;: 1,\n                    &quot;VolumeSizeInGB&quot;: 30\n                }\n            },\n            &quot;AppSpecification&quot;: {\n                &quot;ImageUri&quot;: &quot;246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3&quot;,\n                &quot;ContainerEntrypoint&quot;: [\n                    &quot;python3&quot;,\n                    &quot;\/opt\/ml\/processing\/input\/code\/preprocessor.py&quot;\n                ]\n            },\n            &quot;RoleArn&quot;: &quot;arn:aws:iam::123456789012:role\/foo&quot;,\n            &quot;ProcessingInputs&quot;: [\n                {\n                    &quot;InputName&quot;: &quot;code&quot;,\n                    &quot;AppManaged&quot;: false,\n                    &quot;S3Input&quot;: {\n                        &quot;S3Uri&quot;: &quot;s3:\/\/bucket\/preprocessor.py&quot;,\n                        &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/input\/code&quot;,\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3InputMode&quot;: &quot;File&quot;,\n                        &quot;S3DataDistributionType&quot;: &quot;FullyReplicated&quot;,\n                        &quot;S3CompressionType&quot;: &quot;None&quot;\n                    }\n                }\n            ]\n        }\n    }\n  ]\n}\n<\/code><\/pre>\n<p>You could <em>use<\/em> the above JSON with CloudFormation\/CDK, but you <em>build<\/em> the JSON with the SageMaker SDK<\/p>\n<p>You can also define model building workflows using Step Function State Machines, using the <a href=\"https:\/\/aws-step-functions-data-science-sdk.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Data Science SDK<\/a>, or <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/airflow\/index.html\" rel=\"nofollow noreferrer\">Airflow<\/a><\/p>",
        "Solution_comment_count":6.0,
        "Solution_gpt_summary":"type pipelin model build pipelin serial infer pipelin model build pipelin defin step workflow pre process hyperparamet tune batch transform set endpoint model build pipelin defin json host run sort proprietari serverless fashion creat modifi pipelin api call cli sdk cloudform start stop pipelin studio codepipelin build deploi direct integr",
        "Solution_last_edit_time":1654197850167,
        "Solution_link_count":8.0,
        "Solution_original_content":"call pipelin model build pipelin serial infer pipelin believ model build pipelin defin step workflow pre process hyperparamet tune batch transform set endpoint serial infer pipelin model run model build pipelin defin json host run sort proprietari serverless fashion pipelin stand servic featur option creat consol cloudform cdk resourc creat modifi api call cli sdk cloudform api hood start stop studio left navig bar resourc drop menu pipelin pipelin essenti codepipelin integr unlik codepipelin build deploi direct integr start pipelin sdk cdk cloudform sdk stand librari interact friendli fashion dynam cloudform build pipelin cloudform static json sdk usag processor sklearnprocessor framework version instanc count instanc type larg role role arn process step processingstep process processor processor preprocessor pipelin pipelin foo step process step pipelin upsert role arn pipelin start pipelin definit produc verbos json version metadata paramet pipelineexperimentconfig experimentnam execut pipelinenam trialnam execut pipelineexecutionid step process type process argument processingresourc clusterconfig instancetyp larg instancecount volumesizeingb appspecif imageuri dkr ecr amazonaw com scikit cpu containerentrypoint opt process input preprocessor rolearn arn iam role foo processinginput inputnam appmanag sinput suri bucket preprocessor localpath opt process input sdatatyp sprefix sinputmod file sdatadistributiontyp fullyrepl scompressiontyp json cloudform cdk build json sdk defin model build workflow step function state data scienc sdk airflow",
        "Solution_preprocessed_content":"call pipelin model build pipelin serial infer pipelin believ model build pipelin defin step workflow hyperparamet tune batch transform set endpoint serial infer pipelin model run model build pipelin defin json sort proprietari serverless fashion pipelin option creat consol cloudform cdk resourc api call cli sdk cloudform api hood studio pipelin essenti codepipelin integr unlik codepipelin build deploi direct integr start pipelin sdk cdk cloudform sdk librari interact fashion dynam cloudform build pipelin cloudform static json sdk usag produc verbos json json build json sdk defin model build workflow step function state data scienc sdk airflow",
        "Solution_readability":18.8,
        "Solution_reading_time":68.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":32.0,
        "Solution_word_count":402.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":314.4053583334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a use case wherein I need to refer to the input dataset in the ACI\/AKS which is in a blob (same used for training model). I'm not able to find related resources in the Microsoft official documentation. If anyone suggests to me how to do it, that will be very helpful.<\/p>",
        "Challenge_closed_time":1627304144467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626172285177,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68360738",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":3.83,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":314.4053583334,
        "Challenge_title":"AKS an ACI Deployment with blob mount",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":57,
        "Challenge_word_count":59,
        "Platform":"Stack Overflow",
        "Poster_created_time":1448994884167,
        "Poster_location":null,
        "Poster_reputation_count":265.0,
        "Poster_view_count":156.0,
        "Solution_body":"<p>It will be supported in the near future, Running Python scripts on Azure with Azure Container Instances to connect the blob.\n<a href=\"https:\/\/kohera.be\/tutorials-2\/running-python-scripts-on-azure-with-azure-container-instances\/\" rel=\"nofollow noreferrer\">https:\/\/kohera.be\/tutorials-2\/running-python-scripts-on-azure-with-azure-container-instances\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"link run instanc connect blob futur",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"futur run instanc connect blob http kohera tutori run instanc",
        "Solution_preprocessed_content":"futur run instanc connect blob",
        "Solution_readability":25.9,
        "Solution_reading_time":4.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":81.9709777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking at <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">this<\/a>, which makes all sense. Let us focus on this bit of code:<\/p>\n<pre><code>from sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,\n    inputs=[\n        ProcessingInput(source=&quot;s3:\/\/your-bucket\/path\/to\/your\/data&quot;, destination=&quot;\/opt\/ml\/processing\/input&quot;),\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n\npreprocessing_job_description = sklearn_processor.jobs[-1].describe() \n<\/code><\/pre>\n<p>Here preprocessing.py has to be obviously in the cloud. I am curious, could one also put scripts into an S3 bucket and trigger the job remotely. I can easily to this with hyper parameter optimisation, which does not require dedicated scripts though as I use an OOTB training image.<\/p>\n<p>In this case I can fire off the job like so:<\/p>\n<pre><code>tuning_job_name = &quot;amazing-hpo-job-&quot; + strftime(&quot;%d-%H-%M-%S&quot;, gmtime())\n\nsmclient = boto3.Session().client(&quot;sagemaker&quot;)\nsmclient.create_hyper_parameter_tuning_job(\n    HyperParameterTuningJobName=tuning_job_name,\n    HyperParameterTuningJobConfig=tuning_job_config,\n    TrainingJobDefinition=training_job_definition\n)\n<\/code><\/pre>\n<p>and then monitor the job's progress:<\/p>\n<pre><code>smclient = boto3.Session().client(&quot;sagemaker&quot;)\n\ntuning_job_result = smclient.describe_hyper_parameter_tuning_job(\n    HyperParameterTuningJobName=tuning_job_name\n)\n\nstatus = tuning_job_result[&quot;HyperParameterTuningJobStatus&quot;]\nif status != &quot;Completed&quot;:\n    print(&quot;Reminder: the tuning job has not been completed.&quot;)\n\njob_count = tuning_job_result[&quot;TrainingJobStatusCounters&quot;][&quot;Completed&quot;]\nprint(&quot;%d training jobs have completed&quot; % job_count)\n\nobjective = tuning_job_result[&quot;HyperParameterTuningJobConfig&quot;][&quot;HyperParameterTuningJobObjective&quot;]\nis_minimize = objective[&quot;Type&quot;] != &quot;Maximize&quot;\nobjective_name = objective[&quot;MetricName&quot;]\n\nif tuning_job_result.get(&quot;BestTrainingJob&quot;, None):\n    print(&quot;Best model found so far:&quot;)\n    pprint(tuning_job_result[&quot;BestTrainingJob&quot;])\nelse:\n    print(&quot;No training jobs have reported results yet.&quot;) \n<\/code><\/pre>\n<p>I would think starting and monitoring a SageMaker processing job from a local machine should be possible as with an HPO job but what about the script(s)? Ideally I would like to develop and test them locally and the run remotely. Hope this makes sense?<\/p>",
        "Challenge_closed_time":1662504468407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662209372887,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73592371",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":21.4,
        "Challenge_reading_time":40.76,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":81.9709777778,
        "Challenge_title":"start, monitor and define script of SageMaker processing job from local machine",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":38,
        "Challenge_word_count":224,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>Im not sure I understand the comparison to a Tuning Job.<\/p>\n<p>Based on what you have described, in this case the <code>preprocessing.py<\/code> is actually stored locally. The SageMaker SDK will upload it to S3 for the remote Processing Job to access it. I suggest launching the Job and then taking a look at the inputs in the SageMaker Console.<\/p>\n<p>If you wanted to test the Processing Job locally you can do so using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a>. This will basically imitate the Job locally which aids in debugging the script before kicking off a remote Processing Job. Kindly note docker is required to make use of Local Mode.<\/p>\n<p>Example code for local mode:<\/p>\n<pre><code>from sagemaker.local import LocalSession\nfrom sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n\nsagemaker_session = LocalSession()\nsagemaker_session.config = {'local': {'local_code': True}}\n\n# For local training a dummy role will be sufficient\nrole = 'arn:aws:iam::111111111111:role\/service-role\/AmazonSageMaker-ExecutionRole-20200101T000001'\n\nprocessor = ScriptProcessor(command=['python3'],\n                    image_uri='sagemaker-scikit-learn-processing-local',\n                    role=role,\n                    instance_count=1,\n                    instance_type='local')\n\nprocessor.run(code='processing_script.py',\n                    inputs=[ProcessingInput(\n                        source='.\/input_data\/',\n                        destination='\/opt\/ml\/processing\/input_data\/')],\n                    outputs=[ProcessingOutput(\n                        output_name='word_count_data',\n                        source='\/opt\/ml\/processing\/processed_data\/')],\n                    arguments=['job-type', 'word-count']\n                    )\n\npreprocessing_job_description = processor.jobs[-1].describe()\noutput_config = preprocessing_job_description['ProcessingOutputConfig']\n\nprint(output_config)\n\nfor output in output_config['Outputs']:\n    if output['OutputName'] == 'word_count_data':\n        word_count_data_file = output['S3Output']['S3Uri']\n\nprint('Output file is located on: {}'.format(word_count_data_file))\n\n\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_gpt_summary":"store preprocess local sdk upload remot process job access launch job input consol test process job local local mode imit job local aid debug kick remot process job docker local mode bias summari",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"comparison tune job base preprocess store local sdk upload remot process job access launch job input consol test process job local local mode imit job local aid debug kick remot process job kindli note docker local mode local mode local import localsess process import scriptprocessor processinginput processingoutput session localsess session config local local local train dummi role suffici role arn iam role servic role executionrol processor scriptprocessor imag uri scikit process local role role instanc count instanc type local processor run process input processinginput sourc input data destin opt process input data output processingoutput output count data sourc opt process process data argument job type count preprocess job descript processor job output config preprocess job descript processingoutputconfig print output config output output config output output outputnam count data count data file output soutput suri print output file locat format count data file",
        "Solution_preprocessed_content":"comparison tune job base store local sdk upload remot process job access launch job input consol test process job local local mode imit job local aid debug kick remot process job kindli note docker local mode local mode",
        "Solution_readability":18.2,
        "Solution_reading_time":26.06,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":177.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1595479476676,
        "Answerer_location":"Massachusetts, USA",
        "Answerer_reputation_count":246.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":168.9973711111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create an MLOps Pipeline using Azure DevOps and Azure Databricks. From Azure DevOps, I am submitting a Databricks job to a cluster, which trains a Machine Learning Model and saves it into MLFlow Model Registry with a custom flavour (using PyFunc Custom Model).<\/p>\n<p>Now after the job gets over, I want to export this MLFlow Object (with all dependencies - Conda dependencies, two model files - one <code>.pkl<\/code> and one <code>.h5<\/code>, the Python Class with <code>load_context()<\/code> and <code>predict()<\/code> functions defined so that after exporting I can import it and call predict as we do with MLFlow Models).<\/p>\n<p>How do I export this entire MLFlow Model and save it as an AzureDevOps Artifact to be used in the CD phase (where I will deploy it to an AKS cluster with a custom base image)?<\/p>",
        "Challenge_closed_time":1629787454223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629179063687,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68812238",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":11.34,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":168.9973711111,
        "Challenge_title":"How to export a MLFlow Model from Azure Databricks as an Azure DevOps Artifacts for CD phase?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":575,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>There is no official way to export a Databricks MLflow run from one workspace to another. However, there is an &quot;unofficial&quot; tool that does most of the job with the main limitation being that notebook revisions linked to a run cannot be exported due to lack of a REST API endpoint for this.<\/p>\n<p><a href=\"https:\/\/github.com\/amesar\/mlflow-export-import\" rel=\"nofollow noreferrer\">https:\/\/github.com\/amesar\/mlflow-export-import<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":10.0,
        "Solution_reading_time":5.74,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":57.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1498123491552,
        "Answerer_location":"\u4e2d\u56fdJiangsu Sheng",
        "Answerer_reputation_count":22369.0,
        "Answerer_view_count":3121.0,
        "Challenge_adjusted_solved_time":112.0978552778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I would really like to get access to some of the updated functions in pandas 0.19, but Azure ML studio uses pandas 0.18 as part of the Anaconda 4.0 bundle. Is there a way to update the version that is used within the \"Execute Python Script\" components?<\/p>",
        "Challenge_closed_time":1505456513227,
        "Challenge_comment_count":2,
        "Challenge_created_time":1505401641617,
        "Challenge_favorite_count":2,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46222606",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":5.7,
        "Challenge_reading_time":3.76,
        "Challenge_score_count":5,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":15.2421138889,
        "Challenge_title":"Updating pandas to version 0.19 in Azure ML Studio",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1903,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421081882987,
        "Poster_location":null,
        "Poster_reputation_count":588.0,
        "Poster_view_count":64.0,
        "Solution_body":"<p>I offer the below steps for you to show how to update the version of pandas  library in <code>Execute Python Script<\/code>.<\/p>\n\n<p><strong><em>Step 1<\/em><\/strong> : Use the <code>virtualenv<\/code> component to create an independent python runtime environment in your system.Please install it first with command <code>pip install virtualenv<\/code> if you don't have it.<\/p>\n\n<p>If you installed it successfully ,you could see it in your python\/Scripts file.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZFI2t.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZFI2t.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step2<\/em><\/strong> : Run the commad to create independent python runtime environment.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/nzDqz.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/nzDqz.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 3<\/em><\/strong> : Then go into the created directory's Scripts folder and activate it (this step is important , don't miss it)<\/p>\n\n<p>Please don't close this command window and use <code>pip install pandas==0.19<\/code> to download external libraries in this command window.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Wj857.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Wj857.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 4<\/em><\/strong> : Compress all of the files in the Lib\/site-packages folder into a zip package (I'm calling it pandas - package here)<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ch9Oo.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ch9Oo.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 5<\/em><\/strong> \uff1aUpload the zip package into the Azure Machine Learning WorkSpace DataSet.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/efRkK.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/efRkK.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>specific steps please refer to the <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/cdb56f95-7f4c-404d-bde7-5bb972e6f232\/\" rel=\"noreferrer\">Technical Notes<\/a>.<\/p>\n\n<p>After success, you will see the uploaded package in the DataSet List<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ngGCu.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ngGCu.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 6<\/em><\/strong> \uff1a Before the defination of method <code>azureml_main<\/code> in the Execute Python Script module, you need to remove the old <code>pandas<\/code> modules &amp; its dependencies, then to import <code>pandas<\/code> again, as the code below.<\/p>\n\n<pre><code>import sys\nimport pandas as pd\nprint(pd.__version__)\ndel sys.modules['pandas']\ndel sys.modules['numpy']\ndel sys.modules['pytz']\ndel sys.modules['six']\ndel sys.modules['dateutil']\nsys.path.insert(0, '.\\\\Script Bundle')\nfor td in [m for m in sys.modules if m.startswith('pandas.') or m.startswith('numpy.') or m.startswith('pytz.') or m.startswith('dateutil.') or m.startswith('six.')]:\n    del sys.modules[td]\nimport pandas as pd\nprint(pd.__version__)\n# The entry point function can contain up to two input arguments:\n#   Param&lt;dataframe1&gt;: a pandas.DataFrame\n#   Param&lt;dataframe2&gt;: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n<\/code><\/pre>\n\n<p>Then you can see the result from logs as below, first print the old version <code>0.14.0<\/code>, then print the new version <code>0.19.0<\/code> from the uploaded zip file.<\/p>\n\n<pre><code>[Information]         0.14.0\n[Information]         0.19.0\n<\/code><\/pre>\n\n<p>You could also refer to these threads: <a href=\"https:\/\/stackoverflow.com\/questions\/45749479\/access-blob-file-using-time-stamp-in-azure\/45814318#45814318\">Access blob file using time stamp in Azure<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/12669546\/reload-with-reset\">reload with reset<\/a>.<\/p>\n\n<p>Hope it helps you.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"involv creat independ runtim environ virtualenv compon instal panda environ compress file lib site packag folder zip packag upload zip packag workspac dataset remov panda modul depend import panda log version version upload zip file",
        "Solution_last_edit_time":1505805193896,
        "Solution_link_count":15.0,
        "Solution_original_content":"step updat version panda librari execut step virtualenv compon creat independ runtim environ instal pip instal virtualenv instal successfulli file step run commad creat independ runtim environ step creat directori folder activ step import miss close window pip instal panda download extern librari window step compress file lib site packag folder zip packag call panda packag step upload zip packag workspac dataset step technic note upload packag dataset list step defin execut modul remov panda modul depend import panda import sy import panda print version del sy modul panda del sy modul numpi del sy modul pytz del sy modul del sy modul dateutil sy path insert bundl sy modul startswith panda startswith numpi startswith pytz startswith dateutil startswith del sy modul import panda print version entri function input argument param panda datafram param panda datafram datafram datafram log print version print version upload zip file thread access blob file time stamp reload reset hope",
        "Solution_preprocessed_content":"step updat version panda librari step compon creat independ runtim environ instal instal successfulli file step run commad creat independ runtim environ step creat directori folder activ close window download extern librari window step compress file folder zip packag step upload zip packag workspac dataset step technic note upload packag dataset list step defin execut modul remov modul depend import log print version print version upload zip file thread access blob file time stamp reload reset hope",
        "Solution_readability":10.2,
        "Solution_reading_time":51.18,
        "Solution_score_count":6.0,
        "Solution_sentence_count":43.0,
        "Solution_word_count":375.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1553609947192,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":221.1468555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've created an SageMaker Endpoint from a trained DeepAR-Model using following code:<\/p>\n<pre><code>job_name = estimator.latest_training_job.job_name\n\nendpoint_name = sagemaker_session.endpoint_from_job(\n    job_name=job_name,\n    initial_instance_count=1,\n    instance_type=&quot;ml.m4.xlarge&quot;,\n    image_uri=image_uri,\n    role=role\n)\n<\/code><\/pre>\n<p>Now I want to test my model using a <code>test.json<\/code>-Dataset (<strong>66.2MB<\/strong>).\nI've created that file according to various tutorials\/sample-notebooks (same as <code>train.json<\/code>, but with <code>prediction-length<\/code>-less values.<\/p>\n<p>For that, I've written the following code:<\/p>\n<pre><code>class DeepARPredictor(sagemaker.predictor.Predictor):\n    def set_prediction_parameters(self, freq, prediction_length):\n        self.freq = freq\n        self.prediction_length = prediction_length\n\n    def predict(self, ts, num_samples=100, quantiles=[&quot;0.1&quot;, &quot;0.5&quot;, &quot;0.9&quot;]):\n        prediction_times = [x.index[-1] + pd.Timedelta(1, unit=self.freq) for x in ts]\n        req = self.__encode_request(ts, num_samples, quantiles)\n        res = super(DeepARPredictor, self).predict(req, initial_args={&quot;ContentType&quot;: &quot;application\/json&quot;})\n        return self.__decode_response(res, prediction_times)\n\n    def __encode_request(self, ts, num_samples, quantiles):\n        instances = [{&quot;start&quot;: str(ts[k].index[0]), &quot;target&quot;: list(ts[k])} for k in range(len(ts))]\n        configuration = {\n            &quot;num_samples&quot;: num_samples,\n            &quot;output_types&quot;: [&quot;quantiles&quot;],\n            &quot;quantiles&quot;: quantiles,\n        }\n        http_request_data = {&quot;instances&quot;: instances, &quot;configuration&quot;: configuration}\n        return json.dumps(http_request_data).encode( &quot;utf-8&quot;)\n\n    def __decode_response(self, response, prediction_times):\n        response_data = json.loads(response.decode(&quot;utf-8&quot;))\n        list_of_df = []\n        for k in range(len(prediction_times)):\n            prediction_index = pd.date_range(\n                start=prediction_times[k], freq=self.freq, periods=self.prediction_length\n            )\n            list_of_df.append(\n                pd.DataFrame(data=response_data[&quot;predictions&quot;][k][&quot;quantiles&quot;], index=prediction_index)\n            )\n        return list_of_df\n<\/code><\/pre>\n<p>But after running the following block:<\/p>\n<pre><code>predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\npredictor.set_prediction_parameters(freq, prediction_length)\nlist_of_df = predictor.predict(time_series_training)\n<\/code><\/pre>\n<p>I've getting a BrokenPipeError:<\/p>\n<pre><code>---------------------------------------------------------------------------\nBrokenPipeError                           Traceback (most recent call last)\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    676                 headers=headers,\n--&gt; 677                 chunked=chunked,\n    678             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n    391         else:\n--&gt; 392             conn.request(method, url, **httplib_request_kw)\n    393 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\n   1261         &quot;&quot;&quot;Send a complete request to the server.&quot;&quot;&quot;\n-&gt; 1262         self._send_request(method, url, body, headers, encode_chunked)\n   1263 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_request(self, method, url, body, headers, *args, **kwargs)\n     92         rval = super(AWSConnection, self)._send_request(\n---&gt; 93             method, url, body, headers, *args, **kwargs)\n     94         self._expect_header_set = False\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\n   1307             body = _encode(body, 'body')\n-&gt; 1308         self.endheaders(body, encode_chunked=encode_chunked)\n   1309 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\n   1256             raise CannotSendHeader()\n-&gt; 1257         self._send_output(message_body, encode_chunked=encode_chunked)\n   1258 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_output(self, message_body, *args, **kwargs)\n    119             message_body = None\n--&gt; 120         self.send(msg)\n    121         if self._expect_header_set:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in send(self, str)\n    203             return\n--&gt; 204         return super(AWSConnection, self).send(str)\n    205 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in send(self, data)\n    995         try:\n--&gt; 996             self.sock.sendall(data)\n    997         except TypeError:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in sendall(self, data, flags)\n    974                 while count &lt; amount:\n--&gt; 975                     v = self.send(byte_view[count:])\n    976                     count += v\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in send(self, data, flags)\n    943                     self.__class__)\n--&gt; 944             return self._sslobj.write(data)\n    945         else:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in write(self, data)\n    641         &quot;&quot;&quot;\n--&gt; 642         return self._sslobj.write(data)\n    643 \n\nBrokenPipeError: [Errno 32] Broken pipe\n\nDuring handling of the above exception, another exception occurred:\n\nProtocolError                             Traceback (most recent call last)\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/httpsession.py in send(self, request)\n    319                 decode_content=False,\n--&gt; 320                 chunked=self._chunked(request.headers),\n    321             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    726             retries = retries.increment(\n--&gt; 727                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n    728             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/util\/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)\n    378             # Disabled, indicate to re-raise the error.\n--&gt; 379             raise six.reraise(type(error), error, _stacktrace)\n    380 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/packages\/six.py in reraise(tp, value, tb)\n    733             if value.__traceback__ is not tb:\n--&gt; 734                 raise value.with_traceback(tb)\n    735             raise value\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    676                 headers=headers,\n--&gt; 677                 chunked=chunked,\n    678             )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/urllib3\/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n    391         else:\n--&gt; 392             conn.request(method, url, **httplib_request_kw)\n    393 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\n   1261         &quot;&quot;&quot;Send a complete request to the server.&quot;&quot;&quot;\n-&gt; 1262         self._send_request(method, url, body, headers, encode_chunked)\n   1263 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_request(self, method, url, body, headers, *args, **kwargs)\n     92         rval = super(AWSConnection, self)._send_request(\n---&gt; 93             method, url, body, headers, *args, **kwargs)\n     94         self._expect_header_set = False\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\n   1307             body = _encode(body, 'body')\n-&gt; 1308         self.endheaders(body, encode_chunked=encode_chunked)\n   1309 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\n   1256             raise CannotSendHeader()\n-&gt; 1257         self._send_output(message_body, encode_chunked=encode_chunked)\n   1258 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in _send_output(self, message_body, *args, **kwargs)\n    119             message_body = None\n--&gt; 120         self.send(msg)\n    121         if self._expect_header_set:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/awsrequest.py in send(self, str)\n    203             return\n--&gt; 204         return super(AWSConnection, self).send(str)\n    205 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/http\/client.py in send(self, data)\n    995         try:\n--&gt; 996             self.sock.sendall(data)\n    997         except TypeError:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in sendall(self, data, flags)\n    974                 while count &lt; amount:\n--&gt; 975                     v = self.send(byte_view[count:])\n    976                     count += v\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in send(self, data, flags)\n    943                     self.__class__)\n--&gt; 944             return self._sslobj.write(data)\n    945         else:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/ssl.py in write(self, data)\n    641         &quot;&quot;&quot;\n--&gt; 642         return self._sslobj.write(data)\n    643 \n\nProtocolError: ('Connection aborted.', BrokenPipeError(32, 'Broken pipe'))\n\nDuring handling of the above exception, another exception occurred:\n\nConnectionClosedError                     Traceback (most recent call last)\n&lt;ipython-input-14-95dda20e8a70&gt; in &lt;module&gt;\n      1 predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n      2 predictor.set_prediction_parameters(freq, prediction_length)\n----&gt; 3 list_of_df = predictor.predict(time_series_training)\n\n&lt;ipython-input-13-a0fbac2b9b07&gt; in predict(self, ts, num_samples, quantiles)\n      7         prediction_times = [x.index[-1] + pd.Timedelta(1, unit=self.freq) for x in ts]\n      8         req = self.__encode_request(ts, num_samples, quantiles)\n----&gt; 9         res = super(DeepARPredictor, self).predict(req, initial_args={&quot;ContentType&quot;: &quot;application\/json&quot;})\n     10         return self.__decode_response(res, prediction_times)\n     11 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant)\n    123 \n    124         request_args = self._create_request_args(data, initial_args, target_model, target_variant)\n--&gt; 125         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n    126         return self._handle_response(response)\n    127 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    355                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    356             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 357             return self._make_api_call(operation_name, kwargs)\n    358 \n    359         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    661         else:\n    662             http, parsed_response = self._make_request(\n--&gt; 663                 operation_model, request_dict, request_context)\n    664 \n    665         self.meta.events.emit(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_request(self, operation_model, request_dict, request_context)\n    680     def _make_request(self, operation_model, request_dict, request_context):\n    681         try:\n--&gt; 682             return self._endpoint.make_request(operation_model, request_dict)\n    683         except Exception as e:\n    684             self.meta.events.emit(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in make_request(self, operation_model, request_dict)\n    100         logger.debug(&quot;Making request for %s with params: %s&quot;,\n    101                      operation_model, request_dict)\n--&gt; 102         return self._send_request(request_dict, operation_model)\n    103 \n    104     def create_request(self, params, operation_model=None):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _send_request(self, request_dict, operation_model)\n    135             request, operation_model, context)\n    136         while self._needs_retry(attempts, operation_model, request_dict,\n--&gt; 137                                 success_response, exception):\n    138             attempts += 1\n    139             # If there is a stream associated with the request, we need\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _needs_retry(self, attempts, operation_model, request_dict, response, caught_exception)\n    254             event_name, response=response, endpoint=self,\n    255             operation=operation_model, attempts=attempts,\n--&gt; 256             caught_exception=caught_exception, request_dict=request_dict)\n    257         handler_response = first_non_none_response(responses)\n    258         if handler_response is None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in emit(self, event_name, **kwargs)\n    354     def emit(self, event_name, **kwargs):\n    355         aliased_event_name = self._alias_event_name(event_name)\n--&gt; 356         return self._emitter.emit(aliased_event_name, **kwargs)\n    357 \n    358     def emit_until_response(self, event_name, **kwargs):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in emit(self, event_name, **kwargs)\n    226                  handlers.\n    227         &quot;&quot;&quot;\n--&gt; 228         return self._emit(event_name, kwargs)\n    229 \n    230     def emit_until_response(self, event_name, **kwargs):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in _emit(self, event_name, kwargs, stop_on_response)\n    209         for handler in handlers_to_call:\n    210             logger.debug('Event %s: calling handler %s', event_name, handler)\n--&gt; 211             response = handler(**kwargs)\n    212             responses.append((handler, response))\n    213             if stop_on_response and response is not None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempts, response, caught_exception, **kwargs)\n    181 \n    182         &quot;&quot;&quot;\n--&gt; 183         if self._checker(attempts, response, caught_exception):\n    184             result = self._action(attempts=attempts)\n    185             logger.debug(&quot;Retry needed, action of: %s&quot;, result)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    249     def __call__(self, attempt_number, response, caught_exception):\n    250         should_retry = self._should_retry(attempt_number, response,\n--&gt; 251                                           caught_exception)\n    252         if should_retry:\n    253             if attempt_number &gt;= self._max_attempts:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in _should_retry(self, attempt_number, response, caught_exception)\n    275             # If we've exceeded the max attempts we just let the exception\n    276             # propogate if one has occurred.\n--&gt; 277             return self._checker(attempt_number, response, caught_exception)\n    278 \n    279 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    315         for checker in self._checkers:\n    316             checker_response = checker(attempt_number, response,\n--&gt; 317                                        caught_exception)\n    318             if checker_response:\n    319                 return checker_response\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    221         elif caught_exception is not None:\n    222             return self._check_caught_exception(\n--&gt; 223                 attempt_number, caught_exception)\n    224         else:\n    225             raise ValueError(&quot;Both response and caught_exception are None.&quot;)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in _check_caught_exception(self, attempt_number, caught_exception)\n    357         # the MaxAttemptsDecorator is not interested in retrying the exception\n    358         # then this exception just propogates out past the retry code.\n--&gt; 359         raise caught_exception\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _do_get_response(self, request, operation_model)\n    198             http_response = first_non_none_response(responses)\n    199             if http_response is None:\n--&gt; 200                 http_response = self._send(request)\n    201         except HTTPClientError as e:\n    202             return (None, e)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _send(self, request)\n    267 \n    268     def _send(self, request):\n--&gt; 269         return self.http_session.send(request)\n    270 \n    271 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/httpsession.py in send(self, request)\n    349                 error=e,\n    350                 request=request,\n--&gt; 351                 endpoint_url=request.url\n    352             )\n    353         except Exception as e:\n\nConnectionClosedError: Connection was closed before we received a valid response from endpoint URL\n<\/code><\/pre>\n\n<p>Somebody know's why this happens?<\/p>",
        "Challenge_closed_time":1616165232248,
        "Challenge_comment_count":1,
        "Challenge_created_time":1615369010763,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1615369103568,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66561959",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":16.1,
        "Challenge_reading_time":216.74,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":142,
        "Challenge_solved_time":221.1726347222,
        "Challenge_title":"Sagemaker Endpoint BrokenPipeError at DeepAR Prediction",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":244,
        "Challenge_word_count":1241,
        "Platform":"Stack Overflow",
        "Poster_created_time":1607069622448,
        "Poster_location":null,
        "Poster_reputation_count":101.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>I believe that Tarun might on the right path. The BrokenPipeError that you got is thrown when the connection is abruptly closed. See <a href=\"https:\/\/docs.python.org\/3\/library\/exceptions.html#BrokenPipeError\" rel=\"nofollow noreferrer\">the python docs for BrokenPipeError<\/a>.\nThe SageMaker endpoint probably drops the connection as soon as you go over the limit of 5MB. I suggest you try a smaller dataset. Also the data you send might get enlarged because of how sagemaker.tensorflow.model.TensorFlowPredictor encodes the data according to <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/799#issuecomment-492698717\" rel=\"nofollow noreferrer\">this comment<\/a> on a similar issue.<\/p>\n<p>If that doesn't work I've also seen a couple of people having problems with their networks in general. Specifically firewall\/antivirus (<a href=\"https:\/\/github.com\/aws\/aws-cli\/issues\/3999#issuecomment-531151161\" rel=\"nofollow noreferrer\">for example this comment<\/a>) or network timeout.<\/p>\n<p>Hope this points you in the right direction.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"smaller dataset endpoint drop connect soon limit exceed data sent enlarg tensorflow model tensorflowpredictor encod data network firewal antiviru network timeout",
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"believ tarun path brokenpipeerror thrown connect abruptli close doc brokenpipeerror endpoint probabl drop connect soon limit smaller dataset data send enlarg tensorflow model tensorflowpredictor encod data accord comment coupl peopl network gener firewal antiviru comment network timeout hope direct",
        "Solution_preprocessed_content":"believ tarun path brokenpipeerror thrown connect abruptli close doc brokenpipeerror endpoint probabl drop connect soon limit smaller dataset data send enlarg encod data accord comment coupl peopl network gener network timeout hope direct",
        "Solution_readability":10.4,
        "Solution_reading_time":13.62,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":119.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":14.2479583334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I developed a designer to implement regression models in azure machine learning studio. I have taken the data set pill and then split the data set into train and test in prescribed manner. When I am trying to implement the evaluation metrics and run the pipeline, it was showing a warning and error in the moment I called the dataset for the operation. I am bit confused, with the same implementation, when i tried to run with linear regression and it worked as shown in the image. If the same approach is used to implement logistic regression it was showing some warning and error in building the evaluation metrics.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DbEeq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DbEeq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the above success is in linear regression. When it comes to logistic regression it was showing the warning and error in pipeline.<\/p>\n<p>Any help is appreciated.<\/p>",
        "Challenge_closed_time":1664021303563,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663970010913,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73833320",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":13.02,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":14.2479583334,
        "Challenge_title":"parameters error in azure ML designer in evaluation metrics in regression model",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":29,
        "Challenge_word_count":156,
        "Platform":"Stack Overflow",
        "Poster_created_time":1652172570283,
        "Poster_location":null,
        "Poster_reputation_count":19.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Creating a sample pipeline with designer with mathematical format.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kCx3A.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kCx3A.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>We need to create a compute instance.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/7bQA5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7bQA5.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NQWZV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NQWZV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/MpxPY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/MpxPY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bAfEv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bAfEv.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Assign the compute instance and click on create<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wIS0d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wIS0d.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Now the import data warning will be removed. In the same manner, we will be getting similar error in other pills too.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/zFK74.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zFK74.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yk5gY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yk5gY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Create a mathematical format. If not needed for your case, try to remove that math operation and give the remaining.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/uhKlv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uhKlv.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Assign the column set. Select any option according to the requirement.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mcNZe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mcNZe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Finally, we can find the pills which have no warning or error.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":20.0,
        "Solution_original_content":"creat sampl pipelin design mathemat format creat comput instanc assign comput instanc click creat import data warn remov pill creat mathemat format remov math oper remain assign column set select option accord final pill warn",
        "Solution_preprocessed_content":"creat sampl pipelin design mathemat format creat comput instanc assign comput instanc click creat import data warn remov pill creat mathemat format remov math oper remain assign column set select option accord final pill warn",
        "Solution_readability":14.1,
        "Solution_reading_time":30.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":189.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1383025927423,
        "Answerer_location":"Dhaka, Bangladesh",
        "Answerer_reputation_count":647.0,
        "Answerer_view_count":105.0,
        "Challenge_adjusted_solved_time":1006.4421325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to Sagmaker. I am creating a pipeline in sagemaker where I initialize the number of epochs as a pipeline parameter. But when I upsert, it shows this error.\nCheck the following code for reference, please.<\/p>\n<pre><code>epoch_count = ParameterInteger(name=&quot;EpochCount&quot;, default_value=1)\npipeline = Pipeline(\nname=f&quot;a_name&quot;,\nparameters=[\n    training_instance_type,\n    training_instance_count,\n    epoch_count,\n    hugging_face_model_name,\n    endpoint_instance_type,\n    endpoint_instance_type_alternate,\n],\nsteps=[step_train, step_register, step_deploy_lambda],\nsagemaker_session=sagemaker_session,\n<\/code><\/pre>\n<p>)<\/p>\n<p>Error - ---<\/p>\n<pre><code>---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\n&lt;ipython-input-54-138a517611f0&gt; in &lt;module&gt;\n----&gt; 1 pipeline.upsert(role_arn=role)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in upsert(self, role_arn, description, tags, parallelism_config)\n    217         &quot;&quot;&quot;\n    218         try:\n--&gt; 219             response = self.create(role_arn, description, tags, parallelism_config)\n    220         except ClientError as e:\n    221             error = e.response[&quot;Error&quot;]\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in create(self, role_arn, description, tags, parallelism_config)\n    119             Tags=tags,\n    120         )\n--&gt; 121         return self.sagemaker_session.sagemaker_client.create_pipeline(**kwargs)\n    122 \n    123     def _create_args(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    389                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    390             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 391             return self._make_api_call(operation_name, kwargs)\n    392 \n    393         _api_call.__name__ = str(py_operation_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    717             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)\n    718             error_class = self.exceptions.from_code(error_code)\n--&gt; 719             raise error_class(parsed_response, operation_name)\n    720         else:\n    721             return parsed_response\n\nClientError: An error occurred (ValidationException) when calling the CreatePipeline operation: Cannot assign property reference [Parameters.EpochCount] to argument of type [String]\n<\/code><\/pre>",
        "Challenge_closed_time":1645641627683,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645534662633,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71221741",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":16.7,
        "Challenge_reading_time":32.46,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":29.7125138889,
        "Challenge_title":"ValidationException in Sagemaker pipeline creation",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":608,
        "Challenge_word_count":195,
        "Platform":"Stack Overflow",
        "Poster_created_time":1383025927423,
        "Poster_location":"Dhaka, Bangladesh",
        "Poster_reputation_count":647.0,
        "Poster_view_count":105.0,
        "Solution_body":"<p>I replace<\/p>\n<pre><code>epoch_count = ParameterInteger(name=&quot;EpochCount&quot;, default_value=1)\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>epoch_count = ParameterString(name=&quot;EpochCount&quot;, default_value=&quot;1&quot;)\n<\/code><\/pre>\n<p>And it works. Maybe we can only use an integer in pipeline parameters from the sagemaker notebook. But epoch_count is being used in the docker container, which is not directly something of Sagemaker, and that's my understanding.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"replac parameterinteg parameterstr set default valu validationexcept",
        "Solution_last_edit_time":1649157854310,
        "Solution_link_count":0.0,
        "Solution_original_content":"replac epoch count parameterinteg epochcount default valu epoch count parameterstr epochcount default valu mayb integ pipelin paramet notebook epoch count docker directli",
        "Solution_preprocessed_content":"replac mayb integ pipelin paramet notebook docker directli",
        "Solution_readability":16.6,
        "Solution_reading_time":6.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":48.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1460437080990,
        "Answerer_location":null,
        "Answerer_reputation_count":386.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":1.4118238889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How do you rename an Azure ML Experiment?\u00a0I cannot see any property field you can set. All I can find is Save\u00a0As something else, then delete the\u00a0original\u00a0experiment. <\/p>\n\n<p>When I\u00a0Save for the first time, it doesn't ask me for a name, it just saves it with a standard date. <\/p>\n\n<p>Am I missing something simple and obvious? <\/p>",
        "Challenge_closed_time":1485656495463,
        "Challenge_comment_count":0,
        "Challenge_created_time":1485651412897,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41916570",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":5.3,
        "Challenge_reading_time":4.36,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.4118238889,
        "Challenge_title":"Rename an Azure ML experiment",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":753,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1407201889907,
        "Poster_location":"Redmond, WA",
        "Poster_reputation_count":2674.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>put the cursor on the name text when an experiment is open, and edit away.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"cursor text open edit",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"cursor text open edit awai",
        "Solution_preprocessed_content":"cursor text open edit awai",
        "Solution_readability":5.6,
        "Solution_reading_time":0.98,
        "Solution_score_count":3.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":15.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":6.0155075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In my current job we use AWS managed notebooks on Sagemaker EC2. I am largely okay with the user experience but the lack of data persistency outside <code>~\/Sagemaker<\/code> has been quite inconvenient. Every time should the instance need restarting, I'd lose all the settings and python packages. Wonder why AWS would make this particular decision for Sagemaker. Have used Google Cloud's AI platform before and it does not have such settings and my configurations would always persist.<\/p>",
        "Challenge_closed_time":1625487892607,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625466236780,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68251533",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":7.24,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":6.0155075,
        "Challenge_title":"Why is AWS Sagemaker notebook instance designed to only persist data under ~\/Sagemaker?",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":266,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408744280996,
        "Poster_location":"Vancouver, BC, Canada",
        "Poster_reputation_count":2758.0,
        "Poster_view_count":122.0,
        "Solution_body":"<p>I faced a similar issue on other AWS services. Usually for managed services AWS uses read-only containers approach and leave just one folder of the filesystem for read\/write that persist across the stop\/restart cycle.\nReguarding the packages installation the seems to be to install your custom environment on the notebook instance's Amazon EBS volume, as described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"read servic instal environ notebook instanc eb volum persist packag stop restart cycl",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"servic servic read leav folder filesystem read write persist stop restart cycl reguard packag instal instal environ notebook instanc eb volum",
        "Solution_preprocessed_content":"servic servic leav folder filesystem persist cycl reguard packag instal instal environ notebook instanc eb volum",
        "Solution_readability":13.9,
        "Solution_reading_time":6.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":60.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1526489986910,
        "Answerer_location":"Philadelphia, PA, USA",
        "Answerer_reputation_count":926.0,
        "Answerer_view_count":48.0,
        "Challenge_adjusted_solved_time":9.0913125,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm new to the aws how to set path of my bucket and access file of that bucket?<\/p>\n\n<p>Is there anything i need to change with prefix ?<\/p>\n\n<pre><code>import os\nimport boto3\nimport re\nimport copy\nimport time\nfrom time import gmtime, strftime\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\n\nregion = boto3.Session().region_name\n\nbucket='ltfs1' # Replace with your s3 bucket name\nprefix = 'sagemaker\/ltfs1' # Used as part of the path in the bucket where you store data\n# bucket_path = 'https:\/\/s3-{}.amazonaws.com\/{}'.format(region,bucket) # The URL to access the bucket\n<\/code><\/pre>\n\n<p>I'm using the above code but it's showing file not found error<\/p>",
        "Challenge_closed_time":1562166882852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562134154127,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56863907",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.23,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":9.0913125,
        "Challenge_title":"how to set path of bucket in amazonsagemaker jupyter notebook?",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":852,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559907487263,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>If the file you are accessing is in the root directory of your s3 bucket, you can access the file like this:<\/p>\n\n<pre><code>import pandas as pd\n\nbucket='ltfs1'\ndata_key = 'data.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ntraining_data = pd.read_csv(data_location)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"access file root directori bucket replac ltf bucket data csv file access",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"file access root directori bucket access file import panda bucket ltf data kei data csv data locat format bucket data kei train data read csv data locat",
        "Solution_preprocessed_content":"file access root directori bucket access file",
        "Solution_readability":10.2,
        "Solution_reading_time":3.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":35.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1538757797860,
        "Answerer_location":"Dallas, TX, USA",
        "Answerer_reputation_count":5671.0,
        "Answerer_view_count":629.0,
        "Challenge_adjusted_solved_time":4.7034825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm reading a file from my S3 bucket in a notebook in sagemaker studio (same account) using the following code:<\/p>\n<pre><code>dataset_path_in_h5=&quot;\/Mode1\/SingleFault\/SimulationCompleted\/IDV2\/Mode1_IDVInfo_2_100\/Run1\/processdata&quot;\ns3 = s3fs.S3FileSystem()\nh5_file = h5py.File(s3.open(s3url,'rb'), 'r')\ndata = h5_file.get(dataset_path_in_h5)\n<\/code><\/pre>\n<p>But I don't know what actually append behind the scene, does the whole h5 file is being transferred  ? that's seems unlikely as the code is executed quite fast while the whole file is 20GB. Or is just the dataset in dataset_path_in_h5 is transferred ?\nI suppose that if the whole file is transferred at each call it could cost me a lot.<\/p>",
        "Challenge_closed_time":1662041978820,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662025046283,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73567221",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":9.9,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":4.7034825,
        "Challenge_title":"reading hdf5 file from s3 to sagemaker, is the whole file transferred?",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":18,
        "Challenge_word_count":101,
        "Platform":"Stack Overflow",
        "Poster_created_time":1576136255052,
        "Poster_location":null,
        "Poster_reputation_count":795.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>When you open the file, a file object is created. It has a tiny memory footprint. The dataset values aren't read into memory until you access them.<\/p>\n<p>You are returning <code>data<\/code> as a NumPy array. That loads the entire dataset into memory. (NOTE: the <code>.get()<\/code> method you are using is deprecated. Current syntax is provided in the example.)<\/p>\n<p>As an alternative to returning an array, you can create a dataset object (which also has a small memory foorprint). When you do, the data is read into memory as you need it. Dataset objects behave like NumPy arrays. (Use of a dataset object vs NumPy array depends on downstream usage. Frequently you don't need an array, but sometimes they are required.) Also, if chunked I\/O was enabled when the dataset was created, datasets are read in chunks.<\/p>\n<p>Differences shown below. Note, I used Python's file context manager to open the file. It avoids problems if the file isn't closed properly (you forget or the program exits prematurely).<\/p>\n<pre><code>dataset_path_in_h5=&quot;\/Mode1\/SingleFault\/SimulationCompleted\/IDV2\/Mode1_IDVInfo_2_100\/Run1\/processdata&quot;\ns3 = s3fs.S3FileSystem()\nwith h5py.File(s3.open(s3url,'rb'), 'r') as h5_file:\n     # your way to get a numpy array -- .get() is depreciated:\n     data = h5_file.get(dataset_path_in_h5)\n     # this is the preferred syntax to return an array:\n     data_arr = h5_file[dataset_path_in_h5][()]\n     # this returns a h5py dataset object:\n     data_ds = h5_file[dataset_path_in_h5]  # deleted [()] \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"return numpi arrai creat dataset object small memori footprint read data memori dataset object numpi arrai depend downstream usag frequent arrai chunk enabl dataset creat dataset read chunk",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"open file file object creat tini memori footprint dataset valu aren read memori access return data numpi arrai load entir dataset memori note deprec syntax return arrai creat dataset object small memori foorprint data read memori dataset object behav numpi arrai dataset object numpi arrai depend downstream usag frequent arrai chunk enabl dataset creat dataset read chunk shown note file context open file avoid file isn close properli forget program exit prematur dataset path mode singlefault simulationcomplet idv mode idvinfo run processdata sf sfilesystem hpy file open surl file numpi arrai depreci data file dataset path prefer syntax return arrai data arr file dataset path return hpy dataset object data file dataset path delet",
        "Solution_preprocessed_content":"open file file object creat tini memori footprint dataset valu aren read memori access return numpi arrai load entir dataset memori return arrai creat dataset object data read memori dataset object behav numpi arrai chunk enabl dataset creat dataset read chunk shown note file context open file avoid file isn close properli",
        "Solution_readability":8.1,
        "Solution_reading_time":19.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":207.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1515266756243,
        "Answerer_location":"Tempe, AZ, USA",
        "Answerer_reputation_count":143.0,
        "Answerer_view_count":36.0,
        "Challenge_adjusted_solved_time":2.6999758333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm deploying a Keras model that is failing with the error below. The exception says that I can retrieve the logs by running \"print(service.get_logs())\", but that's giving me empty results. I am deploying the model from my AzureNotebook and I'm using the same \"service\" var to retrieve the logs. <\/p>\n\n<p>Also, how can i retrieve the logs from the container instance? I'm deploying to an AKS compute cluster I created. Sadly, the docs link in the exception also doesnt detail how to retrieve these logs.<\/p>\n\n<pre><code>More information can be found using '.get_logs()' Error: \n<\/code><\/pre>\n\n<pre><code>{   \"code\":\n\"KubernetesDeploymentFailed\",   \"statusCode\": 400,   \"message\":\n\"Kubernetes Deployment failed\",   \"details\": [\n    {\n      \"code\": \"CrashLoopBackOff\",\n      \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check\nthe logs for your container instance: my-model-service. From\nthe AML SDK, you can run print(service.get_logs()) if you have service\nobject to fetch the logs. \\nYou can also try to run image\nmlwks.azurecr.io\/azureml\/azureml_3c0c34b65cf18c8644e8d745943ab7d2:latest\nlocally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails\nfor more information.\"\n    }   ] }\n<\/code><\/pre>\n\n<p>UPDATE<\/p>\n\n<p>Here's my code to deploy the model:<\/p>\n\n<pre><code>environment = Environment('my-environment')\nenvironment.python.conda_dependencies = CondaDependencies.create(pip_packages=[\"azureml-defaults\",\"azureml-dataprep[pandas,fuse]\",\"tensorflow\", \"keras\", \"matplotlib\"])\nservice_name = 'my-model-service'\n\n# Remove any existing service under the same name.\ntry:\n    Webservice(ws, service_name).delete()\nexcept WebserviceException:\n    pass\n\ninference_config = InferenceConfig(entry_script='score.py', environment=environment)\ncomp = ComputeTarget(workspace=ws, name=\"ml-inference-dev\")\nservice = Model.deploy(workspace=ws,\n                       name=service_name,\n                       models=[model],\n                       inference_config=inference_config,\n                       deployment_target=comp \n                      )\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n\n<p>And my score.py<\/p>\n\n<pre><code>import joblib\nimport numpy as np\nimport os\n\nimport keras\n\nfrom keras.models import load_model\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\n\ndef init():\n    global model\n\n    model_path = Model.get_model_path('model.h5')\n    model = load_model(model_path)\n    model = keras.models.load_model(model_path)\n\n\n# The run() method is called each time a request is made to the scoring API.\n#\n# Shown here are the optional input_schema and output_schema decorators\n# from the inference-schema pip package. Using these decorators on your\n# run() method parses and validates the incoming payload against\n# the example input you provide here. This will also generate a Swagger\n# API document for your web service.\n@input_schema('data', NumpyParameterType(np.array([[0.1, 1.2, 2.3, 3.4, 4.5, 5.6, 6.7, 7.8, 8.9, 9.0]])))\n@output_schema(NumpyParameterType(np.array([4429.929236457418])))\ndef run(data):\n\n    return [123] #test\n<\/code><\/pre>\n\n<p><strong>Update 2:<\/strong><\/p>\n\n<p>Here is a screencap of the endpoint page. Is it normal for the CPU to be .1? Also, when i hit the swagger url in the browser, i get the error: \"No ready replicas for service doc-classify-env-service\"<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Uvrfx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Uvrfx.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Update 3<\/strong>\nAfter finally getting to the container logs, it turns out that it was choking  with this error on my score.py<\/p>\n\n<blockquote>\n  <p>ModuleNotFoundError: No module named 'inference_schema'<\/p>\n<\/blockquote>\n\n<p>I then ran a test that commented out the refs for \"input_schema\" and \"output_schema\" and also simplified my pip_packages and the REST endpoint come up! I was also able to get a prediction out of the model. <\/p>\n\n<pre><code>pip_packages=[\"azureml-defaults\",\"tensorflow\", \"keras\"])\n<\/code><\/pre>\n\n<p>So my question is, how should I have my pip_packages for the scoring file to utilize the inference_schema decorators? I'm assuming I need to include azureml-sdk[auotml] pip package, but when i do so, the image creation fails and I see several dependency conflicts.<\/p>",
        "Challenge_closed_time":1588964687420,
        "Challenge_comment_count":2,
        "Challenge_created_time":1588954967507,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1589409955880,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61683506",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":11.4,
        "Challenge_reading_time":56.26,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":42,
        "Challenge_solved_time":2.6999758333,
        "Challenge_title":"Azure ML: how to access logs of a failed Model deployment",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":1564,
        "Challenge_word_count":495,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330016065408,
        "Poster_location":null,
        "Poster_reputation_count":1704.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>Try retrieving your service from the workspace directly <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ws.webservices[service_name].get_logs()\n<\/code><\/pre>\n\n<p>Also, I found deploying an image as an endpoint to be easier than inference+deploy model (depending on your use case) <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>my_image = Image(ws, name='test', version='26')  \nservice = AksWebservice.deploy_from_image(ws, \"test1\", my_image, deployment_config, aks_target)\n<\/code><\/pre>",
        "Solution_comment_count":13.0,
        "Solution_gpt_summary":"retriev servic workspac directli webservic servic log print servic log deploi imag endpoint infer deploi model depend imag imag test version servic akswebservic deploi imag test imag deploy config ak target modulenotfounderror modul infer schema input schema output schema decor",
        "Solution_last_edit_time":1588970402070,
        "Solution_link_count":0.0,
        "Solution_original_content":"retriev servic workspac directli webservic servic log deploi imag endpoint easier infer deploi model depend imag imag test version servic akswebservic deploi imag test imag deploy config ak target",
        "Solution_preprocessed_content":"retriev servic workspac directli deploi imag endpoint easier infer deploi model",
        "Solution_readability":16.5,
        "Solution_reading_time":6.6,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":47.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1556182989007,
        "Answerer_location":null,
        "Answerer_reputation_count":181.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":162.2517566667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use Optuna for hyperparameter tuning of my model.<\/p>\n<p>I am stuck in a place where I want to define a search space having lognormal\/normal distribution. It is possible in <code>hyperopt<\/code> using <code>hp.lognormal<\/code>. Is it possible to define such a space using a combination of the existing <code>suggest_<\/code> api of <code>Optuna<\/code>?<\/p>",
        "Challenge_closed_time":1611382397500,
        "Challenge_comment_count":3,
        "Challenge_created_time":1610971789293,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1610981620636,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65774253",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":10.5,
        "Challenge_reading_time":5.44,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":114.0578352778,
        "Challenge_title":"Is there any equivalent of hyperopts lognormal in Optuna?",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":140,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1445250318392,
        "Poster_location":null,
        "Poster_reputation_count":2164.0,
        "Poster_view_count":298.0,
        "Solution_body":"<p>You could perhaps make use of inverse transforms from <code>suggest_float(..., 0, 1)<\/code> (i.e. U(0, 1)) since Optuna currently doesn't provide <code>suggest_<\/code> variants for those two distributions directly. This example might be a starting point <a href=\"https:\/\/gist.github.com\/hvy\/4ef02ee2945fe50718c71953e1d6381d\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/hvy\/4ef02ee2945fe50718c71953e1d6381d<\/a>\nPlease find the code below<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import erfcinv\n\nimport optuna\n\n\ndef objective(trial):\n    # Suggest from U(0, 1) with Optuna.\n    x = trial.suggest_float(&quot;x&quot;, 0, 1)\n\n    # Inverse transform into normal.\n    y0 = norm.ppf(x, loc=0, scale=1)\n\n    # Inverse transform into lognormal.\n    y1 = np.exp(-np.sqrt(2) * erfcinv(2 * x))\n\n    return y0, y1\n\n\nif __name__ == &quot;__main__&quot;:\n    n_objectives = 2  # Normal and lognormal.\n\n    study = optuna.create_study(\n        sampler=optuna.samplers.RandomSampler(),\n        # Could be &quot;maximize&quot;. Does not matter for this demonstration.\n        directions=[&quot;minimize&quot;] * n_objectives,\n    )\n    study.optimize(objective, n_trials=10000)\n\n    fig, axs = plt.subplots(n_objectives)\n    for i in range(n_objectives):\n        axs[i].hist(list(t.values[i] for t in study.trials), bins=100)\n    plt.show()\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"invers transform defin search space lognorm normal distribut variant distribut directli start",
        "Solution_last_edit_time":1611565726960,
        "Solution_link_count":2.0,
        "Solution_original_content":"invers transform variant distribut directli start http gist github com hvy efeefecedd import matplotlib pyplot plt import numpi scipi stat import norm scipi import erfcinv import object trial trial invers transform normal norm ppf loc scale invers transform lognorm exp sqrt erfcinv return object normal lognorm studi creat studi sampler sampler randomsampl maxim matter demonstr direct minim object studi optim object trial fig ax plt subplot object rang object ax hist list valu studi trial bin plt",
        "Solution_preprocessed_content":"invers transform variant distribut directli start",
        "Solution_readability":10.7,
        "Solution_reading_time":17.76,
        "Solution_score_count":1.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":133.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1430233500800,
        "Answerer_location":null,
        "Answerer_reputation_count":212.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":197.9953861111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Currently I am exploring AWS sagemaker and I am facing a problem e.g. If I want to train my network on 1000s of epochs I cant stay active all the time. But as I logout my the notebook instance also stop execution. Is there any way to keep the instance active even after you logout ? <\/p>",
        "Challenge_closed_time":1541803691470,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541090908080,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53105741",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":4.23,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":197.9953861111,
        "Challenge_title":"Amazon Sagemaker Notebook instance stop execution as I logout",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2598,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Poster_created_time":1492699347027,
        "Poster_location":"Germany",
        "Poster_reputation_count":37.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>Do you mean logging out of AWS console or your laptop? Your training job should still be running on the notebook instance whether you have notebook open or not. Notebook instance will always be active until you manually stop it[1]. You can always access the notebook instance again by opening the notebook through console.<\/p>\n\n<p>[1]<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_gpt_summary":"notebook instanc activ log consol access notebook instanc open notebook consol instanc activ log",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"log consol laptop train job run notebook instanc notebook open notebook instanc activ manual stop access notebook instanc open notebook consol http doc com latest api stopnotebookinst html",
        "Solution_preprocessed_content":"log consol laptop train job run notebook instanc notebook open notebook instanc activ manual stop access notebook instanc open notebook consol",
        "Solution_readability":11.8,
        "Solution_reading_time":7.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1444035983568,
        "Answerer_location":null,
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":2377.1102455556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>My code is in R. And I need to excess external database. I am storing database credentials in AWS Secret Manager.<\/p>\n<p>So I first tried using paws library to get aws secrets in R but that would require storing access key, secret id and session token, and I want to avoid that.<\/p>\n<p>Is there a better way to do this? I have created IAM role for Sagemaker. Is it possible to pass secrets as environment variables?<\/p>\n<p>Edit: I wanted to trigger Sagemaker Processing<\/p>",
        "Challenge_closed_time":1593501738047,
        "Challenge_comment_count":2,
        "Challenge_created_time":1591862670733,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1593501623132,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62319753",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":4,
        "Challenge_readability":6.4,
        "Challenge_reading_time":6.85,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":455.2964761111,
        "Challenge_title":"How to I pass secrets stored in AWS Secret Manager to a Docker container in Sagemaker?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":1063,
        "Challenge_word_count":98,
        "Platform":"Stack Overflow",
        "Poster_created_time":1444035983568,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>I found a simple solution to it. Env variables can be passed via Sagemaker sdk. It minimizes the dependencies.<\/p>\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html<\/a><\/p>\n<p>As another answer suggested, paws can be used as well to get secrets from aws. This would be a better approach<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"pass environ variabl sdk minim depend paw librari secret",
        "Solution_last_edit_time":1602059220016,
        "Solution_link_count":2.0,
        "Solution_original_content":"env variabl pass sdk minim depend http readthedoc stabl api train process html paw secret",
        "Solution_preprocessed_content":"env variabl pass sdk minim depend paw secret",
        "Solution_readability":12.8,
        "Solution_reading_time":5.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":15.3455552778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to find a way to build ML using AWS, preferably using their services such as SageMaker and not just EC2, for object detection in images using an image as input.<\/p>\n\n<p>AWS Rekognition offers Image Comparison and Object detection APIs, but they are not exactly what I'm looking for, the comparison works only with faces and not objects and object detection is too basic.<\/p>\n\n<p>AlibabCloud has that functionality as a service (<a href=\"https:\/\/www.alibabacloud.com\/product\/imagesearch\" rel=\"nofollow noreferrer\">https:\/\/www.alibabacloud.com\/product\/imagesearch<\/a>) but I would like to use something similar on AWS, rather than Alibaba.<\/p>\n\n<p>How would I go about and build something like this?<\/p>\n\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1583500304256,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583445060257,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1586898520163,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60554471",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":9.52,
        "Challenge_score_count":2,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":15.3455552778,
        "Challenge_title":"ML for object search",
        "Challenge_topic":"Database Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":86,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1365591820927,
        "Poster_location":null,
        "Poster_reputation_count":549.0,
        "Poster_view_count":89.0,
        "Solution_body":"<p><em>edited 03\/08\/2020 to add pointers for visual search<\/em><\/p>\n\n<p>Since you seem interested both in the tasks of <strong>object detection<\/strong> (input an image, and return bounding boxes with object classes) and <strong>visual search<\/strong> (input an image and return relevant images) let me give you pointers for both :)<\/p>\n\n<p>For <strong>object detection<\/strong> you have 3 options:<\/p>\n\n<ul>\n<li><strong>Using the managed service <a href=\"https:\/\/aws.amazon.com\/rekognition\/custom-labels-features\/?nc1=h_ls\" rel=\"nofollow noreferrer\">Amazon Rekognition Custom Labels<\/a><\/strong>. The key benefits of this service is that (1) it doesn't require writing ML code, as the service runs autoML internally to find the best model, (2) it is very flexible in terms of interaction (SDKs, console), data loading and annotation and (3) it can work even with small datasets (typically a few hundred images or less).<\/li>\n<li><strong>Using SageMaker Object Detection model<\/strong> (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection.html\" rel=\"nofollow noreferrer\">documentation<\/a>, <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_birds\/object_detection_birds.ipynb\" rel=\"nofollow noreferrer\">demo<\/a>). In this option, the model is also already written (SSD architecture with Resnet or VGG backbone) and you just need to choose  or tune <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection-api-config.html\" rel=\"nofollow noreferrer\">hyperparameters<\/a><\/li>\n<li><strong>Using your own model on Amazon SageMaker<\/strong>. This could be your own code in docker, or code from an ML framework in a SageMaker ML Framework container. There are such containers for <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html\" rel=\"nofollow noreferrer\">Pytorch<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html\" rel=\"nofollow noreferrer\">Tensorflow<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_mxnet.html\" rel=\"nofollow noreferrer\">MXNet<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_chainer.html\" rel=\"nofollow noreferrer\">Chainer<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html\" rel=\"nofollow noreferrer\">Sklearn<\/a>. In terms of model code, I recommend considering <a href=\"https:\/\/gluon-cv.mxnet.io\/\" rel=\"nofollow noreferrer\"><strong><code>gluoncv<\/code><\/strong><\/a>, a compact python computer vision toolkit (based on mxnet backend) that comes with <a href=\"https:\/\/gluon-cv.mxnet.io\/model_zoo\/detection.html\" rel=\"nofollow noreferrer\">many state-of-the-art models<\/a> and <a href=\"https:\/\/gluon-cv.mxnet.io\/build\/examples_detection\/index.html\" rel=\"nofollow noreferrer\">tutorials<\/a> for object detection<\/li>\n<\/ul>\n\n<p>The task of <strong>visual search<\/strong> requires more customization, since you need to provide the info of (1) what you define as search relevancy (eg is it visual similarity? or object complementarity? etc) and (2) the collection among which to search. If all you need is visual similarity, a popular option is to transform images into vectors with a pre-trained neural network and run kNN search between the query image and the collection of transformed images. There are 2 tutos showing how to build such systems on AWS here:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/visual-search-on-aws-part-1-engine-implementation-with-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">Blog Post Visual Search on AWS<\/a> (MXNet resnet embeddings +\nSageMaker kNN)<\/li>\n<li><a href=\"https:\/\/thomasdelteil.github.io\/VisualSearch_MXNet\/\" rel=\"nofollow noreferrer\">Visual Search on MMS demo<\/a> (MXNet resnet\nembeddings + HNSW kNN on AWS Fargate)<\/li>\n<\/ul>",
        "Solution_comment_count":5.0,
        "Solution_gpt_summary":"rekognit label servic run automl intern model object detect flexibl term interact data load annot small dataset object detect model pre written model ssd architectur resnet vgg backbon choos tune hyperparamet model docker framework framework visual search transform imag vector pre train neural network run knn search",
        "Solution_last_edit_time":1583703139203,
        "Solution_link_count":14.0,
        "Solution_original_content":"edit add pointer visual search task object detect input imag return bound box object class visual search input imag return relev imag pointer object detect option servic rekognit label kei benefit servic write servic run automl intern model flexibl term interact sdk consol data load annot small dataset typic imag object detect model document option model written ssd architectur resnet vgg backbon choos tune hyperparamet model docker framework framework pytorch tensorflow mxnet chainer sklearn term model gluoncv compact vision toolkit base mxnet backend come state art model tutori object detect task visual search defin search relev visual object complementar collect search visual popular option transform imag vector pre train neural network run knn search queri imag collect transform imag tuto build system blog visual search mxnet resnet embed knn visual search mm mxnet resnet embed hnsw knn fargat",
        "Solution_preprocessed_content":"edit add pointer visual search task object detect visual search pointer object detect option servic rekognit label kei benefit servic write servic run automl intern model flexibl term interact data load annot small dataset object detect model option model written choos tune hyperparamet model docker framework framework pytorch tensorflow mxnet chainer sklearn term model compact vision toolkit come model tutori object detect task visual search defin search relev collect search visual popular option transform imag vector neural network run knn search queri imag collect transform imag tuto build system blog visual search visual search mm",
        "Solution_readability":16.3,
        "Solution_reading_time":50.42,
        "Solution_score_count":2.0,
        "Solution_sentence_count":25.0,
        "Solution_word_count":377.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":148.3679175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have written a pipeline that I want to run on a remote compute cluster within Azure Machine Learning. My aim is to process a large amount of historical data, and to do this I will need to run the pipeline on a large number of input parameter combinations.<\/p>\n\n<p>Is there a way to restrict the number of nodes that the pipeline uses on the cluster? By default it will use all the nodes available to the cluster, and I would like to restrict it so that it only uses a pre-defined maximum. This allows me to leave the rest of the cluster free for other users.<\/p>\n\n<p>My current code to start the pipeline looks like this:<\/p>\n\n<pre><code># Setup the pipeline\nsteps = [data_import_step] # Contains PythonScriptStep\npipeline = Pipeline(workspace=ws, steps=steps)\npipeline.validate()\n\n# Big long list of historical dates that I want to process data for\ndts = pd.date_range('2019-01-01', '2020-01-01', freq='6H', closed='left')\n# Submit the pipeline job\nfor dt in dts:\n    pipeline_run = Experiment(ws, 'my-pipeline-run').submit(\n        pipeline,\n        pipeline_parameters={\n            'import_datetime': dt.strftime('%Y-%m-%dT%H:00'),\n        }\n    )\n<\/code><\/pre>",
        "Challenge_closed_time":1592597207007,
        "Challenge_comment_count":2,
        "Challenge_created_time":1592308824897,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62407943",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":8.9,
        "Challenge_reading_time":14.85,
        "Challenge_score_count":3,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":80.1061416667,
        "Challenge_title":"Restrict the number of nodes used by an Azure Machine Learning pipeine",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":274,
        "Challenge_word_count":174,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403698315160,
        "Poster_location":"London, UK",
        "Poster_reputation_count":1534.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>For me, the killer feature of Azure ML is not having to worry about load balancing like this. Our team has a compute target with <code>max_nodes=100<\/code> for every feature branch and we have <code>Hyperdrive<\/code> pipelines that result in 130 runs for each pipeline.<\/p>\n<p>We can submit multiple <code>PipelineRun<\/code>s back-to-back and the orchestrator does the heavy lifting of queuing, submitting, all the runs so that the <code>PipelineRun<\/code>s execute in the serial order I submitted them, and that the cluster is never overloaded. This works without issue for us 99% of the time.<\/p>\n<p>If what you're looking for is that you'd like the <code>PipelineRun<\/code>s to be executed in parallel, then you should check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-parallel-run-step#build-and-run-the-pipeline-containing-parallelrunstep\" rel=\"nofollow noreferrer\"><code>ParallelRunStep<\/code><\/a>.<\/p>\n<p>Another option is to isolate your computes. You can have up to 200 <code>ComputeTarget<\/code>s per workspace. Two 50-node <code>ComputeTarget<\/code>s cost the same as one 100-node <code>ComputeTarget<\/code>.<\/p>\n<p>On our team, we use <a href=\"https:\/\/www.pygit2.org\/\" rel=\"nofollow noreferrer\"><code>pygit2<\/code><\/a> to have a <code>ComputeTarget<\/code> created for each feature branch, so that, as data scientists, we can be confident that we're not stepping on our coworkers' toes.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_gpt_summary":"built load balanc featur submit multipl pipelinerun execut serial order cluster overload achiev set node paramet desir maximum node parallelrunstep parallel execut pipelinerun desir isol comput creat multipl computetarget pre defin maximum node pipelin node pre defin maximum leav rest cluster free pygit creat computetarget featur branch data scientist step",
        "Solution_last_edit_time":1592842949400,
        "Solution_link_count":2.0,
        "Solution_original_content":"killer featur worri load balanc team comput target node featur branch hyperdr pipelin run pipelin submit multipl pipelinerun orchestr heavi lift queu submit run pipelinerun execut serial order submit cluster overload time pipelinerun execut parallel parallelrunstep option isol comput computetarget workspac node computetarget cost node computetarget team pygit computetarget creat featur branch data scientist confid step cowork toe",
        "Solution_preprocessed_content":"killer featur worri load balanc team comput target featur branch pipelin run pipelin submit multipl orchestr heavi lift queu submit run execut serial order submit cluster overload time execut parallel option isol comput workspac cost team creat featur branch data scientist confid step cowork toe",
        "Solution_readability":11.9,
        "Solution_reading_time":18.57,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":177.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1553882107003,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":294.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":715.1033155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Somewhere this spring the behaviour of the sagemaker docker image changed and I cannot find the way I need to construct it now.<\/p>\n<p><strong>Directory structure<\/strong><\/p>\n<pre><code>\/src\/some\/package\n\/project1\n    \/some_entrypoint.py\n    \/some_notebook.ipynb\n\/project2\n    \/another_entrypoint.py\n    \/another_notebook.ipynb\nsetup.py\n<\/code><\/pre>\n<p><strong>Docker file<\/strong><\/p>\n<p>Note that I want to shift tensorflow version, so I changed the <code>FROM<\/code> to the latest version. This was the\nbreaking change.<\/p>\n<pre><code># Core\nFROM 763104351884.dkr.ecr.eu-west-1.amazonaws.com\/tensorflow-training:2.3.0-cpu-py37-ubuntu18.04\n\nCOPY . \/opt\/ml\/code\/all\/\nRUN pip install \/opt\/ml\/code\/all\/\n\nWORKDIR &quot;\/opt\/ml\/code&quot;\n<\/code><\/pre>\n<p><strong>Python code<\/strong><\/p>\n<p>This code should start the entrypoints, for example here we have the code of some_notebook.ipynb. I tried all possible combinations of working directory + source_dir (None, '.', or '..'), entry_point (with or without \/), dependencies ('src')...<\/p>\n<ul>\n<li>if setup is present it tries to call my project as a module (python -m some_entrypoint)<\/li>\n<li>if not, it often is not able to find my entrypoint. Which I don't understand because the TensorFlow is supposed to add it to the container, isn't it?<\/li>\n<\/ul>\n<pre><code>estimator = TensorFlow(\n   entry_point='some_entrypoint.py', \n   image_name='ECR.dkr.ecr.eu-west-1.amazonaws.com\/overall-project\/sagemaker-training:latest',\n   source_dir='.',\n#    dependencies=['..\/src\/'],\n   script_mode=True,\n\n   train_instance_type='ml.m5.4xlarge',\n   train_instance_count=1,\n   train_max_run=60*60,  # seconds * minutes\n   train_max_wait=60*60,  # seconds * minutes. Must be &gt;= train_max_run\n   hyperparameters=hyperparameters,\n   metric_definitions=metrics,\n   role=role,\n   framework_version='2.0.0',\n   py_version='py3',\n  )\nestimator.fit({\n    'training': f&quot;s3:\/\/some-data\/&quot;}\n#   , wait=False\n)\n<\/code><\/pre>\n<p>Ideally I would want to understand the logic within: what is called given what settings?<\/p>",
        "Challenge_closed_time":1600272249596,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597694559337,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1597697877660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63457857",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":12.4,
        "Challenge_reading_time":26.99,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":716.0250719444,
        "Challenge_title":"What is called within a sagemaker custom (training) container?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":61,
        "Challenge_word_count":203,
        "Platform":"Stack Overflow",
        "Poster_created_time":1484838464572,
        "Poster_location":"Amsterdam, Nederland",
        "Poster_reputation_count":3937.0,
        "Poster_view_count":387.0,
        "Solution_body":"<p>when the training container runs, your entry_point script will be executed.<\/p>\n<p>Since your notebook file and entry_point script are under the same directory, your <code>source_dir<\/code> should just be &quot;.&quot;<\/p>\n<p>Does your entry_point script import any modules that are not installed by the tensorflow training container by default? Also could you share your stacktrace of the error?<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"set sourc dir notebook file entri directori entri import modul instal tensorflow train default share stacktrac bias respons",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"train run entri execut notebook file entri directori sourc dir entri import modul instal tensorflow train default share stacktrac",
        "Solution_preprocessed_content":"train run execut notebook file directori import modul instal tensorflow train default share stacktrac",
        "Solution_readability":7.6,
        "Solution_reading_time":5.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1476711295896,
        "Answerer_location":"Malabon, Metro Manila, Philippines",
        "Answerer_reputation_count":3520.0,
        "Answerer_view_count":962.0,
        "Challenge_adjusted_solved_time":21.1495677778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am unclear abut Vertex AI pricing for model predictions. In the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/pricing\" rel=\"nofollow noreferrer\">documentation<\/a>, under the heading <strong>More about automatic scaling of prediction nodes<\/strong> one of the points mentioned is:<\/p>\n<blockquote>\n<p>&quot;If you choose automatic scaling, the number of nodes scales\nautomatically, and can scale down to zero for no-traffic durations&quot;<\/p>\n<\/blockquote>\n<p>The example provided in the documentation later also seems to suggest that during a period with no traffic, zero nodes are in use. However, when I create an Endpoint in Vertex AI, under the <strong>Autoscaling<\/strong> heading it says:<\/p>\n<blockquote>\n<p><em>&quot;Autoscaling: If you set a minimum and maximum, compute nodes will scale to meet traffic demand within those boundaries&quot;<\/em><\/p>\n<\/blockquote>\n<p>The  value of 0 under <em>&quot;Minimum number of compute nodes&quot;<\/em> is not allowed so you have to enter 1 or greater, and it is mentioned that:<\/p>\n<blockquote>\n<p>Default is 1. If set to 1 or more, then compute resources will\ncontinuously run even without traffic demand. This can increase cost\nbut avoid dropped requests due to node initialization.<\/p>\n<\/blockquote>\n<p>My question is, what happens when I select autoscaling by setting Minimum to 1 and Maximum to, say, 10. Does 1 node always run continuously? Or does it scale down to 0 nodes in no traffic condition as the documentation suggests.<\/p>\n<p>To test I deployed an Endpoint with Autoscaling (min and max set to 1) and then when I sent a prediction request the response was almost immediate, suggesting the node was already up. I did that again after about an hour and again the response was immediate suggesting that the node never shut down probably. Also, for high latency requirements, is having autoscale to 0 nodes, if that is indeed possible, even practical, i.e., what latency can we expect for starting up from 0 nodes?<\/p>",
        "Challenge_closed_time":1636603298292,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636487376953,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1636603578603,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69904211",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":11.8,
        "Challenge_reading_time":25.55,
        "Challenge_score_count":4,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":32.2003719444,
        "Challenge_title":"Vertex AI prediction - Autoscaling cannot set minimum node to 0",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":894,
        "Challenge_word_count":307,
        "Platform":"Stack Overflow",
        "Poster_created_time":1471292986790,
        "Poster_location":null,
        "Poster_reputation_count":700.0,
        "Poster_view_count":90.0,
        "Solution_body":"<p>Are you using an N1 or a non-N1 machine type? If you want to autoscale to zero, you must use non-N1 machines. See <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/overview#node_allocation_for_online_prediction\" rel=\"nofollow noreferrer\">second note<\/a> from node allocation:<\/p>\n<blockquote>\n<p>Note: Versions that use a Compute Engine (N1) machine type cannot scale down to zero nodes. They can scale down to 1 node, at minimum.<\/p>\n<\/blockquote>\n<p><em>Update<\/em>: AI Platform supports scaling to zero, while Vertex AI currently does not. From the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#scaling\" rel=\"nofollow noreferrer\">scaling<\/a> documentation, nodes can scale but there is no mention that it can scale down to zero. Here's a public <a href=\"https:\/\/issuetracker.google.com\/206042974\" rel=\"nofollow noreferrer\">feature request<\/a> for people who wants to track this issue.<\/p>\n<p>With regards to latency requirements, the actual output will vary. However, one thing to note according to the documentation is that the service may not be able to bring nodes online fast enough to keep up with large spikes of request traffic. If your traffic regularly has steep spikes, and if reliably low latency is important to your application, you may want to consider manual scaling.<\/p>\n<p>Additional Reference: <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/machine-types-online-prediction#automatic_scaling\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/machine-types-online-prediction#automatic_scaling<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_gpt_summary":"autoscal platform scale featur request scale servic node onlin fast larg spike request traffic reliabl low latenc import manual scale option",
        "Solution_last_edit_time":1636679717047,
        "Solution_link_count":5.0,
        "Solution_original_content":"type autoscal note node alloc note version comput engin type scale node scale node minimum updat platform scale scale document node scale scale public featur request peopl track latenc output vari note accord document servic node onlin fast larg spike request traffic traffic regularli steep spike reliabl low latenc import manual scale addit http cloud com platform predict doc type onlin predict automat scale",
        "Solution_preprocessed_content":"type autoscal note node alloc note version comput engin type scale node scale node minimum updat platform scale scale document node scale scale public featur request peopl track latenc output vari note accord document servic node onlin fast larg spike request traffic traffic regularli steep spike reliabl low latenc import manual scale addit",
        "Solution_readability":12.7,
        "Solution_reading_time":21.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":181.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1582182357612,
        "Answerer_location":null,
        "Answerer_reputation_count":155.0,
        "Answerer_view_count":27.0,
        "Challenge_adjusted_solved_time":392.2279305556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an ML model (trained locally) in python. Previously the model has been deployed to a Windows IIS server and it's working fine.<\/p>\n<p>Now, I am trying to deploy it as a service on Azure container instance (ACI) with 1 core, and 1 GB of memory. I took references from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python\" rel=\"nofollow noreferrer\">one<\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-existing-model\" rel=\"nofollow noreferrer\">two<\/a> Microsoft docs. The docs use SDK for all the steps, but <strong>I am using the GUI feature from the Azure portal<\/strong>.<\/p>\n<p>After registering the model, I created an entry script and a conda environment YAML file (see below), and uploaded both to &quot;Custom deployment asset&quot; (at Deploy model area).<\/p>\n<p>Unfortunately, after hitting deploy, the Deployment state is stuck at Transitioning state. Even after 4 hours, the state remains the same and there were no Deployment logs too, so I am unable to find what I am doing wrong here.<\/p>\n<blockquote>\n<p>NOTE: below is just an excerpt of the entry script<\/p>\n<\/blockquote>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport pickle\nimport re, json\nimport numpy as np\nimport sklearn\n\ndef init():\n    global model \n    global classes\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'randomForest50.pkl')\n    model = pickle.load(open(model_path, &quot;rb&quot;))\n\n    classes = lambda x : [&quot;F&quot;, &quot;M&quot;][x]\n\ndef run(data):\n    try:\n        namesList = json.loads(data)[&quot;data&quot;][&quot;names&quot;]\n        pred = list(map(classes, model.predict(preprocessing(namesList))))\n        return str(pred[0])\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: gender_prediction\ndependencies:\n- python\n- numpy\n- scikit-learn\n- pip:\n    - pandas\n    - pickle\n    - re\n    - json\n<\/code><\/pre>",
        "Challenge_closed_time":1612491964670,
        "Challenge_comment_count":1,
        "Challenge_created_time":1611073723990,
        "Challenge_favorite_count":1,
        "Challenge_last_edit_time":1611079944120,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65795579",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":11.3,
        "Challenge_reading_time":25.53,
        "Challenge_score_count":1,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":393.9557444445,
        "Challenge_title":"Debugging AML Model Deployment",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_view_count":106,
        "Challenge_word_count":231,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582182357612,
        "Poster_location":null,
        "Poster_reputation_count":155.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>The issue was in the YAML file. <strong>The dependencies\/libraries in the YAML should be according to conda environment<\/strong>. So, I changed everything accordingly, and it worked.<\/p>\n<p>Modified YAML file:<\/p>\n<pre><code>name: gender_prediction\ndependencies:\n- python=3.7\n- numpy\n- scikit-learn\n- pip:\n    - azureml-defaults\n    - pandas\n    - pickle4\n    - regex\n    - inference-schema[numpy-support]   \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"modifi yaml file depend librari yaml file accord conda environ yaml file accordingli deploy modifi yaml file depend model deploi successfulli",
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"yaml file depend librari yaml accord conda environ accordingli modifi yaml file gender predict depend numpi scikit pip default panda pickl regex infer schema numpi",
        "Solution_preprocessed_content":"yaml file yaml accord conda environ accordingli modifi yaml file",
        "Solution_readability":10.1,
        "Solution_reading_time":5.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1476806455803,
        "Answerer_location":"Holzkirchen, Deutschland",
        "Answerer_reputation_count":3068.0,
        "Answerer_view_count":386.0,
        "Challenge_adjusted_solved_time":1585.9678913889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>An Azure Data Factory pipeline for updating a trained ML model returns this error:<\/p>\n\n<pre><code>HTTP 404. The resource you are looking for (or one of its dependencies) could have been removed, had its name changed, or is temporarily unavailable. Please review the following URL and make sure that it is spelled correctly.\nRequested URL: \/workspaces\/xxxx\/webservices\/xxxx\/endpoints\/update\n\nDiagnostic details: job ID xxxx. Endpoint https:\/\/services.azureml.net\/workspaces\/xxxx\/webservices\/xxxx\/endpoints\/update.\n<\/code><\/pre>\n\n<p>I don't even want to think about why it returned a HTML document...\nI am 100% sure that the endpoint exists and the key provided is correct.\nSo what is my mistake?<\/p>",
        "Challenge_closed_time":1508942544572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1503048195263,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":1503233060163,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45753090",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":9.24,
        "Challenge_score_count":0,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1637.3192525,
        "Challenge_title":"Azure ML endpoint 404 error",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":494,
        "Challenge_word_count":99,
        "Platform":"Stack Overflow",
        "Poster_created_time":1476806455803,
        "Poster_location":"Holzkirchen, Deutschland",
        "Poster_reputation_count":3068.0,
        "Poster_view_count":386.0,
        "Solution_body":"<p>Deleting and creating the endpoint again fixed it.<\/p>\n\n<p>Microsoft...<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":6.8,
        "Solution_reading_time":1.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1506516283190,
        "Answerer_location":"Torino, TO, Italia",
        "Answerer_reputation_count":875.0,
        "Answerer_view_count":52.0,
        "Challenge_adjusted_solved_time":433.2236119444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello Stackoverflowers,<\/p>\n\n<p>I'm using azureml and I'm wondering if it is possible to log a confusion matrix of the xgboost model I'm training, together with the other metrics I'm already logging. Here's a sample of the code I'm using:<\/p>\n\n<pre><code>from azureml.core.model import Model\nfrom azureml.core import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nimport json\n\nwith open('.\/azureml.config', 'r') as f:\n    config = json.load(f)\n\nsvc_pr = ServicePrincipalAuthentication(\n   tenant_id=config['tenant_id'],\n   service_principal_id=config['svc_pr_id'],\n   service_principal_password=config['svc_pr_password'])\n\n\nws = Workspace(workspace_name=config['workspace_name'],\n                        subscription_id=config['subscription_id'],\n                        resource_group=config['resource_group'],\n                        auth=svc_pr)\n\ny_pred = model.predict(dtest)\n\nacc = metrics.accuracy_score(y_test, (y_pred&gt;.5).astype(int))\nrun.log(\"accuracy\",  acc)\nf1 = metrics.f1_score(y_test, (y_pred&gt;.5).astype(int), average='binary')\nrun.log(\"f1 score\",  f1)\n\n\ncmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\nrun.log_confusion_matrix('Confusion matrix', cmtx)\n<\/code><\/pre>\n\n<p>The above code raises this kind of error:<\/p>\n\n<pre><code>TypeError: Object of type ndarray is not JSON serializable\n<\/code><\/pre>\n\n<p>I already tried to transform the matrix in a simpler one, but another error occurred as before I logged a \"manual\" version of it (<code>cmtx = [[30000, 50],[40, 2000]]<\/code>).<\/p>\n\n<pre><code>run.log_confusion_matrix('Confusion matrix', [list([int(y) for y in x]) for x in cmtx])\n\nAzureMLException: AzureMLException:\n    Message: UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-    c5103b205379\/Confusion matrix already exists.\n    InnerException None\n    ErrorResponse \n{\n    \"error\": {\n        \"message\": \"UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-c5103b205379\/Confusion matrix already exists.\"\n    }\n}\n<\/code><\/pre>\n\n<p>This makes me think that I'm not properly handling the command <code>run.log_confusion_matrix()<\/code>. So, again, which is the best way I can log a confusion matrix to my azureml experiments?<\/p>",
        "Challenge_closed_time":1593519812260,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591960207257,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62343056",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":29.81,
        "Challenge_score_count":3,
        "Challenge_self_resolution":1,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":433.2236119444,
        "Challenge_title":"How to log a confusion matrix to azureml platform using python",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1418,
        "Challenge_word_count":216,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506516283190,
        "Poster_location":"Torino, TO, Italia",
        "Poster_reputation_count":875.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>I eventually found a solution thanks to colleague of mine. I'm hence answering myself, in order to close the question and, maybe, help somebody else.<\/p>\n<p>You can find the proper function in this link: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----<\/a>.<\/p>\n<p>Anyway, you also have to consider that, apparently, Azure doesn't work with the standard confusion matrix format returned by sklearn. It accepts indeed ONLY list of list, instead of numpy array, populated with numpy.int64 elements. So you also have to transform the matrix in a simpler format (for the sake of simplicity I used the nested list comprehension in the command below:<\/p>\n<pre><code>cmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\ncmtx = {\n\n&quot;schema_type&quot;: &quot;confusion_matrix&quot;,\n&quot;parameters&quot;: params,\n &quot;data&quot;: {&quot;class_labels&quot;: [&quot;0&quot;, &quot;1&quot;],\n          &quot;matrix&quot;: [[int(y) for y in x] for x in cmtx]}\n}\nrun.log_confusion_matrix('Confusion matrix - error rate', cmtx)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":"run log matrix function sdk accept matric format list list popul numpi element transform matrix format nest list comprehens log",
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"colleagu order close mayb somebodi function link http doc com api core core run run log matrix valu descript appar standard matrix format return sklearn accept list list numpi arrai popul numpi element transform matrix simpler format sake simplic nest list comprehens cmtx metric matrix test pred astyp cmtx schema type matrix paramet param data class label matrix cmtx run log matrix matrix rate cmtx",
        "Solution_preprocessed_content":"colleagu order close mayb somebodi function link appar standard matrix format return sklearn accept list list numpi arrai popul element transform matrix simpler format sake simplic nest list comprehens",
        "Solution_readability":15.4,
        "Solution_reading_time":17.29,
        "Solution_score_count":6.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":126.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":19.7929647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I prefer to use my local laptop for cost reasons + having an IDE like Visual Studio Code. As it stands my IT department is also not able\/reluctant to setup database connectivity for could 9 etc. (so I would have to move data into S3 from local laptop). I also investigated <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/randomcutforest.html\" rel=\"nofollow noreferrer\">this<\/a> to SSH into an EC2. However, again IT is unwilling to open up network connectivity ...<\/p>\n<p>The issue I have is, that my laptop only has 8GB (windows). So from time to time I would like\/have to use the cloud (e.g. to do hyperparameter optimisation etc.) and ultimately deploy models (e.g. as docker images R + plumber - Python + FASt API etc.). Coming across code like this (run in sagemaker notebooks):<\/p>\n<pre><code>estimator = sagemaker.estimator.Estimator(\n    role=role_arn,\n    instance_count=1,\n    instance_type='ml.m5.xlarge',\n    image_uri=container,\n    debugger_hook_config=debugger_hook_config,\n    rules=rules,\n    sagemaker_session=session)\n<\/code><\/pre>\n<p>which is <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">SageMaker Python SDK<\/a> code. Can I also execute everything locally via Visual Studio Code? I understand that SDKs like this are a constant moving target but can I use this and are there any books\/tutorials (Google did not return much). Thanks.<\/p>",
        "Challenge_closed_time":1642413829196,
        "Challenge_comment_count":2,
        "Challenge_created_time":1642342574523,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70730897",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_participation_count":3,
        "Challenge_readability":10.9,
        "Challenge_reading_time":18.6,
        "Challenge_score_count":1,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":19.7929647222,
        "Challenge_title":"sage maker using visual studio code locally instead of notebooks - SDK",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_view_count":879,
        "Challenge_word_count":189,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>You can. Use <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"nofollow noreferrer\">SageMaker Local<\/a> to execute training and inference locally in docker containers on your laptop.<br \/>\nIn the Estimator, you'll specify: <code>instance_type='local'<\/code> or <code>instance_type='local_gpu'<\/code><\/p>",
        "Solution_comment_count":2.0,
        "Solution_gpt_summary":"local execut train infer local docker laptop specifi instanc type local instanc type local gpu estim",
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"local execut train infer local docker laptop estim specifi instanc type local instanc type local gpu",
        "Solution_preprocessed_content":"local execut train infer local docker laptop estim specifi",
        "Solution_readability":23.4,
        "Solution_reading_time":5.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1221528724667,
        "Answerer_location":"West Coast, North America",
        "Answerer_reputation_count":11340.0,
        "Answerer_view_count":737.0,
        "Challenge_adjusted_solved_time":182.4722333334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've created the worker task template for test in AWS Augmented AI.\nHowever, I don't know how to delete those template.<\/p>\n\n<p>Please tell me how to do it.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/CZkiz.png\" rel=\"nofollow noreferrer\">image<\/a><\/p>",
        "Challenge_closed_time":1587598650403,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586941750363,
        "Challenge_favorite_count":0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61225077",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_participation_count":2,
        "Challenge_readability":5.9,
        "Challenge_reading_time":3.94,
        "Challenge_score_count":0,
        "Challenge_self_resolution":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":182.4722333334,
        "Challenge_title":"How to delete worker task templates in AWS Augmented AI?",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_view_count":38,
        "Challenge_word_count":41,
        "Platform":"Stack Overflow",
        "Poster_created_time":1532422348876,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>You cannot currently delete HumanTaskUis. That may be a capability added in the future.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"delet humantaskui capabl futur",
        "Solution_preprocessed_content":"delet humantaskui capabl futur",
        "Solution_readability":8.4,
        "Solution_reading_time":1.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":14.0,
        "Tool":"Amazon SageMaker"
    }
]