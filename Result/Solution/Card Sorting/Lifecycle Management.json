[
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.9458333333,
        "Challenge_answer_count":0,
        "Challenge_body":"I try to follow this Checkpoints tutorial and documentation page https:\/\/dvc.org\/doc\/user-guide\/experiment-management\/checkpoints \r\n\r\nHowever, after adding `dvclive` in the train.py file with this code: \r\n\r\n import the dvclive package with the other imports:\r\n\r\n```python\r\nimport dvclive\r\n...\r\n    ...\r\n    for k, v in metrics.items():\r\n        print('Epoch %s: %s=%s'%(i, k, v))\r\n        dvclive.log(k, v)\r\n    dvclive.next_step()\r\n```\r\nI got an error: \r\n```bash \r\n\u276f dvc exp run\r\nModified checkpoint experiment based on 'exp-defaa' will be created   \r\nRunning stage 'train':                                                                                                                                                                                                                                               \r\n> python train.py\r\n...\r\nEpoch 1: loss=0.1541447937488556\r\nTraceback (most recent call last):\r\n  File \"[USER-PATH]\/checkpoints-tutorial\/train.py\", line 125, in <module>\r\n    main()\r\n  File \"[USER-PATH]\/checkpoints-tutorial\/train.py\", line 118, in main\r\n    dvclive.log(name=k, val=v)\r\nAttributeError: module 'dvclive' has no attribute 'log'\r\n\r\nfile:\/\/\/[USER-PATH]\/checkpoints-tutorial\/dvclive.html\r\nERROR: failed to reproduce 'dvc.yaml': failed to run: python train.py, exited with 1\r\n``` \r\n\r\nI only could run the example with the following trick: \r\n```python\r\nfrom dvclive import Live \r\ndvclive = Live()\r\n```\r\nAre there any updated in `dvclive` API? \r\n\r\nSystem info\r\n```bash \r\n\u276f dvc doctor\r\nDVC version: 2.6.4 (pip)\r\n---------------------------------\r\nPlatform: Python 3.9.4 on macOS-11.6-x86_64-i386-64bit\r\nSupports:\r\n        hdfs (pyarrow = 5.0.0),\r\n        http (requests = 2.26.0),\r\n        https (requests = 2.26.0)\r\nCache types: reflink, hardlink, symlink\r\nCache directory: apfs on \/dev\/disk1s1s1\r\nCaches: local\r\nRemotes: None\r\nWorkspace directory: apfs on \/dev\/disk1s1s1\r\nRepo: dvc, git\r\n```\r\n\r\nFIY @flippedcoder @daavoo ",
        "Challenge_closed_time":1633697794000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1633694389000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the kedro-mlflow plugin not working with projects created with kedro==0.18.1. When the user tries to run the pipeline, an error is raised due to the removal of the private attribute '_active_session' in kedro==0.18.1. The solution is to use the 'after_context_created' hook to retrieve and set up the configuration.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/checkpoints-tutorial\/issues\/1",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":7.7,
        "Challenge_reading_time":20.74,
        "Challenge_repo_contributor_count":2.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":3.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":0.9458333333,
        "Challenge_title":"AttributeError: module 'dvclive' has no attribute 'log'",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":191,
        "Discussion_body":"Thanks for the catch @mike0sv ! No trouble (literally, no trouble at all since it was @mnrozhkov :))",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1393579668636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":1450.0,
        "Answerer_view_count":162.0,
        "Challenge_adjusted_solved_time":0.1505822222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Kedro, we can pipeline different nodes and partially run some nodes. When we are partially running some nodes, we need to save some inputs from the nodes somewhere so that when another node is run it can access the data that the previous node has generated. However, in which file do we write the code for this - pipeline.py, run.py or nodes.py?<\/p>\n\n<p>For instance, I am trying to save a dir path directly to the DataCatalog under a variable name 'model_path'. <\/p>\n\n<p>Snippet from pipeline.py:<\/p>\n\n<pre><code>    # A mapping from a pipeline name to a ``Pipeline`` object.\ndef create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\nio = DataCatalog(dict(\n    model_path=MemoryDataSet()\n))\n\nio.save('model_path', \"data\/06_models\/model_test\")\nprint('****', io.exists('model_path'))\n\npipeline = Pipeline([\n    node(\n        split_files,\n        [\"data_csv\", \"parameters\"],\n        [\"train_filenames\", \"val_filenames\", \"train_labels\", \"val_labels\"],\n        name=\"splitting filenames\"\n    ),\n    # node(\n    #     create_and_train,\n    #     [\"train_filenames\", \"val_filenames\", \"train_labels\", \"val_labels\", \"parameters\"],\n    #     \"model_path\",\n    #     name=\"Create Dataset, Train and Save Model\"\n    # ),\n    node(\n        validate_model,\n        [\"val_filenames\", \"val_labels\", \"model_path\"],\n        None,\n        name=\"Validate Model\",\n    )\n\n]).decorate(decorators.log_time, decorators.mem_profile)\n\nreturn {\n    \"__default__\": pipeline\n}\n<\/code><\/pre>\n\n<p>However, I get the following error when I Kedro run:<\/p>\n\n<pre><code>ValueError: Pipeline input(s) {'model_path'} not found in the DataCatalog\n<\/code><\/pre>",
        "Challenge_closed_time":1571671635460,
        "Challenge_comment_count":0,
        "Challenge_created_time":1571371429583,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in Kedro regarding where to save the output of a node when partially running some nodes. They are trying to save a directory path directly to the DataCatalog under a variable name 'model_path', but are encountering an error stating that the pipeline input 'model_path' is not found in the DataCatalog. The user is unsure whether to write the code for this in pipeline.py, run.py, or nodes.py.",
        "Challenge_last_edit_time":1571671894436,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58443788",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":19.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":83.3905213889,
        "Challenge_title":"Where to perform the saving of an nodeoutput in Kedro?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1606.0,
        "Challenge_word_count":177,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545311054088,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":170.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>Node inputs are automatically loaded by Kedro from the <code>DataCatalog<\/code> before being passed to the node function. Node outputs are consequently saved to the DataCatalog after the node successfully produces some data. DataCatalog configuration by default is taken from <code>conf\/base\/catalog.yml<\/code>. <\/p>\n\n<p>In your example <code>model_path<\/code> is produced by <code>Create Dataset, Train and Save Model<\/code> node and then consumed by <code>Validate Model<\/code>. If required dataset definition is not found in the <code>conf\/base\/catalog.yml<\/code>, Kedro will try to store this dataset in memory using <code>MemoryDataSet<\/code>. This will work if you run the pipeline that contains both <code>Create Dataset...<\/code> and <code>Validate Model<\/code> nodes (given no other issues arise). However, when you are trying to run <code>Validate Model<\/code> node alone, Kedro attempts to read <code>model_path<\/code> dataset from memory, which doesn't exist there.<\/p>\n\n<p>So, <strong>TLDR<\/strong>:<\/p>\n\n<p>To mitigate this, you need to:<\/p>\n\n<p>a) persist <code>model_path<\/code> by adding something like the following to your <code>conf\/base\/catalog.yml<\/code>:<\/p>\n\n<pre><code>model_path:\n  type: TextLocalDataSet\n  filepath: data\/02_intermediate\/model_path.txt\n<\/code><\/pre>\n\n<p>b) run <code>Create Dataset, Train and Save Model<\/code> node (and its dependencies) at least once<\/p>\n\n<p>After completing a) and b) you should be able to start running <code>Validate Model<\/code> separately.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1571672436532,
        "Solution_link_count":0.0,
        "Solution_readability":12.8,
        "Solution_reading_time":19.39,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":183.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":170.4254455556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/run-pipeline.html#run-pipeline-prereq\" rel=\"nofollow noreferrer\">SageMaker documentatin<\/a> explains how to run a pipeline, but it assumes I have just <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html\" rel=\"nofollow noreferrer\">defined it<\/a> and I have the object <code>pipeline<\/code> available.<\/p>\n<p>How can I run an <strong>existing<\/strong> pipeline with <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">Python SDK<\/a>?<\/p>\n<p>I know how to read a pipeline with AWS CLI (i.e. <code>aws sagemaker describe-pipeline --pipeline-name foo<\/code>). Can the same be done with Python code? Then I would have <code>pipeline<\/code> object ready to use.<\/p>",
        "Challenge_closed_time":1659628256872,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659598635513,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to execute an existing SageMaker pipeline using Python SDK, as the documentation assumes that the user has only defined the pipeline and has the object 'pipeline' available. The user is also looking for a way to obtain the 'pipeline' object using Python code, similar to how it can be done using AWS CLI.",
        "Challenge_last_edit_time":1659598939403,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73232032",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":11.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":8.2281552778,
        "Challenge_title":"Start execution of existing SageMaker pipeline using Python SDK",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":106.0,
        "Challenge_word_count":83,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1217615304816,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Poland",
        "Poster_reputation_count":16694.0,
        "Poster_view_count":3155.0,
        "Solution_body":"<p>If the Pipeline has been created, you can use the Python Boto3 SDK to make the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.start_pipeline_execution\" rel=\"nofollow noreferrer\"><code>StartPipelineExecution<\/code><\/a> API call.<\/p>\n<pre><code>response = client.start_pipeline_execution(\n    PipelineName='string',\n    PipelineExecutionDisplayName='string',\n    PipelineParameters=[\n        {\n            'Name': 'string',\n            'Value': 'string'\n        },\n    ],\n    PipelineExecutionDescription='string',\n    ClientRequestToken='string',\n    ParallelismConfiguration={\n        'MaxParallelExecutionSteps': 123\n    }\n)\n<\/code><\/pre>\n<p>If you prefer AWS CLI, the most basic call is:<\/p>\n<pre><code>aws sagemaker start-pipeline-execution --pipeline-name &lt;name-of-the-pipeline&gt;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1660212471007,
        "Solution_link_count":1.0,
        "Solution_readability":25.0,
        "Solution_reading_time":10.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":53.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":0.3402952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Let's say we have multiple long running pipeline nodes.\nIt seems quite straight forward to checkpoint or cache the intermediate results, so when nodes after a checkpoint are changed or added only these nodes must be executed again.<\/p>\n\n<p>Does Kedro provide functionality to make sure, that when I run the pipeline only those steps are \nexecuted that have changed?\nAlso the reverse, is there a way to make sure, that all steps that have changed are executed?<\/p>\n\n<p>Let's say a pipeline producing some intermediate result changed, will it be executed, when i execute a pipeline depending on the output of the first?<\/p>\n\n<p><strong>TL;DR:<\/strong> Does Kedro have <code>makefile<\/code>-like tracking of what needs to be done and what not?<\/p>\n\n<p>I think my question is similar to <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/issues\/341\" rel=\"nofollow noreferrer\">issue #341<\/a>, but I do not require support of cyclic graphs.<\/p>",
        "Challenge_closed_time":1591362515296,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591361290233,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is asking if Kedro supports checkpointing or caching of intermediate results to avoid re-execution of the entire pipeline when only a few nodes have changed. They are also asking if Kedro has a tracking system similar to makefile to determine which steps need to be executed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62215724",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":12.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.3402952778,
        "Challenge_title":"Does Kedro support Checkpointing\/Caching of Results?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":367.0,
        "Challenge_word_count":143,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1494502461176,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":83.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You might want to have a look at the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.IncrementalDataSet.html\" rel=\"nofollow noreferrer\">IncrementalDataSet<\/a> alongside the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/08_advanced_io.html#partitioned-dataset\" rel=\"nofollow noreferrer\">partitioned dataset<\/a> documentation, specifically the section on <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/08_advanced_io.html#incremental-loads-with-incrementaldataset\" rel=\"nofollow noreferrer\">incremental loads with the incremental dataset<\/a> which has a notion of \"checkpointing\", although checkpointing is a manual step and not automated like <code>makefile<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":23.6,
        "Solution_reading_time":9.71,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":51.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0349952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi Team,    <\/p>\n<p>I am trying to deploy 2 ML models ( which is registered in Model Registry ) to Azure Container Instance using DevOps Release pipeline using AZ CLI ML extension    <\/p>\n<p>My ACI Configuration is :    <\/p>\n<p><code>containerResourceRequirements:       cpu: 1       memoryInGB: 4     computeType: ACI <\/code>    <\/p>\n<p>Inference Config :    <\/p>\n<p><code>entryScript: score.py     runtime: python     condaFile: conda_dependencies.yml     extraDockerfileSteps:     schemaFile:     sourceDirectory:     enableGpu: False     baseImage:     baseImageRegistry:<\/code>    <\/p>\n<p>All score.py, conda_dependencies.yml, aciDeploymentConfig.yml is placed in a flattened directory which is publised in to DevOps pipeline artifcat and looks like    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/10000-files.png?platform=QnA\" alt=\"10000-files.png\" \/>    <\/p>\n<p>DevOps Deploy command looks like     <\/p>\n<p><code>az ml model deploy -g $(ml.resourceGroup) -w $(ml.workspace) --name $(service.name.staging) -f .\/model.json -m &quot;GloVe:4&quot; --dc aciDeploymentConfig.yml --ic inferenceConfig.yml --overwrite --debug <\/code>    <\/p>\n<p>Also i have set the working directory as the folder where all above files are placed. something like     <\/p>\n<p>$(System.DefaultWorkingDirectory)\/_Symptom-Code-Indexing\/symptom_model\/a    <\/p>\n<p>Its getting in to an exception as     <\/p>\n<p>2020-06-08T12:50:27.9202657Z     &quot;error&quot;: {    <br \/>\n2020-06-08T12:50:27.9208361Z         &quot;message&quot;: &quot;Received bad response from Model Management Service:\\nResponse Code: 400\\nHeaders: {'Date': 'Mon, 08 Jun 2020 12:50:27 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '823e8483923846b1958c08ffaba074ff', 'x-ms-client-session-id': '62e84a29-b77c-456f-9d91-5ca6be26f79c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{&quot;code&quot;:&quot;BadRequest&quot;,&quot;statusCode&quot;:400,&quot;message&quot;:&quot;The request is invalid.&quot;,&quot;details&quot;:[{&quot;code&quot;:&quot;InvalidOverwriteRequest&quot;,&quot;message&quot;:&quot;Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.&quot;}],&quot;correlation&quot;:{&quot;RequestId&quot;:&quot;823e8483923846b1958c08ffaba074ff&quot;}}'&quot;    <br \/>\n2020-06-08T12:50:27.9212109Z     }    <br \/>\n2020-06-08T12:50:27.9212376Z }}    <br \/>\n2020-06-08T12:50:27.9213437Z {'Azure-cli-ml Version': '1.6.0', 'Error': WebserviceException:    <br \/>\n2020-06-08T12:50:27.9214158Z  Message: Received bad response from Model Management Service:    <br \/>\n2020-06-08T12:50:27.9214688Z Response Code: 400    <br \/>\n2020-06-08T12:50:27.9217800Z Headers: {'Date': 'Mon, 08 Jun 2020 12:50:27 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '823e8483923846b1958c08ffaba074ff', 'x-ms-client-session-id': '62e84a29-b77c-456f-9d91-5ca6be26f79c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}    <br \/>\n2020-06-08T12:50:27.9222115Z Content: b'{&quot;code&quot;:&quot;BadRequest&quot;,&quot;statusCode&quot;:400,&quot;message&quot;:&quot;The request is invalid.&quot;,&quot;details&quot;:[{&quot;code&quot;:&quot;InvalidOverwriteRequest&quot;,&quot;message&quot;:&quot;Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.&quot;}],&quot;correlation&quot;:{&quot;RequestId&quot;:&quot;823e8483923846b1958c08ffaba074ff&quot;}}'    <br \/>\n2020-06-08T12:50:27.9223705Z  InnerException None    <br \/>\n2020-06-08T12:50:27.9224049Z  ErrorResponse     <br \/>\n2020-06-08T12:50:27.9224320Z {    <br \/>\n2020-06-08T12:50:27.9224617Z     &quot;error&quot;: {    <br \/>\n2020-06-08T12:50:27.9229025Z         &quot;message&quot;: &quot;Received bad response from Model Management Service:\\nResponse Code: 400\\nHeaders: {'Date': 'Mon, 08 Jun 2020 12:50:27 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '823e8483923846b1958c08ffaba074ff', 'x-ms-client-session-id': '62e84a29-b77c-456f-9d91-5ca6be26f79c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{&quot;code&quot;:&quot;BadRequest&quot;,&quot;statusCode&quot;:400,&quot;message&quot;:&quot;The request is invalid.&quot;,&quot;details&quot;:[{&quot;code&quot;:&quot;InvalidOverwriteRequest&quot;,&quot;message&quot;:&quot;Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.&quot;}],&quot;correlation&quot;:{&quot;RequestId&quot;:&quot;823e8483923846b1958c08ffaba074ff&quot;}}'&quot;    <br \/>\n2020-06-08T12:50:27.9230782Z     }    <br \/>\n2020-06-08T12:50:27.9230908Z }}    <br \/>\n2020-06-08T12:50:27.9231134Z Event: Cli.PostExecute [&lt;function AzCliLogging.deinit_cmd_metadata_logging at 0x7fea2ca1f730&gt;]    <br \/>\n2020-06-08T12:50:27.9231431Z az_command_data_logger : exit code: 1    <br \/>\n2020-06-08T12:50:27.9275693Z telemetry.save : Save telemetry record of length 7390 in cache    <br \/>\n2020-06-08T12:50:27.9280735Z telemetry.check : Negative: The \/home\/vsts\/work\/_temp\/.azclitask\/telemetry.txt was modified at 2020-06-08 12:47:41.161160, which in less than 600.000000 s    <br \/>\n2020-06-08T12:50:27.9290480Z command ran in 55.735 seconds.    <br \/>\n2020-06-08T12:50:28.1525434Z ##[error]Script failed with exit code: 1    <br \/>\n2020-06-08T12:50:28.1536650Z [command]\/opt\/hostedtoolcache\/Python\/3.6.10\/x64\/bin\/az account clear    <br \/>\n2020-06-08T12:50:29.9078943Z ##[section]Finishing: Deploy Model to ACI    <\/p>\n<p>But when i tried to Deploy it using Python SDK it works as well. Is there any permission issues or login to be set before using DevOps Release. I have not done any sort of login in my DevOps Build pipeline.    <\/p>\n<p>Any pointers on what is going wrong here ? It would be really helpful.    <\/p>\n<p>Thanks,    <br \/>\nSrijith    <\/p>",
        "Challenge_closed_time":1592223420340,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592223294357,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to deploy two ML models to Azure Container Instance using DevOps Release pipeline using AZ CLI ML extension. The deployment is failing with an exception indicating that the request is invalid and cannot update container resource requirements, DNS name label, or deployment type. However, the deployment works fine using Python SDK. The user is seeking help to understand the cause of the issue and any necessary permissions or login required for DevOps Release.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/36104\/deployment-of-multiple-models-to-container-instanc",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":87.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":60,
        "Challenge_solved_time":0.0349952778,
        "Challenge_title":"Deployment of Multiple Models to Container Instance Fails in Azure DevOps",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":509,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>They're actively answering Devops question in dedicated forums here.  <\/p>\n<p><a href=\"https:\/\/developercommunity.visualstudio.com\/spaces\/21\/index.html\">https:\/\/developercommunity.visualstudio.com\/spaces\/21\/index.html<\/a>  <\/p>\n<p>--please don't forget to <strong>Accept as answer<\/strong> if the reply is helpful--  <\/p>\n<hr \/>\n<p>Regards, Dave Patrick ....    <br \/>\nMicrosoft Certified Professional    <br \/>\nMicrosoft MVP [Windows Server] Datacenter Management    <\/p>\n<p>Disclaimer: This posting is provided &quot;AS IS&quot; with no warranties or guarantees, and confers no rights.  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.3,
        "Solution_reading_time":7.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":59.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1324129118823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11756.0,
        "Answerer_view_count":517.0,
        "Challenge_adjusted_solved_time":0.30124,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm looking to deploy the SageMaker pipeline using CDK (<a href=\"https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html<\/a>) but could not find any code examples. Any pointers?<\/p>",
        "Challenge_closed_time":1652283812907,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652282728443,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to deploy an AWS SageMaker pipeline using the cloud development kit (CDK) and is looking for code examples.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72203674",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.3,
        "Challenge_reading_time":5.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.30124,
        "Challenge_title":"Deploy AWS SageMaker pipeline using the cloud development kit (CDK)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":31,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1489685785576,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Canada",
        "Poster_reputation_count":65.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>CDK L1 Constructs correspond 1:1 to a CloudFormation resource of the same name. The construct props match the resouce properties.  The go-to source is therefore the CloudFormation docs.<\/p>\n<p>The <code>AWS::SageMaker::Pipeline<\/code> <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html#aws-resource-sagemaker-pipeline--examples\" rel=\"nofollow noreferrer\">docs have a more complete example<\/a>.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.8,
        "Solution_reading_time":6.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":39.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1613062428296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":141.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":0.1452175,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Having issues with kedro. The 'register_pipelines' function doesn't seem to be running or creating the <strong>default<\/strong> Pipeline that I'm returning from it.<\/p>\n<p>The error is<\/p>\n<pre><code>(kedro-environment) C:\\Users\\cc667216\\OneDrive\\DCS_Pipeline\\dcs_files&gt;kedro run\n2021-03-22 13:30:28,201 - kedro.framework.session.store - INFO - `read()` not implemented for `BaseSessionStore`. Assuming empty store.\nfatal: not a git repository (or any of the parent directories): .git\n2021-03-22 13:30:28,447 - kedro.framework.session.session - WARNING - Unable to git describe C:\\Users\\cc667216\\OneDrive\\DCS_Pipeline\\dcs_files\n2021-03-22 13:30:28,476 - root - INFO - ** Kedro project dcs_files\n2021-03-22 13:30:28,486 - kedro.framework.session.store - INFO - `save()` not implemented for `BaseSessionStore`. Skipping the step.\nTraceback (most recent call last):\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\kedro\\framework\\context\\context.py&quot;, line 304, in _get_pipeline\n    return pipelines[name]\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\dynaconf\\utils\\functional.py&quot;, line 17, in inner\n    return func(self._wrapped, *args)\nKeyError: '__default__'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\Scripts\\kedro-script.py&quot;, line 9, in &lt;module&gt;\n    sys.exit(main())\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\kedro\\framework\\cli\\cli.py&quot;, line 228, in main\n    cli_collection(**cli_context)\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\click\\core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\click\\core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\click\\core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\click\\core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\click\\core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;C:\\Users\\cc667216\\OneDrive\\DCS_Pipeline\\dcs_files\\src\\dcs_package\\cli.py&quot;, line 240, in run\n    pipeline_name=pipeline,\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\kedro\\framework\\session\\session.py&quot;, line 344, in run\n    pipeline = context._get_pipeline(name=pipeline_name)\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\kedro\\framework\\context\\context.py&quot;, line 310, in _get_pipeline\n    ) from exc\nkedro.framework.context.context.KedroContextError: Failed to find the pipeline named '__default__'. It needs to be generated and returned by the 'register_pipelines' function.\n<\/code><\/pre>\n<p>My src\\dcs_package\\pipeline_registry.py looks like this:<\/p>\n<pre><code># Copyright 2021 QuantumBlack Visual Analytics Limited\n#\n# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\n# OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND\n# NONINFRINGEMENT. IN NO EVENT WILL THE LICENSOR OR OTHER CONTRIBUTORS\n# BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY, WHETHER IN AN\n# ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF, OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n#\n# The QuantumBlack Visual Analytics Limited (&quot;QuantumBlack&quot;) name and logo\n# (either separately or in combination, &quot;QuantumBlack Trademarks&quot;) are\n# trademarks of QuantumBlack. The License does not grant you any right or\n# license to the QuantumBlack Trademarks. You may not use the QuantumBlack\n# Trademarks or any confusingly similar mark as a trademark for your product,\n# or use the QuantumBlack Trademarks in any other manner that might cause\n# confusion in the marketplace, including but not limited to in advertising,\n# on websites, or on software.\n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n&quot;&quot;&quot;Project pipelines.&quot;&quot;&quot;\nfrom typing import Dict\nfrom kedro.pipeline import Pipeline, node\nfrom .pipelines.data_processing.pipeline import create_pipeline\nimport logging\n\ndef register_pipelines() -&gt; Dict[str, Pipeline]:\n    &quot;&quot;&quot;Register the project's pipelines.\n\n    Returns:\n        A mapping from a pipeline name to a ``Pipeline`` object.\n    &quot;&quot;&quot;\n    log = logging.getLogger(__name__)\n    log.info(&quot;Start register_pipelines&quot;) \n    data_processing_pipeline = create_pipeline()\n    log.info(&quot;create pipeline done&quot;) \n    \n\n    return {\n        &quot;__default__&quot;: data_processing_pipeline,\n        &quot;dp&quot;: data_processing_pipeline\n    }\n\n<\/code><\/pre>\n<p>Then I have a &quot;src\\dcs_package\\pipelines\\data_processing\\pipeline.py&quot; file with a real simple function that outputs &quot;test string&quot; and nothing else.<\/p>\n<p>I was able to read a few items from my catalog (a csv and a xlsx) so I think all the dependencies are working fine.<\/p>",
        "Challenge_closed_time":1616435908600,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616435385817,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with Kedro where the 'register_pipelines' function is not creating the default pipeline that is returned from it. The error message indicates that the pipeline named '__default__' cannot be found. The user has provided their pipeline_registry.py file and a data_processing pipeline file, which appear to be functioning correctly. The user has also confirmed that their dependencies are working fine.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66751310",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":72.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":52,
        "Challenge_solved_time":0.1452175,
        "Challenge_title":"Kedro : Failed to find the pipeline named '__default__'",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1486.0,
        "Challenge_word_count":536,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1398987181710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":75.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>What version of kedro are you on? There is a bit of a problem with kedro 0.17.2 where the true error is masked and will return the exception that you're seeing instead. It's possible that the root cause of the error is actually some other <code>ModuleNotFoundError<\/code> or <code>AttributeError<\/code>. Try doing a <code>kedro install<\/code> before <code>kedro run<\/code> and see if that fixes it.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":5.05,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1386491614716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2778.0,
        "Answerer_view_count":352.0,
        "Challenge_adjusted_solved_time":1.7360491667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Recently, I have changed account on AWS and faced with weird error in Sagemaker.<\/p>\n<p>Basically, I'm just checking <code>xgboost<\/code> algo with some toy dataset in this manner:<\/p>\n<pre><code>from sagemaker import image_uris\n\nxgb_image_uri = image_uris.retrieve(&quot;xgboost&quot;, boto3.Session().region_name, &quot;1&quot;)\n\nclf = sagemaker.estimator.Estimator(xgb_image_uri,\n                   role, 1, 'ml.c4.2xlarge',\n                   output_path=&quot;s3:\/\/{}\/output&quot;.format(session.default_bucket()),\n                   sagemaker_session=session)\n\nclf.fit(location_data)\n<\/code><\/pre>\n<p>Then the training job is starting to be executed but for some reason, on downloading data step it stops the training job and displays the following message:<\/p>\n<pre><code>2021-10-21 17:33:27 Downloading - Downloading input data\n2021-10-21 17:33:27 Stopping - Stopping the training job\n2021-10-21 17:33:27 Stopped - Training job stopped\nProfilerReport-1634837444: Stopping\n..\nJob ended with status 'Stopped' rather than 'Completed'. This could mean the job timed out or stopped early for some other reason: Consider checking whether it completed as you expect.\n<\/code><\/pre>\n<p>Also, when I'm trying to go back to training jobs section and check for logs in cloudwatch there is nothing to be displayed. Is it common issue and who had faced with that? Are there any workarounds?<\/p>",
        "Challenge_closed_time":1634844298240,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634838048463,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has encountered an issue with a training job in Sagemaker after changing their AWS account. The training job stops during the downloading data step and displays a message indicating that it has been stopped. The user has also tried to check for logs in Cloudwatch but nothing is displayed. The user is seeking advice on whether this is a common issue and if there are any workarounds.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69666500",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":17.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1.7360491667,
        "Challenge_title":"Training Job is Stopping in Sagemaker",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":361.0,
        "Challenge_word_count":165,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1386491614716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2778.0,
        "Poster_view_count":352.0,
        "Solution_body":"<p>The problem was most likely with templates for sagemaker that was runned before creating the instance.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":1.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":12.2413888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI am a little confused about whether S3 Shard key would work when using PIPE mode, here is a example:\n\nAssume I have:\n\n2 instance, each instance have 4 worker;\n\ndata: total 8 files with total size 8GB, each file is 1GB. Put them into 4 different S3 path, that means, each path has 2 files (2GB in total)\n\nIf I use PIPE mode, and s3_input using  distribution='ShardedByS3Key', and create 4 channel (each channel mapping a s3 path, 2 files)\n\ntrain_s3_input_1 = sagemaker.inputs.s3_input(channel_1, distribution='ShardedByS3Key')\n\nQuestion:\n\nHow much data of each worker get to train, 1 file or 2 files? thanks",
        "Challenge_closed_time":1589407880000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589363811000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is confused about whether S3 Shard key would work when using PIPE mode. They have 2 instances, each with 4 workers, and a total of 8 files with a total size of 8GB. They have put the files into 4 different S3 paths, with each path having 2 files. The user wants to know how much data each worker will get to train if they use PIPE mode and s3_input using distribution='ShardedByS3Key' with 4 channels mapping to each S3 path.",
        "Challenge_last_edit_time":1667925675584,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU31DdUqtuQziixKTkPasZKw\/confusion-about-pipe-mode-when-using-s3-shard-key",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":8.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":12.2413888889,
        "Challenge_title":"confusion about PIPE mode when using S3 shard key",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":46.0,
        "Challenge_word_count":108,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\nSageMaker will replicate a subset of data (1\/n ML compute instances) on each ML compute instance that is launched for model training when you specify *ShardedByS3Key*. If there are n ML compute instances launched for a training job, each instance gets approximately 1\/n of the number of S3 objects. This applies in both File and Pipe modes. Keep this in mind when developing algorithms.\n\nTo answer your question:\nHow much data of each worker get to train, 1 file or 2 files? 1 file each from the training channel.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925572408,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":6.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":90.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.01856,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hey,    <br \/>\nIs there any way to export the ML Pipeline as Template\/PNG\/Code ?<\/p>",
        "Challenge_closed_time":1668156834876,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668149568060,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on how to export an Azure ML Pipeline as a template, PNG, or code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1085047\/azure-ml-pipeline-designer-export",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":1.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":2.01856,
        "Challenge_title":"Azure ML pipeline designer export",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":18,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=979eeb5c-297b-4af8-af81-bb25ddabe5d5\">@Kumar Shanu  <\/a> The designer pipelines cannot be exported to code or a template currently.     <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.9,
        "Solution_reading_time":7.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1436771091480,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Brno, \u010cesko",
        "Answerer_reputation_count":51.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":0.1839775,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using AzureML SDK pipeline with AutoMLStep. How can I add PipelineParameter into AutoMLStep configuration? I would like to use it for a definition of max_horizon.\nIt should work with <\/p>\n\n<blockquote>\n  <p>passthru_automl_config=False<\/p>\n<\/blockquote>\n\n<p>but I am getting error <\/p>\n\n<blockquote>\n  <p>Message: Unsupported value of max_horizon. max_horizon must be integer or 'auto'<\/p>\n<\/blockquote>\n\n<pre><code>max_horizon = PipelineParameter(name='max_horizon', default_value=30)\n\nautoml_settings = {\n            \"iteration_timeout_minutes\" : 60\n            \"grain_column_names\": [\"COUNTRY_CODE\"],\n            \"time_column_name\": \"DATE\"\n        }        \n\nautoml_config = AutoMLConfig(task='forecasting',\n                             path = \".\/src\",\n                             primary_metric=primary_metric,\n                             iterations=iterations,\n                             max_concurrent_iterations=max_concurrent_iterations,\n                             training_data = train_data,\n                             label_column_name = label,\n                             n_cross_validations=5,\n                             compute_target = compute_target,\n                             max_horizon= max_horizon,\n                             **automl_settings)\n\ntrainWithAutomlStep = AutoMLStep(name=\"experiment_name\",\n                                 automl_config=automl_config,\n                                 passthru_automl_config=False,\n                                 outputs=[metrics_data, model_data],\n                                 allow_reuse=True)\n<\/code><\/pre>",
        "Challenge_closed_time":1592202183327,
        "Challenge_comment_count":2,
        "Challenge_created_time":1591257136050,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while using AzureML SDK pipeline with AutoMLStep. They are trying to add a PipelineParameter into AutoMLStep configuration for defining max_horizon, but they are getting an error message stating that max_horizon must be an integer or 'auto'. The user has shared their code snippet for reference.",
        "Challenge_last_edit_time":1592201521008,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62189492",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":22.9,
        "Challenge_reading_time":15.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":262.5131325,
        "Challenge_title":"How to pass PipelineParameter into AutoMLStep in AzureML Python SDK",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":316.0,
        "Challenge_word_count":98,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436771091480,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brno, \u010cesko",
        "Poster_reputation_count":51.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Here is a response from Microsoft:<\/p>\n\n<blockquote>\n  <p>PipelineParameter is currently not supported for use with AutoMLConfig parameters inside of AutoMLStep.<\/p>\n  \n  <p>Then, the only workaround in order to use PipelineParameter with\n  AutoMLConfig would be to use AutoML in a PythonScriptStep, which is a\n  similar usage\/approach when you use AutoMLConfig with\n  ParallelRunConfig in pipelines (without using AutoMLStep), like the\n  \u2018Many Models\u2019 solution accelerator does.<\/p>\n<\/blockquote>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.1,
        "Solution_reading_time":6.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":930.0588888889,
        "Challenge_answer_count":0,
        "Challenge_body":"### Version\r\n\r\n22.11\r\n\r\n### Which installation method(s) does this occur on?\r\n\r\n_No response_\r\n\r\n### Describe the bug.\r\n\r\nai-engine fetch command at the 22.11 guide:\r\nhelm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus-ai-engine-**22.09**.tgz --username='$oauthtoken' --password=$API_KEY --untar\r\n\r\nhelm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus-sdk-client-22.09.tgz --username='$oauthtoken' --password=$API_KEY --untar\r\n\r\nhelm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus-mlflow-22.09.tgz --username='$oauthtoken' --password=$API_KEY --untar\r\n\r\n### Minimum reproducible example\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n_No response_\r\n\r\n### Full env printout\r\n\r\n_No response_\r\n\r\n### Other\/Misc.\r\n\r\n_No response_\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow Morpheus' Code of Conduct\r\n- [X] I have searched the [open bugs](https:\/\/github.com\/nv-morpheus\/Morpheus\/issues?q=is%3Aopen+is%3Aissue+label%3Abug) and have found no duplicates for this bug report",
        "Challenge_closed_time":1674883718000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671535506000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The issue is that failed ERT (External Resource Tool) subprocesses are not being registered correctly in mlflow. If the subprocess fails for any reason other than what is hard coded, a return code larger than 0 is ignored, resulting in a \"successful\" run in mlflow instead of a failed one.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/nv-morpheus\/Morpheus\/issues\/576",
        "Challenge_link_count":4,
        "Challenge_participation_count":0,
        "Challenge_readability":13.9,
        "Challenge_reading_time":14.29,
        "Challenge_repo_contributor_count":21.0,
        "Challenge_repo_fork_count":61.0,
        "Challenge_repo_issue_count":962.0,
        "Challenge_repo_star_count":176.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":930.0588888889,
        "Challenge_title":"[BUG]: Helm fetch command for ai-engine,sdk-helper and mlflow includes the 22.09 release instead of 22.11",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":100,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":788.5729277778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi Team,     <\/p>\n<p>When I Submit the Batch Inference Pipeline. It is working.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/158498-1-image-designer.png?platform=QnA\" alt=\"158498-1-image-designer.png\" \/>    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/158597-2-image-designer.png?platform=QnA\" alt=\"158597-2-image-designer.png\" \/>    <\/p>\n<p>After submitting, I can see the file:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/158519-3-image-designer.png?platform=QnA\" alt=\"158519-3-image-designer.png\" \/>    <\/p>\n<p>Then when I Publish, the file is not in the Datastore. The file is not generated again. I didn't get an error.    <\/p>\n<p>Kind regards,     <br \/>\nAnaid    <\/p>",
        "Challenge_closed_time":1642583796710,
        "Challenge_comment_count":2,
        "Challenge_created_time":1639744934170,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the AML Designer - Batch Inference Pipeline where the file generated after submitting is not available in the Datastore after publishing, without any error message.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/667479\/problem-aml-designer-batch-inference-pipeline",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":14.2,
        "Challenge_reading_time":10.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":788.5729277778,
        "Challenge_title":"Problem: AML Designer - Batch Inference Pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":67,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=4bb27b25-616e-491c-b986-136b5bf96f77\">@Anaid  <\/a>     <\/p>\n<p>Hi,    <\/p>\n<p>I\u2019ve enabled one-time Free Technical Support for you.  To create the support request, please do the following:     <\/p>\n<p>\u2022            Go to the Health Advisory section within the Azure Portal: <a href=\"https:\/\/aka.ms\/healthadvisories\">https:\/\/aka.ms\/healthadvisories<\/a>      <br \/>\n\u2022            Select the Issue Name &quot;You have been enabled for one-time Free Technical Support&quot;     <br \/>\n\u2022            Details will populate below in the Summary Tab within the reading pane and you can click on the link &quot;Create a Support Request&quot; to the right of the message    <\/p>\n<p>Let me know what your support request number is so that I can keep track of your case. If you run into any issues, feel free to let me know.    <\/p>\n<p>Regards,    <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":10.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":116.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1526004205792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"China",
        "Answerer_reputation_count":28087.0,
        "Answerer_view_count":3298.0,
        "Challenge_adjusted_solved_time":109.1988627778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have azure ml , I created compute for learning.\nCost for instance is 2-5usd with my use. But cost for p10(premium SSD) Disk 17usd.<\/p>\n<p>I don't know how change it because its not appear in azure Disk and in ML studio i cant find option for manage storage type for compute.<\/p>\n<p>Some one know how change it ?<\/p>",
        "Challenge_closed_time":1617778657403,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617385541497,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing difficulty in changing the disk type for Azure ML as the option to manage storage type for compute is not available in ML studio. The cost for the premium SSD disk is significantly higher than the regular disk, and the user is seeking assistance in changing it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66923216",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":2.6,
        "Challenge_reading_time":4.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":109.1988627778,
        "Challenge_title":"Change Disk Type Azure ML",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":150.0,
        "Challenge_word_count":62,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1561015783552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":97.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>There is no possible way to change the compute disk type if you use the Azure ML compute cluster and compute instance. Only when you use the extra computer, you can manage the separate resources such as the disk, network, and so on. For example, you attach a VM as the target computer to the Azure ML. Then when you create the VM you can set the disk type with HDD.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":4.44,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":71.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426093220648,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, India",
        "Answerer_reputation_count":1861.0,
        "Answerer_view_count":294.0,
        "Challenge_adjusted_solved_time":0.3459813889,
        "Challenge_answer_count":1,
        "Challenge_body":"<ol>\n<li><p>I want to write pytest unit test in <strong>Kedro 0.17.5<\/strong>. They need to perform integrity checks on dataframes created by the pipeline.\nThese dataframes are specified in the <code>catalog.yml<\/code> and already persisted successfully using <code>kedro run<\/code>. The <code>catalog.yml<\/code> is in <code>conf\/base<\/code>.<\/p>\n<\/li>\n<li><p>I have a test module <code>test_my_dataframe.py<\/code> in <code>src\/tests\/pipelines\/my_pipeline\/<\/code>.<\/p>\n<\/li>\n<\/ol>\n<p>How can I load the data catalog based on my <code>catalog.yml<\/code> programmatically from within <code>test_my_dataframe.py<\/code> in order to properly access my specified dataframes?<\/p>\n<p>Or, for that matter, how can I programmatically load the whole project context (including the data catalog) in order to also execute nodes etc.?<\/p>",
        "Challenge_closed_time":1656143487270,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656142241737,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to write pytest unit tests in Kedro 0.17.5 to perform integrity checks on dataframes created by the pipeline. The dataframes are specified in the catalog.yml and already persisted successfully using kedro run. The user is looking for a way to load the data catalog programmatically from within the test module to access the specified dataframes or load the whole project context to execute nodes.",
        "Challenge_last_edit_time":1656265981920,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72752043",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":11.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.3459813889,
        "Challenge_title":"Load existing data catalog programmatically",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":107.0,
        "Challenge_word_count":104,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1258185382660,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":333.0,
        "Poster_view_count":33.0,
        "Solution_body":"<ol>\n<li><p>For unit testing, we test just the function which we are testing, and everything external to the function we should mock\/patch. Check if you really need kedro project context while writing the unit test.<\/p>\n<\/li>\n<li><p>If you really need project context in test, you can do something like following<\/p>\n<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from kedro.framework.project import configure_project\nfrom kedro.framework.session import KedroSession\n\nwith KedroSession.create(package_name=&quot;demo&quot;, project_path=Path.cwd()) as session:\n    context = session.load_context()\n    catalog = context.catalog\n<\/code><\/pre>\n<p>or you can also create pytest fixture to use it again and again with scope of your choice.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@pytest.fixture\ndef get_project_context():\n    session = KedroSession.create(\n        package_name=&quot;demo&quot;,\n        project_path=Path.cwd()\n    )\n    _activate_session(session, force=True)\n    context = session.load_context()\n    return context\n<\/code><\/pre>\n<p>Different args supported by KedroSession create you can check it here <a href=\"https:\/\/kedro.readthedocs.io\/en\/0.17.5\/kedro.framework.session.session.KedroSession.html#kedro.framework.session.session.KedroSession.create\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/0.17.5\/kedro.framework.session.session.KedroSession.html#kedro.framework.session.session.KedroSession.create<\/a><\/p>\n<p>To read more about pytest fixture you can refer to <a href=\"https:\/\/docs.pytest.org\/en\/6.2.x\/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session\" rel=\"nofollow noreferrer\">https:\/\/docs.pytest.org\/en\/6.2.x\/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1656143791536,
        "Solution_link_count":4.0,
        "Solution_readability":19.5,
        "Solution_reading_time":23.58,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":135.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1544645236423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Chihuahua, Mexico",
        "Answerer_reputation_count":661.0,
        "Answerer_view_count":639.0,
        "Challenge_adjusted_solved_time":3.1209341667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a Google Vertex AI pipeline to query from a BigQuery table. In the pipeline, I am using the right project and the service account(which has bigquery.jobs.create access). But I see when it runs, it is accessing another project e1cd7306fb577e88gq-uq. I am not able to figure out where from this project is coming from. I am running the pipeline from Vertex AI user managed notebook<\/p>\n<pre><code>pandas_gbq.exceptions.GenericGBQException: Reason: 403 POST https:\/\/bigquery.googleapis.com\/bigquery\/v2\/projects\/e1cd7306fb577e88gq-uq\/jobs?prettyPrint=false: Access Denied: Project e1cd7306fb577e88gq-uq: User does not have bigquery.jobs.create permission in project e1cd7306fb577e88gq-uq.\n<\/code><\/pre>",
        "Challenge_closed_time":1650042428203,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650031192840,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while running a Google Vertex AI pipeline to query data from a BigQuery table. The pipeline is accessing another project instead of the intended project, and the user is unable to figure out where this project is coming from. The error message indicates that the user does not have the required permission to create jobs in the accessed project.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71884962",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":10.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":3.1209341667,
        "Challenge_title":"Vertex AI Pipeline is failing while trying to get data from BigQuery",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":525.0,
        "Challenge_word_count":99,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530457174832,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1043.0,
        "Poster_view_count":212.0,
        "Solution_body":"<p>The service agent or service account running your code does have the required permission, but your code is trying to access a resource in the wrong project. Due to the way Vertex AI runs your training code, this problem can occur inadvertently if you don't explicitly specify a project ID or project number in your code.<\/p>\n<p>You can explicitly select the project you want this way:<\/p>\n<pre><code>import os\n\nfrom google.cloud import bigquery\n\nproject_number = os.environ[&quot;CLOUD_ML_PROJECT_ID&quot;]\n\nclient = bigquery.Client(project=project_number)\n<\/code><\/pre>\n<p>You can read more about training code requirements <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/code-requirements#other-services\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.6,
        "Solution_reading_time":9.78,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":89.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":165.4480555556,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer wants to enforce these rules in their custom SageMaker containers:\n\n\t\u2022\tProcesses running inside a container must run with a known UID\/GUID and never as root.\n\t\u2022\tAvoid using privilege escalation methods that grant root access (e.g. sudo)\n\nHow do we ensure this?",
        "Challenge_closed_time":1608306337000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607710724000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The customer wants to ensure that processes running inside their custom SageMaker containers do not run as root and have a known UID\/GUID. They also want to avoid using privilege escalation methods that grant root access. The user is seeking guidance on how to enforce these rules.",
        "Challenge_last_edit_time":1667926252792,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUYAkZepq4SgyArKZCC7gT_A\/custom-container-not-running-under-root-account",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":3.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":165.4480555556,
        "Challenge_title":"Custom container not running under root account?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":206.0,
        "Challenge_word_count":49,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"SageMaker requires that Docker containers run without privileged access. See:  https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-toolkits.html\nSageMaker Docker containers do not run in Privileged mode and have the following Linux capabilities removed: SETPCAP, SETFCAP, NET_RAW, MKNOD",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1609348138779,
        "Solution_link_count":1.0,
        "Solution_readability":20.0,
        "Solution_reading_time":3.94,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1611181716003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":119.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":1654.0729555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a published Azure ML Pipeline that I am trying to trigger from an Automate Flow I have that triggers when users edit a document. Since I have the REST Endpoint for the Published Pipeline, I figured I should be able to make a POST request using the HTTP module available in Power Automate to trigger the pipeline.<\/p>\n<p>However, when I actually try this, I get an authentication error. I assume this is because I need to include some access token with the REST Endpoint, but I can't find any documentation that will tell me where to get that token from. Please note that I do not need to pass any data to the Pipeline, it handles its own data collection, I literally just need a way to trigger it.<\/p>\n<p>Does anybody know how to trigger a Published Azure ML Pipeline using the REST Endpoint? Does it make sense to use the HTTP module, or is there a better way to achieve this?<\/p>",
        "Challenge_closed_time":1630892629443,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624934847563,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to trigger an Azure ML Pipeline from a Power Automate Flow using the REST Endpoint, but is encountering an authentication error. They are unsure of how to obtain the necessary access token and are seeking advice on the best way to trigger the pipeline.",
        "Challenge_last_edit_time":1624937966803,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68172002",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":11.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1654.9394111111,
        "Challenge_title":"How to trigger Azure ML Pipeline from Power Automate",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":305.0,
        "Challenge_word_count":172,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1611181716003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":119.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>So I figured out how to do it by following the directions contained within this piece of Microsoft Documentation:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-rest\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-rest<\/a><\/p>\n<p>Specifically, it required performing two of the calls in the documentation;<\/p>\n<ul>\n<li>The first to get an AAD token using an Azure Service Principle that is authorised to access the Machine Learning Instance.<\/li>\n<\/ul>\n<blockquote>\n<p>curl -X POST <a href=\"https:\/\/login.microsoftonline.com\/\" rel=\"nofollow noreferrer\">https:\/\/login.microsoftonline.com\/<\/a>\/oauth2\/token -d &quot;grant_type=client_credentials&amp;resource=https%3A%2F%2Fmanagement.azure.com%2F&amp;client_id=&amp;client_secret=&quot;<\/p>\n<\/blockquote>\n<ul>\n<li>The second to use this token to trigger your pipeline from its rest endpoint. This one I had to figure out myself a little, but below is the basic structure I used.<\/li>\n<\/ul>\n<blockquote>\n<p>curl -X POST {PIPELINE_REST_ENDPOINT} -H &quot;Authorisation:Bearer {AAD_TOKEN}&quot; -H &quot;Content-Type: application\/json&quot; -d &quot;{&quot;ExperimentName&quot;: &quot;{EXPERIMENT_NAME}&quot;,&quot;ParameterAssignments&quot;: {}}&quot;<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":17.8,
        "Solution_reading_time":17.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":118.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":161.8254016667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hei, I'm trying to build a pipeline including a HyperdriveStep to tuen the hyperparameters.  <br \/>\nThe pipeline should later on run automatically and be tuned at each pipeline run.<\/p>\n<p>The pipeline consists of three steps: a preparation step resulting in a PipelineData Object, the HyperdriveStep and a final PythonRegisterStep, where the best model should be registered.<\/p>\n<p>However, when creating the pipeline object I'm getting an error I can not relate to.<\/p>\n<p>Traceback (most recent call last):<\/p>\n<pre><code>      File &quot;\/Users\/xxx\/Desktop\/azure_test\/pipeline-folder\/azure_pipeline_wrapper1.py&quot;, line 168, in &lt;module&gt;\n        pipeline = Pipeline(workspace=ws, steps=pipeline_steps, description=&quot;Pipeline for hyperparameter tuning&quot;)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/core\/_experiment_method.py&quot;, line 104, in wrapper\n        return init_func(self, *args, **kwargs)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/pipeline.py&quot;, line 177, in __init__\n        self._graph = self._graph_builder.build(self._name, steps, finalize=False)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py&quot;, line 1481, in build\n        graph = self.construct(name, steps)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py&quot;, line 1503, in construct\n        self.process_collection(steps)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py&quot;, line 1539, in process_collection\n        builder.process_collection(collection)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py&quot;, line 1830, in process_collection\n        self._base_builder.process_collection(item)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py&quot;, line 1533, in process_collection\n        return self.process_step(collection)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py&quot;, line 1577, in process_step\n        node = step.create_node(self._graph, self._default_datastore, self._context)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/steps\/hyper_drive_step.py&quot;, line 270, in create_node\n        hyperdrive_config, reuse_hashable_config = self._get_hyperdrive_config(context._workspace,\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/steps\/hyper_drive_step.py&quot;, line 346, in _get_hyperdrive_config\n        hyperdrive_dto = _search._create_experiment_dto(self._hyperdrive_config, workspace,\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/train\/hyperdrive\/_search.py&quot;, line 38, in _create_experiment_dto\n        platform_config = hyperdrive_config._get_platform_config(workspace, experiment_name, **kwargs)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/train\/hyperdrive\/runconfig.py&quot;, line 672, in _get_platform_config\n        platform_config.update(self._get_platform_config_data_from_run_config(workspace))\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/train\/hyperdrive\/runconfig.py&quot;, line 686, in _get_platform_config_data_from_run_config\n        run_config = get_run_config_from_script_run(self.run_config)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/core\/script_run_config.py&quot;, line 84, in get_run_config_from_script_run\n        run_config.arguments = deepcopy(script_run_config.arguments)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py&quot;, line 146, in deepcopy\n        y = copier(x, memo)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py&quot;, line 205, in _deepcopy_list\n        append(deepcopy(a, memo))\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py&quot;, line 172, in deepcopy\n        y = _reconstruct(x, memo, *rv)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py&quot;, line 270, in _reconstruct\n        state = deepcopy(state, memo)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py&quot;, line 146, in deepcopy\n        y = copier(x, memo)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py&quot;, line 230, in _deepcopy_dict\n        y[deepcopy(key, memo)] = deepcopy(value, memo)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py&quot;, line 172, in deepcopy\n        y = _reconstruct(x, memo, *rv)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py&quot;, line 270, in _reconstruct\n        state = deepcopy(state, memo)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py&quot;, line 146, in deepcopy\n        y = copier(x, memo)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py&quot;, line 230, in _deepcopy_dict\n        y[deepcopy(key, memo)] = deepcopy(value, memo)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py&quot;, line 172, in deepcopy\n        y = _reconstruct(x, memo, *rv)\n\n      File &quot;\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py&quot;, line 264, in _reconstruct\n        y = func(*args)\n\n      File &quot;\/Users\/xxxr\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copyreg.py&quot;, line 91, in __newobj__\n        return cls.__new__(cls, *args)\n\n    TypeError: __new__() missing 2 required positional arguments: 'workspace' and 'name'\n<\/code><\/pre>\n<p>My Code:<\/p>\n<pre><code># Connect to workspace \nws = Workspace.from_config()\nprint(ws.name, &quot;loaded&quot;)\n\n# Set compute target\ncluster_name = &quot;compcluster234&quot;\npipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n\n# Create new environment\nsklearn_env = Environment(&quot;sklearn_env&quot;)\n# Adds dependencies to PythonSection of sklaern_env\nenv_packages = CondaDependencies.create(conda_packages=['scikit-learn'])\nsklearn_env.docker.enabled = True\nsklearn_env.python.conda_dependencies = env_packages\n# Register the environment\nsklearn_env.register(workspace=ws)\n\n# =============================================================================\n# Run Configuration\n# =============================================================================\n\n# Create Run configuration \n# Pipeline_folder\npipeline_folder = path + '\/pipeline-folder'\n# Create a new runconfig object for the pipeline\npipeline_run_config = RunConfiguration()\n# Use the compute you created above. \npipeline_run_config.target = pipeline_cluster\n# Assign the environment to the run configuration\n# In comparison to the ScriptRunCnfig object, the RunConfig is more generous\npipeline_run_config.environment = sklearn_env\nprint (&quot;Run configuration created.&quot;)\n\n# =============================================================================\n# DataPath\n# =============================================================================\n\n# Get the default datastore\ndefault_ds = ws.get_default_datastore()\n# Create a DataPath object \ndatapath = DataPath(datastore = default_ds,\n                     path_on_datastore = 'cancer-data')\n# Make the datapath a PipelineParameter\ndatapath_pipeline_param = PipelineParameter(name='input-data',   \n                                            default_value=datapath)\ndatapath_input = (datapath_pipeline_param, \n                   DataPathComputeBinding(mode = 'mount'))\n\n# =============================================================================\n# PipelineData\n# =============================================================================\n\n# Create a PipelineData (temporary Data Reference) for the preppared data folder\nprepped_data_folder = PipelineData(name=&quot;prepped_data_folder&quot;,\n                                   datastore=ws.get_default_datastore())\n\n# Create PipelineData objects for the Metrics and the saved model\nmetrics_output_name = 'metrics_output'\nmetrics_data = PipelineData(name='metrics_data',\n                            datastore=default_ds,\n                            pipeline_output_name=metrics_output_name,\n                            training_output=TrainingOutput(&quot;Metrics&quot;))\n\nmodel_output_name = 'model_output'\nsaved_model = PipelineData(name='saved_model',\n                           datastore=default_ds,\n                           pipeline_output_name=model_output_name,\n                           training_output=TrainingOutput(&quot;Model&quot;,\n                                                          model_file=&quot;outputs\/model\/cancer_model.pkl&quot;))\n\n# =============================================================================\n# Pipeline Steps\n# =============================================================================\n\n# Step 1, Run the data prep script\nprep_step = PythonScriptStep(name = &quot;prepare_data&quot;,\n                                source_directory = pipeline_folder,\n                                script_name = &quot;cancer_pipeline_preprocessing.py&quot;,\n                                arguments = ['--input-data', datapath_input,\n                                             '--prepped-data', prepped_data_folder],\n                                inputs=[datapath_input],\n                                outputs=[prepped_data_folder],\n                                compute_target = pipeline_cluster,\n                                runconfig = pipeline_run_config,\n                                allow_reuse = False)\n\n# Define the search strategy and parameter space for hyperparameter tuning\nps = GridParameterSampling({ '--max_depth': choice(1,2,3)})\n# Define a early stopping criteria\nearly_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n# Define a ScriptRunConfig for the Training script\n# The ScriptRunConfig is based on the RunConfig of the Pipeline\nscript_run_config = ScriptRunConfig(script=&quot;cancer_pipeline_tuning.py&quot;,\n                                    source_directory=pipeline_folder,\n                                    # Add non-hyperparameter arguments -in this case, the training dataset\n                                    arguments = ['--training_folder', prepped_data_folder],\n                                    run_config=pipeline_run_config)\n# Define a HyperDriveConfiguration\n# The primary_metric_name must be completely idential to the metric name logged during training (inside the training script)\nhd_config = HyperDriveConfig(run_config=script_run_config, \n                             hyperparameter_sampling=ps,\n                             policy=early_termination_policy,\n                             primary_metric_name='Accuracy', \n                             primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                             max_total_runs=3,\n                             max_concurrent_runs=2)\n\n# Step 2b, define a HyperDriveStep\n# HyperDriveStep can be used to run HyperDrive job as a step in pipeline.\n# No arguments need to be set as they are already set inside the ScriptRunConfig\nhyperdrive_step = HyperDriveStep(name=&quot;tune_hyperparameters&quot;,\n                                 hyperdrive_config=hd_config,\n                                 inputs=[prepped_data_folder],\n                                 outputs=[metrics_data, saved_model])\n\nhyperdrive_step.run_after(prep_step)    \n\n# Step 3, Run the model registration step\nregister_step = PythonScriptStep(name=&quot;register_model&quot;,\n                                       script_name='cancer_pipeline_register1.py',\n                                       source_directory = pipeline_folder,\n                                       arguments=[&quot;--saved_model&quot;, saved_model],\n                                       inputs=[saved_model],\n                                       compute_target = pipeline_cluster,\n                                       runconfig=pipeline_run_config,\n                                       allow_reuse = False)\n\nregister_step.run_after(hyperdrive_step)    \nprint(&quot;Pipeline steps defined&quot;)\n\n\n# Construct the pipeline\npipeline_steps = [prep_step, hyperdrive_step, register_step]\npipeline = Pipeline(workspace=ws, steps=pipeline_steps, description=&quot;Pipeline for hyperparameter tuning&quot;)\nprint(&quot;Pipeline is built.&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1622098105463,
        "Challenge_comment_count":2,
        "Challenge_created_time":1621515534017,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to build a pipeline that includes a HyperdriveStep to tune hyperparameters. The pipeline consists of three steps: a preparation step, a HyperdriveStep, and a final PythonRegisterStep. However, the user is encountering an error while creating the pipeline object. The error message suggests that there is a missing positional argument in the __new__() function.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/403018\/pipeline-can-not-be-built-using-a-hyperdrivestep-i",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":22.8,
        "Challenge_reading_time":150.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":66,
        "Challenge_solved_time":161.8254016667,
        "Challenge_title":"Pipeline can not be built using a HyperdriveStep inside a Pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":701,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Solved the issue!  <\/p>\n<p>Had to remove the <strong>arguments<\/strong>  argument of the ScriptRunConfig and instead set the values to the Hyperdrive Steps <strong>estimator_entry_script_arguments<\/strong> argument.  <\/p>\n<pre><code># Step 1, Run the data prep script\nprep_step = PythonScriptStep(name = &quot;prepare_data&quot;,\n                                source_directory = pipeline_folder,\n                                script_name = &quot;cancer_pipeline_preprocessing.py&quot;,\n                                arguments = ['--input-data', datapath_input,\n                                             '--prepped-data', prepped_data_folder],\n                                inputs=[datapath_input],\n                                outputs=[prepped_data_folder],\n                                compute_target = pipeline_cluster,\n                                runconfig = pipeline_run_config,\n                                allow_reuse=False)\n\n# Define the search strategy and parameter space for hyperparameter tuning\nps = GridParameterSampling({'--max_depth': choice(1,2,3),\n                            '--n_estimators': choice(100,300)})\n# Define a early stopping criteria\nearly_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n# Define a ScriptRunConfig for the Training script\n# The ScriptRunConfig is based on the RunConfig of the Pipeline\nscript_run_config = ScriptRunConfig(script=&quot;cancer_pipeline_tuning.py&quot;,\n                                    source_directory=pipeline_folder,\n                                    run_config=pipeline_run_config)\n# Define a HyperDriveConfiguration\n# The primary_metric_name must be completely idential to the metric name logged during training (inside the training script)\nhd_config = HyperDriveConfig(run_config=script_run_config, \n                             hyperparameter_sampling=ps,\n                             policy=None,\n                             primary_metric_name=&quot;Accuracy&quot;, \n                             primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                             max_total_runs=6,\n                             max_concurrent_runs=2)\n\n# Step 2b, define a HyperDriveStep\n# HyperDriveStep can be used to run HyperDrive job as a step in pipeline.\n# No arguments need to be set as they are already set inside the ScriptRunConfig\nhyperdrive_step = HyperDriveStep(name=&quot;tune_hyperparameters&quot;,\n                                 hyperdrive_config=hd_config,\n                                 # Add non-hyperparameter arguments -in this case, the training dataset\n                                 # IMPORTANT: Don't add them already in the ScriptRunConfig\n                                 estimator_entry_script_arguments=['--training_folder', prepped_data_folder],\n                                 inputs=[prepped_data_folder],\n                                 outputs=[metrics_data, saved_model],\n                                 allow_reuse=False)\n<\/code><\/pre>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":25.2,
        "Solution_reading_time":29.16,
        "Solution_score_count":5.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":183.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1610703423912,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":1452.9271638889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using SageMaker pipeline to do inference on test data. The Pipeline uses a SKLearn perprocessor and a XGBoost model. The pipeline works fine on data without an ID column. However, when I try to include an ID column to track the predictions, it fails. I have given the code snippets below.<\/p>\n<pre><code>import sagemaker\nfrom sagemaker.predictor import json_serializer, csv_serializer, json_deserializer\n\ninput_data_path = 's3:\/\/batch-transform\/input-data\/validation_data.csv'\noutput_data_path = 's3:\/\/batch-transform\/predictions\/'\n\ntransform_job = sagemaker.transformer.Transformer(\n    model_name = model_name,\n    instance_count = 1,\n    instance_type = 'ml.m4.xlarge',\n    strategy = 'MultiRecord',\n    assemble_with = 'Line',\n    output_path = output_data_path,\n    base_transform_job_name='pipeline_with_id',\n    sagemaker_session=sagemaker.Session(),\n    accept = 'text\/csv')\n\ntransform_job.transform(data = input_data_path,\n                        content_type = 'text\/csv', \n                        split_type = 'Line',\n                        input_filter='$[1:]', \n                        join_source='Input')\n                        output_filter='$[0,-1]')\n<\/code><\/pre>\n<p>This results in the following error:<\/p>\n<pre><code>Fail to join data: mismatched line count between the input and the output\n<\/code><\/pre>\n<p>I am following the example given in this page:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/<\/a><\/p>\n<p>Can someone provide pointers to what is causing the error? Thank you<\/p>",
        "Challenge_closed_time":1610703448710,
        "Challenge_comment_count":3,
        "Challenge_created_time":1605472910920,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while using SageMaker Batch Transform to do inference on test data. The pipeline works fine on data without an ID column, but when an ID column is included to track the predictions, it fails with a \"mismatched line count\" error. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64849557",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":17.7,
        "Challenge_reading_time":22.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1452.9271638889,
        "Challenge_title":"SageMaker Batch Transform fails with ID Column",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1080.0,
        "Challenge_word_count":144,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1319019150600,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3073.0,
        "Poster_view_count":341.0,
        "Solution_body":"<p>Came across the same issue.<\/p>\n<p>Check the number of rows returned after prediction in your serving code. In my case, my prediction output didn't have a column header.<\/p>\n<p>e.g. As a text\/csv response, using batch transform with join will post join the input &amp; output.<\/p>\n<p>A single input record would be [[&quot;feature_1&quot;, &quot;feature_2&quot;],[0, 1]], while my model predicted output returned [1].<\/p>\n<p>add column name to predicted output like this [&quot;result&quot;, 1] then returning the csv result will yield [[&quot;result&quot;],[1]] matching input.<\/p>\n<p>P.S. you may need to find a scalable way of doing this for multi-row  batch. Not sure.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":8.53,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":98.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1567164934556,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":15.1199388889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using \"studio (preview)\" from Microsoft Azure Machine Learning to create a pipeline that applies machine learning to a dataset in a blob storage that is connected to our data warehouse.<\/p>\n\n<p>In the \"Designer\", an \"Exectue R Script\" action can be added to the pipeline. I'm using this functionality to execute some of my own machine learning algorithms.<\/p>\n\n<p>I've got a 'hello world' version of this script working (including using the \"script bundle\" to load the functions in my own R files). It applies a very simple manipulation (compute the days difference with the date in the date column and 'today'), and stores the output as a new file. Given that the exported file has the correct information, I know that the R script works well.<\/p>\n\n<p>The script looks like this:<\/p>\n\n<pre><code># R version: 3.5.1\n# The script MUST contain a function named azureml_main\n# which is the entry point for this module.\n\n# The entry point function can contain up to two input arguments:\n#   Param&lt;medals&gt;: a R DataFrame\n#   Param&lt;matches&gt;: a R DataFrame\n\nazureml_main &lt;- function(dataframe1, dataframe2){\n\n  message(\"STARTING R script run.\")\n\n  # If a zip file is connected to the third input port, it is\n  # unzipped under \".\/Script Bundle\". This directory is added\n  # to sys.path.\n\n  message('Adding functions as source...')\n\n  if (FALSE) {\n    # This works...\n      source(\".\/Script Bundle\/first_function_for_script_bundle.R\")\n  } else {\n    # And this works as well!\n    message('Sourcing all available functions...')\n    functions_folder = '.\/Script Bundle'\n\n    list.files(path = functions_folder)\n    list_of_R_functions &lt;- list.files(path = functions_folder, pattern = \"^.*[Rr]$\", include.dirs = FALSE, full.names = TRUE)\n    for (fun in list_of_R_functions) {\n\n      message(sprintf('Sourcing &lt;%s&gt;...', fun))\n\n      source(fun)\n\n    }\n  }\n\n  message('Executing R pipeline...')\n  dataframe1 = calculate_days_difference(dataframe = dataframe1)\n\n  # Return datasets as a Named List\n  return(list(dataset1=dataframe1, dataset2=dataframe2))\n}\n<\/code><\/pre>\n\n<p>And although I do print some messages in the R Script, I haven't been able to find the \"stdoutlogs\" nor the \"stderrlogs\" that should contain these printed messages.<\/p>\n\n<p>I need the printed messages for 1) information on how the analysis went and -most importantly- 2) debugging in case the code failed.<\/p>\n\n<p>Now, I have found (on multiple locations) the files \"stdoutlogs.txt\" and \"stderrlogs.txt\". These can be found under \"Logs\" when I click on \"Exectue R Script\" in the \"Designer\".\nI can also find \"stdoutlogs.txt\" and \"stderrlogs.txt\" files under \"Experiments\" when I click on a finished \"Run\" and then both under the tab \"Outputs\" and under the tab \"Logs\".\nHowever... all of these files are empty.<\/p>\n\n<p>Can anyone tell me how I can print messages from my R Script and help me locate where I can find the printed information?<\/p>",
        "Challenge_closed_time":1579829908723,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579792546180,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using \"studio (preview)\" from Microsoft Azure Machine Learning to create a pipeline that applies machine learning to a dataset in a blob storage that is connected to their data warehouse. They are using the \"Execute R Script\" action to execute their own machine learning algorithms. Although they print some messages in the R Script, they cannot find the \"stdoutlogs\" nor the \"stderrlogs\" that should contain these printed messages. They need the printed messages for debugging in case the code failed. The user has found the files \"stdoutlogs.txt\" and \"stderrlogs.txt\" in multiple locations, but all of these files are empty. They are seeking help to locate where they can find the printed information.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59881727",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":36.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":10.3784841667,
        "Challenge_title":"Debugging R Scripts in azure-ml: Where can stdout and stderr logs be found? (or why are they empty?)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":287.0,
        "Challenge_word_count":415,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1534511592567,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherlands",
        "Poster_reputation_count":423.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>Can you please click on the \"Execute R module\" and download the 70_driver.log? I tried message(\"STARTING R script run.\") in an R sample and can found the output there.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z7s7h.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z7s7h.png\" alt=\"view logs for a execute R script module\"><\/a><\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1579846977960,
        "Solution_link_count":2.0,
        "Solution_readability":7.2,
        "Solution_reading_time":4.55,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1881.8983333333,
        "Challenge_answer_count":0,
        "Challenge_body":"# Context\r\nToday, you can execute a kedro pipeline interactively. The logic would be to load the context, and then to run the pipeline.\r\n\r\n```python\r\nfrom kedro.context import load_context\r\nlocal_context = load_context(\".\")\r\nlocal_context.run(pipeline=local_context.pipelines[PIPELINE_NAME],\r\n                             catalog=local_context.catalog)\r\n```\r\n\r\n# Description\r\nIf the execution fails for some reason (bug in the pipeline), the mlflow run is not closed. This creates unintended side effects: for instance, if you rerun the pipeline, the new run will be nested in the failing runs and the mllflow database will become very messy.\r\n\r\nThis bug does not occur when running from the command line since the mlflow run is automatically closed when exiting.\r\n\r\n# Possible Implementation \r\nImplement a [``on_pipeline_error`` kedro ``Hook``](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/15_hooks.html?highlight=on_pipeline_error#hook-specification) to close the mlflow run when the pipeline fails.",
        "Challenge_closed_time":1598336871000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591562037000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue where the mlflow run is not closed when a pipeline fails in interactive mode, leading to unintended side effects and a messy mlflow database. The suggested solution is to implement an \"on_pipeline_error\" kedro hook to automatically close the mlflow run when the pipeline fails.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/10",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":10.6,
        "Challenge_reading_time":13.06,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":21.0,
        "Challenge_repo_issue_count":414.0,
        "Challenge_repo_star_count":145.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1881.8983333333,
        "Challenge_title":"Close mlflow run when a pipeline fails in interactive mode",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":126,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1548188011640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bellevue, WA, USA",
        "Answerer_reputation_count":131.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":6.7539119444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I\u2019m building out a pipeline that should execute and train fairly frequently.  I\u2019m following this: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline<\/a> <\/p>\n\n<p>Anyways, I\u2019ve got a stream analytics job dumping telemetry into .json files on blob storage (soon to be adls gen2).  Anyways, I want to find all .json files and use all of those files to train with.  I could possibly use just new .json files as well (interesting option honestly).<\/p>\n\n<p>Currently I just have the store mounted to a data lake and available; and it just iterates the mount for the data files and loads them up.<\/p>\n\n<ol>\n<li>How can I use data references for this instead?<\/li>\n<li>What does data references do for me that mounting time stamped data does not?\na.  From an audit perspective, I have version control, execution time and time stamped read only data.  Albeit, doing a replay on this would require additional coding, but is do-able.<\/li>\n<\/ol>",
        "Challenge_closed_time":1566854588780,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566830274697,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is building a pipeline to execute and train frequently using Azure ML SDK DataReference. They have a stream analytics job that dumps telemetry into .json files on blob storage and want to find all .json files to train with. They are looking for information on how to use data references instead of mounting time-stamped data and the benefits it provides from an audit perspective.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57660058",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":7.4,
        "Challenge_reading_time":14.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":6.7539119444,
        "Challenge_title":"Azure ML SDK DataReference - File Pattern - MANY files",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":296.0,
        "Challenge_word_count":159,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384802035143,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Miami Beach, FL",
        "Poster_reputation_count":2682.0,
        "Poster_view_count":1006.0,
        "Solution_body":"<p>You could pass pointer to folder as an input parameter for the pipeline, and then your step can mount the folder to iterate over the json files.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":1.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":27.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.361325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I don't know where else to ask this question so would appreciate any help or feedback. I've been reading the SDK documentation for azure machine learning service (in particular <code>azureml.core<\/code>). There's a class called <code>Pipeline<\/code> that has methdods <code>validate()<\/code> and <code>publish()<\/code>. Here are the docs for this:<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py<\/a><\/p>\n<p>When I call <code>validate()<\/code>, everything validates and I call publish but it seems to only create an API endpoint in the workspace, it doesn't register my pipeline under Pipelines and there's obviously nothing in the designer.<\/p>\n<p>My question: I want to publish my pipeline so I just have to launch from the workspace with one click. I've built it already using the SDK (Python code). I don't want to work with an API. Is there any way to do this or would I have to rebuild the entire pipeline using the designer (drag and drop)?<\/p>",
        "Challenge_closed_time":1595468157440,
        "Challenge_comment_count":1,
        "Challenge_created_time":1595466856670,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has built a pipeline in Python using the SDK for Azure Machine Learning Service and is trying to publish it to the workspace. They have used the Pipeline class with the validate() and publish() methods, but the pipeline is not registering under Pipelines and is only creating an API endpoint. The user wants to know if there is a way to publish the pipeline so that it can be launched from the workspace with one click, without having to rebuild the entire pipeline using the designer.",
        "Challenge_last_edit_time":1595544102140,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63045395",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":16.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.361325,
        "Challenge_title":"Machine learning in Azure: How do I publish a pipeline to the workspace once I've already built it in Python using the SDK?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":739.0,
        "Challenge_word_count":168,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1595292020127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Totally empathize with your confusion. Our team has been working with Azure ML pipelines for quite some time but <code>PublishedPipelines<\/code> still confused me initially because:<\/p>\n<ul>\n<li>what the SDK calls a <code>PublishedPipeline<\/code> is called as a <code>Pipeline Endpoint<\/code> in the Studio UI, and<\/li>\n<li>it is semi-related to <code>Dataset<\/code> and <code>Model<\/code>'s <code>.register()<\/code> method, but fundamentally different.<\/li>\n<\/ul>\n<p><code>TL;DR<\/code>: all <code>Pipeline.publish()<\/code> does is create an endpoint that you can use to:<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb\" rel=\"nofollow noreferrer\">schedule<\/a> and <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb\" rel=\"nofollow noreferrer\">version<\/a> Pipelines, and<\/li>\n<li>re-run the pipeline from other services via a REST API call (e.g. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/transform-data-machine-learning-service\" rel=\"nofollow noreferrer\">via Azure Data Factory<\/a>).<\/li>\n<\/ul>\n<p>You can see <code>PublishedPipelines<\/code> in the Studio UI in two places:<\/p>\n<ul>\n<li>Pipelines page :: Pipeline Endpoints tab<\/li>\n<li>Endpoints page :: Pipeline Endpoints tab<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/UQ6RS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/UQ6RS.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1595468750207,
        "Solution_link_count":5.0,
        "Solution_readability":17.5,
        "Solution_reading_time":22.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0377777778,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nHi, I have a quick question, is it possible to filter all jobs that requested\/accessed a specific connection?\n\nTo explain my use-case, we detected an issue with some data, and we would like to assess how many jobs and how far in the past that data was used in our training jobs.",
        "Challenge_closed_time":1649676909000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649676773000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a way to filter all jobs that accessed a specific connection in order to assess how many jobs and how far in the past that data was used in their training jobs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1487",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":9.6,
        "Challenge_reading_time":4.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0377777778,
        "Challenge_title":"How to filter all jobs that accessed a connection, a dataset, or an artifact",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":67,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"V1.18 the following filters, or any combination, will be possible:\n\nBy connection name connections.name: CONNECTION1 | CONNECTION2\nBy connection tag connections.tags: TAG1 | TAG2\nBy connection kind connections.kind: git or connections.kind: KIND1 | KIND2\nBy artifact name artifacts.name: LINEAGE1 | LINEAGE2\nBy artifact kind artifacts.kind: model or artifacts.kind: KIND1 | KIND2\nBy artifact path artifacts.path: foo\/bar\nBy artifact state artifacts.state: STATE",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":5.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":56.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":0.2689961111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to organize the node functions by Classes in the nodes.py file. For example, functions related to cleaning data are in the \"CleanData\" Class, with a @staticmethod decorator, while other functions will stay in the \"Other\" Class, without any decorator (the names of these classes are merely representative). In the pipeline file, I tried importing the names of the classes, the names of the nodes and the following way: CleanData.function1 (which gave an error) and none of them worked. How can I call the nodes from the classes, if possible, please?<\/p>",
        "Challenge_closed_time":1573209953536,
        "Challenge_comment_count":2,
        "Challenge_created_time":1573208985150,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to organize node functions by classes in the nodes.py file, with some functions having a @staticmethod decorator and others without any decorator. However, the user is facing challenges in calling the nodes from the classes in the pipeline file. The user is seeking guidance on how to call the nodes from the classes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58764792",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.7,
        "Challenge_reading_time":7.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.2689961111,
        "Challenge_title":"How to run functions from a Class in the nodes.py file?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":289.0,
        "Challenge_word_count":102,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572973807452,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>I'm not entirely certain what the error you're getting is. If you're literally trying to do <code>from .nodes import CleanData.function1<\/code> that won't work. Imports don't work like that in Python. If you do something like this:<\/p>\n\n<p><code>nodes.py<\/code> has:<\/p>\n\n<pre><code>class CleanData:\n    def clean(arg1):\n        pass\n<\/code><\/pre>\n\n<p>and <code>pipeline.py<\/code> has:<\/p>\n\n<pre><code>from kedro.pipeline import Pipeline, node\nfrom .nodes import CleanData\n\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                CleanData.clean,\n                \"example_iris_data\",\n                None,\n            )\n        ]\n    )\n<\/code><\/pre>\n\n<p>that should work.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":7.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":68.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.3581641667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created and published a Azure ML pipeline. I want to trigger the ML pipeline from Azure Data Factory.  <\/p>\n<p>In ADF, i have chosen Machine learning execute pipeline and created the linked service to azure machine learning and able to choose the published pipeline endpoint. However while running, i am getting the below error. I couldn't find much information how to resolve the error.   <\/p>\n<p>&quot;Convert Failed. The value type 'System.String', in key 'azureCloudType' is not expected type 'Microsoft.DataTransfer.Common.Models.AzureCloudType&quot;<\/p>",
        "Challenge_closed_time":1645111953528,
        "Challenge_comment_count":10,
        "Challenge_created_time":1645081864137,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to trigger an Azure ML pipeline from Azure Data Factory. They have created and published the pipeline and created a linked service to Azure Machine Learning, but are receiving an error message stating that the value type 'System.String' in key 'azureCloudType' is not the expected type 'Microsoft.DataTransfer.Common.Models.AzureCloudType'. The user is seeking information on how to resolve this error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/739240\/trigger-azure-ml-pipeline-from-azure-data-factory",
        "Challenge_link_count":0,
        "Challenge_participation_count":12,
        "Challenge_readability":8.3,
        "Challenge_reading_time":7.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":8.3581641667,
        "Challenge_title":"Trigger Azure ML Pipeline from Azure Data Factory",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":88,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=0cab9490-181d-4799-9dfb-834a723c261c\">@Vinoth Kumar K  <\/a> ,    <br \/>\nWelcome to Microsoft Q&amp;A platform and thankyou for posting your query.     <br \/>\nAs per the details you have shared in the query, it looks like a product bug. I have raised this issue with the internal Product team. Once I hear back from them, I will keep everyone posted on this. Thanks for your patience!<\/p>\n",
        "Solution_comment_count":14.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.9,
        "Solution_reading_time":5.11,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":62.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1535502574980,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Scottsdale, AZ, USA",
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":57.7218555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a rule of thumb for how to choose the number of epochs per trial in <a href=\"https:\/\/optuna.org\/\" rel=\"nofollow noreferrer\">Optuna<\/a>?<\/p>",
        "Challenge_closed_time":1612636645883,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612428847203,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to determine the appropriate number of epochs per trial in Optuna.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66042246",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":2.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":57.7218555556,
        "Challenge_title":"How can I choose the right number of epochs per trial in Optuna?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":253.0,
        "Challenge_word_count":33,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1593039974520,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":235.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>I would imagine epochs are directly correlated with your computational costs, but perhaps that's a parameter worth optimizing. If you aren't sure, start with your best guess and run a few optimization studies with different epoch values. Once you confirm the importance of your epochs, you can conduct separate studies on just the epoch value.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":4.35,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1547398724312,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":667.1184413889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have completed a labelling job in AWS ground truth and started working on the notebook template for object detection.<\/p>\n\n<p>I have 2 manifests which has 293 labeled images for birds in a train and validation set like this:<\/p>\n\n<pre><code>{\"source-ref\":\"s3:\/\/XXXXXXX\/Train\/Blackbird_1.JPG\",\"Bird-Label-Train\":{\"workerId\":XXXXXXXX,\"imageSource\":{\"s3Uri\":\"s3:\/\/XXXXXXX\/Train\/Blackbird_1.JPG\"},\"boxesInfo\":{\"annotatedResult\":{\"boundingBoxes\":[{\"width\":1612,\"top\":841,\"label\":\"Blackbird\",\"left\":1276,\"height\":757}],\"inputImageProperties\":{\"width\":3872,\"height\":2592}}}},\"Bird-Label-Train-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"bird-label-train\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-16T17:28:23+0000\"}}\n<\/code><\/pre>\n\n<p>Below are the parameters I am using for the notebook instance:<\/p>\n\n<pre><code>training_params = \\\n{\n    \"AlgorithmSpecification\": {\n        \"TrainingImage\": training_image, # NB. This is one of the named constants defined in the first cell.\n        \"TrainingInputMode\": \"Pipe\"\n    },\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": s3_output_path\n    },\n    \"ResourceConfig\": {\n        \"InstanceCount\": 1,   \n        \"InstanceType\": \"ml.p3.2xlarge\",\n        \"VolumeSizeInGB\": 5\n    },\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": { # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n         \"base_network\": \"resnet-50\",\n         \"use_pretrained_model\": \"1\",\n         \"num_classes\": \"1\",\n         \"mini_batch_size\": \"16\",\n         \"epochs\": \"5\",\n         \"learning_rate\": \"0.001\",\n         \"lr_scheduler_step\": \"3,6\",\n         \"lr_scheduler_factor\": \"0.1\",\n         \"optimizer\": \"rmsprop\",\n         \"momentum\": \"0.9\",\n         \"weight_decay\": \"0.0005\",\n         \"overlap_threshold\": \"0.5\",\n         \"nms_threshold\": \"0.45\",\n         \"image_shape\": \"300\",\n         \"label_width\": \"350\",\n         \"num_training_samples\": str(num_training_samples)\n    },\n    \"StoppingCondition\": {\n        \"MaxRuntimeInSeconds\": 86400\n    },\n \"InputDataConfig\": [\n    {\n        \"ChannelName\": \"train\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                \"S3Uri\": s3_train_data_path,\n                \"S3DataDistributionType\": \"FullyReplicated\",\n                \"AttributeNames\": [\"source-ref\",\"Bird-Label-Train\"] # NB. This must correspond to the JSON field names in your augmented manifest.\n            }\n        },\n        \"ContentType\": \"image\/jpeg\",\n        \"RecordWrapperType\": \"None\",\n        \"CompressionType\": \"None\"\n    },\n    {\n        \"ChannelName\": \"validation\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                \"S3Uri\": s3_validation_data_path,\n                \"S3DataDistributionType\": \"FullyReplicated\",\n                \"AttributeNames\": [\"source-ref\",\"Bird-Label\"] # NB. This must correspond to the JSON field names in your augmented manifest.\n            }\n        },\n        \"ContentType\": \"image\/jpeg\",\n        \"RecordWrapperType\": \"None\",\n        \"CompressionType\": \"None\"\n    }\n]\n<\/code><\/pre>\n\n<p>I would end up with this being printed after running my ml.p3.2xlarge instance:<\/p>\n\n<pre><code>InProgress Starting\nInProgress Starting\nInProgress Starting\nInProgress Training\nFailed Failed\n<\/code><\/pre>\n\n<p>Followed by this error message: \n<strong>'ClientError: train channel is not specified.'<\/strong><\/p>\n\n<p>Does anyone have any thoughts for how I can get this running with no errors? Any help is much apreciated!<\/p>\n\n<p><strong>Successful run:<\/strong> Below is the paramaters that were used, along with the Augmented Manifest JSON Objects for a successful run.<\/p>\n\n<pre><code>training_params = \\\n{\n    \"AlgorithmSpecification\": {\n        \"TrainingImage\": training_image, # NB. This is one of the named constants defined in the first cell.\n        \"TrainingInputMode\": \"Pipe\"\n    },\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": s3_output_path\n    },\n    \"ResourceConfig\": {\n        \"InstanceCount\": 1,   \n        \"InstanceType\": \"ml.p3.2xlarge\",\n        \"VolumeSizeInGB\": 50\n    },\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": { # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n         \"base_network\": \"resnet-50\",\n         \"use_pretrained_model\": \"1\",\n         \"num_classes\": \"3\",\n         \"mini_batch_size\": \"1\",\n         \"epochs\": \"5\",\n         \"learning_rate\": \"0.001\",\n         \"lr_scheduler_step\": \"3,6\",\n         \"lr_scheduler_factor\": \"0.1\",\n         \"optimizer\": \"rmsprop\",\n         \"momentum\": \"0.9\",\n         \"weight_decay\": \"0.0005\",\n         \"overlap_threshold\": \"0.5\",\n         \"nms_threshold\": \"0.45\",\n         \"image_shape\": \"300\",\n         \"label_width\": \"350\",\n         \"num_training_samples\": str(num_training_samples)\n    },\n    \"StoppingCondition\": {\n        \"MaxRuntimeInSeconds\": 86400\n    },\n    \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                    \"S3Uri\": s3_train_data_path,\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"AttributeNames\": attribute_names # NB. This must correspond to the JSON field names in your **TRAIN** augmented manifest.\n                }\n            },\n            \"ContentType\": \"application\/x-recordio\",\n            \"RecordWrapperType\": \"RecordIO\",\n            \"CompressionType\": \"None\"\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                    \"S3Uri\": s3_validation_data_path,\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"AttributeNames\": [\"source-ref\",\"ValidateBird\"] # NB. This must correspond to the JSON field names in your **VALIDATION** augmented manifest.\n                }\n            },\n            \"ContentType\": \"application\/x-recordio\",\n            \"RecordWrapperType\": \"RecordIO\",\n            \"CompressionType\": \"None\"\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>Training Augmented Manifest File generated during the running of the training job<\/p>\n\n<pre><code>Line 1\n{\"source-ref\":\"s3:\/\/XXXXX\/Train\/Blackbird_1.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":1613,\"top\":840,\"height\":766,\"left\":1293}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:21:29.829003\",\"type\":\"groundtruth\/object-detection\"}}\n\n\nLine 2\n{\"source-ref\":\"s3:\/\/xxxxx\/Train\/Blackbird_2.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":897,\"top\":665,\"height\":1601,\"left\":1598}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:22:34.502274\",\"type\":\"groundtruth\/object-detection\"}}\n\n\nLine 3\n{\"source-ref\":\"s3:\/\/XXXXX\/Train\/Blackbird_3.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":1040,\"top\":509,\"height\":1695,\"left\":1548}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:20:26.660164\",\"type\":\"groundtruth\/object-detection\"}}\n<\/code><\/pre>\n\n<p>I then unzip the model.tar file to get the following files:hyperparams.JSON, model_algo_1-0000.params and model_algo_1-symbol<\/p>\n\n<p>hyperparams.JSON looks like this:<\/p>\n\n<pre><code>{\"label_width\": \"350\", \"early_stopping_min_epochs\": \"10\", \"epochs\": \"5\", \"overlap_threshold\": \"0.5\", \"lr_scheduler_factor\": \"0.1\", \"_num_kv_servers\": \"auto\", \"weight_decay\": \"0.0005\", \"mini_batch_size\": \"1\", \"use_pretrained_model\": \"1\", \"freeze_layer_pattern\": \"\", \"lr_scheduler_step\": \"3,6\", \"early_stopping\": \"False\", \"early_stopping_patience\": \"5\", \"momentum\": \"0.9\", \"num_training_samples\": \"11\", \"optimizer\": \"rmsprop\", \"_tuning_objective_metric\": \"\", \"early_stopping_tolerance\": \"0.0\", \"learning_rate\": \"0.001\", \"kv_store\": \"device\", \"nms_threshold\": \"0.45\", \"num_classes\": \"1\", \"base_network\": \"resnet-50\", \"nms_topk\": \"400\", \"_kvstore\": \"device\", \"image_shape\": \"300\"}\n<\/code><\/pre>",
        "Challenge_closed_time":1549801330896,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547399704507,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user encountered an error while running an ml.p3.2xlarge instance for object detection using AWS ground truth. The error message 'ClientError: train channel is not specified' appeared after running the instance. The user provided the parameters used for the instance and the augmented manifest JSON objects for a successful run.",
        "Challenge_last_edit_time":1551005141750,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54171261",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":102.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":56,
        "Challenge_solved_time":667.1184413889,
        "Challenge_title":"ClientError: train channel is not specified with AWS object_detection_augmented_manifest_training using ground truth images",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1312.0,
        "Challenge_word_count":542,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1547398724312,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Thank you again for your help. All of which were valid in helping me get further. Having received a response on the AWS forum pages, I finally got it working.<\/p>\n\n<p>I understood that my JSON was slightly different to the augmented manifest training guide. Having gone back to basics, I created another labelling job, but used the 'Bounding Box' type as opposed to the 'Custom - Bounding box template'. My output matched what was expected. This ran with no errors!<\/p>\n\n<p>As my purpose was to have multiple labels, I was able to edit the files and mapping of my output manifests, which also worked!<\/p>\n\n<p>i.e.<\/p>\n\n<pre><code>{\"source-ref\":\"s3:\/\/xxxxx\/Blackbird_15.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":0,\"width\":2023,\"top\":665,\"height\":1421,\"left\":1312}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.174131\",\"type\":\"groundtruth\/object-detection\"}}\n{\"source-ref\":\"s3:\/\/xxxx\/Pigeon_19.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":2,\"width\":784,\"top\":634,\"height\":1657,\"left\":1306}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"2\":\"Pigeon\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.074809\",\"type\":\"groundtruth\/object-detection\"}} \n<\/code><\/pre>\n\n<p>The original mapping was 0:'Bird' for all images through the labelling job.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.7,
        "Solution_reading_time":21.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":119.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1450889293150,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Finland",
        "Answerer_reputation_count":398.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":0.0838130556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to use sentence_transformers in AML to run XLM-Roberta model for sentence embedding. I have a script in which I import sentence_transformers:<\/p>\n<pre><code>from sentence_transformers import SentenceTransformer\n<\/code><\/pre>\n<p>Once I run my AML pipeline, the run fails on this script with the following error:<\/p>\n<pre><code>AzureMLCompute job failed.\nUserProcessKilledBySystemSignal: Job failed since the user script received system termination signal usually due to out-of-memory or segfault.\n    Cause: segmentation fault\n    TaskIndex: \n    NodeIp: #####\n    NodeId: #####\n<\/code><\/pre>\n<p>I'm pretty sure that this import is causing this error, because if I comment out this import, the rest of the script will run.\nThis is weird because the installation of the sentence_transformers succeed.<\/p>\n<p>This is the details of my compute:<\/p>\n<pre><code>Virtual machine size\nSTANDARD_NV24 (24 Cores, 224 GB RAM, 1440 GB Disk)\nProcessing Unit\nGPU - 4 x NVIDIA Tesla M60\n<\/code><\/pre>\n<p>Agent Pool:<\/p>\n<pre><code>Azure Pipelines\n<\/code><\/pre>\n<p>Agent Specification:<\/p>\n<pre><code>ubuntu-16.04\n<\/code><\/pre>\n<p>requirements.txt file:<\/p>\n<pre><code>torch==1.4.0\nsentence-transformers\n<\/code><\/pre>\n<p>Does anyone have a solution for this error?<\/p>",
        "Challenge_closed_time":1606866538208,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606861020137,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a segmentation fault error while trying to import sentence_transformers in Azure Machine Learning Service Nvidia Compute. The error occurs when running the AML pipeline and is likely caused by the import statement. The user has confirmed that the installation of sentence_transformers was successful and has provided details of their compute, agent pool, agent specification, and requirements.txt file. The user is seeking a solution to this error.",
        "Challenge_last_edit_time":1606866767496,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65099376",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":17.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.5327975,
        "Challenge_title":"Segmentation fault error in importing sentence_transformers in Azure Machine Learning Service Nvidia Compute",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":530.0,
        "Challenge_word_count":168,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1450889293150,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Finland",
        "Poster_reputation_count":398.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>I fixed the issue by changing the pytorch version from 1.4.0 to 1.6.0.\nSo the requirements.txt looks like this:<\/p>\n<pre><code>torch==1.6.0\nsentence-transformers\n<\/code><\/pre>\n<p>At first I tried one of the older versions of sentence-transformers which was compatible with pytorch 1.4.0. But the older version doesn't support &quot;xml-roberta-base&quot; model, so I tried to upgrade the pytorch version.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1606867069223,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":5.24,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":44.5576536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using ML Designer and i have created a sub-pipeline that i want to use it in other pipelines. how do i call that sub-pipeline from the designer?  <\/p>\n<p>The purpose of the subpipeline is to transform data, so the output is a dataset.  <\/p>",
        "Challenge_closed_time":1619449185636,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619288778083,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to call a sub-pipeline created in Azure ML Designer from other pipelines. The sub-pipeline is designed to transform data and produce a dataset as output. The user is seeking guidance on how to call the sub-pipeline from the designer.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/370433\/azure-ml-designer-how-to-call-a-pipeline-from-anot",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":3.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":44.5576536111,
        "Challenge_title":"azure ml designer: how to call a pipeline from another pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@javier-8889Thanks for the question. Can you please add more details about the pipeline steps. You can implement an <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-batch-scoring-classification#create-the-pipeline-step\">AML pipeline<\/a> with Python code, but also with the new AML designer which under the covers is creating an AML Pipeline defining that \u201cvisual workflow\u201d. Basically you can register a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-model-designer\">trained model in Designer<\/a> bring it out with SDK\/CLI to deploy it. Currently only Data Drift Monitor (Data Drift-&gt;EventGrid-&gt;LogicApp-&gt;Trigger Pipeline) allows to trigger a pipeline when dataset drift has been detected.    <\/p>\n<p>When designing a pipeline in Azure ML Designer, each step or module creates intermediate datasets that can be seen using the UI using Visualize option. Those datasets are persisted in the blob storage.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.9,
        "Solution_reading_time":12.75,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":116.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1384730587840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"France",
        "Answerer_reputation_count":717.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":71.1103577778,
        "Challenge_answer_count":1,
        "Challenge_body":"<pre><code>pins 1.0.1\nAzureStor 3.7.0\n<\/code><\/pre>\n<p>I'm getting this error<\/p>\n<pre><code>Error in withr::local_options(azure_storage_progress_bar = progress, .local_envir = env) : \n  unused argument (azure_storage_progress_bar = progress)\nCalls: %&gt;% ... pin_meta.pins_board_azure -&gt; azure_download -&gt; local_azure_progress\nExecution halted\n<\/code><\/pre>\n<p>when running <code>pin_read()<\/code> in the following code (<code>pin_list()<\/code> works fine)<\/p>\n<pre><code>bl_endp_key &lt;- storage_endpoint(endpoint = &lt;endpoint URL&gt;, key =&lt;endpoint key&gt;&quot;)\ncontainer &lt;- storage_container(endpoint = bl_endp_key, name = &lt;blob name&gt;)\nboard &lt;- board_azure(container = container, path = &quot;accidentsdata&quot;)\ncat(&quot;Testing pins:\\n&quot;)\nprint(board %&gt;% pin_list())\naccidents2 &lt;- board %&gt;% pins::pin_read('accidents') %&gt;% as_tibble()\n<\/code><\/pre>\n<p>My goal is to &quot;pin_read&quot; a dataset located on a Azure Blob Storage from an R script being run from <strong>pipelineJoB (YAML)<\/strong> including a <code>command: Rscript script.R ...<\/code> and an <code>environment:<\/code> based on a dockerfile installing <strong>R version 4.0.0<\/strong> (2020-04-24) -- &quot;Arbor Day&quot;<\/p>\n<p>The pipelineJob is being called from an Azure DevOps Pipeline task with <code>az ml job create &lt;pipelineJob YAML&gt; &lt;resource grp&gt; &lt;aml workspace name&gt;<\/code>.<\/p>\n<p>Note: the R script runs fine on my Windows RStudio desktop, with R version 4.1.3 (2022-03-10) -- &quot;One Push-Up&quot;.<\/p>\n<p>I've already tried with<\/p>\n<p><code>options(azure_storage_progress_bar=FALSE)<\/code> or<\/p>\n<p><code>withr::local_options(azure_storage_progress_bar=FALSE)<\/code><\/p>\n<p>but I'm getting the same <code>unused argument (azure_storage_progress_bar ...<\/code> error.<\/p>\n<p>FYI: <code>local_azure_progress<\/code> is defined here <a href=\"https:\/\/rdrr.io\/github\/rstudio\/pins\/src\/R\/board_azure.R#sym-local_azure_progress\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Challenge_closed_time":1656936659776,
        "Challenge_comment_count":1,
        "Challenge_created_time":1656349974407,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while running the `pin_read()` function in R, with the error message indicating an unused argument for `azure_storage_progress_bar`. The user is attempting to read a dataset located on Azure Blob Storage from an R script being run from pipelineJob (YAML) with R version 4.0.0. The R script runs fine on the user's Windows RStudio desktop with R version 4.1.3. The user has tried using `options(azure_storage_progress_bar=FALSE)` and `withr::local_options(azure_storage_progress_bar=FALSE)` but is still encountering the same error.",
        "Challenge_last_edit_time":1656680662488,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72775967",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":27.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":162.9681580556,
        "Challenge_title":"R, pins and AzureStor: unused argument (azure_storage_progress_bar = progress)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":80.0,
        "Challenge_word_count":189,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384730587840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"France",
        "Poster_reputation_count":717.0,
        "Poster_view_count":112.0,
        "Solution_body":"<p>Issue has been filed in <a href=\"https:\/\/github.com\/rstudio\/pins\/issues\/624\" rel=\"nofollow noreferrer\">pins<\/a>, it seems that is not an AzureStor issue.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.0,
        "Solution_reading_time":2.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":72.1441666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nThe plugin does not work with projects created with ``kedro==0.18.1``\r\n\r\n## Context\r\n\r\nTry to launch ``kedro run`` in a project with ``kedro==0.18.1`` and kedro-mlflow installed.\r\n\r\n\r\n## Steps to Reproduce\r\n\r\n```python\r\nconda create -n temp python=3.8 -y\r\nconda activate temp\r\npip install kedro==0.18.1 kedro-mlflow==0.9.0\r\nkedro new --starter=pandas-iris\r\ncd pandas-iris\r\nkedro mlflow init\r\nkedro run\r\n```\r\n\r\n## Expected Result\r\n\r\nThis should run the pipeleine and log the parameters.\r\n\r\n## Actual Result\r\n\r\nThis raises the following error:\r\n\r\n```bash\r\nAttributeError: module 'kedro.framework.session.session' has no attribute '_active_session'\r\n```\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): ``kedro==0.18.1`` and ``kedro-mlflow<=0.9.0``\r\n* Python version used (`python -V`): All\r\n* Operating system and version: All\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nCurrently, kedro-mlflow uses [the private ``_active_session`` global variable to access the configuration](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/e855f59faa76c881b32616880608d41c064c23a0\/kedro_mlflow\/config\/kedro_mlflow_config.py#L233-L247) inside a hook. \r\n\r\nWith kedro==0.18.1, this private attribute was removed and the new recommandation is to use the ``after_context_created`` hook. \r\n\r\nRetrieving the configuration and set it up should be moved to this new hook:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/963c338d6259dd118232c45801abe0a2b0a463df\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L108-L109",
        "Challenge_closed_time":1652640252000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1652380533000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the kedro-mlflow plugin not working with projects created with kedro==0.18.1. When the user tries to run the pipeline, an error is raised due to the removal of the private attribute '_active_session' in kedro==0.18.1. The solution is to use the 'after_context_created' hook to retrieve and set up the configuration.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/309",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":21.98,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":21.0,
        "Challenge_repo_issue_count":414.0,
        "Challenge_repo_star_count":145.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":72.1441666667,
        "Challenge_title":"kedro-mlflow is broken with kedro==0.18.1",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":185,
        "Discussion_body":"Closed by #313 ",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":339.8152741667,
        "Challenge_answer_count":8,
        "Challenge_body":"<p>Hi all,    <\/p>\n<p>I'm trying to create an inference pipeline with the AML designer.     <br \/>\nI clicked on the &quot;Create inference pipeline&quot; button:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/205064-image.png?platform=QnA\" alt=\"205064-image.png\" \/>    <\/p>\n<p>and now I want to do some changes in the pipeline. I added at the end two more steps and linked the Webservice output component to the last step:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/205028-image.png?platform=QnA\" alt=\"205028-image.png\" \/>    <\/p>\n<p>I clicked on save and submit it.     <br \/>\nThe result is the following:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/204999-image.png?platform=QnA\" alt=\"204999-image.png\" \/>    <\/p>\n<p>The two new steps are present and executed, but the webservice output step is disappeared! I've tried multiple time with the same result.     <br \/>\nThe webservice input step is correctly present at the beginning of the pipeline.    <\/p>\n<p>Also, after making the change and saving correctly, if I exit and reopen the pipeline the step &quot;Web Service Output&quot; is no longer there    <\/p>\n<p>Can you help me?    <\/p>\n<p>Thanks!    <\/p>\n<p>G    <\/p>",
        "Challenge_closed_time":1654609016260,
        "Challenge_comment_count":1,
        "Challenge_created_time":1653385681273,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure Machine Learning Designer where the Webservice output component disappears from the inference pipeline after adding two more steps and linking it to the last step. The user has tried multiple times with the same result and even after saving the changes, the step disappears when the pipeline is reopened.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/861882\/azure-machine-learning-designer-webservice-input-o",
        "Challenge_link_count":3,
        "Challenge_participation_count":9,
        "Challenge_readability":9.6,
        "Challenge_reading_time":16.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":339.8152741667,
        "Challenge_title":"Azure Machine Learning Designer - Webservice input\/output disappear",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":158,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=061108cd-43c2-45e6-aefa-0aaa3ab2e335\">@Antonio  <\/a>,     <\/p>\n<p>Sorry for the inconvenience caused.    <br \/>\nThis is a known bug and we've fixed. Could you please retry to see if you can still repro? I tried from my side either manually build an inference pipeline or modify the auto-gen inference pipeline, the web service input\/output components are still there.     <\/p>\n<p>If you can still repro, could you please provide following info for us to investigate?    <\/p>\n<ul>\n<li> your inference pipeline draft URL    <\/li>\n<li>  inference pipeline job URL of which the webservice input\/output components disappear    <\/li>\n<li> Is your workspace in Vnet?    <\/li>\n<\/ul>\n<p>We're also happy to set up a call to investigate, could you please send me an email so that I can send the meeting request?     <br \/>\nWe're based in Beijing (UTC+8).    <\/p>\n<p>Thanks!     <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":10.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":135.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":34.5506816667,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hello,<\/p>\n<p>We use Pytorch Lightning for training and we use Kubeflow Pipelines and are thinking about using wandb to track and visualize the training and test metrics.<\/p>\n<p>Kubeflow pipelines offers the possibility to view a static html page (see <a href=\"https:\/\/www.kubeflow.org\/docs\/components\/pipelines\/sdk\/output-viewer\/#single-html-file\" rel=\"noopener nofollow ugc\">this link<\/a> ).<br>\nI was wondering if it would be possible via the wandb python sdk to get a read-only embeded code (iframe) that I could then simply pass to Kubeflow pipeline sdk to show the html ?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1643930787864,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643806405410,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking a way to visualize HTML run in Kubeflow Pipeline using wandb to track and visualize training and test metrics. They are wondering if it is possible to get a read-only embedded code (iframe) via the wandb python sdk to pass to Kubeflow pipeline sdk to show the HTML.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-visualize-html-run-in-kubeflow-pipeline\/1862",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":11.4,
        "Challenge_reading_time":8.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":34.5506816667,
        "Challenge_title":"How to visualize HTML run in Kubeflow Pipeline?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":890.0,
        "Challenge_word_count":89,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Ok I found the solution.<br>\nKubeflow Pipelines also support markdown visualization therefore instead of using kubeflow HTML output I used markdown and since markdown supports html inline I was able to directly use the wandb run html.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/21d1b9f84b7948659b75b981b04f21235e528615.png\" data-download-href=\"\/uploads\/short-url\/4Pb5MStVV77kGP5bCR1nBTvaK7H.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/21d1b9f84b7948659b75b981b04f21235e528615_2_690x333.png\" alt=\"image\" data-base62-sha1=\"4Pb5MStVV77kGP5bCR1nBTvaK7H\" width=\"690\" height=\"333\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/21d1b9f84b7948659b75b981b04f21235e528615_2_690x333.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/21d1b9f84b7948659b75b981b04f21235e528615_2_1035x499.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/21d1b9f84b7948659b75b981b04f21235e528615_2_1380x666.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/21d1b9f84b7948659b75b981b04f21235e528615_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1678\u00d7812 57.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Here is the code if someone is interested :<\/p>\n<pre><code class=\"lang-python\">import kfp\nfrom kfp.v2.dsl import component, Output, Markdown, pipeline\n\n@component(packages_to_install=['wandb'])\ndef wandb_visualization(markdown_artifact: Output[Markdown]):\n    import wandb\n    wandb.login(key=\"you_key\")\n\n    run = wandb.init(project=\"your-project\", entity=\"your-entity\")\n\n    wandb.log({\"train\/loss\" : 5.0})\n    wandb.log({\"train\/loss\" : 4.0})\n    wandb.log({\"train\/loss\" : 3.0})\n    wandb.log({\"train\/loss\" : 2.0})\n    wandb.log({\"train\/loss\" : 1.0})\n\n    wandb.finish()\n    with open(markdown_artifact.path, 'w') as f:\n        f.write(f\"&lt;iframe src=\\\"{run.get_url()}\\\" width=\\\"100%\\\" height=\\\"700\\\"\/&gt;\")\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":23.3,
        "Solution_reading_time":32.05,
        "Solution_score_count":null,
        "Solution_sentence_count":14.0,
        "Solution_word_count":126.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1324129118823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11756.0,
        "Answerer_view_count":517.0,
        "Challenge_adjusted_solved_time":8.0286480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created an EventBridge rule that triggers a Sagemaker Pipeline when someone uploads a new file to an S3 bucket. As new input files become available, they will be uploaded to the bucket for processing. I'd like the pipeline to process only the uploaded file, and so thought to pass in the S3 URL of the file as a parameter to the Pipeline. Since the full URL doesn't exist as a single field value in the S3 event, I was wondering if there is some way to concatenate multiple field values into a single parameter value that EventBridge will pass on to the target.<\/p>\n<p>For example, I know the name of the uploaded file can be sent from EventBridge using <code>$.detail.object.key<\/code> and the bucket name can be sent using <code>$.detail.bucket.name<\/code>, so I'm wondering if I can send both somehow to get something like this to the Sagemaker Pipeline <code>s3:\/\/my-bucket\/path\/to\/file.csv<\/code><\/p>\n<p>For what it's worth, I tried splitting the parameter into two (one being <code>s3:\/\/bucket-name\/<\/code> and the other being <code>default_file.csv<\/code>) when defining the pipeline, but got an error saying <code>Pipeline variables do not support concatenation<\/code> when combining the two into one.<\/p>\n<p>The relevant pipeline step is<\/p>\n<p><code>step_transform = TransformStep(name = &quot;Name&quot;, transformer=transformer,inputs=TransformInput(data=variable_of_s3_path)<\/code><\/p>",
        "Challenge_closed_time":1651734900863,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651705997730,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create an AWS EventBridge rule that triggers a Sagemaker Pipeline when a new file is uploaded to an S3 bucket. They want to pass the S3 URL of the uploaded file as a parameter to the Pipeline, but since the URL doesn't exist as a single field value in the S3 event, they are wondering if there is a way to concatenate multiple field values into a single parameter value that EventBridge will pass on to the target. The user tried splitting the parameter into two, but got an error saying Pipeline variables do not support concatenation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72120382",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":18.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":8.0286480556,
        "Challenge_title":"AWS EventBridge: Add multiple event details to a target parameter",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":809.0,
        "Challenge_word_count":207,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324682328743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":969.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/eventbridge\/latest\/userguide\/eb-transform-target-input.html\" rel=\"nofollow noreferrer\">Input transformers<\/a> manipulate the event payload that EventBridge sends to the target.  Transforms consist of (1) an &quot;input path&quot; that maps substitution variable names to JSON-paths in the event and (2) a &quot;template&quot; that references the substitution variables.<\/p>\n<p>Input path:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;detail-bucket-name&quot;: &quot;$.detail.bucket.name&quot;,\n  &quot;detail-object-key&quot;: &quot;$.detail.object.key&quot;\n}\n<\/code><\/pre>\n<p>Input template that concatenates the s3 url and outputs it along with the original event payload:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;s3Url&quot;: &quot;s3:\/\/&lt;detail-bucket-name&gt;\/&lt;detail-object-key&gt;&quot;,\n  &quot;original&quot;: &quot;$&quot;\n}\n<\/code><\/pre>\n<p>Define the transform in the EventBridge console by editing the rule: <code>Rule &gt; Select Targets &gt; Additional Settings<\/code>.<\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.8,
        "Solution_reading_time":14.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":93.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":27.6607583333,
        "Challenge_answer_count":1,
        "Challenge_body":"I start a training job from a sagemaker notebook using boto3.client():\n\nclient = boto3.client(service_name='sagemaker')\nclient.create_training_job(**training_params)\n\nI see the training job in progress on the console. Is it then an error to stop and delete the notebook even if the model has not finished training?",
        "Challenge_closed_time":1683838814256,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683739235526,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is starting a training job from a Sagemaker notebook using boto3.client() and is wondering if it is an error to stop and delete the notebook even if the model has not finished training.",
        "Challenge_last_edit_time":1684085641956,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUT7JnzaUJRs-s13xiawMEFA\/keep-sagemaker-notebook-up-during-model-training",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":4.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":27.6607583333,
        "Challenge_title":"Keep sagemaker notebook up during model training?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":46.0,
        "Challenge_word_count":49,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @fascani, no, once the training job has been created, you don't need to keep the notebook running. You can continue other explorations\/model building on the notebook, check the progress of the training job using APIs, but that's not required.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1683838814256,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":3.03,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":85.8187694445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm currently trying to implement MLFlow Tracking into my training pipeline and would like to log the hyperparameters of my hyperparameter Tuning of each training job.<\/p>\n\n<p>Does anyone know, how to pull the list of hyperparameters that can be seen on the sagemaker training job interface (on the AWS console)? Is there any other smarter way to list how models perform in comparison in Sagemaker (and displayed)?<\/p>\n\n<p>I would assume there must be an easy and Pythonic way to do this (either boto3 or the sagemaker api) to get this data. I wasn't able to find it in Cloudwatch.<\/p>\n\n<p>Many thanks in advance!<\/p>",
        "Challenge_closed_time":1592173650467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591864702897,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to implement MLFlow Tracking into their training pipeline and is looking for a way to pull the list of hyperparameters used in each training job in Sagemaker. They are seeking a Pythonic way to get this data through either boto3 or the Sagemaker API, but have not been able to find it in Cloudwatch.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62320331",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":8.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":85.8187694445,
        "Challenge_title":"Sagemaker API to list Hyperparameters",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":484.0,
        "Challenge_word_count":107,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1469720117216,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>there is indeed a rather pythonic way in the SageMaker python SDK:<\/p>\n\n<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')\n\nresults = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!\n<\/code><\/pre>\n\n<p>See full example here <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":33.1,
        "Solution_reading_time":7.83,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":161.6828269445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an experiment running without problems when I run single modules as selected parts.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/aVsSC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aVsSC.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The situation is pretty different when I run the entire experiment. In that case it fails, but I cannot know why.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/N2h11.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/N2h11.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The experiment returns an error and obviously it doesn't let me deploy the web service, while:<\/p>\n\n<p>1) I cannot know on <strong>which module<\/strong> my error is.<\/p>\n\n<p>2) I don't have an overall description of the error.<\/p>\n\n<p>3) It could be related to the error here but I cannot know because I don't have any feedback about that. I know that it could be a <strong>bug<\/strong> Azure is trying to solve but this is not reported anywhere.<\/p>\n\n<p>I really need to know if that's a bug and if I can do something about that.<\/p>",
        "Challenge_closed_time":1469689033830,
        "Challenge_comment_count":4,
        "Challenge_created_time":1469106975653,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error in their experiment when running the entire experiment, but they cannot determine which module the error is coming from or what the overall description of the error is. They suspect it may be a bug that Azure is trying to solve, but there is no feedback or information available to confirm this. The user is seeking assistance in identifying the issue and determining if there is a solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38505336",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":9.1,
        "Challenge_reading_time":14.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":161.6828269445,
        "Challenge_title":"The \"perfect error\": untraceable, unnamed, from neverland",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":75.0,
        "Challenge_word_count":153,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436432728608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation_count":809.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>This issue has been resolved. Please let me know if this happens again<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":1.1,
        "Solution_reading_time":0.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.5649966667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>There is very less components compared to classical mode, how can I use prebuilt components in custom mode? Is that possible? How should I get it? <\/p>\n<p>Can I get some help here? Much appreciated.<\/p>",
        "Challenge_closed_time":1680082887128,
        "Challenge_comment_count":2,
        "Challenge_created_time":1680044853140,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in using prebuilt components in custom pipeline mode as there are very few components available compared to classical mode. They are seeking help on how to obtain and use prebuilt components in custom mode.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1194061\/can-i-use-prebuilt-component-in-custom-pipeline-mo",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.5,
        "Challenge_reading_time":3.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":10.5649966667,
        "Challenge_title":"can I use prebuilt component in custom pipeline mode?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":43,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello @merten<\/p>\n<p>Thanks for reaching out to us, as you know Designer supports two type of components, classic prebuilt components and custom components. These two types of components <strong>are not compatible. So a quick answer for your question is you can not use it together.<\/strong><\/p>\n<p>Classic prebuilt components provides prebuilt components majorly for data processing and traditional machine learning tasks like regression and classification. This type of component continues to be supported but will not have any new components added.<\/p>\n<p>Custom components allow you to provide your own code as a component. It supports sharing across workspaces and seamless authoring across Studio, CLI, and SDK interfaces.<\/p>\n<p>I am sorry for all inconveniences. If you can share more details about your scenario, we are happy to discuss with product team.<\/p>\n<p>Regards,<\/p>\n<p>Yutong<\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":12.56,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":147.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":18.9067786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to write a script which can download the outputs from an Azure ML experiment Run after the fact.<\/p>\n<p>Essentially, I want to know how I can get a Run by its <code>runId<\/code> property (or some other identifier).<\/p>\n<p>I am aware that I have access to the Run object when I create it for the purposes of training. What I want is a way to recreate this Run object later in a separate script, possibly from a completely different environment.<\/p>\n<p>What I've found so far is a way to get a list of ScriptRun objects from an experiment via the <code>get_runs()<\/code> function. But I don't see a way to use one of these ScriptRun objects to create a Run object representing the original Run and allowing me to download the outputs.<\/p>\n<p>Any help appreciated.<\/p>",
        "Challenge_closed_time":1612384991070,
        "Challenge_comment_count":1,
        "Challenge_created_time":1612316926667,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to download the outputs of a historical Azure ML experiment Run using the python API. They are looking for a way to recreate the Run object later in a separate script, possibly from a completely different environment. They have found a way to get a list of ScriptRun objects from an experiment via the get_runs() function, but they are unable to use one of these ScriptRun objects to create a Run object representing the original Run and allowing them to download the outputs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66020144",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":10.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":18.9067786111,
        "Challenge_title":"How can one download the outputs of historical Azure ML experiment Runs via the python API",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1402.0,
        "Challenge_word_count":150,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1612316384927,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>I agree that this could probably be better documented, but fortunately, it's a simple implementation.<\/p>\n<p>this is how you get a run object for an already submitted run for <code>azureml-sdk&gt;=1.16.0<\/code> (for the older approach <a href=\"https:\/\/stackoverflow.com\/questions\/62949488\/amls-experiment-run-stuck-in-status-running\/62958369#62958369\">see my answer here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace\n\nws = Workspace.from_config()\nrun = ws.get_run('YOUR_RUN_ID')\n<\/code><\/pre>\n<p>once you have the <code>run<\/code> object, you can call methods like<\/p>\n<ul>\n<li><code>.get_file_names()<\/code> to see what files are available (the logs in <code>azureml-logs\/<\/code> and <code>logs\/azureml\/<\/code> will also be listed)<\/li>\n<li><code>.download_file()<\/code> to download an individual file<\/li>\n<li><code>.download_files()<\/code> to download all files that match a given prefix (or all the files)<\/li>\n<\/ul>\n<p>See the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py&amp;WT.mc_id=AI-MVP-5003930\" rel=\"noreferrer\">Run object docs<\/a> for more details.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.9,
        "Solution_reading_time":15.62,
        "Solution_score_count":7.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":110.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":198.4418677778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello!  <\/p>\n<p>I created an Azure ML pipeline in Python and used multiple PythonScriptSteps for each of my tasks. For example, I have three training steps running in parallel, so I create three PythonScriptSteps in a for loop with my train.py script and different data. Later, I came across the <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-how-to-use-modulestep.ipynb\">ModuleStep<\/a>, which seems to do exactly this, but with an extra layer of (seemingly pointless) abstraction. What does the ModuleStep add to a PythonScriptStep?  <\/p>\n<p>Also, I imagined the ModuleStep might make it possible to use a custom PythonScriptStep in the pipeline designer (by creating a new drag and drop module), however this doesn't seem to be the case. Is there any way of doing this?   <\/p>",
        "Challenge_closed_time":1628020406187,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627306015463,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user created an Azure ML pipeline in Python using multiple PythonScriptSteps for each task. They later discovered the ModuleStep, which adds an extra layer of abstraction, and are questioning its usefulness compared to PythonScriptStep. They also wonder if ModuleStep allows for custom PythonScriptSteps in the pipeline designer.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/489671\/what-is-the-point-of-azureml-modules",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":9.9,
        "Challenge_reading_time":11.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":198.4418677778,
        "Challenge_title":"What is the point of AzureML modules?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":124,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Just to close this question, I have since discovered that the ModuleStep <strong>does<\/strong> create a custom drag-and-drop module in the designer. I don't know if I'd missed this (I imagine so) or if this is a new feature. Either way, that's the answer. <a href=\"\/users\/na\/?userid=ad870133-9538-4d77-adc8-2b5ffc5c1b45\">@YutongTie-MSFT  <\/a> can you confirm if this was recently added?    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.5,
        "Solution_reading_time":4.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":225.2988875,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,    <\/p>\n<p>Is there any way to access the X_tain, y_train, X_test, y_test data that was used in azure automl pipeline step during training?<\/p>",
        "Challenge_closed_time":1672240955832,
        "Challenge_comment_count":2,
        "Challenge_created_time":1671429879837,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking a way to access the X_train, y_train, X_test, and y_test data used in the Azure AutoML pipeline training.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1133620\/how-to-access-the-data-used-during-the-azure-autom",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.8,
        "Challenge_reading_time":2.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":225.2988875,
        "Challenge_title":"How to access the data used during the azure automl pipeline training?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":35,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>You can access the data that was used during the training of an Azure AutoML model by using the TrainingData property of the Model object in the Azure Machine Learning SDK.    <\/p>\n<p>Here's an example of how you can do this:    <\/p>\n<pre><code>from azureml.core import Workspace, Dataset, Experiment, Model  \n  \n# Load the workspace  \nws = Workspace.from_config()  \n  \n# Get the experiment that contains the model  \nexperiment = Experiment(workspace=ws, name='my-experiment')  \n  \n# Get the model  \nmodel = Model(workspace=ws, name='my-model', version='1')  \n  \n# Get the training data  \ntraining_data = model.training_data  \n  \n# Access the X_train and y_train data  \nX_train = training_data.split['train']['X']  \ny_train = training_data.split['train']['y']  \n  \n# Access the X_test and y_test data  \nX_test = training_data.split['test']['X']  \ny_test = training_data.split['test']['y']  \n<\/code><\/pre>\n<p>Note that the training_data object is a TabularDataset object, which represents a dataset in tabular format (i.e., rows and columns). The split property of this object is a dictionary that contains the training and test data splits. The keys of this dictionary are 'train' and 'test', and the values are dictionaries containing the 'X' and 'y' data for each split.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":15.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":159.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":1.1481591667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm deploying a SageMaker inference pipeline composed of two PyTorch models (<code>model_1<\/code> and <code>model_2<\/code>), and I am wondering if it's possible to pass the same input to both the models composing the pipeline.<\/p>\n<p>What I have in mind would work more or less as follows<\/p>\n<ol>\n<li><p>Invoke the endpoint sending a binary encoded payload (namely <code>payload_ser<\/code>), for example:<\/p>\n<pre><code>client.invoke_endpoint(EndpointName=ENDPOINT,\n                       ContentType='application\/x-npy',\n                       Body=payload_ser)\n<\/code><\/pre>\n<\/li>\n<li><p>The first model parses the payload with <code>inut_fn<\/code> function, runs the predictor on it, and returns the output of the predictor. As a simplified example:<\/p>\n<pre><code>def input_fn(request_body, request_content_type):\n    if request_content_type == &quot;application\/x-npy&quot;:\n        input = some_function_to_parse_input(request_body)\n    return input\n\ndef predict_fn(input_object, predictor):\n    outputs = predictor(input_object)\n    return outputs\n\ndef output_fn(predictions, response_content_type):\n    return json.dumps(predictions)\n<\/code><\/pre>\n<\/li>\n<li><p>The second model gets as payload both the original payload (<code>payload_ser<\/code>) and the output of the previous model (predictions). Possibly, the <code>input_fn<\/code> function would be used to parse the output of model_1 (as in the &quot;standard case&quot;), but I'd need some way to also make the original payload available to model_2.  In this way, model_2 will use both the original payload and the output of model_1 to make the final prediction and return it to whoever invoked the endpoint.<\/p>\n<\/li>\n<\/ol>\n<p>Any idea if this is achievable?<\/p>",
        "Challenge_closed_time":1638812806036,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638808672663,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is deploying a SageMaker inference pipeline with two PyTorch models and wants to know if it's possible to pass the same input to both models composing the pipeline. The first model parses the payload and runs the predictor on it, returning the output of the predictor. The second model gets both the original payload and the output of the previous model as payload to make the final prediction. The user is seeking advice on whether this is achievable.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70248817",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.7,
        "Challenge_reading_time":22.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1.1481591667,
        "Challenge_title":"Shared input in Sagemaker inference pipeline models",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":210.0,
        "Challenge_word_count":205,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1508881117760,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":157.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>Sounds like you need an inference DAG. Amazon SageMaker Inference pipelines currently supports only a chain of handlers, where the output of handler N is the input for handler N+1.<\/p>\n<p>You could change model1's predict_fn() to return both (input_object, outputs), and output_fn(). output_fn() will receive these two objects as the predictions, and will handle serializing both as json. model2's input_fn() will need to know how to parse this pair input.<\/p>\n<p>Consider implementing this as a generic pipeline handling mechanism that adds the input to the model's output. This way you could reuse it for all models and pipelines.<\/p>\n<p>You could allow the model to be deployed as a standalone model, and as a part of a pipeline, and apply the relevant input\/output handling behavior that will be triggered by the presence of an environment variable (<code>Environment<\/code> dict), which you can specify when <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model\" rel=\"nofollow noreferrer\">creating<\/a> the inference pipelines model.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":14.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":150.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.2782375,
        "Challenge_answer_count":1,
        "Challenge_body":"So as mentioned in my [other recent post](https:\/\/repost.aws\/questions\/QUAL9Vn9abQ6KKCs2ASwwmzg\/adjusting-sagemaker-xgboost-project-to-tensorflow-or-even-just-different-folder-name), I'm trying to modify the sagemaker example abalone xgboost template to use tensorfow.\n\nMy current problem is that running the pipeline I get a failure and in the logs I see:\n\n```\nModuleNotFoundError: No module named 'transformers'\n```\n\nNOTE: I am importing 'transformers' in `preprocess.py` not in `pipeline.py`\n\nNow I have 'transformers' listed in various places as a dependency including:\n\n* `setup.py` - `required_packages = [\"sagemaker==2.93.0\", \"sklearn\", \"transformers\", \"openpyxl\"]`\n* `pipelines.egg-info\/requires.txt` - `transformers` (auto-generated from setup.py?)\n\nbut so I'm keen to understand, how can I ensure that additional dependencies are available in the pipline itself?\n\nMany thanks in advance\n\n------------\n------------\n------------\nADDITIONAL DETAILS ON HOW I ENCOUNTERED THE ERROR\n\nFrom one particular notebook (see [previous post](https:\/\/repost.aws\/questions\/QUAL9Vn9abQ6KKCs2ASwwmzg\/adjusting-sagemaker-xgboost-project-to-tensorflow-or-even-just-different-folder-name) for more details)  I have succesfully constructed the new topic\/tensorflow pipeline and run the following steps:\n\n```\npipeline.upsert(role_arn=role)\nexecution = pipeline.start()\nexecution.describe()\n```\n\nthe `describe()` method gives this output:\n\n```\n{'PipelineArn': 'arn:aws:sagemaker:eu-west-1:398371982844:pipeline\/topicpipeline-example',\n 'PipelineExecutionArn': 'arn:aws:sagemaker:eu-west-1:398371982844:pipeline\/topicpipeline-example\/execution\/0aiczulkjoaw',\n 'PipelineExecutionDisplayName': 'execution-1664394415255',\n 'PipelineExecutionStatus': 'Executing',\n 'PipelineExperimentConfig': {'ExperimentName': 'topicpipeline-example',\n  'TrialName': '0aiczulkjoaw'},\n 'CreationTime': datetime.datetime(2022, 9, 28, 19, 46, 55, 147000, tzinfo=tzlocal()),\n 'LastModifiedTime': datetime.datetime(2022, 9, 28, 19, 46, 55, 147000, tzinfo=tzlocal()),\n 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-1:398371982844:user-profile\/d-5qgy6ubxlbdq\/sjoseph-reg-genome-com-273',\n  'UserProfileName': 'sjoseph-reg-genome-com-273',\n  'DomainId': 'd-5qgy6ubxlbdq'},\n 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-1:398371982844:user-profile\/d-5qgy6ubxlbdq\/sjoseph-reg-genome-com-273',\n  'UserProfileName': 'sjoseph-reg-genome-com-273',\n  'DomainId': 'd-5qgy6ubxlbdq'},\n 'ResponseMetadata': {'RequestId': 'f949d6f4-1865-4a01-b7a2-a96c42304071',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': 'f949d6f4-1865-4a01-b7a2-a96c42304071',\n   'content-type': 'application\/x-amz-json-1.1',\n   'content-length': '882',\n   'date': 'Wed, 28 Sep 2022 19:47:02 GMT'},\n  'RetryAttempts': 0}}\n```\nWaiting for the execution I get:\n\n```\n---------------------------------------------------------------------------\nWaiterError                               Traceback (most recent call last)\n<ipython-input-14-72be0c8b7085> in <module>\n----> 1 execution.wait()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in wait(self, delay, max_attempts)\n    581             waiter_id, model, self.sagemaker_session.sagemaker_client\n    582         )\n--> 583         waiter.wait(PipelineExecutionArn=self.arn)\n    584 \n    585 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/waiter.py in wait(self, **kwargs)\n     53     # method.\n     54     def wait(self, **kwargs):\n---> 55         Waiter.wait(self, **kwargs)\n     56 \n     57     wait.__doc__ = WaiterDocstring(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/waiter.py in wait(self, **kwargs)\n    376                     name=self.name,\n    377                     reason=reason,\n--> 378                     last_response=response,\n    379                 )\n    380             if num_attempts >= max_attempts:\n\nWaiterError: Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\"\n```\nWhich I assume is corresponding to the failure I see in the logs:\n\n![buildl pipeline error message on preprocessing step](\/media\/postImages\/original\/IMMpF6LeI6TgWxp20TnPZbUw)\n\nI did also run `python setup.py build` to ensure my build directory was up to date ... here's the terminal output of that command:\n\n```\nsagemaker-user@studio$ python setup.py build\n\/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n  warnings.warn(\n\/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/config\/setupcfg.py:508: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.\n  warnings.warn(msg, warning_class)\nrunning build\nrunning build_py\ncopying pipelines\/topic\/pipeline.py -> build\/lib\/pipelines\/topic\nrunning egg_info\nwriting pipelines.egg-info\/PKG-INFO\nwriting dependency_links to pipelines.egg-info\/dependency_links.txt\nwriting entry points to pipelines.egg-info\/entry_points.txt\nwriting requirements to pipelines.egg-info\/requires.txt\nwriting top-level names to pipelines.egg-info\/top_level.txt\nreading manifest file 'pipelines.egg-info\/SOURCES.txt'\nadding license file 'LICENSE'\nwriting manifest file 'pipelines.egg-info\/SOURCES.txt'\n```\nIt seems like the dependencies are being written to `pipelines.egg-info\/requires.txt` but are these not being picked up by the pipeline?",
        "Challenge_closed_time":1664451755510,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664396753855,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a `ModuleNotFoundError` while using the transformers module with a SageMaker Studio project. The user has listed transformers as a dependency in various places, including `setup.py` and `pipelines.egg-info\/requires.txt`, but is still unable to ensure that additional dependencies are available in the pipeline itself. The user has also encountered a `WaiterError` while waiting for the pipeline execution to complete.",
        "Challenge_last_edit_time":1668528521211,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUdd2zOBY0Q4CEG1ZdbgNsgA\/using-transformers-module-with-sagemaker-studio-project-modulenotfounderror-no-module-named-transformers",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.7,
        "Challenge_reading_time":71.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":43,
        "Challenge_solved_time":15.2782375,
        "Challenge_title":"using transformers module with sagemaker studio project: ModuleNotFoundError: No module named 'transformers'",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":133.0,
        "Challenge_word_count":440,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! There are two places where you need to install the dependencies \/ requirements:\n\n1. In your environment where you execute `pipeline.start()` \u2013 can be Amazon SageMaker Studio, your local machine or CI\/CD pipeline executor, e. g. AWS CodeBuild. These dependencies are installed in `setup.py`.\n2. Inside the SageMaker processing and training jobs as well as in inference endpoints. This is usually done via `requirements.txt` file that you submit as part of your `source_dir`.\n\nIn your example, I recommend you to use the `TensorFlowProcessor`. The way how to install dependencies into it is described [in the corresponding section of the documentation](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-job-frameworks-tensorflow.html), in particular:\n> SageMaker Processing installs the dependencies in `requirements.txt` in the container for you.\n\nSame applies to your model training and to the `TensorFlow` estimator. See the section [Use third-party libraries](https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#use-third-party-libraries) in the TensorFlow documentation of the SageMaker Python SDK, in particular:\n> If there are other packages you want to use with your script, you can use a `requirements.txt` to install other dependencies at runtime. \n\nHope it helps!",
        "Solution_comment_count":14.0,
        "Solution_last_edit_time":1664531543836,
        "Solution_link_count":2.0,
        "Solution_readability":10.9,
        "Solution_reading_time":16.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":167.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1510527902860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Zurich, Switzerland",
        "Answerer_reputation_count":1078.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":1.3906147222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a pipeline in Kedro that looks like this:<\/p>\n<pre><code>from kedro.pipeline import Pipeline, node\nfrom .nodes import *\n\ndef foo():\n    return Pipeline([\n        node(a, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=dict(bar_a=&quot;bar_a&quot;), name=&quot;A&quot;),\n        node(b, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=dict(bar_b=&quot;bar_b&quot;), name=&quot;B&quot;),\n        node(c, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=dict(bar_c=&quot;bar_c&quot;), name=&quot;C&quot;),\n        node(d, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=dict(bar_d=&quot;bar_d&quot;), name=&quot;D&quot;),\n        \n    ])\n<\/code><\/pre>\n<p>The nodes A, B, and C are not very resource-intensive, but they take a while so I want to run them in parallel, node D, on the other hand, uses pretty much all my memory, and it will fail if it's executed alongside the other nodes. Is there a way that I can tell Kedro to wait for A, B, and C to finish before executing node D and keep the code organized?<\/p>",
        "Challenge_closed_time":1626713371123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626707886057,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has a pipeline in Kedro with four nodes, where nodes A, B, and C are not resource-intensive but take a while to run, and node D is memory-intensive and will fail if executed alongside the other nodes. The user is looking for a way to make Kedro wait for nodes A, B, and C to finish before executing node D while keeping the code organized.",
        "Challenge_last_edit_time":1626708364910,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68442999",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":13.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.5236294445,
        "Challenge_title":"Waiting for nodes to finish in Kedro",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":245.0,
        "Challenge_word_count":122,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423164285360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Itabira, Brazil",
        "Poster_reputation_count":856.0,
        "Poster_view_count":106.0,
        "Solution_body":"<p>Kedro determines the execution order based on the interdependencies between the inputs\/outputs of different nodes. In your case, node D doesn't depend on any of the other nodes, so execution order cannot be guaranteed. Similarly, it cannot be ensured that node D will <em>not<\/em> run in parallel to A, B and C if using a parallel runner.<\/p>\n<p>That said, there are a couple of workarounds one could use achieve a particular execution order.<\/p>\n<h5 id=\"preferred-run-the-nodes-separately-62tl\">1 [Preferred] Run the nodes separately<\/h5>\n<p>Instead of doing <code>kedro run --parallel<\/code>, you could do:<\/p>\n<pre><code>kedro run --pipeline foo --node A --node B --node C --parallel; kedro run --pipeline foo --node D\n<\/code><\/pre>\n<p>This is arguably the preferred solution because it requires no code changes (which is good in case you ever run the same pipeline on a different machine). You could do <code>&amp;&amp;<\/code> instead of <code>;<\/code> if you want node D to run only if A, B and C succeded. If the running logic gets more complex, you could store it in a Makefile\/bash script.<\/p>\n<h5 id=\"using-dummy-inputsoutputs-j7un\">2 Using dummy inputs\/outputs<\/h5>\n<p>You could also force the execution order by introducing dummy datasets. Something like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def foo():\n    return Pipeline([\n        node(a, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=[dict(bar_a=&quot;bar_a&quot;), &quot;a_done&quot;], name=&quot;A&quot;),\n        node(b, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=[dict(bar_b=&quot;bar_b&quot;), &quot;b_done&quot;], name=&quot;B&quot;),\n        node(c, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=[dict(bar_c=&quot;bar_c&quot;), &quot;c_done&quot;], name=&quot;C&quot;),\n        node(d, inputs=[&quot;train_x&quot;, &quot;test_x&quot;, &quot;a_done&quot;, &quot;b_done&quot;, &quot;c_done&quot;], outputs=dict(bar_d=&quot;bar_d&quot;), name=&quot;D&quot;),     \n    ])\n<\/code><\/pre>\n<p>Empty lists could do for the dummy datasets. The underlying functions would also have to return\/take the additional arguments.<\/p>\n<p>The advantage of this approach is that <code>kedro run --parallel<\/code> will immediately result in the desired execution logic. The disadvantage is that it pollutes the definition of nodes and underlying functions.<\/p>\n<p>If you go down this road, you'll also have to decide whether you want to store the dummy datasets in the data catalog (pollutes even more, but allows to run node D on its own) or not (node D cannot run on its own).<\/p>\n<hr \/>\n<p>Related discussions [<a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/issues\/132\" rel=\"nofollow noreferrer\">1<\/a>, <a href=\"https:\/\/stackoverflow.com\/questions\/58686533\/how-to-run-the-nodes-in-sequence-as-declared-in-kedro-pipeline\">2<\/a>]<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.1,
        "Solution_reading_time":36.36,
        "Solution_score_count":3.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":324.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1342713532568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seoul, South Korea",
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":3.9274925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In DVC one may define pipelines.  In Unix, one typically does not work at the root level.  Further, DVC expects files to be inside the git repository.<\/p>\n<p>So, this seems like a typical problem.<\/p>\n<p>Suppose I have the following:<\/p>\n<pre><code>\/home\/user\/project\/content-folder\/data\/data-type\/cfg.json\n\/home\/user\/project\/content-folder\/app\/foo.py\n<\/code><\/pre>\n<p>Git starts at <code>\/home\/user\/project\/<\/code><\/p>\n<pre><code>cd ~\/project\/content-folder\/data\/data-type\n..\/..\/app\/foo.py do-this --with cfg.json --dest $(pwd) \n<\/code><\/pre>\n<p>Seems reasonable to me: the script takes a configuration, which is stored in a particular location, runs it against some encapsulated functionality, and outputs it to the destination using an absolute path.<\/p>\n<p>The default behavior of <code>--dest<\/code> is to output to the current working directory.  This seems like another reasonable default.<\/p>\n<hr \/>\n<p>Next, I go to configure the <code>params.yaml<\/code> file for <code>dvc<\/code>, and I am immediately confusing and unsure what is going to happen.  I write:<\/p>\n<pre><code>foodoo:\n  params: do-this --with ????\/cfg.json --dest ????\n<\/code><\/pre>\n<p>What I want to write (and would in a shell script):<\/p>\n<pre><code>#!\/usr\/bin\/env bash\norigin:=$(git rev-parse --show-toplevel)\n\nverb=do-this\nparams=--with $(origin)\/content-folder\/data\/data-type\/cfg.json --dest $(origin)\/content-folder\/data\/data-type\n<\/code><\/pre>\n<hr \/>\n<p>But, in DVC, the pathing seems to be implicit, and I do not know where to start as either:<\/p>\n<ol>\n<li>DVC will calculate the path to my script locally<\/li>\n<li>Not calculate the path to my script locally<\/li>\n<\/ol>\n<p>Which is fine -- I can discover that.  But I am reasonably sure that DVC will absolutely not prefix the directory and file params in my params.yaml with the path to my project.<\/p>\n<hr \/>\n<p>How does one achieve path control that does not assume a fixed project location, like I would in BASH?<\/p>",
        "Challenge_closed_time":1608686869860,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608672730887,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with defining pipelines in DVC due to the use of absolute paths and project paths in the pipeline parameters. While the user is able to run the script with absolute paths in Unix, they are unsure how to configure the params.yaml file in DVC to achieve the same path control without assuming a fixed project location. The user is confused about the implicit pathing in DVC and is unsure if DVC will calculate the path to their script locally or not.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65416056",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":25.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":3.9274925,
        "Challenge_title":"Data Version Control: Absolute Paths and Project Paths in the Pipeline Parameters?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":448.0,
        "Challenge_word_count":262,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405262190020,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Atlanta, GA",
        "Poster_reputation_count":26244.0,
        "Poster_view_count":1383.0,
        "Solution_body":"<p>By default, DVC will run your stage command from the same directory as the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files#dvcyaml-file\" rel=\"nofollow noreferrer\">dvc.yaml<\/a> file. If you need to run the command from a different location, you can specify an alternate working directory via <code>wdir<\/code>, which should be a path relative to <code>dvc.yaml<\/code>'s location.<\/p>\n<p>Paths for everything else in your stage (like <code>params.yaml<\/code>) should be specified as relative to <code>wdir<\/code> (or relative to <code>dvc.yaml<\/code> if <code>wdir<\/code> is not provided).<\/p>\n<p>Looking at your example, there also seems to be a bit of confusion on parameters in DVC. In a DVC stage, <code>params<\/code> is for specifying <a href=\"https:\/\/dvc.org\/doc\/command-reference\/params\" rel=\"nofollow noreferrer\">parameter dependencies<\/a>, not used for specifying command-line flags. The full command including flags\/options should be included  the <code>cmd<\/code> section for your stage. If you wanted to make sure that your stage was rerun every time certain values in <code>cfg.json<\/code> have changed, your stage's <code>params<\/code> section would look something like:<\/p>\n<pre><code>params:\n  &lt;relpath from dvc.yaml&gt;\/cfg.json:\n    - param1\n    - param2\n    ...\n<\/code><\/pre>\n<p>So your example <code>dvc.yaml<\/code> would look something like:<\/p>\n<pre><code>stages:\n  foodoo:\n    cmd: &lt;relpath from dvc.yaml&gt;\/foo.py do-this --with &lt;relpath from dvc.yaml&gt;\/cfg.json --dest &lt;relpath from dvc.yaml&gt;\/...\n    deps:\n      &lt;relpath from dvc.yaml&gt;\/foo.py\n    params:\n      &lt;relpath from dvc.yaml&gt;\/cfg.json:\n        ...\n    ...\n<\/code><\/pre>\n<p>This would make the command <code>dvc repro<\/code> rerun your stage any time that the code in foo.py has changed, or the specified parameters in <code>cfg.json<\/code> have changed.<\/p>\n<p>You may also want to refer to the docs for <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#run\" rel=\"nofollow noreferrer\">dvc run<\/a>, which can be used to generate or update a <code>dvc.yaml<\/code> stage (rather than writing <code>dvc.yaml<\/code> by hand)<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":9.4,
        "Solution_reading_time":26.99,
        "Solution_score_count":2.0,
        "Solution_sentence_count":26.0,
        "Solution_word_count":248.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1327588060552,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":802.0,
        "Answerer_view_count":91.0,
        "Challenge_adjusted_solved_time":599.2239861111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In my pipeline multiple steps are independent and so I would like them to run in parallel based on input dependencies.<\/p>\n<p>As the compute I use has multiple nodes I would have expected this to be the default.<\/p>\n<p>For example:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Iye85.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iye85.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>All 3 upper steps should run in parallel, then both <code>finetune<\/code> steps in parallel as soon as their inputs are satisfied and the same for <code>rgb_test<\/code>.<\/p>\n<p>Currently only 1 step runs at a time, the other are <code>Queued<\/code>.<\/p>",
        "Challenge_closed_time":1632769372630,
        "Challenge_comment_count":2,
        "Challenge_created_time":1630612166280,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to run multiple independent `PythonScriptStep` steps in parallel based on input dependencies in their pipeline. They expected this to be the default as their compute has multiple nodes, but currently only one step runs at a time and the others are queued.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69036277",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":599.2239861111,
        "Challenge_title":"Run independent `PythonScriptStep` steps in parallel",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":156.0,
        "Challenge_word_count":93,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1327588060552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":802.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p>It ended up being because of vCPU quota.<\/p>\n<p>After increasing the quota, parallel tasks can run at the same time as expected.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":1.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1363409191768,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":404.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":1393.6949186111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I would like to update previous runs done with MLFlow, ie. changing\/updating a parameter value to accommodate a change in the implementation. Typical uses cases:<\/p>\n<ul>\n<li>Log runs using a parameter A, and much later, log parameters A and B. It would be useful to update the value of parameter B of previous runs using its default value.<\/li>\n<li>&quot;Specialize&quot; a parameter. Implement a model using a boolean flag as a parameter. Update the implementation to take a string instead. Now we need to update the values of the parameter for the previous runs so that it stays consistent with the new behavior.<\/li>\n<li>Correct a wrong parameter value loggued in the previous runs.<\/li>\n<\/ul>\n<p>It is not always easy to trash the whole experiment as I need to keep the previous runs for statistical purpose. I would like also not to generate new experiments just for a single new parameter, to keep a single database of runs.<\/p>\n<p>What is the best way to do this?<\/p>",
        "Challenge_closed_time":1606920349240,
        "Challenge_comment_count":2,
        "Challenge_created_time":1601903047533,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to update previous runs done with MLFlow, specifically changing\/updating a parameter value to accommodate a change in the implementation. The user wants to avoid trashing the whole experiment and generating new experiments just for a single new parameter, to keep a single database of runs.",
        "Challenge_last_edit_time":1607788863848,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64209196",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.4,
        "Challenge_reading_time":12.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":1393.6949186111,
        "Challenge_title":"How to update a previous run into MLFlow?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":2834.0,
        "Challenge_word_count":171,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1347312347147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1022.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>To add or correct a parameter, metric or artifact of an existing run, pass run_id instead of experiment_id to mlflow.start_run function<\/p>\n<pre><code>with mlflow.start_run(run_id=&quot;your_run_id&quot;) as run:\n    mlflow.log_param(&quot;p1&quot;,&quot;your_corrected_value&quot;)\n    mlflow.log_metric(&quot;m1&quot;,42.0) # your corrected metrics\n    mlflow.log_artifact(&quot;data_sample.html&quot;) # your corrected artifact file\n<\/code><\/pre>\n<p>You can correct, add to, or delete any MLflow run any time after it is complete. Get the run_id either from the UI or by using <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.search_runs\" rel=\"noreferrer\">mlflow.search_runs<\/a>.<\/p>\n<p>Source: <a href=\"https:\/\/towardsdatascience.com\/5-tips-for-mlflow-experiment-tracking-c70ae117b03f\" rel=\"noreferrer\">https:\/\/towardsdatascience.com\/5-tips-for-mlflow-experiment-tracking-c70ae117b03f<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.6,
        "Solution_reading_time":12.37,
        "Solution_score_count":10.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":69.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":93.9451402778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Apologies for the long post.<\/p>\n<p>Originally, I had data in one location on an S3 bucket and used to train deep learning image classification models on this data using the typical 'File' mode and passing the S3 uri where the data is stored as training input. To try and accelerate training, I wanted to switch to using:<\/p>\n<ol>\n<li>Pipe mode, to stream data and not download all the data at the beginning of the training, starting training faster and saving disk space.<\/li>\n<li>Augmented Manifest File coupled with 1., so that I don't have to place my data in a single location on S3, so I avoid moving data around when I train models.<\/li>\n<\/ol>\n<p>I was making my script similar to <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=934156#934156\" rel=\"nofollow noreferrer\">the one in this example<\/a>. I printed the steps done when parsing the data, however I noticed that the data might not have been read because when printing it shows the following:<\/p>\n<pre><code>step 1 Tensor(&quot;ParseSingleExample\/ParseExample\/ParseExampleV2:0&quot;, shape=(), dtype=string)\nstep 2 Tensor(&quot;DecodePng:0&quot;, shape=(None, None, 3), dtype=uint8)\nstep 3 Tensor(&quot;Cast:0&quot;, shape=(None, None, 3), dtype=float32)\n<\/code><\/pre>\n<p>I guess the image is not being read\/found since the shape is <code>[None, None, 3]<\/code> when it should be <code>[224, 224, 3]<\/code>, so maybe the problem is from the Augmented Manifest file?<\/p>\n<p>Here's an example of how my Augmented Manifest file is written:<\/p>\n<pre><code>{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image1.png&quot;, &quot;label&quot;: 1}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image2.png&quot;, &quot;label&quot;: 2}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image3.png&quot;, &quot;label&quot;: 3}\n<\/code><\/pre>\n<p>Some other details I should probably mention:<\/p>\n<ol>\n<li>When I create the Training Input I pass <code>'content_type': 'application\/x-recordio', 'record_wrapping': 'RecordIO'<\/code>, even though my data are in .png format, but I assumed that as the augmented manifest file is read the data get wrapped in the RecordIO format.<\/li>\n<li>Following my first point, I pass <code>PipeModeDataset(channel=channel, record_format='RecordIO')<\/code>, so also not sure about the RecordIO thing.<\/li>\n<\/ol>\n<p>There isn't an actual error that is raised, just when I start fitting the model nothing happens, it keeps on running but nothing actually runs so I'm trying to find the issue.<\/p>\n<hr \/>\n<p>EDIT: It now reads the shape correctly, but there's still the issue where it enters the .fit method and does nothing, just keeps running without doing anything. Find part of the script below.<\/p>\n<pre><code>def train_input_fn(train_channel):\n    &quot;&quot;&quot;Returns input function that feeds the model during training&quot;&quot;&quot;\n    return _input_fn(train_channel)\n\ndef _input_fn(channel):\n    &quot;&quot;&quot;\n        Returns a Dataset which reads from a SageMaker PipeMode channel.\n    &quot;&quot;&quot;\n    \n    features = {\n        'image-ref': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([3], tf.int64),\n    }\n \n    def combine(records):\n        return records[0], records[1]\n \n    def parse(record):\n        \n        parsed = tf.io.parse_single_example(record, features)\n        \n                 \n\n        image = tf.io.decode_png(parsed[&quot;image-ref&quot;], channels=3, dtype=tf.uint8)\n        image = tf.reshape(image, [224, 224, 3])\n        \n        lbl = parsed['label']\n        print(image, lbl)\n        return (image, lbl)\n \n    ds = PipeModeDataset(channel=channel, record_format='RecordIO')\n    ds = ds.map(parse, num_parallel_calls=AUTOTUNE)\n    ds = ds.prefetch(AUTOTUNE)\n \n    return ds\n\ndef model(dataset):\n    &quot;&quot;&quot;Generate a simple model&quot;&quot;&quot;\n    inputs = Input(shape=(224, 224, 3))\n    prediction_layer = Dense(2, activation = 'softmax')\n\n\n    x = inputs\n    x = tf.keras.applications.mobilenet.MobileNet(include_top=False, input_shape=(224,224,3), weights='imagenet')(x)\n    outputs = prediction_layer(x)\n    rec_model = tf.keras.Model(inputs, outputs)    \n    \n    rec_model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        metrics=['accuracy']\n    )\n    \n    \n    rec_model.fit(\n        dataset\n    )\n\n    return rec_model\n\ndef main(params):\n    \n    epochs = params['epochs']\n    train_channel = params['train_channel']\n    record_format = params['record_format']\n    batch_size = params['batch_size']\n        \n    train_spec = train_input_fn(train_channel)\n    model_classifier = model(train_spec)\n<\/code><\/pre>",
        "Challenge_closed_time":1647607100132,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647258085310,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to switch to using Pipe mode and Augmented Manifest File to train deep learning image classification models on data stored in different locations on an S3 bucket. However, the user is facing issues with the image not being read\/found, and the model not fitting, even though there are no errors raised. The user suspects that the issue might be with the Augmented Manifest file. The user has provided details about the Augmented Manifest file and the code used for training.",
        "Challenge_last_edit_time":1647268897627,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71467176",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":58.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":96.9485616667,
        "Challenge_title":"How can I verify that my training job is reading the augmented manifest file?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":127.0,
        "Challenge_word_count":493,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1576016596283,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Beirut, Lebanon",
        "Poster_reputation_count":15.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>From <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<blockquote>\n<p>A PipeModeDataset can read TFRecord, RecordIO, or text line records.<\/p>\n<\/blockquote>\n<p>While your'e trying to read binary (PNG) files. I don't see a relevant <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions\/tree\/tf-2\/src\/pipemode_op\/RecordReader\" rel=\"nofollow noreferrer\">record reader here<\/a> to help you do that.<br \/>\nYou could build your own format pipe implementation like shown <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">here<\/a>, but it's considerably more effort.<\/p>\n<p>Alternatively, you mentioned your files are scattered in different folders, but if your files common path contains less than 2M files, you could use <a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/10\/amazon-sagemaker-fast-file-mode\/\" rel=\"nofollow noreferrer\">FastFile mode<\/a> to <strong>stream<\/strong> data. Currently, FastFile only supports an S3 Prefix, so you won't be able to use a manifest.<\/p>\n<p>Also see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/choose-the-best-data-source-for-your-amazon-sagemaker-training-job\/\" rel=\"nofollow noreferrer\">general pros\/cons discussion of the different available storage and input types available in SageMaker<\/a>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":15.7,
        "Solution_reading_time":19.46,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":128.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1501114346136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Australia",
        "Answerer_reputation_count":37.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":203.8750488889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I built a multiclass SVM model in R and used Create R model module from azure to train and predict my testing dataset. Here are the trainer and the score R scripts.<\/p>\n\n<p><strong>Trainer R script:<\/strong> <\/p>\n\n<pre><code>library(e1071)\nfeatures &lt;- get.feature.columns(dataset)\nlabels   &lt;- as.factor(get.label.column(dataset))\ntrain.data &lt;- data.frame(features, labels)\nfeature.names &lt;- get.feature.column.names(dataset)\nnames(train.data) &lt;- c(feature.names, \"Class\")\nmodel &lt;- svm(Class ~ . , train.data)\n<\/code><\/pre>\n\n<p><strong>Scores R script:<\/strong><\/p>\n\n<pre><code>library(e1071)    \nclasses &lt;- predict(model, dataset)\nclasses &lt;- as.factor(classes)\nres &lt;- data.frame(classes, probabilities = 0.5)\nprint(str(res))\nprint(res)\nscores &lt;- res\n<\/code><\/pre>\n\n<p>Note in my code, I hardcoded the probability values to simplify the code.<\/p>\n\n<p>Here is my component design in Azure: <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/hjMC4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hjMC4.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I run the experiment, all the components work fine. However, in the score model, the scored dataset port does not show the predicted values. It only shows feature values from the testing dataset. I checked the output log of <em>Score model<\/em> and I could see the model has nicely predicted the testing data (note I added print commands in the Scores R script). But this is not enough and I need the prediction returned from the score model so I can pass it via API.<\/p>\n\n<p>Has anyone faced this issue before?<\/p>",
        "Challenge_closed_time":1534294797823,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533539693247,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has built a multiclass SVM model in R and used Azure to train and predict the testing dataset. However, the score model does not return the predicted values, only the feature values from the testing dataset. The user has checked the output log of the score model and found that the model has predicted the testing data correctly. The user needs the prediction returned from the score model to pass it via API.",
        "Challenge_last_edit_time":1533560847647,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51702359",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":21.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":209.7512711111,
        "Challenge_title":"In Azure ML Studio, score model doesn't return predicted values from an R model",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":657.0,
        "Challenge_word_count":216,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501114346136,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Australia",
        "Poster_reputation_count":37.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>I found an answer for this. In fact, I cannot see the result in the outcome of the scoring model but when I linked it to a <em>select column in the dataset<\/em> module, I see the predicted columns there.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":2.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":32.6179794445,
        "Challenge_answer_count":1,
        "Challenge_body":"Using Sagemaker's Python SDK 2.11 when I run my pipeline, I see this strange warning message: \n```\n\/personal_dir\/lib\/python3.8\/site-packages\/sagemaker\/workflow\/pipeline_context.py:233: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n```\n Before, I ran the exact same pipeline script with LocalPipelineSession without any problems and without any kind of weird warning messages. \n\nThis is how I am creating the PipelineSession object:\n```\ndef get_session(region, default_bucket):\n    boto_session = boto3.Session(region_name=region)\n    sagemaker_client = boto_session.client(\"sagemaker\")\n\n    return PipelineSession(\n        boto_session=boto_session,\n        sagemaker_client=sagemaker_client,\n        default_bucket=default_bucket\n    )\n```\nIm getting the region in the following way:\n```\nimport boto3\n\nregion = boto3.Session().region_name\n```\nI have tried to search the web for the meaning of that warning message, but could not find anything. What does that warning message means?? Am I doing something wrong and what can I do to make that warning disapear",
        "Challenge_closed_time":1669741153535,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669623728809,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a strange warning message while running a pipeline using Sagemaker's Python SDK 2.11. The warning message states that there will be no wait, no logs, and no job being started as the user is running within a PipelineSession. The user did not face any problems while running the same pipeline script with LocalPipelineSession. The user is seeking clarification on the meaning of the warning message and how to make it disappear.",
        "Challenge_last_edit_time":1669970468600,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUm9Ml6PX2QdOA5VIaDMblQg\/sagemaker-pipeline-strange-warning-message",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":14.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":32.6179794445,
        "Challenge_title":"Sagemaker Pipeline strange warning message",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":118.0,
        "Challenge_word_count":132,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, this warning is simply to clarify that running in a pipeline session will defer execution of jobs - generating pipeline step definitions instead of kicking off the jobs straight away.\n\nFor example calls such as `Estimator.fit()` or `Processor.run()` when using a pipeline session won't **start a job** (or wait for it to complete, or stream logs from CloudWatch), just prepare a definition to build up a pipeline that can be started later.\n\nIf you're already familiar with how PipelineSession works, I would say you can ignore it :-)  If not, can refer to the [SDK docs here for more details](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_model_building_pipeline.html#pipeline-session).\n\nCould be that there's an inconsistency between `LocalPipelineSession` versus `PipelineSession` in showing the message? Or that you disagree this message should be at warning level... Either way I'd suggest raising an issue on the [SageMaker Python SDK GitHub](https:\/\/github.com\/aws\/sagemaker-python-sdk) might be a good way to log that feedback with the team!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1669741153536,
        "Solution_link_count":2.0,
        "Solution_readability":10.9,
        "Solution_reading_time":13.4,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":149.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1548390570396,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":124.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":26.2102008333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm doing following tutorial. I failed to run &quot;Create a control script&quot;.<\/p>\n<p>What could be wrong?<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-hello-world\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-hello-world<\/a><\/p>\n<pre><code>azureuser@kensmlcompute:~\/cloudfiles\/code\/Users\/my.name\/get-started$ python run-hello.py \nFailure while loading azureml_run_type_providers. Failed to load entrypoint automl = \nazureml.train.automl.run:AutoMLRun._from_run_dto with exception (pyarrow 4.0.0 \n(\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages), \nRequirement.parse('pyarrow&lt;4.0.0,&gt;=0.17.0'), {'azureml-dataset-runtime'}).\nhttps:\/\/ml.azure.com\/runs\/day1-experiment-hello_1623766747_073126f5? \nwsid=\/subscriptions\/1679753a-501e-4e46-9bff- \n6120ed5694cf\/resourcegroups\/kensazuremlrg\/workspaces\/kensazuremlws&amp;tid=94fe1041-ba47-4f49- \n866b- \n06c297c116cc\nazureuser@kensmlcompute:~\/cloudfiles\/code\/Users\/my.name\/get-started$\n<\/code><\/pre>",
        "Challenge_closed_time":1623861331176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623766974453,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an issue while running the \"Create a control script\" step in the Azure ML tutorial. The error message indicates a failure to load the entry point automl, with a specific exception related to pyarrow. The user has provided a link to the tutorial and the error message for reference.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67988138",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":26.4,
        "Challenge_reading_time":15.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":26.2102008333,
        "Challenge_title":"Azure ML Tutorial - Failed to load entrypoint automl",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1241.0,
        "Challenge_word_count":54,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1478251050692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Finland",
        "Poster_reputation_count":1519.0,
        "Poster_view_count":375.0,
        "Solution_body":"<p>I think the error indicates that your environment is using pyarrow package which is of version 4.0.0 whereas azureml-dataset-runtime requires the package to be &gt;=0.17.0 but &lt;4.0.0<\/p>\n<p>It would be easier for you to uninstall the package and install a specific version. The list of releases of pyarrow are available here.<\/p>\n<p>Since you are using a notebook create new cells and run these commands.<\/p>\n<pre><code> !pip uninstall pyarrow\n !pip install -y pyarrow==3.0.0\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":6.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":3.9519652778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Amazon SageMaker has <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">inference pipelines<\/a> that process requests for inferences on data. It sounds as though inferences are similar (or perhaps identical) to predictions. Are there any differences between inferences and predictions? If so, what? If not, why not just call it a prediction pipeline?<\/p>",
        "Challenge_closed_time":1561569885852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561555658777,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the difference between \"inference\" and \"prediction\" in Amazon SageMaker's inference pipelines. They are questioning whether the terms are interchangeable and why the term \"prediction pipeline\" is not used instead.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56773989",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":6.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.9519652778,
        "Challenge_title":"In Amazon SageMaker, what (if any) is the difference between an inference and prediction?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":42.0,
        "Challenge_word_count":61,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1336973807643,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Minneapolis, MN, United States",
        "Poster_reputation_count":1907.0,
        "Poster_view_count":174.0,
        "Solution_body":"<p>Inference usually refers to applying a learned transformation to input data. That learned transformation could be something else than a prediction (eg dim reduction, clustering, entity extraction etc). So calling that process a prediction would be a bit too restrictive in my opinion<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":3.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":9.1086713889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a <code>sagemaker.workflow.pipeline.Pipeline<\/code> which contains multiple <code>sagemaker.workflow.steps.ProcessingStep<\/code> and each <code>ProcessingStep<\/code> contains <code>sagemaker.processing.ScriptProcessor<\/code>.<\/p>\n<p>The current pipeline graph look like the below shown image. It will take data from multiple sources from S3, process it and create a final dataset using the data from previous steps.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6XImq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6XImq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As the <code>Pipeline<\/code> object doesn't support <code>.deploy<\/code> method, how to deploy this pipeline?<\/p>\n<p>While inference\/scoring, When we receive a raw data(single row for each source), how to trigger the pipeline?<\/p>\n<p>or Sagemaker Pipeline is designed for only data processing and model training on huge\/batch data? Not for the inference with the single data point?<\/p>",
        "Challenge_closed_time":1639073178300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639040387083,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a sagemaker.workflow.pipeline.Pipeline that contains multiple sagemaker.workflow.steps.ProcessingStep and sagemaker.processing.ScriptProcessor. They are unsure how to deploy the pipeline as the Pipeline object does not support the .deploy method. Additionally, they are unsure how to trigger the pipeline during inference\/scoring with a single data point.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70287087",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":13.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":9.1086713889,
        "Challenge_title":"How to deploy sagemaker.workflow.pipeline.Pipeline?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":313.0,
        "Challenge_word_count":112,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1564208933767,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":491.0,
        "Poster_view_count":59.0,
        "Solution_body":"<blockquote>\n<p>As the Pipeline object doesn't support .deploy method, how to deploy this pipeline?<\/p>\n<\/blockquote>\n<p>Pipeline does not have a <code>.deploy()<\/code> method, no<\/p>\n<p>Use <code>pipeline.upsert(role_arn='...')<\/code> to create\/update the pipeline definition to SageMaker, then call <code>pipeline.start()<\/code> . Docs <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#pipeline\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<blockquote>\n<p>While inference\/scoring, When we receive a raw data(single row for each source), how to trigger the pipeline?<\/p>\n<\/blockquote>\n<p>There are actually two types of pipelines in SageMaker. Model Building Pipelines (which you have in your question), and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>, which are used for Inference. AWS definitely should have called the former &quot;workflows&quot;<\/p>\n<p>You can use a model building pipeline to setup a serial inference pipeline<\/p>\n<p>To do pre-processing in a serial inference pipeline, you want to train an encoder\/estimator (such as SKLearn) and save its model. Then train a learning algorithm, and save its model, then create a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/pipeline.html\" rel=\"nofollow noreferrer\">PipelineModel<\/a> using both models<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.5,
        "Solution_reading_time":18.8,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":150.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1619204963587,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1442.0,
        "Answerer_view_count":879.0,
        "Challenge_adjusted_solved_time":478.5142619445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have successfully run the following pipeline that creates a dataset, trains a model, and deploys to an endpoint using VertexAIs pipeline tool when everything is based in us-central1. Now, when I change the region to europe-west2, I get the following error:<\/p>\n<pre><code>debug_error_string = &quot;{&quot;created&quot;:&quot;@1647430410.324290053&quot;,&quot;description&quot;:&quot;Error received from peer\nipv4:172.217.169.74:443&quot;,&quot;file&quot;:&quot;src\/core\/lib\/surface\/call.cc&quot;,&quot;file_line&quot;:1066,\n&quot;grpc_message&quot;:&quot;List of found errors:\\t1.Field: name; Message: \nThe provided location ID doesn't match the endpoint. The valid location ID is `us-central1`.\\t&quot;,&quot;grpc_status&quot;:3}&quot;\n<\/code><\/pre>\n<p>This error occurs after the dataset is created in europe-west2, and before the model starts to train. Here is my code:<\/p>\n<pre><code>#import libraries\nfrom typing import NamedTuple\nimport kfp\nfrom kfp import dsl\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import (Artifact, Input, InputPath, Model, Output, \n                        OutputPath, ClassificationMetrics, Metrics, component)\nfrom kfp.v2.components.types.artifact_types import Dataset\nfrom kfp.v2.google.client import AIPlatformClient\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom google.api_core.exceptions import NotFound\n\n@kfp.dsl.pipeline(name=f&quot;lookalike-model-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = f&quot;bq:\/\/{PROJECT_ID}.{DATASET_ID}.{TABLE_NAME}&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;europe-west2&quot;,\n    api_endpoint: str = &quot;europe-west2-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auPrc&quot;: 0.5}',\n):\n            \n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project,\n        display_name=display_name, \n        bq_source=bq_source,\n        location = gcp_region\n    )\n\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        location=gcp_region,\n        predefined_split_column_name=&quot;set&quot;,\n        column_transformations=[\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;agentId&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;postcode&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;isMobile&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;gender&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;timeOfDay&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;set&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;sale&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;sale&quot;,\n    )\n\ncompiler.Compiler().compile(\n    pipeline_func=pipeline, package_path=&quot;tab_classif_pipeline.json&quot;\n)\n\nml_pipeline_job = aiplatform.PipelineJob(\n    display_name=f&quot;{MODEL_PREFIX}_training&quot;,\n    template_path=&quot;tab_classif_pipeline.json&quot;,\n    pipeline_root=PIPELINE_ROOT,\n    parameter_values={&quot;project&quot;: PROJECT_ID, &quot;display_name&quot;: DISPLAY_NAME},\n    enable_caching=True,\n    location=&quot;europe-west2&quot;\n)\nml_pipeline_job.submit()\n<\/code><\/pre>\n<p>As previously mentioned, the dataset gets created so I suspect that the issue must lie in <code>training_op = gcc_aip.AutoMLTabularTrainingJobRunOp<\/code><\/p>\n<p>I tried providing another endpoint: <code>eu-aiplatform.googleapis.com<\/code> which yielded the following error:<\/p>\n<pre><code>google.api_core.exceptions.InvalidArgument: 400 List of found errors: 1.Field: name; Message: \nThe provided location ID doesn't match the endpoint. The valid location ID is `us-central1`.\n\nFail to send metric: [rpc error: code = PermissionDenied desc = Permission monitoring.metricDescriptors.create \ndenied (or the resource may not exist).; rpc error: code = PermissionDenied desc = Permission monitoring.timeSeries.create\n denied (or the resource may not exist).]\n<\/code><\/pre>\n<p>I understand that I am not passing api-endpoint to any of the methods above, but I thought I'd highlight that the error changed slightly.<\/p>\n<p>Does anyone know what the issue may be? Or how I can run <code>gcc_aip.AutoMLTabularTrainingJobRunOp<\/code> in europe-west2 (or EU in general)?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1649155832383,
        "Challenge_comment_count":1,
        "Challenge_created_time":1647433181040,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to run a pipeline using VertexAI in the europe-west2 region. The error message states that the provided location ID does not match the endpoint, and suggests that the valid location ID is us-central1. The error occurs after the dataset is created and before the model starts to train. The user has tried changing the API endpoint to eu-aiplatform.googleapis.com, but the error persists. The user is seeking help to resolve the issue and run the pipeline in europe-west2 or EU in general.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71496966",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":17.4,
        "Challenge_reading_time":59.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":478.5142619445,
        "Challenge_title":"VertexAI Pipelines: The provided location ID doesn't match the endpoint",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":220.0,
        "Challenge_word_count":357,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562706291280,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>Try Updating the pipeline component using the command:<\/p>\n<p><code>pip3 install --force-reinstall google_cloud_pipeline_components==0.1.3<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.2,
        "Solution_reading_time":2.07,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":12.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1343828614448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seville, Spain",
        "Answerer_reputation_count":359.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":84.2769869445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>is there a way to know the progress percentage a ParallelRunStep has already computed on a pipeline?<\/p>\n<p>As the total number of inputs is known in advance, I think it should not be hard to get this information.<\/p>\n<p>This would be a great feedback for pipelines that takes long time to finish.<\/p>",
        "Challenge_closed_time":1619693209260,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619390617157,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to track the progress percentage of a ParallelRunStep in AzureML pipeline, as it would provide useful feedback for long-running pipelines.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67258917",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":4.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":84.0533619444,
        "Challenge_title":"AzureML ParallelRunStep progress information",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":23.0,
        "Challenge_word_count":55,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343828614448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seville, Spain",
        "Poster_reputation_count":359.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>Answer from python azure sdk: <em>In Studio, if you go to the step's Metrics tab, you will be able to see a chart\/table of execution progress, including remaining items, remaining mini batches, failed items, etc.<\/em><\/p>\n<p><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/18357\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/18357<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1619694014310,
        "Solution_link_count":2.0,
        "Solution_readability":11.9,
        "Solution_reading_time":5.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6764758334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I only have a few components. What\u2019s wrong with my workspace? I have removed all filters but not working. Can I get some guidance from it?<\/p>",
        "Challenge_closed_time":1677513365643,
        "Challenge_comment_count":1,
        "Challenge_created_time":1677510930330,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is experiencing an issue where some components are disappearing from their workspace despite having removed all filters. They are seeking guidance on how to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1184712\/components-disappear",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.6,
        "Challenge_reading_time":2.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.6764758334,
        "Challenge_title":"Components disappear",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":27,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=d55db955-94bc-459c-9988-f19bdf6adaee\">sona sathe<\/a>, <\/p>\n<p>Thanks you for reaching out to us here. I just did some researches and I found there is a reason may cause your issue. Could you please confirm if you are in the classic prebuild pipeline so that you have the component you want? If you are not, please try the Classic prebuilt pipeline. <\/p>\n<p>If you are in but you can not  find the component you want, please share the name to me and the screenshot, I will forward it to product team. <\/p>\n<p>I hope this helps! <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/dfae8944-0259-492d-bd74-d6393214c5c9?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p>Regards,<\/p>\n<p>Yutong<\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.2,
        "Solution_reading_time":11.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":118.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":625.9599527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How to I properly cancel all child runs in an Azure ML experiment? When I use the code below as expected from documentation, I get an error. &quot;RunConfigurationException:  <br \/>\nMessage: Error in deserialization. dict fields don't have list element type information. field=output_data, list_element_type=&lt;class 'azureml.core.runconfig.OutputData'&gt;...} with exception <strong>init<\/strong>() missing 2 required positional arguments: 'datastore_name' and 'relative_path'&quot;<\/p>\n<p>run = Run.get(ws, 'run-id-123456789')<\/p>\n<p>for child in run.get_children():  <br \/>\nprint(child.get_details())  <br \/>\ntry:  <br \/>\nchild.cancel()  <br \/>\nexcept Exception as e:  <br \/>\nprint(e)  <br \/>\ncontinue<\/p>\n<p>The datasets and runs were configured properly because they run just fine.<\/p>",
        "Challenge_closed_time":1651506755547,
        "Challenge_comment_count":1,
        "Challenge_created_time":1649253299717,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to cancel all child runs in an Azure ML experiment using the provided code, which results in a RunConfigurationException error. The datasets and runs were configured properly, but the code is not working as expected.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/802549\/cancel-all-child-runs-in-azure-ml",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.0,
        "Challenge_reading_time":10.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":625.9599527778,
        "Challenge_title":"Cancel all child runs in Azure ML",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":95,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>You should cancel all the children run by canceling the parent.   <\/p>\n<p>Any benefit to cancel child once a time? Just curious <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":1.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":68.5333333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Firstly I'd like to apologize if this is a dummy question.\r\nI'm following the tutorial to get introduced to kedro mlflow,; after running the command \"kedro mlflow init\" I tried to run the command \"kedro mlflofw ui\" but I get an error:\r\n\r\nINFO     The 'mlflow_tracking_uri' key in mlflow.yml is relative ('server.mlflow_tracking_uri = mlruns'). It is converted to a valid uri: 'file:\/\/\/C:\/Users\/e107338\/PycharmProjects\/mlflow\/kedro-mlflow-example\/mlruns'                                                   kedro_mlflow_config.py:202\r\n\r\nAfter the Traceback I get an error: FileNotFoundErrror\r\n",
        "Challenge_closed_time":1664786016000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1664539296000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with KedroPipelineModel where unnecessary pipeline input dependencies are required to execute. The `initial_catalog` property is causing problems as it contains some Kedro Datasets that are not necessary to train the model. The user used a Kedro plugin to load a specific dataset during training, but after updating the plugin, the model cannot be loaded as the load function uses the old Kedro Catalog with the old plugin version. The user suggests logging only necessary information in MLflow to avoid this issue. The user hopes for a solution where the Kedro Catalog can be updated without having to retrain the models.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/361",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":7.26,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":21.0,
        "Challenge_repo_issue_count":414.0,
        "Challenge_repo_star_count":145.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":68.5333333333,
        "Challenge_title":"kedro mlflow ui gets a FileNotFoundError",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":74,
        "Discussion_body":"Hi, \r\n\r\nI am sorry to see you are experiencing issues. this is not a dummy question, it sounds like a bug. \r\n\r\nI've just ran this: \r\n\r\n```bash\r\nconda create -n km-361 python=3.9 -y\r\nconda activate km-361\r\npip install kedro==0.18.3\r\npip install mlflow==1.29.0\r\npip install kedro-mlflow==0.11.3\r\nkedro new --starter=pandas-iris\r\ncd iris\r\nkedro mlflow init\r\nkedro mlflow ui\r\n```\r\n\r\nthen I opened ``http:\/\/127.0.0.1:5000`` and th UI opened as expected. \r\n\r\nCan you tell me: \r\n- your python version\r\n- your OS\r\n- your ``kedro`` \/ ``mlflow`` \/ ``kedro-mlflow`` version\r\n- the project using\r\n- the exact error message\r\n- check if you have a ``MLFLOW_TRACKING_URI`` environment set It turned out fine  after trying again! Sorry and thanks for your consideration!",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1492048364132,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cambridge, MA, USA",
        "Answerer_reputation_count":4436.0,
        "Answerer_view_count":713.0,
        "Challenge_adjusted_solved_time":0.2273408334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created an endpoint on us-east-1. try to create a predictor:<\/p>\n\n<pre><code>In [106]: sagemaker.predictor.RealTimePredictor(&lt;endpoint name&gt;)\n<\/code><\/pre>\n\n<p>and get<\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the DescribeEndpoint operation: \nCould not find endpoint \"arn:aws:sagemaker:us-east-2:&lt;account number&gt;:endpoint\/&lt;endpoint name&gt;\".\n<\/code><\/pre>\n\n<p>which is perfectly correct, since the endpoint is on us-east-1.  Probably I could change some defaults, but I'd rather not - I work on us-east-2 99% of the time.<\/p>\n\n<p>So, how can I set a different region when initializing the predictor?<\/p>",
        "Challenge_closed_time":1573497881147,
        "Challenge_comment_count":1,
        "Challenge_created_time":1573497062720,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an endpoint on us-east-1 and is trying to create a predictor but is encountering an error as the endpoint is not found in us-east-2. The user wants to know how to set a different region when initializing the predictor.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58806807",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.6,
        "Challenge_reading_time":9.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.2273408334,
        "Challenge_title":"Create a predictor from an endpoint in a different region",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":478.0,
        "Challenge_word_count":86,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553808322940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>The (python) <code>Predictors<\/code> <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"nofollow noreferrer\">documentation<\/a> shows that you can pass a <code>Session<\/code> object. In turn, the <code>Session<\/code> can be <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.Session\" rel=\"nofollow noreferrer\">initialized<\/a> with a <em>client<\/em> and a <em>runtime client<\/em> - the former does everything except endpoint invocations, the latter does... endpoint invocations.<\/p>\n\n<p>Those clients are tied to specific regions. It seems like you should be able to set the runtime client region to match your endpoint, by manually instantiating it, while leaving the regular client alone (disclaimer here: I haven't tried this - if you do, let me\/us know how it goes :)).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.1,
        "Solution_reading_time":10.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":94.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.4025,
        "Challenge_answer_count":0,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nWhen we open a pull request, chart-testing (lint) step in [release.yaml](https:\/\/github.com\/community-charts\/helm-charts\/blob\/main\/.github\/workflows\/release.yml#L60) file getting the following error.\r\n\r\n```\r\nError: Error linting charts: Error processing charts\r\n------------------------------------------------------------------------------------------------------------------------\r\n \u2716\ufe0e mlflow => (version: \"0.1.47\", path: \"charts\/mlflow\") > Error validating maintainer 'Burak Ince': 404 Not Found\r\n------------------------------------------------------------------------------------------------------------------------\r\n```\r\n\r\nBecause of maintainer name for the `ct lint` command must be a GitHub username rather than a real name.\n\n### What's your helm version?\n\nv3.9.0\n\n### What's your kubectl version?\n\nv1.24.2\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.1.47\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nct lint --debug --config .\/.github\/configs\/ct-lint.yaml --lint-conf .\/.github\/configs\/lintconf.yaml\n\n### Anything else we need to know?\n\n_No response_",
        "Challenge_closed_time":1656583953000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656578904000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue with MLFlow active run not being reported correctly in the MLFlow UI while using the `TrainingArguments` with `report_to=['mlflow']` and `run_name=\"run0\"`. The cause of the issue was identified as an incorrect check for the MLFlow active run in `src\/transformers\/integrations.py`. The expected behavior was for the MLFlow UI to report a run with a Run Name of `run0`.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/2",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":7.8,
        "Challenge_reading_time":18.48,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":15.0,
        "Challenge_repo_issue_count":43.0,
        "Challenge_repo_star_count":16.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":1.4025,
        "Challenge_title":"[mlflow] Run chart-testing (lint) step returns Error validating maintainer 404 Not Found error",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":147,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":16.3358102778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,     <\/p>\n<p>is it possible (or will be in the future) using Python SDK v2 to create pipeline endpoint (or endpoint + deployment)?    <br \/>\nIm looking for a way to submit a job for a created pipeline with a REST request.     <\/p>\n<p>For SDK v1 pipeline i was able to acquire satisfying result using Pipeline.publish method.    <\/p>\n<p>Thanks for any advice!<\/p>",
        "Challenge_closed_time":1664861262760,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664802453843,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking advice on whether it is possible to create a pipeline endpoint or endpoint with deployment using Python SDK v2 and submit a job for a created pipeline with a REST request. They were able to achieve satisfactory results with SDK v1 pipeline using Pipeline.publish method.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1033206\/publishing-aml-pipelines-with-sdk-v2",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":4.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":16.3358102778,
        "Challenge_title":"Publishing AML Pipelines with SDK v2",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":65,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=c12c38b8-0908-400e-b8ac-72519d30e7db\">@Maciej Stefaniak  <\/a>    <\/p>\n<p>Thanks for using Microsoft Q&amp;A platform. For how to publish pipeline, I don't find anything currently. But for deploy endpoint, please check on this sample repo for SDK v2, there are several samples for you to refer about how to deploy endpoint - <a href=\"https:\/\/github.com\/Azure\/azureml-examples\/tree\/v2samplesreorg\/sdk\/python\">https:\/\/github.com\/Azure\/azureml-examples\/tree\/v2samplesreorg\/sdk\/python<\/a>    <\/p>\n<p>Also, there is an example about using Azure Machine Learning (Azure ML) to create a production ready machine learning (ML) project, using AzureML Python SDK v2 (preview). - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-python-sdk\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-python-sdk<\/a>    <\/p>\n<p>I hope this helps, please let me know if you need more information or have any questiion regarding to above examples.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.7,
        "Solution_reading_time":14.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":123.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":19.3151194444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi    <br \/>\nI am unable to create a VM in Compute. Status is at Creating for an hour and then it fails.     <br \/>\nI tried several times without luck.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/78763-image.png?platform=QnA\" alt=\"78763-image.png\" \/>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/78781-image.png?platform=QnA\" alt=\"78781-image.png\" \/>    <\/p>\n<p>Does anyone know how to solve this?<\/p>",
        "Challenge_closed_time":1616052673920,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615983139490,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to create a VM in Compute and the status remains at \"Creating\" for an hour before failing. The user has tried multiple times without success and is seeking a solution to the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/318685\/cant-create-a-vm-in-compute-creation-failed",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":10.9,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":19.3151194444,
        "Challenge_title":"Can't create a VM in compute - Creation failed",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":52,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I was able to delete all the failed VMs and create one today.  <br \/>\nThe solution in this case was to wait it out.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":1.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1368311783008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Diego, CA",
        "Answerer_reputation_count":1297.0,
        "Answerer_view_count":165.0,
        "Challenge_adjusted_solved_time":1972.6603797222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need to import function from different python scripts, which will used inside <code>preprocessing.py<\/code> file. I was not able to find a way to pass the dependent files to <code>SKLearnProcessor<\/code> Object, due to which I am getting <code>ModuleNotFoundError<\/code>.<\/p>\n<p><strong>Code:<\/strong><\/p>\n<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     instance_type='ml.m5.xlarge',\n                                     instance_count=1)\n\n\nsklearn_processor.run(code='preprocessing.py',\n                      inputs=[ProcessingInput(\n                        source=input_data,\n                        destination='\/opt\/ml\/processing\/input')],\n                      outputs=[ProcessingOutput(output_name='train_data',\n                                                source='\/opt\/ml\/processing\/train'),\n                               ProcessingOutput(output_name='test_data',\n                                                source='\/opt\/ml\/processing\/test')],\n                      arguments=['--train-test-split-ratio', '0.2']\n                     )\n<\/code><\/pre>\n<p>I would like to pass,\n<code>dependent_files = ['file1.py', 'file2.py', 'requirements.txt']<\/code>. So, that <code>preprocessing.py<\/code> have access to all the dependent modules.<\/p>\n<p>And also need to install libraries from <code>requirements.txt<\/code> file.<\/p>\n<p>Can you share any work around or a right way to do this?<\/p>\n<p><strong>Update-25-11-2021:<\/strong><\/p>\n<p><strong>Q1.<\/strong>(Answered but looking to solve using <code>FrameworkProcessor<\/code>)<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/99f023e76a5db060907a796d4d8fee550f005844\/src\/sagemaker\/processing.py#L1426\" rel=\"noreferrer\">Here<\/a>, the <code>get_run_args<\/code> function, is handling <code>dependencies<\/code>, <code>source_dir<\/code> and <code>code<\/code> parameters by using <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/99f023e76a5db060907a796d4d8fee550f005844\/src\/sagemaker\/processing.py#L1265\" rel=\"noreferrer\">FrameworkProcessor<\/a>. Is there any way that we can set this parameters from <code>ScriptProcessor<\/code> or <code>SKLearnProcessor<\/code> or any other <code>Processor<\/code> to set them?<\/p>\n<p><strong>Q2.<\/strong><\/p>\n<p>Can you also please show some reference to use our <code>Processor<\/code> as <code>sagemaker.workflow.steps.ProcessingStep<\/code> and then use in <code>sagemaker.workflow.pipeline.Pipeline<\/code>?<\/p>\n<p>For having <code>Pipeline<\/code>, do we need <code>sagemaker-project<\/code> as mandatory or can we create <code>Pipeline<\/code> directly without any <code>Sagemaker-Project<\/code>?<\/p>",
        "Challenge_closed_time":1637782762627,
        "Challenge_comment_count":5,
        "Challenge_created_time":1630681185260,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in passing dependent files to the SKLearnProcessor object in Sagemaker, resulting in a ModuleNotFoundError. They are seeking a workaround or a proper way to pass dependent modules and install libraries from requirements.txt file. Additionally, they are looking for a way to set parameters like dependencies, source_dir, and code from ScriptProcessor or SKLearnProcessor or any other Processor to set them. They also want to know how to use their Processor as sagemaker.workflow.steps.ProcessingStep and then use it in sagemaker.workflow.pipeline.Pipeline.",
        "Challenge_last_edit_time":1637936310430,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69046990",
        "Challenge_link_count":2,
        "Challenge_participation_count":7,
        "Challenge_readability":16.5,
        "Challenge_reading_time":34.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":11.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":1972.6603797222,
        "Challenge_title":"How to pass dependency files to sagemaker SKLearnProcessor and use it in Pipeline?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":2139.0,
        "Challenge_word_count":201,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1500824148408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":4419.0,
        "Poster_view_count":962.0,
        "Solution_body":"<p>There are a couple of options for you to accomplish that.<\/p>\n<p>One that is really simple is adding all additional files to a folder, example:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 my_package\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file1.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file2.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 preprocessing.py\n<\/code><\/pre>\n<p>Then send this entire folder as another input under the same <code>\/opt\/ml\/processing\/input\/code\/<\/code>, example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor = SKLearnProcessor(\n    framework_version=&quot;0.20.0&quot;,\n    role=role,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    instance_count=1,\n)\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,  # &lt;- this gets uploaded as \/opt\/ml\/processing\/input\/code\/preprocessing.py\n    inputs=[\n        ProcessingInput(source=input_data, destination='\/opt\/ml\/processing\/input'),\n        # Send my_package as \/opt\/ml\/processing\/input\/code\/my_package\/\n        ProcessingInput(source='my_package\/', destination=&quot;\/opt\/ml\/processing\/input\/code\/my_package\/&quot;)\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n<\/code><\/pre>\n<p>What happens is that <code>sagemaker-python-sdk<\/code> is going to put your argument <code>code=&quot;preprocessing.py&quot;<\/code> under <code>\/opt\/ml\/processing\/input\/code\/<\/code> and you will have <code>my_package\/<\/code> under the same directory.<\/p>\n<p><strong>Edit:<\/strong><\/p>\n<p>For the <code>requirements.txt<\/code>, you can add to your <code>preprocessing.py<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nsubprocess.check_call([\n    sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;-r&quot;,\n    &quot;\/opt\/ml\/processing\/input\/code\/my_package\/requirements.txt&quot;,\n])\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1637783228436,
        "Solution_link_count":0.0,
        "Solution_readability":23.3,
        "Solution_reading_time":27.94,
        "Solution_score_count":17.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":134.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":162.2120102778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've created an Azure ML Endpoint Pipeline with a single 'Execute Python Script'.  From the script, I am looking for a way to access the input 'ParameterAssignments' that I POST to the endpoint to trigger the pipeline.  I expected to see them somewhere in Run.get_context(), but I haven't had any luck.  I simply need a way to POST arbitrary values that my Python scripts can access.  Thank you!<\/p>",
        "Challenge_closed_time":1603069686240,
        "Challenge_comment_count":2,
        "Challenge_created_time":1602485723003,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is having trouble accessing input parameter assignments in Azure Machine Learning endpoints. They have created a pipeline with a single 'Execute Python Script' and are looking for a way to access the input 'ParameterAssignments' that they POST to the endpoint to trigger the pipeline. They are unable to find them in Run.get_context() and need a way to POST arbitrary values that their Python scripts can access.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/123204\/how-do-i-access-an-input-parameter-in-azure-machin",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.3,
        "Challenge_reading_time":5.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":162.2120102778,
        "Challenge_title":"How do I access an input parameter in Azure Machine Learning endpoints?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I just confirmed with our engineer that you cannot set up a pipeline parameter and use it without tying it with any of the module parameter. So the workaround is  - make the pipeline parameter as one of the inputs (i.e. dataset) to &quot;Execute Python Script&quot; module and set it as pipeline parameter. Then you can change it every time when calling the pipeline.<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":4.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1032.455,
        "Challenge_answer_count":0,
        "Challenge_body":"Since we changed the vscode-azureml-remote extension to be of UI type it is not supported anymore in the web or codespaces.\r\n\r\nGiven that main extension depends on vscode-azureml-remote, main is also unavailable in the web or codespaces.\r\n\r\nChanging the dependency should enable the main extension in the web context again.",
        "Challenge_closed_time":1665527881000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1661811043000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to run and debug experiments locally on AzureML extension to VS Code version 0.10.0 as the option is not available in the settings. The user had to downgrade to version 0.6x to access the option. The current version also lacks documentation on the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1655",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.0,
        "Challenge_reading_time":4.74,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":100.0,
        "Challenge_repo_issue_count":2059.0,
        "Challenge_repo_star_count":290.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1032.455,
        "Challenge_title":"Can't use Azure ML features when remotely connected to a compute",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Discussion_body":"Seems to work when remotely connected via VS Code desktop, but it definitely doesn't work when connected via vscode.dev. The azure ml remote extension is disabled in this case. Seems like we should be able to support this. Yes, this is only on web platforms like vscode.dev or codespaces.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":58.5657183334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am interested in knowing how can I integrate a repository with Azure Machine Learning Workspace.<\/p>\n<h2>What have I tried ?<\/h2>\n<p>I have some experience with Azure Data Factory and usually I have setup workflows where<\/p>\n<ol>\n<li><p>I have a <code>dev<\/code> azure data factory instance that is linked to azure repository.<\/p>\n<\/li>\n<li><p>Changes made to the repository using the code editor.<\/p>\n<\/li>\n<li><p>These changes are published via the <code>adf_publish<\/code> branch to the live <code>dev<\/code> instance<\/p>\n<\/li>\n<li><p>I use CI \/ CD pipeline and the AzureRMTemplate task to deploy the templates in the publish branch to release the changes to <code>production<\/code> environment<\/p>\n<\/li>\n<\/ol>\n<h2>Question:<\/h2>\n<ul>\n<li>How can I achieve the same \/ similar workflow with Azure Machine Learning Workspace ?<\/li>\n<li>How is CI \/ CD done with Azure ML Workspace<\/li>\n<\/ul>",
        "Challenge_closed_time":1655682328283,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655471491697,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on how to integrate a repository with Azure Machine Learning Workspace and how to implement CI\/CD pipeline for the same. They have experience with Azure Data Factory and have set up workflows using CI\/CD pipeline and AzureRMTemplate task to deploy templates to the production environment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72659937",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":11.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":58.5657183334,
        "Challenge_title":"CI \/ CD and repository integration for Azure ML Workspace",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":136,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1271093246887,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United States",
        "Poster_reputation_count":9826.0,
        "Poster_view_count":1238.0,
        "Solution_body":"<p>The following workflow is the official practice to be followed to achieve the task required.<\/p>\n<ol>\n<li>Starting with the architecture mentioned below<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KdRUa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KdRUa.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ul>\n<li>we need to have a specific data store to handle the dataset<\/li>\n<li>Perform the regular code modifications using the IDE like Jupyter Notebook or VS Code<\/li>\n<li>Train and test the model<\/li>\n<li>To register and operate on the model, deploy the model image as a web service and operate the rest.<\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>Configure the CI Pipeline:<\/strong><\/li>\n<\/ol>\n<ul>\n<li><p>Follow the below steps to complete the procedure<\/p>\n<p><strong>Before implementation:<\/strong><\/p>\n<pre><code>- We need azure subscription enabled account\n- DevOps activation must be activated.\n<\/code><\/pre>\n<\/li>\n<li><p>Open DevOps portal with enabled SSO<\/p>\n<\/li>\n<li><p>Navigate to <strong>Pipeline -&gt; Builds -&gt; Choose the model which was created -&gt; Click on EDIT<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/yUVZl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/yUVZl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Build pipeline will be looking like below screen\n<a href=\"https:\/\/i.stack.imgur.com\/VSKJq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VSKJq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>We need to use Anaconda distribution for this example to get all the dependencies.<\/p>\n<\/li>\n<li><p>To install environment dependencies, check the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/tasks\/package\/conda-environment?view=azure-devops&amp;viewFallbackFrom=azdevops\" rel=\"nofollow noreferrer\">link<\/a><\/p>\n<\/li>\n<li><p>Use the python environment, under <strong>Install Requirements<\/strong> in user setup.<\/p>\n<\/li>\n<li><p>Select <strong>create or get workspace<\/strong> select your account subscription as mentioned in below screen<\/p>\n<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/vt0el.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vt0el.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ul>\n<li>Save the changes happened in other tasks and all those muse be in same subscription.\n<a href=\"https:\/\/i.stack.imgur.com\/WJxCL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WJxCL.png\" alt=\"enter image description here\" \/><\/a><\/li>\n<\/ul>\n<p>The entire CI\/CD procedure and solution was documented in <a href=\"https:\/\/www.azuredevopslabs.com\/labs\/vstsextend\/aml\/#author-praneet-singh-solanki\" rel=\"nofollow noreferrer\">link<\/a><\/p>\n<p><strong>Document Credit: Praneet Singh Solanki<\/strong><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":12.0,
        "Solution_readability":14.0,
        "Solution_reading_time":36.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":278.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1553088438367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tehran, Tehran Province, Iran",
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":68.1138777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For the first time, it is proceeding mlflow with port 5000.<\/p>\n<p>Testing Mlflow, problem is no attribute last_active_run in mlflow<\/p>\n<p>But, It was an example provided by Mlflow. <br \/>\nlink is here <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/sklearn_autolog\" rel=\"nofollow noreferrer\">mlflow<\/a><\/p>\n<p>What is problem and how can I change code?<\/p>\n<p>shell<\/p>\n<pre><code>wget https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/master\/examples\/sklearn_autolog\/utils.py\nwget https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/master\/examples\/sklearn_autolog\/pipeline.py\n<\/code><\/pre>\n<p>pipeline.py<\/p>\n<pre><code>from pprint import pprint\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nimport mlflow\nfrom utils import fetch_logged_data\n\n\ndef main():\n    # enable autologging\n    mlflow.sklearn.autolog()\n\n    # prepare training data\n    X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n    y = np.dot(X, np.array([1, 2])) + 3\n\n    # train a model\n    pipe = Pipeline([(&quot;scaler&quot;, StandardScaler()), (&quot;lr&quot;, LinearRegression())])\n    pipe.fit(X, y)\n    run_id = mlflow.last_active_run().info.run_id\n    print(&quot;Logged data and model in run: {}&quot;.format(run_id))\n\n    # show logged data\n    for key, data in fetch_logged_data(run_id).items():\n        print(&quot;\\n---------- logged {} ----------&quot;.format(key))\n        pprint(data)\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>utils.py<\/p>\n<pre><code>import mlflow\nfrom mlflow.tracking import MlflowClient\n\n\ndef yield_artifacts(run_id, path=None):\n    &quot;&quot;&quot;Yield all artifacts in the specified run&quot;&quot;&quot;\n    client = MlflowClient()\n    for item in client.list_artifacts(run_id, path):\n        if item.is_dir:\n            yield from yield_artifacts(run_id, item.path)\n        else:\n            yield item.path\n\n\ndef fetch_logged_data(run_id):\n    &quot;&quot;&quot;Fetch params, metrics, tags, and artifacts in the specified run&quot;&quot;&quot;\n    client = MlflowClient()\n    data = client.get_run(run_id).data\n    # Exclude system tags: https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#system-tags\n    tags = {k: v for k, v in data.tags.items() if not k.startswith(&quot;mlflow.&quot;)}\n    artifacts = list(yield_artifacts(run_id))\n    return {\n        &quot;params&quot;: data.params,\n        &quot;metrics&quot;: data.metrics,\n        &quot;tags&quot;: tags,\n        &quot;artifacts&quot;: artifacts,\n    }\n<\/code><\/pre>\n<p>Error message<\/p>\n<pre><code>INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '8cc3f4e03b4e417b95a64f1a9a41be63', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\nTraceback (most recent call last):\n  File &quot;\/Users\/taein\/Desktop\/mlflow\/pipeline.py&quot;, line 33, in &lt;module&gt;\n    main()\n  File &quot;\/Users\/taein\/Desktop\/mlflow\/pipeline.py&quot;, line 23, in main\n    run_id = mlflow.last_active_run().info.run_id\nAttributeError: module 'mlflow' has no attribute 'last_active_run'\n<\/code><\/pre>\n<p>Thanks for your helping<\/p>",
        "Challenge_closed_time":1658706466923,
        "Challenge_comment_count":1,
        "Challenge_created_time":1658461256963,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while testing Mlflow, specifically the 'last_active_run' attribute is missing in the mlflow module. The user has provided the code they are using, which is an example provided by Mlflow, and is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73074887",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":40.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":68.1138777778,
        "Challenge_title":"'mlflow' has no attribute 'last_active_run'",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":215.0,
        "Challenge_word_count":275,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1598180751547,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seoul, Repulic of Korea",
        "Poster_reputation_count":158.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>It's because of the mlflow version that you mentioned in the comments. <code>mlflow.last_active_run()<\/code> API was introduced in <a href=\"https:\/\/github.com\/mlflow\/mlflow\/releases\/tag\/v1.25.0\" rel=\"nofollow noreferrer\">mlflow 1.25.0\n<\/a>. So you should upgrade the mlflow or you can use the previous version of the code available <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/5e2cb3baef544b00a972dff9dd6fb764be20510b\/examples\/sklearn_autolog\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<pre><code>wget https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/5e2cb3baef544b00a972dff9dd6fb764be20510b\/examples\/sklearn_autolog\/utils.py\nwget https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/5e2cb3baef544b00a972dff9dd6fb764be20510b\/examples\/sklearn_autolog\/pipeline.py\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.5,
        "Solution_reading_time":10.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":49.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.1780152778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am building an Azure ML pipeline with the azureml Python SDK. The pipeline calls a PythonScriptStep which stores data on the workspaceblobstore of the AML workspace. <\/p>\n\n<p>I would like to extend the pipeline to export the pipeline data to an Azure Data Lake (Gen 1). Connecting the output of the PythonScriptStep directly to Azure Data Lake (Gen 1) is not supported by Azure ML as far as I understand. Therefore, I added an extra DataTransferStep to the pipeline, which takes the output from the PythonScriptStep as input directly into the DataTransferStep. According to the Microsoft documentation this should be possible.<\/p>\n\n<p>So far I have built this solution, only this results in a file of 0 bytes on the Gen 1 Data Lake. I think the output_export_blob PipelineData does not correctly references the test.csv, and therefore the DataTransferStep cannot find the input. How can I connect the DataTransferStep correctly with the PipelineData output from the PythonScriptStep?<\/p>\n\n<p>Example I followed:\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-with-data-dependency-steps.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-with-data-dependency-steps.ipynb<\/a><\/p>\n\n<p>pipeline.py<\/p>\n\n<pre><code>input_dataset = delimited_dataset(\n    datastore=prdadls_datastore,\n    folderpath=FOLDER_PATH_INPUT,\n    filepath=INPUT_PATH\n)\n\noutput_export_blob = PipelineData(\n    'export_blob',\n    datastore=workspaceblobstore_datastore,\n)\n\ntest_step = PythonScriptStep(\n    script_name=\"test_upload_stackoverflow.py\",\n    arguments=[\n        \"--output_extract\", output_export_blob,\n    ],\n    inputs=[\n        input_dataset.as_named_input('input'),\n    ],\n    outputs=[output_export_blob],\n    compute_target=aml_compute,\n    source_directory=\".\"\n)\n\noutput_export_adls = DataReference(\n    datastore=prdadls_datastore, \n    path_on_datastore=os.path.join(FOLDER_PATH_OUTPUT, 'test.csv'),\n    data_reference_name='export_adls'        \n)\n\nexport_to_adls = DataTransferStep(\n    name='export_output_to_adls',\n    source_data_reference=output_export_blob,\n    source_reference_type='file',\n    destination_data_reference=output_export_adls,\n    compute_target=adf_compute\n)\n\npipeline = Pipeline(\n    workspace=aml_workspace, \n    steps=[\n        test_step, \n        export_to_adls\n    ]\n)\n<\/code><\/pre>\n\n<p>test_upload_stackoverflow.py<\/p>\n\n<pre><code>import os\nimport pathlib\nfrom azureml.core import Datastore, Run\n\nparser = argparse.ArgumentParser(\"train\")\nparser.add_argument(\"--output_extract\", type=str)\nargs = parser.parse_args() \n\nrun = Run.get_context()\ndf_data_all = (\n    run\n    .input_datasets[\"input\"]\n    .to_pandas_dataframe()\n)\n\nos.makedirs(args.output_extract, exist_ok=True)\ndf_data_all.to_csv(\n    os.path.join(args.output_extract, \"test.csv\"), \n    index=False\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1591813300647,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591811660073,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is building an Azure ML pipeline with the azureml Python SDK and wants to export pipeline data to an Azure Data Lake (Gen 1). They added a DataTransferStep to the pipeline, but it results in a file of 0 bytes on the Gen 1 Data Lake. The user suspects that the output_export_blob PipelineData does not correctly reference the test.csv, and therefore the DataTransferStep cannot find the input. They are seeking guidance on how to connect the DataTransferStep correctly with the PipelineData output from the PythonScriptStep.",
        "Challenge_last_edit_time":1591812659792,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62310010",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.5,
        "Challenge_reading_time":38.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":0.455715,
        "Challenge_title":"Azure ML PipelineData with DataTransferStep results in 0 bytes file",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":917.0,
        "Challenge_word_count":243,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509387489888,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":73.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>The code example is immensely helpful. Thanks for that. You're right that it can be confusing to get <code>PythonScriptStep -&gt; PipelineData<\/code>. Working initially even without the <code>DataTransferStep<\/code>.<\/p>\n\n<p>I don't know 100% what's going on, but I thought I'd spitball some ideas:<\/p>\n\n<ol>\n<li>Does your <code>PipelineData<\/code>,  <code>export_blob<\/code>, contain the \"test.csv\" file? I would verify that before troubleshooting the <code>DataTransferStep<\/code>. You can verify this using the SDK, or more easily with the UI.\n\n<ol>\n<li>Go to the PipelineRun page, click on the <code>PythonScriptStep<\/code> in question.<\/li>\n<li>On \"Outputs + Logs\" page, there's a \"Data Outputs\" Section (that is slow to load initially)<\/li>\n<li>Open it and you'll see the output PipelineDatas then click on \"View Output\"<\/li>\n<li>Navigate to given path either in the Azure Portal or Azure Storage Explorer.\n<a href=\"https:\/\/i.stack.imgur.com\/9LaEq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9LaEq.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/XbnhC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XbnhC.png\" alt=\"enter image description here\"><\/a><\/li>\n<\/ol><\/li>\n<li>In <code>test_upload_stackoverflow.py<\/code> you are treating the <code>PipelineData<\/code> as a directory when call <code>.to_csv()<\/code> as opposed to a file which would be you just calling <code>df_data_all.to_csv(args.output_extract, index=False)<\/code>. Perhaps try defining the <code>PipelineData<\/code> with <code>is_directory=True<\/code>. Not sure if this is required though.<\/li>\n<\/ol>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.0,
        "Solution_reading_time":21.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":184.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1545311054088,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":170.0,
        "Answerer_view_count":33.0,
        "Challenge_adjusted_solved_time":8.4162855556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Kedro pipeline, nodes (something like python functions) are declared sequentially. In some cases, the input of one node is the output of the previous node. However, sometimes, when kedro run API is called in the commandline, the nodes are not run sequentially.<\/p>\n\n<p>In kedro documentation, it says that by default the nodes are ran in sequence. <\/p>\n\n<p>My run.py code:<\/p>\n\n<pre><code>def main(\ntags: Iterable[str] = None,\nenv: str = None,\nrunner: Type[AbstractRunner] = None,\nnode_names: Iterable[str] = None,\nfrom_nodes: Iterable[str] = None,\nto_nodes: Iterable[str] = None,\nfrom_inputs: Iterable[str] = None,\n):\n\nproject_context = ProjectContext(Path.cwd(), env=env)\nproject_context.run(\n    tags=tags,\n    runner=runner,\n    node_names=node_names,\n    from_nodes=from_nodes,\n    to_nodes=to_nodes,\n    from_inputs=from_inputs,\n)\n<\/code><\/pre>\n\n<p>Currently my last node is sometimes ran before my first few nodes.<\/p>",
        "Challenge_closed_time":1572835437910,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572835098980,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with running nodes in sequence as declared in Kedro pipeline. Despite the default setting of running nodes sequentially, the nodes are not being executed in order when the Kedro run API is called in the command line. The user's last node is sometimes being executed before the first few nodes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58686533",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":12.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.0941472223,
        "Challenge_title":"How to run the nodes in sequence as declared in kedro pipeline?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1741.0,
        "Challenge_word_count":118,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545311054088,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":170.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>The answer that I recieved from Kedro github:<\/p>\n\n<blockquote>\n  <p>Pipeline determines the node execution order exclusively based on\n  dataset dependencies (node inputs and outputs) at the moment. So the\n  only option to dictate that the node A should run before node B is to\n  put a dummy dataset as an output of node A and an input of node B.<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1572865397608,
        "Solution_link_count":0.0,
        "Solution_readability":12.8,
        "Solution_reading_time":4.38,
        "Solution_score_count":5.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":61.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1645110475503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":15.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":104.3447397222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I follow the official tutotial from microsoft: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool<\/a><\/p>\n<p>When I execute:<\/p>\n<pre><code>#Bind model within Spark session\n model = pcontext.bind_model(\n     return_types=RETURN_TYPES, \n     runtime=RUNTIME, \n     model_alias=&quot;Sales&quot;, #This alias will be used in PREDICT call to refer  this   model\n     model_uri=AML_MODEL_URI, #In case of AML, it will be AML_MODEL_URI\n     aml_workspace=ws #This is only for AML. In case of ADLS, this parameter can be removed\n ).register()\n<\/code><\/pre>\n<p>I got : No module named 'azureml.automl'<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/g0UCX.png\" rel=\"nofollow noreferrer\">My Notebook<\/a><\/p>",
        "Challenge_closed_time":1648911550928,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648558433353,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is following a Microsoft tutorial on Synapse Analytics Auto ML and is encountering an error message \"No module named 'azureml.automl'\" when trying to execute a code block.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71662401",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":17.4,
        "Challenge_reading_time":12.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":98.0882152778,
        "Challenge_title":"Synapse Analytics Auto ML Predict No module named 'azureml.automl'",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":271.0,
        "Challenge_word_count":81,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1645110475503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I solved it. In my case it works best like this:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/JKEmr.png\" rel=\"nofollow noreferrer\">Imports<\/a><\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>#Import libraries\nfrom pyspark.sql.functions import col, pandas_udf,udf,lit\nfrom notebookutils.mssparkutils import azureML\nfrom azureml.core import Workspace, Model\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nfrom azureml.core.model import Model\nimport joblib\nimport pandas as pd\n\nws = azureML.getWorkspace(\"AzureMLService\")\nspark.conf.set(\"spark.synapse.ml.predict.enabled\",\"true\")<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ij760.png\" rel=\"nofollow noreferrer\">Predict function<\/a><\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>def forecastModel():\n    model_path = Model.get_model_path(model_name=\"modelName\", _workspace=ws)\n    modeljob = joblib.load(model_path + \"\/model.pkl\")\n\n    validation_data = spark.read.format(\"csv\") \\\n                            .option(\"header\", True) \\\n                            .option(\"inferSchema\",True) \\\n                            .option(\"sep\", \";\") \\\n                            .load(\"abfss:\/\/....csv\")\n\n    validation_data_pd = validation_data.toPandas()\n\n\n    predict = modeljob.forecast(validation_data_pd)\n\n    return predict<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1648934074416,
        "Solution_link_count":2.0,
        "Solution_readability":20.1,
        "Solution_reading_time":20.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1629385138956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":395.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":6.9681888889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>This is my first time using Google's Vertex AI Pipelines. I checked <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro?hl=en#0\" rel=\"nofollow noreferrer\">this codelab<\/a> as well as <a href=\"https:\/\/towardsdatascience.com\/how-to-set-up-custom-vertex-ai-pipelines-step-by-step-467487f81cad\" rel=\"nofollow noreferrer\">this post<\/a> and <a href=\"https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153\" rel=\"nofollow noreferrer\">this post<\/a>, on top of some links derived from the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/introduction?hl=es-419\" rel=\"nofollow noreferrer\">official documentation<\/a>. I decided to put all that knowledge to work, in some toy example: I was planning to build a pipeline consisting of 2 components: &quot;get-data&quot; (which reads some .csv file stored in Cloud Storage) and &quot;report-data&quot; (which basically returns the shape of the .csv data read in the previous component). Furthermore, I was cautious to include <a href=\"https:\/\/stackoverflow.com\/questions\/71351821\/reading-file-from-vertex-ai-and-google-cloud-storage\">some suggestions<\/a> provided in this forum. The code I currently have, goes as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output\nfrom google.cloud import aiplatform\n\n# Components section   \n\n@component(\n    packages_to_install=[\n        &quot;google-cloud-storage&quot;,\n        &quot;pandas&quot;,\n    ],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;get_data.yaml&quot;\n)\ndef get_data(\n    bucket: str,\n    url: str,\n    dataset: Output[Dataset],\n):\n    import pandas as pd\n    from google.cloud import storage\n    \n    storage_client = storage.Client(&quot;my-project&quot;)\n    bucket = storage_client.get_bucket(bucket)\n    blob = bucket.blob(url)\n    blob.download_to_filename('localdf.csv')\n    \n    # path = &quot;gs:\/\/my-bucket\/program_grouping_data.zip&quot;\n    df = pd.read_csv('localdf.csv', compression='zip')\n    df['new_skills'] = df['new_skills'].apply(ast.literal_eval)\n    df.to_csv(dataset.path + &quot;.csv&quot; , index=False, encoding='utf-8-sig')\n\n\n@component(\n    packages_to_install=[&quot;pandas&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;report_data.yaml&quot;\n)\ndef report_data(\n    inputd: Input[Dataset],\n):\n    import pandas as pd\n    df = pd.read_csv(inputd.path)\n    return df.shape\n\n\n# Pipeline section\n\n@pipeline(\n    # Default pipeline root. You can override it when submitting the pipeline.\n    pipeline_root=PIPELINE_ROOT,\n    # A name for the pipeline.\n    name=&quot;my-pipeline&quot;,\n)\ndef my_pipeline(\n    url: str = &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n    bucket: str = &quot;my-bucket&quot;\n):\n    dataset_task = get_data(bucket, url)\n\n    dimensions = report_data(\n        dataset_task.output\n    )\n\n# Compilation section\n\ncompiler.Compiler().compile(\n    pipeline_func=my_pipeline, package_path=&quot;pipeline_job.json&quot;\n)\n\n# Running and submitting job\n\nfrom datetime import datetime\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun1 = aiplatform.PipelineJob(\n    display_name=&quot;my-pipeline&quot;,\n    template_path=&quot;pipeline_job.json&quot;,\n    job_id=&quot;mlmd-pipeline-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={&quot;url&quot;: &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;, &quot;bucket&quot;: &quot;my-bucket&quot;},\n    enable_caching=True,\n)\n\nrun1.submit()\n<\/code><\/pre>\n<p>I was happy to see that the pipeline compiled with no errors, and managed to submit the job. However &quot;my happiness lasted short&quot;, as when I went to Vertex AI Pipelines, I stumbled upon some &quot;error&quot;, which goes like:<\/p>\n<blockquote>\n<p>The DAG failed because some tasks failed. The failed tasks are: [get-data].; Job (project_id = my-project, job_id = 4290278978419163136) is failed due to the above error.; Failed to handle the job: {project_number = xxxxxxxx, job_id = 4290278978419163136}<\/p>\n<\/blockquote>\n<p>I did not find any related info on the web, neither could I find any log or something similar, and I feel a bit overwhelmed that the solution to this (seemingly) easy example, is still eluding me.<\/p>\n<p>Quite obviously, I don't what or where I am mistaking. Any suggestion?<\/p>",
        "Challenge_closed_time":1650657469576,
        "Challenge_comment_count":5,
        "Challenge_created_time":1650588056410,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to read data in Vertex AI Pipelines. They have followed various resources and created a pipeline with two components, but the \"get-data\" component is failing and causing the entire pipeline to fail. The user is unsure of what is causing the error and is seeking suggestions for a solution.",
        "Challenge_last_edit_time":1650845389252,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71962260",
        "Challenge_link_count":5,
        "Challenge_participation_count":7,
        "Challenge_readability":13.8,
        "Challenge_reading_time":56.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":19.281435,
        "Challenge_title":"Reading Data in Vertex AI Pipelines",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":892.0,
        "Challenge_word_count":382,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1629385138956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":395.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>With some suggestions provided in the comments, I think I managed to make my demo pipeline work. I will first include the updated code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output\nfrom datetime import datetime\nfrom google.cloud import aiplatform\nfrom typing import NamedTuple\n\n\n# Importing 'COMPONENTS' of the 'PIPELINE'\n\n@component(\n    packages_to_install=[\n        &quot;google-cloud-storage&quot;,\n        &quot;pandas&quot;,\n    ],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;get_data.yaml&quot;\n)\ndef get_data(\n    bucket: str,\n    url: str,\n    dataset: Output[Dataset],\n):\n    &quot;&quot;&quot;Reads a csv file, from some location in Cloud Storage&quot;&quot;&quot;\n    import ast\n    import pandas as pd\n    from google.cloud import storage\n    \n    # 'Pulling' demo .csv data from a know location in GCS\n    storage_client = storage.Client(&quot;my-project&quot;)\n    bucket = storage_client.get_bucket(bucket)\n    blob = bucket.blob(url)\n    blob.download_to_filename('localdf.csv')\n    \n    # Reading the pulled demo .csv data\n    df = pd.read_csv('localdf.csv', compression='zip')\n    df['new_skills'] = df['new_skills'].apply(ast.literal_eval)\n    df.to_csv(dataset.path + &quot;.csv&quot; , index=False, encoding='utf-8-sig')\n\n\n@component(\n    packages_to_install=[&quot;pandas&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;report_data.yaml&quot;\n)\ndef report_data(\n    inputd: Input[Dataset],\n) -&gt; NamedTuple(&quot;output&quot;, [(&quot;rows&quot;, int), (&quot;columns&quot;, int)]):\n    &quot;&quot;&quot;From a passed csv file existing in Cloud Storage, returns its dimensions&quot;&quot;&quot;\n    import pandas as pd\n    \n    df = pd.read_csv(inputd.path+&quot;.csv&quot;)\n    \n    return df.shape\n\n\n# Building the 'PIPELINE'\n\n@pipeline(\n    # i.e. in my case: PIPELINE_ROOT = 'gs:\/\/my-bucket\/test_vertex\/pipeline_root\/'\n    # Can be overriden when submitting the pipeline\n    pipeline_root=PIPELINE_ROOT,\n    name=&quot;readcsv-pipeline&quot;,  # Your own naming for the pipeline.\n)\ndef my_pipeline(\n    url: str = &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n    bucket: str = &quot;my-bucket&quot;\n):\n    dataset_task = get_data(bucket, url)\n\n    dimensions = report_data(\n        dataset_task.output\n    )\n    \n\n# Compiling the 'PIPELINE'    \n\ncompiler.Compiler().compile(\n    pipeline_func=my_pipeline, package_path=&quot;pipeline_job.json&quot;\n)\n\n\n# Running the 'PIPELINE'\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun1 = aiplatform.PipelineJob(\n    display_name=&quot;my-pipeline&quot;,\n    template_path=&quot;pipeline_job.json&quot;,\n    job_id=&quot;mlmd-pipeline-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={\n        &quot;url&quot;: &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n        &quot;bucket&quot;: &quot;my-bucket&quot;\n    },\n    enable_caching=True,\n)\n\n# Submitting the 'PIPELINE'\n\nrun1.submit()\n<\/code><\/pre>\n<p>Now, I will add some complementary comments, which in sum, managed to solve my problem:<\/p>\n<ul>\n<li>First, having the &quot;Logs Viewer&quot; (roles\/logging.viewer) enabled for your user, will greatly help to troubleshoot any existing error in your pipeline (Note: that role worked for me, however you might want to look for a better matching role for you own purposes <a href=\"https:\/\/cloud.google.com\/logging\/docs\/access-control?&amp;_ga=2.212939101.-833851896.1650314090&amp;_gac=1.182768084.1650632490.Cj0KCQjwpImTBhCmARIsAKr58cw9X0TGy2YeN-CFsM4RvEXDH_vL54Ce1ECrWh4_aJNoPEhgtXusUhwaAlIUEALw_wcB#permissions_and_roles\" rel=\"nofollow noreferrer\">here<\/a>). Those errors will appear as &quot;Logs&quot;, which can be accessed by clicking the corresponding button:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jpgfU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jpgfU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ul>\n<li>NOTE: In the picture above, when the &quot;Logs&quot; are displayed, it might be helpful to carefully check each log (close to the time when you created you pipeline), as generally each eof them corresponds with a single warning or error line:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/CpTkZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CpTkZ.png\" alt=\"Verte AI Pipelines Logs\" \/><\/a><\/p>\n<ul>\n<li>Second, the output of my pipeline was a tuple. In my original approach, I just returned the plain tuple, but it is advised to return a <a href=\"https:\/\/docs.python.org\/3\/library\/typing.html#typing.NamedTuple\" rel=\"nofollow noreferrer\">NamedTuple<\/a> instead. In general, if you need to input \/ output one or more &quot;<em>small values<\/em>&quot; (int or str, for any reason), pick a NamedTuple to do so.<\/li>\n<li>Third, when the connection between your pipelines is <code>Input[Dataset]<\/code> or <code>Ouput[Dataset]<\/code>, adding the file extension is needed (and quite easy to forget). Take for instance the ouput of the <code>get_data<\/code> component, and notice how the data is recorded by specifically adding the file extension, i.e. <code>dataset.path + &quot;.csv&quot;<\/code>.<\/li>\n<\/ul>\n<p>Of course, this is a very tiny example, and projects can easily scale to huge projects, however as some sort of &quot;Hello Vertex AI Pipelines&quot; it will work well.<\/p>\n<p>Thank you.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1650870474732,
        "Solution_link_count":6.0,
        "Solution_readability":12.7,
        "Solution_reading_time":68.71,
        "Solution_score_count":4.0,
        "Solution_sentence_count":42.0,
        "Solution_word_count":498.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.7819525,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a compute cluster in Azure ML and  I am able to see it in designer but not in the notebook. Can someone let me know the reason for this?<\/p>",
        "Challenge_closed_time":1608293911592,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608276696563,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user created a compute cluster in Azure ML, which is visible in the designer but not in the notebook. They are seeking assistance in understanding why this is happening.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/203139\/compute-clusters-in-azure-ml-notebook",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":2.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":4.7819525,
        "Challenge_title":"Compute Clusters in Azure ML Notebook",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":36,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=c0af539e-5b19-433b-85a1-2ca81772cd40\">@Srinivasan G  <\/a> This is an expected behavior while using notebooks on Azure ML portal. Compute clusters are used to train models and run experiments using the designer or pipelines. These cannot be used with notebooks.     <br \/>\nNotebooks are integrated to run on an <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#notebookvm\">compute instance<\/a> which was previously termed as notebook VM. You can start\/stop the compute instance while using your notebook from Azure ML portal.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":7.71,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":70.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":213.2411183333,
        "Challenge_answer_count":9,
        "Challenge_body":"<p>I am loving <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a> and all it can do for me. I have a question whose answer I cannot find anywhere.<br>\nAmong the various fields in the wandb.config file are a few that wandb generates automatically. One of them is <code>Description<\/code>. I tried setting it from a Python program via my configuration file, but to no avail. So I am wondering how to set the Description field programmatically. This will allow me to \u201cdescribe\u201d several hundred simulations for easy retrieval. Thanks,<\/p>",
        "Challenge_closed_time":1660860326460,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660092658434,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is having trouble setting the \"Description\" field in their wandb.config file programmatically through their Python program and is seeking guidance on how to do so. They want to use this field to describe several hundred simulations for easy retrieval.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/description-field\/2881",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":6.9,
        "Challenge_reading_time":6.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":213.2411183333,
        "Challenge_title":"Description field",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":131.0,
        "Challenge_word_count":83,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/erlebacher\">@erlebacher<\/a> , It\u2019s pretty expensive to do pattern filtering in MySQL, especially on a large column like <code>notes<\/code> . The engineering team decided this feature will not be implemented. I will mark this resolved but please let me know if there is anything else I can answer for you.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":4.27,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":50.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1426093220648,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, India",
        "Answerer_reputation_count":1861.0,
        "Answerer_view_count":294.0,
        "Challenge_adjusted_solved_time":11.1307344444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any way to access the kedro pipeline environment name? Actually below is my problem.<\/p>\n<p>I am loading the config paths as below<\/p>\n<pre><code>conf_paths = [&quot;conf\/base&quot;, &quot;conf\/local&quot;]  \nconf_loader = ConfigLoader(conf_paths)\nparameters = conf_loader.get(&quot;parameters*&quot;, &quot;parameters*\/**&quot;)\ncatalog = conf_loader.get(&quot;catalog*&quot;)\n\n<\/code><\/pre>\n<p>But  I have few environments like  <code>&quot;conf\/server&quot; <\/code>, <code>&quot;conf\/test&quot;<\/code> etc, So if I have env name available I can add it to conf_paths as <code>&quot;conf\/&lt;env_name&gt;&quot;<\/code>  so that kedro will read the files from the respective env folder.\nBut now if the env path is not added to conf_paths, the files are not being read by kedro even if i specify the env name while I  run kedro like    <code>kedro run --env=server <\/code>\nI searched for all the docs but was not able to find any solution.<\/p>\n<p>EDIT:\nElaborating more on the problem.\nI am using the above-given parameters and catalog dicts in the nodes. I only have keys that are common for all runs in <code>conf\/base\/parameters.yml<\/code> and the environment specific keys in <code>conf\/server\/parameters.yml<\/code> but when i do <code>kedro run --env=server<\/code> I am getting <code>keyerror<\/code> which means the keys in <code>conf\/server\/parameters.yml<\/code> is not available in the parameters dict. If I add  <code>conf\/server<\/code> to config_paths kedro is running well without keyerror.<\/p>",
        "Challenge_closed_time":1639600021140,
        "Challenge_comment_count":8,
        "Challenge_created_time":1639518159733,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a problem in accessing the kedro pipeline environment name. They are trying to load config paths for different environments but are unable to do so without adding the environment path to conf_paths. Even after specifying the environment name while running kedro, the files are not being read. The user is getting a KeyError as the keys in the environment-specific parameters.yml file are not available in the parameters dict.",
        "Challenge_last_edit_time":1639559950496,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70355869",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":11.3,
        "Challenge_reading_time":19.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":22.7392797222,
        "Challenge_title":"How to access environment name in kedro pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":712.0,
        "Challenge_word_count":201,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495105930728,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>You don't need to define config paths, config loader etc unless you are trying to override something.<\/p>\n<p>If you are using kedro 0.17.x, the hooks.py will look something like this.<\/p>\n<p>Kedro will pass, base, local and the env you specified during runtime in <code>conf_paths<\/code> into <code>ConfigLoader<\/code>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class ProjectHooks:\n    @hook_impl\n    def register_config_loader(\n        self, conf_paths: Iterable[str], env: str, extra_params: Dict[str, Any]\n    ) -&gt; ConfigLoader:\n        return ConfigLoader(conf_paths)\n\n    @hook_impl\n    def register_catalog(\n        self,\n        catalog: Optional[Dict[str, Dict[str, Any]]],\n        credentials: Dict[str, Dict[str, Any]],\n        load_versions: Dict[str, str],\n        save_version: str,\n        journal: Journal,\n    ) -&gt; DataCatalog:\n        return DataCatalog.from_config(\n            catalog, credentials, load_versions, save_version, journal\n        )\n<\/code><\/pre>\n<p>In question, I can see you have defined <code>conf_paths<\/code> and <code>conf_loader<\/code> and the env path is not present. So kedro will ignore the env passed during runtime.<\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.7,
        "Solution_reading_time":13.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":121.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":171.2597222222,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nI tried to load a KedroPipelineModel from mlflow, and I got a \"cannot pickle context artifacts\" error, which is due do the \r\n\r\n## Context\r\n\r\nI cannot load a previously saved KedroPipelineModel generated by pipeline_ml_factory.\r\n\r\n## Steps to Reproduce\r\n\r\nSave A KedroPipelineModel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer)\r\n\r\n## Expected Result\r\n\r\nThe model should be loaded\r\n\r\n## Actual Result\r\n\r\nAn error is raised\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used: 0.16.5 and 0.4.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Windows 10 & CentOS were tested\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n# Potential solution\r\n\r\nThe faulty line is:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/mlflow\/kedro_pipeline_model.py#L45",
        "Challenge_closed_time":1606599848000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1605983313000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a TypeError when using the suggested VSCode configuration for debugging Kedro. The error is caused by commandline arguments being None when running the pipeline directly through run.py.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/122",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.1,
        "Challenge_reading_time":13.32,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":21.0,
        "Challenge_repo_issue_count":414.0,
        "Challenge_repo_star_count":145.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":171.2597222222,
        "Challenge_title":"A KedroPipelineModel cannot be loaded from mlflow if its catalog contains non deepcopy-able DataSets",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":137,
        "Discussion_body":"Does removing the faulty line and using directly the initial_catalog make the model loadable again ? if Yes, we have two options :\r\n\r\n* We no longer deepcopy the initial_catalog\r\n* We copy each DataSet of the catalog with his own loader (for example, we use tf.keras.models.clone_model for keras model DataSet ...)\r\n\r\nKnowing that the `KedroPipelineModel` is intented to be used in a separated process (at inference-time), we can just remove the deepcopy part (there won't be a conflict with another function using the same catalog)\r\n After some investigation, the issues comes from the MLflowAbstractModelDataSet, and particularly the `self._mlflow_model_module` attribute which is a module and not deepcopiable by nature. I suggest to store it as a string, and have a property attribute to load the module on the fly.\r\n\r\nNote that this is a problem which occurs only when the DataSet is not deepcopiable (and not the underlying value the DataSet can load(), so we can quite safely assume that it should not occur often). If it does, we should consider a more radical solution among the ones you suggest.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1555488556820,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Baillargues, France",
        "Answerer_reputation_count":56447.0,
        "Answerer_view_count":9158.0,
        "Challenge_adjusted_solved_time":4.9571033333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a <code>Vertex AI custom training job<\/code> (machine learnin training using custom container) on <code>GCP<\/code>. I would like to create a <code>Pub\/Sub<\/code> message when the job failed so I can post a message on some chat like Slack. Logfile (<code>Cloud Logging)<\/code> is looking like that:<\/p>\n<pre><code>{\ninsertId: &quot;xxxxx&quot;\nlabels: {\nml.googleapis.com\/endpoint: &quot;&quot;\nml.googleapis.com\/job_state: &quot;FAILED&quot;\n}\nlogName: &quot;projects\/xxx\/logs\/ml.googleapis.com%2F1113875647681265664&quot;\nreceiveTimestamp: &quot;2021-07-09T15:05:52.702295640Z&quot;\nresource: {\nlabels: {\njob_id: &quot;1113875647681265664&quot;\nproject_id: &quot;xxx&quot;\ntask_name: &quot;service&quot;\n}\ntype: &quot;ml_job&quot;\n}\nseverity: &quot;INFO&quot;\ntextPayload: &quot;Job failed.&quot;\ntimestamp: &quot;2021-07-09T15:05:52.187968162Z&quot;\n}\n<\/code><\/pre>\n<p>I am creating a Logs Router Sink with the following query:<\/p>\n<pre><code>resource.type=&quot;ml_job&quot; AND textPayload:&quot;Job failed&quot; AND labels.&quot;ml.googleapis.com\/job_state&quot;:&quot;FAILED&quot;\n<\/code><\/pre>\n<p>The issue I am facing is that Vertex AI will retry the job 3 times before declaring the job as a failure but in the logfile the message is identical. Below you have 3 examples, only the last one that failed 3 times really failed at the end.\n<a href=\"https:\/\/i.stack.imgur.com\/NSGPb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NSGPb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>In the logfile, I don't have any count id for example. Any idea how to solve this ? Creating a BigQuery table to keep track of the number of failure per <code>resource.labels.job_id<\/code> seems to be an overkill if I need to do that in all my project. Is there a way to do a group by <code>resource.labels.job_id<\/code> and count within Logs Router Sink ?<\/p>",
        "Challenge_closed_time":1627331282972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1627312838200,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while creating a Logs Router Sink for a Vertex AI custom training job on GCP. The job fails after three attempts, but the log message remains identical. The user wants to create a Pub\/Sub message when the job fails and post it on Slack. However, there is no count ID in the log file, and creating a BigQuery table seems like an overkill. The user is looking for a way to group by resource.labels.job_id and count within Logs Router Sink.",
        "Challenge_last_edit_time":1627313437400,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68532457",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":25.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":5.1235477778,
        "Challenge_title":"How to create a Logs Router Sink when a Vertex AI training job failed (after 3 attempts)?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":216.0,
        "Challenge_word_count":232,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1465222092252,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Z\u00fcrich, Switzerland",
        "Poster_reputation_count":1414.0,
        "Poster_view_count":478.0,
        "Solution_body":"<p>The log sink is quite simple: provide a filter, it will publish in a PubSub topic each entry which match this filter. No group by, no count, nothing!!<\/p>\n<p>I propose you to use a combination of log-based metrics and Cloud monitoring.<\/p>\n<ol>\n<li>Firstly, create a <a href=\"https:\/\/cloud.google.com\/logging\/docs\/logs-based-metrics\" rel=\"nofollow noreferrer\">log based metrics<\/a> on your job failed log entry<\/li>\n<li>Create an <a href=\"https:\/\/cloud.google.com\/logging\/docs\/logs-based-metrics\/charts-and-alerts\" rel=\"nofollow noreferrer\">alert on this log based metrics<\/a> with the following key values<\/li>\n<\/ol>\n<ul>\n<li>Set the group by that you want, for example, the jobID (i don't know what is the relevant value for VertexAI job)<\/li>\n<li>Set an alert when the threshold is equal or above 3<\/li>\n<li>Add a notification channel and set a PubSub notification (still in beta)<\/li>\n<\/ul>\n<p>With this configuration, the alert will be posted only once in PubSub when 3 occurrences of the same jobID will occur.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.7,
        "Solution_reading_time":12.96,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":142.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1458548318740,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":1568.0,
        "Answerer_view_count":266.0,
        "Challenge_adjusted_solved_time":873.3076302778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a DAG in Airflow with SageMakerOperators and I have not been able to make them work. The title is the error that appears in the airflow GUI. For solving it, I have made the following tries:<\/p>\n\n<pre><code>sudo pip3 uninstall urllib3 &amp;&amp; sudo pip3 install urllib3==1.22 \nsudo pip3 install urllib3==1.22 --upgrade\nsudo pip3 install urllib3==1.22 -t \/home\/ubuntu\/.local\/lib\/python3.7\/site-packages -upgrade\n<\/code><\/pre>\n\n<p>But I am still getting the error in the GUI. Plus, in the console of the webserver I am getting:<\/p>\n\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages\/urllib3-1.22.dist-info\/METADATA'\n<\/code><\/pre>\n\n<p>The thing is that if I make <code>pip3 show urllib3<\/code> I get the version 1.22:\n<a href=\"https:\/\/i.stack.imgur.com\/i5y8i.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/i5y8i.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>However, it says dist-packages instead of site-packages. In addition, trying to go to <code>\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages\/urllib3-1.22.dist-info\/<\/code> for trying to solve the metadata file not found error, the directory does not exists. \n<a href=\"https:\/\/i.stack.imgur.com\/44H4I.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/44H4I.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/2CnJl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2CnJl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am totally lost at this point. How could I solve this problem?<\/p>",
        "Challenge_closed_time":1571189751312,
        "Challenge_comment_count":3,
        "Challenge_created_time":1568045843843,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a DAG in Airflow with SageMakerOperators but is unable to make them work due to the error \"Broken DAG: urllib3 1.25.3 (\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages), Requirement.parse('urllib3<1.25,>=1.21'), {'sagemaker'}\". The user has tried to solve the issue by uninstalling and reinstalling urllib3, but the error persists. Additionally, the console of the webserver shows a \"metadata file not found\" error. The user is unsure how to solve the problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57857726",
        "Challenge_link_count":6,
        "Challenge_participation_count":4,
        "Challenge_readability":10.4,
        "Challenge_reading_time":22.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":873.3076302778,
        "Challenge_title":"Broken DAG: urllib3 1.25.3 (\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages), Requirement.parse('urllib3<1.25,>=1.21'), {'sagemaker'}",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":343.0,
        "Challenge_word_count":181,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1523298968403,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1754.0,
        "Poster_view_count":197.0,
        "Solution_body":"<p>Here you go.<\/p>\n\n<p>Airflow is looking in the local (user) Python installation for the library but <code>urllib3<\/code> is installed for all users. It's weird but try doing <code>pip3 install --user urllib3==1.22<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":2.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1601729162436,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":887.0,
        "Answerer_view_count":130.0,
        "Challenge_adjusted_solved_time":406.6022325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>To submit a parameter in an az ml cli <code>run submit-pipeline<\/code> command we use the syntax:<\/p>\n<pre><code>az ml run submit-pipeline \u2013datapaths [DataPATHS Name=datastore\/datapath] --experiment-name [Experiment_Name] --parameters [String_parameters Name=Value] --pipeline-id [ID]--resource-group [RGP] --subscription-id [SUB_ID] --workspace-name [AML_WS_NAME]\n<\/code><\/pre>\n<p>This will submit Datapaths and some string parameters with the pipeline. How do we submit Dataset references using az ml cli <code>run submit-pipeline<\/code> command?<\/p>\n<p>For example, the Documentation Notebook: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb\" rel=\"nofollow noreferrer\">aml-pipelines-showcasing-dataset-and-pipelineparameter<\/a><\/p>\n<p>To submit a Dataset Class reference we do:<\/p>\n<pre><code>iris_tabular_ds = Dataset.Tabular.from_delimited_files('link\/iris.csv')\npipeline_run_with_params = experiment.submit(pipeline, pipeline_parameters={'tabular_ds_param': iris_tabular_ds})\n<\/code><\/pre>\n<p>Using REST Call the syntax is:<\/p>\n<pre><code>response = requests.post(rest_endpoint, \n                         headers=aad_token, \n                         json={&quot;ExperimentName&quot;: &quot;MyRestPipeline&quot;,\n                               &quot;RunSource&quot;: &quot;SDK&quot;,\n                               &quot;DataSetDefinitionValueAssignments&quot;: { &quot;tabular_ds_param&quot;: {&quot;SavedDataSetReference&quot;: {&quot;Id&quot;: iris_tabular_ds.id}}}\n                              }\n                        )\n<\/code><\/pre>\n<p>What is the syntax to achieve this using <code>az ml cli<\/code>?<\/p>",
        "Challenge_closed_time":1617345815328,
        "Challenge_comment_count":1,
        "Challenge_created_time":1616413555030,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to submit Dataset references using the az ml cli run submit-pipeline command, as they are familiar with how to submit Datapaths and string parameters. They have provided examples of how to do this using REST Call and a Documentation Notebook, but are unsure of the syntax for az ml cli.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66745404",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":20.6,
        "Challenge_reading_time":22.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":258.9611938889,
        "Challenge_title":"How to submit Dataset Input as a Parameter to AZ ML CLI run submit-pipeline command?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":268.0,
        "Challenge_word_count":128,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>To consume this from the AZ ML CLI we use the following syntax:<\/p>\n<pre><code>    curl -X POST [Pipeline_REST_Endpoint] -H &quot;Authorization: Bearer $(az account get-access-token --query accessToken -o tsv)&quot; -H &quot;Content-Type: application\/json&quot; --data-binary @- &lt;&lt;DATA\n{&quot;ExperimentName&quot;: &quot;[ExperimentName]&quot;,\n                               &quot;RunSource&quot;: &quot;SDK&quot;,\n                               &quot;DataSetDefinitionValueAssignments&quot;: {&quot;tabular_ds_param&quot;: \n                                                                     {&quot;SavedDataSetReference&quot;: \n                                                                      {&quot;Id&quot;:&quot;[Dataset_ID]&quot;}\n                                                                     }\n                                                                    }\n                              }\nDATA\n<\/code><\/pre>\n<p>We use the simple REST call because <code>az ml run submit-pipeline<\/code> does not have the dataset parameter and datapath does not achieve the desired result.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1617877323067,
        "Solution_link_count":0.0,
        "Solution_readability":36.9,
        "Solution_reading_time":9.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":68.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.1316666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\nSageMaker supports training data streaming via [PIPE mode][1], and also reading from [FSx][2] distributed file system.\nThose options seem to provide same value: low latency, high throughput.\n\n - What are the reasons for using one or the other?\n - Do we have any benchmark of PIPE vs FSx for SageMaker, in terms of costs and speed?\n\n\n  [1]: https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\n  [2]: https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/08\/amazon-sagemaker-works-with-amazon-fsx-lustre-amazon-efs-model-training\/",
        "Challenge_closed_time":1579732148000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579692074000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking to understand the differences between SageMaker's PIPE mode and FSx distributed file system for training data streaming. They are looking for benchmarks comparing the two options in terms of costs and speed.",
        "Challenge_last_edit_time":1668074106092,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUyS6bjxG4R4qtnrXzA3uSeg\/sagemaker-pipe-mode-vs-fsx",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":7.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":11.1316666667,
        "Challenge_title":"SageMaker PIPE Mode vs FSx ?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":261.0,
        "Challenge_word_count":64,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I can think of the following scenarios \n\nPipemode cons\n\n** UPDATED**\n\n1.  Data Shuffling -  In pipe mode you are working with streaming data and hence you cannot perform data shuffle operations unless you are prepared to shuffle within batches (as in wait to read a batch of records and shuffle within the batch in Pipe mode). Of if your data is distributed across multiples files, then you could use [Sagemaker data shuffle][1] to perform file level shuffle\n\n2.  Data readers -   There are default data readers for pipemode that come with Tensorflow for formats like csv, tfrecord etc. But if you have custom data formats or using a  different deep leaning  framework, yYou would have to use custom data readers to deal with the raw bytes and understand the logical end of record. You could also use [ml-io][2] to see if any of the built-in pipe mode readers work for your usecase\n\n3. PIPE mode streams the data for each epoch from S3 and hence will be slower than FSX when you run a few epochs\n\n\nFSX:\n\n1. FSX works by lazy loading the  s3 file and hence it has a start up delay but gets faster during repeated training. \n\n2. There is no  dependency on the framework and your existing code will work as is..\n\n3. The only con of using FSX is the additional storage costs, but I would almost prefer FSX to pipe mode in most cases.\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ShuffleConfig.html\n  [2]: https:\/\/github.com\/awslabs\/ml-io#Python",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925565015,
        "Solution_link_count":2.0,
        "Solution_readability":9.5,
        "Solution_reading_time":17.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":242.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":2.1417511111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have defined an Azure Machine Learning Pipeline with three steps:<\/p>\n<pre><code>e2e_steps=[etl_model_step, train_model_step, evaluate_model_step]\ne2e_pipeline = Pipeline(workspace=ws, steps = e2e_steps)\n<\/code><\/pre>\n<p>The idea is to run the Pipeline in the given sequence:<\/p>\n<ol>\n<li>etl_model_step<\/li>\n<li>train_model_step<\/li>\n<li>evaluate_model_step<\/li>\n<\/ol>\n<p>However, my experiment is failing because it is trying to execute evaluate_model_step before train_model_step:\n<a href=\"https:\/\/i.stack.imgur.com\/0fO0t.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0fO0t.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>How do I enforce the sequence of execution?<\/p>",
        "Challenge_closed_time":1624544151467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624536441163,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in organizing the sequence of execution for their Azure Machine Learning Pipeline with three steps, as the experiment is failing due to the pipeline trying to execute a step before its prerequisite. The user is seeking guidance on how to enforce the sequence of execution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68115476",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.6,
        "Challenge_reading_time":10.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2.1417511111,
        "Challenge_title":"How to organize one step after another in Azure Machine Learning Pipelines?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":515.0,
        "Challenge_word_count":78,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p><code>azureml.pipeline.core.StepSequence<\/code> lets you do exactly that.<\/p>\n<blockquote>\n<p>A StepSequence can be used to easily run steps in a specific order, without needing to specify data dependencies through the use of PipelineData.<\/p>\n<\/blockquote>\n<p>See <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.stepsequence?view=azure-ml-py\" rel=\"nofollow noreferrer\">the docs<\/a> to read more.<\/p>\n<p>However, the preferable way to have steps run in order is stitching them together via <code>PipelineData<\/code> or <code>OutputFileDatasetConfig<\/code>. In your example, does the <code>train_step<\/code> depend on outputs from the <code>etl step<\/code>? If so, consider having that be the way that steps are run in sequence. For more info see <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-data-in-out-of-pipelines\" rel=\"nofollow noreferrer\">this tutorial<\/a> for more info<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":12.71,
        "Solution_score_count":3.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":98.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.7680130556,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi everyone!<\/p>\n<p>I am trying to retrieve  filtered runs from a project using the WandB API and a filter dictionary.<\/p>\n<p>I try to do the following:<\/p>\n<pre><code class=\"lang-auto\">api = wandb.Api()\nfilter_dict = {\"job_type\":  \"my_job_type\"}\nruns = api.runs(\"my_entity\/my_project\", filters=filter_dict)\nfor run in runs:\n    print(run)\n<\/code><\/pre>\n<p>When I do this, I get the following error message:<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\apis\\public.py\", line 980, in __next__\n    if not self._load_page():\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\apis\\public.py\", line 965, in _load_page\n    self.last_response = self.client.execute(\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\retry.py\", line 168, in wrapped_fn\n    return retrier(*args, **kargs)\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\retry.py\", line 108, in __call__\n    result = self._call_fn(*args, **kwargs)\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\apis\\public.py\", line 207, in execute\n    return self._client.execute(*args, **kwargs)\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\wandb_gql\\client.py\", line 52, in execute\n    result = self._get_result(document, *args, **kwargs)\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\wandb_gql\\client.py\", line 60, in _get_result\n    return self.transport.execute(document, *args, **kwargs)\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\wandb_gql\\transport\\requests.py\", line 39, in execute\n    request.raise_for_status()\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\requests\\models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https:\/\/api.wandb.ai\/graphql\n<\/code><\/pre>\n<p>However, other filters do work. For example I can do the above described procedure with<\/p>\n<pre><code class=\"lang-auto\">filter_dict = {\"group\":  \"my_group\"}\n<\/code><\/pre>\n<p>and it yields the correctly filtered jobs.<\/p>\n<p>What I am currently doing as a workaround is this:<\/p>\n<pre><code class=\"lang-auto\">api = wandb.Api()\nruns = api.runs(\"my_entity\/my_project\")\nfor run in runs:\n    if run.job_type == \"my_job_type\":\n        print(run)\n<\/code><\/pre>\n<p>However, I would prefer to directly filter the runs with the API call. Any idea what I am doing wrong?<\/p>\n<p>Thanks in advance!<\/p>",
        "Challenge_closed_time":1667924724247,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667921959400,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to filter runs by job_type using the WandB API and a filter dictionary. The API call is resulting in a Bad Request error message, while other filters are working correctly. The user has implemented a workaround by filtering the runs after retrieving them, but is seeking a solution to directly filter the runs with the API call.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/filtering-runs-by-job-type-using-api-is-not-working\/3390",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":13.4,
        "Challenge_reading_time":34.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":0.7680130556,
        "Challenge_title":"Filtering runs by job_type using API is not working",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":183.0,
        "Challenge_word_count":235,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kolja\">@kolja<\/a> thank you for writing in! Could you please try if the following would work for you?<\/p>\n<pre><code class=\"lang-auto\">api = wandb.Api()\nfilter_dict = {\"jobType\":  \"my_job_type\"}\nruns = api.runs(\"my_entity\/my_project\", filters=filter_dict)\nfor run in runs:\n    print(run)\n<\/code><\/pre>\n<p>The queries in <code>filters<\/code> are using the MongoDB query language. Hope this helps!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":5.55,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":48.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1655446100500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":161.0086202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Previously, using Kubeflow Pipelines SDK v1, the status of a pipeline could be inferred during pipeline execution by passing an Argo placeholder, <code>{{workflow.status}}<\/code>, to the component, as shown below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import kfp.dsl as dsl\n\ncomponent_1 = dsl.ContainerOp(\n    name='An example component',\n    image='eu.gcr.io\/...\/my-component-img',\n    arguments=[\n               'python3', 'main.py',\n               '--status', &quot;{{workflow.status}}&quot;\n              ]\n)\n<\/code><\/pre>\n<p>This placeholder would take the value <code>Succeeded<\/code> or <code>Failed<\/code> when passed to the component. One use-case for this would be to send a failure-warning to eg. Slack, in combination with <code>dsl.ExitHandler<\/code>.<\/p>\n<p>However, when using Pipeline SDK version 2, <code>kfp.v2<\/code>, together with Vertex AI to compile and run the pipeline the Argo placeholders no longer work, as described by <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/7614\" rel=\"nofollow noreferrer\">this open issue<\/a>. Because of this, I would need another way to check the status of the pipeline within the component. I was thinking I could use the <code>kfp.Client<\/code> <a href=\"https:\/\/kubeflow-pipelines.readthedocs.io\/en\/latest\/source\/kfp.client.html\" rel=\"nofollow noreferrer\">class<\/a>, but I'm assuming this won't work using Vertex AI, since there is no &quot;host&quot; really. Also, there seems to be supported placeholders for to pass the run id (<code>dsl.PIPELINE_JOB_ID_PLACEHOLDER<\/code>) as a placeholder, as per <a href=\"https:\/\/stackoverflow.com\/questions\/68348026\/run-id-in-kubeflow-pipelines-on-vertex-ai\">this SO post<\/a>, but I can't find anything around <code>status<\/code>.<\/p>\n<p>Any ideas how to get the status of a pipeline run within a component, running on Vertex AI?<\/p>",
        "Challenge_closed_time":1655447541936,
        "Challenge_comment_count":1,
        "Challenge_created_time":1651551486133,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in getting the status of a pipeline run within a component while using Pipeline SDK version 2 with Vertex AI. The Argo placeholders that were used previously to infer the status of a pipeline during execution are no longer working. The user is looking for alternative ways to check the status of the pipeline within the component, but is unsure if using the kfp.Client class would work with Vertex AI. The user is seeking suggestions on how to get the status of a pipeline run within a component on Vertex AI.",
        "Challenge_last_edit_time":1654867910903,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72094768",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":10.9,
        "Challenge_reading_time":24.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":1082.2377230556,
        "Challenge_title":"How to get the status of a pipeline run within a component, running on Vertex AI?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":520.0,
        "Challenge_word_count":219,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562750927332,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Stockholm, Sverige",
        "Poster_reputation_count":803.0,
        "Poster_view_count":73.0,
        "Solution_body":"<p>Each pipeline run is automatically logged to Google Logging, and so are also the failed pipeline runs.\nThe error logs also contain information about the pipeline and the component that failed.<\/p>\n<p>We can use this information to monitor our logs and set up an alert via email for example.<\/p>\n<p>The logs for our Vertex AI Pipeline runs we get with the following filter<\/p>\n<p>resource.type=\u201daiplatform.googleapis.com\/PipelineJob\u201d\nseverity=(ERROR OR CRITICAL OR ALERT OR EMERGENCY)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/e4jFR.png\" rel=\"nofollow noreferrer\">Vertex AI Pipeline Logs<\/a><\/p>\n<p>Based on those logs you can set up log-based alerts <a href=\"https:\/\/cloud.google.com\/logging\/docs\/alerting\/log-based-alerts\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/logging\/docs\/alerting\/log-based-alerts<\/a>. Notifications via email, Slack, SMS, and many more are possible.<\/p>\n<p>source:\n<a href=\"https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":13.4,
        "Solution_reading_time":15.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":107.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3852777778,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to submit a training job on sagemaker. I tried it on notebook and it works. When I try the following I get `ModuleNotFoundError: No module named 'nltk'`\n\nMy code is\n\n    import sagemaker  \n    from sagemaker.pytorch import PyTorch\n\n    JOB_PREFIX   = 'pyt-ic'\n    FRAMEWORK_VERSION = '1.3.1'\n\n    estimator = PyTorch(entry_point='finetune-T5.py',\n                       source_dir='..\/src',\n                       train_instance_type='ml.p2.xlarge' ,\n                       train_instance_count=1,\n                       role=sagemaker.get_execution_role(),\n                       framework_version=FRAMEWORK_VERSION, \n                       debugger_hook_config=False,  \n                       py_version='py3',\n                       base_job_name=JOB_PREFIX)\n\n    estimator.fit()\n\n\n\n\n`finetune-T5.py` have many other libraries that are not installed. How can I install the missing library? Or is there a better way to run the training job?",
        "Challenge_closed_time":1598914035000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598912648000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a `ModuleNotFoundError` when trying to submit a training job on Sagemaker. The error is caused by a missing library (`nltk`) in the `finetune-T5.py` file. The user is seeking advice on how to install the missing library or if there is a better way to run the training job.",
        "Challenge_last_edit_time":1668620734992,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUJMd_4s52RpWXDXITXFsQdw\/modulenotfounderror-when-starting-a-training-job-on-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":10.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.3852777778,
        "Challenge_title":"ModuleNotFoundError when starting a training job on Sagemaker",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":833.0,
        "Challenge_word_count":87,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Check out this [link][1] (Using third-party libraries section) on how to install third-party libraries for training jobs.  You need to create requirement.txt file in the same directory as your training script to install other dependencies at runtime.\n\n\n  [1]: https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#using-third-party-libraries",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925593038,
        "Solution_link_count":1.0,
        "Solution_readability":14.3,
        "Solution_reading_time":4.77,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.6992033333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>&quot;We want  the model  to automatically register model every time there is a new model. we created the model in the process and write it out to a pipeline data set.To persist it then we upload and read it for registration.    <\/p>\n<p>We are using .\/output to send the file to output. The issue is that it cannot find it in the file path . How can we validate its existence?  &quot;  <\/p>\n<p>[Note: As we migrate from MSDN, this question has been posted by an\u202fAzure Cloud Engineer\u202fas a frequently asked question] Source: <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/1369621f-ebc2-4e79-abe9-9f1165aea6c6\/model-file-is-not-found-for-registration-of-model-in-training-pipeline?forum=MachineLearning\">MSDN<\/a>  <\/p>",
        "Challenge_closed_time":1589360659692,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589329342560,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with finding the model file for registration in the training pipeline. They have created the model and written it out to a pipeline data set, but it cannot be found in the file path. The user is seeking help to validate its existence.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/26470\/model-file-is-not-found-for-registration-of-model",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":8.6992033333,
        "Challenge_title":"Model file is not found for Registration of model in training Pipeline.",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":108,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Can you verify that the script that is actually writing the model file to the location you expect:<\/p>\n<pre><code>with open(model_name, 'wb') as file:\n       joblib.dump(value = model, filename = os.path.join('.\/outputs\/', model_name))\n<\/code><\/pre>\n<p>Inside in your train python script, you just need to do something like this:<\/p>\n<h1 id=\"persist-the-model-to-the-local-machine\">persist the model to the local machine<\/h1>\n<pre><code>tf.saved_model.save(model,'.\/outputs\/model\/')\n<\/code><\/pre>\n<h1 id=\"register-the-model-with-run-object\">register the model with run object<\/h1>\n<pre><code>run.register_model(model_name,'.\/outputs\/model\/')\n<\/code><\/pre>\n<p>Source: <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/1369621f-ebc2-4e79-abe9-9f1165aea6c6\/model-file-is-not-found-for-registration-of-model-in-training-pipeline?forum=MachineLearning\">MSDN<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":21.3,
        "Solution_reading_time":11.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":65.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1457555855467,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":86.0,
        "Answerer_view_count":23.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a big pipeline, taking a few hours to run. A small part of it needs to run quite often, how do I run it without triggering the entire pipeline?<\/p>",
        "Challenge_closed_time":1574848206347,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574848206347,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has a large Kedro pipeline that takes several hours to run, but only a small part of it needs to be run frequently. They are looking for a way to run that specific part without triggering the entire pipeline.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59067349",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.8,
        "Challenge_reading_time":2.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0,
        "Challenge_title":"How to run parts of your Kedro pipeline conditionally?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":4724.0,
        "Challenge_word_count":39,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457555855467,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":86.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>There are multiple ways to specify which nodes or parts of your pipeline to run. <\/p>\n\n<ol>\n<li><p>Use <code>kedro run<\/code> parameters like <code>--to-nodes<\/code>\/<code>--from-nodes<\/code>\/<code>--node<\/code> to explicitly define what needs to be run.<\/p><\/li>\n<li><p>In <code>kedro&gt;=0.15.2<\/code> you can define multiple pipelines, and then run only one of them with <code>kedro run --pipeline &lt;name&gt;<\/code>. If no <code>--pipeline<\/code> parameter is specified, the default pipeline is run. The default pipeline might combine several other pipelines. More information about using modular pipelines: <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/06_pipelines.html#modular-pipelines\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/06_pipelines.html#modular-pipelines<\/a><\/p><\/li>\n<li><p>Use tags. Tag a small portion of your pipeline with something like \"small\", and then do <code>kedro run --tag small<\/code>. Read more here: <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/05_nodes.html#tagging-nodes\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/05_nodes.html#tagging-nodes<\/a><\/p><\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.7,
        "Solution_reading_time":16.14,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":107.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":1807.7083702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to add an alert if Azure ML pipeline fails. It looks that one of the ways is to create a monitor in the Azure Portal. The problem is that I cannot find a correct signal name (required when setting up condition), which would identify pipeline fail. What signal name should I use? Or is there another way to send an email if Azure pipeline fails?<\/p>",
        "Challenge_closed_time":1651723432756,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651478303433,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to set up an alert for when an Azure ML pipeline fails, but is having trouble finding the correct signal name to identify the failure. They are seeking advice on what signal name to use or if there is another way to receive an email notification for pipeline failures.",
        "Challenge_last_edit_time":1651956277630,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72083832",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.5,
        "Challenge_reading_time":4.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":68.0914786111,
        "Challenge_title":"Send alert if Azure ML pipeline fails",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":158.0,
        "Challenge_word_count":74,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1313536247312,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Vilnius, Lithuania",
        "Poster_reputation_count":563.0,
        "Poster_view_count":108.0,
        "Solution_body":"<blockquote>\n<p>What signal name should I use?<\/p>\n<\/blockquote>\n<p>You can use <code>PipelineChangeEvent<\/code> category of <code>AmlPipelineEvent<\/code> table to view events when ML pipeline draft or endpoint or module are accessed (read, created, or deleted).<\/p>\n<p>For example, according to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/monitor-azure-machine-learning#analyzing-logs\" rel=\"nofollow noreferrer\">documentation<\/a>, use <code>AmlComputeJobEvent<\/code> to get failed jobs in the last five days:<\/p>\n<pre><code>AmlComputeJobEvent\n| where TimeGenerated &gt; ago(5d) and EventType == &quot;JobFailed&quot;\n| project  TimeGenerated , ClusterId , EventType , ExecutionState , ToolType\n<\/code><\/pre>\n<p><strong>Updated answer:<\/strong><\/p>\n<p>According to <a href=\"https:\/\/stackoverflow.com\/users\/897665\/laurynas-g\">Laurynas G<\/a>:<\/p>\n<pre><code>AmlRunStatusChangedEvent \n| where Status == &quot;Failed&quot; or Status == &quot;Canceled&quot;\n<\/code><\/pre>\n<p>You can refer to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/monitor-azure-machine-learning#analyzing-logs\" rel=\"nofollow noreferrer\">Monitor Azure Machine Learning<\/a>, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Log &amp; view metrics and log files<\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-pipelines\" rel=\"nofollow noreferrer\">Troubleshooting machine learning pipelines<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1658464027763,
        "Solution_link_count":5.0,
        "Solution_readability":19.3,
        "Solution_reading_time":20.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":111.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":6.3718908334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to <code>azure-ml<\/code>, and have been tasked to make some integration tests for a couple of pipeline steps. I have prepared some input test data and some expected output data, which I store on a <code>'test_datastore'<\/code>. The following example code is a simplified version of what I want to do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ws = Workspace.from_config('blabla\/config.json')\nds = Datastore.get(ws, datastore_name='test_datastore')\n\nmain_ref = DataReference(datastore=ds,\n                            data_reference_name='main_ref'\n                            )\n\ndata_ref = DataReference(datastore=ds,\n                            data_reference_name='main_ref',\n                            path_on_datastore='\/data'\n                            )\n\n\ndata_prep_step = PythonScriptStep(\n            name='data_prep',\n            script_name='pipeline_steps\/data_prep.py',\n            source_directory='\/.',\n            arguments=['--main_path', main_ref,\n                        '--data_ref_folder', data_ref\n                        ],\n            inputs=[main_ref, data_ref],\n            outputs=[data_ref],\n            runconfig=arbitrary_run_config,\n            allow_reuse=False\n            )\n<\/code><\/pre>\n<p>I would like:<\/p>\n<ul>\n<li>my <code>data_prep_step<\/code> to run,<\/li>\n<li>have it store some data on the path to my <code>data_ref<\/code>), and<\/li>\n<li>I would then like to access this stored data afterwards outside of the pipeline<\/li>\n<\/ul>\n<p>But, I can't find a useful function in the documentation. Any guidance would be much appreciated.<\/p>",
        "Challenge_closed_time":1616626076987,
        "Challenge_comment_count":7,
        "Challenge_created_time":1616603138180,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to access the output folder from a PythonScriptStep in Azure Machine Learning. They have prepared input test data and expected output data stored on a 'test_datastore'. They want the data_prep_step to run, store data on the path to data_ref, and then access this stored data outside of the pipeline. However, they are unable to find a useful function in the documentation.",
        "Challenge_last_edit_time":1616961515960,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66785273",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":13.6,
        "Challenge_reading_time":17.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":6.3718908334,
        "Challenge_title":"How to acces output folder from a PythonScriptStep?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1327.0,
        "Challenge_word_count":138,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459511191443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":445.0,
        "Poster_view_count":96.0,
        "Solution_body":"<p>two big ideas here -- let's start with the main one.<\/p>\n<h2>main ask<\/h2>\n<blockquote>\n<p>With an Azure ML Pipeline, how can I access the output data of a <code>PythonScriptStep<\/code> outside of the context of the pipeline?<\/p>\n<\/blockquote>\n<h3>short answer<\/h3>\n<p>Consider using <code>OutputFileDatasetConfig<\/code> (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.output_dataset_config.outputdatasetconfig?view=azure-ml-py&amp;viewFallbackFrom=experimental&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">docs<\/a> <a href=\"http:\/\/%20https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-data-in-out-of-pipelines?WT.mc_id=AI-MVP-5003930#use-outputfiledatasetconfig-for-intermediate-data\" rel=\"nofollow noreferrer\">example<\/a>), instead of <code>DataReference<\/code>.<\/p>\n<p>To your example above, I would just change your last two definitions.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>data_ref = OutputFileDatasetConfig(\n    name='data_ref',\n    destination=(ds, '\/data')\n).as_upload()\n\n\ndata_prep_step = PythonScriptStep(\n    name='data_prep',\n    script_name='pipeline_steps\/data_prep.py',\n    source_directory='\/.',\n    arguments=[\n        '--main_path', main_ref,\n        '--data_ref_folder', data_ref\n                ],\n    inputs=[main_ref, data_ref],\n    outputs=[data_ref],\n    runconfig=arbitrary_run_config,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p>some notes:<\/p>\n<ul>\n<li>be sure to check out how <code>DataPath<\/code>s work. Can be tricky at first glance.<\/li>\n<li>set <code>overwrite=False<\/code> in the `.as_upload() method if you don't want future runs to overwrite the first run's data.<\/li>\n<\/ul>\n<h3>more context<\/h3>\n<p><code>PipelineData<\/code> used to be the defacto object to pass data ephemerally between pipeline steps. The idea was to make it easy to:<\/p>\n<ol>\n<li>stitch steps together<\/li>\n<li>get the data after the pipeline runs if need be (<code>datastore\/azureml\/{run_id}\/data_ref<\/code>)<\/li>\n<\/ol>\n<p>The downside was that you have no control over <em>where<\/em> the pipeline is saved. If you wanted to data for more than just as a baton that gets passed between steps, you could have a <code>DataTransferStep<\/code> to land the <code>PipelineData<\/code> wherever you please after the <code>PythonScriptStep<\/code> finishes.<\/p>\n<p>This downside is what motivated <code>OutputFileDatasetConfig<\/code><\/p>\n<h2>auxilary ask<\/h2>\n<blockquote>\n<p>how might I programmatically test the functionality of my Azure ML pipeline?<\/p>\n<\/blockquote>\n<p>there are not enough people talking about data pipeline testing, IMHO.<\/p>\n<p>There are three areas of data pipeline testing:<\/p>\n<ol>\n<li>unit testing (the code in the step works?<\/li>\n<li>integration testing (the code works when submitted to the Azure ML service)<\/li>\n<li>data expectation testing (the data coming out of the meets my expectations)<\/li>\n<\/ol>\n<p>For #1, I think it should be done outside of the pipeline perhaps as part of a package of helper functions\nFor #2, Why not just see if the whole pipeline completes, I think get more information that way. That's how we run our CI.<\/p>\n<p>#3 is the juiciest, and we do this in our pipelines with the <a href=\"https:\/\/greatexpectations.io\/\" rel=\"nofollow noreferrer\">Great Expectations (GE)<\/a> Python library. The GE community calls these &quot;expectation tests&quot;. To me you have two options for including expectation tests in your Azure ML pipeline:<\/p>\n<ol>\n<li>within the <code>PythonScriptStep<\/code> itself, i.e.\n<ol>\n<li>run whatever code you have<\/li>\n<li>test the outputs with GE before writing them out; or,<\/li>\n<\/ol>\n<\/li>\n<li>for each functional <code>PythonScriptStep<\/code>, hang a downstream <code>PythonScriptStep<\/code> off of it in which you run your expectations against the output data.<\/li>\n<\/ol>\n<p>Our team does #1, but either strategy should work. What's great about this approach is that you can run your expectation tests by just running your pipeline (which also makes integration testing easy).<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1616626562700,
        "Solution_link_count":3.0,
        "Solution_readability":12.2,
        "Solution_reading_time":51.55,
        "Solution_score_count":3.0,
        "Solution_sentence_count":27.0,
        "Solution_word_count":453.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1630695701727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":101.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":2874.6210316667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a requirement to use azure machine learning to develop a pipeline. In this pipeline we don't pass data as inputs\/outputs but variables (for example a list or an int). I have looked on the Microsoft documentation but could not seem to find something fitting my case. Also tried to use the PipelineData class but could not retrieve my variables.<\/p>\n<ol>\n<li>Is this possible?<\/li>\n<li>Is this a good approach?<\/li>\n<\/ol>\n<p>Thanks for your help.<\/p>",
        "Challenge_closed_time":1658826630380,
        "Challenge_comment_count":1,
        "Challenge_created_time":1648478111533,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to develop a pipeline using Azure Machine Learning and wants to pass variables (such as a list or an int) from one step to another instead of passing data as inputs\/outputs. They have looked at Microsoft documentation and tried using the PipelineData class but have not been successful in retrieving their variables. The user is seeking help to determine if this is possible and if it is a good approach.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71649163",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":2874.5885686111,
        "Challenge_title":"Can azureml pass variables from one step to another?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":399.0,
        "Challenge_word_count":83,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1648477773363,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I know I'm a bit late to the party but here we go:<\/p>\n<p><strong>Passing variables between AzureML Pipeline Steps<\/strong><\/p>\n<p>To directly answer your question, to my knowledge it is not possible to pass variables directly between PythonScriptSteps in an AzureML Pipeline.<\/p>\n<p>The reason for that is that the steps are executed in isolation, i.e. the code is run in different processes or even computes. The only interface a PythonScriptStep has is (a) command line arguments that need to be set prior to submission of the pipeline and (b) data.<\/p>\n<p><strong>Using datasets to pass information between PythonScriptSteps<\/strong><\/p>\n<p>As a workaround you can use PipelineData to pass data between steps.\nThe previously posted blog post may help: <a href=\"https:\/\/vladiliescu.net\/3-ways-to-pass-data-between-azure-ml-pipeline-steps\/\" rel=\"nofollow noreferrer\">https:\/\/vladiliescu.net\/3-ways-to-pass-data-between-azure-ml-pipeline-steps\/<\/a><\/p>\n<p>As for your concrete problem:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># pipeline.py\n\n# This will make Azure create a unique directory on the datastore everytime the pipeline is run.\nvariables_data = PipelineData(&quot;variables_data&quot;, datastore=datastore)\n\n# `variables_data` will be mounted on the target compute and a path is given as a command line argument\nwrite_variable = PythonScriptStep(\n    script_name=&quot;write_variable.py&quot;,\n    arguments=[\n        &quot;--data_path&quot;,\n        variables_data\n    ],\n    outputs=[variables_data],\n)\n\nread_variable = PythonScriptStep(\n    script_name=&quot;read_variable.py&quot;,\n    arguments=[\n        &quot;--data_path&quot;,\n        variables_data\n    ],\n    inputs=[variables_data],\n)\n\n<\/code><\/pre>\n<p>In your script you'll want to serialize the variable \/ object that you're trying to pass between steps:<\/p>\n<p>(You could of course use JSON or any other serialization method)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># write_variable.py\n\nimport argparse\nimport pickle\nfrom pathlib import Path\n\nparser = argparse.ArgumentParser()\nparser.add_argument(&quot;--data_path&quot;)\nargs = parser.parse_args()\n\nobj = [1, 2, 3, 4]\n\nPath(args.data_path).mkdir(parents=True, exist_ok=True)\nwith open(args.data_path + &quot;\/obj.pkl&quot;, &quot;wb&quot;) as f:\n    pickle.dump(obj, f)\n<\/code><\/pre>\n<p>Finally, you can read the variable in the next step:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># read_variable.py\n\nimport argparse\nimport pickle\n\nparser = argparse.ArgumentParser()\nparser.add_argument(&quot;--data_path&quot;)\nargs = parser.parse_args()\n\n\nwith open(args.data_path + &quot;\/obj.pkl&quot;, &quot;rb&quot;) as f:\n    obj = pickle.load(f)\n\nprint(obj)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1658826747247,
        "Solution_link_count":2.0,
        "Solution_readability":11.5,
        "Solution_reading_time":34.54,
        "Solution_score_count":1.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":274.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":14.2520105556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have just removed a DVC tracking file by mistake using the command <code>dvc remove training_data.dvc -p<\/code>, which led to all my training dataset gone completely. I know in Git, we can easily revert a deleted branch based on its hash. Does anyone know how to revert all my lost data in DVC?<\/p>",
        "Challenge_closed_time":1592457436920,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592445622650,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user accidentally removed a DVC tracking file using the command \"dvc remove\" which resulted in the loss of all their training dataset. They are seeking help to revert the lost data in DVC.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62441146",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":4.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3.2817416667,
        "Challenge_title":"Revert a dvc remove -p command",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":687.0,
        "Challenge_word_count":58,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1467943515392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Poster_reputation_count":173.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>You should be safe (at least data is not gone) most likely. From the <code>dvc remove<\/code> <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remove\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Note that it does not remove files from the DVC cache or remote storage (see dvc gc). However, remember to run <code>dvc push<\/code> to save the files you actually want to use or share in the future.<\/p>\n<\/blockquote>\n\n<p>So, if you created <code>training_data.dvc<\/code> as with <code>dvc add<\/code> and\/or <code>dvc run<\/code> and <code>dvc remove -p<\/code> didn't ask\/warn you about anything, means that data is cached similar to Git in the <code>.dvc\/cache<\/code>. <\/p>\n\n<p>There are ways to retrieve it, but I would need to know a little bit more details - how exactly did you add your dataset? Did you commit <code>training_data.dvc<\/code> or it's completely gone? Was it the only data you have added so far? (happy to help you in comments).<\/p>\n\n<h2>Recovering a directory<\/h2>\n\n<p>First of all, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files-and-directories#structure-of-cache-directory\" rel=\"nofollow noreferrer\">here<\/a> is the document that describes briefly how DVC stores directories in the cache.<\/p>\n\n<p>What we can do is to find all <code>.dir<\/code> files in the <code>.dvc\/cache<\/code>:<\/p>\n\n<p><code>find .dvc\/cache -type f -name \"*.dir\"<\/code><\/p>\n\n<p>outputs something like:<\/p>\n\n<pre><code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir\n.dvc\/cache\/00\/db872eebe1c914dd13617616bb8586.dir\n.dvc\/cache\/2d\/1764cb0fc973f68f31f5ff90ee0883.dir\n<\/code><\/pre>\n\n<p>(if the local cache is lost and we are restoring data from the remote storage, the same logic applies, commands (e.g. to find files on S3 with .dir extension) look different)<\/p>\n\n<p>Each <code>.dir<\/code> file is a JSON with a content of one version of a directory (file names, hashes, etc). It has all the information needed to restore it. The next thing we need to do is to understand which one do we need. There is no one single rule for that, what I would recommend to check (and pick depending on your use case):<\/p>\n\n<ul>\n<li>Check the date modified (if you remember when this data was added).<\/li>\n<li>Check the content of those files - if you remember a specific file name that was present only in the directory you are looking for - just grep it.<\/li>\n<li>Try to restore them one by one and check the directory content.<\/li>\n<\/ul>\n\n<p>Okay, now let's imagine we decided that we want to restore <code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir<\/code>, (e.g. because content of it looks like:<\/p>\n\n<pre><code>[\n{\"md5\": \"6f597d341ceb7d8fbbe88859a892ef81\", \"relpath\": \"test.tsv\"}, {\"md5\": \"32b715ef0d71ff4c9e61f55b09c15e75\", \"relpath\": \"train.tsv\"}\n]\n<\/code><\/pre>\n\n<p>and we want to get a directory with <code>train.tsv<\/code>).<\/p>\n\n<p>The only thing we need to do is to create a <code>.dvc<\/code> file that references this directory:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n- md5: 20b786b6e6f80e2b3fcf17827ad18597.dir\n  path: my-directory\n<\/code><\/pre>\n\n<p>(note, that path \/20\/b786b6e6f80e2b3fcf17827ad18597.dir became a hash value: 20b786b6e6f80e2b3fcf17827ad18597.dir)<\/p>\n\n<p>And run <code>dvc pull<\/code> on this file.<\/p>\n\n<p>That should be it.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1592496929888,
        "Solution_link_count":2.0,
        "Solution_readability":7.9,
        "Solution_reading_time":41.6,
        "Solution_score_count":3.0,
        "Solution_sentence_count":36.0,
        "Solution_word_count":420.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1383611307000,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York",
        "Answerer_reputation_count":10846.0,
        "Answerer_view_count":984.0,
        "Challenge_adjusted_solved_time":10.0108433334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I <code>dvc add<\/code>-ed a file I did not mean to add. I have not yet committed.<\/p>\n\n<p>How do I undo this operation? In Git, you would do <code>git rm --cached &lt;filename&gt;<\/code>.<\/p>\n\n<p>To be clear: I want to make DVC forget about the file, and I want the file to remain untouched in my working tree. This is the opposite of what <code>dvc remove<\/code> does.<\/p>\n\n<p>One <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1524\" rel=\"nofollow noreferrer\">issue<\/a> on the DVC issue tracker suggests that <code>dvc unprotect<\/code> is the right command. But reading the <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/unprotect\" rel=\"nofollow noreferrer\">manual page<\/a> suggests otherwise.<\/p>\n\n<p>Is this possible with DVC?<\/p>",
        "Challenge_closed_time":1568693889196,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568689927047,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user accidentally added a file using the 'dvc add' command and wants to undo the operation without affecting the file in the working tree. They are seeking guidance on how to make DVC forget about the file, and are unsure if 'dvc unprotect' is the right command to use.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57966851",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":9.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1.1005969444,
        "Challenge_title":"Undo 'dvc add' operation",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1304.0,
        "Challenge_word_count":100,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1383611307000,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York",
        "Poster_reputation_count":10846.0,
        "Poster_view_count":984.0,
        "Solution_body":"<p>As per mroutis on the DVC Discord server:<\/p>\n\n<ol>\n<li><code>dvc unprotect<\/code> the file; this won't be necessary if you don't use <code>symlink<\/code> or <code>hardlink<\/code> caching, but it can't hurt.<\/li>\n<li>Remove the .dvc file<\/li>\n<li>If you need to delete the cache entry itself, run <code>dvc gc<\/code>, or look up the MD5 in <code>data.dvc<\/code> and manually remove it from <code>.dvc\/cache<\/code>.<\/li>\n<\/ol>\n\n<p><em>Edit<\/em> -- there is now an issue on their Github page to add this to the manual: <a href=\"https:\/\/github.com\/iterative\/dvc.org\/issues\/625\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc.org\/issues\/625<\/a><\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1568725966083,
        "Solution_link_count":2.0,
        "Solution_readability":9.6,
        "Solution_reading_time":8.49,
        "Solution_score_count":7.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":79.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":59.1833333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Hey,\n\ni'm trying to understand the MLOps Pipeline with the CI\/CD-Automation (Stage 2 Maturity Level) and struggle with the Feature Store as the component feeding the Automated Pipeline with data. What i found out in the internet was, that Feature Stores extract data from different sources, transform them and create training data which can be used to train the model (retraining with new data). But in the pipeline the steps like Data preperation and Data extraction come after the Feature Store.\n\nCan somebody explain to me, whats the output of the Feature Store and how it is used to serve the data for the Automated Pipeline and the Prediction Service?\n\nThanks in advance",
        "Challenge_closed_time":1678111680000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677898620000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is struggling to understand how the Feature Store component works in the MLOps Pipeline with CI\/CD-Automation. They have researched that Feature Stores extract and transform data to create training data for model retraining, but are confused about how it fits into the pipeline as data preparation and extraction come after the Feature Store. The user is seeking an explanation of the output of the Feature Store and how it serves data for the Automated Pipeline and Prediction Service.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Feature-Store-MLOps-Pipeline\/m-p\/528764#M1375",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":8.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":59.1833333333,
        "Challenge_title":"Feature Store MLOps Pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":104.0,
        "Challenge_word_count":116,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The Feature Store is just a centralized repository of features. By that, its output is just a set of features typically used to train an ML model. Depending on your specific needs, you can serve the ingested data in the Feature Store to the model right away (in what is called feature serving) or export feature values and do further preparation of data.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":4.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":68.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":48.2963777778,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi there,\n\nIs it possible to use R model training and serving in SageMaker ML Pipelines? Looked in examples [here](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/r_examples).  And it doesn't look that R is fully supported currently by ML Pipelines.  Any examples and success stories are very welcome. \n\nThanks.",
        "Challenge_closed_time":1643404063708,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643230196748,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of using R model training and serving in SageMaker ML Pipelines. They have looked at examples but it doesn't seem like R is fully supported currently by ML Pipelines. They are seeking examples and success stories.",
        "Challenge_last_edit_time":1668030995400,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU17aS4s7uSRqmiLuveuchBw\/using-r-model-in-sagemaker-ml-pipelines",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":4.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":48.2963777778,
        "Challenge_title":"Using R model in SageMaker ML pipelines",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":244.0,
        "Challenge_word_count":48,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"In general it is possible to use the SageMaker python SDK and boto3  using the reticulate package in R, However do not have direct examples of SageMaker Pipelines using R.\n\nIt is possible to orchestrate the production pipeline using the R Containers for training and serving and setting up the DAG can be done with reticulate and SageMaker Python SDK and can be achieved using the AWS Step Functions. Please refer to the following example for reference.\n\n\nhttps:\/\/github.com\/aws-samples\/reinvent2020-aim404-productionize-r-using-amazon-sagemaker\nhttps:\/\/www.youtube.com\/watch?v=Zpp0nfvqDCA",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1643404063708,
        "Solution_link_count":2.0,
        "Solution_readability":15.9,
        "Solution_reading_time":7.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":2.3951186111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've got a basic ScriptStep in my AML Pipeline and it's just trying to read an attached dataset. When i execute this simple example, the pipeline fails with the following in the driver log:<\/p>\n\n<blockquote>\n  <p>ImportError: azureml-dataprep is not installed. Dataset cannot be used\n  without azureml-dataprep. Please make sure\n  azureml-dataprep[fuse,pandas] is installed by specifying it in the\n  conda dependencies. pandas is optional and should be only installed if\n  you intend to create a pandas DataFrame from the dataset.<\/p>\n<\/blockquote>\n\n<p>I then modified my step to include the conda package but then the driver fails with \"ResolvePackageNotFound: azureml-dataprep\". The entire log file can be accessed <a href=\"https:\/\/www.dropbox.com\/s\/372ht6jkvzu9loo\/conda.err.txt?dl=0\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<pre><code># create a new runconfig object\nrun_config = RunConfiguration()\nrun_config.environment.docker.enabled = True\nrun_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\nrun_config.environment.python.user_managed_dependencies = False\nrun_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['azureml-dataprep[pandas,fuse]'])\n\nsource_directory = '.\/read-step'\nprint('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))\nstep2 = PythonScriptStep(name=\"read_step\",\n                         script_name=\"Read.py\", \n                         arguments=[\"--dataFilePath\", dataset.as_named_input('local_ds').as_mount() ],\n                         compute_target=aml_compute, \n                         source_directory=source_directory,\n                         runconfig=run_config,\n                         allow_reuse=False)\n<\/code><\/pre>\n\n<p>I'm out of ideas, would deeply appreciate any help here!<\/p>",
        "Challenge_closed_time":1587162956780,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587154334353,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with AzureML where a ScriptStep in their AML Pipeline is failing to read an attached dataset due to the missing installation of azureml-dataprep. The user tried modifying the step to include the conda package, but the driver fails with \"ResolvePackageNotFound: azureml-dataprep\".",
        "Challenge_last_edit_time":1591825691630,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61279914",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":22.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":2.3951186111,
        "Challenge_title":"AzureML: ResolvePackageNotFound azureml-dataprep",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1244.0,
        "Challenge_word_count":155,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330016065408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1704.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>The <code>azureml-sdk<\/code> isn't available on conda, you need to install it with <code>pip<\/code>.<\/p>\n\n<pre><code>myenv = Environment(name=\"myenv\")\nconda_dep = CondaDependencies().add_pip_package(\"azureml-dataprep[pandas,fuse]\")\nmyenv.python.conda_dependencies=conda_dep\nrun_config.environment = myenv\n<\/code><\/pre>\n\n<p>For more information, about this error, the logs tab has a log named <code>20_image_build_log.txt<\/code> which Docker build logs. It contains the error where <code>conda<\/code> failed to failed to find <code>azureml-dataprep<\/code><\/p>\n\n<p>EDIT:<\/p>\n\n<p>Soon, you won't have to specify this dependency anymore. the Azure Data4ML team says <code>azureml-dataprep[pandas,fuse]<\/code> is getting added as a dependency for <code>azureml-defaults<\/code> which is automatically installed on all images. <\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1587416360076,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":10.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1422222222,
        "Challenge_answer_count":0,
        "Challenge_body":"Hello,\r\n\r\nAfter I tried to build a Conda environment using mlu-tab.yml I was ran out of space with no environment created. After I deleted all files from my home folder I still had 95% of my space used. There is no way to \"reimage\" my Studio Lab instance and get back the initial 30Gb of space.\r\n\r\nI followed the AWS Machine Learning University course and cloned the examples for Tabular data course: [(https:\/\/github.com\/aws-samples\/aws-machine-learning-university-accelerated-tab)]\r\n\r\nAfter that I was stupid enough to try creating the Conda environment using the mlu-tab.yml file. the environment creation ate all my space available and creation was failed.\r\nCurrently I have 95% space usage of my \/home\/studio-lab-user folder with no files in it.\r\n\r\nHow can I reimage SageMaker Studio Lab instance to get the space back or uninstall all libraries installed by creating the Conda environment?\r\n\r\nOS: Windows 10\r\nBrowser: Chrome 107.0.5304.107\r\n\r\n![space issue1](https:\/\/user-images.githubusercontent.com\/12427856\/202601233-b7378b40-17d6-4ea3-8e8c-c96bebde0010.png)\r\n![space issue2](https:\/\/user-images.githubusercontent.com\/12427856\/202601236-c6fe41d5-0171-4539-8d82-3eaf0577f427.png)\r\n",
        "Challenge_closed_time":1668738306000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1668737794000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"SageMaker Studio Lab is experiencing elevated errors starting runtimes since November 16, 2022, at 04:00 PM PST. Users are unable to open projects and are receiving an ERR_EMPTY_RESPONSE error in the browser. The SageMaker Studio Lab team is working to restore the service, and users are advised to try again later.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/167",
        "Challenge_link_count":3,
        "Challenge_participation_count":5,
        "Challenge_readability":10.1,
        "Challenge_reading_time":15.87,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":113.0,
        "Challenge_repo_issue_count":218.0,
        "Challenge_repo_star_count":408.0,
        "Challenge_repo_watch_count":20.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.1422222222,
        "Challenge_title":"inability to reimage SageMaker Studio Lab instance to get the space back",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":160,
        "Discussion_body":"Sorry, I haven't done enough research in the existing issue. In fact, there is a solution in issue#42. While a simple button like reset the instance would help, the removing .conda folder helped to release the space.  Thank you for your asking the question and self-service solving! I am glad to hear existing issue (#42 ) worked for you. \r\n\r\nIf you need the reset feature, please up vote ( #75 ) :) .\r\n Hello you can follow the following steps and run the codes provided herewith in a terminal (you can open a terminal after ) :\r\n\r\nAfter you have done all of this stop your project from sagemaker main dashboard. Open a new project. Now you will get a fresh new notebook with almost all the space available. (You can check available space by running \"df -h\", take a look at Avail row beside \"\/home\/studio-lab-user\").\r\n\r\nNote that this process frees us 'almsot' all he space, you may still have 2-3 gb of occupies space. Still d'ont know how to fix it\r\n\r\n```\r\n#1.  switch to base conda environment\r\nconda activate base\r\n\r\n#2. List all conda environments available in your account\r\nconda  list envs\r\n\r\n#Take a look at all the conda environments  available\r\n#3. Delete all the conda environments except environment named 'base'\r\nconda remove -n \"env_name\" --all\r\n\r\n#4.Delete all subdirectories from user folder\r\nrm -rf \/home\/studio-lab-user\/*\r\n\r\n#Restart project\/runtime\r\n``` Thank you for your response @yeasin-arafat-rafio !\r\nThe issue has already been closed.\r\nThe problem was that new environment was not created because available space was not sufficient to install all packages.\r\nYou can reproduce the issue by cloning MLU course from Github with enabled option to create environment.yml file. [aws-machine-learning-university-accelerated-tab ](https:\/\/github.com\/aws-samples\/aws-machine-learning-university-accelerated-tab). Then you can use that yml file to create a conda environment required for the course.\r\nSince it was the default course suggested on the title page of Sagemaker Studio Lab I thought it was tested and safe to use, but I was wrong. @boris-korotkov \r\nHey, how did  you reset the  instance. I haven't tried that cuz I couldn't figure  out  how you did that. Did  you like restart the instance from the main dashboard by stopping the runtime and again starting the runtime",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":219.2780555556,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nService Workbench appears to be unable to launch SageMaker notebook instances at all, due to a missing permission for `sagemaker:AddTags`. This seems to also be the case when custom tags aren't included in the workspace configuration.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Install Service Workbench from the latest version.\r\n2. Create a workspace configuration for a SageMaker notebook.\r\n3. Launch a workspace using the new configuration.\r\n4. Wait a few minutes and observe the error.\r\n\r\n**Expected behavior**\r\nExpected the notebook to launch :)\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/900469\/181163664-98441ee8-7316-4d29-8f85-79d3e5e6ed3c.png)\r\n```\r\nError provisioning environment TestNotebook1. Reason: Errors from CloudFormation: [{LogicalResourceId : SC-455040667691-pp-auh6sv7j6dwr2, ResourceType : AWS::CloudFormation::Stack, StatusReason : The following resource(s) failed to create: [BasicNotebookInstance]. Rollback requested by user.}, {LogicalResourceId : BasicNotebookInstance, ResourceType : AWS::SageMaker::NotebookInstance, StatusReason : User: arn:aws:sts::XXXXXXXXXXXX:assumed-role\/dev-syd-timswb-LaunchConstraint\/servicecatalog is not authorized to perform: sagemaker:AddTags on resource: arn:aws:sagemaker:ap-southeast-2:XXXXXXXXXXXX:assumed:notebook-instance\/basicnotebookinstance-y4ices04e3sv because no identity-based policy allows the sagemaker:AddTags action (Service: AmazonSageMaker; Status Code: 400; Error Code: AccessDeniedException; Request ID: adee97b7-1c89-47e2-8ca7-5aa374a80004; Proxy: null)}, {LogicalResourceId : IAMRole, ResourceType : AWS::IAM::Role, StatusReason : Resource creation Initiated}, {LogicalResourceId : SecurityGroup, ResourceType : AWS::EC2::SecurityGroup, StatusReason : Resource creation Initiated}, {LogicalResourceId : InstanceRolePermissionBoundary, ResourceType : AWS::IAM::ManagedPolicy, StatusReason : Resource creation Initiated}, {LogicalResourceId : BasicNotebookInstanceLifecycleConfig, ResourceType : AWS::SageMaker::NotebookInstanceLifecycleConfig, StatusReason : Resource creation Initiated}, {LogicalResourceId : SC-455040667691-pp-auh6sv7j6dwr2, ResourceType : AWS::CloudFormation::Stack, StatusReason : User Initiated}]\r\n```\r\n\r\n**Versions (please complete the following information):**\r\n5.2.0\r\n(also replicated on an older 5.0.0 install)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1659686697000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1658897296000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user's SageMaker Notebook-v3 workspace that was previously working fine is now showing an \"Unknown\" status and cannot be connected to. Clicking on connect results in an empty window and an error message appears when going back to the SWB page. The expected behavior is that the workspace should be \"Stopped\" and accessible when clicking on Connect. The issue occurred despite the workspace working fine the previous week. The release version installed is 3.3.1.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1018",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":19.7,
        "Challenge_reading_time":33.11,
        "Challenge_repo_contributor_count":43.0,
        "Challenge_repo_fork_count":116.0,
        "Challenge_repo_issue_count":1196.0,
        "Challenge_repo_star_count":165.0,
        "Challenge_repo_watch_count":25.0,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":219.2780555556,
        "Challenge_title":"[Bug] SageMaker instances can't be launched due to missing tags permission",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":223,
        "Discussion_body":"Further context - this only started happening approx. 32 hours ago. If I had to guess... maybe it should have never worked and something just happened to get 'fixed' in the IAM API yesterday? \ud83d\ude01  Hi @tdmalone is this still an issue? Hi @kcadette, it is, yes:\r\n\r\n```\r\nUser: arn:aws:sts::xxxxxxxxxxxx:assumed-role\/dev-syd-timswb-LaunchConstraint\/servicecatalog is not authorized to perform: sagemaker:AddTags on resource: arn:aws:sagemaker:ap-southeast-2:xxxxxxxxxxxx:notebook-instance\/basicnotebookinstance-lqerepcrnmaw because no identity-based policy allows the sagemaker:AddTags action (Service: AmazonSageMaker; Status Code: 400; Error Code: AccessDeniedException; Request ID: 4f72193d-aa17-41b9-8ed0-ee381686cb5b; Proxy: null)}\r\n```\r\n\r\nIt should be a one-line fix, so I've submitted a PR: https:\/\/github.com\/awslabs\/service-workbench-on-aws\/pull\/1021",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1291793452900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berkeley, CA, United States",
        "Answerer_reputation_count":752.0,
        "Answerer_view_count":120.0,
        "Challenge_adjusted_solved_time":3886.8446766667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Short form:\nI am trying to figure out how can I run the hyperparam within a <strong>training step<\/strong> (i.e. train_step = PythonScriptStep(...)) in the pipeline, I am not sure where shall I put the &quot;config=hyperdrive&quot;<\/p>\n<p>Long form:<\/p>\n<p>General:<\/p>\n<pre><code># Register the environment \ndiabetes_env.register(workspace=ws)\nregistered_env = Environment.get(ws, 'diabetes-pipeline-env')\n\n# Create a new runconfig object for the pipeline\nrun_config = RunConfiguration()\n\n# Use the compute you created above. \nrun_config.target = ComputerTarget_Crea\n\n# Assign the environment to the run configuration\nrun_config.environment = registered_env\n<\/code><\/pre>\n<p>Hyperparam:<\/p>\n<pre><code>script_config = ScriptRunConfig(source_directory=experiment_folder,\n                                script='diabetes_training.py',\n                                # Add non-hyperparameter arguments -in this case, the training dataset\n                                arguments = ['--input-data', diabetes_ds.as_named_input('training_data')],\n                                environment=sklearn_env,\n                                compute_target = training_cluster)\n\n# Sample a range of parameter values\nparams = GridParameterSampling(\n    {\n        # Hyperdrive will try 6 combinations, adding these as script arguments\n        '--learning_rate': choice(0.01, 0.1, 1.0),\n        '--n_estimators' : choice(10, 100)\n    }\n)\n\n# Configure hyperdrive settings\nhyperdrive = HyperDriveConfig(run_config=script_config, \n                          hyperparameter_sampling=params, \n                          policy=None, # No early stopping policy\n                          primary_metric_name='AUC', # Find the highest AUC metric\n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                          max_total_runs=6, # Restict the experiment to 6 iterations\n                          max_concurrent_runs=2) # Run up to 2 iterations in parallel\n\n# Run the experiment if I only want to run hyperparam alone without the pipeline\n#experiment = Experiment(workspace=ws, name='mslearn-diabetes-hyperdrive')\n#run = experiment.submit(**config=hyperdrive**)\n<\/code><\/pre>\n<p>PipeLine:<\/p>\n<pre><code>prep_step = PythonScriptStep(name = &quot;Prepare Data&quot;,\n                                source_directory = experiment_folder,\n                                script_name = &quot;prep_diabetes.py&quot;,\n                                arguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n                                             '--prepped-data', prepped_data_folder],\n                                outputs=[prepped_data_folder],\n                                compute_target = ComputerTarget_Crea,\n                                runconfig = run_config,\n                                allow_reuse = True)\n\n# Step 2, run the training script\ntrain_step = PythonScriptStep(name = &quot;Train and Register Model&quot;,\n                                source_directory = experiment_folder,\n                                script_name = &quot;train_diabetes.py&quot;,\n                                arguments = ['--training-folder', prepped_data_folder],\n                                inputs=[prepped_data_folder],\n                                compute_target = ComputerTarget_Crea,\n                                runconfig = run_config,\n                                allow_reuse = True)\n# Construct the pipeline\npipeline_steps = [prep_step, train_step]\npipeline = Pipeline(workspace=ws, steps=pipeline_steps)\nprint(&quot;Pipeline is built.&quot;)\n\n# Create an experiment and run the pipeline\n**#How do I need to change these below lines to use hyperdrive????**\nexperiment = Experiment(workspace=ws, name = 'mslearn-diabetes-pipeline')\npipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n<\/code><\/pre>\n<p>Not sure where I need to put <strong>config=hyperdrive<\/strong> in the Pipeline section?<\/p>",
        "Challenge_closed_time":1633922581063,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619929277080,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to figure out how to run hyperparameters within a training step in a pipeline using AzureML SDK. They have already created a hyperdrive configuration and a pipeline with two steps, but they are unsure where to put \"config=hyperdrive\" in the pipeline section to use hyperdrive.",
        "Challenge_last_edit_time":1619929940227,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67352949",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":20.0,
        "Challenge_reading_time":41.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":3887.0288841667,
        "Challenge_title":"How to combine pipeline and hyperparameter in AzureML SDK in the training step",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":276.0,
        "Challenge_word_count":280,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1375186444008,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Auckland, New Zealand",
        "Poster_reputation_count":912.0,
        "Poster_view_count":288.0,
        "Solution_body":"<p>here's how to combine hyperparameters with an AML pipeline: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.hyperdrivestep?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.hyperdrivestep?view=azure-ml-py<\/a><\/p>\n<p>Alternatively, here's a sample notebook: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":57.4,
        "Solution_reading_time":11.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1477314855767,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Gothenburg, Sweden",
        "Answerer_reputation_count":4772.0,
        "Answerer_view_count":641.0,
        "Challenge_adjusted_solved_time":3.6071191667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I used <a href=\"https:\/\/github.com\/googleapis\/java-aiplatform\/blob\/master\/google-cloud-aiplatform\/src\/main\/java\/com\/google\/cloud\/aiplatform\/v1\/PipelineServiceClient.java\" rel=\"nofollow noreferrer\">this code<\/a>, straight from the Javadocs, to delete a VertexAI Training Pipeline<\/p>\n<pre><code>try (PipelineServiceClient pipelineServiceClient = PipelineServiceClient.create()) {\n  TrainingPipelineName name =\n      TrainingPipelineName.of(&quot;[PROJECT]&quot;, &quot;[LOCATION]&quot;, &quot;[TRAINING_PIPELINE]&quot;);\n  pipelineServiceClient.deleteTrainingPipelineAsync(name).get();\n}\n<\/code><\/pre>\n<p>I get this error. From what I can see, this means that this API, though officially documented, is simply unimplemented. How do we delete Training Pipelines using Java?<\/p>\n<pre><code>Error in deleting \/\/aiplatform.googleapis.com\/projects\/746859988231\/locations\/us-central1\/trainingPipelines\/186468439399187392: \njava.util.concurrent.ExecutionException: \ncom.google.api.gax.rpc.UnimplementedException: io.grpc.StatusRuntimeException:\nUNIMPLEMENTED: HTTP status code 404\n...\n&lt;!DOCTYPE html&gt;\n&lt;html lang=en&gt;\n....\n  &lt;p&gt;The requested URL &lt;code&gt;\/google.cloud.aiplatform.v1.PipelineService\n\/DeleteTrainingPipeline&lt;\/code&gt; was not found on this server.  \n&lt;ins&gt;That\u2019s all we know.&lt;\/ins&gt;\n<\/code><\/pre>",
        "Challenge_closed_time":1631875853752,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631862868123,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to delete a VertexAI Training Pipeline using Java code from the official documentation. The Delete Training Pipeline REST endpoint seems to be unimplemented, resulting in a 404 error. The user is seeking an alternative method to delete Training Pipelines using Java.",
        "Challenge_last_edit_time":1631987478696,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69219230",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":20.2,
        "Challenge_reading_time":19.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":3.6071191667,
        "Challenge_title":"In GCP Vertex AI, why is Delete Training Pipeline REST endpoint unimplemented?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":259.0,
        "Challenge_word_count":99,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1227171471292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":17500.0,
        "Poster_view_count":1561.0,
        "Solution_body":"<p>According to the official documentation, <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest<\/a> , the supported service URLs for this service are:<\/p>\n<pre><code>https:\/\/us-central1-aiplatform.googleapis.com\nhttps:\/\/us-east1-aiplatform.googleapis.com\nhttps:\/\/us-east4-aiplatform.googleapis.com\nhttps:\/\/us-west1-aiplatform.googleapis.com\nhttps:\/\/northamerica-northeast1-aiplatform.googleapis.com\nhttps:\/\/europe-west1-aiplatform.googleapis.com\nhttps:\/\/europe-west2-aiplatform.googleapis.com\nhttps:\/\/europe-west4-aiplatform.googleapis.com\nhttps:\/\/asia-east1-aiplatform.googleapis.com\nhttps:\/\/asia-northeast1-aiplatform.googleapis.com\nhttps:\/\/asia-northeast3-aiplatform.googleapis.com\nhttps:\/\/asia-southeast1-aiplatform.googleapis.com\nhttps:\/\/australia-southeast1-aiplatform.googleapis.com\n<\/code><\/pre>\n<hr \/>\n<pre><code>  PipelineServiceSettings pipelineServiceSettings =\n        PipelineServiceSettings.newBuilder()\n            .setEndpoint(&quot;us-central1-aiplatform.googleapis.com:443&quot;)\n            .build();\n<\/code><\/pre>\n<hr \/>\n<pre><code> try (PipelineServiceClient pipelineServiceClient =\n      PipelineServiceClient.create(pipelineServiceSettings)) {\n\n  String location = &quot;us-central1&quot;;\n  TrainingPipelineName trainingPipelineName =\n      TrainingPipelineName.of(project, location, trainingPipelineId);\n\n  OperationFuture&lt;Empty, DeleteOperationMetadata&gt; operationFuture =\n      pipelineServiceClient.deleteTrainingPipelineAsync(trainingPipelineName);\n\n  System.out.format(&quot;Operation name: %s\\n&quot;, operationFuture.getInitialFuture().get().getName());\n  System.out.println(&quot;Waiting for operation to finish...&quot;);\n  operationFuture.get(300, TimeUnit.SECONDS);\n\n  System.out.format(&quot;Deleted Training Pipeline.&quot;);\n}\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1631888060710,
        "Solution_link_count":15.0,
        "Solution_readability":45.0,
        "Solution_reading_time":25.4,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":72.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6132516667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,    <\/p>\n<p>I tried to use the pipe operation %&gt;% of R in an azure notebook without success ...    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/16866-image.png?platform=QnA\" alt=\"16866-image.png\" \/>    <\/p>\n<p>is possible to use it or it is a limitation in azure notebooks ?<\/p>",
        "Challenge_closed_time":1597097474163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597095266457,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges while using the pipe operation %>% of R in an azure notebook and is seeking clarification on whether it is possible to use it or if it is a limitation in azure notebooks.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/63863\/pipe-gt-for-r-is-not-working-in-azure-notebooks",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":4.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.6132516667,
        "Challenge_title":"Pipe %&gt;% for R is not working in azure notebooks",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":46,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,  <\/p>\n<p>Fixed.  <\/p>\n<p>Azure Notebook release the session after some time of inactivity, therefore the dplyr package wasn\u00b4t loaded in the session<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":1.97,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1327570314367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation_count":2854.0,
        "Answerer_view_count":324.0,
        "Challenge_adjusted_solved_time":30.6574666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are deploying a data consortium between more than 10 companies. Wi will deploy several machine learning models (in general advanced analytics models) for all the companies and we will administrate all the models. We are looking for a solution that administrates several servers, clusters and data science pipelines. I love kedro, but not sure what is the best option to administrate all while using kedro.<\/p>\n<p>In summary, we are looking for the best solution to administrate several models, tasks and pipelines in different servers and possibly Spark clusters. Our current options are:<\/p>\n<ul>\n<li><p>AWS as our data warehouse and Databricks for administrating servers, clusters and tasks. I don't feel that the notebooks of databricks are a good solution for building pipelines and to work collaboratively, so I would like to connect kedro to databricks (is it good? is it easy to schedule the run of the kedro pipelines using databricks?)<\/p>\n<\/li>\n<li><p>Using GCP for data warehouse and use kubeflow (iin GCP) for deploying models and the administration and the schedule of the pipelines and the needed resources<\/p>\n<\/li>\n<li><p>Setting up servers from ASW or GCP, install kedro and schedule the pipelines with airflow (I see a big problem administrating 20 servers and 40 pipelines)<\/p>\n<\/li>\n<\/ul>\n<p>I would like to know if someone knows what is the best option between these alternatives, their  downsides and advantages, or if there are more possibilities.<\/p>",
        "Challenge_closed_time":1605891844630,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605830422657,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a solution to administer several machine learning models and pipelines for a data consortium between more than 10 companies. They are considering using AWS with Databricks and Kedro, GCP with Kubeflow, or setting up servers with Kedro and Airflow. They are seeking advice on the best option and its advantages and disadvantages.",
        "Challenge_last_edit_time":1605836750283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64921833",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":19.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":17.0616591667,
        "Challenge_title":"DataBricks + Kedro Vs GCP + Kubeflow Vs Server + Kedro + Airflow",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":967.0,
        "Challenge_word_count":241,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605828724552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":59.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I'll try and summarise what I know, but be aware that I've not been part of a KubeFlow project.<\/p>\n<h2>Kedro on Databricks<\/h2>\n<p>Our approach was to build our project with CI and then execute the pipeline from a notebook. We <em>did not<\/em> use the <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/11_tools_integration\/03_databricks.html\" rel=\"nofollow noreferrer\">kedro recommended approach<\/a> of using databricks-connect due to the <a href=\"https:\/\/databricks.com\/product\/aws-pricing\" rel=\"nofollow noreferrer\">large price difference<\/a> between Jobs and Interactive Clusters (which are needed for DB-connect). If you're working on several TB's of data, this quickly becomes relevant.<\/p>\n<p>As a DS, this approach may feel natural, as a SWE though it does not. Running pipelines in notebooks feels hacky. It works but it feels non-industrialised. Databricks performs well in automatically spinning up and down clusters &amp; taking care of the runtime for you. So their value add is abstracting IaaS away from you (more on that later).<\/p>\n<h2>GCP &amp; &quot;Cloud Native&quot;<\/h2>\n<p><strong>Pro<\/strong>: GCP's main selling point is BigQuery. It is an incredibly powerful platform, simply because you can be productive from day 0. I've seen people build entire web API's on top of it. KubeFlow isn't tied to GCP so you could port this somewhere else later on. Kubernetes will also allow you to run anything else you wish on the cluster, API's, streaming, web services, websites, you name it.<\/p>\n<p><strong>Con<\/strong>: Kubernetes is complex. If you have 10+ engineers to run this project long-term, you should be OK. But don't underestimate the complexity of Kubernetes. It is to the cloud what Linux is to the OS world. Think log management, noisy neighbours (one cluster for web APIs + batch spark jobs), multi-cluster management (one cluster per department\/project), security, resource access etc.<\/p>\n<h2>IaaS server approach<\/h2>\n<p>Your last alternative, the manual installation of servers is one I would recommend only if you have a large team, extremely large data and are building a long-term product who's revenue can sustain the large maintenance costs.<\/p>\n<h2>The people behind it<\/h2>\n<p>How does the talent market look like in your region? If you can hire experienced engineers with GCP knowledge, I'd go for the 2nd solution. GCP is a mature, &quot;native&quot; platform in the sense that it abstracts a lot away for customers. If your market has mainly AWS engineers, that may be a better road to take. If you have a number of kedro engineers, that also has relevance. Note that kedro is agnostic enough to run anywhere. It's really just python code.<\/p>\n<p><strong>Subjective advise<\/strong>:<\/p>\n<p>Having worked mostly on AWS projects and a few GCP projects, I'd go for GCP. I'd use the platform's components (BigQuery, Cloud Run, PubSub, Functions, K8S) as a toolbox to choose from and build an organisation around that. Kedro can run in any of these contexts, as a triggered job by the Scheduler, as a container on Kubernetes or as a ETL pipeline bringing data into (or out of) BigQuery.<\/p>\n<p>While Databricks is &quot;less management&quot; than raw AWS, it's still servers to think about and VPC networking charges to worry over. BigQuery is simply GB queried. Functions are simply invocation count. These high level components will allow you to quickly show value to customers and you only need to go deeper (RaaS -&gt; PaaS -&gt; IaaS) as you scale.<\/p>\n<p>AWS also has these higher level abstractions over IaaS but in general, it appears (to me) that Google's offering is the most mature. Mainly because they have published tools they've been using internally for almost a decade whereas AWS has built new tools for the market. AWS is the king of IaaS though.<\/p>\n<p>Finally, a bit of content, <a href=\"https:\/\/youtu.be\/kjhXMTOLtac?t=618\" rel=\"nofollow noreferrer\">two former colleagues have discussed ML industrialisation frameworks earlier this fall<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1605947117163,
        "Solution_link_count":3.0,
        "Solution_readability":7.9,
        "Solution_reading_time":49.9,
        "Solution_score_count":4.0,
        "Solution_sentence_count":41.0,
        "Solution_word_count":606.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1489644560420,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Planet Earth",
        "Answerer_reputation_count":791.0,
        "Answerer_view_count":253.0,
        "Challenge_adjusted_solved_time":3.8482858333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to install python package cassandra driver in Azure Machine Learning studio. I am following this answer from <a href=\"https:\/\/stackoverflow.com\/questions\/44371692\/install-python-packages-in-azure-ml\">here<\/a>. Unfortunately i don't see any wheel file for cassandra-driver <a href=\"https:\/\/pypi.python.org\/pypi\/cassandra-driver\/\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/cassandra-driver\/<\/a> so i downloaded the .tar file and converted to zip.<\/p>\n<p>I included this .zip file as dataset and connected to python script<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/omsO9.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/omsO9.jpg\" alt=\"jpg1\" \/><\/a><\/p>\n<p>But when i run it, it says No module named cassandra\n<a href=\"https:\/\/i.stack.imgur.com\/4DKTB.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4DKTB.jpg\" alt=\"jpg2\" \/><\/a><\/p>\n<p>Does this work only with wheel file? Any solution is much appreciated.<\/p>\n<p>I am using Python Version :  Anoconda 4.0\/Python 3.5<\/p>",
        "Challenge_closed_time":1519124227916,
        "Challenge_comment_count":2,
        "Challenge_created_time":1519110374087,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an ImportError while trying to install the cassandra driver python package in Azure Machine Learning Studio. They downloaded the .tar file and converted it to a zip file, included it as a dataset, and connected it to a python script. However, when they run it, it says \"No module named cassandra\". The user is using Python Version: Anoconda 4.0\/Python 3.5 and is seeking a solution to the issue.",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48879595",
        "Challenge_link_count":7,
        "Challenge_participation_count":3,
        "Challenge_readability":10.9,
        "Challenge_reading_time":14.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":3.8482858333,
        "Challenge_title":"ImportError: No module named cassandra in Azure Machine Learning Studio",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":500.0,
        "Challenge_word_count":111,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1489644560420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Planet Earth",
        "Poster_reputation_count":791.0,
        "Poster_view_count":253.0,
        "Solution_body":"<p>I got it working. Changed the folder inside .zip file to <code>\"cassandra\"<\/code> (just like cassandra package). <\/p>\n\n<p>And in the Python script, i added <\/p>\n\n<pre><code>from cassandra import *\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":2.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":29.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1620154324507,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":1169.0,
        "Answerer_view_count":2077.0,
        "Challenge_adjusted_solved_time":131.3345208333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to track the resources consumed by a VertexAI pipeline run, similar to how it is possible to do for Dataflow where it shows a live graph of how many nodes are currently running to execute the pipeline?<\/p>",
        "Challenge_closed_time":1628260970907,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628219232657,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of tracking resources used by a VertexAI pipeline run, similar to how it is done for Dataflow.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68675615",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":11.5939583334,
        "Challenge_title":"Tracking resources used by VertexAI pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":272.0,
        "Challenge_word_count":45,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1471292986790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":700.0,
        "Poster_view_count":90.0,
        "Solution_body":"<p>Vertex AI Pipeline provides a feature for <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/visualize-pipeline\" rel=\"nofollow noreferrer\">Visualizing and analyzing<\/a> the pipeline results.<\/p>\n<p>This feature can be used to check the resource utilization once the Pipeline is deployed.<\/p>\n<p><strong>steps:<\/strong><\/p>\n<pre><code>Go to vertex AI pipeline-&gt;\n         Select a pipeline-&gt;\n               pipeline step-&gt;\n                     view job(from Pipeline run analysis pane)\n<\/code><\/pre>\n<p>In the View Job pane we can check for the resources utilized i.e machine types,machine count,CPU utilization graph for the pipeline step and we can view the logs too.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BTMXZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BTMXZ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Utilizations:<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8WQRc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8WQRc.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As per this <a href=\"https:\/\/cloud.google.com\/monitoring\/api\/metrics_gcp#gcp-aiplatform\" rel=\"nofollow noreferrer\">document<\/a>, metrics from the Vertex AI like CPU utilization, CPU load are in the <a href=\"https:\/\/cloud.google.com\/products\/#product-launch-stages\" rel=\"nofollow noreferrer\">Beta<\/a> launch stage. However, you can examine the metrics like CPU utilization from Cloud Monitoring by referring to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/monitoring-metrics\" rel=\"nofollow noreferrer\">document<\/a> and also find the below snap for more reference.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yyvnd.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yyvnd.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For changing the timeline of the graph you have to select the <strong>custom<\/strong> option in <strong>metrics explorer<\/strong> and provide the date and time for the duration that you want to view as shown in the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6ZUgE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6ZUgE.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1628692036932,
        "Solution_link_count":12.0,
        "Solution_readability":12.1,
        "Solution_reading_time":28.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":211.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1393579668636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":1450.0,
        "Answerer_view_count":162.0,
        "Challenge_adjusted_solved_time":4.1181655556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p><a href=\"https:\/\/stackoverflow.com\/questions\/tagged\/kedro\"><code>kedro<\/code><\/a> recommends storing parameters in <code>conf\/base\/parameters.yml<\/code>. Let's assume it looks like this:<\/p>\n\n<pre><code>step_size: 1\nmodel_params:\n    learning_rate: 0.01\n    test_data_ratio: 0.2\n    num_train_steps: 10000\n<\/code><\/pre>\n\n<p>And now imagine I have some <code>data_engineering<\/code> pipeline whose <code>nodes.py<\/code> has a function that looks something like this:<\/p>\n\n<pre><code>def some_pipeline_step(num_train_steps):\n    \"\"\"\n    Takes the parameter `num_train_steps` as argument.\n    \"\"\"\n    pass\n<\/code><\/pre>\n\n<p>How would I go about and pass that nested parameters straight to this function in <code>data_engineering\/pipeline.py<\/code>? I unsuccessfully tried:<\/p>\n\n<pre><code>from kedro.pipeline import Pipeline, node\n\nfrom .nodes import split_data\n\n\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                some_pipeline_step,\n                [\"params:model_params.num_train_steps\"],\n                dict(\n                    train_x=\"train_x\",\n                    train_y=\"train_y\",\n                ),\n            )\n        ]\n    )\n<\/code><\/pre>\n\n<p>I know that I could just pass all parameters into the function by using <code>['parameters']<\/code> or just pass all <code>model_params<\/code> parameters with <code>['params:model_params']<\/code> but it seems unelegant and I feel like there must be a way. Would appreciate any input!<\/p>",
        "Challenge_closed_time":1587979890916,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587965065520,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to pass nested parameters directly to a function in Kedro's data engineering pipeline, but is having trouble doing so. They have tried using the \"params:model_params.num_train_steps\" syntax, but it did not work. The user is looking for a more elegant solution to pass the nested parameters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61452211",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.3,
        "Challenge_reading_time":17.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":4.1181655556,
        "Challenge_title":"Kedro - how to pass nested parameters directly to node",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1403.0,
        "Challenge_word_count":138,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525290575943,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":143.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>(Disclaimer: I'm part of the Kedro team)<\/p>\n\n<p>Thank you for your question. Current version of Kedro, unfortunately, does not support nested parameters. The interim solution would be to use top-level keys inside the node (as you already pointed out) or decorate your node function with some sort of a parameter filter, which is not elegant either.<\/p>\n\n<p>Probably the most viable solution would be to customise your <code>ProjectContext<\/code> (in <code>src\/&lt;package_name&gt;\/run.py<\/code>) class by overwriting <code>_get_feed_dict<\/code> method as follows:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>class ProjectContext(KedroContext):\n    # ...\n\n\n    def _get_feed_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Get parameters and return the feed dictionary.\"\"\"\n        params = self.params\n        feed_dict = {\"parameters\": params}\n\n        def _add_param_to_feed_dict(param_name, param_value):\n            \"\"\"This recursively adds parameter paths to the `feed_dict`,\n            whenever `param_value` is a dictionary itself, so that users can\n            specify specific nested parameters in their node inputs.\n\n            Example:\n\n                &gt;&gt;&gt; param_name = \"a\"\n                &gt;&gt;&gt; param_value = {\"b\": 1}\n                &gt;&gt;&gt; _add_param_to_feed_dict(param_name, param_value)\n                &gt;&gt;&gt; assert feed_dict[\"params:a\"] == {\"b\": 1}\n                &gt;&gt;&gt; assert feed_dict[\"params:a.b\"] == 1\n            \"\"\"\n            key = \"params:{}\".format(param_name)\n            feed_dict[key] = param_value\n\n            if isinstance(param_value, dict):\n                for key, val in param_value.items():\n                    _add_param_to_feed_dict(\"{}.{}\".format(param_name, key), val)\n\n        for param_name, param_value in params.items():\n            _add_param_to_feed_dict(param_name, param_value)\n\n        return feed_dict\n<\/code><\/pre>\n\n<p>Please also note that this issue has already been <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/commit\/529606273e201a736f10338ada73ac6206081730\" rel=\"nofollow noreferrer\">addressed on develop<\/a> and will become available in the next release. The fix uses the approach from the snippet above.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.7,
        "Solution_reading_time":25.18,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":203.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":146.8910691667,
        "Challenge_answer_count":9,
        "Challenge_body":"<p>I have a compute environment where I was running wandb offline for quite a while. I am now hoping to use it online (to get automatic syncing), however I seem to be unable to set this up now. The following is a minimal reproducible example:<\/p>\n<pre><code class=\"lang-auto\">&gt;&gt; import wandb\n&gt;&gt; test = wandb.init(mode='online')\nTraceback (most recent call last):\n  File \"[path]\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_init.py\", line 867, in init\n    wi.setup(kwargs)\n  File \"[path]\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_init.py\", line 182, in setup\n    user_settings = self._wl._load_user_settings()\n  File \"[path]\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_setup.py\", line 183, in _load_user_settings\n    flags = self._server._flags\nAttributeError: 'NoneType' object has no attribute '_flags'\nwandb: ERROR Abnormal program exit\n<\/code><\/pre>\n<p>I have tried<\/p>\n<ul>\n<li>running wandb online in the terminal<\/li>\n<li>setting the wandb mode environment variable to be online<\/li>\n<li>uninstalling and reinstalling wandb<\/li>\n<\/ul>\n<p>Is there any way I can run this online?<\/p>",
        "Challenge_closed_time":1637034448659,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636505640810,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user was previously running wandb offline but now wants to switch to online mode for automatic syncing. However, they are encountering an error when trying to initialize wandb in online mode. They have tried various solutions such as setting the wandb mode environment variable and reinstalling wandb, but the issue persists. The user is seeking help to resolve the problem and run wandb online.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/unable-to-run-wandb-online-after-running-offline\/1252",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":10.4,
        "Challenge_reading_time":14.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":146.8910691667,
        "Challenge_title":"Unable to run wandb online after running offline",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":390.0,
        "Challenge_word_count":133,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/dimaduev\">@dimaduev<\/a> , thanks so much for the fixes! I think I was able to resolve this through looking at the different WANDB_DIR locations\u2026 I had several in different bashrc\/zshrc files and I suspect this was causing an issue. It seems to be resolved now!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":3.72,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":46.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":354.7508880556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>  Would like to know is Windows Server Compute Instance available in Azure ML Workspace?  <\/p>\n<p>  I do understand there is DSVM for windows, but it won't have direct link to datasources and the jupyter notebook, or it is possible for me to link the compute once I have created the VM?  <\/p>\n<p>  I'm having a problem to migrate a conda environment from Windows to Linux, may I know what is the best practice for such migration?  <\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1628119217447,
        "Challenge_comment_count":1,
        "Challenge_created_time":1626842114250,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about the availability of Windows Server Compute Instance in Azure ML Workspace and is facing challenges in migrating a conda environment from Windows to Linux. They are also seeking advice on the best practices for such migration.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/483713\/azure-ml-workspace-windows-server-compute-instance",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.6,
        "Challenge_reading_time":6.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":354.7508880556,
        "Challenge_title":"Azure ML Workspace Windows Server Compute Instance",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":89,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=6760e7be-77ee-4c41-9c63-8bbcdab1aaae\">@SoonJoo@Genting  <\/a> Hello, I have reached out to DSVM, actually they support Azure service and JupyterNotebook.<\/p>\n<p>As the document said:<\/p>\n<blockquote>\n<p>&gt; It also allows you to access services on the Azure cloud platform. Azure provides several compute, storage, data analytics, and other services that you can administer and access from your DSVM.<\/p>\n<p>To administer your Azure subscription and cloud resources, you have two options:<\/p>\n<p>Use your browser and go to the Azure portal.<\/p>\n<p>Use PowerShell scripts. Run Azure PowerShell from a shortcut on the desktop or from the Start menu. See the Microsoft Azure PowerShell documentation for full details.<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/vm-do-ten-things#manage-azure-resources\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/vm-do-ten-things#manage-azure-resources<\/a><\/p>\n<blockquote>\n<p>To start the Jupyter Notebook, select the Jupyter Notebook icon on the Start menu or on the desktop. In the DSVM command prompt, you can also run the command jupyter notebook from the directory where you have existing notebooks or where you want to create new notebooks.<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/vm-do-ten-things#use-jupyter-notebooks\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/vm-do-ten-things#use-jupyter-notebooks<\/a><\/p>\n<p>Hope this helps.<\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.0,
        "Solution_reading_time":22.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":161.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1468845236196,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":55.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":0.0645166667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>so I've been poking around with sacred a little bit and it seems great. unfortunately I did not find any multiple files use-cases examples like I am trying to implement.<\/p>\n\n<p>so i have this file called configuration.py, it is intended to contain different variables which will (using sacred) be plugged in to the rest of the code (laying in different files):<\/p>\n\n<pre><code>from sacred import Experiment\nex = Experiment('Analysis')\n\n@ex.config\ndef configure_analysis_default():\n    \"\"\"Initializes default  \"\"\"\n    generic_name = \"C:\\\\basic_config.cfg\" # configuration filename\n    message = \"This is my generic name: %s!\" % generic_name\n    print(message)\n\n@ex.automain #automain function needs to be at the end of the file. Otherwise everything below it is not defined yet\n#  when the experiment is run.\ndef my_main(message):\n    print(message)\n<\/code><\/pre>\n\n<p>This by itself works great. sacred is working as expected. However, when I'm trying to introduce a second file named Analysis.py:<\/p>\n\n<pre><code>import configuration\nfrom sacred import Experiment\nex = Experiment('Analysis')\n\n@ex.capture\ndef what_is_love(generic_name):\n    message = \" I don't know\"\n    print(message)\n    print(generic_name)\n\n@ex.automain\ndef my_main1():\n    what_is_love()\n<\/code><\/pre>\n\n<p>running Analysis.py yields:<\/p>\n\n<p><strong>Error:<\/strong><\/p>\n\n<blockquote>\n  <p>TypeError: what_is_love is missing value(s) for ['generic_name']<\/p>\n<\/blockquote>\n\n<p>I expected that the 'import configuration' statement to include the configuration.py file, thus importing everything that was configured in there including configure_analysis_default() alongside its decorator @ex.config and then inject it to what_is_love(generic_name).\nWhat am I doing wrong? how can i fix this?<\/p>\n\n<p>Appreciate it!<\/p>",
        "Challenge_closed_time":1513160793190,
        "Challenge_comment_count":0,
        "Challenge_created_time":1513160560930,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while using the Sacred library in Python. They have created a file called configuration.py to contain variables that will be used in the rest of the code. However, when they try to introduce a second file named Analysis.py, they encounter an error stating that the function what_is_love is missing value(s) for ['generic_name']. The user expected the 'import configuration' statement to include the configuration.py file and import everything that was configured in there, including configure_analysis_default() alongside its decorator @ex.config, and then inject it to what_is_love(generic_name).",
        "Challenge_last_edit_time":1513161714983,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47790619",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.6,
        "Challenge_reading_time":22.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":0.0645166667,
        "Challenge_title":"sacred, python - ex.config in one file and ex",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1446.0,
        "Challenge_word_count":221,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1468845236196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>So, pretty dumb, but I'll post it here in favour of whoever will have similar issue...<\/p>\n\n<p>My issue is that I have created a different instance of experiment. I needed simply to import my experiment from the configuration file.<\/p>\n\n<p>replacing this:<\/p>\n\n<pre><code>import configuration\nfrom sacred import Experiment\nex = Experiment('Analysis')\n<\/code><\/pre>\n\n<p>with this:<\/p>\n\n<pre><code>import configuration\nex = configuration.ex\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.6,
        "Solution_reading_time":5.76,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":57.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":1343237570416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"California",
        "Answerer_reputation_count":1325.0,
        "Answerer_view_count":131.0,
        "Challenge_adjusted_solved_time":5957.5074197222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html#sagemaker.estimator.Framework\" rel=\"noreferrer\">SageMaker python sdk<\/a> and was hoping to pass in some arguments to be used by my entrypoint, I'm not seeing how to do this.<\/p>\n\n<pre><code>from sagemaker.sklearn.estimator import SKLearn  # sagemaker python sdk\n\nentrypoint = 'entrypoint_script.py'\n\nsklearn = SKLearn(entry_point=entrypoint,  # &lt;-- need to pass args to this\n                  train_instance_type=instance_class,\n                  role=role,\n                  sagemaker_session=sm)\n<\/code><\/pre>",
        "Challenge_closed_time":1556949846448,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556867880610,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to pass arguments to the entrypoint of a SageMaker estimator using the SageMaker python sdk but is unable to find a way to do so. They have provided a code snippet for reference.",
        "Challenge_last_edit_time":1556899526172,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55964972",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.3,
        "Challenge_reading_time":7.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":22.7682883333,
        "Challenge_title":"Can I pass arguments to the entrypoint of a SageMaker estimator?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":2780.0,
        "Challenge_word_count":61,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343237570416,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"California",
        "Poster_reputation_count":1325.0,
        "Poster_view_count":131.0,
        "Solution_body":"<p>The answer is no as there is no parameter on the Estimator base class, or the fit method, that accepts arguments to pass to the entrypoint.<\/p>\n\n<p>I resolved this by passing the parameter as part of the hyperparameter dictionary. This gets passed to the entrypoint as arguments.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1578346552883,
        "Solution_link_count":0.0,
        "Solution_readability":9.4,
        "Solution_reading_time":3.51,
        "Solution_score_count":6.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":47.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.3998158333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,    <\/p>\n<p>I created a working pipeline in azure machine learning studio but I am stuck how i can use it with a live dataset. Could anybody help to me in this issue? I dont have such option to deploy it.    <\/p>\n<p>thank you in advance    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/160869-pipeline.png?platform=QnA\" alt=\"160869-pipeline.png\" \/>    <\/p>",
        "Challenge_closed_time":1640726658680,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640685619343,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has created a working pipeline in Azure Machine Learning Studio but is unsure how to use it with a live dataset as they do not have the option to deploy it. They are seeking help with this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/677175\/how-can-i-use-a-working-pipeline",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":5.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":11.3998158333,
        "Challenge_title":"How can I use a working pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, please review <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy#test-the-real-time-endpoint\">Test the real-time endpoint<\/a> for more details on how to test your model. You can consume your model using a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service\">Client<\/a> or <a href=\"https:\/\/learn.microsoft.com\/en-us\/power-bi\/connect-data\/service-aml-integrate\">PowerBI<\/a>.<\/p>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":19.3,
        "Solution_reading_time":7.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1586454383232,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":44.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":87.4057897222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am running the mlflow registry using <code>mlflow server<\/code> (<a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/model-registry.html<\/a>). The server runs fine. If the server crashes for any reason it restart automatically. But for the time of restart the server is not available.<\/p>\n\n<p>Is it possible to run multiple isntances in parallel behind a load balancer? Is this safe or could it be possible that there are any inconsistencies?<\/p>",
        "Challenge_closed_time":1588382828087,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588079911073,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is running MLFlow Registry using \"mlflow server\" and is facing an issue where the server restarts automatically in case of a crash, but during the restart, the server is not available. The user is wondering if it is possible to run multiple instances in parallel behind a load balancer and if it is safe to do so without any inconsistencies.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61481147",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":7.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":84.143615,
        "Challenge_title":"MLFlow Registry high availability",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":501.0,
        "Challenge_word_count":67,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378563249260,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":470.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>Yes, it's possible to have multiple instances of MLflow Tracker Service running behind a load balancer.<\/p>\n\n<p>Because the Tracking server is stateless, you could have multiple instances log to a replicated primary DB as a store. A second hot standby can take over if the primary fails.<\/p>\n\n<p>As for the documentation in how to set up replicated instances of your backend store will vary on which one you elect to use, we cannot definitely document all different scenarios and their configurations.<\/p>\n\n<p>I would check the respective documentation of your backend DB and load balancer for how to federate requests to multiple instances of an MLflow tracking server, how to failover to a hot standby or replicated DB, or how to configure a hot-standby replicated DB instance.<\/p>\n\n<p>The short of it: MLflow tracking server is stateless.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1588394571916,
        "Solution_link_count":0.0,
        "Solution_readability":12.1,
        "Solution_reading_time":10.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":135.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":766.1991666667,
        "Challenge_answer_count":0,
        "Challenge_body":"<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\n\r\n## Expected Behavior\r\n<WARNING:elasticsearch:PUT https:\/\/my aws ES endpoint\/table_a54a9a96-c246-4bcd-b417-2d8c005c3290 [status:400 request:0.069s]\r\nINFO:databuilder.callback.call_back:No callbacks to notify\r\nTraceback (most recent call last):\r\n  File \"example\/scripts\/sample_data_loader_neptune.py\", line 403, in <module>\r\n    job_es_table.launch()\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/job\/job.py\", line 76, in launch\r\n    raise e\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/job\/job.py\", line 72, in launch\r\n    self.publisher.publish()\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/base_publisher.py\", line 40, in publish\r\n    raise e\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/base_publisher.py\", line 37, in publish\r\n    self.publish_impl()\r\n  File \"\/tmp\/damundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/elasticsearch_publisher.py\", line 93, in publish_impl\r\n    self.elasticsearch_client.indices.create(index=self.elasticsearch_new_index, body=self.elasticsearch_mapping)\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/client\/utils.py\", line 347, in _wrapped\r\n    return func(*args, params=params, headers=headers, **kwargs)\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/client\/indices.py\", line 146, in create\r\n    \"PUT\", _make_path(index), params=params, headers=headers, body=body\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/transport.py\", line 466, in perform_request\r\n    raise e\r\n  File \"\/tmp\/damundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/transport.py\", line 434, in perform_request\r\n    timeout=timeout,\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/connection\/http_requests.py\", line 216, in perform_request\r\n    self._raise_error(response.status_code, raw_data)\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/connection\/base.py\", line 329, in _raise_error\r\n    status_code, error_message, additional_info\r\n\r\n\r\nelasticsearch.exceptions.RequestError: RequestError(400, 'mapper_parsing_exception', 'Root mapping definition has unsupported parameters:  [schema : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [cluster : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [description : {analyzer=simple, type=text}] [display_name : {type=keyword}] [column_descriptions : {analyzer=simple, type=text}] [programmatic_descriptions : {analyzer=simple, type=text}] [tags : {type=keyword}] [badges : {type=keyword}] [database : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [total_usage : {type=long}] [name : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [last_updated_timestamp : {format=epoch_second, type=date}] [unique_usage : {type=long}] [column_names : {analyzer=simple, type=text, fields={raw={normalizer=column_names_normalizer, type=keyword}}}] [key : {type=keyword}]')->\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n* Amunsen version used: Databuilder: 6.7.1 Common 0.26.0 Amundsen-Gremlin 0.0.13 AWS ES : 6.8\r\n",
        "Challenge_closed_time":1649071896000,
        "Challenge_comment_count":7,
        "Challenge_created_time":1646313579000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"Neptune ML notebooks have a bug where the genre returned for a node classification task on `Toy Story` is incorrectly stated as `Comedy` instead of `Drama`.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1748",
        "Challenge_link_count":1,
        "Challenge_participation_count":7,
        "Challenge_readability":22.0,
        "Challenge_reading_time":43.86,
        "Challenge_repo_contributor_count":209.0,
        "Challenge_repo_fork_count":928.0,
        "Challenge_repo_issue_count":2115.0,
        "Challenge_repo_star_count":3927.0,
        "Challenge_repo_watch_count":244.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":766.1991666667,
        "Challenge_title":"Bug Report elasticsearch exception for sample_neptune_loader",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":216,
        "Discussion_body":"Thanks for opening your first issue here!\n This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.\n Same problem here, does you solved? @amandeep848 could you fix the problem? @amandeep848 could you fix the problem? Hello!\r\n\r\nI have been fixed the problem by putting the version of amundsen-common to 0.24.1\r\n\r\n- My `requirements.txt` file is setup as shown below:\r\n\r\n```text\r\namundsen-databuilder==6.5.2\r\namundsen-gremlin==0.0.13\r\ngremlinpython==3.4.10\r\nrequests-aws4auth==1.1.1\r\nboto3==1.21.23\r\nbotocore==1.24.23\r\ntyping-extensions==4.1.1\r\noverrides==6.1.0\r\namundsen-common==0.24.1\r\n```\r\n\r\n- My Glue databuilder script:\r\n\r\n```python\r\nimport logging\r\nimport os\r\nimport uuid\r\nimport boto3\r\nimport textwrap\r\nimport json\r\n\r\nfrom datetime import date\r\n\r\nfrom elasticsearch import Elasticsearch\r\nfrom pyhocon import ConfigFactory\r\n\r\nfrom databuilder.clients.neptune_client import NeptuneSessionClient\r\nfrom databuilder.extractor.es_last_updated_extractor import EsLastUpdatedExtractor\r\nfrom databuilder.extractor.neptune_search_data_extractor import NeptuneSearchDataExtractor\r\n\r\nfrom databuilder.job.job import DefaultJob\r\nfrom databuilder.loader.file_system_elasticsearch_json_loader import FSElasticsearchJSONLoader\r\nfrom databuilder.loader.file_system_neptune_csv_loader import FSNeptuneCSVLoader\r\nfrom databuilder.publisher.elasticsearch_constants import (\r\n    DASHBOARD_ELASTICSEARCH_INDEX_MAPPING, USER_ELASTICSEARCH_INDEX_MAPPING,\r\n)\r\nfrom databuilder.publisher.elasticsearch_publisher import ElasticsearchPublisher\r\nfrom databuilder.publisher.neptune_csv_publisher import NeptuneCSVPublisher\r\nfrom databuilder.task.task import DefaultTask\r\nfrom databuilder.transformer.base_transformer import ChainedTransformer, NoopTransformer\r\nfrom databuilder.transformer.dict_to_model import MODEL_CLASS, DictToModel\r\nfrom databuilder.transformer.generic_transformer import (\r\n    CALLBACK_FUNCTION, FIELD_NAME, GenericTransformer,\r\n)\r\n\r\nfrom databuilder.extractor.glue_extractor import GlueExtractor\r\nfrom databuilder.task.neptune_staleness_removal_task import NeptuneStalenessRemovalTask\r\n\r\n\r\nes_host = os.getenv('ES_HOST')\r\n\r\nneptune_host = os.getenv('NEPTUNE_HOST')\r\nneptune_port = os.getenv('NEPTUNE_PORT', 8182)\r\nneptune_iam_role_name = os.getenv('NEPTUNE_IAM_ROLE')\r\n\r\nS3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME')\r\ntoday = date.today()\r\nS3_DATA_PATH = f'amundsen_data\/glue_extractor\/year={today.year}\/month={today.month}\/day={today.day}'\r\n\r\nAWS_REGION = os.getenv('AWS_REGION')\r\nGLUE_DATABASE_IDENTIFIER = os.getenv('GLUE_DATABASE_IDENTIFIER')\r\n\r\nes = Elasticsearch(\r\n    '{}'.format(es_host),\r\n    scheme=\"https\",\r\n    port=443,\r\n)\r\n\r\nNEPTUNE_ENDPOINT = '{}:{}'.format(neptune_host, neptune_port)\r\n\r\nLOGGER = logging.getLogger(__name__)\r\n\r\n\r\ndef run_glue_job(job_name):\r\n    \"\"\"Run Glue metadata extraction\r\n\r\n    Args:\r\n        job_name (string): job name\r\n    \"\"\"\r\n\r\n    tmp_folder = '\/var\/tmp\/amundsen\/{job_name}'.format(job_name=job_name)\r\n    node_files_folder = '{tmp_folder}\/nodes'.format(tmp_folder=tmp_folder)\r\n    relationship_files_folder = '{tmp_folder}\/relationships'.format(tmp_folder=tmp_folder)\r\n\r\n    loader = FSNeptuneCSVLoader()\r\n    publisher = NeptuneCSVPublisher()\r\n\r\n    with open(\"databases.json\") as jsonFile:\r\n\r\n        filters = json.load(jsonFile)\r\n\r\n    job_config = ConfigFactory.from_dict({\r\n        f'extractor.glue.{GlueExtractor.CLUSTER_KEY}': GLUE_DATABASE_IDENTIFIER,\r\n        f'extractor.glue.{GlueExtractor.FILTER_KEY}': filters,\r\n        loader.get_scope(): {\r\n            FSNeptuneCSVLoader.NODE_DIR_PATH: node_files_folder,\r\n            FSNeptuneCSVLoader.RELATION_DIR_PATH: relationship_files_folder,\r\n            FSNeptuneCSVLoader.SHOULD_DELETE_CREATED_DIR: True,\r\n            FSNeptuneCSVLoader.JOB_PUBLISHER_TAG: 'unique_tag'\r\n        },\r\n        publisher.get_scope(): {\r\n            NeptuneCSVPublisher.NODE_FILES_DIR: node_files_folder,\r\n            NeptuneCSVPublisher.RELATION_FILES_DIR: relationship_files_folder,\r\n            NeptuneCSVPublisher.AWS_S3_BUCKET_NAME: S3_BUCKET_NAME,\r\n            NeptuneCSVPublisher.AWS_BASE_S3_DATA_PATH: S3_DATA_PATH,\r\n            NeptuneCSVPublisher.NEPTUNE_HOST: NEPTUNE_ENDPOINT,\r\n            NeptuneCSVPublisher.AWS_IAM_ROLE_NAME: neptune_iam_role_name,\r\n            NeptuneCSVPublisher.AWS_REGION: AWS_REGION\r\n        },\r\n    })\r\n\r\n    DefaultJob(\r\n        conf=job_config,\r\n        task=DefaultTask(\r\n            extractor=GlueExtractor(),\r\n            loader=loader,\r\n            transformer=NoopTransformer()\r\n        ),\r\n        publisher=publisher\r\n    ).launch()\r\n\r\ndef create_remove_stale_data_job():\r\n    \"\"\"Run remove stale data from Neptune\r\n\r\n    Returns:\r\n        NeptuneStalenessRemovalTask: Neptune stateleness data job\r\n    \"\"\"\r\n\r\n    target_relations = ['DESCRIPTION', 'DESCRIPTION_OF', 'COLUMN', 'COLUMN_OF', 'TABLE', 'TABLE_OF']\r\n    target_nodes = ['Table', 'Column', 'Programmatic_Description', \"Schema\"]\r\n\r\n    staleness_max_pct = 5\r\n\r\n    while True:\r\n\r\n        try:\r\n\r\n            LOGGER.info(f'Delete stale data at threshold - {staleness_max_pct}%')\r\n\r\n            job_config = ConfigFactory.from_dict({\r\n                'task.remove_stale_data': {\r\n                    NeptuneStalenessRemovalTask.TARGET_RELATIONS: target_relations,\r\n                    NeptuneStalenessRemovalTask.TARGET_NODES: target_nodes,\r\n                    NeptuneStalenessRemovalTask.STALENESS_CUT_OFF_IN_SECONDS: 86400,  # 1 day\r\n                    NeptuneStalenessRemovalTask.STALENESS_MAX_PCT: staleness_max_pct,\r\n                    'neptune.client': {\r\n                        NeptuneSessionClient.NEPTUNE_HOST_NAME: NEPTUNE_ENDPOINT,\r\n                        NeptuneSessionClient.AWS_REGION: AWS_REGION,\r\n                    }\r\n                }\r\n            })\r\n\r\n            job = DefaultJob(\r\n                conf=job_config,\r\n                task=NeptuneStalenessRemovalTask()\r\n            )\r\n\r\n            job.launch()\r\n\r\n            break\r\n\r\n        except Exception as ex:\r\n\r\n            LOGGER.error(ex)\r\n            LOGGER.info(f'Increase stale data threshold')\r\n\r\n            staleness_max_pct += 5\r\n\r\n            if staleness_max_pct == 105:\r\n\r\n                break\r\n\r\n\r\ndef create_es_publisher_job(elasticsearch_index_alias='table_search_index',\r\n                            elasticsearch_doc_type_key='table',\r\n                            model_name='databuilder.models.table_elasticsearch_document.TableESDocument',\r\n                            entity_type='table',\r\n                            elasticsearch_mapping=None):\r\n    \"\"\"\r\n    :param elasticsearch_index_alias:  alias for Elasticsearch used in\r\n                                       amundsensearchlibrary\/search_service\/config.py as an index\r\n    :param elasticsearch_doc_type_key: name the ElasticSearch index is prepended with. Defaults to `table` resulting in\r\n                                       `table_{uuid}`\r\n    :param model_name:                 the Databuilder model class used in transporting between Extractor and Loader\r\n    :param entity_type:                Entity type handed to the `Neo4jSearchDataExtractor` class, used to determine\r\n                                       Cypher query to extract data from Neo4j. Defaults to `table`.\r\n    :param elasticsearch_mapping:      Elasticsearch field mapping \"DDL\" handed to the `ElasticsearchPublisher` class,\r\n                                       if None is given (default) it uses the `Table` query baked into the Publisher\r\n    \"\"\"\r\n    # loader saves data to this location and publisher reads it from here\r\n    extracted_search_data_path = '\/var\/tmp\/amundsen\/search_data.json'\r\n    loader = FSElasticsearchJSONLoader()\r\n    extractor = NeptuneSearchDataExtractor()\r\n\r\n    task = DefaultTask(\r\n        loader=loader,\r\n        extractor=extractor,\r\n        transformer=NoopTransformer()\r\n    )\r\n\r\n    # elastic search client instance\r\n    elasticsearch_client = es\r\n\r\n    # unique name of new index in Elasticsearch\r\n    elasticsearch_new_index_key = '{}_'.format(elasticsearch_doc_type_key) + str(uuid.uuid4())\r\n\r\n    publisher = ElasticsearchPublisher()\r\n\r\n    session = boto3.Session(region_name=AWS_REGION)\r\n\r\n    aws_creds = session.get_credentials()\r\n    aws_access_key = aws_creds.access_key\r\n    aws_access_secret = aws_creds.secret_key\r\n    aws_token = aws_creds.token\r\n\r\n    job_config = ConfigFactory.from_dict({\r\n        extractor.get_scope(): {\r\n            NeptuneSearchDataExtractor.ENTITY_TYPE_CONFIG_KEY: entity_type,\r\n            NeptuneSearchDataExtractor.MODEL_CLASS_CONFIG_KEY: model_name,\r\n            'neptune.client': {\r\n                NeptuneSessionClient.NEPTUNE_HOST_NAME: NEPTUNE_ENDPOINT,\r\n                NeptuneSessionClient.AWS_REGION: AWS_REGION,\r\n                NeptuneSessionClient.AWS_ACCESS_KEY: aws_access_key,\r\n                NeptuneSessionClient.AWS_SECRET_ACCESS_KEY: aws_access_secret,\r\n                NeptuneSessionClient.AWS_SESSION_TOKEN: aws_token\r\n            }\r\n        },\r\n        'loader.filesystem.elasticsearch.file_path': extracted_search_data_path,\r\n        'loader.filesystem.elasticsearch.mode': 'w',\r\n        publisher.get_scope(): {\r\n            'file_path': extracted_search_data_path,\r\n            'mode': 'r',\r\n            'client': elasticsearch_client,\r\n            'new_index': elasticsearch_new_index_key,\r\n            'doc_type': elasticsearch_doc_type_key,\r\n            'alias': elasticsearch_index_alias\r\n        }\r\n    })\r\n\r\n    # only optionally add these keys, so need to dynamically `put` them\r\n    if elasticsearch_mapping:\r\n        job_config.put('publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_MAPPING_CONFIG_KEY),\r\n                       elasticsearch_mapping)\r\n\r\n    job = DefaultJob(\r\n        conf=job_config,\r\n        task=task,\r\n        publisher=ElasticsearchPublisher()\r\n    )\r\n\r\n    return job\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    logging.basicConfig(level=logging.INFO)\r\n\r\n    LOGGER.info('ES Host: ' +  es_host)\r\n    LOGGER.info('Neptune Host: ' + neptune_host)\r\n    LOGGER.info('Neptune Port: ' + str(neptune_port))\r\n    LOGGER.info('Neptune IAM Role Name: ' + neptune_iam_role_name)\r\n    LOGGER.info('S3 Bucket Name: ' + S3_BUCKET_NAME)\r\n    LOGGER.info('S3 Data Path: ' + S3_DATA_PATH)\r\n    LOGGER.info('AWS Region: ' + AWS_REGION)\r\n\r\n    logging.info('>>> Running Remove Stale Data Job <<<')\r\n\r\n    create_remove_stale_data_job()\r\n\r\n    logging.info('>>> Running Glue Extractor <<<')\r\n\r\n    run_glue_job('amundsen_glue_extractor')\r\n\r\n    logging.info('>>> Running ES Publisher <<<')\r\n\r\n    job_es_table = create_es_publisher_job(\r\n        elasticsearch_index_alias='table_search_index',\r\n        elasticsearch_doc_type_key='table',\r\n        entity_type='table',\r\n        model_name='databuilder.models.table_elasticsearch_document.TableESDocument'\r\n    )\r\n    job_es_table.launch()\r\n```\r\n\r\n- databases.json\r\n\r\n```json\r\n[]\r\n```\r\n\r\n- .env\r\n\r\n```env\r\nES_HOST=<ES_HOST>\r\nNEPTUNE_HOST=<NEPTUNE_HOST>\r\nNEPTUNE_PORT=8182\r\nNEPTUNE_IAM_ROLE=<NEPTUNE_IAM_ROLE>\r\nS3_BUCKET_NAME=<S3_BUCKET_NAME>\r\nAWS_REGION=<AWS_REGION>\r\nSECRET_NAME=<SECRET_NAME>\r\nGLUE_DATABASE_IDENTIFIER=<GLUE_DATABASE_IDENTIFIER>\r\n```\r\n\r\n\r\nHope this help!\r\n\r\nBest Regards.\r\nBill\r\n I encountered this issue, too, as I installed data builder from codebase with `python setup.py install`, and after rebase with the main branch, the previous version was not clean up when we simply rerun `python setup.py install`, the way out was to do `pip uninstall amundsen-databuilder` and `pip uninstall amundsen-common` until non of packages existed(there could be multiple versions left, more than once per each package could be required).\r\n\r\nThen the expected elastic-related code is up to date w\/o this error anymore.",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":85.8187694445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm currently trying to implement MLFlow Tracking into my training pipeline and would like to log the hyperparameters of my hyperparameter Tuning of each training job.<\/p>\n\n<p>Does anyone know, how to pull the list of hyperparameters that can be seen on the sagemaker training job interface (on the AWS console)? Is there any other smarter way to list how models perform in comparison in Sagemaker (and displayed)?<\/p>\n\n<p>I would assume there must be an easy and Pythonic way to do this (either boto3 or the sagemaker api) to get this data. I wasn't able to find it in Cloudwatch.<\/p>\n\n<p>Many thanks in advance!<\/p>",
        "Challenge_closed_time":1592173650467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591864702897,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to implement MLFlow Tracking into their training pipeline and is looking for a way to pull the list of hyperparameters used in each training job in Sagemaker. They are seeking a Pythonic way to get this data through either boto3 or the Sagemaker API, but have not been able to find it in Cloudwatch.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62320331",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":8.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":85.8187694445,
        "Challenge_title":"Sagemaker API to list Hyperparameters",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":484.0,
        "Challenge_word_count":107,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1469720117216,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>there is indeed a rather pythonic way in the SageMaker python SDK:<\/p>\n\n<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')\n\nresults = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!\n<\/code><\/pre>\n\n<p>See full example here <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":33.1,
        "Solution_reading_time":7.83,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":34.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":16.7675258334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently in the process of setting up a custom sagemaker container to run training jobs on Sagemaker and have succeeded in doing so. However, I am a bit confused over this question which is currently bugging me and is definitely something that I need to consider in the future<\/p>\n<ol>\n<li>Is it possible to run custom scripts on a custom container when declaring a sagemaker training job?<\/li>\n<\/ol>\n<p>My current understanding when it comes to creating a custom sagemaker image is that I create a train file that gets executed when running a training job, but I could never find documentation on whether is it possible to overwrite this and run a training script (but using the same custom container), like how we run training jobs using in-built algorithms. Is it the case that for custom algorithms we are restricted by this limitation?<\/p>",
        "Challenge_closed_time":1656147044190,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656086681097,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has successfully set up a custom sagemaker container to run training jobs on Sagemaker, but is unsure if it is possible to run custom scripts on a custom container when declaring a sagemaker training job. They are confused about whether it is possible to overwrite the train file and run a training script using the same custom container.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72746701",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.5,
        "Challenge_reading_time":11.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":16.7675258334,
        "Challenge_title":"Running custom scripts in a custom container while running a sagemaker training job",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":26.0,
        "Challenge_word_count":157,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644981356940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I don't fully understand your question (can you make an example please?), specially when you say<\/p>\n<blockquote>\n<p>like how we run training jobs using in-built algorithms<\/p>\n<\/blockquote>\n<p>But basically you can do whatever you want in your container, as probably you already did, you have a proper <code>train<\/code> file in your container, which is the one that sagemaker calls as the entrypoint. In that file you can call external script (which are in your container too) and also pass parameters to your container (see how for example hyperparameters are passed). There is a quite clear documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/docker-containers-create.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":9.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":100.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1588516515763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"UK",
        "Answerer_reputation_count":29087.0,
        "Answerer_view_count":3080.0,
        "Challenge_adjusted_solved_time":0.0864069444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm experimenting with <a href=\"https:\/\/aws.amazon.com\/sagemaker\/\" rel=\"nofollow noreferrer\">AWS Sagemaker<\/a> using a Free Tier account. According to the <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">Sagemaker pricing<\/a>, I can use 50 hours of m4.xlarge and m5.xlarge instances for training in the free tier. (I am safely within the two-month limit.) But when I attempt to train an algorithm with the XGBoost container using m5.xlarge, I get the error shown below the code.<\/p>\n<p>Are the ml-type and non-ml-type instances the same with just a fancy prefix for those that one would use with Sagemaker or are they entirely different? The <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/\" rel=\"nofollow noreferrer\">EC2 page<\/a> doesn't even list the ml instances.<\/p>\n<pre><code>sess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(container,\n                                    role, \n                                    instance_count=1, \n                                    instance_type='m5.xlarge',\n                                    output_path=output_location,\n                                    sagemaker_session=sess)\n<\/code><\/pre>\n<blockquote>\n<p>ClientError: An error occurred (ValidationException) when calling the\nCreateTrainingJob operation: 1 validation error detected: Value\n'm5.xlarge' at 'resourceConfig.instanceType' failed to satisfy\nconstraint: Member must satisfy enum value set: [ml.p2.xlarge,\nml.m5.4xlarge, ml.m4.16xlarge, ml.p4d.24xlarge, ml.c5n.xlarge,\nml.p3.16xlarge, ml.m5.large, ml.p2.16xlarge, ml.c4.2xlarge,\nml.c5.2xlarge, ml.c4.4xlarge, ml.c5.4xlarge, ml.c5n.18xlarge,\nml.g4dn.xlarge, ml.g4dn.12xlarge, ml.c4.8xlarge, ml.g4dn.2xlarge,\nml.c5.9xlarge, ml.g4dn.4xlarge, ml.c5.xlarge, ml.g4dn.16xlarge,\nml.c4.xlarge, ml.g4dn.8xlarge, ml.c5n.2xlarge, ml.c5n.4xlarge,\nml.c5.18xlarge, ml.p3dn.24xlarge, ml.p3.2xlarge, ml.m5.xlarge,\nml.m4.10xlarge, ml.c5n.9xlarge, ml.m5.12xlarge, ml.m4.xlarge,\nml.m5.24xlarge, ml.m4.2xlarge, ml.p2.8xlarge, ml.m5.2xlarge,\nml.p3.8xlarge, ml.m4.4xlarge]<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1607865492452,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607865181387,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to use an m5.xlarge instance for training an algorithm with the XGBoost container in AWS Sagemaker using a Free Tier account. However, the user is encountering an error that indicates that the instance type is not valid. The user is questioning whether the ml-type and non-ml-type instances are the same or different, as the EC2 page does not list the ml instances.",
        "Challenge_last_edit_time":1608039265823,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65276017",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":11.9,
        "Challenge_reading_time":25.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":0.0864069444,
        "Challenge_title":"What's the difference between regular and ml AWS EC2 instances?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":7690.0,
        "Challenge_word_count":192,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1493109317327,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1152.0,
        "Poster_view_count":113.0,
        "Solution_body":"<p>The instances with the <code>ml<\/code> prefix are instance classes specifically for use in Sagemaker.<\/p>\n<p>In addition to being used within the Sagemaker service, the instance will be running an AMI with all the necessary libraries and packages such as Jupyter.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.39,
        "Solution_score_count":12.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1475.5611111111,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nKedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). We claim to support both, but the CLI commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## Steps to Reproduce\r\n\r\nCall ``kedro mlflow init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## Expected Result\r\n\r\nThe cli commands should be available (``init``)\r\n\r\n## Actual Result\r\nOnly the ``new`` command is available. This is not considered as a kedro project.\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): kedro==16.6, kedro-mlflow==0.4.1\r\n* Python version used (`python -V`): 3.7.9\r\n* Operating system and version: Windows 7\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nThe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Challenge_closed_time":1615716614000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1610404594000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering issues with kedro-viz and kedro pipeline list when using PipelineML objects in hooks.py with kedro template>=0.16.5. The kedro run command works fine, but the visualization commands fail. The user has tried different versions of kedro-viz and kedro but the issue persists. The potential solution is to implement the __add__ method of the PipelineML class.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/157",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":14.22,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":21.0,
        "Challenge_repo_issue_count":414.0,
        "Challenge_repo_star_count":145.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1475.5611111111,
        "Challenge_title":"kedro mlflow cli is broken if configuration is declared in pyproject.toml",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":167,
        "Discussion_body":"This will wait the migration to `kedro>=0.17.0` (cf. #144) in milestone 0.6.0 because kedro has bradnd new utilities to handle this part. This will remove boilerplate code from the plugin and ensure consistency with future kedro changes.",
        "Discussion_score_count":2.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":347.8353516667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello All,  <\/p>\n<p>How to pass a Datapath as a parameter in Azure ML Pipeline activity?   <\/p>\n<p>More details here : Have opened an issue here : <a href=\"https:\/\/github.com\/Azure\/Azure-DataFactory\/issues\/216\">https:\/\/github.com\/Azure\/Azure-DataFactory\/issues\/216<\/a>  <\/p>\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1601023399256,
        "Challenge_comment_count":4,
        "Challenge_created_time":1599771191990,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking help on how to pass a Datapath as a parameter in Azure ML Pipeline activity and has opened an issue on GitHub for assistance.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/91785\/azure-data-factory-how-to-pass-datapath-as-a-param",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":11.0,
        "Challenge_reading_time":4.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":347.8353516667,
        "Challenge_title":"Azure Data Factory : How to pass DataPath as a parameter to Azure ML Pipeline activity?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":43,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thanks <a href=\"\/users\/na\/?userid=1e1dfdb6-a824-42dc-8b1c-8e2c3f669d59\">@Sriram Narayanan  <\/a> for your patience!    <\/p>\n<p>I discussed with the Product team and they confirmed that there is no datatype supported for &quot;DataPath&quot; parameter today in Azure Data Factory(ADF). However, there is a feature already raised for the same and work is in progress for it.     <\/p>\n<p>I would recommend you also to submit an idea in <a href=\"https:\/\/feedback.azure.com\/forums\/270578-data-factory\">feedback forum<\/a>. The ideas in this forum are closely monitored by data factory product team and will prioritize implementing them in future releases.    <\/p>\n<p>Sorry for the inconvenience!     <\/p>\n",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.5,
        "Solution_reading_time":8.76,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6197222222,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to create simple templates for scientists so that they can fit their models easily into a continuous integration\/continuous delivery (CI\/CD) pipeline. I want to know about success stories of AWS customers performing CI\/CD on machine learning pipelines. ",
        "Challenge_closed_time":1592579742000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592577511000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking success stories of AWS customers who have implemented CI\/CD on machine learning pipelines to create templates for scientists to easily fit their models into a CI\/CD pipeline.",
        "Challenge_last_edit_time":1667926686451,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUwLq6HNRZSOK7ODKKc_lC3Q\/can-you-share-success-stories-of-aws-customers-performing-ml-ci-cd",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.6197222222,
        "Challenge_title":"Can you share success stories of AWS customers performing ML CI\/CD?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":49,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Amazon has released the [Amazon SageMaker Pipelines][1] that are the first purpose-built CI\/CD service for machine learning: \n[1]: https:\/\/aws.amazon.com\/sagemaker\/pipelines\/\n\nFor more information, see [New \u2013 Amazon SageMaker Pipelines brings DevOps capabilities to your machine learning projects] [2]\n[2]: https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-pipelines-brings-devops-to-machine-learning-projects\/\n\nAdditionally, we have a case-study where a customer created one on their own for model development using Airflow.  For more information, see [NerdWallet uses machine learning on AWS to power recommendations platform][3] and \n[Using Amazon SageMaker to build a machine learning platform with just three engineers][4].\n[3]: https:\/\/aws.amazon.com\/solutions\/case-studies\/nerdwallet-case-study\/\n[4]: https:\/\/www.nerdwallet.com\/blog\/engineering\/machine-learning-platform-amazon-sagemaker\/\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925550032,
        "Solution_link_count":4.0,
        "Solution_readability":20.8,
        "Solution_reading_time":11.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.09,
        "Challenge_answer_count":0,
        "Challenge_body":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel.\r\n\r\n## Expected behavior\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n- Enchanter version: 0.7.0\r\n- Python version: 3.6.6\r\n- OS: Ubuntu 18.04\r\n- (Optional) Other libraries and their versions:\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## Reproducible examples (optional)\r\n\r\n```python\r\n# python code\r\n```\r\n\r\n## Additional context (optional)\r\n\r\n<!-- Please add any other context or screenshots about the problem here. -->",
        "Challenge_closed_time":1600309841000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1600305917000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering issues with the pipeline `sentence_embedding\/dvc.yaml` as it is not correctly defined for `evaluation:deps`. This is causing problems with pulling the model `biobert_nli_sts_cord19_v1` and running the training stage before the evaluation stage for the models `tf_idf\/` and `count\/`. The user is unable to run `dvc pull -d` and `dvc repro -f` without errors about missing files.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/khirotaka\/enchanter\/issues\/132",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":8.81,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":211.0,
        "Challenge_repo_star_count":7.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":1.09,
        "Challenge_title":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel.",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":96,
        "Discussion_body":"Issue-Label Bot is automatically applying the label `bug` to this issue, with a confidence of 0.68. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \n\n Links: [app homepage](https:\/\/github.com\/marketplace\/issue-label-bot), [dashboard](https:\/\/mlbot.net\/data\/khirotaka\/enchanter) and [code](https:\/\/github.com\/hamelsmu\/MLapp) for this bot.",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1288806614312,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dallas, TX, United States",
        "Answerer_reputation_count":14103.0,
        "Answerer_view_count":2018.0,
        "Challenge_adjusted_solved_time":11.6636425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to execute a R script every time an azure function is triggered. The R script executes perfectly on Azure machine learning Studio. But I am failing to execute through azure function.\nIs there any way to execute it?<\/p>",
        "Challenge_closed_time":1612564671216,
        "Challenge_comment_count":1,
        "Challenge_created_time":1612522682103,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having difficulty executing an R script through an Azure function, despite it working properly on Azure machine learning Studio. They are seeking a solution to execute the script through the function.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66062015",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.6,
        "Challenge_reading_time":3.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":11.6636425,
        "Challenge_title":"Executing R script from Azure function",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":640.0,
        "Challenge_word_count":45,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1475153705707,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":79.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>AFAIK you'll have to create your own Runtime as <code>R<\/code> isn't supported natively.<\/p>\n<p>Have you already tried <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/functions-create-function-linux-custom-image?tabs=bash%2Cportal&amp;pivots=programming-language-other\" rel=\"nofollow noreferrer\">&quot;Create a function on Linux using a custom container&quot;<\/a>? Interestingly they have given <code>R<\/code> as the example of custom runtime, so hopefully that answers your question.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.2,
        "Solution_reading_time":6.79,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":6.8229247222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to figure out how viable Azure ML in production; I would like to accomplish the following:<\/p>\n\n<ol>\n<li>Specify <em>custom environments<\/em> for my pipelines using a <em>pip file<\/em> and use them in a pipeline<\/li>\n<li><em>Declaratively<\/em> specify my workspace, environments and pipelines in an <em>Azure DevOps repo<\/em><\/li>\n<li><em>Reproducibly<\/em> deploy my Azure ML workspace to my subscription using an <em>Azure DevOps pipeline<\/em><\/li>\n<\/ol>\n\n<p>I found an <a href=\"https:\/\/stackoverflow.com\/questions\/60506398\/how-do-i-use-an-environment-in-an-ml-azure-pipeline\">explanation of how to specify environments using notebooks<\/a> but this seems ill-suited for the second and third requirements I have.<\/p>",
        "Challenge_closed_time":1583340584892,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583316022363,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to figure out how to version control Azure ML workspaces with custom environments and pipelines. They want to specify custom environments for their pipelines using a pip file, declaratively specify their workspace, environments, and pipelines in an Azure DevOps repo, and reproducibly deploy their Azure ML workspace to their subscription using an Azure DevOps pipeline. The user found an explanation of how to specify environments using notebooks, but it doesn't meet their requirements.",
        "Challenge_last_edit_time":1583348292107,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60523435",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.4,
        "Challenge_reading_time":10.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":6.8229247222,
        "Challenge_title":"How do I version control Azure ML workspaces with custom environments and pipelines?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":659.0,
        "Challenge_word_count":98,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1317398821727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1263.0,
        "Poster_view_count":44.0,
        "Solution_body":"<p>Currently, we have a python script, <code>pipeline.py<\/code> that uses the <code>azureml-sdk<\/code>to create, register and run all of our ML artifacts (envs, pipelines, models). We call this script in our Azure DevOps CI pipeline with a Python Script task after building the right pip env from the requirements file in our repo.<\/p>\n\n<p>However, it is worth noting there is YAML support for ML artifact definition. Though I don't know if the existing support will cover all of your bases (though that is the plan).<\/p>\n\n<p>Here's some great docs from MSFT to get you started:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/microsoft\/MLOpsPython\" rel=\"nofollow noreferrer\">GitHub Template repo of an end-to-end example of ML pipeline + deployment<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">How to define\/create an environment (using Pip or Conda) and use it in a remote compute context<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/targets\/azure-machine-learning?context=azure%2Fmachine-learning%2Fservice%2Fcontext%2Fml-context&amp;view=azure-devops&amp;tabs=yaml\" rel=\"nofollow noreferrer\">Azure Pipelines guidance on CI\/CD for ML Service<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-pipeline-yaml\" rel=\"nofollow noreferrer\">Defining ML pipelines in YAML<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1583340824232,
        "Solution_link_count":4.0,
        "Solution_readability":13.8,
        "Solution_reading_time":18.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":148.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":77.8924658334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I deployed my Training pipeline and my Real-time inference pipeline.  <br \/>\nWith the REST-Api of my training pipeline I'm able to retrain my ML model. Is it possible to use that retrained model automated in my real inference pipeline?  <br \/>\nWhen i trigger the pipeline in ML studio I have to update my real inference pipeline manually. Since I want to trigger my retraining external that is not possible.  <br \/>\nThanks in advance.<\/p>",
        "Challenge_closed_time":1615579149187,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615298736310,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has deployed a training pipeline and a real-time inference pipeline. They are able to retrain their ML model using the REST-API of the training pipeline, but are unable to automate the use of the retrained model in the real inference pipeline. They currently have to update the real inference pipeline manually and are seeking a solution to trigger the retraining externally.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/305899\/update-real-interference-pipeline",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.5,
        "Challenge_reading_time":5.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":77.8924658334,
        "Challenge_title":"update real interference pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":76,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, here's a reference on which <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-ml-pipelines#which-azure-pipeline-technology-should-i-use\">technology<\/a> to use based on a given scenario. For your scenario, you should be able to create an Azure Machine Learning pipeline using the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-trigger-published-pipeline\">SDK to trigger a pipeline<\/a> based on a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-trigger-published-pipeline#create-a-schedule\">time\/change based schedule<\/a> and then <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-update-web-service\">update the web service<\/a> accordingly. Depending on the complexity of your triggers or data prep needs, you can leverage other technologies such as <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-trigger-published-pipeline#use-azure-logic-apps-for-complex-triggers\">Logic Apps<\/a> or <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-trigger-published-pipeline#call-machine-learning-pipelines-from-azure-data-factory-pipelines\">Azure Data Factory<\/a> to trigger your Azure Machine Learning pipeline. Currently, you can only use the Azure Machine Learning SDK to automatically update the web service. Hope this helps.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":17.6,
        "Solution_reading_time":18.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":105.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.2836719444,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi there.<\/p>\n<p>We currently use wandb artifacts for model versioning during experiments. We\u2019d also like to integrate this into our production pipeline so that we can automatically pull specific model versions during builds.<\/p>\n<p>I am wondering if it\u2019s possible to get credentials that are not tied to a specific wandb user so that they don\u2019t expire if the team member that implements this happens to leave our company.<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1667922870256,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667918249037,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking a way to integrate wandb artifacts into their production pipeline and is looking for credentials that are not tied to a specific wandb user to avoid expiration if the team member implementing it leaves the company.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/ci-credentials-not-tied-to-user\/3388",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.5,
        "Challenge_reading_time":5.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.2836719444,
        "Challenge_title":"CI Credentials Not Tied to User",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":243.0,
        "Challenge_word_count":74,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/willjstone\">@willjstone<\/a> thank you for writing in! You can do this using a <code>service account<\/code>, the steps to add this account type to your team are explained in our documentation <a href=\"https:\/\/docs.wandb.ai\/guides\/technical-faq\/general#what-is-a-service-account-and-why-is-it-useful\">here<\/a>. Would this work for your use case?<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.4,
        "Solution_reading_time":5.01,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7538.9758333333,
        "Challenge_answer_count":0,
        "Challenge_body":"This guidance results in an error:\r\n\r\n\"To install the default packages in an environment without a previous version of the package installed, run the following command.\" \r\n\r\nPS C:\\> pip install azureml-sdk\r\n\r\n`ERROR: Could not find a version that satisfies the requirement azureml-sdk (from versions: none)\r\nERROR: No matching distribution found for azureml-sdk`\r\n\r\nWhat am I missing?\r\n\r\nThanks,\r\nclaw\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 8e0e12a4-b363-2726-06b4-9db2015efb32\r\n* Version Independent ID: e39a91ac-375b-a2cc-350d-a82cb7b0b035\r\n* Content: [Install the Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/install.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/install.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @harneetvirk\r\n* Microsoft Alias: **harnvir**",
        "Challenge_closed_time":1637097588000,
        "Challenge_comment_count":15,
        "Challenge_created_time":1609957275000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while running a PythonScriptStep in Azure ML SDK version 1.11.0. The error message states that \"azureml-train-automl-runtime\" is required but not installed in the current environment. The user has provided a RunConfiguration that includes the missing dependency, but the error persists. This issue is preventing the user from running any pipelines that were previously working.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1285",
        "Challenge_link_count":2,
        "Challenge_participation_count":15,
        "Challenge_readability":13.9,
        "Challenge_reading_time":14.88,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":7538.9758333333,
        "Challenge_title":"Error Installing Azureml. (Python 3.9 support)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":110,
        "Discussion_body":"@klawrawkz :  What is your OS? What is the python and pip version? \r\n\r\nazureml-sdk only supports Python 3.5 to 3.8. So, if you're using an out-of-range version of Python (older or newer), then you'll need to use a different version. Thanks for the reply @harneetvirk. I'm pretty sure it's not a python version issue.\r\n```\r\npy --version\r\nPython 3.9.1\r\n```\r\nCould be a Win 10 version issue?\r\n![image](https:\/\/user-images.githubusercontent.com\/48074223\/103943498-2f478c00-5100-11eb-9bfd-43443a4cb582.png)\r\n\r\nI ran this command and got farther. \r\n```\r\npip install --upgrade --upgrade-strategy eager azureml-sdk\r\n```\r\nI am stuck at this point now.\r\n```\r\n...\r\nINFO: pip is looking at multiple versions of azure-core to determine which version is compatible with other requirements. This could take a while.\r\nINFO: pip is looking at multiple versions of azure-mgmt-containerregistry to determine which version is compatible with other requirements. This could take a while.\r\nCollecting azure-mgmt-containerregistry>=2.0.0\r\n  Downloading azure_mgmt_containerregistry-2.7.0-py2.py3-none-any.whl (509 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 509 kB ...\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https:\/\/pip.pypa.io\/surveys\/backtracking\r\n  Downloading azure_mgmt_containerregistry-2.6.0-py2.py3-none-any.whl (501 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501 kB 1.6 MB\/s\r\nINFO: pip is looking at multiple versions of azure-mgmt-core to determine which version is compatible with other requirements. This could take a while.\r\n  Downloading azure_mgmt_containerregistry-2.5.0-py2.py3-none-any.whl (494 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 494 kB 6.4 MB\/s\r\n  Downloading azure_mgmt_containerregistry-2.4.0-py2.py3-none-any.whl (482 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 482 kB 6.4 MB\/s\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https:\/\/pip.pypa.io\/surveys\/backtracking\r\n  Downloading azure_mgmt_containerregistry-2.3.0-py2.py3-none-any.whl (481 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 481 kB 6.8 MB\/s\r\n```\r\n\r\nWhat's your advice on commands to provide \"stricter constraints to reduce runtime?\" The command (above) has been \"running\" for ~24 hours, so I'm guessing that it's dead in the H20.\r\n\r\nKlaw azureml-sdk only supports Python 3.5 to 3.8, but you are having python 3.9.1 installed in the environment.  Please change the python version between 3.5 to 3.8.\r\n\r\nAlso, the latest pip 20.3 has a new dependency resolver which is resulting in this long running dependency resolutions. If you switch to older version of pip (<20.3), you will notice the difference in the performance. Gotcha, thanks for the info. I'll make the change.\r\n\r\nKlaw If azureml-sdk does not support Python 3.9, then the metadata should be updated from:\r\n```\r\nRequires-Python: >=3.5,<4\r\n```\r\nto:\r\n```\r\nRequires-Python: >=3.5,<3.9\r\n```\r\nIs this also true for the hundreds of subpackages that azureml-sdk depends on? When is Python 3.9 support coming? when will azureml-core be compatible with python 3.9? I am currently using azureml-sdk under Python 3.9 by installing with pip's `--ignore-requires-python` option, and everything I am using seems to work fine. But there are probably some other parts that don't work... @johan12345 is this in production environment? you are using it like this? or in your local env? In my local development environment.  `azureml-core` now supports Python 3.9. unfortunately although `azureml-core` might install w\/o errors in 3.9, `azureml-sdk` still creates errors. Installed w\/o errors in 3.8.12   azureml-sdk is a meta package.  azureml-core is one of the upstream that supports python 3.9 but there are some other AutoML dependencies in azureml-sdk  which do not support python 3.9.\r\n I have just updated azureml-sdk to allow Python 3.9.\r\nThis should be included in the next Azure ML SDK release, 1.45.0. What about 3.10? 3.11 is coming out soon too. @adamjstewart Python 3.10 is already supported in the new SDK V2 preview: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-v2\r\nI expect that we will support 3.10 in SDK V1 as well but I don't have a date for that.",
        "Discussion_score_count":16.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":0.5489747222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I haven't used sagemaker for a while and today I started a training job (with the same old settings I always used before), but this time I noticed that a processing job has been automatically created and it's running while my training job run (I presume for debugging purpose).\nI'm sure that this is the first time that it happens.. Is that a new feature introduced by sagemaker? I didn't find any related in documentation, but it's important to know because I don't want extra costs..<\/p>\n<p>This is the image used by the processing job, with a instance type of <code>ml.m5.2xlarge<\/code> which I didn't set anywhere..<\/p>\n<blockquote>\n<p>929884845733.dkr.ecr.eu-west-1.amazonaws.com\/sagemaker-debugger-rules:latest<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1611573127772,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611569494177,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user started a training job on AWS Sagemaker and noticed that a processing job was automatically created and running alongside the training job, which they presume is for debugging purposes. The user is concerned about incurring extra costs and is unsure if this is a new feature introduced by Sagemaker as they did not find any related documentation. The processing job is using an image with an instance type of ml.m5.2xlarge, which the user did not set anywhere.",
        "Challenge_last_edit_time":1611571151463,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65882686",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.0093319444,
        "Challenge_title":"AWS Sagemaker processing Job automatically created?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":301.0,
        "Challenge_word_count":114,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416346350292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation_count":2302.0,
        "Poster_view_count":227.0,
        "Solution_body":"<p>I can answer my question.. it seems to be a new feature as highlighted <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/use-debugger-built-in-rules.html\" rel=\"nofollow noreferrer\">here<\/a>. You can turn it off as suggested in the doc:<\/p>\n<pre><code>To disable both monitoring and profiling, include the disable_profiler parameter to your estimator and set it to True. \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.6,
        "Solution_reading_time":5.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":47.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1305851487736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Challenge_adjusted_solved_time":80.7456638889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In my workflow, I do the following:<\/p>\n<ol>\n<li>Acquire raw data (e.g. a video containing people)<\/li>\n<li>Transform it (e.g. automatically extract all crops with faces)<\/li>\n<li>Manually label them (e.g. identify the person in each crop). The labels are stored in json files along with the crops.<\/li>\n<li>Train a model on these data.<\/li>\n<\/ol>\n<p><strong>How should I track this pipeline with DVC?<\/strong><\/p>\n<p>My concerns:<\/p>\n<ol>\n<li>If stage 2 is changed (e.g. crops are extracted with a different size), the manual data should be invalidated (and so should the final model).<\/li>\n<li>The 3rd step is manual and therefore not precisely reproducible. But I do need its input to be reproducible.<\/li>\n<li>Stage 4 has an element of randomness, so it's not precisely reproducible either.<\/li>\n<\/ol>",
        "Challenge_closed_time":1655523571800,
        "Challenge_comment_count":2,
        "Challenge_created_time":1655411665790,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to track a pipeline with DVC that involves acquiring raw data, transforming it, manually labeling it, and training a model. The user is concerned about how to handle changes in the pipeline, the manual labeling step, and the randomness in the final stage.",
        "Challenge_last_edit_time":1655415427550,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72651603",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.1,
        "Challenge_reading_time":10.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":31.0850027778,
        "Challenge_title":"Adding files that rely on pipeline outputs",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":128,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>Stage 3 is manual so you can't really codify it or automate it, nor guarantee its reproducibility (due to possible human error). But there's a way to get you as close as possible:<\/p>\n<p>You could replace it with a helper script that just checks whether all the labels are annotated. If so, output a text file with content &quot;green&quot;, otherwise &quot;red&quot; (for example) and error out.<\/p>\n<p>Stage 4 should depend on both the inputs from stages 2 and 3, so it will only run if BOTH the face crops changed AND if they are thoroughly annotated.\nInternally, it first checks the semaphore file (from 3) and dies on red. On green, it trains the model :)<\/p>\n<p>The <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">DAG<\/a> looks like this:<\/p>\n<pre><code>          +-----------+       \n          | 1-acquire |       \n          +-----------+       \n                *          \n                *          \n                *          \n          +---------+       \n          | 2-xform |       \n          +---------+       \n you      **        **     \n   --&gt;  **            **   \n       *                ** \n+---------+               *\n| 3-check |             ** \n+---------+           **   \n          **        **     \n            **    **       \n              *  *         \n          +---------+      \n          | 4-train |      \n          +---------+      \n<\/code><\/pre>\n<blockquote>\n<p>re randomness: while not ideal, non-determinism technically only <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#avoiding-unexpected-behavior\" rel=\"nofollow noreferrer\">affects intermediate stages<\/a> of the pipeline, because it causes everything after that to always run. In this case, since it's in the last stage, it won't affect DVC's job.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1655706111940,
        "Solution_link_count":2.0,
        "Solution_readability":12.0,
        "Solution_reading_time":17.8,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":174.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1409041899323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":324.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":4.6613055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a team where many member has permission to submit Spark tasks to YARN (the resource management) by command line. It's hard to track who is using how much cores, who is using how much memory...e.g. Now I'm looking for a software, framework or something could help me monitor the parameters that each member used. It will be a bridge between client and YARN. Then I could used it to filter the submit commands.<\/p>\n\n<p>I did take a look at <a href=\"http:\/\/www.mlflow.org\" rel=\"nofollow noreferrer\">mlflow<\/a> and I really like the MLFlow Tracking but it was designed for ML training process. I wonder if there is an alternative for my purpose? Or there is any other solution for the problem.<\/p>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1562766864887,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562750084187,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having difficulty tracking the parameters used by team members submitting Spark tasks to YARN via command line. They are looking for a software or framework to monitor the parameters used by each member and act as a bridge between the client and YARN. They have considered MLFlow Tracking but are looking for alternatives.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56967364",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":9.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":4.6613055556,
        "Challenge_title":"Keep track of all the parameters of spark-submit",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":93.0,
        "Challenge_word_count":128,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1413431014112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>My recommendation would be to build such a tool yourself as its not too complicated,\nhave a wrapper script to spark submit which logs the usage in a DB and after the spark job finishes the wrapper will know to release information. could be done really easily.\nIn addition you can even block new spark submits if your team already asked for too much information.<\/p>\n\n<p>And as you build it your self its really flexible as you can even create \"sub teams\" or anything you want.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":5.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":86.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.5115797222,
        "Challenge_answer_count":1,
        "Challenge_body":"I see Autosave Documents is checked in the Settings menu of my notebook, but I'm curious what the default interval is and if that's configurable?",
        "Challenge_closed_time":1673973257172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673960615485,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the auto-save interval on SageMaker Studio Notebooks and whether it is configurable. They have noticed that Autosave Documents is checked in the Settings menu but are unsure of the default interval.",
        "Challenge_last_edit_time":1674306814308,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUPNplairnRPWo3zA_otNpWA\/is-there-a-way-to-configure-the-auto-save-interval-on-sagemaker-studio-notebooks",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":3.5115797222,
        "Challenge_title":"Is there a way to configure the auto-save interval on SageMaker Studio Notebooks?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":38.0,
        "Challenge_word_count":37,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"SageMaker Studio uses the default JupyterLab configuration, i.e., auto save every 2 minutes. \n\nYou can modify it through the `%autosave` magic command, or use an extension. You can see a medium article with details [here](https:\/\/medium.com\/nabla-squared\/how-to-change-the-autosave-interval-in-jupyter-notebooks-2ab996fe4446).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1673973257172,
        "Solution_link_count":1.0,
        "Solution_readability":12.6,
        "Solution_reading_time":4.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":35.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":70.4102408333,
        "Challenge_answer_count":3,
        "Challenge_body":"based on the docs here https:\/\/github.com\/aws-samples\/sagemaker-pipelines-callback-step-for-batch-transform\/blob\/main\/batch_transform_with_callback.ipynb, a separate pipeline is created to perform a batch transform within sagemaker pipeline. the example utilizes a lambda and sqs to achieve this.  couldn't the batch transform job can simply be part of the training pipeline? once the model is trained, and added to model registry, one should be able to query the registry and get the latest model and run a batch transformation job on that, without the callback set up in the docs, right? any examples of running a batch transform job directly from a training pipeline?",
        "Challenge_closed_time":1671913943536,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671660466669,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to run a batch transform within a SageMaker pipeline and has found an example that uses a separate pipeline with a lambda and SQS. The user is questioning whether the batch transform job can be part of the training pipeline and if it's possible to run a batch transform job directly from a training pipeline without the need for a callback setup. The user is seeking examples of running a batch transform job directly from a training pipeline.",
        "Challenge_last_edit_time":1672008224227,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUFQCzo_y3TdiQ5iWO4sFR-Q\/how-to-run-an-inference-in-sagemaker-pipeline",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.2,
        "Challenge_reading_time":9.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":70.4102408333,
        "Challenge_title":"how to run an inference in sagemaker pipeline?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":459.0,
        "Challenge_word_count":102,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\n\nThe solution can be various and it depends on what you are trying to achieve. In general, I believe it is a good idea to build a generic pipeline and utilize parameters for different jobs. The image below shows a typical ML pattern with stages.\n\n\n![Enter image description here](\/media\/postImages\/original\/IMgAGV22YXQ16wVlctBVCnwg)\n\nYou can use condition steps to orchestrate Sagemaker jobs, more information with code examples are below:\n\nhttps:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker-pipelines\/tabular\/abalone_build_train_deploy\/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.html\n\nHope it helps,",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1671913943536,
        "Solution_link_count":1.0,
        "Solution_readability":15.8,
        "Solution_reading_time":8.21,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":249.1975413889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>I would like to know if there is the possibility to use a custom environment (created from the AML portal) for the execution of a Python Script Step in the Azure Machine Learning Designer (only using the designer, not using azureml sdk to publish the pipeline from the code).   <\/p>\n<p>Thanks,  <br \/>\nG<\/p>",
        "Challenge_closed_time":1648494342676,
        "Challenge_comment_count":2,
        "Challenge_created_time":1647597231527,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of using a custom environment created from the AML portal for executing a Python Script Step in the Azure Machine Learning Designer without using the azureml sdk to publish the pipeline from the code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/777745\/use-custom-environment-in-azure-machine-learning-d",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.4,
        "Challenge_reading_time":4.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":249.1975413889,
        "Challenge_title":"Use custom environment in Azure Machine Learning Designer",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":62,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thanks for your feedback. Based on your comments above, it seems you want to configure a custom environment in AML designer and install unsupported python libraries. These are the supported <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment\">Custom Environments<\/a>. However, in AML designer, the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-designer-python\">execute python script component<\/a> enables you to write custom python scripts and install python libraries. This particular <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/component-reference\/execute-python-script\">document<\/a> shows how to configure execute python script. You can install packages that aren't in the preinstalled list by using the following command:<\/p>\n<pre><code>import os  \nos.system(f&quot;pip install scikit-misc&quot;)  \n<\/code><\/pre>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.5,
        "Solution_reading_time":13.4,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":96.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1479159384132,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Illinois, United States",
        "Answerer_reputation_count":513.0,
        "Answerer_view_count":113.0,
        "Challenge_adjusted_solved_time":42.2916322222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I want to run a pipeline for different files, but some of them don't need all of the defined nodes. How can I pass them?<\/p>",
        "Challenge_closed_time":1573135218943,
        "Challenge_comment_count":2,
        "Challenge_created_time":1572974635783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to run a pipeline for different files but some files do not require all the defined nodes. They are seeking a solution to pass those files.",
        "Challenge_last_edit_time":1572982969067,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58716474",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":3.0,
        "Challenge_reading_time":2.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":44.6064333334,
        "Challenge_title":"How to run a pipeline except for a few nodes?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":859.0,
        "Challenge_word_count":34,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572973807452,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>To filter out a few lines of a pipeline you can simply filter the pipeline list from inside of python, my favorite way is to use a list comprehension.<\/p>\n\n<p><strong>by name<\/strong><\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>nodes_to_run = [node for node in pipeline.nodes if 'dont_run_me' not in node.name]\nrun(nodes_to_run, io)\n<\/code><\/pre>\n\n<p><strong>by tag<\/strong><\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>nodes_to_run = [node for node in pipeline.nodes if 'dont_run_tag' not in node.tags]\nrun(nodes_to_run, io)\n<\/code><\/pre>\n\n<p>It's possible to filter by any attribute tied to the pipeline node, (name, inputs, outputs, short_name, tags)<\/p>\n\n<p>If you need to run your pipeline this way in production or from the command line, you can either tag your pipeline to run with tags, or add a custom <code>click.option<\/code> to your <code>run<\/code> function inside of <code>kedro_cli.py<\/code> then run this filter when the flag is <code>True<\/code>.<\/p>\n\n<p><strong>Note<\/strong>\nThis assumes that you have your pipeline loaded into memory as <code>pipeline<\/code> and catalog loaded in as <code>io<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":14.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":148.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1604093818187,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Krakow, Poland",
        "Answerer_reputation_count":1200.0,
        "Answerer_view_count":263.0,
        "Challenge_adjusted_solved_time":3.6352397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run a simple Ada-boosted Decision Tree regressor on GCP Vertex AI. To parse hyperparams and other arguments I use Click for Python, a very simple CLI library. Here's the setup for my task function:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@click.command()\n@click.argument(&quot;input_path&quot;, type=str)\n@click.option(&quot;--output-path&quot;, type=str, envvar='AIP_MODEL_DIR')\n@click.option('--gcloud', is_flag=True, help='Run as if in Google Cloud Vertex AI Pipeline')\n@click.option('--grid', is_flag=True, help='Perform a grid search instead of a single run. Ignored with --gcloud')\n@click.option(&quot;--max_depth&quot;, type=int, default=4, help='Max depth of decision tree', show_default=True)\n@click.option(&quot;--n_estimators&quot;, type=int, default=50, help='Number of AdaBoost boosts', show_default=True)\ndef click_main(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators)\n\n\ndef train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    print(input_path, output_path, gcloud)\n    logger = logging.getLogger(__name__)\n    logger.info(&quot;training models from processed data&quot;)\n    ...\n<\/code><\/pre>\n<p>When I run it locally like below, Click correctly grabs the params both from console and environment and proceeds with model training (<code>AIP_MODEL_DIR<\/code> is <code>gs:\/\/(BUCKET_NAME)\/models<\/code>)<\/p>\n<pre><code>\u276f python3 -m src.models.train_model gs:\/\/(BUCKET_NAME)\/data\/processed --gcloud\n\ngs:\/\/(BUCKET_NAME)\/data\/processed gs:\/\/(BUCKET_NAME)\/models True\n\n<\/code><\/pre>\n<p>However, when I put this code on the Vertex AI Pipeline, it throws an error, namely<\/p>\n<pre><code>FileNotFoundError: b\/(BUCKET_NAME)\/o\/data%2Fprocessed%20%20--gcloud%2Fprocessed_features.csv\n<\/code><\/pre>\n<p>As it is clearly seen, Click grabs both the parameter and the <code>--gcloud<\/code> option and assigns it to <code>input_path<\/code>. The print statement before that confirms it, both by having one too many spaces and <code>--gcloud<\/code> being parsed as false.<\/p>\n<pre><code>gs:\/\/(BUCKET_NAME)\/data\/processed  --gcloud gs:\/\/(BUCKET_NAME)\/models\/1\/model\/ False\n<\/code><\/pre>\n<p>Has anyone here encountered this issue or have any idea how to solve it?<\/p>",
        "Challenge_closed_time":1641810988040,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641797901177,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Python Click library incorrectly parsing arguments when called in Vertex AI Pipeline. Click is not able to correctly grab the parameters and options, and assigns them to the wrong variables, causing an error in the pipeline. The user is seeking help to solve this issue.",
        "Challenge_last_edit_time":1641896853596,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70648776",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.5,
        "Challenge_reading_time":31.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":3.6352397222,
        "Challenge_title":"Python click incorrectly parses arguments when called in Vertex AI Pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":129.0,
        "Challenge_word_count":240,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1579801831103,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tempe, AZ, USA",
        "Poster_reputation_count":71.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p>I think is due the nature of <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/arguments\/?highlight=arguments\" rel=\"nofollow noreferrer\">arguments<\/a> and <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/options\/?highlight=options\" rel=\"nofollow noreferrer\">options<\/a>, you are mixing arguments and options although is not implicit stated in the documentation but argument will eat up the options that follow. If nargs is not allocated it will default to 1 considering everything after it follows as string which it looks like this is the case.<\/p>\n<blockquote>\n<p>nargs \u2013 the number of arguments to match. If not 1 the return value is a tuple instead of single value. The default for nargs is 1 (except if the type is a tuple, then it\u2019s the arity of the tuple).<\/p>\n<\/blockquote>\n<p>I think you should first use options followed by the argument as display on the <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/documentation\/?highlight=arguments\" rel=\"nofollow noreferrer\">documentation page<\/a>. Other approach is to group it under a command as show on this <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/commands\/\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.9,
        "Solution_reading_time":15.04,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":143.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":30.0475236111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was wondering if it was possible to delete particular runs using the Python SDK.   <br \/>\nthis would be rather useful to delete old failed runs.  <br \/>\nit already has functions such as cancel(), fail(), submit(). <\/p>",
        "Challenge_closed_time":1638494197368,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638386026283,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a way to delete specific runs in AzureML using the Python SDK, which would be helpful in removing old failed runs. The SDK currently has functions like cancel(), fail(), and submit().",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/648089\/is-there-a-way-to-delete-azureml-runs-using-the-py",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":3.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":30.0475236111,
        "Challenge_title":"is there a way to delete azureml runs using the python sdk?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":48,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=2dc066be-691a-47bd-9f7a-67e426d994d9\">@Antara Das  <\/a>  Thanks, Run history documents, which may contain personal user information, are stored in the storage account in blob storage, in subfolders of \/azureml. You can download and delete the data from the portal.    <\/p>\n<p> Here is the document to delete workspace data.     <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-export-delete-data\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-export-delete-data<\/a>    <\/p>\n<p>There is a Private Preview for deleting an experiment, however such functionality does not delete the intermediate data generated for the run or any child run.    <br \/>\n\u2022 Not deleted:    <br \/>\no Files in azureml-blobstore-GUID\/azureml\/{run_id}    <br \/>\no Code snapshot (zip files)    <br \/>\no Pipeline intermediate data and child runs    <br \/>\no Metric data    <\/p>\n<p>\u2022 Deleted    <br \/>\no The output folder content    <br \/>\no Log files<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.2,
        "Solution_reading_time":12.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":116.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":140.1573411111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an azureml studio with a notebook and suddenly since today, I cant run notebooks cells anymore.  It says kernel not connected.  <br \/>\nI cant either open the terminal it never loads.  <\/p>\n<p>I restarted the compute instance several time, but that didnt fix the problem  <\/p>",
        "Challenge_closed_time":1646131552128,
        "Challenge_comment_count":1,
        "Challenge_created_time":1645626985700,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to run notebook cells and open the terminal in their AzureML studio due to the kernel not being connected. Despite restarting the compute instance multiple times, the issue persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/747823\/kernel-not-connected",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.0,
        "Challenge_reading_time":3.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":140.1573411111,
        "Challenge_title":"Kernel not connected",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":50,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=0b725e52-0000-0003-0000-000000000000\">@Luis Valencia  <\/a>    <\/p>\n<p>There was an issue causing the &quot;kernal not connected&quot; issue, but it has been fixed. Please let us know if you are still blocked by this issue. Thanks a lot!    <\/p>\n<p>Regards,    <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.1,
        "Solution_reading_time":3.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1546942930440,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":18.6231941667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to follow the tutorial <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"noreferrer\">here<\/a> to implement a custom inference pipeline for feature preprocessing. It uses the python sklearn sdk to bring in custom preprocessing pipeline from a script. For example:<\/p>\n\n<pre><code>from sagemaker.sklearn.estimator import SKLearn\n\nscript_path = 'preprocessing.py'\n\nsklearn_preprocessor = SKLearn(\n    entry_point=script_path,\n    role=role,\n    train_instance_type=\"ml.c4.xlarge\",\n    sagemaker_session=sagemaker_session)\n<\/code><\/pre>\n\n<p>However I can't find a way to send multiple files. The reason I need multiple files is because I have a custom class used in the sklearn pipeline needs to be imported from a custom module. Without importing,  it raises error <code>AttributeError: module '__main__' has no attribute 'CustomClassName'<\/code> when having the custom class in the same preprocessing.py file due to the way pickle works (at least I think it's related to pickle). <\/p>\n\n<p>Anyone know if sending multiple files is even possible?<\/p>\n\n<p>Newbie to Sagemaker, thanks!!<\/p>",
        "Challenge_closed_time":1548251021872,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548183978373,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to implement a custom inference pipeline for feature preprocessing using the python sklearn sdk in AWS Sagemaker. However, they are unable to send multiple files, which is necessary as they have a custom class used in the sklearn pipeline that needs to be imported from a custom module. The user is seeking help to know if sending multiple files is possible.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54314876",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.3,
        "Challenge_reading_time":16.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":18.6231941667,
        "Challenge_title":"AWS Sagemaker SKlearn entry point allow multiple script",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":2019.0,
        "Challenge_word_count":142,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455047963123,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":85.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>There's a source_dir parameter which will \"lift\" a directory of files to the container and put it on your import path.<\/p>\n\n<p>You're entrypoint script should be put there to and referenced from that location.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":2.67,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.8236111111,
        "Challenge_answer_count":2,
        "Challenge_body":"What is the difference between SageMaker Pipelines and SageMaker Step Function SDK?\n\nThe [official Pipelines notebook][1] is basically only doing a workflow - pretty much a copy cat of what step functions has been doing for years. In the nice video from [Julien Simon][2] I see CICD capacities mentioned, where are those? any demos?\n\n\n  [1]: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-pipelines\/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.ipynb\n  [2]: https:\/\/www.youtube.com\/watch?v=Hvz2GGU3Z8g",
        "Challenge_closed_time":1607015065000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606994100000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the difference between SageMaker Pipelines and SageMaker Step Function SDK, particularly in regards to the CICD capabilities mentioned in a video by Julien Simon. The user notes that the official Pipelines notebook appears to only offer workflow capabilities similar to Step Functions and is looking for demos of the CICD features.",
        "Challenge_last_edit_time":1668621791288,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU2iheeTzhSTmWw4aqVEeqOQ\/what-is-the-difference-between-sagemaker-pipelines-and-sagemaker-step-function-sdk",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":13.3,
        "Challenge_reading_time":8.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":5.8236111111,
        "Challenge_title":"What is the difference between SageMaker Pipelines and SageMaker Step Function SDK?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1344.0,
        "Challenge_word_count":68,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey, that demo is missing the project part of Pipelines and therefore the SM provided project templates. Go to SM studio and on the Studio summary hit edit settings and then enable access and provisioning of Service Catalog Portfolio of products in SM Studio. Then check your service catalog portfolios.\nHaven't tried it out yet though.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1610011923648,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":4.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1412669622830,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":7.0974458333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using the SageMaker HuggingFace Processor to create a custom tokenizer on a large volume of text data.\nIs there a way to make this job data distributed - meaning read partitions of data across nodes and train the tokenizer leveraging multiple CPUs\/GPUs.<\/p>\n<p>At the moment, providing more nodes to the processing cluster merely replicates the tokenization process (basically duplicates the process of creation), which is redundant. You can primarily only scale vertically.<\/p>\n<p>Any insights into this?<\/p>",
        "Challenge_closed_time":1662653309532,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662621424647,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using SageMaker HuggingFace Processor to create a custom tokenizer on a large volume of text data. They are looking for a way to distribute the job data across nodes and train the tokenizer leveraging multiple CPUs\/GPUs, but adding more nodes to the processing cluster only duplicates the process of creation. The user is seeking insights on how to scale the process horizontally.",
        "Challenge_last_edit_time":1662627758727,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73645084",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":7.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":8.8569125,
        "Challenge_title":"Create Hugging Face Transformers Tokenizer using Amazon SageMaker in a distributed way",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":27.0,
        "Challenge_word_count":88,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1662621266503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":48.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Considering the following example code for\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-job-frameworks-hugging-face.html\" rel=\"nofollow noreferrer\">HuggingFaceProcessor<\/a>:<\/p>\n<p>If you have 100 large files in S3 and use a ProcessingInput with\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ProcessingS3Input.html#:%7E:text=S3DataDistributionType\" rel=\"nofollow noreferrer\">s3_data_distribution_type<\/a>=&quot;ShardedByS3Key&quot; (instead of FullyReplicated), the objects in your S3 prefix will be sharded and distributed to your instances.<\/p>\n<p>For example, if you have 100 large files and want to filter records from them using HuggingFace on 5 instances, the s3_data_distribution_type=&quot;ShardedByS3Key&quot; will put 20 objects on each instance, and each instance can read the files from its own path, filter out records, and write (uniquely named) files to the output paths, and SageMaker Processing will put the filtered files in S3.<\/p>\n<p>However, if your filtering criteria is stateful or depends on doing a full pass over the dataset first (such as: filtering outliers based on mean and standard deviation on a feature - in case of using SKLean Processor for example): you'll need to pass that information in to the job so each instance can know how to filter. To send information to the instances launched, you have to use the\u00a0<code>\/opt\/ml\/config\/resourceconfig.json<\/code>\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-your-own-processing-container.html#byoc-config\" rel=\"nofollow noreferrer\">file<\/a>:<\/p>\n<p><code>{ &quot;current_host&quot;: &quot;algo-1&quot;, &quot;hosts&quot;: [&quot;algo-1&quot;,&quot;algo-2&quot;,&quot;algo-3&quot;] }<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":17.1,
        "Solution_reading_time":22.86,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":186.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1601729162436,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":887.0,
        "Answerer_view_count":130.0,
        "Challenge_adjusted_solved_time":9.0815319445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>If we have an AzureML Pipeline published, how can we trigger it from Azure DevOps <strong>without using Python Script Step or Azure CLI Step<\/strong>?<\/p>\n<p>The AzureML Steps supported natively in Azure DevOps include Model_Deployment and Model_Profiling.<\/p>\n<p>Is there any step in Azure DevOps which can be used to directly trigger a published Azure Machine Learning Pipeline while maintaining capabilities like using Service Connections and passing environmental variables, Gated Release (Deployment)?<\/p>\n<p>Edit:\nThis process can then be used to run as an agentless job.<\/p>",
        "Challenge_closed_time":1612256282836,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612203126923,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking a way to trigger a published Azure Machine Learning Pipeline from Azure DevOps without using Python Script Step or Azure CLI Step. They are looking for a step in Azure DevOps that can directly trigger the pipeline while maintaining capabilities like using Service Connections and passing environmental variables, Gated Release (Deployment). The user also mentions that this process can be used to run as an agentless job.",
        "Challenge_last_edit_time":1612249107728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65997961",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":8.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":14.7655313889,
        "Challenge_title":"How to trigger an AzureML Pipeline from Azure DevOps?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1923.0,
        "Challenge_word_count":91,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>Assumptions:<\/p>\n<ol>\n<li>An AzureML Pipeline is published and the REST endpoint is ready- To be referred to in this answer as &lt;AML_PIPELINE_REST_URI&gt;. And Published Pipeline ID is also ready- To be referred to in this answer as &lt;AML_PIPELINE_ID&gt;<\/li>\n<li>You have the Azure Machine Learning Extension installed: <a href=\"https:\/\/marketplace.visualstudio.com\/items?itemName=ms-air-aiagility.vss-services-azureml&amp;ssr=false#review-details\" rel=\"nofollow noreferrer\">Azure Machine Learning Extension<\/a><\/li>\n<\/ol>\n<p>To Invoke the Azure Machine Learning Pipeline we use the <code>Invoke ML Pipeline<\/code> step available in Azure DevOps. It is available when running an Agentless Job.<\/p>\n<p>To trigger it the workflow is as follows:<\/p>\n<ol>\n<li>Create a New Pipeline. Using the Classic Editor, delete the default Agent Job 1 stage.\n<a href=\"https:\/\/i.stack.imgur.com\/phzL3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/phzL3.png\" alt=\"enter image description here\" \/><\/a><\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/QkiPY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QkiPY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ol start=\"2\">\n<li><p>Add an agentless job:\n<a href=\"https:\/\/i.stack.imgur.com\/0PXwg.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0PXwg.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Add a task to this Agentless Job:\n<a href=\"https:\/\/i.stack.imgur.com\/trW7j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/trW7j.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Use AzureML Published Pipeline Task:\n<a href=\"https:\/\/i.stack.imgur.com\/3rl4z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3rl4z.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Use the Service Connection Mapped to the AML Workspace. You can find more on this at the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/library\/service-endpoints?view=azure-devops&amp;tabs=yaml\" rel=\"nofollow noreferrer\">official documentation<\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/mnV36.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mnV36.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Choose the Pipeline to trigger using the &lt;AML_PIPELINE_ID&gt;:\n<a href=\"https:\/\/i.stack.imgur.com\/fbpQW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fbpQW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Give The experiment name and Pipeline Parameters if any:\n<a href=\"https:\/\/i.stack.imgur.com\/og1kx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/og1kx.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>That's it, you can Save and Queue:\n<a href=\"https:\/\/i.stack.imgur.com\/iCwdl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/iCwdl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<\/ol>\n<p>Alternatively, you can simply use the following jobs:<\/p>\n<pre><code>- job: Job_2\n  displayName: Agentless job\n  pool: server\n  steps:\n  - task: MLPublishedPipelineRestAPITask@0\n    displayName: Invoke ML pipeline\n    inputs:\n      connectedServiceName: &lt;REDACTED-AML-WS-Level-Service_Connection-ID&gt;\n      PipelineId: &lt;AML_PIPELINE_ID&gt;\n      ExperimentName: experimentname\n      PipelineParameters: ''\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1612281801243,
        "Solution_link_count":20.0,
        "Solution_readability":14.9,
        "Solution_reading_time":45.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":26.0,
        "Solution_word_count":299.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.30024,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Trying to download a report as latex causes an instrument.js error, and the waiting symbol turns forever. I use chrome on MacOS.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77.jpeg\" data-download-href=\"\/uploads\/short-url\/aV3jmQ0drwgzx2rJ9iyEpD7TJQj.jpeg?dl=1\" title=\"Bildschirmfoto 2023-02-13 um 17.42.14\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77_2_690x307.jpeg\" alt=\"Bildschirmfoto 2023-02-13 um 17.42.14\" data-base62-sha1=\"aV3jmQ0drwgzx2rJ9iyEpD7TJQj\" width=\"690\" height=\"307\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77_2_690x307.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77_2_1035x460.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77_2_1380x614.jpeg 2x\" data-dominant-color=\"959190\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Bildschirmfoto 2023-02-13 um 17.42.14<\/span><span class=\"informations\">1886\u00d7841 173 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1676311598268,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676306917404,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to download a report as latex, which causes an instrument.js error and the waiting symbol to turn forever. The user is using Chrome on MacOS.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/download-report-as-latex-causes-js-errors\/3872",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":24.9,
        "Challenge_reading_time":21.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.30024,
        "Challenge_title":"Download report as latex causes js errors",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":218.0,
        "Challenge_word_count":76,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Found a solution: When carefully loading each graph by scrolling slowly over the whole page, the download finally works.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":1.6,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":19.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":109.4256513889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was following the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-python-sdk\" rel=\"nofollow noreferrer\">SDK v2 Python tutorial<\/a> in order to create a pipeline job with my own assets. I notice that in this tutorial they let you use a csv file that can be downloaded but Im trying to use a registered dataset that I already registered by my own. The problem that I facing is that I dont know where I need to specify the dataset.<\/p>\n<p>The funny part is that at the beginning they create this dataset like this:<\/p>\n<pre><code>credit_data = ml_client.data.create_or_update(credit_data)\nprint(\n    f&quot;Dataset with name {credit_data.name} was registered to workspace, the dataset version is {credit_data.version}&quot;\n)\n<\/code><\/pre>\n<p>But the only part where they refer to this dataset is on the last part where they # the line:<\/p>\n<pre><code>registered_model_name = &quot;credit_defaults_model&quot;\n\n# Let's instantiate the pipeline with the parameters of our choice\npipeline = credit_defaults_pipeline(\n    # pipeline_job_data_input=credit_data,\n    pipeline_job_data_input=Input(type=&quot;uri_file&quot;, path=web_path),\n    pipeline_job_test_train_ratio=0.2,\n    pipeline_job_learning_rate=0.25,\n    pipeline_job_registered_model_name=registered_model_name,\n)\n<\/code><\/pre>\n<p>For me this means that I can use this data like this (a already registered dataset), the problem is that I don't know where I need to do the changes (I know that in the data_prep.py and in the code below but I don\u00b4t know where else) and I don't know how to set this:<\/p>\n<pre><code>%%writefile {data_prep_src_dir}\/data_prep.py\n...\n\ndef main():\n    &quot;&quot;&quot;Main function of the script.&quot;&quot;&quot;\n\n    # input and output arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(&quot;--data&quot;, type=str, help=&quot;path to input data&quot;) # &lt;=== Here, but I don\u00b4t know how\n    parser.add_argument(&quot;--test_train_ratio&quot;, type=float, required=False, default=0.25)\n    parser.add_argument(&quot;--train_data&quot;, type=str, help=&quot;path to train data&quot;)\n    parser.add_argument(&quot;--test_data&quot;, type=str, help=&quot;path to test data&quot;)\n    args = parser.parse_args()\n\n...\n<\/code><\/pre>\n<p>Does anyone have experience working as registered datasets?<\/p>",
        "Challenge_closed_time":1657082550892,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656688618547,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is following a tutorial to create a pipeline job with their own assets. They want to use a registered dataset that they have already created, but they are unsure where to specify the dataset. The tutorial only refers to the dataset in one part of the code, and the user is unsure where else to make changes. They are specifically looking for guidance on how to set the path to the input data in the data_prep.py file.",
        "Challenge_last_edit_time":1657482591390,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72831360",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":30.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":109.4256513889,
        "Challenge_title":"Use dataset registed in on pipelines in AML",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":85.0,
        "Challenge_word_count":259,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1636569000947,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":9.0,
        "Poster_view_count":2.0,
        "Solution_body":"<blockquote>\n<p>parser.add_argument(&quot;--data&quot;, type=str, help=&quot;path to input data&quot;) # &lt;=== Here, but I don\u00b4t know how<\/p>\n<\/blockquote>\n<p>To get the path to input data, according to <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/how-to-train-with-datasets.md\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n<ul>\n<li><p>You can get <code>--input-data<\/code> by ID which you can access in your training script.<\/p>\n<\/li>\n<li><p>Use it as <code>argument<\/code> on <code>mounted_input_path<\/code><\/p>\n<\/li>\n<\/ul>\n<p>For example, try the following three code snippets taken from the <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/how-to-train-with-datasets.md\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n<p><strong>Access dataset in training script:<\/strong><\/p>\n<pre><code>parser = argparse.ArgumentParser()\nparser.add_argument(&quot;--input-data&quot;, type=str)\nargs = parser.parse_args()\n\nrun = Run.get_context()\nws = run.experiment.workspace\n\n# get the input dataset by ID\ndataset = Dataset.get_by_id(ws, id=args.input_data)\n<\/code><\/pre>\n<p><strong>Configure the training run:<\/strong><\/p>\n<pre><code>src = ScriptRunConfig(source_directory=script_folder,\n                      script='train_titanic.py',\n                      # pass dataset as an input with friendly name 'titanic'\n                      arguments=['--input-data', titanic_ds.as_named_input('titanic')],\n                      compute_target=compute_target,\n                      environment=myenv)\n<\/code><\/pre>\n<p><strong>Pass <code>mounted_input_path<\/code> as argument:<\/strong><\/p>\n<pre><code>mounted_input_path = sys.argv[1]\nmounted_output_path = sys.argv[2]\n\nprint(&quot;Argument 1: %s&quot; % mounted_input_path)\nprint(&quot;Argument 2: %s&quot; % mounted_output_path)\n<\/code><\/pre>\n<p>References: <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/v1\/how-to-create-register-datasets.md\" rel=\"nofollow noreferrer\">How to create register dataset<\/a> and <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/datasets-tutorial\/scriptrun-with-data-input-output\/how-to-use-scriptrun.ipynb\" rel=\"nofollow noreferrer\">How to use configure a training run with data input and output<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":21.5,
        "Solution_reading_time":30.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":155.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":1014.9057013889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a training job yesterday, same as usual, just adding few more training data. I didn't have any problem with this in the last 2 years (the same exact procedure and code). This time after 14 hours more or less simply stalled.\nTraining job is still &quot;in processing&quot;, but cloudwatch is not logging anything since then. Right now 8 more hours passed and no new entry is in the logs, no errors no crash.\nCan someone explain this ? Unfortunately I don't have any AWS support plan.\nAs you can see from the picture below after 11am there is nothing..<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hswD7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hswD7.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The training job is supposed to complete in the next couple of hours, but now I'm not sure if is actually running (in this case would be a cloudwatch problem) or not..<\/p>\n<p><strong>UPDATE<\/strong><\/p>\n<p>Suddenly the training job failed, without any further log. The reason is<\/p>\n<blockquote>\n<p>ClientError: Artifact upload failed:Error 7: The credentials received\nhave been expired<\/p>\n<\/blockquote>\n<p>But there is still nothing in the logs after 11am. Very weird.<\/p>",
        "Challenge_closed_time":1616684677672,
        "Challenge_comment_count":5,
        "Challenge_created_time":1612979073253,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user created a training job on AWS Sagemaker, but it got stuck in the processing state for more than 14 hours. Cloudwatch is not logging anything since then, and there are no errors or crashes. The user does not have an AWS support plan and is seeking an explanation for this issue. The training job failed suddenly without any further log, and the reason is \"Artifact upload failed: Error 7: The credentials received have been expired\".",
        "Challenge_last_edit_time":1613031017147,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66142193",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":7.3,
        "Challenge_reading_time":15.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1029.3345608333,
        "Challenge_title":"AWS Sagemaker training job stuck in progress state",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":755.0,
        "Challenge_word_count":188,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416346350292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation_count":2302.0,
        "Poster_view_count":227.0,
        "Solution_body":"<p>For future readers I can confirm that is something that can happen very rarely (I' haven't experienced it anymore since then), but it's AWS fault. Same data, same algorithm.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":2.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":341.7419444444,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi,\r\n\r\nI created a custom docker container to deploy my model on Vertex AI. The model uses LightGBM, so I can't use the pre-built container images available for TF\/SKL\/XGBoost. I was able to deploy the model and get predictions, but I get errors while trying to get **explainable** predictions from the model. I have tried to follow the Vertex AI guidelines to configure the model for explanations.\r\nThe example below shows a simplified version of the model that still reproduces the issue, with only two input features 'A' and 'B'.\r\n\r\nPlease take a look and tell me if the explanation metadata is supposed to be set differently, or if there is something wrong with this approach.\r\n\r\n\r\n#### Environment details\r\n\r\n  - Google Cloud Notebook\r\n  - Python version: 3.7.12\r\n  - pip version: 21.3.1\r\n  - `google-cloud-aiplatform` version: 1.15.0\r\n\r\n#### Reference\r\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/explainable-ai\/configuring-explanations#custom-container\r\n\r\n#### explanation-metadata.json\r\n(_Model output is unkeyed. The Vertex AI guide suggests using any memorable string for output key._)\r\n```\r\n{\r\n    \"inputs\": {\r\n        \"A\": {},\r\n        \"B\": {}\r\n    },\r\n    \"outputs\": {\r\n        \"Y\": {}\r\n    }\r\n}\r\n```\r\n#### Model upload with explanation parameters and metadata\r\n```\r\n! gcloud ai models upload \\\r\n  --region=$REGION \\\r\n  --display-name=$MODEL_NAME \\\r\n  --container-image-uri=$PRED_IMAGE_URI \\\r\n  --artifact-uri=$ARTIFACT_LOCATION_GCS \\\r\n  --explanation-method=sampled-shapley \\\r\n  --explanation-path-count=10 \\\r\n  --explanation-metadata-file=explanation-metadata.json\r\n```\r\n\r\n#### Prediction\/Explanation Input\r\n```\r\ninstances = [{\"A\": 1.1, \"B\": 20}, {\"A\": 2.2, \"B\": 21}]\r\n# Prediction (works fine):\r\nendpoint.predict(instances=instances)\r\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\r\nendpoint.explain(instances=instances) # Returns error (1) shown in stack trace below\r\n\r\n# Another example\r\ninstances_2 = [[1.1,20], [2.2,21]]\r\n# Prediction (works fine):\r\nendpoint.predict(instances=instances_2)\r\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\r\nendpoint.explain(instances=instances_2) # Returns error\r\n# Error: Nameless inputs are allowed only if there is a single input in the explanation metadata.\r\n```\r\n#### Prediction Server (Flask)\r\n```python\r\n# Custom Flask server to serve online predictions\r\n# Input for prediction\r\nraw_input = request.get_json()\r\ninput = raw_input['instances']\r\ndf = pd.DataFrame(input, columns = ['A', 'B'])\r\n# Prediction from model (loaded from GCP bucket)\r\npredictions = model.predict(df).tolist() # [0, 1]\r\nresponse = jsonify({\"predictions\": predictions})\r\nreturn response\r\n```\r\n\r\n#### Stack trace of error (1)\r\n```\r\n---------------------------------------------------------------------------\r\n_InactiveRpcError                         Traceback (most recent call last)\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\r\n     49         try:\r\n---> 50             return callable_(*args, **kwargs)\r\n     51         except grpc.RpcError as exc:\r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\r\n    945                                       wait_for_ready, compression)\r\n--> 946         return _end_unary_response_blocking(state, call, False, None)\r\n    947 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\r\n    848     else:\r\n--> 849         raise _InactiveRpcError(state)\r\n    850 \r\n\r\n_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\r\n\tstatus = StatusCode.INVALID_ARGUMENT\r\n\tdetails = \"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\"\r\n\tdebug_error_string = \"{\"created\":\"@1658310559.755090975\",\"description\":\"Error received from peer ipv4:74.125.133.95:443\",\"file\":\"src\/core\/lib\/surface\/call.cc\",\"file_line\":1069,\"grpc_message\":\"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\",\"grpc_status\":3}\"\r\n>\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nInvalidArgument                           Traceback (most recent call last)\r\n\/tmp\/ipykernel_2590\/4024017963.py in <module>\r\n----> 3 print(endpoint.explain(instances=instances, parameters={}))\r\n\r\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in explain(self, instances, parameters, deployed_model_id, timeout)\r\n   1563             parameters=parameters,\r\n   1564             deployed_model_id=deployed_model_id,\r\n-> 1565             timeout=timeout,\r\n   1566         )\r\n   1567 \r\n\r\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1\/services\/prediction_service\/client.py in explain(self, request, endpoint, instances, parameters, deployed_model_id, retry, timeout, metadata)\r\n    917             retry=retry,\r\n    918             timeout=timeout,\r\n--> 919             metadata=metadata,\r\n    920         )\r\n    921 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method.py in __call__(self, timeout, retry, *args, **kwargs)\r\n    152             kwargs[\"metadata\"] = metadata\r\n    153 \r\n--> 154         return wrapped_func(*args, **kwargs)\r\n    155 \r\n    156 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\r\n     50             return callable_(*args, **kwargs)\r\n     51         except grpc.RpcError as exc:\r\n---> 52             raise exceptions.from_grpc_error(exc) from exc\r\n     53 \r\n     54     return error_remapped_callable\r\n\r\nInvalidArgument: 400 {\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\r\n---------------------------------------------------------------------------\r\n```",
        "Challenge_closed_time":1659550833000,
        "Challenge_comment_count":8,
        "Challenge_created_time":1658320562000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue with the outdated Vertex AI blog post after the 0.20.0 release. The guide to run a pipeline using Vertex AI fails because ZenML does not have a `metadata-store` stack category. The user tried to run `zenml metadata-store` but received an error message. Without adding the `metadata-store`, the Vertex AI pipeline fails.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/googleapis\/python-aiplatform\/issues\/1526",
        "Challenge_link_count":1,
        "Challenge_participation_count":8,
        "Challenge_readability":14.2,
        "Challenge_reading_time":79.22,
        "Challenge_repo_contributor_count":80.0,
        "Challenge_repo_fork_count":227.0,
        "Challenge_repo_issue_count":2273.0,
        "Challenge_repo_star_count":360.0,
        "Challenge_repo_watch_count":58.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":341.7419444444,
        "Challenge_title":"Error while trying to get explanation from (custom container) model deployed on Vertex AI (Prediction without explanation works fine)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":578,
        "Discussion_body":"Hi @jaycee-li,\r\nAny update on this? Would really appreciate your inputs! Hi @pankajrsingla, sorry for the late reply!\r\n\r\nSince instance_2 prediction works for your model, looks like your model takes unkeyed input. Could you please try this metadata setting:\r\n```\r\n{\r\n    \"inputs\": {\r\n        \"X\": {},\r\n    },\r\n    \"outputs\": {\r\n        \"Y\": {}\r\n    }\r\n}\r\n```\r\nThen update the model, endpoint, and try:\r\n```\r\ninstances = [[1.1,20], [2.2,21]]\r\nendpoint.explain(instances=instances)\r\n```\r\n\r\nPlease let me know if this works for you. Hi @jaycee-li,\r\nThank you so much for your response.\r\nI tried your suggestion, but I got the same error as before.\r\n`Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\"`\r\n\r\nIf you see the code for my prediction server, it can take both unkeyed as well as keyed input (prediction works fine for both cases), since it converts the input to a dataframe. The output is definitely unkeyed. However, I am still confused as to what should be the contents of the explanation-metadata.json file.\r\n\r\nAlso, just to be sure - the same API (predict) in the flask server is supposed to work for both predictions and explanations, right? Or do I need to create a separate API for 'explain'?\r\n\r\nIf you have any other suggestions, I would be more than happy to try them out. \r\n(If that would help, I can also send you the full contents of the Jupyter notebook - all code one place - if you share your email id.)\r\n\r\nPlease let me know!\r\n\r\nThank you! It would be helpful if you can share the notebook to jayceeli@google.com\r\n\r\nThank you very much! Done!\r\nThanks! :) Hi @pankajrsingla ,\r\n\r\nI got `AttributeError: 'Blob' object has no attribute 'open'` for `with blob.open(\"wb\") as f:` in your TRAIN_IMAGE_URI. So I was stuck here and didn't reproduce the error you got. \r\n\r\nYou mixed CLI, gapic API, and SDK in your code. Since I'm not familiar with CLI tool, I'm not very sure what the problem is. Maybe it's due to your PRED_IMAGE_URI? I would suggest you to try a pre-built container(`us-docker.pkg.dev\/vertex-ai\/prediction\/sklearn-cpu.1-0:latest`) when uploading the model.\r\n\r\nI drafted a notebook that used SDK only to train, upload, deploy a same model as yours. And it can successfully make predictions and explanations. I've shared the notebook with you for your reference.\r\n\r\nPlease let me know if you still get the error. Thanks! Hi @pankajrsingla ,\r\n\r\nPlease check this [notebook](https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/ml_ops\/stage6\/get_started_with_xai_and_custom_server.ipynb) (Specifically **Create the model server** and **Build a FastAPI HTTP server** sections) for how to use XAI with a custom container. Thanks a lot, @jaycee-li! This is exactly what I was looking for!\r\nI will give this a try for my model, and will update you once I have the results. This should work.\r\n\r\nThank you!",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":27.8041516667,
        "Challenge_answer_count":1,
        "Challenge_body":"When I was building model for analyzing in SageMaker Canvas, it just run for 1h 2m and then I got this notification:\n\nModel building failed: Failed to run Neo compilation or generate explainability report. client_request_id is f76d5bf7-6780-4257-9631-500101632b1e\n\nWhere should I contact the admin for this issue? Thank you so much!",
        "Challenge_closed_time":1644863324414,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644763229468,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an issue while building a model in SageMaker Canvas, which failed after running for 1 hour and 2 minutes. The error message indicated a failure to run Neo compilation or generate an explainability report, and the user is seeking guidance on where to report the issue to the admin.",
        "Challenge_last_edit_time":1668456865503,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUpsGPPvV7SbuofE1fnwC5AA\/where-should-i-report-to-when-encounter-a-trouble-at-sagemaker-canvas",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":5.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":27.8041516667,
        "Challenge_title":"Where should I report to when encounter a trouble at SageMaker Canvas?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":139.0,
        "Challenge_word_count":61,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Ailee, you can submit a ticket here and engineering will get back to you: https:\/\/t.corp.amazon.com\/create\/templates\/20b56bae-3fca-4281-9b94-69b6e50128cd. Thanks!",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1644863324414,
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":2.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker"
    }
]