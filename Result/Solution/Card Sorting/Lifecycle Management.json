[
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":44.5576536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using ML Designer and i have created a sub-pipeline that i want to use it in other pipelines. how do i call that sub-pipeline from the designer?  <\/p>\n<p>The purpose of the subpipeline is to transform data, so the output is a dataset.  <\/p>",
        "Challenge_closed_time":1619449185636,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619288778083,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to call a sub-pipeline created in Azure ML Designer from other pipelines. The sub-pipeline is designed to transform data and produce a dataset as output. The user is seeking guidance on how to call the sub-pipeline from the designer.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/370433\/azure-ml-designer-how-to-call-a-pipeline-from-anot",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":3.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":44.5576536111,
        "Challenge_title":"azure ml designer: how to call a pipeline from another pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@javier-8889Thanks for the question. Can you please add more details about the pipeline steps. You can implement an <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-batch-scoring-classification#create-the-pipeline-step\">AML pipeline<\/a> with Python code, but also with the new AML designer which under the covers is creating an AML Pipeline defining that \u201cvisual workflow\u201d. Basically you can register a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-model-designer\">trained model in Designer<\/a> bring it out with SDK\/CLI to deploy it. Currently only Data Drift Monitor (Data Drift-&gt;EventGrid-&gt;LogicApp-&gt;Trigger Pipeline) allows to trigger a pipeline when dataset drift has been detected.    <\/p>\n<p>When designing a pipeline in Azure ML Designer, each step or module creates intermediate datasets that can be seen using the UI using Visualize option. Those datasets are persisted in the blob storage.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.9,
        "Solution_reading_time":12.75,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":116.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1508520702036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":21.2055166667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>when running an AML pipeline on AML compute, I get this kind of error : <\/p>\n\n<p>I can try rebooting the cluster, but that may not fix the problem (if storage gets accumulated no the nodes, that should be cleaned.<\/p>\n\n<pre><code>Session ID: 933fc468-7a22-425d-aa1b-94eba5784faa\n{\"error\":{\"code\":\"ServiceError\",\"message\":\"Job preparation failed: [Errno 28] No space left on device\",\"detailsUri\":null,\"target\":null,\"details\":[],\"innerError\":null,\"debugInfo\":{\"type\":\"OSError\",\"message\":\"[Errno 28] No space left on device\",\"stackTrace\":\" File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1-setup\/job_prep.py\\\", line 126, in &lt;module&gt;\\n invoke()\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1-setup\/job_prep.py\\\", line 97, in invoke\\n extract_project(project_dir, options.project_zip, options.snapshots)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1-setup\/job_prep.py\\\", line 60, in extract_project\\n project_fetcher.fetch_project_snapshot(snapshot[\\\"Id\\\"], snapshot[\\\"PathStack\\\"])\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 72, in fetch_project_snapshot\\n _download_tree(sas_tree, path_stack)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 106, in _download_tree\\n _download_tree(child, path_stack)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 106, in _download_tree\\n _download_tree(child, path_stack)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 98, in _download_tree\\n fh.write(response.read())\\n\",\"innerException\":null,\"data\":null,\"errorResponse\":null}},\"correlation\":null,\"environment\":null,\"location\":null,\"time\":\"0001-01-01T00:00:00+00:00\"}\n<\/code><\/pre>\n\n<p>I would expect the job to run as it should. And in fact, I've checked on the node and the node do have lots of available harddrive space :<\/p>\n\n<pre><code>root@4f57957ac829466a86bad4d4dc51fadd000001:~# df -kh                                                                                               Filesystem      Size  Used Avail Use% Mounted on\nudev             28G     0   28G   0% \/dev\ntmpfs           5.6G  9.0M  5.5G   1% \/run\n\/dev\/sda1       125G  2.8G  122G   3% \/\ntmpfs            28G     0   28G   0% \/dev\/shm\ntmpfs           5.0M     0  5.0M   0% \/run\/lock\ntmpfs            28G     0   28G   0% \/sys\/fs\/cgroup\n\/dev\/sdb1       335G  6.7G  311G   3% \/mnt\ntmpfs           5.6G     0  5.6G   0% \/run\/user\/1002\n<\/code><\/pre>\n\n<p>Suggestions on what I should check?<\/p>",
        "Challenge_closed_time":1568309206067,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568232866207,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error message stating \"No space left on device\" when running an AML pipeline on AML compute. The user has checked the node and found that there is enough available hard drive space. The user is seeking suggestions on what to check next.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57896195",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":15.5,
        "Challenge_reading_time":42.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":21.2055166667,
        "Challenge_title":"Out of disk space",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":738.0,
        "Challenge_word_count":210,
        "Platform":"Stack Overflow",
        "Poster_created_time":1538275960603,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Montreal, QC, Canada",
        "Poster_reputation_count":381.0,
        "Poster_view_count":50.0,
        "Solution_body":"<p>Seems like you've run into Azure file share constraints. You can use the following sample code to change your runs to use blob storage which can scale to large number of jobs running in parallel:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data#accessing-source-code-during-training\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data#accessing-source-code-during-training<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.0,
        "Solution_reading_time":6.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.09,
        "Challenge_answer_count":0,
        "Challenge_body":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel.\r\n\r\n## Expected behavior\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n- Enchanter version: 0.7.0\r\n- Python version: 3.6.6\r\n- OS: Ubuntu 18.04\r\n- (Optional) Other libraries and their versions:\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## Reproducible examples (optional)\r\n\r\n```python\r\n# python code\r\n```\r\n\r\n## Additional context (optional)\r\n\r\n<!-- Please add any other context or screenshots about the problem here. -->",
        "Challenge_closed_time":1600309841000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1600305917000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering issues with the pipeline `sentence_embedding\/dvc.yaml` as it is not correctly defined for `evaluation:deps`. This is causing problems with pulling the model `biobert_nli_sts_cord19_v1` and running the training stage before the evaluation stage for the models `tf_idf\/` and `count\/`. The user is unable to run `dvc pull -d` and `dvc repro -f` without errors about missing files.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/khirotaka\/enchanter\/issues\/132",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":8.81,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":211.0,
        "Challenge_repo_star_count":7.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":1.09,
        "Challenge_title":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel.",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":96,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Issue-Label Bot is automatically applying the label `bug` to this issue, with a confidence of 0.68. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \n\n Links: [app homepage](https:\/\/github.com\/marketplace\/issue-label-bot), [dashboard](https:\/\/mlbot.net\/data\/khirotaka\/enchanter) and [code](https:\/\/github.com\/hamelsmu\/MLapp) for this bot.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.3,
        "Solution_reading_time":4.88,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":38.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.2782375,
        "Challenge_answer_count":1,
        "Challenge_body":"So as mentioned in my [other recent post](https:\/\/repost.aws\/questions\/QUAL9Vn9abQ6KKCs2ASwwmzg\/adjusting-sagemaker-xgboost-project-to-tensorflow-or-even-just-different-folder-name), I'm trying to modify the sagemaker example abalone xgboost template to use tensorfow.\n\nMy current problem is that running the pipeline I get a failure and in the logs I see:\n\n```\nModuleNotFoundError: No module named 'transformers'\n```\n\nNOTE: I am importing 'transformers' in `preprocess.py` not in `pipeline.py`\n\nNow I have 'transformers' listed in various places as a dependency including:\n\n* `setup.py` - `required_packages = [\"sagemaker==2.93.0\", \"sklearn\", \"transformers\", \"openpyxl\"]`\n* `pipelines.egg-info\/requires.txt` - `transformers` (auto-generated from setup.py?)\n\nbut so I'm keen to understand, how can I ensure that additional dependencies are available in the pipline itself?\n\nMany thanks in advance\n\n------------\n------------\n------------\nADDITIONAL DETAILS ON HOW I ENCOUNTERED THE ERROR\n\nFrom one particular notebook (see [previous post](https:\/\/repost.aws\/questions\/QUAL9Vn9abQ6KKCs2ASwwmzg\/adjusting-sagemaker-xgboost-project-to-tensorflow-or-even-just-different-folder-name) for more details)  I have succesfully constructed the new topic\/tensorflow pipeline and run the following steps:\n\n```\npipeline.upsert(role_arn=role)\nexecution = pipeline.start()\nexecution.describe()\n```\n\nthe `describe()` method gives this output:\n\n```\n{'PipelineArn': 'arn:aws:sagemaker:eu-west-1:398371982844:pipeline\/topicpipeline-example',\n 'PipelineExecutionArn': 'arn:aws:sagemaker:eu-west-1:398371982844:pipeline\/topicpipeline-example\/execution\/0aiczulkjoaw',\n 'PipelineExecutionDisplayName': 'execution-1664394415255',\n 'PipelineExecutionStatus': 'Executing',\n 'PipelineExperimentConfig': {'ExperimentName': 'topicpipeline-example',\n  'TrialName': '0aiczulkjoaw'},\n 'CreationTime': datetime.datetime(2022, 9, 28, 19, 46, 55, 147000, tzinfo=tzlocal()),\n 'LastModifiedTime': datetime.datetime(2022, 9, 28, 19, 46, 55, 147000, tzinfo=tzlocal()),\n 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-1:398371982844:user-profile\/d-5qgy6ubxlbdq\/sjoseph-reg-genome-com-273',\n  'UserProfileName': 'sjoseph-reg-genome-com-273',\n  'DomainId': 'd-5qgy6ubxlbdq'},\n 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-1:398371982844:user-profile\/d-5qgy6ubxlbdq\/sjoseph-reg-genome-com-273',\n  'UserProfileName': 'sjoseph-reg-genome-com-273',\n  'DomainId': 'd-5qgy6ubxlbdq'},\n 'ResponseMetadata': {'RequestId': 'f949d6f4-1865-4a01-b7a2-a96c42304071',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': 'f949d6f4-1865-4a01-b7a2-a96c42304071',\n   'content-type': 'application\/x-amz-json-1.1',\n   'content-length': '882',\n   'date': 'Wed, 28 Sep 2022 19:47:02 GMT'},\n  'RetryAttempts': 0}}\n```\nWaiting for the execution I get:\n\n```\n---------------------------------------------------------------------------\nWaiterError                               Traceback (most recent call last)\n<ipython-input-14-72be0c8b7085> in <module>\n----> 1 execution.wait()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in wait(self, delay, max_attempts)\n    581             waiter_id, model, self.sagemaker_session.sagemaker_client\n    582         )\n--> 583         waiter.wait(PipelineExecutionArn=self.arn)\n    584 \n    585 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/waiter.py in wait(self, **kwargs)\n     53     # method.\n     54     def wait(self, **kwargs):\n---> 55         Waiter.wait(self, **kwargs)\n     56 \n     57     wait.__doc__ = WaiterDocstring(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/waiter.py in wait(self, **kwargs)\n    376                     name=self.name,\n    377                     reason=reason,\n--> 378                     last_response=response,\n    379                 )\n    380             if num_attempts >= max_attempts:\n\nWaiterError: Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\"\n```\nWhich I assume is corresponding to the failure I see in the logs:\n\n![buildl pipeline error message on preprocessing step](\/media\/postImages\/original\/IMMpF6LeI6TgWxp20TnPZbUw)\n\nI did also run `python setup.py build` to ensure my build directory was up to date ... here's the terminal output of that command:\n\n```\nsagemaker-user@studio$ python setup.py build\n\/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n  warnings.warn(\n\/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/config\/setupcfg.py:508: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.\n  warnings.warn(msg, warning_class)\nrunning build\nrunning build_py\ncopying pipelines\/topic\/pipeline.py -> build\/lib\/pipelines\/topic\nrunning egg_info\nwriting pipelines.egg-info\/PKG-INFO\nwriting dependency_links to pipelines.egg-info\/dependency_links.txt\nwriting entry points to pipelines.egg-info\/entry_points.txt\nwriting requirements to pipelines.egg-info\/requires.txt\nwriting top-level names to pipelines.egg-info\/top_level.txt\nreading manifest file 'pipelines.egg-info\/SOURCES.txt'\nadding license file 'LICENSE'\nwriting manifest file 'pipelines.egg-info\/SOURCES.txt'\n```\nIt seems like the dependencies are being written to `pipelines.egg-info\/requires.txt` but are these not being picked up by the pipeline?",
        "Challenge_closed_time":1664451755510,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664396753855,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a `ModuleNotFoundError` while using the transformers module with a SageMaker Studio project. The user has listed transformers as a dependency in various places, including `setup.py` and `pipelines.egg-info\/requires.txt`, but is still unable to ensure that additional dependencies are available in the pipeline itself. The user has also encountered a `WaiterError` while waiting for the pipeline execution to complete.",
        "Challenge_last_edit_time":1668528521211,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUdd2zOBY0Q4CEG1ZdbgNsgA\/using-transformers-module-with-sagemaker-studio-project-modulenotfounderror-no-module-named-transformers",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.7,
        "Challenge_reading_time":71.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":43,
        "Challenge_solved_time":15.2782375,
        "Challenge_title":"using transformers module with sagemaker studio project: ModuleNotFoundError: No module named 'transformers'",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":133.0,
        "Challenge_word_count":440,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! There are two places where you need to install the dependencies \/ requirements:\n\n1. In your environment where you execute `pipeline.start()` \u2013 can be Amazon SageMaker Studio, your local machine or CI\/CD pipeline executor, e. g. AWS CodeBuild. These dependencies are installed in `setup.py`.\n2. Inside the SageMaker processing and training jobs as well as in inference endpoints. This is usually done via `requirements.txt` file that you submit as part of your `source_dir`.\n\nIn your example, I recommend you to use the `TensorFlowProcessor`. The way how to install dependencies into it is described [in the corresponding section of the documentation](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-job-frameworks-tensorflow.html), in particular:\n> SageMaker Processing installs the dependencies in `requirements.txt` in the container for you.\n\nSame applies to your model training and to the `TensorFlow` estimator. See the section [Use third-party libraries](https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#use-third-party-libraries) in the TensorFlow documentation of the SageMaker Python SDK, in particular:\n> If there are other packages you want to use with your script, you can use a `requirements.txt` to install other dependencies at runtime. \n\nHope it helps!",
        "Solution_comment_count":14.0,
        "Solution_last_edit_time":1664531543836,
        "Solution_link_count":2.0,
        "Solution_readability":10.9,
        "Solution_reading_time":16.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":167.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":171.2597222222,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nI tried to load a KedroPipelineModel from mlflow, and I got a \"cannot pickle context artifacts\" error, which is due do the \r\n\r\n## Context\r\n\r\nI cannot load a previously saved KedroPipelineModel generated by pipeline_ml_factory.\r\n\r\n## Steps to Reproduce\r\n\r\nSave A KedroPipelineModel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer)\r\n\r\n## Expected Result\r\n\r\nThe model should be loaded\r\n\r\n## Actual Result\r\n\r\nAn error is raised\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used: 0.16.5 and 0.4.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Windows 10 & CentOS were tested\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n# Potential solution\r\n\r\nThe faulty line is:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/mlflow\/kedro_pipeline_model.py#L45",
        "Challenge_closed_time":1606599848000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1605983313000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a TypeError when using the suggested VSCode configuration for debugging Kedro. The error is caused by commandline arguments being None when running the pipeline directly through run.py.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/122",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.1,
        "Challenge_reading_time":13.32,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":171.2597222222,
        "Challenge_title":"A KedroPipelineModel cannot be loaded from mlflow if its catalog contains non deepcopy-able DataSets",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":137,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Does removing the faulty line and using directly the initial_catalog make the model loadable again ? if Yes, we have two options :\r\n\r\n* We no longer deepcopy the initial_catalog\r\n* We copy each DataSet of the catalog with his own loader (for example, we use tf.keras.models.clone_model for keras model DataSet ...)\r\n\r\nKnowing that the `KedroPipelineModel` is intented to be used in a separated process (at inference-time), we can just remove the deepcopy part (there won't be a conflict with another function using the same catalog)\r\n After some investigation, the issues comes from the MLflowAbstractModelDataSet, and particularly the `self._mlflow_model_module` attribute which is a module and not deepcopiable by nature. I suggest to store it as a string, and have a property attribute to load the module on the fly.\r\n\r\nNote that this is a problem which occurs only when the DataSet is not deepcopiable (and not the underlying value the DataSet can load(), so we can quite safely assume that it should not occur often). If it does, we should consider a more radical solution among the ones you suggest.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":13.43,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":175.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":249.1975413889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>I would like to know if there is the possibility to use a custom environment (created from the AML portal) for the execution of a Python Script Step in the Azure Machine Learning Designer (only using the designer, not using azureml sdk to publish the pipeline from the code).   <\/p>\n<p>Thanks,  <br \/>\nG<\/p>",
        "Challenge_closed_time":1648494342676,
        "Challenge_comment_count":2,
        "Challenge_created_time":1647597231527,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of using a custom environment created from the AML portal for executing a Python Script Step in the Azure Machine Learning Designer without using the azureml sdk to publish the pipeline from the code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/777745\/use-custom-environment-in-azure-machine-learning-d",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.4,
        "Challenge_reading_time":4.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":249.1975413889,
        "Challenge_title":"Use custom environment in Azure Machine Learning Designer",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":62,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thanks for your feedback. Based on your comments above, it seems you want to configure a custom environment in AML designer and install unsupported python libraries. These are the supported <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment\">Custom Environments<\/a>. However, in AML designer, the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-designer-python\">execute python script component<\/a> enables you to write custom python scripts and install python libraries. This particular <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/component-reference\/execute-python-script\">document<\/a> shows how to configure execute python script. You can install packages that aren't in the preinstalled list by using the following command:<\/p>\n<pre><code>import os  \nos.system(f&quot;pip install scikit-misc&quot;)  \n<\/code><\/pre>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.5,
        "Solution_reading_time":13.4,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":96.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":166.3613888889,
        "Challenge_answer_count":0,
        "Challenge_body":"I tried to run benchmark.py, with WandB, but got an error because the config is too large, probably due to the train_selection array being too big. `ERROR Error while calling W&B API: run config cannot exceed 15 MB (<Response [400]>)`\r\n\r\nPerhaps the data selections does not need to be uploaded to WandB?\r\n\r\nThe full message is: \r\n```(graphnet) [peter@hep04 northern_tracks]$ python benchmark.py \r\ngraphnet: INFO     2022-10-19 10:33:19 - get_logger - Writing log to logs\/graphnet_20221019-103308.log\r\ngraphnet: WARNING  2022-10-19 10:33:25 - warn_once - `icecube` not available. Some functionality may be missing.\r\nwandb: Currently logged in as: peterandresen (graphnet-team). Use `wandb login --relogin` to force relogin\r\nwandb: wandb version 0.13.4 is available!  To upgrade, please run:\r\nwandb:  $ pip install wandb --upgrade\r\nwandb: Tracking run with wandb version 0.13.1\r\nwandb: Run data is saved locally in .\/wandb\/wandb\/run-20221019_103334-47u9ascy\r\nwandb: Run `wandb offline` to turn off syncing.\r\nwandb: Syncing run woven-water-2\r\nwandb: \u2b50\ufe0f View project at https:\/\/wandb.ai\/graphnet-team\/NortherenTracks_Benchmark\r\nwandb: \ud83d\ude80 View run at https:\/\/wandb.ai\/graphnet-team\/NortherenTracks_Benchmark\/runs\/47u9ascy\r\nwandb: WARNING Serializing object of type list that is 14743672 bytes\r\nwandb: WARNING Serializing object of type list that is 4914592 bytes\r\nwandb: WARNING Serializing object of type list that is 4914600 bytes\r\nwandb: WARNING Serializing object of type list that is 15673400 bytes\r\nwandb: WARNING Serializing object of type list that is 5429640 bytes\r\nwandb: WARNING Serializing object of type list that is 5429640 bytes\r\ngraphnet: INFO     2022-10-19 10:33:54 - train - features: ['dom_x', 'dom_y', 'dom_z', 'dom_time', 'charge', 'rde', 'pmt_area']\r\ngraphnet: INFO     2022-10-19 10:33:54 - train - truth: ['energy', 'energy_track', 'position_x', 'position_y', 'position_z', 'azimuth', 'zenith', 'pid', 'elasticity', 'sim_type', 'interaction_type', 'interaction_time', 'inelasticity']\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\n\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/core\/lightning.py:22: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\r\n  rank_zero_deprecation(\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\r\n\r\n  | Name      | Type            | Params\r\n----------------------------------------------\r\n0 | _detector | IceCubeDeepCore | 0     \r\n1 | _gnn      | DynEdge         | 1.3 M \r\n2 | _tasks    | ModuleList      | 258   \r\n----------------------------------------------\r\n1.3 M     Trainable params\r\n0         Non-trainable params\r\n1.3 M     Total params\r\n5.376     Total estimated model params size (MB)\r\nEpoch  0:   0%|                                                                                                            | 0\/4800 [00:00<?, ? batch(es)\/s]wandb: ERROR Error while calling W&B API: run config cannot exceed 15 MB (<Response [400]>)\r\nThread SenderThread:\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py\", line 25, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 1465, in upsert_run\r\n    response = self.gql(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py\", line 113, in __call__\r\n    result = self._call_fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 204, in execute\r\n    return self.client.execute(*args, **kwargs)  # type: ignore\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 52, in execute\r\n    result = self._get_result(document, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 60, in _get_result\r\n    return self.transport.execute(document, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/transport\/requests.py\", line 39, in execute\r\n    request.raise_for_status()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/requests\/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https:\/\/api.wandb.ai\/graphql\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 51, in run\r\n    self._run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 95, in _run\r\n    self._debounce()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal.py\", line 316, in _debounce\r\n    self._sm.debounce()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 387, in debounce\r\n    self._debounce_config()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 393, in _debounce_config\r\n    self._api.upsert_run(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py\", line 27, in wrapper\r\n    raise CommError(err.response, err)\r\nwandb.errors.CommError: <Response [400]>\r\nwandb: ERROR Internal wandb error: file data was not synced\r\nEpoch  0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4800\/4800 [09:03<00:00,  8.83 batch(es)\/s, loss=-1.22]Traceback (most recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1200\/1200 [01:17<00:00, 15.53 batch(es)\/s]\r\n  File \"benchmark.py\", line 204, in <module>\r\n    main()\r\n  File \"benchmark.py\", line 200, in main\r\n    train(config)\r\n  File \"benchmark.py\", line 142, in train\r\n    trainer.fit(model, training_dataloader, validation_dataloader)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 696, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 650, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 735, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1166, in _run\r\n    results = self._run_stage()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1252, in _run_stage\r\n    return self._run_train()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1283, in _run_train\r\n    self.fit_loop.run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 200, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 271, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 201, in run\r\n    self.on_advance_end()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 241, in on_advance_end\r\n    self._run_validation()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 299, in _run_validation\r\n    self.val_loop.run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 207, in run\r\n    output = self.on_run_end()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/dataloader\/evaluation_loop.py\", line 198, in on_run_end\r\n    self.trainer._logger_connector.log_eval_end_metrics(all_logged_outputs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 142, in log_eval_end_metrics\r\n    self.log_metrics(metrics)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 109, in log_metrics\r\n    logger.log_metrics(metrics=scalar_metrics, step=step)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 390, in log_metrics\r\n    self.experiment.log(dict(metrics, **{\"trainer\/global_step\": step}))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 289, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 255, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1591, in log\r\n    self._log(data=data, step=step, commit=commit)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1375, in _log\r\n    self._partial_history_callback(data, step, commit)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1259, in _partial_history_callback\r\n    self._backend.interface.publish_partial_history(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 553, in publish_partial_history\r\n    self._publish_partial_history(partial_history)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 67, in _publish_partial_history\r\n    self._publish(rec)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_sock.py\", line 51, in _publish\r\n    self._sock_client.send_record_publish(record)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 150, in send_record_publish\r\n    self.send_server_request(server_req)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 84, in send_server_request\r\n    self._send_message(msg)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\r\n    self._sendall_with_error_handle(header + data)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\r\n    sent = self._sock.send(data[total_sent:])\r\nBrokenPipeError: [Errno 32] Broken pipe\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\r\n    self._sendall_with_error_handle(header + data)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\r\n    sent = self._sock.send(data[total_sent:])\r\nBrokenPipeError: [Errno 32] Broken pipe```\r\n",
        "Challenge_closed_time":1666770135000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1666171234000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running benchmark.py with WandB due to the config being too large, resulting in a 400 error. The error message suggests that the train_selection array may be too big. The user suggests that the data selections may not need to be uploaded to WandB.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/graphnet-team\/graphnet\/issues\/316",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":18.9,
        "Challenge_reading_time":171.15,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":20.0,
        "Challenge_repo_issue_count":377.0,
        "Challenge_repo_star_count":23.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":126,
        "Challenge_solved_time":166.3613888889,
        "Challenge_title":"WandB fails when config is too large",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":858,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yeah, I wouldn't call this a bug _per se_. It's just that `WandbLogger` has some limitations that we need to navigate.\r\n\r\nI think your options are to:\r\n\r\n1. not log the training selection; \r\n2. log the test selection instead, as it should be considerably smaller; \r\n3. encode the selection in the data pipeline such that the train\/test label is a column in your database rather than a separate array, and then just log this column name; or \r\n4. implement and log the selection as a reproducible prescription (e.g., `test = event_no % 5 == 0` and `train = not test`) rather than as an explicit array of indices. \r\n\r\nI don't think (1) is a good option, but (2-4) could all work and I think they are all pretty straightforward to do.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":8.58,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":127.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4452663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I tried invoking an Azure ML pipeline from an Azure DevOps pipeline, I keep running into errors, Can you please share any sample that works.<\/p>",
        "Challenge_closed_time":1658317648816,
        "Challenge_comment_count":1,
        "Challenge_created_time":1658316045857,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering errors while trying to invoke an Azure ML pipeline from an Azure DevOps pipeline and is seeking a working sample.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/934296\/error-invoking-the-azure-ml-pipeline-from-azure-de",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":2.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.4452663889,
        "Challenge_title":"Error invoking the azure ML pipeline from Azure Devops",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":34,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=71c0cf97-895c-43f1-ac54-98e1e9833ae4\">@A-4824  <\/a> Thanks for the question. yes this is possible just use the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/devops\/pipelines\/tasks\/deploy\/azure-cli?view=azure-devops\">Azure CLI task - Azure Pipelines<\/a>  step and run command line or Python scripts inside that to submit your pipelines.     <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":4.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":35.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":339.8062697222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I have started to create a MLOps pipeline that is training multiple models within multiple pipeline steps.<\/p>\n<p>The picture below is the graphical representation of the coded pipeline steps within the Azure ML Studio.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/QvZxX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QvZxX.png\" alt=\"pipeline steps\" \/><\/a><\/p>\n<p>These steps run fine and both the end steps produce the models I want (Train Data - Non EOW TFIDF and Train Data - EOW TFIDF)...<\/p>\n<p>However this is where I get stuck with registering and packaging the model parts for deployment. These models get produced and stored within the individual pipeline step (see below for the model output of Train Data - Non EOW TFIDF)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/MoE5W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/MoE5W.png\" alt=\"non eow step output\" \/><\/a><\/p>\n<p>but I don't know how I would register the model outputs from both pipeline steps together as the docs I have read for registering a model seem to only reference the ability to register one model from one path.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none--sample-input-dataset-none--sample-output-dataset-none--resource-configuration-none-\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none--sample-input-dataset-none--sample-output-dataset-none--resource-configuration-none-<\/a><\/p>\n<p>Basically, is it possible to produce multiple model outputs from multiple pipeline steps and register them together as one??<\/p>\n<p>Thanks in advance for the help!<\/p>",
        "Challenge_closed_time":1627574880243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626102561633,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in registering and packaging multiple model outputs from multiple pipeline steps in Azure ML Studio for deployment. The user is unsure how to register the model outputs from both pipeline steps together as the documentation only references the ability to register one model from one path. The user is seeking help to know if it is possible to produce multiple model outputs from multiple pipeline steps and register them together as one.",
        "Challenge_last_edit_time":1626351577672,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68349739",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":17.2,
        "Challenge_reading_time":28.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":408.9773916667,
        "Challenge_title":"Packaging multiple models from Azure ML experiment",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":578.0,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Poster_created_time":1567669433587,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":130.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>Here is sample for Multi-model Register and deploy. <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":72.9,
        "Solution_reading_time":5.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":1.1481591667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm deploying a SageMaker inference pipeline composed of two PyTorch models (<code>model_1<\/code> and <code>model_2<\/code>), and I am wondering if it's possible to pass the same input to both the models composing the pipeline.<\/p>\n<p>What I have in mind would work more or less as follows<\/p>\n<ol>\n<li><p>Invoke the endpoint sending a binary encoded payload (namely <code>payload_ser<\/code>), for example:<\/p>\n<pre><code>client.invoke_endpoint(EndpointName=ENDPOINT,\n                       ContentType='application\/x-npy',\n                       Body=payload_ser)\n<\/code><\/pre>\n<\/li>\n<li><p>The first model parses the payload with <code>inut_fn<\/code> function, runs the predictor on it, and returns the output of the predictor. As a simplified example:<\/p>\n<pre><code>def input_fn(request_body, request_content_type):\n    if request_content_type == &quot;application\/x-npy&quot;:\n        input = some_function_to_parse_input(request_body)\n    return input\n\ndef predict_fn(input_object, predictor):\n    outputs = predictor(input_object)\n    return outputs\n\ndef output_fn(predictions, response_content_type):\n    return json.dumps(predictions)\n<\/code><\/pre>\n<\/li>\n<li><p>The second model gets as payload both the original payload (<code>payload_ser<\/code>) and the output of the previous model (predictions). Possibly, the <code>input_fn<\/code> function would be used to parse the output of model_1 (as in the &quot;standard case&quot;), but I'd need some way to also make the original payload available to model_2.  In this way, model_2 will use both the original payload and the output of model_1 to make the final prediction and return it to whoever invoked the endpoint.<\/p>\n<\/li>\n<\/ol>\n<p>Any idea if this is achievable?<\/p>",
        "Challenge_closed_time":1638812806036,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638808672663,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is deploying a SageMaker inference pipeline with two PyTorch models and wants to know if it's possible to pass the same input to both models composing the pipeline. The first model parses the payload and runs the predictor on it, returning the output of the predictor. The second model gets both the original payload and the output of the previous model as payload to make the final prediction. The user is seeking advice on whether this is achievable.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70248817",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.7,
        "Challenge_reading_time":22.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1.1481591667,
        "Challenge_title":"Shared input in Sagemaker inference pipeline models",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":210.0,
        "Challenge_word_count":205,
        "Platform":"Stack Overflow",
        "Poster_created_time":1508881117760,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":157.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>Sounds like you need an inference DAG. Amazon SageMaker Inference pipelines currently supports only a chain of handlers, where the output of handler N is the input for handler N+1.<\/p>\n<p>You could change model1's predict_fn() to return both (input_object, outputs), and output_fn(). output_fn() will receive these two objects as the predictions, and will handle serializing both as json. model2's input_fn() will need to know how to parse this pair input.<\/p>\n<p>Consider implementing this as a generic pipeline handling mechanism that adds the input to the model's output. This way you could reuse it for all models and pipelines.<\/p>\n<p>You could allow the model to be deployed as a standalone model, and as a part of a pipeline, and apply the relevant input\/output handling behavior that will be triggered by the presence of an environment variable (<code>Environment<\/code> dict), which you can specify when <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model\" rel=\"nofollow noreferrer\">creating<\/a> the inference pipelines model.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":14.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":150.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":219.0516333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to perform some data transformations using the Python script module in Designer for which i would need to access some pipeline parameters. How can i get those values?  <\/p>\n<p>What would be the equivalent for an R script?<\/p>",
        "Challenge_closed_time":1621310816287,
        "Challenge_comment_count":4,
        "Challenge_created_time":1620522230407,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to access pipeline parameters from a Python script module in Azure ML Designer to perform data transformations. They are seeking guidance on how to obtain these values and also inquiring about the equivalent process for an R script.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/387875\/how-can-i-access-azure-ml-pipeline-parameters-from",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":7.5,
        "Challenge_reading_time":4.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":219.0516333334,
        "Challenge_title":"How can i access azure ml pipeline parameters from a python script running in designer?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=1b597528-b52c-41fc-932a-baf4d96cb15b\">@javier  <\/a>  Thanks, Currently passing a pipeline parameter to the script of Execute Python\/R Module is not supported. We have a new feature custom module which is in private preview. you can write your own module and use in Designer. If it's a common case, it might be better to use custom module.<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.4,
        "Solution_reading_time":4.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":222.8511111111,
        "Challenge_answer_count":0,
        "Challenge_body":"When I register a dataset in the catalog.yml\r\n\r\n```yaml\r\nmy_dataset:\r\n  type : kedro_mlflow.io.MlflowDataSet \r\n  data_set : \r\n    type: pickle.PickleDataSet\r\n    filepath: data\/02_intermediate\/my_dataset.pkl\r\n```\r\n\r\nand I run `kedro run` I got a `expected string or bytes-like object` when **the local path is linux AND the `mlflow_tracking_uri` is an Azure blob storage (it works locally)**. I don't know really why this append, but it can be fied by replacing `self._filepath` by `self._filepath.as_posix()` in these 2 locations: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/kedro_mlflow\/io\/mlflow_dataset.py#L51\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/kedro_mlflow\/io\/mlflow_dataset.py#L55\r\n\r\n@kaemo @akruszewski did you experience some issues with S3 too?\r\n\r\n**EDIT**: @akruszewski it is [the very same issue you encountered here](https:\/\/github.com\/akruszewski\/kedro-mlflow\/commit\/41e9e3fdd2c54a774cca69e1cb52e26cadf50b1e)",
        "Challenge_closed_time":1602278580000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601476316000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue where the mlflow run is not closed when a pipeline fails in interactive mode, leading to unintended side effects and a messy mlflow database. The suggested solution is to implement an \"on_pipeline_error\" kedro hook to close the mlflow run when the pipeline fails.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/74",
        "Challenge_link_count":3,
        "Challenge_participation_count":0,
        "Challenge_readability":10.6,
        "Challenge_reading_time":14.69,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":222.8511111111,
        "Challenge_title":"MlflowDataSet fails to log on remote storage when underlying dataset filepath is converted as a PurePosixPath",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":106,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1377156004256,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":1.8426047222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to get a a simple Azure ML pipeline with the dogs vs cats data set following the steps  - <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">documented here<\/a><\/p>\n\n<p>My notebook contains the following -<\/p>\n\n<pre><code>import azureml.core\nfrom azureml.core import Workspace, Datastore\nfrom azureml.core import Environment\nfrom azureml.core.environment import CondaDependencies\nfrom azureml.pipeline.steps import PythonScriptStep\n\nws = Workspace.from_config()\n\nmyenv = Environment(name=\"myenv\")\nconda_dep = CondaDependencies()\nconda_dep.add_conda_package(\"keras\")\nconda_dep.add_conda_package(\"PIL\")\nmyenv.python.conda_dependencies=conda_dep\nmyenv.register(workspace=ws)\n<\/code><\/pre>\n\n<p>After setting up the data reference and the compute, here's how I am creating the pipeline -<\/p>\n\n<pre><code>trainStep = PythonScriptStep(\n    script_name=\"dogs_vs_cats.py\",\n    arguments=[\"--input\", blob_input_data, \"--output\", output_data1],\n    inputs=[blob_input_data],\n    outputs=[output_data1],\n    compute_target=compute_target,\n    source_directory=\"..\/dogs-vs-cats\"\n)\n\nSteps = [trainStep]\n\nfrom azureml.pipeline.core import Pipeline\npipeline1 = Pipeline(workspace=ws, steps=[Steps])\n\nfrom azureml.core import Experiment\n\npipeline_run1 = Experiment(ws, 'dogs_vs_cats_exp').submit(pipeline1)\npipeline_run1.wait_for_completion()\n<\/code><\/pre>\n\n<p>Once this steps is executed, the experiment fails and I get the following error after a bunch of information -<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"dogs_vs_cats.py\", line 30, in &lt;module&gt;\n    import keras\nModuleNotFoundError: No module named 'keras'\n<\/code><\/pre>\n\n<p>The terminal shows my conda environment set to azureml_py36 and Keras seems be listed in the output of <code>conda list<\/code>.<\/p>\n\n<p>Am I setting up the environment correctly? What is mising <\/p>",
        "Challenge_closed_time":1570089071900,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570082438523,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering a ModuleNotFoundError while trying to execute a simple Azure ML pipeline with the dogs vs cats data set. The error occurs due to the absence of the 'keras' module, even though it is listed in the output of 'conda list'. The user is unsure if the environment is set up correctly and is seeking assistance to identify the missing element.",
        "Challenge_last_edit_time":1570094136667,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58213125",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":25.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1.8426047222,
        "Challenge_title":"ModuleNotFoundError: No module named 'keras' in Azure ML Pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":697.0,
        "Challenge_word_count":186,
        "Platform":"Stack Overflow",
        "Poster_created_time":1570078371343,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":3.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>From the way you have specified your environment, it's hard to see if it's a proper RunConfiguration object. If it is, it should be a matter of adding it to you PythonScriptStep.<\/p>\n\n<pre><code>trainStep = PythonScriptStep(\n    script_name=\"dogs_vs_cats.py\",\n    arguments=[\"--input\", blob_input_data, \"--output\", output_data1],\n    inputs=[blob_input_data],\n    outputs=[output_data1],\n    compute_target=compute_target,\n    source_directory=\"..\/dogs-vs-cats\",\n    runconfig=myenv\n)\n<\/code><\/pre>\n\n<p>Right now you're defining the environment, but no using it anywhere it seems. If your trouble persists maybe try defining your RunConfiguration like they do under the \"Specify the environment to run the script\" step in this notebook:<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.7,
        "Solution_reading_time":14.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":244.3194444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Is it possible to test locally SageMaker Inference Pipelines? I would like to be able to easily troubleshoot and find the appropriate serialization between containers",
        "Challenge_closed_time":1601037561000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600158011000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on whether it is possible to test SageMaker Inference Pipelines locally to troubleshoot and find the appropriate serialization between containers.",
        "Challenge_last_edit_time":1668454837872,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU8R_MjbU1QPm66SCgld4spQ\/is-it-possible-to-test-locally-sagemaker-inference-pipelines",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":2.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":244.3194444444,
        "Challenge_title":"Is it possible to test locally SageMaker Inference Pipelines?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":340.0,
        "Challenge_word_count":33,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"If you are referring to using local mode via the SM PySDK, then pipeline deployment is not supported.\n\nAs an alternative, given your three inference containers, you could manually run the services locally and then implement a kind of facade function that invokes the three services in pipeline and manages input\/output accordingly.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1609868433251,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":4.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":52.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.7391458334,
        "Challenge_answer_count":1,
        "Challenge_body":"I had a Sagemaker user I wasn't using, so I tried to delete it and initially came across this tutorial:\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-delete-domain.html . As part of the larger process for how to delete a domain, it shows how to delete any user profiles within that domain. The steps are:\n* Choose the user.\n* On the User Details page, for each non-failed app in the Apps list, choose Delete app.\n* On the Delete app dialog, choose Yes, delete app, type delete in the confirmation field, and then choose Delete.\n* When the Status for all apps show as Deleted, choose Delete user.\n\nThe problem comes on the final step: I wasn't able to find a \"Delete user\" button. This feels like a bug, because without such a button the only way to stop charges on a Sagemaker user is to use the CLI, which I eventually did. You can only delete the domain if you have deleted all users, meaning it only works using the CLI for that as well. For every other AWS service I've used, there is an easy way to delete everything from the GUI.",
        "Challenge_closed_time":1640094687751,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640092026826,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an issue while trying to delete a Sagemaker user profile. The tutorial provided by AWS shows how to delete user profiles within a domain, but the final step of deleting the user profile does not have a \"Delete user\" button. This forces the user to use the CLI to stop charges on the user profile and delete the domain. The user suggests that this may be a bug and highlights the lack of an easy way to delete everything from the GUI, unlike other AWS services.",
        "Challenge_last_edit_time":1668549723952,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUmSJa7T1nRm6PQkiXDxZJqA\/closing-up-a-sagemaker-user-profile-intended-behavior",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":13.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.7391458334,
        "Challenge_title":"Closing up a Sagemaker user profile - intended behavior?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1149.0,
        "Challenge_word_count":190,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Try clicking on the user, then Edit, and then Delete? I don't remember if that is the exact flow, but I do know that you can do it in the GUI. I've done it a few times.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1640094687751,
        "Solution_link_count":0.0,
        "Solution_readability":3.4,
        "Solution_reading_time":1.94,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1466783310736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":2701.0,
        "Answerer_view_count":260.0,
        "Challenge_adjusted_solved_time":27.5513030556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to train YOLOv5 on aws sagemaker also deploy the model on sagemaker itself,need to know about entrypoint python script as well. how can I build a pipeline for this?<\/p>",
        "Challenge_closed_time":1632853607047,
        "Challenge_comment_count":1,
        "Challenge_created_time":1632686467053,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know how to train and deploy YOLOv5 on AWS Sagemaker, including information on the entrypoint python script and how to build a pipeline for the process.",
        "Challenge_last_edit_time":1632754422356,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69338516",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.0,
        "Challenge_reading_time":2.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":46.4277761111,
        "Challenge_title":"how to train and deploy YOLOv5 on aws sagemaker",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":2510.0,
        "Challenge_word_count":39,
        "Platform":"Stack Overflow",
        "Poster_created_time":1556022524712,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>This official AWS <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/speed-up-yolov4-inference-to-twice-as-fast-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">blog post<\/a> has information on how to deploy YOLOv4. I wonder if you can use it as a guide and change the model to v5.<\/p>\n<p>If not, there is a 3rd party implementation of YOLOv5 <a href=\"https:\/\/github.com\/HKT-SSA\/yolov5-on-sagemaker\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.7,
        "Solution_reading_time":5.89,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.361325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I don't know where else to ask this question so would appreciate any help or feedback. I've been reading the SDK documentation for azure machine learning service (in particular <code>azureml.core<\/code>). There's a class called <code>Pipeline<\/code> that has methdods <code>validate()<\/code> and <code>publish()<\/code>. Here are the docs for this:<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py<\/a><\/p>\n<p>When I call <code>validate()<\/code>, everything validates and I call publish but it seems to only create an API endpoint in the workspace, it doesn't register my pipeline under Pipelines and there's obviously nothing in the designer.<\/p>\n<p>My question: I want to publish my pipeline so I just have to launch from the workspace with one click. I've built it already using the SDK (Python code). I don't want to work with an API. Is there any way to do this or would I have to rebuild the entire pipeline using the designer (drag and drop)?<\/p>",
        "Challenge_closed_time":1595468157440,
        "Challenge_comment_count":1,
        "Challenge_created_time":1595466856670,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has built a pipeline in Python using the SDK for Azure Machine Learning Service and is trying to publish it to the workspace. They have used the Pipeline class with the validate() and publish() methods, but the pipeline is not registering under Pipelines and is only creating an API endpoint. The user wants to know if there is a way to publish the pipeline so that it can be launched from the workspace with one click, without having to rebuild the entire pipeline using the designer.",
        "Challenge_last_edit_time":1595544102140,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63045395",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":16.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.361325,
        "Challenge_title":"Machine learning in Azure: How do I publish a pipeline to the workspace once I've already built it in Python using the SDK?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":739.0,
        "Challenge_word_count":168,
        "Platform":"Stack Overflow",
        "Poster_created_time":1595292020127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Totally empathize with your confusion. Our team has been working with Azure ML pipelines for quite some time but <code>PublishedPipelines<\/code> still confused me initially because:<\/p>\n<ul>\n<li>what the SDK calls a <code>PublishedPipeline<\/code> is called as a <code>Pipeline Endpoint<\/code> in the Studio UI, and<\/li>\n<li>it is semi-related to <code>Dataset<\/code> and <code>Model<\/code>'s <code>.register()<\/code> method, but fundamentally different.<\/li>\n<\/ul>\n<p><code>TL;DR<\/code>: all <code>Pipeline.publish()<\/code> does is create an endpoint that you can use to:<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb\" rel=\"nofollow noreferrer\">schedule<\/a> and <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb\" rel=\"nofollow noreferrer\">version<\/a> Pipelines, and<\/li>\n<li>re-run the pipeline from other services via a REST API call (e.g. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/transform-data-machine-learning-service\" rel=\"nofollow noreferrer\">via Azure Data Factory<\/a>).<\/li>\n<\/ul>\n<p>You can see <code>PublishedPipelines<\/code> in the Studio UI in two places:<\/p>\n<ul>\n<li>Pipelines page :: Pipeline Endpoints tab<\/li>\n<li>Endpoints page :: Pipeline Endpoints tab<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/UQ6RS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/UQ6RS.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1595468750207,
        "Solution_link_count":5.0,
        "Solution_readability":17.5,
        "Solution_reading_time":22.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1492048364132,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cambridge, MA, USA",
        "Answerer_reputation_count":4436.0,
        "Answerer_view_count":713.0,
        "Challenge_adjusted_solved_time":0.2273408334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created an endpoint on us-east-1. try to create a predictor:<\/p>\n\n<pre><code>In [106]: sagemaker.predictor.RealTimePredictor(&lt;endpoint name&gt;)\n<\/code><\/pre>\n\n<p>and get<\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the DescribeEndpoint operation: \nCould not find endpoint \"arn:aws:sagemaker:us-east-2:&lt;account number&gt;:endpoint\/&lt;endpoint name&gt;\".\n<\/code><\/pre>\n\n<p>which is perfectly correct, since the endpoint is on us-east-1.  Probably I could change some defaults, but I'd rather not - I work on us-east-2 99% of the time.<\/p>\n\n<p>So, how can I set a different region when initializing the predictor?<\/p>",
        "Challenge_closed_time":1573497881147,
        "Challenge_comment_count":1,
        "Challenge_created_time":1573497062720,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an endpoint on us-east-1 and is trying to create a predictor but is encountering an error as the endpoint is not found in us-east-2. The user wants to know how to set a different region when initializing the predictor.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58806807",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.6,
        "Challenge_reading_time":9.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.2273408334,
        "Challenge_title":"Create a predictor from an endpoint in a different region",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":478.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553808322940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>The (python) <code>Predictors<\/code> <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"nofollow noreferrer\">documentation<\/a> shows that you can pass a <code>Session<\/code> object. In turn, the <code>Session<\/code> can be <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.Session\" rel=\"nofollow noreferrer\">initialized<\/a> with a <em>client<\/em> and a <em>runtime client<\/em> - the former does everything except endpoint invocations, the latter does... endpoint invocations.<\/p>\n\n<p>Those clients are tied to specific regions. It seems like you should be able to set the runtime client region to match your endpoint, by manually instantiating it, while leaving the regular client alone (disclaimer here: I haven't tried this - if you do, let me\/us know how it goes :)).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.1,
        "Solution_reading_time":10.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":94.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":100.5253722222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I have been working with ML Studio (classic) and facing a problem with &quot;Execute Python&quot; scripts. I have noticed that it takes time to perform some internal tasks after which it starts executing the actual Python code in ML Studio. This delay has caused an increased time of 40-60 seconds per module which is aggregating and causing a delay of 400-500 seconds per execution when consumed through Batch Execution System or on running the experiments manually. (I've multiple Modules of &quot;Execute Python&quot; scripts)  <\/p>\n<p>Can you please help understand the reason behind this or any optimization that can be done?  <\/p>\n<p>Regards,  <br \/>\nAnant<\/p>",
        "Challenge_closed_time":1594049839263,
        "Challenge_comment_count":2,
        "Challenge_created_time":1593687947923,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a problem with executing Python scripts in Azure Machine Learning Studio (classic) as it takes additional time to perform internal tasks before executing the actual code, causing a delay of 40-60 seconds per module and 400-500 seconds per execution. The user is seeking help to understand the reason behind this delay and any optimization that can be done.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/42134\/why-does-azure-machine-learning-studio-(classic)-t",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.0,
        "Challenge_reading_time":9.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":100.5253722222,
        "Challenge_title":"Why does Azure Machine Learning Studio (classic) take additonal time to execute python scripts?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":119,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I completely understand, sorry for the inconvenience, however, this is a known limitation as optimizing performance in Azure ML Studio (Classic) isn't supported. We recommend that customers use Designer for advanced capabilities and active updates\/improvements for the service. Thanks.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.6,
        "Solution_reading_time":3.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":29.6803536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Microsoft Azure Support,<\/p>\n<p>I am currently learning to use Azure Cloud, specifically Microsoft Azure Machine Learning Studio. I am using the example dataset &quot;Automobile price data&quot; and have successfully completed a classic training pipeline, which includes the following blocks: &quot;Select Columns in Dataset&quot;, &quot;Clean Missing Data&quot;, &quot;Normalize Data&quot;, &quot;Split Data&quot;, &quot;Linear Regression&quot;, &quot;Train Model&quot;, &quot;Score Model&quot;, and &quot;Evaluate Model&quot;. I have also successfully created a real-time inference pipeline, which includes an &quot;Enter Data Manually&quot; block, a &quot;Web Service Input&quot; block, an &quot;Execute Python Script&quot; block code, and a &quot;Web Service Output&quot; block.<\/p>\n<p>However, I am encountering an issue when I try to deploy the pipeline for inference using the &quot;Set up real-time endpoint&quot; window. Specifically, when I click on the &quot;Deploy&quot; button, nothing happens and the same window remains open. My setup is &quot;Deploy new real-time endpoint&quot; with the following details: Name: predict-auto-price, Description: Auto Price Regression, Compute type: Azure Container Instance.<\/p>\n<p>I have tried creating a new Workspace, as well as checking the access policies and permissions on the resources used in the pipeline and deployment, but the issue persists. I am using an Azure Free Account with 200 USD of credit, and I still have more than 190 USD of credit remaining.<\/p>\n<p>I would greatly appreciate your help in resolving this issue, as my goal is to finish the Coursera DP-100 specialization and apply for the Data Science Azure Certification.<\/p>\n<p>Thank you for your attention to this matter.<\/p>\n<p>--<\/p>\n<p>Carlos Alanis<\/p>\n<p>Attach some images: <\/p>\n<p><strong>Pipeline Completed without errors:<\/strong><\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/85dc896d-a94e-4372-8249-0b2de8dc9438?platform=QnA\" alt=\"real time inference pipeline completed\" \/><\/p>\n<p><strong>Coursera Instructions:<\/strong><\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/e1482fa9-bca7-44c0-a85a-4482807cbb95?platform=QnA\" alt=\"Coursera instructions\" \/><\/p>\n<p><strong>Set up real-time endpoint WINDOW:<\/strong> <\/p>\n<p><strong>DEPLOY BUTTON UNPRESSED:<\/strong> <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/75d73443-c414-4064-9a55-3589092a87c1?platform=QnA\" alt=\"set up real time endpoint - unpressed button\" \/><\/p>\n<p><strong>DEPLOY BUTTON PRESSED:<\/strong><\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/3f429b37-2bd9-4012-8d6d-2afdbcdf6b14?platform=QnA\" alt=\"set up real time endpoint - PRESSED DEPLOY button\" \/><\/p>\n<p><strong>BUT NOTHING HAPPENS!!!<\/strong> <\/p>\n<p><strong>ALSO I SHARE YOU THE WORKSPACE OVERVIEW:<\/strong> <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/cfd21661-f06c-4cd5-97ab-1ad2b953230e?platform=QnA\" alt=\"workspace overview\" \/><\/p>\n<p>I hope that some people of the expert team can Help me, Thank you! <\/p>",
        "Challenge_closed_time":1683799513366,
        "Challenge_comment_count":11,
        "Challenge_created_time":1683692664093,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to deploy a real-time inference pipeline using Microsoft Azure Machine Learning Studio. Specifically, when the user clicks on the \"Deploy\" button in the \"Set up real-time endpoint\" window, nothing happens and the same window remains open. The user has tried creating a new Workspace and checking the access policies and permissions on the resources used in the pipeline and deployment, but the issue persists. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1281429\/issues-for-implementing-a-real-time-inference-pipe",
        "Challenge_link_count":5,
        "Challenge_participation_count":12,
        "Challenge_readability":13.9,
        "Challenge_reading_time":41.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":29.6803536111,
        "Challenge_title":"Issues for implementing a real-time inference pipeline! The DEPLOY button does nothing!",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":343,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=8f9f275d-8a35-4900-94e8-03effa4fecb8\">@Carlos Alanis  <\/a> <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=d8849772-db6e-4cbc-aac4-7e2bbe39dc8a\">Rania Boukhriss<\/a> <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=c23a88da-a1fe-4209-9289-829c28dfa73b\">Carlos Garc\u00eda Gonz\u00e1lez<\/a> <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=259d2834-c6dc-4038-b068-fe422681470d\">anzilparviz<\/a> <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=f1b6c4a4-3330-4115-9b7a-eb63e63a4faa\">SpaceBuddha<\/a> <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=c057ba5f-2c81-43c2-a518-cd25ef60490f\">jaime reinoso<\/a> <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=a5ece606-8fef-4154-982f-69ded88859d4\">Cesar Olivares Espinosa<\/a> The hotfix deployment is complete. Could you please retry the Deployment action? If you still face any issues please do let us know the region of your workspace. Thanks!! <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/3343ee39-8aff-4859-9a73-1afd64140fad?platform=QnA\" alt=\"test_deploy_hotfix2\" \/><\/p>\n<p>If this answers your query, do click <code>Accept Answer<\/code> and <code>Yes<\/code> for was this answer helpful. And, if you have any further query do let us know.<\/p>\n",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":14.2,
        "Solution_reading_time":17.89,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":127.0472208333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is a fundamental AWS Sagemaker question. When I run training with one of Sagemaker's built in algorithms I am able to take advantage of the massive speedup from distributing the job to many instances by increasing the instance_count argument of the training algorithm. However, when I package my own custom algorithm then increasing the instance count seems to just duplicate the training on every instance, leading to no speedup. <\/p>\n\n<p>I suspect that when I am packaging my own algorithm there is something special I need to do to control how it handles the training differently for a particular instance inside of the my custom train() function (otherwise, how would it know how the job should be distributed?), but I have not been able to find any discussion of how to do this online. <\/p>\n\n<p>Does anyone know how to handle this? Thank you very much in advance.<\/p>\n\n<p>Specific examples:\n=> It works well in a standard algorithm: I verified that increasing train_instance_count in the first documented sagemaker example speeds things up here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a><\/p>\n\n<p>=> It does not work in my custom algorithm. I tried taking the standard sklearn build-your-own-model example and adding a few extra sklearn variants inside of the training and then printing out results to compare. When I increase the train_instance_count that is passed to the Estimator object, it runs the same training on every instance, so the output gets duplicated across each instance (the printouts of the results are duplicated) and there is no speedup.\nThis is the sklearn example base: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a> . The third argument of the Estimator object partway down in this notebook is what lets you control the number of training instances.<\/p>",
        "Challenge_closed_time":1517706762272,
        "Challenge_comment_count":1,
        "Challenge_created_time":1517249392277,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in taking advantage of the speedup from distributing the job to many instances by increasing the instance count argument of the training algorithm when using a custom algorithm in AWS Sagemaker. Increasing the instance count duplicates the training on every instance, leading to no speedup. The user suspects that there is something special they need to do to control how the training is handled differently for a particular instance inside their custom train() function. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48507471",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":14.0,
        "Challenge_reading_time":29.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":127.0472208333,
        "Challenge_title":"AWS Sagemaker custom user algorithms: how to take advantage of extra instances",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1369.0,
        "Challenge_word_count":295,
        "Platform":"Stack Overflow",
        "Poster_created_time":1368093601223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Distributed training requires having a way to sync the results of the training between the training workers. Most of the traditional libraries, such as scikit-learn are designed to work with a single worker, and can't just be used in a distributed environment. Amazon SageMaker is distributing the data across the workers, but it is up to you to make sure that the algorithm can benefit from the multiple workers. Some algorithms, such as Random Forest, are easier to take advantage of the distribution, as each worker can build a different part of the forest, but other algorithms need more help. <\/p>\n\n<p>Spark MLLib has distributed implementations of popular algorithms such as k-means, logistic regression, or PCA, but these implementations are not good enough for some cases. Most of them were too slow and some even crushed when a lot of data was used for the training. The Amazon SageMaker team reimplemented many of these algorithms from scratch to benefit from the scale and economics of the cloud (20 hours of one instance costs the same as 1 hour of 20 instances, just 20 times faster). Many of these algorithms are now more stable and much faster beyond the linear scalability. See more details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a><\/p>\n\n<p>For the deep learning frameworks (TensorFlow and MXNet) SageMaker is using the built-in parameters server that each one is using, but it is taking the heavy lifting of the building the cluster and configuring the instances to communicate with it. <\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.1,
        "Solution_reading_time":20.32,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":249.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":27.8660444444,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>I made a change to my script and now I have to manually synchronise my runs, my script contains<\/p>\n<pre><code>if args.dry_run:\n    os.environ['WANDB_MODE'] = 'dryrun'\n\nwandb.init(project=args.project_name, notes=args.notes)\n\n# log all experimental args to wandb\nwandb.config.update(args)\n<\/code><\/pre>\n<p>The change I made was the first line, setting <code>WANDB_MODE=dryrun<\/code>. From that point on I cannot re-enable automatic synchronisation.<\/p>\n<p>I\u2019ve run <code>wandb online<\/code> and run my script with <code>dryrun=False<\/code>. I also realised that this doesn\u2019t unset WANDB_MODE so I tried setting it to \u2018online\u2019 when <code>dryrun==False<\/code>. But it always ends up logging to <code>wandb\/offline-run-*<\/code> and I have to manually sync it.<\/p>\n<p>Is there another step to re-enable sync\u2019ing?<\/p>",
        "Challenge_closed_time":1662355289728,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662254971968,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user made a change to their script by setting WANDB_MODE=dryrun, which disabled automatic synchronization. They have tried running the script with dryrun=False and setting WANDB_MODE to 'online', but it still logs to wandb\/offline-run-* and requires manual synchronization. The user is seeking advice on how to re-enable automatic synchronization.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-reenable-automatic-synchronisation\/3061",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":9.7,
        "Challenge_reading_time":10.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":27.8660444444,
        "Challenge_title":"How to reenable automatic synchronisation",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":161.0,
        "Challenge_word_count":105,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I\u2019ve found a way around this - I\u2019m not really sure why it\u2019s happening but I noticed that the huggingface trainer logs the metrics at the end of training as follows:<\/p>\n<pre><code>                if not args.load_best_model_at_end\n                else {\n                    f\"eval\/{args.metric_for_best_model}\": state.best_metric,\n                    \"train\/total_floss\": state.total_flos,\n                }\n<\/code><\/pre>\n<p>Meaning it logs the validation loss, but only if you train with <code>load_best_model_at_end=True<\/code> and set <code>save_strategy==evaluation_strategy<\/code> (epoch or steps) and <code>save_steps=eval_steps<\/code>.<\/p>\n<p>Doing this means I didn\u2019t need to perform the separate eval step since it\u2019s already logged the evaluation loss from the best model during training.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.4,
        "Solution_reading_time":9.24,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":86.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":788.5729277778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi Team,     <\/p>\n<p>When I Submit the Batch Inference Pipeline. It is working.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/158498-1-image-designer.png?platform=QnA\" alt=\"158498-1-image-designer.png\" \/>    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/158597-2-image-designer.png?platform=QnA\" alt=\"158597-2-image-designer.png\" \/>    <\/p>\n<p>After submitting, I can see the file:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/158519-3-image-designer.png?platform=QnA\" alt=\"158519-3-image-designer.png\" \/>    <\/p>\n<p>Then when I Publish, the file is not in the Datastore. The file is not generated again. I didn't get an error.    <\/p>\n<p>Kind regards,     <br \/>\nAnaid    <\/p>",
        "Challenge_closed_time":1642583796710,
        "Challenge_comment_count":2,
        "Challenge_created_time":1639744934170,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the AML Designer - Batch Inference Pipeline where the file generated after submitting is not available in the Datastore after publishing, without any error message.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/667479\/problem-aml-designer-batch-inference-pipeline",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":14.2,
        "Challenge_reading_time":10.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":788.5729277778,
        "Challenge_title":"Problem: AML Designer - Batch Inference Pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":67,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=4bb27b25-616e-491c-b986-136b5bf96f77\">@Anaid  <\/a>     <\/p>\n<p>Hi,    <\/p>\n<p>I\u2019ve enabled one-time Free Technical Support for you.  To create the support request, please do the following:     <\/p>\n<p>\u2022            Go to the Health Advisory section within the Azure Portal: <a href=\"https:\/\/aka.ms\/healthadvisories\">https:\/\/aka.ms\/healthadvisories<\/a>      <br \/>\n\u2022            Select the Issue Name &quot;You have been enabled for one-time Free Technical Support&quot;     <br \/>\n\u2022            Details will populate below in the Summary Tab within the reading pane and you can click on the link &quot;Create a Support Request&quot; to the right of the message    <\/p>\n<p>Let me know what your support request number is so that I can keep track of your case. If you run into any issues, feel free to let me know.    <\/p>\n<p>Regards,    <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":10.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":116.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.2302777778,
        "Challenge_answer_count":1,
        "Challenge_body":"Does SageMaker pipe mode serve as a cost saving measure? Or is is just faster than file mode but generally not much cheaper? The cost savings of it might be 1. no need to copy data to training instances and 2. training instances need less space. Are these savings generally significant for customers?",
        "Challenge_closed_time":1590165887000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590161458000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning whether SageMaker pipe mode is a cost-saving measure or just faster than file mode. They are considering the potential cost savings of not needing to copy data to training instances and requiring less space on training instances, but are unsure if these savings are significant for customers.",
        "Challenge_last_edit_time":1668521878124,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QURbsBp9m5TsqKWWDdP8VJyw\/sagemaker-pipe-mode",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.2,
        "Challenge_reading_time":3.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.2302777778,
        "Challenge_title":"SageMaker Pipe Mode",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":67.0,
        "Challenge_word_count":55,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"To the best of my understanding, pipe mode decreases startup times, but frequently increases the bill.\n\nThe SageMaker billing starts after the data has been copied onto the container in File mode and control is transferred to the user script. \n\nReading the data in pipe mode starts after control is transferred, so the data transfer happens during the billable time. \n\nFurther the data is, to the best of my knowledge, not hitting the disk (EBS). This is fast, but also means that if you pass over your data multiple times, you have to re-read it again, on your dime (S3 requests and container wait times).\n\nPipe mode is still a good idea. For example if you have only few passes over the data and the data is rather large, so that it would not fit on an EBS volume.\n\nAlso, in PyTorch for example, data loading can happen in parallel. So while the GPU is chucking away on one batch, the CPUs load and prepare the data for the next batch.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925574687,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":11.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":171.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":93.9451402778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Apologies for the long post.<\/p>\n<p>Originally, I had data in one location on an S3 bucket and used to train deep learning image classification models on this data using the typical 'File' mode and passing the S3 uri where the data is stored as training input. To try and accelerate training, I wanted to switch to using:<\/p>\n<ol>\n<li>Pipe mode, to stream data and not download all the data at the beginning of the training, starting training faster and saving disk space.<\/li>\n<li>Augmented Manifest File coupled with 1., so that I don't have to place my data in a single location on S3, so I avoid moving data around when I train models.<\/li>\n<\/ol>\n<p>I was making my script similar to <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=934156#934156\" rel=\"nofollow noreferrer\">the one in this example<\/a>. I printed the steps done when parsing the data, however I noticed that the data might not have been read because when printing it shows the following:<\/p>\n<pre><code>step 1 Tensor(&quot;ParseSingleExample\/ParseExample\/ParseExampleV2:0&quot;, shape=(), dtype=string)\nstep 2 Tensor(&quot;DecodePng:0&quot;, shape=(None, None, 3), dtype=uint8)\nstep 3 Tensor(&quot;Cast:0&quot;, shape=(None, None, 3), dtype=float32)\n<\/code><\/pre>\n<p>I guess the image is not being read\/found since the shape is <code>[None, None, 3]<\/code> when it should be <code>[224, 224, 3]<\/code>, so maybe the problem is from the Augmented Manifest file?<\/p>\n<p>Here's an example of how my Augmented Manifest file is written:<\/p>\n<pre><code>{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image1.png&quot;, &quot;label&quot;: 1}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image2.png&quot;, &quot;label&quot;: 2}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image3.png&quot;, &quot;label&quot;: 3}\n<\/code><\/pre>\n<p>Some other details I should probably mention:<\/p>\n<ol>\n<li>When I create the Training Input I pass <code>'content_type': 'application\/x-recordio', 'record_wrapping': 'RecordIO'<\/code>, even though my data are in .png format, but I assumed that as the augmented manifest file is read the data get wrapped in the RecordIO format.<\/li>\n<li>Following my first point, I pass <code>PipeModeDataset(channel=channel, record_format='RecordIO')<\/code>, so also not sure about the RecordIO thing.<\/li>\n<\/ol>\n<p>There isn't an actual error that is raised, just when I start fitting the model nothing happens, it keeps on running but nothing actually runs so I'm trying to find the issue.<\/p>\n<hr \/>\n<p>EDIT: It now reads the shape correctly, but there's still the issue where it enters the .fit method and does nothing, just keeps running without doing anything. Find part of the script below.<\/p>\n<pre><code>def train_input_fn(train_channel):\n    &quot;&quot;&quot;Returns input function that feeds the model during training&quot;&quot;&quot;\n    return _input_fn(train_channel)\n\ndef _input_fn(channel):\n    &quot;&quot;&quot;\n        Returns a Dataset which reads from a SageMaker PipeMode channel.\n    &quot;&quot;&quot;\n    \n    features = {\n        'image-ref': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([3], tf.int64),\n    }\n \n    def combine(records):\n        return records[0], records[1]\n \n    def parse(record):\n        \n        parsed = tf.io.parse_single_example(record, features)\n        \n                 \n\n        image = tf.io.decode_png(parsed[&quot;image-ref&quot;], channels=3, dtype=tf.uint8)\n        image = tf.reshape(image, [224, 224, 3])\n        \n        lbl = parsed['label']\n        print(image, lbl)\n        return (image, lbl)\n \n    ds = PipeModeDataset(channel=channel, record_format='RecordIO')\n    ds = ds.map(parse, num_parallel_calls=AUTOTUNE)\n    ds = ds.prefetch(AUTOTUNE)\n \n    return ds\n\ndef model(dataset):\n    &quot;&quot;&quot;Generate a simple model&quot;&quot;&quot;\n    inputs = Input(shape=(224, 224, 3))\n    prediction_layer = Dense(2, activation = 'softmax')\n\n\n    x = inputs\n    x = tf.keras.applications.mobilenet.MobileNet(include_top=False, input_shape=(224,224,3), weights='imagenet')(x)\n    outputs = prediction_layer(x)\n    rec_model = tf.keras.Model(inputs, outputs)    \n    \n    rec_model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        metrics=['accuracy']\n    )\n    \n    \n    rec_model.fit(\n        dataset\n    )\n\n    return rec_model\n\ndef main(params):\n    \n    epochs = params['epochs']\n    train_channel = params['train_channel']\n    record_format = params['record_format']\n    batch_size = params['batch_size']\n        \n    train_spec = train_input_fn(train_channel)\n    model_classifier = model(train_spec)\n<\/code><\/pre>",
        "Challenge_closed_time":1647607100132,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647258085310,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to switch to using Pipe mode and Augmented Manifest File to train deep learning image classification models on data stored in different locations on an S3 bucket. However, the user is facing issues with the image not being read\/found, and the model not fitting, even though there are no errors raised. The user suspects that the issue might be with the Augmented Manifest file. The user has provided details about the Augmented Manifest file and the code used for training.",
        "Challenge_last_edit_time":1647268897627,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71467176",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":58.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":96.9485616667,
        "Challenge_title":"How can I verify that my training job is reading the augmented manifest file?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":127.0,
        "Challenge_word_count":493,
        "Platform":"Stack Overflow",
        "Poster_created_time":1576016596283,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Beirut, Lebanon",
        "Poster_reputation_count":15.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>From <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<blockquote>\n<p>A PipeModeDataset can read TFRecord, RecordIO, or text line records.<\/p>\n<\/blockquote>\n<p>While your'e trying to read binary (PNG) files. I don't see a relevant <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions\/tree\/tf-2\/src\/pipemode_op\/RecordReader\" rel=\"nofollow noreferrer\">record reader here<\/a> to help you do that.<br \/>\nYou could build your own format pipe implementation like shown <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">here<\/a>, but it's considerably more effort.<\/p>\n<p>Alternatively, you mentioned your files are scattered in different folders, but if your files common path contains less than 2M files, you could use <a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/10\/amazon-sagemaker-fast-file-mode\/\" rel=\"nofollow noreferrer\">FastFile mode<\/a> to <strong>stream<\/strong> data. Currently, FastFile only supports an S3 Prefix, so you won't be able to use a manifest.<\/p>\n<p>Also see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/choose-the-best-data-source-for-your-amazon-sagemaker-training-job\/\" rel=\"nofollow noreferrer\">general pros\/cons discussion of the different available storage and input types available in SageMaker<\/a>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":15.7,
        "Solution_reading_time":19.46,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":128.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":72.1441666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nThe plugin does not work with projects created with ``kedro==0.18.1``\r\n\r\n## Context\r\n\r\nTry to launch ``kedro run`` in a project with ``kedro==0.18.1`` and kedro-mlflow installed.\r\n\r\n\r\n## Steps to Reproduce\r\n\r\n```python\r\nconda create -n temp python=3.8 -y\r\nconda activate temp\r\npip install kedro==0.18.1 kedro-mlflow==0.9.0\r\nkedro new --starter=pandas-iris\r\ncd pandas-iris\r\nkedro mlflow init\r\nkedro run\r\n```\r\n\r\n## Expected Result\r\n\r\nThis should run the pipeleine and log the parameters.\r\n\r\n## Actual Result\r\n\r\nThis raises the following error:\r\n\r\n```bash\r\nAttributeError: module 'kedro.framework.session.session' has no attribute '_active_session'\r\n```\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): ``kedro==0.18.1`` and ``kedro-mlflow<=0.9.0``\r\n* Python version used (`python -V`): All\r\n* Operating system and version: All\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nCurrently, kedro-mlflow uses [the private ``_active_session`` global variable to access the configuration](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/e855f59faa76c881b32616880608d41c064c23a0\/kedro_mlflow\/config\/kedro_mlflow_config.py#L233-L247) inside a hook. \r\n\r\nWith kedro==0.18.1, this private attribute was removed and the new recommandation is to use the ``after_context_created`` hook. \r\n\r\nRetrieving the configuration and set it up should be moved to this new hook:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/963c338d6259dd118232c45801abe0a2b0a463df\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L108-L109",
        "Challenge_closed_time":1652640252000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1652380533000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the kedro-mlflow plugin not working with projects created with kedro==0.18.1. When the user tries to run the pipeline, an error is raised due to the removal of the private attribute '_active_session' in kedro==0.18.1. The solution is to use the 'after_context_created' hook to retrieve and set up the configuration.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/309",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":21.98,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":72.1441666667,
        "Challenge_title":"kedro-mlflow is broken with kedro==0.18.1",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":185,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Closed by #313 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-2.7,
        "Solution_reading_time":0.18,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":2.1417511111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have defined an Azure Machine Learning Pipeline with three steps:<\/p>\n<pre><code>e2e_steps=[etl_model_step, train_model_step, evaluate_model_step]\ne2e_pipeline = Pipeline(workspace=ws, steps = e2e_steps)\n<\/code><\/pre>\n<p>The idea is to run the Pipeline in the given sequence:<\/p>\n<ol>\n<li>etl_model_step<\/li>\n<li>train_model_step<\/li>\n<li>evaluate_model_step<\/li>\n<\/ol>\n<p>However, my experiment is failing because it is trying to execute evaluate_model_step before train_model_step:\n<a href=\"https:\/\/i.stack.imgur.com\/0fO0t.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0fO0t.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>How do I enforce the sequence of execution?<\/p>",
        "Challenge_closed_time":1624544151467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624536441163,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in organizing the sequence of execution for their Azure Machine Learning Pipeline with three steps, as the experiment is failing due to the pipeline trying to execute a step before its prerequisite. The user is seeking guidance on how to enforce the sequence of execution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68115476",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.6,
        "Challenge_reading_time":10.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2.1417511111,
        "Challenge_title":"How to organize one step after another in Azure Machine Learning Pipelines?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":515.0,
        "Challenge_word_count":78,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p><code>azureml.pipeline.core.StepSequence<\/code> lets you do exactly that.<\/p>\n<blockquote>\n<p>A StepSequence can be used to easily run steps in a specific order, without needing to specify data dependencies through the use of PipelineData.<\/p>\n<\/blockquote>\n<p>See <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.stepsequence?view=azure-ml-py\" rel=\"nofollow noreferrer\">the docs<\/a> to read more.<\/p>\n<p>However, the preferable way to have steps run in order is stitching them together via <code>PipelineData<\/code> or <code>OutputFileDatasetConfig<\/code>. In your example, does the <code>train_step<\/code> depend on outputs from the <code>etl step<\/code>? If so, consider having that be the way that steps are run in sequence. For more info see <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-data-in-out-of-pipelines\" rel=\"nofollow noreferrer\">this tutorial<\/a> for more info<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":12.71,
        "Solution_score_count":3.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":98.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1610703423912,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":1452.9271638889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using SageMaker pipeline to do inference on test data. The Pipeline uses a SKLearn perprocessor and a XGBoost model. The pipeline works fine on data without an ID column. However, when I try to include an ID column to track the predictions, it fails. I have given the code snippets below.<\/p>\n<pre><code>import sagemaker\nfrom sagemaker.predictor import json_serializer, csv_serializer, json_deserializer\n\ninput_data_path = 's3:\/\/batch-transform\/input-data\/validation_data.csv'\noutput_data_path = 's3:\/\/batch-transform\/predictions\/'\n\ntransform_job = sagemaker.transformer.Transformer(\n    model_name = model_name,\n    instance_count = 1,\n    instance_type = 'ml.m4.xlarge',\n    strategy = 'MultiRecord',\n    assemble_with = 'Line',\n    output_path = output_data_path,\n    base_transform_job_name='pipeline_with_id',\n    sagemaker_session=sagemaker.Session(),\n    accept = 'text\/csv')\n\ntransform_job.transform(data = input_data_path,\n                        content_type = 'text\/csv', \n                        split_type = 'Line',\n                        input_filter='$[1:]', \n                        join_source='Input')\n                        output_filter='$[0,-1]')\n<\/code><\/pre>\n<p>This results in the following error:<\/p>\n<pre><code>Fail to join data: mismatched line count between the input and the output\n<\/code><\/pre>\n<p>I am following the example given in this page:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/<\/a><\/p>\n<p>Can someone provide pointers to what is causing the error? Thank you<\/p>",
        "Challenge_closed_time":1610703448710,
        "Challenge_comment_count":3,
        "Challenge_created_time":1605472910920,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while using SageMaker Batch Transform to do inference on test data. The pipeline works fine on data without an ID column, but when an ID column is included to track the predictions, it fails with a \"mismatched line count\" error. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64849557",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":17.7,
        "Challenge_reading_time":22.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1452.9271638889,
        "Challenge_title":"SageMaker Batch Transform fails with ID Column",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1080.0,
        "Challenge_word_count":144,
        "Platform":"Stack Overflow",
        "Poster_created_time":1319019150600,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3073.0,
        "Poster_view_count":341.0,
        "Solution_body":"<p>Came across the same issue.<\/p>\n<p>Check the number of rows returned after prediction in your serving code. In my case, my prediction output didn't have a column header.<\/p>\n<p>e.g. As a text\/csv response, using batch transform with join will post join the input &amp; output.<\/p>\n<p>A single input record would be [[&quot;feature_1&quot;, &quot;feature_2&quot;],[0, 1]], while my model predicted output returned [1].<\/p>\n<p>add column name to predicted output like this [&quot;result&quot;, 1] then returning the csv result will yield [[&quot;result&quot;],[1]] matching input.<\/p>\n<p>P.S. you may need to find a scalable way of doing this for multi-row  batch. Not sure.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":8.53,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":98.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1450889293150,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Finland",
        "Answerer_reputation_count":398.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":0.0838130556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to use sentence_transformers in AML to run XLM-Roberta model for sentence embedding. I have a script in which I import sentence_transformers:<\/p>\n<pre><code>from sentence_transformers import SentenceTransformer\n<\/code><\/pre>\n<p>Once I run my AML pipeline, the run fails on this script with the following error:<\/p>\n<pre><code>AzureMLCompute job failed.\nUserProcessKilledBySystemSignal: Job failed since the user script received system termination signal usually due to out-of-memory or segfault.\n    Cause: segmentation fault\n    TaskIndex: \n    NodeIp: #####\n    NodeId: #####\n<\/code><\/pre>\n<p>I'm pretty sure that this import is causing this error, because if I comment out this import, the rest of the script will run.\nThis is weird because the installation of the sentence_transformers succeed.<\/p>\n<p>This is the details of my compute:<\/p>\n<pre><code>Virtual machine size\nSTANDARD_NV24 (24 Cores, 224 GB RAM, 1440 GB Disk)\nProcessing Unit\nGPU - 4 x NVIDIA Tesla M60\n<\/code><\/pre>\n<p>Agent Pool:<\/p>\n<pre><code>Azure Pipelines\n<\/code><\/pre>\n<p>Agent Specification:<\/p>\n<pre><code>ubuntu-16.04\n<\/code><\/pre>\n<p>requirements.txt file:<\/p>\n<pre><code>torch==1.4.0\nsentence-transformers\n<\/code><\/pre>\n<p>Does anyone have a solution for this error?<\/p>",
        "Challenge_closed_time":1606866538208,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606861020137,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a segmentation fault error while trying to import sentence_transformers in Azure Machine Learning Service Nvidia Compute. The error occurs when running the AML pipeline and is likely caused by the import statement. The user has confirmed that the installation of sentence_transformers was successful and has provided details of their compute, agent pool, agent specification, and requirements.txt file. The user is seeking a solution to this error.",
        "Challenge_last_edit_time":1606866767496,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65099376",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":17.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.5327975,
        "Challenge_title":"Segmentation fault error in importing sentence_transformers in Azure Machine Learning Service Nvidia Compute",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":530.0,
        "Challenge_word_count":168,
        "Platform":"Stack Overflow",
        "Poster_created_time":1450889293150,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Finland",
        "Poster_reputation_count":398.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>I fixed the issue by changing the pytorch version from 1.4.0 to 1.6.0.\nSo the requirements.txt looks like this:<\/p>\n<pre><code>torch==1.6.0\nsentence-transformers\n<\/code><\/pre>\n<p>At first I tried one of the older versions of sentence-transformers which was compatible with pytorch 1.4.0. But the older version doesn't support &quot;xml-roberta-base&quot; model, so I tried to upgrade the pytorch version.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1606867069223,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":5.24,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1548188011640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bellevue, WA, USA",
        "Answerer_reputation_count":131.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":102.9447575,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using Azure Machine Learning Service with the azureml-sdk python library.<\/p>\n\n<p>I'm using azureml.core version 1.0.8<\/p>\n\n<p>I'm following this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline<\/a> tutorial.<\/p>\n\n<p>I've got it working when I use Azure Compute resources. But I would like to run it locally.<\/p>\n\n<p>I get the following error<\/p>\n\n<pre><code>raise ErrorResponseException(self._deserialize, response)\nazureml.pipeline.core._restclients.aeva.models.error_response.ErrorResponseException: (BadRequest) Response status code does not indicate success: 400 (Bad Request).\nTrace id: [uuid], message: Can't build command text for [train.py], moduleId [uuid] executionId [id]: Assignment for parameter Target is not specified\n<\/code><\/pre>\n\n<p>My code looks like:<\/p>\n\n<pre><code>run_config = RunConfiguration()\ncompute_target = LocalTarget()\nrun_config.target = LocalTarget()    \nrun_config.environment.python.conda_dependencies = CondaDependencies(conda_dependencies_file_path='environment.yml')\nrun_config.environment.python.interpreter_path = 'C:\/Projects\/aml_test\/.conda\/envs\/aml_test_env\/python.exe'\nrun_config.environment.python.user_managed_dependencies = True\nrun_config.environment.docker.enabled = False\n\ntrainStep = PythonScriptStep(\n    script_name=\"train.py\",\n    compute_target=compute_target,\n    source_directory='.',\n    allow_reuse=False,\n    runconfig=run_config\n)\n\nsteps = [trainStep]\n\n# Build the pipeline\npipeline = Pipeline(workspace=ws, steps=[steps])\npipeline.validate()\n\nexperiment = Experiment(ws, 'Test')\n\n# Fails, locally, works on Azure Compute\nrun = experiment.submit(pipeline)\n\n\n# Works both locally and on Azure Compute\nsrc = ScriptRunConfig(source_directory='.', script='train.py', run_config=run_config)\nrun = experiment.submit(src)\n<\/code><\/pre>\n\n<p>The <code>train.py<\/code> is a very simple self contained script only dependent on numpy that approximates pi.<\/p>",
        "Challenge_closed_time":1548188011640,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547817410513,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to run an Azure Machine Learning Service pipeline locally using the azureml-sdk python library. The error message indicates that the target parameter is not specified in the train.py script. The user's code includes a run configuration and a PythonScriptStep, and the train.py script is a simple self-contained script that approximates pi. The code works when using Azure Compute resources but fails when running locally.",
        "Challenge_last_edit_time":1554703788007,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54254830",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.2,
        "Challenge_reading_time":28.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":102.9447575,
        "Challenge_title":"Running Azure Machine Learning Service pipeline locally",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":2476.0,
        "Challenge_word_count":164,
        "Platform":"Stack Overflow",
        "Poster_created_time":1313998995867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3148.0,
        "Poster_view_count":347.0,
        "Solution_body":"<p>Local compute cannot be used with ML Pipelines. Please see this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-set-up-training-targets#supported-compute-targets\" rel=\"noreferrer\">article<\/a>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1548188395572,
        "Solution_link_count":1.0,
        "Solution_readability":23.7,
        "Solution_reading_time":3.2,
        "Solution_score_count":7.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":14.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":29.0836047222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I have been successfully building and operating machine learning pipelines with Azure SDK, but there is something I fail to fully understand, and I'm wondering if my code can be simplified in some way.<\/p>\n<p>Let's say I have a simple pipeline with two steps: the first step processes data located at 'training_data_path' in Blob storage and then saves it to the same location, and the second step reads that processed data to do something else. So my code is as follows:<\/p>\n<pre><code>def_data_store = ws.get_default_datastore()\ntraining_data_path = (def_data_store, 'training_data')\n\nstep_1_config = OutputFileDatasetConfig(destination = training_data_path)\nstep_2_config = OutputFileDatasetConfig(destination = training_data_path)\n\nstep_1 = PythonScriptStep(\n    name=&quot;Step 1&quot;,\n    script_name=&quot;step_1.py&quot;,\n    source_directory=&quot;.\/&quot;,\n    outputs=[step_1_config],\n    arguments = [\n        &quot;--training-data-path&quot;, step_1_config\n        ],    \n    compute_target=compute_target,\n    runconfig=aml_run_config,\n    allow_reuse=False\n)\n\nstep_2 = PythonScriptStep(\n    name=&quot;Step 2&quot;,\n    script_name=&quot;step_2.py&quot;,\n    source_directory=&quot;.\/&quot;,\n    inputs=[step_1_config.as_input('training_data')],\n    arguments = [\n        &quot;--training-data-path&quot;, step_2_config\n        ],    \n    compute_target=compute_target,\n    runconfig=aml_run_config,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p>I have two questions about that:<\/p>\n<p>1) Even though the path to the data is the same in each step, it seems like I have to create a separate OutputFileDatasetConfig object for each step. So if my pipeline has 10 steps, I will create step_1_config, step_2_config, step_3_config... Isn't there a way to reuse the same OutputFileDatasetConfig object for multiple steps?<\/p>\n<p>2) As far as I know, in step 2, I could delete the 'inputs' parameter and modify the 'arguments' parameter as follows, the result would be the same.<\/p>\n<pre><code>step_2 = PythonScriptStep(\n    name=&quot;Step 2&quot;,\n    script_name=&quot;step_2.py&quot;,\n    source_directory=&quot;.\/&quot;,\n    arguments = [\n        &quot;--training-data-path&quot;, step_1_config.as_input('training_data')\n        ],    \n    compute_target=compute_target,\n    runconfig=aml_run_config,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p>My question is: is there any difference when specifying the input using both the 'inputs' and 'arguments' parameters Vs. using only the 'arguments' parameter?<\/p>\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1654261344427,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654156643450,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is building a machine learning pipeline with Azure SDK and is encountering challenges in passing data between pipeline steps using OutputFileDatasetConfig. The user has two questions: whether it is possible to reuse the same OutputFileDatasetConfig object for multiple steps and whether there is any difference in specifying the input using both the 'inputs' and 'arguments' parameters Vs. using only the 'arguments' parameter.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/874019\/passing-data-between-azureml-pipeline-steps-with-o",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":32.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":29.0836047222,
        "Challenge_title":"Passing data between AzureML pipeline steps with OutputFileDatasetConfig: difference between 'inputs\/outputs' and 'arguments'?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":258,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@ThierryL-3166 I think the recommendation to use separate OutputFileDatasetConfig objects for different steps is to avoid concurrent writes to a single object. As stated in a note in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-data-in-out-of-pipelines#use-outputfiledatasetconfig-for-intermediate-data\">documentation<\/a>:    <\/p>\n<pre><code>Concurrent writes to a OutputFileDatasetConfig will fail. Do not attempt to use a single OutputFileDatasetConfig concurrently. Do not share a single OutputFileDatasetConfig in a multiprocessing situation, such as when using distributed training.  \n<\/code><\/pre>\n<p>If your steps do not run in parallel then you can try to use a single object and check though.    <\/p>\n<p>With respect to using inputs or arguments, If you are using the same for the same operation then arguments would pass the same as input to the script used in the same step and you would need to use an argparser to retrieve the value in the script. Whereas, inputs would provide the same value as the run objects context in the same script. The section <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-data-in-out-of-pipelines#access-datasets-within-your-script\">access datasets within script<\/a> provides an example here for a train and test dataset where train is passed with arguments and test with inputs.    <\/p>\n<pre><code>smaller_dataset = iris_dataset.take_sample(0.1, seed=seed) # 10%  \ntrain, test = smaller_dataset.random_split(percentage=0.8, seed=seed)  \n  \n# In pipeline definition script:  \n# Code for demonstration only: It would be very confusing to split datasets between `arguments` and `inputs`  \ntrain_step = PythonScriptStep(  \n    name=&quot;train_data&quot;,  \n    script_name=&quot;train.py&quot;,  \n    compute_target=cluster,  \n    arguments=['--training-folder', train.as_named_input('train').as_download()],  \n    inputs=[test.as_named_input('test').as_download()]  \n)  \n  \n# In pipeline script  \nparser = argparse.ArgumentParser()  \nparser.add_argument('--training-folder', type=str, dest='train_folder', help='training data folder mounting point')  \nargs = parser.parse_args()  \ntraining_data_folder = args.train_folder  \n  \ntesting_data_folder = Run.get_context().input_datasets['test']  \n<\/code><\/pre>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.2,
        "Solution_reading_time":29.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":233.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1586454383232,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":44.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":87.4057897222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am running the mlflow registry using <code>mlflow server<\/code> (<a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/model-registry.html<\/a>). The server runs fine. If the server crashes for any reason it restart automatically. But for the time of restart the server is not available.<\/p>\n\n<p>Is it possible to run multiple isntances in parallel behind a load balancer? Is this safe or could it be possible that there are any inconsistencies?<\/p>",
        "Challenge_closed_time":1588382828087,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588079911073,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is running MLFlow Registry using \"mlflow server\" and is facing an issue where the server restarts automatically in case of a crash, but during the restart, the server is not available. The user is wondering if it is possible to run multiple instances in parallel behind a load balancer and if it is safe to do so without any inconsistencies.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61481147",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":7.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":84.143615,
        "Challenge_title":"MLFlow Registry high availability",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":501.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378563249260,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":470.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>Yes, it's possible to have multiple instances of MLflow Tracker Service running behind a load balancer.<\/p>\n\n<p>Because the Tracking server is stateless, you could have multiple instances log to a replicated primary DB as a store. A second hot standby can take over if the primary fails.<\/p>\n\n<p>As for the documentation in how to set up replicated instances of your backend store will vary on which one you elect to use, we cannot definitely document all different scenarios and their configurations.<\/p>\n\n<p>I would check the respective documentation of your backend DB and load balancer for how to federate requests to multiple instances of an MLflow tracking server, how to failover to a hot standby or replicated DB, or how to configure a hot-standby replicated DB instance.<\/p>\n\n<p>The short of it: MLflow tracking server is stateless.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1588394571916,
        "Solution_link_count":0.0,
        "Solution_readability":12.1,
        "Solution_reading_time":10.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":135.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1443482042487,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"United States",
        "Answerer_reputation_count":1995.0,
        "Answerer_view_count":150.0,
        "Challenge_adjusted_solved_time":56.7013711111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using an Amazon SageMaker Notebook that has 72 cores and 144 GB RAM, and I carried out 2 tests with a sample of the whole data to check if the Dask cluster was working.<\/p>\n\n<p>The sample has 4500 rows and 735 columns from 5 different \"assets\" (I mean 147 columns for each asset). The code is filtering the columns and creating a feature matrix for each filtered Dataframe.<\/p>\n\n<p>First, I initialized the cluster as follows, I received 72 workers, and got 17 minutes of running. (I assume I created 72 workers with one core each.)<\/p>\n\n<pre><code>    from dask.distributed import Client, LocalCluster\n    cluster = LocalCluster(processes=True,n_workers=72,threads_per_worker=72)\n\n    def main():\n      import featuretools as ft\n      list_columns = list(df_concat_02.columns)\n\n      list_df_features=[]\n      from tqdm.notebook import tqdm\n\n      for asset in tqdm(list_columns,total=len(list_columns)):\n        dataframe = df_sma.filter(regex=\"^\"+asset, axis=1).reset_index()\n\n        es = ft.EntitySet()  \n        es = es.entity_from_dataframe(entity_id = 'MARKET', dataframe =dataframe, \n                                      index = 'index', \n                                      time_index = 'Date')\n        fm, features = ft.dfs(entityset=es, \n                              target_entity='MARKET',\n                              trans_primitives = ['divide_numeric'],\n                              agg_primitives = [],\n                              max_depth=1,\n                              verbose=True,\n                              dask_kwargs={'cluster': client.scheduler.address}\n\n                              )\n        list_df_features.append(fm)\n      return list_df_features\n\n    if __name__ == \"__main__\":\n        list_df = main()\n<\/code><\/pre>\n\n<p>Second, I initialized the cluster as follows, I received 9 workers, and got 3,5 minutes of running. (I assume I created 9 workers with 8 cores each.)<\/p>\n\n<pre><code>from dask.distributed import Client, LocalCluster\ncluster = LocalCluster(processes=True)\n\ndef main():\n  import featuretools as ft\n  list_columns = list(df_concat_02.columns)\n\n  list_df_features=[]\n  from tqdm.notebook import tqdm\n\n  for asset in tqdm(list_columns,total=len(list_columns)):\n    dataframe = df_sma.filter(regex=\"^\"+asset, axis=1).reset_index()\n\n    es = ft.EntitySet()  \n    es = es.entity_from_dataframe(entity_id = 'MARKET', dataframe =dataframe, \n                                  index = 'index', \n                                  time_index = 'Date')\n    fm, features = ft.dfs(entityset=es, \n                          target_entity='MARKET',\n                          trans_primitives = ['divide_numeric'],\n                          agg_primitives = [],\n                          max_depth=1,\n                          verbose=True,\n                          dask_kwargs={'cluster': client.scheduler.address}\n\n                          )\n    list_df_features.append(fm)\n  return list_df_features\n\nif __name__ == \"__main__\":\n    list_df = main()\n<\/code><\/pre>\n\n<p>For me, it's mind-blowing because I thought that 72 workers could carry the work out faster! Once I'm not a specialist neither in Dask nor in FeatureTools I guess that I'm setting something wrong.<\/p>\n\n<p>I would appreciate any kind of help and advice!<\/p>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1583793190592,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583540635953,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with Featuretools slowing down when increasing the number of Dask workers. They conducted two tests with a sample of the whole data and found that initializing the cluster with 9 workers with 8 cores each resulted in faster running time (3.5 minutes) compared to initializing the cluster with 72 workers with one core each (17 minutes). The user is seeking advice on how to optimize the process.",
        "Challenge_last_edit_time":1583589065656,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60573260",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":34.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":70.1540663889,
        "Challenge_title":"Why does Featuretools slows down when I increase the number of Dask workers?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":192.0,
        "Challenge_word_count":297,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509012479112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>You are correctly setting <code>dask_kwargs<\/code> in DFS. I think the slow down happens as a result of additional overhead and less cores in each worker. The more workers there are, the more overhead exists from transmitting data. Additionally, 8 cores from 1 worker can be leveraged to make computations run faster than 1 core from 8 workers.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":4.33,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":57.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1601729162436,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":887.0,
        "Answerer_view_count":130.0,
        "Challenge_adjusted_solved_time":9.0815319445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>If we have an AzureML Pipeline published, how can we trigger it from Azure DevOps <strong>without using Python Script Step or Azure CLI Step<\/strong>?<\/p>\n<p>The AzureML Steps supported natively in Azure DevOps include Model_Deployment and Model_Profiling.<\/p>\n<p>Is there any step in Azure DevOps which can be used to directly trigger a published Azure Machine Learning Pipeline while maintaining capabilities like using Service Connections and passing environmental variables, Gated Release (Deployment)?<\/p>\n<p>Edit:\nThis process can then be used to run as an agentless job.<\/p>",
        "Challenge_closed_time":1612256282836,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612203126923,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking a way to trigger a published Azure Machine Learning Pipeline from Azure DevOps without using Python Script Step or Azure CLI Step. They are looking for a step in Azure DevOps that can directly trigger the pipeline while maintaining capabilities like using Service Connections and passing environmental variables, Gated Release (Deployment). The user also mentions that this process can be used to run as an agentless job.",
        "Challenge_last_edit_time":1612249107728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65997961",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":8.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":14.7655313889,
        "Challenge_title":"How to trigger an AzureML Pipeline from Azure DevOps?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1923.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>Assumptions:<\/p>\n<ol>\n<li>An AzureML Pipeline is published and the REST endpoint is ready- To be referred to in this answer as &lt;AML_PIPELINE_REST_URI&gt;. And Published Pipeline ID is also ready- To be referred to in this answer as &lt;AML_PIPELINE_ID&gt;<\/li>\n<li>You have the Azure Machine Learning Extension installed: <a href=\"https:\/\/marketplace.visualstudio.com\/items?itemName=ms-air-aiagility.vss-services-azureml&amp;ssr=false#review-details\" rel=\"nofollow noreferrer\">Azure Machine Learning Extension<\/a><\/li>\n<\/ol>\n<p>To Invoke the Azure Machine Learning Pipeline we use the <code>Invoke ML Pipeline<\/code> step available in Azure DevOps. It is available when running an Agentless Job.<\/p>\n<p>To trigger it the workflow is as follows:<\/p>\n<ol>\n<li>Create a New Pipeline. Using the Classic Editor, delete the default Agent Job 1 stage.\n<a href=\"https:\/\/i.stack.imgur.com\/phzL3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/phzL3.png\" alt=\"enter image description here\" \/><\/a><\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/QkiPY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QkiPY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ol start=\"2\">\n<li><p>Add an agentless job:\n<a href=\"https:\/\/i.stack.imgur.com\/0PXwg.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0PXwg.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Add a task to this Agentless Job:\n<a href=\"https:\/\/i.stack.imgur.com\/trW7j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/trW7j.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Use AzureML Published Pipeline Task:\n<a href=\"https:\/\/i.stack.imgur.com\/3rl4z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3rl4z.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Use the Service Connection Mapped to the AML Workspace. You can find more on this at the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/library\/service-endpoints?view=azure-devops&amp;tabs=yaml\" rel=\"nofollow noreferrer\">official documentation<\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/mnV36.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mnV36.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Choose the Pipeline to trigger using the &lt;AML_PIPELINE_ID&gt;:\n<a href=\"https:\/\/i.stack.imgur.com\/fbpQW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fbpQW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Give The experiment name and Pipeline Parameters if any:\n<a href=\"https:\/\/i.stack.imgur.com\/og1kx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/og1kx.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>That's it, you can Save and Queue:\n<a href=\"https:\/\/i.stack.imgur.com\/iCwdl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/iCwdl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<\/ol>\n<p>Alternatively, you can simply use the following jobs:<\/p>\n<pre><code>- job: Job_2\n  displayName: Agentless job\n  pool: server\n  steps:\n  - task: MLPublishedPipelineRestAPITask@0\n    displayName: Invoke ML pipeline\n    inputs:\n      connectedServiceName: &lt;REDACTED-AML-WS-Level-Service_Connection-ID&gt;\n      PipelineId: &lt;AML_PIPELINE_ID&gt;\n      ExperimentName: experimentname\n      PipelineParameters: ''\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1612281801243,
        "Solution_link_count":20.0,
        "Solution_readability":14.9,
        "Solution_reading_time":45.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":26.0,
        "Solution_word_count":299.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":213.8297222222,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\nKedro Telemetry installed alongside a packaged and installed Kedro project breaks the project by assuming that the `pyproject.toml` file exists. The `pyproject.toml` is only a recipe for building the project and should not be assumed to be existing in the current folder in all cases.\r\n\r\nThe problem was introduced with https:\/\/github.com\/kedro-org\/kedro-plugins\/pull\/62\r\n\r\n## Context\r\nWhen deploying Kedro projects and if you have installed Kedro Telemetry, it breaks your project.\r\n\r\n## Steps to Reproduce\r\n1. Create a Kedro project\r\n2. Add a dependency on kedro-telemetry\r\n3. Package it through `kedro package`\r\n4. Install it in a different environment\r\n5. Run the project through `.\/<project>` in a folder where only the `conf\/` is\r\n\r\n## Expected Result\r\nThe project should run.\r\n\r\n## Actual Result\r\nAn exception is thrown.\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* Kedro version used (`pip show kedro` or `kedro -V`): 0.18.x\r\n* Kedro plugin and kedro plugin version used (`pip show kedro-telemetry`): 0.2.2 \r\n* Python version used (`python -V`): Not relevant\r\n* Operating system and version: Not relevant\r\n",
        "Challenge_closed_time":1670415546000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669645759000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing issues while trying to run a simple spaceflights example with Astrocloud using kedro-airflow. The expected result is a complete Kedro run on local Airflow image, but the actual result is a failure in the local Airflow image with errors related to `BaseSessionStore`, `git describe`, and `Negsignal.SIGKILL`. The user has provided relevant details about the environment, including the Kedro-Airflow plugin version, Airflow version, Kedro version, Python version, and operating system.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/83",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":7.5,
        "Challenge_reading_time":15.64,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":13.0,
        "Challenge_repo_issue_count":97.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":213.8297222222,
        "Challenge_title":"Kedro Telemetry breaks packaged projects due to wrongly assuming `pyproject.toml` exists",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":182,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":0.0578158333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>This may be a very basic question (I imagine it is)<\/p>\n<pre><code>estimator = PyTorch(entry_point='train.py',\n                   source_dir = 'code',\n                    role = role,\n                   framework_version = '1.5.0',\n                   py_version = 'py3',\n                   instance_count = 2,\n                   instance_type = 'ml.g4dn.2xlarge',\n                   hyperparameters={&quot;epochs&quot;: 2,\n                                     &quot;num_labels&quot;: 46,\n                                     &quot;backend&quot;: &quot;gloo&quot;,    \n                                    },\n                   profiler_config=profiler_config,\n                    debugger_hook_config=debugger_hook_config,\n                    rules=rules\n                   )\n<\/code><\/pre>\n<p>I declare my estimator as above, and put this into training using fit().<br \/>\nI have done several of these on my sagemaker, and there are several training jobs in the aws training job log.<br \/>\nBut they all appear in the form 'pytorch-training-2021 ....'. <br \/>\nIs there anyway I could declare the name of the training job like 'custom-model-xgboost-ver1' ?<br \/>\nI thought it would be possible as one of the parameter of estimator, but i couldn't find it.<\/p>\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1626250161860,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626249953723,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to assign a custom name to their AWS SageMaker training job, but all the jobs appear in the form 'pytorch-training-2021'. The user is wondering if there is a way to declare a custom name for the training job, but they couldn't find a parameter for it in the estimator.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68374280",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.3,
        "Challenge_reading_time":12.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.0578158333,
        "Challenge_title":"Assigning name to AWS SageMaker Training job",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":445.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600718448276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seoul, South Korea",
        "Poster_reputation_count":69.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>When you call <code>fit()<\/code> you can pass this parameter <code>job_name=yourJobName<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.3,
        "Solution_reading_time":1.35,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.30024,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Trying to download a report as latex causes an instrument.js error, and the waiting symbol turns forever. I use chrome on MacOS.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77.jpeg\" data-download-href=\"\/uploads\/short-url\/aV3jmQ0drwgzx2rJ9iyEpD7TJQj.jpeg?dl=1\" title=\"Bildschirmfoto 2023-02-13 um 17.42.14\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77_2_690x307.jpeg\" alt=\"Bildschirmfoto 2023-02-13 um 17.42.14\" data-base62-sha1=\"aV3jmQ0drwgzx2rJ9iyEpD7TJQj\" width=\"690\" height=\"307\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77_2_690x307.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77_2_1035x460.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77_2_1380x614.jpeg 2x\" data-dominant-color=\"959190\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Bildschirmfoto 2023-02-13 um 17.42.14<\/span><span class=\"informations\">1886\u00d7841 173 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1676311598268,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676306917404,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to download a report as latex, which causes an instrument.js error and the waiting symbol to turn forever. The user is using Chrome on MacOS.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/download-report-as-latex-causes-js-errors\/3872",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":24.9,
        "Challenge_reading_time":21.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.30024,
        "Challenge_title":"Download report as latex causes js errors",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":218.0,
        "Challenge_word_count":76,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Found a solution: When carefully loading each graph by scrolling slowly over the whole page, the download finally works.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":1.6,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":19.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1324129118823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11756.0,
        "Answerer_view_count":517.0,
        "Challenge_adjusted_solved_time":8.0286480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created an EventBridge rule that triggers a Sagemaker Pipeline when someone uploads a new file to an S3 bucket. As new input files become available, they will be uploaded to the bucket for processing. I'd like the pipeline to process only the uploaded file, and so thought to pass in the S3 URL of the file as a parameter to the Pipeline. Since the full URL doesn't exist as a single field value in the S3 event, I was wondering if there is some way to concatenate multiple field values into a single parameter value that EventBridge will pass on to the target.<\/p>\n<p>For example, I know the name of the uploaded file can be sent from EventBridge using <code>$.detail.object.key<\/code> and the bucket name can be sent using <code>$.detail.bucket.name<\/code>, so I'm wondering if I can send both somehow to get something like this to the Sagemaker Pipeline <code>s3:\/\/my-bucket\/path\/to\/file.csv<\/code><\/p>\n<p>For what it's worth, I tried splitting the parameter into two (one being <code>s3:\/\/bucket-name\/<\/code> and the other being <code>default_file.csv<\/code>) when defining the pipeline, but got an error saying <code>Pipeline variables do not support concatenation<\/code> when combining the two into one.<\/p>\n<p>The relevant pipeline step is<\/p>\n<p><code>step_transform = TransformStep(name = &quot;Name&quot;, transformer=transformer,inputs=TransformInput(data=variable_of_s3_path)<\/code><\/p>",
        "Challenge_closed_time":1651734900863,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651705997730,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create an AWS EventBridge rule that triggers a Sagemaker Pipeline when a new file is uploaded to an S3 bucket. They want to pass the S3 URL of the uploaded file as a parameter to the Pipeline, but since the URL doesn't exist as a single field value in the S3 event, they are wondering if there is a way to concatenate multiple field values into a single parameter value that EventBridge will pass on to the target. The user tried splitting the parameter into two, but got an error saying Pipeline variables do not support concatenation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72120382",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":18.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":8.0286480556,
        "Challenge_title":"AWS EventBridge: Add multiple event details to a target parameter",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":809.0,
        "Challenge_word_count":207,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324682328743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":969.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/eventbridge\/latest\/userguide\/eb-transform-target-input.html\" rel=\"nofollow noreferrer\">Input transformers<\/a> manipulate the event payload that EventBridge sends to the target.  Transforms consist of (1) an &quot;input path&quot; that maps substitution variable names to JSON-paths in the event and (2) a &quot;template&quot; that references the substitution variables.<\/p>\n<p>Input path:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;detail-bucket-name&quot;: &quot;$.detail.bucket.name&quot;,\n  &quot;detail-object-key&quot;: &quot;$.detail.object.key&quot;\n}\n<\/code><\/pre>\n<p>Input template that concatenates the s3 url and outputs it along with the original event payload:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;s3Url&quot;: &quot;s3:\/\/&lt;detail-bucket-name&gt;\/&lt;detail-object-key&gt;&quot;,\n  &quot;original&quot;: &quot;$&quot;\n}\n<\/code><\/pre>\n<p>Define the transform in the EventBridge console by editing the rule: <code>Rule &gt; Select Targets &gt; Additional Settings<\/code>.<\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.8,
        "Solution_reading_time":14.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":93.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1548188011640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bellevue, WA, USA",
        "Answerer_reputation_count":131.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":6.7539119444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I\u2019m building out a pipeline that should execute and train fairly frequently.  I\u2019m following this: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline<\/a> <\/p>\n\n<p>Anyways, I\u2019ve got a stream analytics job dumping telemetry into .json files on blob storage (soon to be adls gen2).  Anyways, I want to find all .json files and use all of those files to train with.  I could possibly use just new .json files as well (interesting option honestly).<\/p>\n\n<p>Currently I just have the store mounted to a data lake and available; and it just iterates the mount for the data files and loads them up.<\/p>\n\n<ol>\n<li>How can I use data references for this instead?<\/li>\n<li>What does data references do for me that mounting time stamped data does not?\na.  From an audit perspective, I have version control, execution time and time stamped read only data.  Albeit, doing a replay on this would require additional coding, but is do-able.<\/li>\n<\/ol>",
        "Challenge_closed_time":1566854588780,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566830274697,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is building a pipeline to execute and train frequently using Azure ML SDK DataReference. They have a stream analytics job that dumps telemetry into .json files on blob storage and want to find all .json files to train with. They are looking for information on how to use data references instead of mounting time-stamped data and the benefits it provides from an audit perspective.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57660058",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":7.4,
        "Challenge_reading_time":14.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":6.7539119444,
        "Challenge_title":"Azure ML SDK DataReference - File Pattern - MANY files",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":296.0,
        "Challenge_word_count":159,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384802035143,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Miami Beach, FL",
        "Poster_reputation_count":2682.0,
        "Poster_view_count":1006.0,
        "Solution_body":"<p>You could pass pointer to folder as an input parameter for the pipeline, and then your step can mount the folder to iterate over the json files.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":1.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":27.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":6.8229247222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to figure out how viable Azure ML in production; I would like to accomplish the following:<\/p>\n\n<ol>\n<li>Specify <em>custom environments<\/em> for my pipelines using a <em>pip file<\/em> and use them in a pipeline<\/li>\n<li><em>Declaratively<\/em> specify my workspace, environments and pipelines in an <em>Azure DevOps repo<\/em><\/li>\n<li><em>Reproducibly<\/em> deploy my Azure ML workspace to my subscription using an <em>Azure DevOps pipeline<\/em><\/li>\n<\/ol>\n\n<p>I found an <a href=\"https:\/\/stackoverflow.com\/questions\/60506398\/how-do-i-use-an-environment-in-an-ml-azure-pipeline\">explanation of how to specify environments using notebooks<\/a> but this seems ill-suited for the second and third requirements I have.<\/p>",
        "Challenge_closed_time":1583340584892,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583316022363,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to figure out how to version control Azure ML workspaces with custom environments and pipelines. They want to specify custom environments for their pipelines using a pip file, declaratively specify their workspace, environments, and pipelines in an Azure DevOps repo, and reproducibly deploy their Azure ML workspace to their subscription using an Azure DevOps pipeline. The user found an explanation of how to specify environments using notebooks, but it doesn't meet their requirements.",
        "Challenge_last_edit_time":1583348292107,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60523435",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.4,
        "Challenge_reading_time":10.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":6.8229247222,
        "Challenge_title":"How do I version control Azure ML workspaces with custom environments and pipelines?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":659.0,
        "Challenge_word_count":98,
        "Platform":"Stack Overflow",
        "Poster_created_time":1317398821727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1263.0,
        "Poster_view_count":44.0,
        "Solution_body":"<p>Currently, we have a python script, <code>pipeline.py<\/code> that uses the <code>azureml-sdk<\/code>to create, register and run all of our ML artifacts (envs, pipelines, models). We call this script in our Azure DevOps CI pipeline with a Python Script task after building the right pip env from the requirements file in our repo.<\/p>\n\n<p>However, it is worth noting there is YAML support for ML artifact definition. Though I don't know if the existing support will cover all of your bases (though that is the plan).<\/p>\n\n<p>Here's some great docs from MSFT to get you started:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/microsoft\/MLOpsPython\" rel=\"nofollow noreferrer\">GitHub Template repo of an end-to-end example of ML pipeline + deployment<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">How to define\/create an environment (using Pip or Conda) and use it in a remote compute context<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/targets\/azure-machine-learning?context=azure%2Fmachine-learning%2Fservice%2Fcontext%2Fml-context&amp;view=azure-devops&amp;tabs=yaml\" rel=\"nofollow noreferrer\">Azure Pipelines guidance on CI\/CD for ML Service<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-pipeline-yaml\" rel=\"nofollow noreferrer\">Defining ML pipelines in YAML<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1583340824232,
        "Solution_link_count":4.0,
        "Solution_readability":13.8,
        "Solution_reading_time":18.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":148.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1620154324507,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":1169.0,
        "Answerer_view_count":2077.0,
        "Challenge_adjusted_solved_time":332.7862925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm here because I'm facing a problem with scheduled jobs in Google Cloud.\nIn Vertex AI Workbench, I created a notebook in Python 3 that creates a pipeline that trains AutoML with data from the public credit card dataset.\nIf I run the job at the end of its creation, everything works. However, if I schedule the job run <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/run-pipeline#scheduling_a_recurring_pipeline_run_using_the\" rel=\"nofollow noreferrer\">as described here<\/a> in Job Cloud Scheduler, the pipeline is enabled but the run fails.<\/p>\n<p>Here is the code that I have:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\n# import sys\nimport google.cloud.aiplatform as aip\nimport kfp\n# from kfp.v2.dsl import component\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import compiler\n# from kfp.v2.google.client import AIPlatformClient\n\nPROJECT_ID = &quot;fraud-detection-project-329506&quot;\nREGION = &quot;us-central1&quot;\n\ncredential_path = r&quot;C:\\Users\\...\\fraud-detection-project-329506-4d16889a494a.json&quot;\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n      \nBUCKET_NAME = &quot;gs:\/\/...&quot;\nSERVICE_ACCOUNT = &quot;...@fraud-detection-project-329506.iam.gserviceaccount.com&quot;\n\nAPI_ENDPOINT = &quot;{}-aiplatform.googleapis.com&quot;.format(REGION)\nPIPELINE_ROOT = &quot;{}\/dataset&quot;.format(BUCKET_NAME)\n\naip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)\n\n# file names\nTRAIN_FILE_NAME = &quot;creditcard_train.csv&quot;\nTEST_FILE_NAME = &quot;creditcard_test.csv&quot;\n\n# path for train and test dataset \ngcs_csv_path_train = f&quot;{PIPELINE_ROOT}\/{TRAIN_FILE_NAME}&quot;\ngcs_csv_path_test = f&quot;{PIPELINE_ROOT}\/{TEST_FILE_NAME}&quot;\n\n#gcs location where the output is to be written to\ngcs_destination_prefix = &quot;{}\/output&quot;.format(BUCKET_NAME)\n\n@kfp.dsl.pipeline(name=&quot;automl-tab-training-v2&quot;)\ndef pipeline(project: str = PROJECT_ID):\n    \n    # create tabular dataset\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=&quot;creditcard&quot;, gcs_source=gcs_csv_path_train\n    )\n    \n  \n    # Training with AutoML\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=&quot;train-automl-fraud-detection&quot;,\n        optimization_prediction_type=&quot;classification&quot;,\n        column_transformations=[\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Time&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V1&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V2&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V3&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V4&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V5&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V6&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V7&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V8&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V9&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V10&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V11&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V12&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V13&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V14&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V15&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V16&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V17&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V18&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V19&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V20&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V21&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V22&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V23&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V24&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V25&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V26&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V27&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;V28&quot;}},\n            {&quot;numeric&quot;: {&quot;column_name&quot;: &quot;Amount&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],#dataset_with_FeatEng,\n        target_column=&quot;Class&quot;,\n        budget_milli_node_hours=1000,\n    )\n    \n    # batch prediction after training\n    batchprediction_op = gcc_aip.ModelBatchPredictOp(\n        model=training_op.outputs[&quot;model&quot;],\n        job_display_name='prediction1',\n        gcs_source=gcs_csv_path_test,\n        project=project,\n        machine_type=&quot;n1-standard-2&quot;,\n        gcs_destination_prefix=gcs_destination_prefix,\n    )\n    \n\nCOMPILED_PIPELINE_PATH = r&quot;C:\\Users\\...\\tabular_classification_pipeline.json&quot;\nSCHEDULE = &quot;5 5 * * *&quot;\nDISPLAY_NAME = 'fraud_detection'\n\n# compile pipeline\ncompiler.Compiler().compile(\n    pipeline_func=pipeline,\n    package_path=COMPILED_PIPELINE_PATH,\n)\n\n# job run after its creation\njob = aip.PipelineJob(\n    display_name=DISPLAY_NAME,\n    template_path=COMPILED_PIPELINE_PATH,\n    pipeline_root=PIPELINE_ROOT,\n)\njob.run()\n\n# api_client = AIPlatformClient(project_id=PROJECT_ID, region=REGION)\n\n# schedule training\/prediction every day at a certain hour\n# api_client.create_schedule_from_job_spec(\n#    job_spec_path=COMPILED_PIPELINE_PATH,\n#    pipeline_root=PIPELINE_ROOT,\n#    schedule=SCHEDULE,\n# )\n<\/code><\/pre>\n<p>Looking at the error log, I found:<\/p>\n<pre><code>{\nhttpRequest: {\nstatus: 404\n}\ninsertId: &quot;13yj575g2rylrz9&quot;\njsonPayload: {\n@type: &quot;type.googleapis.com\/google.cloud.scheduler.logging.AttemptFinished&quot;\njobName: &quot;projects\/fraud-detection-project-329506\/locations\/us-central1\/jobs\/pipeline_pipeline_179e648c_0-11-a-a-a&quot;\nstatus: &quot;NOT_FOUND&quot;\ntargetType: &quot;HTTP&quot;\nurl: &quot;https:\/\/us-central1-fraud-detection-project-329506.cloudfunctions.net\/templated_http_request-v1&quot;\n}\nlogName: &quot;projects\/fraud-detection-project-329506\/logs\/cloudscheduler.googleapis.com%2Fexecutions&quot;\nreceiveTimestamp: &quot;2021-10-19T18:00:00.309225533Z&quot;\nresource: {\nlabels: {\njob_id: &quot;pipeline_pipeline_179e648c_0-11-a-a-a&quot;\nlocation: &quot;us-central1&quot;\nproject_id: &quot;fraud-detection-project-329506&quot;\n}\ntype: &quot;cloud_scheduler_job&quot;\n}\nseverity: &quot;ERROR&quot;\ntimestamp: &quot;2021-10-19T18:00:00.309225533Z&quot;\n}\n<\/code><\/pre>\n<p>Does it mean that I have to create the URL before running the notebook? I have no idea how to go on.\nThank you in advance.<\/p>",
        "Challenge_closed_time":1636443694643,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634804848290,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with scheduled jobs in Google Cloud. They have created a notebook in Python 3 that creates a pipeline that trains AutoML with data from the public credit card dataset. If they run the job at the end of its creation, everything works. However, if they schedule the job run in Job Cloud Scheduler, the pipeline is enabled but the run fails. The error log shows a 404 status and the user is unsure how to proceed.",
        "Challenge_last_edit_time":1635245663990,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69658459",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":24.3,
        "Challenge_reading_time":91.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":455.2350980556,
        "Challenge_title":"Jobs-Cloud Scheduler (Google Cloud) fails to run scheduled pipelines",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":330.0,
        "Challenge_word_count":404,
        "Platform":"Stack Overflow",
        "Poster_created_time":1616589293616,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Alatri, Frosinone, FR",
        "Poster_reputation_count":67.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>From the error you shared, apparently <strong>Cloud Function<\/strong> failed to create the job.<\/p>\n<pre><code>status: &quot;NOT_FOUND&quot; \ntargetType: &quot;HTTP&quot; \nurl: &quot;https:\/\/us-central1-fraud-detection-project-329506.cloudfunctions.net\/templated_http_request-v1&quot;\n<\/code><\/pre>\n<p>A possible reason from the Cloud Function side could be if <strong>Cloud Build API<\/strong> is not used in your project before or it is disabled. Can you check if it is enabled and try again? If you have enabled this API recently, wait for a few minutes for the action to propagate to the systems and retry.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":7.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":77.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":215.8874158333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>hi I would like to know how to estimate the price for designer ?<\/p>",
        "Challenge_closed_time":1665358871540,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664581676843,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to estimate the price for a designer using machine learning.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1031348\/machine-learning-pricing",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.2,
        "Challenge_reading_time":1.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":215.8874158333,
        "Challenge_title":"machine learning pricing",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":16,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=5c0df626-d144-412c-8dd3-6bbe9f527d31\">@Louis  <\/a>     <\/p>\n<p>Thanks for using Microsoft Q&amp;A platform, I am sorry I should have seen your question earlier.     <\/p>\n<p>This price depends on your settings about the compute. I would say you should try the calculator to estimate your price to avoid surprise - <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/calculator\/\">https:\/\/azure.microsoft.com\/en-us\/pricing\/calculator\/<\/a>    <\/p>\n<p>The screenshot of pricing calculator is as below -    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/248822-image.png?platform=QnA\" alt=\"248822-image.png\" \/>    <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <br \/>\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.7,
        "Solution_reading_time":10.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":88.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.5003305556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working with Azure AI and initially had been using Machine Learning Studio Classic.<\/p>\n\n<p>It was working well but was slow training my models. From looking around, it seems that if I use Azure Machine Learning Studio, I can control the hardware used to run the experiments, so this is what I am trying.<\/p>\n\n<p>My issue is that Azure Machine Learning Studio is extremely slow in starting the experiments-it can take 10 minutes to even start.<\/p>\n\n<p>Is this as expected or am I missing something?<\/p>\n\n<p>Incidentally, NC24 was actually slower than NC6 - is this because of the configuration of my experiment?<\/p>\n\n<p>GPU Training    Whole run\nNC6 2m 36s  10m 48s\nNC24    2m 52s  16m 48s<\/p>",
        "Challenge_closed_time":1581956292927,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581954491737,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing slow start-up times while using Azure Machine Learning Studio, which is taking up to 10 minutes to start an experiment. They had previously used Machine Learning Studio Classic, which was slow in training models. The user is also unsure if the slower performance of NC24 compared to NC6 is due to the experiment configuration.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60265992",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":8.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.5003305556,
        "Challenge_title":"Azure Machine Learning - slow to start",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1143.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1420792743172,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3531.0,
        "Poster_view_count":312.0,
        "Solution_body":"<p>I'm assuming that you're:<\/p>\n\n<ul>\n<li>using Azure ML Studio (i.e. <a href=\"https:\/\/ml.azure.com\/\" rel=\"nofollow noreferrer\"><code>ml.azure.com\/<\/code><\/a>)'s Pipeline Designer, and <\/li>\n<li>creating a new compute target before running?<\/li>\n<\/ul>\n\n<p>If so, then 10 minutes is normal for the first run, given that a cluster of VMs has to be created and provisioned with a Docker container and Conda environment. After the run first completes the compute target is configured to stay on and available for two hours so future runs should execute without the 10 minute delay (provided you don't change the Conda dependencies or choose a new compute target).<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-set-up-training-targets#amlcompute\" rel=\"nofollow noreferrer\">more information about AMLCompute<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.5,
        "Solution_reading_time":10.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":104.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1384730587840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"France",
        "Answerer_reputation_count":717.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":71.1103577778,
        "Challenge_answer_count":1,
        "Challenge_body":"<pre><code>pins 1.0.1\nAzureStor 3.7.0\n<\/code><\/pre>\n<p>I'm getting this error<\/p>\n<pre><code>Error in withr::local_options(azure_storage_progress_bar = progress, .local_envir = env) : \n  unused argument (azure_storage_progress_bar = progress)\nCalls: %&gt;% ... pin_meta.pins_board_azure -&gt; azure_download -&gt; local_azure_progress\nExecution halted\n<\/code><\/pre>\n<p>when running <code>pin_read()<\/code> in the following code (<code>pin_list()<\/code> works fine)<\/p>\n<pre><code>bl_endp_key &lt;- storage_endpoint(endpoint = &lt;endpoint URL&gt;, key =&lt;endpoint key&gt;&quot;)\ncontainer &lt;- storage_container(endpoint = bl_endp_key, name = &lt;blob name&gt;)\nboard &lt;- board_azure(container = container, path = &quot;accidentsdata&quot;)\ncat(&quot;Testing pins:\\n&quot;)\nprint(board %&gt;% pin_list())\naccidents2 &lt;- board %&gt;% pins::pin_read('accidents') %&gt;% as_tibble()\n<\/code><\/pre>\n<p>My goal is to &quot;pin_read&quot; a dataset located on a Azure Blob Storage from an R script being run from <strong>pipelineJoB (YAML)<\/strong> including a <code>command: Rscript script.R ...<\/code> and an <code>environment:<\/code> based on a dockerfile installing <strong>R version 4.0.0<\/strong> (2020-04-24) -- &quot;Arbor Day&quot;<\/p>\n<p>The pipelineJob is being called from an Azure DevOps Pipeline task with <code>az ml job create &lt;pipelineJob YAML&gt; &lt;resource grp&gt; &lt;aml workspace name&gt;<\/code>.<\/p>\n<p>Note: the R script runs fine on my Windows RStudio desktop, with R version 4.1.3 (2022-03-10) -- &quot;One Push-Up&quot;.<\/p>\n<p>I've already tried with<\/p>\n<p><code>options(azure_storage_progress_bar=FALSE)<\/code> or<\/p>\n<p><code>withr::local_options(azure_storage_progress_bar=FALSE)<\/code><\/p>\n<p>but I'm getting the same <code>unused argument (azure_storage_progress_bar ...<\/code> error.<\/p>\n<p>FYI: <code>local_azure_progress<\/code> is defined here <a href=\"https:\/\/rdrr.io\/github\/rstudio\/pins\/src\/R\/board_azure.R#sym-local_azure_progress\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Challenge_closed_time":1656936659776,
        "Challenge_comment_count":1,
        "Challenge_created_time":1656349974407,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while running the `pin_read()` function in R, with the error message indicating an unused argument for `azure_storage_progress_bar`. The user is attempting to read a dataset located on Azure Blob Storage from an R script being run from pipelineJob (YAML) with R version 4.0.0. The R script runs fine on the user's Windows RStudio desktop with R version 4.1.3. The user has tried using `options(azure_storage_progress_bar=FALSE)` and `withr::local_options(azure_storage_progress_bar=FALSE)` but is still encountering the same error.",
        "Challenge_last_edit_time":1656680662488,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72775967",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":27.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":162.9681580556,
        "Challenge_title":"R, pins and AzureStor: unused argument (azure_storage_progress_bar = progress)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":80.0,
        "Challenge_word_count":189,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384730587840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"France",
        "Poster_reputation_count":717.0,
        "Poster_view_count":112.0,
        "Solution_body":"<p>Issue has been filed in <a href=\"https:\/\/github.com\/rstudio\/pins\/issues\/624\" rel=\"nofollow noreferrer\">pins<\/a>, it seems that is not an AzureStor issue.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.0,
        "Solution_reading_time":2.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":6.6429136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/schedule-cloud-scheduler\" rel=\"nofollow noreferrer\">this<\/a> tutorial to create my first scheduled Vertex AI Pipeline to run every minute. The only thing it does is prints <code>&quot;Hello, &lt;any-greet-string&gt;&quot;<\/code> and also returns this same string. I can see that it is running because the last run time updates and the last run result is &quot;Success&quot; every time.<\/p>\n<p>My question is very simple: Where can I see this string printed and the output of my pipeline?<\/p>",
        "Challenge_closed_time":1639667359983,
        "Challenge_comment_count":1,
        "Challenge_created_time":1639659363547,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a scheduled Vertex AI Pipeline that prints a greeting string and returns the same string, which runs successfully every minute. However, the user is unable to locate where the string is printed and where the output of the pipeline can be viewed.",
        "Challenge_last_edit_time":1639726542407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70379395",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.6,
        "Challenge_reading_time":7.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2.2212322222,
        "Challenge_title":"Vertex AI - Viewing Pipeline Output",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":346.0,
        "Challenge_word_count":79,
        "Platform":"Stack Overflow",
        "Poster_created_time":1407245826703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":745.0,
        "Poster_view_count":168.0,
        "Solution_body":"<p>The output of the <code>print()<\/code> statements in the pipeline can be found in &quot;Cloud Logging&quot; with the appropriate filters. To check logs for each component in the pipeline, click on the respective component in the console and click &quot;<strong>VIEW LOGS<\/strong>&quot; in the right pane. A new pane with the logs will open in the pipeline page which will allow us to see the output of the component. Refer to the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I ran a sample <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#4\" rel=\"nofollow noreferrer\">pipeline<\/a> from this codelab, <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#0\" rel=\"nofollow noreferrer\">Intro to Vertex Pipelines<\/a> and below is the output for one of the <code>print()<\/code> statements in the pipeline.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<br \/>\n<h4>UPDATE:<\/h4>\n<p>Every component in a pipeline run is deployed as an individual Vertex AI custom job. Corresponding to the sample pipeline consisting 3 components, there are 3 entries in the &quot;CUSTOM JOBS&quot; section as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Therefore, to view the logs on the run level, we would need to query the log entries with the respective <code>job_id<\/code>s of the pipeline components and the <code>job_id<\/code> of the Cloud Scheduler job. The query would look like this.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.labels.job_id=(&quot;JOB_ID_1&quot; OR &quot;JOB_ID_2&quot; [OR &quot;JOB_ID_N&quot;...])\nseverity&gt;=DEFAULT\n<\/code><\/pre>\n<p>If there are no simultaneous pipeline runs, a simpler query like below can be used.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.type=(&quot;cloud_scheduler_job&quot; OR &quot;ml_job&quot;)\nseverity&gt;=DEFAULT\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1639750456896,
        "Solution_link_count":8.0,
        "Solution_readability":11.1,
        "Solution_reading_time":29.64,
        "Solution_score_count":2.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":241.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1398051491376,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Massachusetts",
        "Answerer_reputation_count":459.0,
        "Answerer_view_count":108.0,
        "Challenge_adjusted_solved_time":0.1220702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have close to 100 processing jobs to which I want to add certain tags. I've found commands that you can use to tag one resource with a list of tags. Is there any way I can do this for multiple jobs? Through CLI or through python+boto?<\/p>",
        "Challenge_closed_time":1657516175783,
        "Challenge_comment_count":2,
        "Challenge_created_time":1657515736330,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to add tags to almost 100 processing jobs and is looking for a way to do it for multiple jobs using AWS CLI or python+boto.",
        "Challenge_last_edit_time":1657607287156,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72933908",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":3.4,
        "Challenge_reading_time":3.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1220702778,
        "Challenge_title":"Can I use AWS CLI to add tags to all processing jobs matching a certain regex",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":38.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1498814861883,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":37.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You can <code>ResourceGroupsTaggingAPI<\/code>'s method <code>tag_resources()<\/code>.<br \/>\nThis is used to apply one or more tags to the specified list of resources.<\/p>\n<p>References:<\/p>\n<ol>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/resourcegroupstaggingapi.html#ResourceGroupsTaggingAPI.Client.tag_resources\" rel=\"nofollow noreferrer\">Tag Resources using boto3<\/a><\/li>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/resourcegroupstaggingapi.html#ResourceGroupsTaggingAPI.Client.untag_resources\" rel=\"nofollow noreferrer\">UnTag Resources using boto3<\/a><\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":31.0,
        "Solution_reading_time":9.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":37.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":0.3402952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Let's say we have multiple long running pipeline nodes.\nIt seems quite straight forward to checkpoint or cache the intermediate results, so when nodes after a checkpoint are changed or added only these nodes must be executed again.<\/p>\n\n<p>Does Kedro provide functionality to make sure, that when I run the pipeline only those steps are \nexecuted that have changed?\nAlso the reverse, is there a way to make sure, that all steps that have changed are executed?<\/p>\n\n<p>Let's say a pipeline producing some intermediate result changed, will it be executed, when i execute a pipeline depending on the output of the first?<\/p>\n\n<p><strong>TL;DR:<\/strong> Does Kedro have <code>makefile<\/code>-like tracking of what needs to be done and what not?<\/p>\n\n<p>I think my question is similar to <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/issues\/341\" rel=\"nofollow noreferrer\">issue #341<\/a>, but I do not require support of cyclic graphs.<\/p>",
        "Challenge_closed_time":1591362515296,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591361290233,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is asking if Kedro supports checkpointing or caching of intermediate results to avoid re-execution of the entire pipeline when only a few nodes have changed. They are also asking if Kedro has a tracking system similar to makefile to determine which steps need to be executed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62215724",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":12.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.3402952778,
        "Challenge_title":"Does Kedro support Checkpointing\/Caching of Results?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":367.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1494502461176,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":83.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You might want to have a look at the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.IncrementalDataSet.html\" rel=\"nofollow noreferrer\">IncrementalDataSet<\/a> alongside the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/08_advanced_io.html#partitioned-dataset\" rel=\"nofollow noreferrer\">partitioned dataset<\/a> documentation, specifically the section on <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/08_advanced_io.html#incremental-loads-with-incrementaldataset\" rel=\"nofollow noreferrer\">incremental loads with the incremental dataset<\/a> which has a notion of \"checkpointing\", although checkpointing is a manual step and not automated like <code>makefile<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":23.6,
        "Solution_reading_time":9.71,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":51.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1436771091480,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Brno, \u010cesko",
        "Answerer_reputation_count":51.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":0.1839775,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using AzureML SDK pipeline with AutoMLStep. How can I add PipelineParameter into AutoMLStep configuration? I would like to use it for a definition of max_horizon.\nIt should work with <\/p>\n\n<blockquote>\n  <p>passthru_automl_config=False<\/p>\n<\/blockquote>\n\n<p>but I am getting error <\/p>\n\n<blockquote>\n  <p>Message: Unsupported value of max_horizon. max_horizon must be integer or 'auto'<\/p>\n<\/blockquote>\n\n<pre><code>max_horizon = PipelineParameter(name='max_horizon', default_value=30)\n\nautoml_settings = {\n            \"iteration_timeout_minutes\" : 60\n            \"grain_column_names\": [\"COUNTRY_CODE\"],\n            \"time_column_name\": \"DATE\"\n        }        \n\nautoml_config = AutoMLConfig(task='forecasting',\n                             path = \".\/src\",\n                             primary_metric=primary_metric,\n                             iterations=iterations,\n                             max_concurrent_iterations=max_concurrent_iterations,\n                             training_data = train_data,\n                             label_column_name = label,\n                             n_cross_validations=5,\n                             compute_target = compute_target,\n                             max_horizon= max_horizon,\n                             **automl_settings)\n\ntrainWithAutomlStep = AutoMLStep(name=\"experiment_name\",\n                                 automl_config=automl_config,\n                                 passthru_automl_config=False,\n                                 outputs=[metrics_data, model_data],\n                                 allow_reuse=True)\n<\/code><\/pre>",
        "Challenge_closed_time":1592202183327,
        "Challenge_comment_count":2,
        "Challenge_created_time":1591257136050,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while using AzureML SDK pipeline with AutoMLStep. They are trying to add a PipelineParameter into AutoMLStep configuration for defining max_horizon, but they are getting an error message stating that max_horizon must be an integer or 'auto'. The user has shared their code snippet for reference.",
        "Challenge_last_edit_time":1592201521008,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62189492",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":22.9,
        "Challenge_reading_time":15.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":262.5131325,
        "Challenge_title":"How to pass PipelineParameter into AutoMLStep in AzureML Python SDK",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":316.0,
        "Challenge_word_count":98,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436771091480,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brno, \u010cesko",
        "Poster_reputation_count":51.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Here is a response from Microsoft:<\/p>\n\n<blockquote>\n  <p>PipelineParameter is currently not supported for use with AutoMLConfig parameters inside of AutoMLStep.<\/p>\n  \n  <p>Then, the only workaround in order to use PipelineParameter with\n  AutoMLConfig would be to use AutoML in a PythonScriptStep, which is a\n  similar usage\/approach when you use AutoMLConfig with\n  ParallelRunConfig in pipelines (without using AutoMLStep), like the\n  \u2018Many Models\u2019 solution accelerator does.<\/p>\n<\/blockquote>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.1,
        "Solution_reading_time":6.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.6992033333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>&quot;We want  the model  to automatically register model every time there is a new model. we created the model in the process and write it out to a pipeline data set.To persist it then we upload and read it for registration.    <\/p>\n<p>We are using .\/output to send the file to output. The issue is that it cannot find it in the file path . How can we validate its existence?  &quot;  <\/p>\n<p>[Note: As we migrate from MSDN, this question has been posted by an\u202fAzure Cloud Engineer\u202fas a frequently asked question] Source: <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/1369621f-ebc2-4e79-abe9-9f1165aea6c6\/model-file-is-not-found-for-registration-of-model-in-training-pipeline?forum=MachineLearning\">MSDN<\/a>  <\/p>",
        "Challenge_closed_time":1589360659692,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589329342560,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with finding the model file for registration in the training pipeline. They have created the model and written it out to a pipeline data set, but it cannot be found in the file path. The user is seeking help to validate its existence.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/26470\/model-file-is-not-found-for-registration-of-model",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":8.6992033333,
        "Challenge_title":"Model file is not found for Registration of model in training Pipeline.",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":108,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Can you verify that the script that is actually writing the model file to the location you expect:<\/p>\n<pre><code>with open(model_name, 'wb') as file:\n       joblib.dump(value = model, filename = os.path.join('.\/outputs\/', model_name))\n<\/code><\/pre>\n<p>Inside in your train python script, you just need to do something like this:<\/p>\n<h1 id=\"persist-the-model-to-the-local-machine\">persist the model to the local machine<\/h1>\n<pre><code>tf.saved_model.save(model,'.\/outputs\/model\/')\n<\/code><\/pre>\n<h1 id=\"register-the-model-with-run-object\">register the model with run object<\/h1>\n<pre><code>run.register_model(model_name,'.\/outputs\/model\/')\n<\/code><\/pre>\n<p>Source: <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/1369621f-ebc2-4e79-abe9-9f1165aea6c6\/model-file-is-not-found-for-registration-of-model-in-training-pipeline?forum=MachineLearning\">MSDN<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":21.3,
        "Solution_reading_time":11.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":65.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1544390307847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Palo Alto, CA, USA",
        "Answerer_reputation_count":151.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":270.3311944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've seen examples of labeling data using SageMaker Ground Truth and then using that data to train off-the-shelf SageMaker models. However, am I able to use this same annotation format with TensorFlow Script Mode? <\/p>\n\n<p>More specifically, I have a tensorflow.keras model I'm training using TF Script Mode, and I'd like to take data labeled with Ground Truth and convert my script from File mode to Pipe mode.<\/p>",
        "Challenge_closed_time":1549915976767,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548942784467,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to determine if they can use the annotation format from SageMaker Ground Truth with TensorFlow Script Mode to train a tensorflow.keras model and convert their script from File mode to Pipe mode.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54462105",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":5.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":270.3311944445,
        "Challenge_title":"SageMaker Ground Truth with TensorFlow",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":454.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>I am from Amazon SageMaker Ground Truth team and happy to assist you in your experiment. Just to be clear our understanding, are you running TF model in SageMaker using TF estimator in your own container (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst<\/a>)? <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.3,
        "Solution_reading_time":5.85,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.6181130556,
        "Challenge_answer_count":1,
        "Challenge_body":"According to [Sagemaker's Pipeline Python SDK documenation](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html), looks like there is no specific pipeline step for model deployment. \n\nCan you please confirm this and, also, if there is a plan to have such a step? \n\nWhat is the recommended way to add a pipeline step to deploy the trained model, resulting in an enpoint being created?",
        "Challenge_closed_time":1669881877547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669850852340,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the absence of a specific pipeline step for model deployment in Sagemaker's Pipeline Python SDK documentation and is seeking confirmation and information on any plans to introduce such a step. They are also asking for recommendations on the best way to add a pipeline step for deploying a trained model and creating an endpoint.",
        "Challenge_last_edit_time":1670198562116,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUiGdmBa_oQJuiiewjUhe9OA\/sagemaker-pipeline-deploy-model-step",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":5.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":8.6181130556,
        "Challenge_title":"Sagemaker Pipeline Deploy Model Step",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":312.0,
        "Challenge_word_count":60,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, there is indeed no specific pipeline step for model deployment. The idea is that SageMaker Pipelines is more about \"batch mode\", but customers do ask for this feature, so it might be added. \n\nYou can implement it quite easily using Lambda Step.\n\n1st create a Lambda function to deploy\/update the model:\n```\n%%writefile deploy_model_lambda.py\n\n\n\"\"\"\nThis Lambda function deploys the model to SageMaker Endpoint. \nIf Endpoint exists, then Endpoint will be updated with new Endpoint Config.\n\"\"\"\n\nimport json\nimport boto3\nimport time\n\n\nsm_client = boto3.client(\"sagemaker\")\n\n\ndef lambda_handler(event, context):\n\n    print(f\"Received Event: {event}\")\n\n    current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())\n    endpoint_instance_type = event[\"endpoint_instance_type\"]\n    model_name = event[\"model_name\"]\n    endpoint_config_name = \"{}-{}\".format(event[\"endpoint_config_name\"], current_time)\n    endpoint_name = event[\"endpoint_name\"]\n\n    # Create Endpoint Configuration\n    create_endpoint_config_response = sm_client.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=[\n            {\n                \"InstanceType\": endpoint_instance_type,\n                \"InitialVariantWeight\": 1,\n                \"InitialInstanceCount\": 1,\n                \"ModelName\": model_name,\n                \"VariantName\": \"AllTraffic\",\n            }\n        ],\n    )\n    print(f\"create_endpoint_config_response: {create_endpoint_config_response}\")\n\n    # Check if an endpoint exists. If no - Create new endpoint, if yes - Update existing endpoint\n    list_endpoints_response = sm_client.list_endpoints(\n        SortBy=\"CreationTime\",\n        SortOrder=\"Descending\",\n        NameContains=endpoint_name,\n    )\n    print(f\"list_endpoints_response: {list_endpoints_response}\")\n\n    if len(list_endpoints_response[\"Endpoints\"]) > 0:\n        print(\"Updating Endpoint with new Endpoint Configuration\")\n        update_endpoint_response = sm_client.update_endpoint(\n            EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n        )\n        print(f\"update_endpoint_response: {update_endpoint_response}\")\n    else:\n        print(\"Creating Endpoint\")\n        create_endpoint_response = sm_client.create_endpoint(\n            EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n        )\n        print(f\"create_endpoint_response: {create_endpoint_response}\")\n\n    return {\"statusCode\": 200, \"body\": json.dumps(\"Endpoint Created Successfully\")}\n```\n\nThen create the Lambda step:\n```\ndeploy_model_lambda_function_name = \"sagemaker-deploy-model-lambda-\" + current_time\n\ndeploy_model_lambda_function = Lambda(\n    function_name=deploy_model_lambda_function_name,\n    execution_role_arn=lambda_role,\n    script=\"deploy_model_lambda.py\",\n    handler=\"deploy_model_lambda.lambda_handler\",\n)\n```\n\nYou can see a full working example in [this notebook](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint.ipynb).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1669881962643,
        "Solution_link_count":1.0,
        "Solution_readability":23.2,
        "Solution_reading_time":37.71,
        "Solution_score_count":2.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":197.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":30.6442191667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>On the AWS developer docs for Sagemaker, they recommend us to use PIPE mode to directly stream large datasets from S3 to the model training containers (since it's faster, uses less disk storage, reduces training time, etc.).<\/p>\n\n<p>However, they don't include information on whether this data streaming transfer is charged for (they only include data transfer pricing for their model building &amp; deployment stages, not training).<\/p>\n\n<p>So, I wanted to ask if anyone knew whether this data transfer in PIPE mode is charged for, since if it is, I don't get how this would be recommended for large datasets, since streaming a few epochs for each model iteration can get prohibitively expensive for large datasets (my dataset, for example, is 6.3TB on S3).<\/p>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1564985810716,
        "Challenge_comment_count":0,
        "Challenge_created_time":1564927931767,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is inquiring whether AWS Sagemaker charges for data streaming in PIPE mode from S3 to model training containers, as the documentation does not provide information on this. The user is concerned about the cost implications of streaming large datasets, such as their 6.3TB dataset on S3.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57347278",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.9,
        "Challenge_reading_time":10.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":16.0774858333,
        "Challenge_title":"Does AWS Sagemaker charge for S3 streamed data in PIPE mode (for model training)?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":828.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1401475981032,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":440.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>You are charged for the S3 GET calls that you do similarly to what you would be charged if you used the FILE option of the training. However, these charges are usually marginal compared to the alternatives. <\/p>\n\n<p>When you are using the FILE mode, you need to pay for the local EBS on the instances, and for the extra time that your instances are up and only copying the data from S3. If you are running multiple epochs, you will not benefit much from the PIPE mode, however, when you have so much data (6.3 TB), you don't really need to run multiple epochs. <\/p>\n\n<p>The best usage of PIPE mode is when you can use a <strong>single pass<\/strong> over the data. In the era of big data, this is a better model of operation, as you can't retrain your models often. In SageMaker, you can point to your \"old\" model in the \"model\" channel, and your \"new\" data in the \"train\" channel and benefit from the PIPE mode to the maximum. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1565038250956,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":11.15,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":174.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1629385138956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":395.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":6.9681888889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>This is my first time using Google's Vertex AI Pipelines. I checked <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro?hl=en#0\" rel=\"nofollow noreferrer\">this codelab<\/a> as well as <a href=\"https:\/\/towardsdatascience.com\/how-to-set-up-custom-vertex-ai-pipelines-step-by-step-467487f81cad\" rel=\"nofollow noreferrer\">this post<\/a> and <a href=\"https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153\" rel=\"nofollow noreferrer\">this post<\/a>, on top of some links derived from the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/introduction?hl=es-419\" rel=\"nofollow noreferrer\">official documentation<\/a>. I decided to put all that knowledge to work, in some toy example: I was planning to build a pipeline consisting of 2 components: &quot;get-data&quot; (which reads some .csv file stored in Cloud Storage) and &quot;report-data&quot; (which basically returns the shape of the .csv data read in the previous component). Furthermore, I was cautious to include <a href=\"https:\/\/stackoverflow.com\/questions\/71351821\/reading-file-from-vertex-ai-and-google-cloud-storage\">some suggestions<\/a> provided in this forum. The code I currently have, goes as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output\nfrom google.cloud import aiplatform\n\n# Components section   \n\n@component(\n    packages_to_install=[\n        &quot;google-cloud-storage&quot;,\n        &quot;pandas&quot;,\n    ],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;get_data.yaml&quot;\n)\ndef get_data(\n    bucket: str,\n    url: str,\n    dataset: Output[Dataset],\n):\n    import pandas as pd\n    from google.cloud import storage\n    \n    storage_client = storage.Client(&quot;my-project&quot;)\n    bucket = storage_client.get_bucket(bucket)\n    blob = bucket.blob(url)\n    blob.download_to_filename('localdf.csv')\n    \n    # path = &quot;gs:\/\/my-bucket\/program_grouping_data.zip&quot;\n    df = pd.read_csv('localdf.csv', compression='zip')\n    df['new_skills'] = df['new_skills'].apply(ast.literal_eval)\n    df.to_csv(dataset.path + &quot;.csv&quot; , index=False, encoding='utf-8-sig')\n\n\n@component(\n    packages_to_install=[&quot;pandas&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;report_data.yaml&quot;\n)\ndef report_data(\n    inputd: Input[Dataset],\n):\n    import pandas as pd\n    df = pd.read_csv(inputd.path)\n    return df.shape\n\n\n# Pipeline section\n\n@pipeline(\n    # Default pipeline root. You can override it when submitting the pipeline.\n    pipeline_root=PIPELINE_ROOT,\n    # A name for the pipeline.\n    name=&quot;my-pipeline&quot;,\n)\ndef my_pipeline(\n    url: str = &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n    bucket: str = &quot;my-bucket&quot;\n):\n    dataset_task = get_data(bucket, url)\n\n    dimensions = report_data(\n        dataset_task.output\n    )\n\n# Compilation section\n\ncompiler.Compiler().compile(\n    pipeline_func=my_pipeline, package_path=&quot;pipeline_job.json&quot;\n)\n\n# Running and submitting job\n\nfrom datetime import datetime\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun1 = aiplatform.PipelineJob(\n    display_name=&quot;my-pipeline&quot;,\n    template_path=&quot;pipeline_job.json&quot;,\n    job_id=&quot;mlmd-pipeline-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={&quot;url&quot;: &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;, &quot;bucket&quot;: &quot;my-bucket&quot;},\n    enable_caching=True,\n)\n\nrun1.submit()\n<\/code><\/pre>\n<p>I was happy to see that the pipeline compiled with no errors, and managed to submit the job. However &quot;my happiness lasted short&quot;, as when I went to Vertex AI Pipelines, I stumbled upon some &quot;error&quot;, which goes like:<\/p>\n<blockquote>\n<p>The DAG failed because some tasks failed. The failed tasks are: [get-data].; Job (project_id = my-project, job_id = 4290278978419163136) is failed due to the above error.; Failed to handle the job: {project_number = xxxxxxxx, job_id = 4290278978419163136}<\/p>\n<\/blockquote>\n<p>I did not find any related info on the web, neither could I find any log or something similar, and I feel a bit overwhelmed that the solution to this (seemingly) easy example, is still eluding me.<\/p>\n<p>Quite obviously, I don't what or where I am mistaking. Any suggestion?<\/p>",
        "Challenge_closed_time":1650657469576,
        "Challenge_comment_count":5,
        "Challenge_created_time":1650588056410,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to read data in Vertex AI Pipelines. They have followed various resources and created a pipeline with two components, but the \"get-data\" component is failing and causing the entire pipeline to fail. The user is unsure of what is causing the error and is seeking suggestions for a solution.",
        "Challenge_last_edit_time":1650845389252,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71962260",
        "Challenge_link_count":5,
        "Challenge_participation_count":7,
        "Challenge_readability":13.8,
        "Challenge_reading_time":56.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":19.281435,
        "Challenge_title":"Reading Data in Vertex AI Pipelines",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":892.0,
        "Challenge_word_count":382,
        "Platform":"Stack Overflow",
        "Poster_created_time":1629385138956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":395.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>With some suggestions provided in the comments, I think I managed to make my demo pipeline work. I will first include the updated code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output\nfrom datetime import datetime\nfrom google.cloud import aiplatform\nfrom typing import NamedTuple\n\n\n# Importing 'COMPONENTS' of the 'PIPELINE'\n\n@component(\n    packages_to_install=[\n        &quot;google-cloud-storage&quot;,\n        &quot;pandas&quot;,\n    ],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;get_data.yaml&quot;\n)\ndef get_data(\n    bucket: str,\n    url: str,\n    dataset: Output[Dataset],\n):\n    &quot;&quot;&quot;Reads a csv file, from some location in Cloud Storage&quot;&quot;&quot;\n    import ast\n    import pandas as pd\n    from google.cloud import storage\n    \n    # 'Pulling' demo .csv data from a know location in GCS\n    storage_client = storage.Client(&quot;my-project&quot;)\n    bucket = storage_client.get_bucket(bucket)\n    blob = bucket.blob(url)\n    blob.download_to_filename('localdf.csv')\n    \n    # Reading the pulled demo .csv data\n    df = pd.read_csv('localdf.csv', compression='zip')\n    df['new_skills'] = df['new_skills'].apply(ast.literal_eval)\n    df.to_csv(dataset.path + &quot;.csv&quot; , index=False, encoding='utf-8-sig')\n\n\n@component(\n    packages_to_install=[&quot;pandas&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;report_data.yaml&quot;\n)\ndef report_data(\n    inputd: Input[Dataset],\n) -&gt; NamedTuple(&quot;output&quot;, [(&quot;rows&quot;, int), (&quot;columns&quot;, int)]):\n    &quot;&quot;&quot;From a passed csv file existing in Cloud Storage, returns its dimensions&quot;&quot;&quot;\n    import pandas as pd\n    \n    df = pd.read_csv(inputd.path+&quot;.csv&quot;)\n    \n    return df.shape\n\n\n# Building the 'PIPELINE'\n\n@pipeline(\n    # i.e. in my case: PIPELINE_ROOT = 'gs:\/\/my-bucket\/test_vertex\/pipeline_root\/'\n    # Can be overriden when submitting the pipeline\n    pipeline_root=PIPELINE_ROOT,\n    name=&quot;readcsv-pipeline&quot;,  # Your own naming for the pipeline.\n)\ndef my_pipeline(\n    url: str = &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n    bucket: str = &quot;my-bucket&quot;\n):\n    dataset_task = get_data(bucket, url)\n\n    dimensions = report_data(\n        dataset_task.output\n    )\n    \n\n# Compiling the 'PIPELINE'    \n\ncompiler.Compiler().compile(\n    pipeline_func=my_pipeline, package_path=&quot;pipeline_job.json&quot;\n)\n\n\n# Running the 'PIPELINE'\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun1 = aiplatform.PipelineJob(\n    display_name=&quot;my-pipeline&quot;,\n    template_path=&quot;pipeline_job.json&quot;,\n    job_id=&quot;mlmd-pipeline-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={\n        &quot;url&quot;: &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n        &quot;bucket&quot;: &quot;my-bucket&quot;\n    },\n    enable_caching=True,\n)\n\n# Submitting the 'PIPELINE'\n\nrun1.submit()\n<\/code><\/pre>\n<p>Now, I will add some complementary comments, which in sum, managed to solve my problem:<\/p>\n<ul>\n<li>First, having the &quot;Logs Viewer&quot; (roles\/logging.viewer) enabled for your user, will greatly help to troubleshoot any existing error in your pipeline (Note: that role worked for me, however you might want to look for a better matching role for you own purposes <a href=\"https:\/\/cloud.google.com\/logging\/docs\/access-control?&amp;_ga=2.212939101.-833851896.1650314090&amp;_gac=1.182768084.1650632490.Cj0KCQjwpImTBhCmARIsAKr58cw9X0TGy2YeN-CFsM4RvEXDH_vL54Ce1ECrWh4_aJNoPEhgtXusUhwaAlIUEALw_wcB#permissions_and_roles\" rel=\"nofollow noreferrer\">here<\/a>). Those errors will appear as &quot;Logs&quot;, which can be accessed by clicking the corresponding button:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jpgfU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jpgfU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ul>\n<li>NOTE: In the picture above, when the &quot;Logs&quot; are displayed, it might be helpful to carefully check each log (close to the time when you created you pipeline), as generally each eof them corresponds with a single warning or error line:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/CpTkZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CpTkZ.png\" alt=\"Verte AI Pipelines Logs\" \/><\/a><\/p>\n<ul>\n<li>Second, the output of my pipeline was a tuple. In my original approach, I just returned the plain tuple, but it is advised to return a <a href=\"https:\/\/docs.python.org\/3\/library\/typing.html#typing.NamedTuple\" rel=\"nofollow noreferrer\">NamedTuple<\/a> instead. In general, if you need to input \/ output one or more &quot;<em>small values<\/em>&quot; (int or str, for any reason), pick a NamedTuple to do so.<\/li>\n<li>Third, when the connection between your pipelines is <code>Input[Dataset]<\/code> or <code>Ouput[Dataset]<\/code>, adding the file extension is needed (and quite easy to forget). Take for instance the ouput of the <code>get_data<\/code> component, and notice how the data is recorded by specifically adding the file extension, i.e. <code>dataset.path + &quot;.csv&quot;<\/code>.<\/li>\n<\/ul>\n<p>Of course, this is a very tiny example, and projects can easily scale to huge projects, however as some sort of &quot;Hello Vertex AI Pipelines&quot; it will work well.<\/p>\n<p>Thank you.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1650870474732,
        "Solution_link_count":6.0,
        "Solution_readability":12.7,
        "Solution_reading_time":68.71,
        "Solution_score_count":4.0,
        "Solution_sentence_count":42.0,
        "Solution_word_count":498.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1547219189528,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":18.8979666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've created an AML Pipeline with a single DatabricksStep. I've need to pass a parameter to the Databricks notebook when I run the published pipeline.<\/p>\n<p>When I run the published pipeline, the Databricks steps always take the default value of the PipelineParameter, no matter what value I choose when I submit the pipeline.<\/p>\n<p>Here the code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>start_date_param = PipelineParameter(name=&quot;StartDate&quot;, default_value='2022-01-19')\n\n# Define data ingestion step\ndata_loading_step = DatabricksStep(\n        name=&quot;Data Loading&quot;,\n        existing_cluster_id=db_cluster_id,\n        notebook_path=data_loading_path,\n        run_name=&quot;Loading raw data&quot;,\n        notebook_params={\n            'StartDate': start_date_param,\n        },\n        compute_target=dbricks_compute,\n        instance_pool_id=instance_pool_id,\n        num_workers=num_workers,\n        allow_reuse=False\n    )\n<\/code><\/pre>\n<p>And this is the Databricks notebook:<\/p>\n<pre><code>dbutils.widgets.text(&quot;StartDate&quot;, &quot;&quot;, &quot;StartDate(YYYY-MM-DD)&quot;)\n<\/code><\/pre>\n<p>The default value of StartDate is 2022-01-19.Even though I set the StartDate parameter to '2021-01-19' the Databricks notebook still takes 2022-01-19 as StartDate.<\/p>\n<p>What am I doing wrong?<\/p>\n<p>Thanks for any help,<\/p>\n<p>G<\/p>",
        "Challenge_closed_time":1642668696820,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642600664140,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an AML Pipeline with a single DatabricksStep and needs to pass a parameter to the Databricks notebook when running the published pipeline. However, the Databricks steps always take the default value of the PipelineParameter, regardless of the value chosen when submitting the pipeline. The user has shared the code and Databricks notebook and is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70771911",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":17.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":18.8979666667,
        "Challenge_title":"How to use PipelineParameter in DatabricksStep (Python)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":175.0,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Poster_created_time":1547219189528,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I've found the solution of the problem. I hope it can be of help to someone.\nIt works if you set all the parameter name and widget name in lower case.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>start_date_param = PipelineParameter(name=&quot;start_date&quot;, default_value='2022-01-19')\n\n# Define data ingestion step\ndata_loading_step = DatabricksStep(\n        name=&quot;Data Loading&quot;,\n        existing_cluster_id=db_cluster_id,\n        notebook_path=data_loading_path,\n        run_name=&quot;Loading raw data&quot;,\n        notebook_params={\n            'start_date': start_date_param,\n        },\n        compute_target=dbricks_compute,\n        instance_pool_id=instance_pool_id,\n        num_workers=num_workers,\n        allow_reuse=False\n    )\n<\/code><\/pre>\n<pre><code>dbutils.widgets.text(&quot;start_date&quot;, &quot;&quot;, &quot;start_date(YYYY-MM-DD)&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":10.81,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1611181716003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":119.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":2450.9835063889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am currently developing an Azure ML pipeline that as one of its outputs is maintaining a SQL table holding all of the unique items that are fed into it. There is no way to know in advance if the data fed into the pipeline is new unique items or repeats of previous items, so before updating the table that it maintains it pulls the data already in that table and drops any of the new items that already appear.<\/p>\n<p>However, due to this there are cases where this self-reference results in zero new items being found, and as such there is nothing to export to the SQL table. When this happens Azure ML throws an error, as it is considered an error for there to be zero lines of data to export. In my case, however, this is expected behaviour, and as such absolutely fine.<\/p>\n<p>Is there any way for me to suppress this error, so that when it has zero lines of data to export it just skips the export module and moves on?<\/p>",
        "Challenge_closed_time":1630895031176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622071490553,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is developing an Azure ML pipeline that maintains a SQL table holding unique items. The pipeline drops any new items that already appear in the table before updating it. However, in cases where there are zero new items, Azure ML throws an error as it is considered an error for there to be zero lines of data to export. The user is looking for a way to suppress this error and skip the export module when there are zero lines of data to export.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67713876",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.6,
        "Challenge_reading_time":12.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2450.9835063889,
        "Challenge_title":"Is there a way to stop Azure ML throwing an error when exporting zero lines of data?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":192,
        "Platform":"Stack Overflow",
        "Poster_created_time":1611181716003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":119.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>This issue has been resolved by an update to Azure Machine Learning; You can now run pipelines with a flag set to &quot;Continue on Failure Step&quot;, which means that steps following the failed data export will continue to run.<\/p>\n<p>This does mean you will need to design your pipeline to be able to handles upstream failures in its downstream modules; this must be done very carefully.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":4.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606390824848,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":336.5876852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using the ML-pipeline designer in MS Azure it is possible to clean missing data, namely by replacing them by means or constant values.<\/p>\n<p>In my dataset I have gaps, when the measured value did not change enough, thus I should want to replace the missing data with the last existing entry.\nSo from<\/p>\n<pre><code>VALUE A\n2\nNONE\nNONE\nNONE\n3\nNONE\nNONE\n<\/code><\/pre>\n<p>I would like to get<\/p>\n<pre><code>VALUE A\n2\n2\n2\n2\n3\n3\n3\n<\/code><\/pre>\n<p>This option is not available in the pipeline designer as far as I know. Can I manipulate the dataset somehow else within Azure, before training?<\/p>",
        "Challenge_closed_time":1647248303807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646036588140,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in replacing missing data points with the last existing entry in MS Azure's ML-pipeline designer. They are looking for a way to manipulate the dataset before training as this option is not available in the pipeline designer.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71292240",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":8.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":336.5876852778,
        "Challenge_title":"How to replace missing datapoints with prior in MS Azure?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":13.0,
        "Challenge_word_count":113,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606390824848,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>I figured it out, by using the Notebooks (do not work in Firefox for me, only on Chrome).\nThere it is possible to handle the dataset in python, transform it to pandas, manipulate it and save it to the datastore.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":2.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":162.2120102778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've created an Azure ML Endpoint Pipeline with a single 'Execute Python Script'.  From the script, I am looking for a way to access the input 'ParameterAssignments' that I POST to the endpoint to trigger the pipeline.  I expected to see them somewhere in Run.get_context(), but I haven't had any luck.  I simply need a way to POST arbitrary values that my Python scripts can access.  Thank you!<\/p>",
        "Challenge_closed_time":1603069686240,
        "Challenge_comment_count":2,
        "Challenge_created_time":1602485723003,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is having trouble accessing input parameter assignments in Azure Machine Learning endpoints. They have created a pipeline with a single 'Execute Python Script' and are looking for a way to access the input 'ParameterAssignments' that they POST to the endpoint to trigger the pipeline. They are unable to find them in Run.get_context() and need a way to POST arbitrary values that their Python scripts can access.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/123204\/how-do-i-access-an-input-parameter-in-azure-machin",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.3,
        "Challenge_reading_time":5.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":162.2120102778,
        "Challenge_title":"How do I access an input parameter in Azure Machine Learning endpoints?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I just confirmed with our engineer that you cannot set up a pipeline parameter and use it without tying it with any of the module parameter. So the workaround is  - make the pipeline parameter as one of the inputs (i.e. dataset) to &quot;Execute Python Script&quot; module and set it as pipeline parameter. Then you can change it every time when calling the pipeline.<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":4.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":3.9519652778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Amazon SageMaker has <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">inference pipelines<\/a> that process requests for inferences on data. It sounds as though inferences are similar (or perhaps identical) to predictions. Are there any differences between inferences and predictions? If so, what? If not, why not just call it a prediction pipeline?<\/p>",
        "Challenge_closed_time":1561569885852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561555658777,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the difference between \"inference\" and \"prediction\" in Amazon SageMaker's inference pipelines. They are questioning whether the terms are interchangeable and why the term \"prediction pipeline\" is not used instead.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56773989",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":6.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.9519652778,
        "Challenge_title":"In Amazon SageMaker, what (if any) is the difference between an inference and prediction?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":42.0,
        "Challenge_word_count":61,
        "Platform":"Stack Overflow",
        "Poster_created_time":1336973807643,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Minneapolis, MN, United States",
        "Poster_reputation_count":1907.0,
        "Poster_view_count":174.0,
        "Solution_body":"<p>Inference usually refers to applying a learned transformation to input data. That learned transformation could be something else than a prediction (eg dim reduction, clustering, entity extraction etc). So calling that process a prediction would be a bit too restrictive in my opinion<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":3.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1555488556820,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Baillargues, France",
        "Answerer_reputation_count":56447.0,
        "Answerer_view_count":9158.0,
        "Challenge_adjusted_solved_time":4.9571033333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a <code>Vertex AI custom training job<\/code> (machine learnin training using custom container) on <code>GCP<\/code>. I would like to create a <code>Pub\/Sub<\/code> message when the job failed so I can post a message on some chat like Slack. Logfile (<code>Cloud Logging)<\/code> is looking like that:<\/p>\n<pre><code>{\ninsertId: &quot;xxxxx&quot;\nlabels: {\nml.googleapis.com\/endpoint: &quot;&quot;\nml.googleapis.com\/job_state: &quot;FAILED&quot;\n}\nlogName: &quot;projects\/xxx\/logs\/ml.googleapis.com%2F1113875647681265664&quot;\nreceiveTimestamp: &quot;2021-07-09T15:05:52.702295640Z&quot;\nresource: {\nlabels: {\njob_id: &quot;1113875647681265664&quot;\nproject_id: &quot;xxx&quot;\ntask_name: &quot;service&quot;\n}\ntype: &quot;ml_job&quot;\n}\nseverity: &quot;INFO&quot;\ntextPayload: &quot;Job failed.&quot;\ntimestamp: &quot;2021-07-09T15:05:52.187968162Z&quot;\n}\n<\/code><\/pre>\n<p>I am creating a Logs Router Sink with the following query:<\/p>\n<pre><code>resource.type=&quot;ml_job&quot; AND textPayload:&quot;Job failed&quot; AND labels.&quot;ml.googleapis.com\/job_state&quot;:&quot;FAILED&quot;\n<\/code><\/pre>\n<p>The issue I am facing is that Vertex AI will retry the job 3 times before declaring the job as a failure but in the logfile the message is identical. Below you have 3 examples, only the last one that failed 3 times really failed at the end.\n<a href=\"https:\/\/i.stack.imgur.com\/NSGPb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NSGPb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>In the logfile, I don't have any count id for example. Any idea how to solve this ? Creating a BigQuery table to keep track of the number of failure per <code>resource.labels.job_id<\/code> seems to be an overkill if I need to do that in all my project. Is there a way to do a group by <code>resource.labels.job_id<\/code> and count within Logs Router Sink ?<\/p>",
        "Challenge_closed_time":1627331282972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1627312838200,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while creating a Logs Router Sink for a Vertex AI custom training job on GCP. The job fails after three attempts, but the log message remains identical. The user wants to create a Pub\/Sub message when the job fails and post it on Slack. However, there is no count ID in the log file, and creating a BigQuery table seems like an overkill. The user is looking for a way to group by resource.labels.job_id and count within Logs Router Sink.",
        "Challenge_last_edit_time":1627313437400,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68532457",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":25.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":5.1235477778,
        "Challenge_title":"How to create a Logs Router Sink when a Vertex AI training job failed (after 3 attempts)?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":216.0,
        "Challenge_word_count":232,
        "Platform":"Stack Overflow",
        "Poster_created_time":1465222092252,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Z\u00fcrich, Switzerland",
        "Poster_reputation_count":1414.0,
        "Poster_view_count":478.0,
        "Solution_body":"<p>The log sink is quite simple: provide a filter, it will publish in a PubSub topic each entry which match this filter. No group by, no count, nothing!!<\/p>\n<p>I propose you to use a combination of log-based metrics and Cloud monitoring.<\/p>\n<ol>\n<li>Firstly, create a <a href=\"https:\/\/cloud.google.com\/logging\/docs\/logs-based-metrics\" rel=\"nofollow noreferrer\">log based metrics<\/a> on your job failed log entry<\/li>\n<li>Create an <a href=\"https:\/\/cloud.google.com\/logging\/docs\/logs-based-metrics\/charts-and-alerts\" rel=\"nofollow noreferrer\">alert on this log based metrics<\/a> with the following key values<\/li>\n<\/ol>\n<ul>\n<li>Set the group by that you want, for example, the jobID (i don't know what is the relevant value for VertexAI job)<\/li>\n<li>Set an alert when the threshold is equal or above 3<\/li>\n<li>Add a notification channel and set a PubSub notification (still in beta)<\/li>\n<\/ul>\n<p>With this configuration, the alert will be posted only once in PubSub when 3 occurrences of the same jobID will occur.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.7,
        "Solution_reading_time":12.96,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":142.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":341.7419444444,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi,\r\n\r\nI created a custom docker container to deploy my model on Vertex AI. The model uses LightGBM, so I can't use the pre-built container images available for TF\/SKL\/XGBoost. I was able to deploy the model and get predictions, but I get errors while trying to get **explainable** predictions from the model. I have tried to follow the Vertex AI guidelines to configure the model for explanations.\r\nThe example below shows a simplified version of the model that still reproduces the issue, with only two input features 'A' and 'B'.\r\n\r\nPlease take a look and tell me if the explanation metadata is supposed to be set differently, or if there is something wrong with this approach.\r\n\r\n\r\n#### Environment details\r\n\r\n  - Google Cloud Notebook\r\n  - Python version: 3.7.12\r\n  - pip version: 21.3.1\r\n  - `google-cloud-aiplatform` version: 1.15.0\r\n\r\n#### Reference\r\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/explainable-ai\/configuring-explanations#custom-container\r\n\r\n#### explanation-metadata.json\r\n(_Model output is unkeyed. The Vertex AI guide suggests using any memorable string for output key._)\r\n```\r\n{\r\n    \"inputs\": {\r\n        \"A\": {},\r\n        \"B\": {}\r\n    },\r\n    \"outputs\": {\r\n        \"Y\": {}\r\n    }\r\n}\r\n```\r\n#### Model upload with explanation parameters and metadata\r\n```\r\n! gcloud ai models upload \\\r\n  --region=$REGION \\\r\n  --display-name=$MODEL_NAME \\\r\n  --container-image-uri=$PRED_IMAGE_URI \\\r\n  --artifact-uri=$ARTIFACT_LOCATION_GCS \\\r\n  --explanation-method=sampled-shapley \\\r\n  --explanation-path-count=10 \\\r\n  --explanation-metadata-file=explanation-metadata.json\r\n```\r\n\r\n#### Prediction\/Explanation Input\r\n```\r\ninstances = [{\"A\": 1.1, \"B\": 20}, {\"A\": 2.2, \"B\": 21}]\r\n# Prediction (works fine):\r\nendpoint.predict(instances=instances)\r\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\r\nendpoint.explain(instances=instances) # Returns error (1) shown in stack trace below\r\n\r\n# Another example\r\ninstances_2 = [[1.1,20], [2.2,21]]\r\n# Prediction (works fine):\r\nendpoint.predict(instances=instances_2)\r\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\r\nendpoint.explain(instances=instances_2) # Returns error\r\n# Error: Nameless inputs are allowed only if there is a single input in the explanation metadata.\r\n```\r\n#### Prediction Server (Flask)\r\n```python\r\n# Custom Flask server to serve online predictions\r\n# Input for prediction\r\nraw_input = request.get_json()\r\ninput = raw_input['instances']\r\ndf = pd.DataFrame(input, columns = ['A', 'B'])\r\n# Prediction from model (loaded from GCP bucket)\r\npredictions = model.predict(df).tolist() # [0, 1]\r\nresponse = jsonify({\"predictions\": predictions})\r\nreturn response\r\n```\r\n\r\n#### Stack trace of error (1)\r\n```\r\n---------------------------------------------------------------------------\r\n_InactiveRpcError                         Traceback (most recent call last)\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\r\n     49         try:\r\n---> 50             return callable_(*args, **kwargs)\r\n     51         except grpc.RpcError as exc:\r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\r\n    945                                       wait_for_ready, compression)\r\n--> 946         return _end_unary_response_blocking(state, call, False, None)\r\n    947 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\r\n    848     else:\r\n--> 849         raise _InactiveRpcError(state)\r\n    850 \r\n\r\n_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\r\n\tstatus = StatusCode.INVALID_ARGUMENT\r\n\tdetails = \"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\"\r\n\tdebug_error_string = \"{\"created\":\"@1658310559.755090975\",\"description\":\"Error received from peer ipv4:74.125.133.95:443\",\"file\":\"src\/core\/lib\/surface\/call.cc\",\"file_line\":1069,\"grpc_message\":\"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\",\"grpc_status\":3}\"\r\n>\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nInvalidArgument                           Traceback (most recent call last)\r\n\/tmp\/ipykernel_2590\/4024017963.py in <module>\r\n----> 3 print(endpoint.explain(instances=instances, parameters={}))\r\n\r\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in explain(self, instances, parameters, deployed_model_id, timeout)\r\n   1563             parameters=parameters,\r\n   1564             deployed_model_id=deployed_model_id,\r\n-> 1565             timeout=timeout,\r\n   1566         )\r\n   1567 \r\n\r\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1\/services\/prediction_service\/client.py in explain(self, request, endpoint, instances, parameters, deployed_model_id, retry, timeout, metadata)\r\n    917             retry=retry,\r\n    918             timeout=timeout,\r\n--> 919             metadata=metadata,\r\n    920         )\r\n    921 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method.py in __call__(self, timeout, retry, *args, **kwargs)\r\n    152             kwargs[\"metadata\"] = metadata\r\n    153 \r\n--> 154         return wrapped_func(*args, **kwargs)\r\n    155 \r\n    156 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\r\n     50             return callable_(*args, **kwargs)\r\n     51         except grpc.RpcError as exc:\r\n---> 52             raise exceptions.from_grpc_error(exc) from exc\r\n     53 \r\n     54     return error_remapped_callable\r\n\r\nInvalidArgument: 400 {\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\r\n---------------------------------------------------------------------------\r\n```",
        "Challenge_closed_time":1659550833000,
        "Challenge_comment_count":8,
        "Challenge_created_time":1658320562000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue with the outdated Vertex AI blog post after the 0.20.0 release. The guide to run a pipeline using Vertex AI fails because ZenML does not have a `metadata-store` stack category. The user tried to run `zenml metadata-store` but received an error message. Without adding the `metadata-store`, the Vertex AI pipeline fails.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/googleapis\/python-aiplatform\/issues\/1526",
        "Challenge_link_count":1,
        "Challenge_participation_count":8,
        "Challenge_readability":14.2,
        "Challenge_reading_time":79.22,
        "Challenge_repo_contributor_count":75.0,
        "Challenge_repo_fork_count":188.0,
        "Challenge_repo_issue_count":1846.0,
        "Challenge_repo_star_count":283.0,
        "Challenge_repo_watch_count":53.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":341.7419444444,
        "Challenge_title":"Error while trying to get explanation from (custom container) model deployed on Vertex AI (Prediction without explanation works fine)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":578,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @jaycee-li,\r\nAny update on this? Would really appreciate your inputs! Hi @pankajrsingla, sorry for the late reply!\r\n\r\nSince instance_2 prediction works for your model, looks like your model takes unkeyed input. Could you please try this metadata setting:\r\n```\r\n{\r\n    \"inputs\": {\r\n        \"X\": {},\r\n    },\r\n    \"outputs\": {\r\n        \"Y\": {}\r\n    }\r\n}\r\n```\r\nThen update the model, endpoint, and try:\r\n```\r\ninstances = [[1.1,20], [2.2,21]]\r\nendpoint.explain(instances=instances)\r\n```\r\n\r\nPlease let me know if this works for you. Hi @jaycee-li,\r\nThank you so much for your response.\r\nI tried your suggestion, but I got the same error as before.\r\n`Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\"`\r\n\r\nIf you see the code for my prediction server, it can take both unkeyed as well as keyed input (prediction works fine for both cases), since it converts the input to a dataframe. The output is definitely unkeyed. However, I am still confused as to what should be the contents of the explanation-metadata.json file.\r\n\r\nAlso, just to be sure - the same API (predict) in the flask server is supposed to work for both predictions and explanations, right? Or do I need to create a separate API for 'explain'?\r\n\r\nIf you have any other suggestions, I would be more than happy to try them out. \r\n(If that would help, I can also send you the full contents of the Jupyter notebook - all code one place - if you share your email id.)\r\n\r\nPlease let me know!\r\n\r\nThank you! It would be helpful if you can share the notebook to jayceeli@google.com\r\n\r\nThank you very much! Done!\r\nThanks! :) Hi @pankajrsingla ,\r\n\r\nI got `AttributeError: 'Blob' object has no attribute 'open'` for `with blob.open(\"wb\") as f:` in your TRAIN_IMAGE_URI. So I was stuck here and didn't reproduce the error you got. \r\n\r\nYou mixed CLI, gapic API, and SDK in your code. Since I'm not familiar with CLI tool, I'm not very sure what the problem is. Maybe it's due to your PRED_IMAGE_URI? I would suggest you to try a pre-built container(`us-docker.pkg.dev\/vertex-ai\/prediction\/sklearn-cpu.1-0:latest`) when uploading the model.\r\n\r\nI drafted a notebook that used SDK only to train, upload, deploy a same model as yours. And it can successfully make predictions and explanations. I've shared the notebook with you for your reference.\r\n\r\nPlease let me know if you still get the error. Thanks! Hi @pankajrsingla ,\r\n\r\nPlease check this [notebook](https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/ml_ops\/stage6\/get_started_with_xai_and_custom_server.ipynb) (Specifically **Create the model server** and **Build a FastAPI HTTP server** sections) for how to use XAI with a custom container. Thanks a lot, @jaycee-li! This is exactly what I was looking for!\r\nI will give this a try for my model, and will update you once I have the results. This should work.\r\n\r\nThank you!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":5.5,
        "Solution_reading_time":35.96,
        "Solution_score_count":null,
        "Solution_sentence_count":38.0,
        "Solution_word_count":449.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1342713532568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seoul, South Korea",
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":3.9274925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In DVC one may define pipelines.  In Unix, one typically does not work at the root level.  Further, DVC expects files to be inside the git repository.<\/p>\n<p>So, this seems like a typical problem.<\/p>\n<p>Suppose I have the following:<\/p>\n<pre><code>\/home\/user\/project\/content-folder\/data\/data-type\/cfg.json\n\/home\/user\/project\/content-folder\/app\/foo.py\n<\/code><\/pre>\n<p>Git starts at <code>\/home\/user\/project\/<\/code><\/p>\n<pre><code>cd ~\/project\/content-folder\/data\/data-type\n..\/..\/app\/foo.py do-this --with cfg.json --dest $(pwd) \n<\/code><\/pre>\n<p>Seems reasonable to me: the script takes a configuration, which is stored in a particular location, runs it against some encapsulated functionality, and outputs it to the destination using an absolute path.<\/p>\n<p>The default behavior of <code>--dest<\/code> is to output to the current working directory.  This seems like another reasonable default.<\/p>\n<hr \/>\n<p>Next, I go to configure the <code>params.yaml<\/code> file for <code>dvc<\/code>, and I am immediately confusing and unsure what is going to happen.  I write:<\/p>\n<pre><code>foodoo:\n  params: do-this --with ????\/cfg.json --dest ????\n<\/code><\/pre>\n<p>What I want to write (and would in a shell script):<\/p>\n<pre><code>#!\/usr\/bin\/env bash\norigin:=$(git rev-parse --show-toplevel)\n\nverb=do-this\nparams=--with $(origin)\/content-folder\/data\/data-type\/cfg.json --dest $(origin)\/content-folder\/data\/data-type\n<\/code><\/pre>\n<hr \/>\n<p>But, in DVC, the pathing seems to be implicit, and I do not know where to start as either:<\/p>\n<ol>\n<li>DVC will calculate the path to my script locally<\/li>\n<li>Not calculate the path to my script locally<\/li>\n<\/ol>\n<p>Which is fine -- I can discover that.  But I am reasonably sure that DVC will absolutely not prefix the directory and file params in my params.yaml with the path to my project.<\/p>\n<hr \/>\n<p>How does one achieve path control that does not assume a fixed project location, like I would in BASH?<\/p>",
        "Challenge_closed_time":1608686869860,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608672730887,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with defining pipelines in DVC due to the use of absolute paths and project paths in the pipeline parameters. While the user is able to run the script with absolute paths in Unix, they are unsure how to configure the params.yaml file in DVC to achieve the same path control without assuming a fixed project location. The user is confused about the implicit pathing in DVC and is unsure if DVC will calculate the path to their script locally or not.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65416056",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":25.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":3.9274925,
        "Challenge_title":"Data Version Control: Absolute Paths and Project Paths in the Pipeline Parameters?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":448.0,
        "Challenge_word_count":262,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405262190020,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Atlanta, GA",
        "Poster_reputation_count":26244.0,
        "Poster_view_count":1383.0,
        "Solution_body":"<p>By default, DVC will run your stage command from the same directory as the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files#dvcyaml-file\" rel=\"nofollow noreferrer\">dvc.yaml<\/a> file. If you need to run the command from a different location, you can specify an alternate working directory via <code>wdir<\/code>, which should be a path relative to <code>dvc.yaml<\/code>'s location.<\/p>\n<p>Paths for everything else in your stage (like <code>params.yaml<\/code>) should be specified as relative to <code>wdir<\/code> (or relative to <code>dvc.yaml<\/code> if <code>wdir<\/code> is not provided).<\/p>\n<p>Looking at your example, there also seems to be a bit of confusion on parameters in DVC. In a DVC stage, <code>params<\/code> is for specifying <a href=\"https:\/\/dvc.org\/doc\/command-reference\/params\" rel=\"nofollow noreferrer\">parameter dependencies<\/a>, not used for specifying command-line flags. The full command including flags\/options should be included  the <code>cmd<\/code> section for your stage. If you wanted to make sure that your stage was rerun every time certain values in <code>cfg.json<\/code> have changed, your stage's <code>params<\/code> section would look something like:<\/p>\n<pre><code>params:\n  &lt;relpath from dvc.yaml&gt;\/cfg.json:\n    - param1\n    - param2\n    ...\n<\/code><\/pre>\n<p>So your example <code>dvc.yaml<\/code> would look something like:<\/p>\n<pre><code>stages:\n  foodoo:\n    cmd: &lt;relpath from dvc.yaml&gt;\/foo.py do-this --with &lt;relpath from dvc.yaml&gt;\/cfg.json --dest &lt;relpath from dvc.yaml&gt;\/...\n    deps:\n      &lt;relpath from dvc.yaml&gt;\/foo.py\n    params:\n      &lt;relpath from dvc.yaml&gt;\/cfg.json:\n        ...\n    ...\n<\/code><\/pre>\n<p>This would make the command <code>dvc repro<\/code> rerun your stage any time that the code in foo.py has changed, or the specified parameters in <code>cfg.json<\/code> have changed.<\/p>\n<p>You may also want to refer to the docs for <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#run\" rel=\"nofollow noreferrer\">dvc run<\/a>, which can be used to generate or update a <code>dvc.yaml<\/code> stage (rather than writing <code>dvc.yaml<\/code> by hand)<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":9.4,
        "Solution_reading_time":26.99,
        "Solution_score_count":2.0,
        "Solution_sentence_count":26.0,
        "Solution_word_count":248.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.5649966667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>There is very less components compared to classical mode, how can I use prebuilt components in custom mode? Is that possible? How should I get it? <\/p>\n<p>Can I get some help here? Much appreciated.<\/p>",
        "Challenge_closed_time":1680082887128,
        "Challenge_comment_count":2,
        "Challenge_created_time":1680044853140,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in using prebuilt components in custom pipeline mode as there are very few components available compared to classical mode. They are seeking help on how to obtain and use prebuilt components in custom mode.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1194061\/can-i-use-prebuilt-component-in-custom-pipeline-mo",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.5,
        "Challenge_reading_time":3.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":10.5649966667,
        "Challenge_title":"can I use prebuilt component in custom pipeline mode?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":43,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello @merten<\/p>\n<p>Thanks for reaching out to us, as you know Designer supports two type of components, classic prebuilt components and custom components. These two types of components <strong>are not compatible. So a quick answer for your question is you can not use it together.<\/strong><\/p>\n<p>Classic prebuilt components provides prebuilt components majorly for data processing and traditional machine learning tasks like regression and classification. This type of component continues to be supported but will not have any new components added.<\/p>\n<p>Custom components allow you to provide your own code as a component. It supports sharing across workspaces and seamless authoring across Studio, CLI, and SDK interfaces.<\/p>\n<p>I am sorry for all inconveniences. If you can share more details about your scenario, we are happy to discuss with product team.<\/p>\n<p>Regards,<\/p>\n<p>Yutong<\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":12.56,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":147.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":625.9599527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How to I properly cancel all child runs in an Azure ML experiment? When I use the code below as expected from documentation, I get an error. &quot;RunConfigurationException:  <br \/>\nMessage: Error in deserialization. dict fields don't have list element type information. field=output_data, list_element_type=&lt;class 'azureml.core.runconfig.OutputData'&gt;...} with exception <strong>init<\/strong>() missing 2 required positional arguments: 'datastore_name' and 'relative_path'&quot;<\/p>\n<p>run = Run.get(ws, 'run-id-123456789')<\/p>\n<p>for child in run.get_children():  <br \/>\nprint(child.get_details())  <br \/>\ntry:  <br \/>\nchild.cancel()  <br \/>\nexcept Exception as e:  <br \/>\nprint(e)  <br \/>\ncontinue<\/p>\n<p>The datasets and runs were configured properly because they run just fine.<\/p>",
        "Challenge_closed_time":1651506755547,
        "Challenge_comment_count":1,
        "Challenge_created_time":1649253299717,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to cancel all child runs in an Azure ML experiment using the provided code, which results in a RunConfigurationException error. The datasets and runs were configured properly, but the code is not working as expected.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/802549\/cancel-all-child-runs-in-azure-ml",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.0,
        "Challenge_reading_time":10.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":625.9599527778,
        "Challenge_title":"Cancel all child runs in Azure ML",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":95,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>You should cancel all the children run by canceling the parent.   <\/p>\n<p>Any benefit to cancel child once a time? Just curious <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":1.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":426.2658333333,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug description\r\n\r\nThe pipeline `sentence_embedding\/dvc.yaml` is not correctly defined for `evaluation:deps`.\r\n\r\nThis creates the following issues:\r\n  - The evaluation stage does not know how to pull the model `biobert_nli_sts_cord19_v1\/`.\r\n  - The training stage does not know it has to run before the evaluation stage for the models `tf_idf\/` and `count\/`.\r\n\r\n## To reproduce\r\n\r\n```\r\ngit checkout 12988ef564dd4e6373a7455f5ee30c0608e2e972\r\nexport PIPELINE=data_and_models\/pipelines\/sentence_embedding\/dvc.yaml\r\ndvc pull -d $PIPELINE\r\ndvc repro -f $PIPELINE\r\n```\r\n\r\nThis will give the error:\r\n```\r\nRunning stage 'data_and_models\/pipelines\/sentence_embedding\/dvc.yaml:evaluation@biobert_nli_sts_cord19_v1':\r\n...\r\nAttributeError: Path ..\/..\/models\/sentence_embedding\/biobert_nli_sts_cord19_v1\/ not found\r\n```\r\n\r\nAfter manually pulling `biobert_nli_sts_cord19_v1`, this will give the error:\r\n```\r\nRunning stage 'data_and_models\/pipelines\/sentence_embedding\/dvc.yaml:evaluation@tf_idf':\r\n...\r\nFileNotFoundError: [Errno 2] No such file or directory: '..\/..\/models\/sentence_embedding\/tf_idf\/model.pkl'\r\n```\r\n\r\n## Expected behavior\r\n\r\n`dvc pull -d` and `dvc repro -f` should run without errors about missing files.",
        "Challenge_closed_time":1626683431000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625148874000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with znNodes not working with `dvc.<...>` and is trying to fix the docstring and test it with a Node that has `dvc.params` and `dvc.outs`.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/BlueBrain\/Search\/issues\/396",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":13.8,
        "Challenge_reading_time":16.11,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":7.0,
        "Challenge_repo_issue_count":644.0,
        "Challenge_repo_star_count":29.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":426.2658333333,
        "Challenge_title":"Fix the definition of pipelines\/sentence_embedding\/dvc.yaml",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":118,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1368311783008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Diego, CA",
        "Answerer_reputation_count":1297.0,
        "Answerer_view_count":165.0,
        "Challenge_adjusted_solved_time":1972.6603797222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need to import function from different python scripts, which will used inside <code>preprocessing.py<\/code> file. I was not able to find a way to pass the dependent files to <code>SKLearnProcessor<\/code> Object, due to which I am getting <code>ModuleNotFoundError<\/code>.<\/p>\n<p><strong>Code:<\/strong><\/p>\n<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     instance_type='ml.m5.xlarge',\n                                     instance_count=1)\n\n\nsklearn_processor.run(code='preprocessing.py',\n                      inputs=[ProcessingInput(\n                        source=input_data,\n                        destination='\/opt\/ml\/processing\/input')],\n                      outputs=[ProcessingOutput(output_name='train_data',\n                                                source='\/opt\/ml\/processing\/train'),\n                               ProcessingOutput(output_name='test_data',\n                                                source='\/opt\/ml\/processing\/test')],\n                      arguments=['--train-test-split-ratio', '0.2']\n                     )\n<\/code><\/pre>\n<p>I would like to pass,\n<code>dependent_files = ['file1.py', 'file2.py', 'requirements.txt']<\/code>. So, that <code>preprocessing.py<\/code> have access to all the dependent modules.<\/p>\n<p>And also need to install libraries from <code>requirements.txt<\/code> file.<\/p>\n<p>Can you share any work around or a right way to do this?<\/p>\n<p><strong>Update-25-11-2021:<\/strong><\/p>\n<p><strong>Q1.<\/strong>(Answered but looking to solve using <code>FrameworkProcessor<\/code>)<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/99f023e76a5db060907a796d4d8fee550f005844\/src\/sagemaker\/processing.py#L1426\" rel=\"noreferrer\">Here<\/a>, the <code>get_run_args<\/code> function, is handling <code>dependencies<\/code>, <code>source_dir<\/code> and <code>code<\/code> parameters by using <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/99f023e76a5db060907a796d4d8fee550f005844\/src\/sagemaker\/processing.py#L1265\" rel=\"noreferrer\">FrameworkProcessor<\/a>. Is there any way that we can set this parameters from <code>ScriptProcessor<\/code> or <code>SKLearnProcessor<\/code> or any other <code>Processor<\/code> to set them?<\/p>\n<p><strong>Q2.<\/strong><\/p>\n<p>Can you also please show some reference to use our <code>Processor<\/code> as <code>sagemaker.workflow.steps.ProcessingStep<\/code> and then use in <code>sagemaker.workflow.pipeline.Pipeline<\/code>?<\/p>\n<p>For having <code>Pipeline<\/code>, do we need <code>sagemaker-project<\/code> as mandatory or can we create <code>Pipeline<\/code> directly without any <code>Sagemaker-Project<\/code>?<\/p>",
        "Challenge_closed_time":1637782762627,
        "Challenge_comment_count":5,
        "Challenge_created_time":1630681185260,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in passing dependent files to the SKLearnProcessor object in Sagemaker, resulting in a ModuleNotFoundError. They are seeking a workaround or a proper way to pass dependent modules and install libraries from requirements.txt file. Additionally, they are looking for a way to set parameters like dependencies, source_dir, and code from ScriptProcessor or SKLearnProcessor or any other Processor to set them. They also want to know how to use their Processor as sagemaker.workflow.steps.ProcessingStep and then use it in sagemaker.workflow.pipeline.Pipeline.",
        "Challenge_last_edit_time":1637936310430,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69046990",
        "Challenge_link_count":2,
        "Challenge_participation_count":7,
        "Challenge_readability":16.5,
        "Challenge_reading_time":34.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":11.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":1972.6603797222,
        "Challenge_title":"How to pass dependency files to sagemaker SKLearnProcessor and use it in Pipeline?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":2139.0,
        "Challenge_word_count":201,
        "Platform":"Stack Overflow",
        "Poster_created_time":1500824148408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":4419.0,
        "Poster_view_count":962.0,
        "Solution_body":"<p>There are a couple of options for you to accomplish that.<\/p>\n<p>One that is really simple is adding all additional files to a folder, example:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 my_package\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file1.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file2.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 preprocessing.py\n<\/code><\/pre>\n<p>Then send this entire folder as another input under the same <code>\/opt\/ml\/processing\/input\/code\/<\/code>, example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor = SKLearnProcessor(\n    framework_version=&quot;0.20.0&quot;,\n    role=role,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    instance_count=1,\n)\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,  # &lt;- this gets uploaded as \/opt\/ml\/processing\/input\/code\/preprocessing.py\n    inputs=[\n        ProcessingInput(source=input_data, destination='\/opt\/ml\/processing\/input'),\n        # Send my_package as \/opt\/ml\/processing\/input\/code\/my_package\/\n        ProcessingInput(source='my_package\/', destination=&quot;\/opt\/ml\/processing\/input\/code\/my_package\/&quot;)\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n<\/code><\/pre>\n<p>What happens is that <code>sagemaker-python-sdk<\/code> is going to put your argument <code>code=&quot;preprocessing.py&quot;<\/code> under <code>\/opt\/ml\/processing\/input\/code\/<\/code> and you will have <code>my_package\/<\/code> under the same directory.<\/p>\n<p><strong>Edit:<\/strong><\/p>\n<p>For the <code>requirements.txt<\/code>, you can add to your <code>preprocessing.py<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nsubprocess.check_call([\n    sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;-r&quot;,\n    &quot;\/opt\/ml\/processing\/input\/code\/my_package\/requirements.txt&quot;,\n])\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1637783228436,
        "Solution_link_count":0.0,
        "Solution_readability":23.3,
        "Solution_reading_time":27.94,
        "Solution_score_count":17.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":134.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458548318740,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":1568.0,
        "Answerer_view_count":266.0,
        "Challenge_adjusted_solved_time":873.3076302778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a DAG in Airflow with SageMakerOperators and I have not been able to make them work. The title is the error that appears in the airflow GUI. For solving it, I have made the following tries:<\/p>\n\n<pre><code>sudo pip3 uninstall urllib3 &amp;&amp; sudo pip3 install urllib3==1.22 \nsudo pip3 install urllib3==1.22 --upgrade\nsudo pip3 install urllib3==1.22 -t \/home\/ubuntu\/.local\/lib\/python3.7\/site-packages -upgrade\n<\/code><\/pre>\n\n<p>But I am still getting the error in the GUI. Plus, in the console of the webserver I am getting:<\/p>\n\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages\/urllib3-1.22.dist-info\/METADATA'\n<\/code><\/pre>\n\n<p>The thing is that if I make <code>pip3 show urllib3<\/code> I get the version 1.22:\n<a href=\"https:\/\/i.stack.imgur.com\/i5y8i.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/i5y8i.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>However, it says dist-packages instead of site-packages. In addition, trying to go to <code>\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages\/urllib3-1.22.dist-info\/<\/code> for trying to solve the metadata file not found error, the directory does not exists. \n<a href=\"https:\/\/i.stack.imgur.com\/44H4I.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/44H4I.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/2CnJl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2CnJl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am totally lost at this point. How could I solve this problem?<\/p>",
        "Challenge_closed_time":1571189751312,
        "Challenge_comment_count":3,
        "Challenge_created_time":1568045843843,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a DAG in Airflow with SageMakerOperators but is unable to make them work due to the error \"Broken DAG: urllib3 1.25.3 (\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages), Requirement.parse('urllib3<1.25,>=1.21'), {'sagemaker'}\". The user has tried to solve the issue by uninstalling and reinstalling urllib3, but the error persists. Additionally, the console of the webserver shows a \"metadata file not found\" error. The user is unsure how to solve the problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57857726",
        "Challenge_link_count":6,
        "Challenge_participation_count":4,
        "Challenge_readability":10.4,
        "Challenge_reading_time":22.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":873.3076302778,
        "Challenge_title":"Broken DAG: urllib3 1.25.3 (\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages), Requirement.parse('urllib3<1.25,>=1.21'), {'sagemaker'}",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":343.0,
        "Challenge_word_count":181,
        "Platform":"Stack Overflow",
        "Poster_created_time":1523298968403,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1754.0,
        "Poster_view_count":197.0,
        "Solution_body":"<p>Here you go.<\/p>\n\n<p>Airflow is looking in the local (user) Python installation for the library but <code>urllib3<\/code> is installed for all users. It's weird but try doing <code>pip3 install --user urllib3==1.22<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":2.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":14.9545102778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Advanced Scoring Scripting for AzureML webservice, to automatically generate a schema for our web service, we provide a sample of the input and\/or output in the constructor for one of the defined type objects. The type and sample are used to automatically create the schema.\nTo use schema generation, we include the open-source inference-schema package version 1.1.0 or above. The types that I can find include Numpy Type, Pandas Type, Abstract Parameter type.\nHow do we define the schema for a Nested Dictionary of (generalized) format:<\/p>\n<pre><code>{    &quot;top_level_key&quot;: [\n                         {&quot;nested_key_1&quot;: &quot;string_1&quot;,\n                          &quot;nested_key_2&quot;: &lt;float_number&gt;, \n                          &quot;nested_key_3&quot;: &lt;True\/False&gt;}\n                      ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1622006064007,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621952227770,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to generate an inference schema for a nested dictionary with a specific format using the Azure InferenceSchema package. They are using Advanced Scoring Scripting for AzureML webservice and need to provide a sample of the input and\/or output in the constructor for one of the defined type objects to automatically create the schema. The user is looking for information on how to define the schema for a nested dictionary with a specific format.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67689868",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":10.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":14.9545102778,
        "Challenge_title":"How to generate Inference Schema for Dictionary with nested structure using Azure InferenceSchema package?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":172.0,
        "Challenge_word_count":109,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>we don\u2019t have a good way to extend the handling for generic Python class objects. However, we are planning to add support for that, basically by providing more information on the necessary hooks, and allowing users to extend a base class to implement the hook to match the desired class structure.\nThese types are currently supported:<\/p>\n<p>pandas\nnumpy\npyspark\nStandard Python object<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#automatically-generate-a-swagger-schema\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#automatically-generate-a-swagger-schema<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.0,
        "Solution_reading_time":9.37,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1535502574980,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Scottsdale, AZ, USA",
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":57.7218555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a rule of thumb for how to choose the number of epochs per trial in <a href=\"https:\/\/optuna.org\/\" rel=\"nofollow noreferrer\">Optuna<\/a>?<\/p>",
        "Challenge_closed_time":1612636645883,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612428847203,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to determine the appropriate number of epochs per trial in Optuna.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66042246",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":2.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":57.7218555556,
        "Challenge_title":"How can I choose the right number of epochs per trial in Optuna?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":253.0,
        "Challenge_word_count":33,
        "Platform":"Stack Overflow",
        "Poster_created_time":1593039974520,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":235.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>I would imagine epochs are directly correlated with your computational costs, but perhaps that's a parameter worth optimizing. If you aren't sure, start with your best guess and run a few optimization studies with different epoch values. Once you confirm the importance of your epochs, you can conduct separate studies on just the epoch value.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":4.35,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1513106638900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":276.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":1546.0056783333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am building a time series usecase to automate the preprocess and retrain tasks.At first the data is preprocessed using numpy, pandas, statsmodels etc &amp; later a machine learning algorithm is applied to make predictions.\nThe reason for using inference pipeline is that it reuses the same preprocess code for training and inference. I have checked the examples given by AWS sagemaker team with spark and sci-kit learn. In both the examples they use a sci-kit learn container to fit &amp; transform their preprocess code. Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code? <\/p>\n\n<p>Can someone give me a custom example of using these pipelines? Any help is appreciated!<\/p>\n\n<p><strong>Sources looked into:<\/strong><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline<\/a>\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia<\/a><\/p>",
        "Challenge_closed_time":1575505485728,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574408498230,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is building a time series use case to automate preprocess and retrain tasks using AWS Sagemaker. They want to use an inference pipeline to reuse the same preprocess code for training and inference. However, they are unsure if they need to create a container as the examples provided by AWS Sagemaker team use sci-kit learn containers. The user is seeking a custom example of using these pipelines.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58989610",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":19.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":304.7187494444,
        "Challenge_title":"How to custom code an inference pipeline in AWS sagemaker?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1186.0,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553712330910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":103.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Apologies for the late response.<\/p>\n\n<p>Below is some documentation on inference pipelines:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html<\/a>\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-real-time.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-real-time.html<\/a><\/p>\n\n<blockquote>\n  <p>Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code?<\/p>\n<\/blockquote>\n\n<p>Your container is an encapsulation of the environment needed for your custom code needed to run properly. Based on the requirements listed above, <code>numpy, pandas, statsmodels etc &amp; later a machine learning algorithm<\/code>, I would create a container if you wish to isolate your dependencies or modify an existing predefined SageMaker container, such as the scikit-learn one, and add your dependencies into that.<\/p>\n\n<blockquote>\n  <p>Can someone give me a custom example of using these pipelines? Any help is appreciated!<\/p>\n<\/blockquote>\n\n<p>Unfortunately, the two example notebooks referenced above are the only examples utilizing inference pipelines. The biggest hurdle most likely is creating containers that fulfill the preprocessing and prediction task you are seeking and then combining those two together into the inference pipeline.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1579974118672,
        "Solution_link_count":4.0,
        "Solution_readability":15.8,
        "Solution_reading_time":19.52,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":168.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":139.4886111111,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nThere are some errors: https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3402182291\/jobs\/5657762171#step:3:1022\r\n\r\n```\r\n=========================== short test summary info ============================\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\n======================== 48 warnings, 3 errors in 3.79s ========================\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nINFO:submit_groupwise_azureml_pytest.py:Test execution completed!\r\n\r\n```\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":1668591607000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1668089448000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with the SASRec integration test taking an unusually long time on an AzureML compute cluster triggered using a GitHub workflow. The runtime varies significantly from the ADO pipeline, and both machines are of the same type and use the same CUDA and CuDNN versions. The user needs to investigate why this is happening.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1841",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":32.96,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":139.4886111111,
        "Challenge_title":"[BUG] Error in some of the AzureML tests",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":152,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@pradnyeshjoshi any thoughts for this error?",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":0.57,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":6.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":58.5657183334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am interested in knowing how can I integrate a repository with Azure Machine Learning Workspace.<\/p>\n<h2>What have I tried ?<\/h2>\n<p>I have some experience with Azure Data Factory and usually I have setup workflows where<\/p>\n<ol>\n<li><p>I have a <code>dev<\/code> azure data factory instance that is linked to azure repository.<\/p>\n<\/li>\n<li><p>Changes made to the repository using the code editor.<\/p>\n<\/li>\n<li><p>These changes are published via the <code>adf_publish<\/code> branch to the live <code>dev<\/code> instance<\/p>\n<\/li>\n<li><p>I use CI \/ CD pipeline and the AzureRMTemplate task to deploy the templates in the publish branch to release the changes to <code>production<\/code> environment<\/p>\n<\/li>\n<\/ol>\n<h2>Question:<\/h2>\n<ul>\n<li>How can I achieve the same \/ similar workflow with Azure Machine Learning Workspace ?<\/li>\n<li>How is CI \/ CD done with Azure ML Workspace<\/li>\n<\/ul>",
        "Challenge_closed_time":1655682328283,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655471491697,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on how to integrate a repository with Azure Machine Learning Workspace and how to implement CI\/CD pipeline for the same. They have experience with Azure Data Factory and have set up workflows using CI\/CD pipeline and AzureRMTemplate task to deploy templates to the production environment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72659937",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":11.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":58.5657183334,
        "Challenge_title":"CI \/ CD and repository integration for Azure ML Workspace",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":136,
        "Platform":"Stack Overflow",
        "Poster_created_time":1271093246887,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United States",
        "Poster_reputation_count":9826.0,
        "Poster_view_count":1238.0,
        "Solution_body":"<p>The following workflow is the official practice to be followed to achieve the task required.<\/p>\n<ol>\n<li>Starting with the architecture mentioned below<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KdRUa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KdRUa.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ul>\n<li>we need to have a specific data store to handle the dataset<\/li>\n<li>Perform the regular code modifications using the IDE like Jupyter Notebook or VS Code<\/li>\n<li>Train and test the model<\/li>\n<li>To register and operate on the model, deploy the model image as a web service and operate the rest.<\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>Configure the CI Pipeline:<\/strong><\/li>\n<\/ol>\n<ul>\n<li><p>Follow the below steps to complete the procedure<\/p>\n<p><strong>Before implementation:<\/strong><\/p>\n<pre><code>- We need azure subscription enabled account\n- DevOps activation must be activated.\n<\/code><\/pre>\n<\/li>\n<li><p>Open DevOps portal with enabled SSO<\/p>\n<\/li>\n<li><p>Navigate to <strong>Pipeline -&gt; Builds -&gt; Choose the model which was created -&gt; Click on EDIT<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/yUVZl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/yUVZl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Build pipeline will be looking like below screen\n<a href=\"https:\/\/i.stack.imgur.com\/VSKJq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VSKJq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>We need to use Anaconda distribution for this example to get all the dependencies.<\/p>\n<\/li>\n<li><p>To install environment dependencies, check the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/tasks\/package\/conda-environment?view=azure-devops&amp;viewFallbackFrom=azdevops\" rel=\"nofollow noreferrer\">link<\/a><\/p>\n<\/li>\n<li><p>Use the python environment, under <strong>Install Requirements<\/strong> in user setup.<\/p>\n<\/li>\n<li><p>Select <strong>create or get workspace<\/strong> select your account subscription as mentioned in below screen<\/p>\n<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/vt0el.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vt0el.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ul>\n<li>Save the changes happened in other tasks and all those muse be in same subscription.\n<a href=\"https:\/\/i.stack.imgur.com\/WJxCL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WJxCL.png\" alt=\"enter image description here\" \/><\/a><\/li>\n<\/ul>\n<p>The entire CI\/CD procedure and solution was documented in <a href=\"https:\/\/www.azuredevopslabs.com\/labs\/vstsextend\/aml\/#author-praneet-singh-solanki\" rel=\"nofollow noreferrer\">link<\/a><\/p>\n<p><strong>Document Credit: Praneet Singh Solanki<\/strong><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":12.0,
        "Solution_readability":14.0,
        "Solution_reading_time":36.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":278.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":1796.2886094445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>My <code>API_ENDPOINT<\/code> is set to <code>europe-west1-aiplatform.googleapis.com<\/code>.<\/p>\n<p>I define a pipeline:<\/p>\n<pre><code>def pipeline(project: str = PROJECT_ID, region: str = REGION, api_endpoint: str = API_ENDPOINT):\n<\/code><\/pre>\n<p>when I run it:<\/p>\n<pre><code>job = aip.PipelineJob(\ndisplay_name=DISPLAY_NAME,\ntemplate_path=&quot;image classification_pipeline.json&quot;.replace(&quot; &quot;, &quot;_&quot;),)\njob.run()\n<\/code><\/pre>\n<p>it is always created in USandA:<\/p>\n<pre><code>INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. \nResource name: projects\/my_proj_id\/locations\/us-central1\/pipelineJobs\/automl-image-training-v2-anumber\n<\/code><\/pre>\n<p>How do I get it into Europe?<\/p>",
        "Challenge_closed_time":1641305260580,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641293452093,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has set the API_ENDPOINT to \"europe-west1-aiplatform.googleapis.com\" in their GCP Vertex pipeline, but when they run the pipeline, it is always created in the US. The user is seeking a solution to get the pipeline created in Europe.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70577610",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":14.9,
        "Challenge_reading_time":10.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.2801352778,
        "Challenge_title":"Why is my GCP Vertex pipeline api_endpoint not right?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":92.0,
        "Challenge_word_count":61,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>The <code>location<\/code> parameter in the <code>aip.PipelineJob()<\/code> class can be used to specify in which region the pipeline will be deployed. Refer to this <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/aiplatform#class-googlecloudaiplatformpipelinejobdisplayname-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-templatepath-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-jobid-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-pipelineroot-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-parametervalues-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-any--none-enablecaching-optionalboolhttpspythonreadthedocsioenlatestlibraryfunctionshtmlbool--none-encryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-labels-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-credentials-optionalgoogleauthcredentialscredentialshttpsgoogleapisdevpythongoogle-authlatestreferencegoogleauthcredentialshtmlgoogleauthcredentialscredentials--none-project-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-location-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none\" rel=\"nofollow noreferrer\">documentation<\/a> for more information about the <code>PipelineJob()<\/code> method.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>REGION = &quot;europe-west1&quot;\n\njob = aip.PipelineJob(\n          display_name=DISPLAY_NAME,\n          template_path=&quot;image classification_pipeline.json&quot;.replace(&quot; &quot;, &quot;_&quot;),\n          location=REGION)\n\njob.run()\n<\/code><\/pre>\n<p>The above code will deploy a pipeline in the <code>europe-west1<\/code> region. The code returns the following output. The job is now deployed in the specified region.<\/p>\n<pre><code>INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\nINFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects\/&lt;project-id&gt;\/locations\/europe-west1\/pipelineJobs\/hello-world-pipeline\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1647760091087,
        "Solution_link_count":1.0,
        "Solution_readability":50.8,
        "Solution_reading_time":30.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":82.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1327570314367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation_count":2854.0,
        "Answerer_view_count":324.0,
        "Challenge_adjusted_solved_time":30.6574666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are deploying a data consortium between more than 10 companies. Wi will deploy several machine learning models (in general advanced analytics models) for all the companies and we will administrate all the models. We are looking for a solution that administrates several servers, clusters and data science pipelines. I love kedro, but not sure what is the best option to administrate all while using kedro.<\/p>\n<p>In summary, we are looking for the best solution to administrate several models, tasks and pipelines in different servers and possibly Spark clusters. Our current options are:<\/p>\n<ul>\n<li><p>AWS as our data warehouse and Databricks for administrating servers, clusters and tasks. I don't feel that the notebooks of databricks are a good solution for building pipelines and to work collaboratively, so I would like to connect kedro to databricks (is it good? is it easy to schedule the run of the kedro pipelines using databricks?)<\/p>\n<\/li>\n<li><p>Using GCP for data warehouse and use kubeflow (iin GCP) for deploying models and the administration and the schedule of the pipelines and the needed resources<\/p>\n<\/li>\n<li><p>Setting up servers from ASW or GCP, install kedro and schedule the pipelines with airflow (I see a big problem administrating 20 servers and 40 pipelines)<\/p>\n<\/li>\n<\/ul>\n<p>I would like to know if someone knows what is the best option between these alternatives, their  downsides and advantages, or if there are more possibilities.<\/p>",
        "Challenge_closed_time":1605891844630,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605830422657,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a solution to administer several machine learning models and pipelines for a data consortium between more than 10 companies. They are considering using AWS with Databricks and Kedro, GCP with Kubeflow, or setting up servers with Kedro and Airflow. They are seeking advice on the best option and its advantages and disadvantages.",
        "Challenge_last_edit_time":1605836750283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64921833",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":19.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":17.0616591667,
        "Challenge_title":"DataBricks + Kedro Vs GCP + Kubeflow Vs Server + Kedro + Airflow",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":967.0,
        "Challenge_word_count":241,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605828724552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":59.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I'll try and summarise what I know, but be aware that I've not been part of a KubeFlow project.<\/p>\n<h2>Kedro on Databricks<\/h2>\n<p>Our approach was to build our project with CI and then execute the pipeline from a notebook. We <em>did not<\/em> use the <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/11_tools_integration\/03_databricks.html\" rel=\"nofollow noreferrer\">kedro recommended approach<\/a> of using databricks-connect due to the <a href=\"https:\/\/databricks.com\/product\/aws-pricing\" rel=\"nofollow noreferrer\">large price difference<\/a> between Jobs and Interactive Clusters (which are needed for DB-connect). If you're working on several TB's of data, this quickly becomes relevant.<\/p>\n<p>As a DS, this approach may feel natural, as a SWE though it does not. Running pipelines in notebooks feels hacky. It works but it feels non-industrialised. Databricks performs well in automatically spinning up and down clusters &amp; taking care of the runtime for you. So their value add is abstracting IaaS away from you (more on that later).<\/p>\n<h2>GCP &amp; &quot;Cloud Native&quot;<\/h2>\n<p><strong>Pro<\/strong>: GCP's main selling point is BigQuery. It is an incredibly powerful platform, simply because you can be productive from day 0. I've seen people build entire web API's on top of it. KubeFlow isn't tied to GCP so you could port this somewhere else later on. Kubernetes will also allow you to run anything else you wish on the cluster, API's, streaming, web services, websites, you name it.<\/p>\n<p><strong>Con<\/strong>: Kubernetes is complex. If you have 10+ engineers to run this project long-term, you should be OK. But don't underestimate the complexity of Kubernetes. It is to the cloud what Linux is to the OS world. Think log management, noisy neighbours (one cluster for web APIs + batch spark jobs), multi-cluster management (one cluster per department\/project), security, resource access etc.<\/p>\n<h2>IaaS server approach<\/h2>\n<p>Your last alternative, the manual installation of servers is one I would recommend only if you have a large team, extremely large data and are building a long-term product who's revenue can sustain the large maintenance costs.<\/p>\n<h2>The people behind it<\/h2>\n<p>How does the talent market look like in your region? If you can hire experienced engineers with GCP knowledge, I'd go for the 2nd solution. GCP is a mature, &quot;native&quot; platform in the sense that it abstracts a lot away for customers. If your market has mainly AWS engineers, that may be a better road to take. If you have a number of kedro engineers, that also has relevance. Note that kedro is agnostic enough to run anywhere. It's really just python code.<\/p>\n<p><strong>Subjective advise<\/strong>:<\/p>\n<p>Having worked mostly on AWS projects and a few GCP projects, I'd go for GCP. I'd use the platform's components (BigQuery, Cloud Run, PubSub, Functions, K8S) as a toolbox to choose from and build an organisation around that. Kedro can run in any of these contexts, as a triggered job by the Scheduler, as a container on Kubernetes or as a ETL pipeline bringing data into (or out of) BigQuery.<\/p>\n<p>While Databricks is &quot;less management&quot; than raw AWS, it's still servers to think about and VPC networking charges to worry over. BigQuery is simply GB queried. Functions are simply invocation count. These high level components will allow you to quickly show value to customers and you only need to go deeper (RaaS -&gt; PaaS -&gt; IaaS) as you scale.<\/p>\n<p>AWS also has these higher level abstractions over IaaS but in general, it appears (to me) that Google's offering is the most mature. Mainly because they have published tools they've been using internally for almost a decade whereas AWS has built new tools for the market. AWS is the king of IaaS though.<\/p>\n<p>Finally, a bit of content, <a href=\"https:\/\/youtu.be\/kjhXMTOLtac?t=618\" rel=\"nofollow noreferrer\">two former colleagues have discussed ML industrialisation frameworks earlier this fall<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1605947117163,
        "Solution_link_count":3.0,
        "Solution_readability":7.9,
        "Solution_reading_time":49.9,
        "Solution_score_count":4.0,
        "Solution_sentence_count":41.0,
        "Solution_word_count":606.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.6597683333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Tried invoking an Azure ML pipeline from an Azure DevOps pipeline ? I keep running into errors, so I want to make sure my high level process is correct.<\/p>",
        "Challenge_closed_time":1667271469656,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667236694490,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing errors while trying to invoke an Azure ML pipeline from an Azure DevOps pipeline and is seeking guidance to ensure the high-level process is correct.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1069739\/azure-ml-pipeline-from-an-azure-devops-pipeline",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":2.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":9.6597683333,
        "Challenge_title":"Azure ML pipeline from an Azure DevOps pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":35,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=71c0cf97-895c-43f1-ac54-98e1e9833ae4\">@A-4824  <\/a> Thanks for the question. You can use the Azure CLI task - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/devops\/pipelines\/tasks\/deploy\/azure-cli?view=azure-devops\">Azure Pipelines | Microsoft Learn<\/a> step and run command line or Python scripts inside that to submit your pipelines.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":4.94,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645110475503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":15.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":104.3447397222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I follow the official tutotial from microsoft: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool<\/a><\/p>\n<p>When I execute:<\/p>\n<pre><code>#Bind model within Spark session\n model = pcontext.bind_model(\n     return_types=RETURN_TYPES, \n     runtime=RUNTIME, \n     model_alias=&quot;Sales&quot;, #This alias will be used in PREDICT call to refer  this   model\n     model_uri=AML_MODEL_URI, #In case of AML, it will be AML_MODEL_URI\n     aml_workspace=ws #This is only for AML. In case of ADLS, this parameter can be removed\n ).register()\n<\/code><\/pre>\n<p>I got : No module named 'azureml.automl'<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/g0UCX.png\" rel=\"nofollow noreferrer\">My Notebook<\/a><\/p>",
        "Challenge_closed_time":1648911550928,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648558433353,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is following a Microsoft tutorial on Synapse Analytics Auto ML and is encountering an error message \"No module named 'azureml.automl'\" when trying to execute a code block.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71662401",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":17.4,
        "Challenge_reading_time":12.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":98.0882152778,
        "Challenge_title":"Synapse Analytics Auto ML Predict No module named 'azureml.automl'",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":271.0,
        "Challenge_word_count":81,
        "Platform":"Stack Overflow",
        "Poster_created_time":1645110475503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I solved it. In my case it works best like this:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/JKEmr.png\" rel=\"nofollow noreferrer\">Imports<\/a><\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>#Import libraries\nfrom pyspark.sql.functions import col, pandas_udf,udf,lit\nfrom notebookutils.mssparkutils import azureML\nfrom azureml.core import Workspace, Model\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nfrom azureml.core.model import Model\nimport joblib\nimport pandas as pd\n\nws = azureML.getWorkspace(\"AzureMLService\")\nspark.conf.set(\"spark.synapse.ml.predict.enabled\",\"true\")<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ij760.png\" rel=\"nofollow noreferrer\">Predict function<\/a><\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>def forecastModel():\n    model_path = Model.get_model_path(model_name=\"modelName\", _workspace=ws)\n    modeljob = joblib.load(model_path + \"\/model.pkl\")\n\n    validation_data = spark.read.format(\"csv\") \\\n                            .option(\"header\", True) \\\n                            .option(\"inferSchema\",True) \\\n                            .option(\"sep\", \";\") \\\n                            .load(\"abfss:\/\/....csv\")\n\n    validation_data_pd = validation_data.toPandas()\n\n\n    predict = modeljob.forecast(validation_data_pd)\n\n    return predict<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1648934074416,
        "Solution_link_count":2.0,
        "Solution_readability":20.1,
        "Solution_reading_time":20.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":6.3718908334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to <code>azure-ml<\/code>, and have been tasked to make some integration tests for a couple of pipeline steps. I have prepared some input test data and some expected output data, which I store on a <code>'test_datastore'<\/code>. The following example code is a simplified version of what I want to do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ws = Workspace.from_config('blabla\/config.json')\nds = Datastore.get(ws, datastore_name='test_datastore')\n\nmain_ref = DataReference(datastore=ds,\n                            data_reference_name='main_ref'\n                            )\n\ndata_ref = DataReference(datastore=ds,\n                            data_reference_name='main_ref',\n                            path_on_datastore='\/data'\n                            )\n\n\ndata_prep_step = PythonScriptStep(\n            name='data_prep',\n            script_name='pipeline_steps\/data_prep.py',\n            source_directory='\/.',\n            arguments=['--main_path', main_ref,\n                        '--data_ref_folder', data_ref\n                        ],\n            inputs=[main_ref, data_ref],\n            outputs=[data_ref],\n            runconfig=arbitrary_run_config,\n            allow_reuse=False\n            )\n<\/code><\/pre>\n<p>I would like:<\/p>\n<ul>\n<li>my <code>data_prep_step<\/code> to run,<\/li>\n<li>have it store some data on the path to my <code>data_ref<\/code>), and<\/li>\n<li>I would then like to access this stored data afterwards outside of the pipeline<\/li>\n<\/ul>\n<p>But, I can't find a useful function in the documentation. Any guidance would be much appreciated.<\/p>",
        "Challenge_closed_time":1616626076987,
        "Challenge_comment_count":7,
        "Challenge_created_time":1616603138180,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to access the output folder from a PythonScriptStep in Azure Machine Learning. They have prepared input test data and expected output data stored on a 'test_datastore'. They want the data_prep_step to run, store data on the path to data_ref, and then access this stored data outside of the pipeline. However, they are unable to find a useful function in the documentation.",
        "Challenge_last_edit_time":1616961515960,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66785273",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":13.6,
        "Challenge_reading_time":17.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":6.3718908334,
        "Challenge_title":"How to acces output folder from a PythonScriptStep?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1327.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459511191443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":445.0,
        "Poster_view_count":96.0,
        "Solution_body":"<p>two big ideas here -- let's start with the main one.<\/p>\n<h2>main ask<\/h2>\n<blockquote>\n<p>With an Azure ML Pipeline, how can I access the output data of a <code>PythonScriptStep<\/code> outside of the context of the pipeline?<\/p>\n<\/blockquote>\n<h3>short answer<\/h3>\n<p>Consider using <code>OutputFileDatasetConfig<\/code> (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.output_dataset_config.outputdatasetconfig?view=azure-ml-py&amp;viewFallbackFrom=experimental&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">docs<\/a> <a href=\"http:\/\/%20https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-data-in-out-of-pipelines?WT.mc_id=AI-MVP-5003930#use-outputfiledatasetconfig-for-intermediate-data\" rel=\"nofollow noreferrer\">example<\/a>), instead of <code>DataReference<\/code>.<\/p>\n<p>To your example above, I would just change your last two definitions.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>data_ref = OutputFileDatasetConfig(\n    name='data_ref',\n    destination=(ds, '\/data')\n).as_upload()\n\n\ndata_prep_step = PythonScriptStep(\n    name='data_prep',\n    script_name='pipeline_steps\/data_prep.py',\n    source_directory='\/.',\n    arguments=[\n        '--main_path', main_ref,\n        '--data_ref_folder', data_ref\n                ],\n    inputs=[main_ref, data_ref],\n    outputs=[data_ref],\n    runconfig=arbitrary_run_config,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p>some notes:<\/p>\n<ul>\n<li>be sure to check out how <code>DataPath<\/code>s work. Can be tricky at first glance.<\/li>\n<li>set <code>overwrite=False<\/code> in the `.as_upload() method if you don't want future runs to overwrite the first run's data.<\/li>\n<\/ul>\n<h3>more context<\/h3>\n<p><code>PipelineData<\/code> used to be the defacto object to pass data ephemerally between pipeline steps. The idea was to make it easy to:<\/p>\n<ol>\n<li>stitch steps together<\/li>\n<li>get the data after the pipeline runs if need be (<code>datastore\/azureml\/{run_id}\/data_ref<\/code>)<\/li>\n<\/ol>\n<p>The downside was that you have no control over <em>where<\/em> the pipeline is saved. If you wanted to data for more than just as a baton that gets passed between steps, you could have a <code>DataTransferStep<\/code> to land the <code>PipelineData<\/code> wherever you please after the <code>PythonScriptStep<\/code> finishes.<\/p>\n<p>This downside is what motivated <code>OutputFileDatasetConfig<\/code><\/p>\n<h2>auxilary ask<\/h2>\n<blockquote>\n<p>how might I programmatically test the functionality of my Azure ML pipeline?<\/p>\n<\/blockquote>\n<p>there are not enough people talking about data pipeline testing, IMHO.<\/p>\n<p>There are three areas of data pipeline testing:<\/p>\n<ol>\n<li>unit testing (the code in the step works?<\/li>\n<li>integration testing (the code works when submitted to the Azure ML service)<\/li>\n<li>data expectation testing (the data coming out of the meets my expectations)<\/li>\n<\/ol>\n<p>For #1, I think it should be done outside of the pipeline perhaps as part of a package of helper functions\nFor #2, Why not just see if the whole pipeline completes, I think get more information that way. That's how we run our CI.<\/p>\n<p>#3 is the juiciest, and we do this in our pipelines with the <a href=\"https:\/\/greatexpectations.io\/\" rel=\"nofollow noreferrer\">Great Expectations (GE)<\/a> Python library. The GE community calls these &quot;expectation tests&quot;. To me you have two options for including expectation tests in your Azure ML pipeline:<\/p>\n<ol>\n<li>within the <code>PythonScriptStep<\/code> itself, i.e.\n<ol>\n<li>run whatever code you have<\/li>\n<li>test the outputs with GE before writing them out; or,<\/li>\n<\/ol>\n<\/li>\n<li>for each functional <code>PythonScriptStep<\/code>, hang a downstream <code>PythonScriptStep<\/code> off of it in which you run your expectations against the output data.<\/li>\n<\/ol>\n<p>Our team does #1, but either strategy should work. What's great about this approach is that you can run your expectation tests by just running your pipeline (which also makes integration testing easy).<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1616626562700,
        "Solution_link_count":3.0,
        "Solution_readability":12.2,
        "Solution_reading_time":51.55,
        "Solution_score_count":3.0,
        "Solution_sentence_count":27.0,
        "Solution_word_count":453.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.3080294444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi, the UI is very confused, not straightforward as Studio. Can you guide me to the next step to use the pipeline?     <\/p>",
        "Challenge_closed_time":1661278037916,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661266129010,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has created a pipeline but is finding the UI confusing and not as straightforward as Studio. They are seeking guidance on what the next step is to use the pipeline.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/978610\/whats-the-next-step-after-creating-a-pipeline",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.7,
        "Challenge_reading_time":2.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.3080294444,
        "Challenge_title":"What's the next step after creating a pipeline?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":30,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=a1c46131-fcca-467d-9bfb-d1c7cdf021ad\">@Nichole\u2019s  <\/a>     <\/p>\n<p>Thanks for using Microsft Q&amp;A platform. I think you are on the stage of designing your pipeline and running it.     <\/p>\n<p>The next step should be submit your pipeline and evaluate your model - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score#submit-the-pipeline\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score#submit-the-pipeline<\/a>    <\/p>\n<p>When you feel good with your model, you can then deploy your pipeline as this guidance - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy<\/a>    <\/p>\n<p>You may then want to test and update your endpoint as above guidance.     <\/p>\n<p>Each time you run a pipeline, the configuration of the pipeline and its results are stored in your workspace as a pipeline job. You can go back to any pipeline job to inspect it for troubleshooting or auditing. Clone a pipeline job to create a new pipeline draft for you to edit.    <\/p>\n<p>Pipeline jobs are grouped into experiments to organize job history. You can set the experiment for every pipeline job.    <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.0,
        "Solution_reading_time":19.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":172.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":14.2520105556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have just removed a DVC tracking file by mistake using the command <code>dvc remove training_data.dvc -p<\/code>, which led to all my training dataset gone completely. I know in Git, we can easily revert a deleted branch based on its hash. Does anyone know how to revert all my lost data in DVC?<\/p>",
        "Challenge_closed_time":1592457436920,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592445622650,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user accidentally removed a DVC tracking file using the command \"dvc remove\" which resulted in the loss of all their training dataset. They are seeking help to revert the lost data in DVC.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62441146",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":4.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3.2817416667,
        "Challenge_title":"Revert a dvc remove -p command",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":687.0,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Poster_created_time":1467943515392,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Poster_reputation_count":173.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>You should be safe (at least data is not gone) most likely. From the <code>dvc remove<\/code> <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remove\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Note that it does not remove files from the DVC cache or remote storage (see dvc gc). However, remember to run <code>dvc push<\/code> to save the files you actually want to use or share in the future.<\/p>\n<\/blockquote>\n\n<p>So, if you created <code>training_data.dvc<\/code> as with <code>dvc add<\/code> and\/or <code>dvc run<\/code> and <code>dvc remove -p<\/code> didn't ask\/warn you about anything, means that data is cached similar to Git in the <code>.dvc\/cache<\/code>. <\/p>\n\n<p>There are ways to retrieve it, but I would need to know a little bit more details - how exactly did you add your dataset? Did you commit <code>training_data.dvc<\/code> or it's completely gone? Was it the only data you have added so far? (happy to help you in comments).<\/p>\n\n<h2>Recovering a directory<\/h2>\n\n<p>First of all, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files-and-directories#structure-of-cache-directory\" rel=\"nofollow noreferrer\">here<\/a> is the document that describes briefly how DVC stores directories in the cache.<\/p>\n\n<p>What we can do is to find all <code>.dir<\/code> files in the <code>.dvc\/cache<\/code>:<\/p>\n\n<p><code>find .dvc\/cache -type f -name \"*.dir\"<\/code><\/p>\n\n<p>outputs something like:<\/p>\n\n<pre><code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir\n.dvc\/cache\/00\/db872eebe1c914dd13617616bb8586.dir\n.dvc\/cache\/2d\/1764cb0fc973f68f31f5ff90ee0883.dir\n<\/code><\/pre>\n\n<p>(if the local cache is lost and we are restoring data from the remote storage, the same logic applies, commands (e.g. to find files on S3 with .dir extension) look different)<\/p>\n\n<p>Each <code>.dir<\/code> file is a JSON with a content of one version of a directory (file names, hashes, etc). It has all the information needed to restore it. The next thing we need to do is to understand which one do we need. There is no one single rule for that, what I would recommend to check (and pick depending on your use case):<\/p>\n\n<ul>\n<li>Check the date modified (if you remember when this data was added).<\/li>\n<li>Check the content of those files - if you remember a specific file name that was present only in the directory you are looking for - just grep it.<\/li>\n<li>Try to restore them one by one and check the directory content.<\/li>\n<\/ul>\n\n<p>Okay, now let's imagine we decided that we want to restore <code>.dvc\/cache\/20\/b786b6e6f80e2b3fcf17827ad18597.dir<\/code>, (e.g. because content of it looks like:<\/p>\n\n<pre><code>[\n{\"md5\": \"6f597d341ceb7d8fbbe88859a892ef81\", \"relpath\": \"test.tsv\"}, {\"md5\": \"32b715ef0d71ff4c9e61f55b09c15e75\", \"relpath\": \"train.tsv\"}\n]\n<\/code><\/pre>\n\n<p>and we want to get a directory with <code>train.tsv<\/code>).<\/p>\n\n<p>The only thing we need to do is to create a <code>.dvc<\/code> file that references this directory:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n- md5: 20b786b6e6f80e2b3fcf17827ad18597.dir\n  path: my-directory\n<\/code><\/pre>\n\n<p>(note, that path \/20\/b786b6e6f80e2b3fcf17827ad18597.dir became a hash value: 20b786b6e6f80e2b3fcf17827ad18597.dir)<\/p>\n\n<p>And run <code>dvc pull<\/code> on this file.<\/p>\n\n<p>That should be it.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1592496929888,
        "Solution_link_count":2.0,
        "Solution_readability":7.9,
        "Solution_reading_time":41.6,
        "Solution_score_count":3.0,
        "Solution_sentence_count":36.0,
        "Solution_word_count":420.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1426093220648,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, India",
        "Answerer_reputation_count":1861.0,
        "Answerer_view_count":294.0,
        "Challenge_adjusted_solved_time":0.3459813889,
        "Challenge_answer_count":1,
        "Challenge_body":"<ol>\n<li><p>I want to write pytest unit test in <strong>Kedro 0.17.5<\/strong>. They need to perform integrity checks on dataframes created by the pipeline.\nThese dataframes are specified in the <code>catalog.yml<\/code> and already persisted successfully using <code>kedro run<\/code>. The <code>catalog.yml<\/code> is in <code>conf\/base<\/code>.<\/p>\n<\/li>\n<li><p>I have a test module <code>test_my_dataframe.py<\/code> in <code>src\/tests\/pipelines\/my_pipeline\/<\/code>.<\/p>\n<\/li>\n<\/ol>\n<p>How can I load the data catalog based on my <code>catalog.yml<\/code> programmatically from within <code>test_my_dataframe.py<\/code> in order to properly access my specified dataframes?<\/p>\n<p>Or, for that matter, how can I programmatically load the whole project context (including the data catalog) in order to also execute nodes etc.?<\/p>",
        "Challenge_closed_time":1656143487270,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656142241737,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to write pytest unit tests in Kedro 0.17.5 to perform integrity checks on dataframes created by the pipeline. The dataframes are specified in the catalog.yml and already persisted successfully using kedro run. The user is looking for a way to load the data catalog programmatically from within the test module to access the specified dataframes or load the whole project context to execute nodes.",
        "Challenge_last_edit_time":1656265981920,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72752043",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":11.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.3459813889,
        "Challenge_title":"Load existing data catalog programmatically",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":107.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1258185382660,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":333.0,
        "Poster_view_count":33.0,
        "Solution_body":"<ol>\n<li><p>For unit testing, we test just the function which we are testing, and everything external to the function we should mock\/patch. Check if you really need kedro project context while writing the unit test.<\/p>\n<\/li>\n<li><p>If you really need project context in test, you can do something like following<\/p>\n<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from kedro.framework.project import configure_project\nfrom kedro.framework.session import KedroSession\n\nwith KedroSession.create(package_name=&quot;demo&quot;, project_path=Path.cwd()) as session:\n    context = session.load_context()\n    catalog = context.catalog\n<\/code><\/pre>\n<p>or you can also create pytest fixture to use it again and again with scope of your choice.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@pytest.fixture\ndef get_project_context():\n    session = KedroSession.create(\n        package_name=&quot;demo&quot;,\n        project_path=Path.cwd()\n    )\n    _activate_session(session, force=True)\n    context = session.load_context()\n    return context\n<\/code><\/pre>\n<p>Different args supported by KedroSession create you can check it here <a href=\"https:\/\/kedro.readthedocs.io\/en\/0.17.5\/kedro.framework.session.session.KedroSession.html#kedro.framework.session.session.KedroSession.create\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/0.17.5\/kedro.framework.session.session.KedroSession.html#kedro.framework.session.session.KedroSession.create<\/a><\/p>\n<p>To read more about pytest fixture you can refer to <a href=\"https:\/\/docs.pytest.org\/en\/6.2.x\/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session\" rel=\"nofollow noreferrer\">https:\/\/docs.pytest.org\/en\/6.2.x\/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1656143791536,
        "Solution_link_count":4.0,
        "Solution_readability":19.5,
        "Solution_reading_time":23.58,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":135.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.9458333333,
        "Challenge_answer_count":0,
        "Challenge_body":"I try to follow this Checkpoints tutorial and documentation page https:\/\/dvc.org\/doc\/user-guide\/experiment-management\/checkpoints \r\n\r\nHowever, after adding `dvclive` in the train.py file with this code: \r\n\r\n import the dvclive package with the other imports:\r\n\r\n```python\r\nimport dvclive\r\n...\r\n    ...\r\n    for k, v in metrics.items():\r\n        print('Epoch %s: %s=%s'%(i, k, v))\r\n        dvclive.log(k, v)\r\n    dvclive.next_step()\r\n```\r\nI got an error: \r\n```bash \r\n\u276f dvc exp run\r\nModified checkpoint experiment based on 'exp-defaa' will be created   \r\nRunning stage 'train':                                                                                                                                                                                                                                               \r\n> python train.py\r\n...\r\nEpoch 1: loss=0.1541447937488556\r\nTraceback (most recent call last):\r\n  File \"[USER-PATH]\/checkpoints-tutorial\/train.py\", line 125, in <module>\r\n    main()\r\n  File \"[USER-PATH]\/checkpoints-tutorial\/train.py\", line 118, in main\r\n    dvclive.log(name=k, val=v)\r\nAttributeError: module 'dvclive' has no attribute 'log'\r\n\r\nfile:\/\/\/[USER-PATH]\/checkpoints-tutorial\/dvclive.html\r\nERROR: failed to reproduce 'dvc.yaml': failed to run: python train.py, exited with 1\r\n``` \r\n\r\nI only could run the example with the following trick: \r\n```python\r\nfrom dvclive import Live \r\ndvclive = Live()\r\n```\r\nAre there any updated in `dvclive` API? \r\n\r\nSystem info\r\n```bash \r\n\u276f dvc doctor\r\nDVC version: 2.6.4 (pip)\r\n---------------------------------\r\nPlatform: Python 3.9.4 on macOS-11.6-x86_64-i386-64bit\r\nSupports:\r\n        hdfs (pyarrow = 5.0.0),\r\n        http (requests = 2.26.0),\r\n        https (requests = 2.26.0)\r\nCache types: reflink, hardlink, symlink\r\nCache directory: apfs on \/dev\/disk1s1s1\r\nCaches: local\r\nRemotes: None\r\nWorkspace directory: apfs on \/dev\/disk1s1s1\r\nRepo: dvc, git\r\n```\r\n\r\nFIY @flippedcoder @daavoo ",
        "Challenge_closed_time":1633697794000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1633694389000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the kedro-mlflow plugin not working with projects created with kedro==0.18.1. When the user tries to run the pipeline, an error is raised due to the removal of the private attribute '_active_session' in kedro==0.18.1. The solution is to use the 'after_context_created' hook to retrieve and set up the configuration.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/checkpoints-tutorial\/issues\/1",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":7.7,
        "Challenge_reading_time":20.74,
        "Challenge_repo_contributor_count":2.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":3.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":0.9458333333,
        "Challenge_title":"AttributeError: module 'dvclive' has no attribute 'log'",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":191,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for the catch @mike0sv ! No trouble (literally, no trouble at all since it was @mnrozhkov :))",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.1,
        "Solution_reading_time":1.22,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1574932278796,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":18454.0,
        "Answerer_view_count":3762.0,
        "Challenge_adjusted_solved_time":12.6983352778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run a Python script in a DevOps pipeline upon check in. A basic 'hellow world' script works, but when I import azureml.core, it errors out with ModuleNotFoundError: No module named 'azureml'.<\/p>\n<p>That makes sense, since I don't know how it was going to find azureml.core. My question is: how do I get the Python script to find the module? Do I need to check it in as part of my code base in DevOps? Or is there some way to reference it via a hyperlink?<\/p>\n<p>Here is my YML file:<\/p>\n<pre><code>trigger:\n- master\n\npool:\n  vmImage: ubuntu-latest\n\nsteps:\n- task: PythonScript@0\n  inputs:\n    scriptSource: 'filepath'\n    scriptPath: test.py\n<\/code><\/pre>\n<p>And here is my python script:<\/p>\n<pre><code>print('hello world')\n\nimport azureml.core\nfrom azureml.core import Workspace\n\n# Load the workspace from the saved config file\nws = Workspace.from_config()\nprint('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))\n<\/code><\/pre>",
        "Challenge_closed_time":1623837729747,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623792015740,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to run a Python script in a DevOps pipeline upon check-in. The error occurs when importing azureml.core, resulting in a ModuleNotFoundError. The user is unsure how to make the Python script find the module and is seeking guidance on whether to check it in as part of the code base or reference it via a hyperlink. The user has provided their YML file and Python script for reference.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67993641",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.4,
        "Challenge_reading_time":12.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":12.6983352778,
        "Challenge_title":"DevOps pipeline running python script error on import azureml.core",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":410.0,
        "Challenge_word_count":150,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320273319470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Calgary, Canada",
        "Poster_reputation_count":78.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You have to install the lost package into your python, the default python does not have that. Please use the below yml:<\/p>\n<pre><code>trigger:\n- master\n\npool:\n  vmImage: 'ubuntu-latest'\n\nvariables:\n  solution: '**\/*.sln'\n  buildPlatform: 'Any CPU'\n  buildConfiguration: 'Release'\n\nsteps:\n- task: UsePythonVersion@0\n  displayName: 'Use Python 3.8'\n  inputs:\n    versionSpec: 3.8\n\n- script: python3 -m pip install --upgrade pip\n  displayName: 'upgrade pip'\n\n- script: python3 -m pip install azureml.core\n  displayName: 'Install azureml.core'\n\n\n\n- task: PythonScript@0\n  inputs: \n    scriptSource: 'filepath'\n    scriptPath: test.py\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":7.82,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1288806614312,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dallas, TX, United States",
        "Answerer_reputation_count":14103.0,
        "Answerer_view_count":2018.0,
        "Challenge_adjusted_solved_time":11.6636425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to execute a R script every time an azure function is triggered. The R script executes perfectly on Azure machine learning Studio. But I am failing to execute through azure function.\nIs there any way to execute it?<\/p>",
        "Challenge_closed_time":1612564671216,
        "Challenge_comment_count":1,
        "Challenge_created_time":1612522682103,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having difficulty executing an R script through an Azure function, despite it working properly on Azure machine learning Studio. They are seeking a solution to execute the script through the function.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66062015",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.6,
        "Challenge_reading_time":3.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":11.6636425,
        "Challenge_title":"Executing R script from Azure function",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":640.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1475153705707,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":79.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>AFAIK you'll have to create your own Runtime as <code>R<\/code> isn't supported natively.<\/p>\n<p>Have you already tried <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/functions-create-function-linux-custom-image?tabs=bash%2Cportal&amp;pivots=programming-language-other\" rel=\"nofollow noreferrer\">&quot;Create a function on Linux using a custom container&quot;<\/a>? Interestingly they have given <code>R<\/code> as the example of custom runtime, so hopefully that answers your question.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.2,
        "Solution_reading_time":6.79,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":1807.7083702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to add an alert if Azure ML pipeline fails. It looks that one of the ways is to create a monitor in the Azure Portal. The problem is that I cannot find a correct signal name (required when setting up condition), which would identify pipeline fail. What signal name should I use? Or is there another way to send an email if Azure pipeline fails?<\/p>",
        "Challenge_closed_time":1651723432756,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651478303433,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to set up an alert for when an Azure ML pipeline fails, but is having trouble finding the correct signal name to identify the failure. They are seeking advice on what signal name to use or if there is another way to receive an email notification for pipeline failures.",
        "Challenge_last_edit_time":1651956277630,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72083832",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.5,
        "Challenge_reading_time":4.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":68.0914786111,
        "Challenge_title":"Send alert if Azure ML pipeline fails",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":158.0,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Poster_created_time":1313536247312,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Vilnius, Lithuania",
        "Poster_reputation_count":563.0,
        "Poster_view_count":108.0,
        "Solution_body":"<blockquote>\n<p>What signal name should I use?<\/p>\n<\/blockquote>\n<p>You can use <code>PipelineChangeEvent<\/code> category of <code>AmlPipelineEvent<\/code> table to view events when ML pipeline draft or endpoint or module are accessed (read, created, or deleted).<\/p>\n<p>For example, according to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/monitor-azure-machine-learning#analyzing-logs\" rel=\"nofollow noreferrer\">documentation<\/a>, use <code>AmlComputeJobEvent<\/code> to get failed jobs in the last five days:<\/p>\n<pre><code>AmlComputeJobEvent\n| where TimeGenerated &gt; ago(5d) and EventType == &quot;JobFailed&quot;\n| project  TimeGenerated , ClusterId , EventType , ExecutionState , ToolType\n<\/code><\/pre>\n<p><strong>Updated answer:<\/strong><\/p>\n<p>According to <a href=\"https:\/\/stackoverflow.com\/users\/897665\/laurynas-g\">Laurynas G<\/a>:<\/p>\n<pre><code>AmlRunStatusChangedEvent \n| where Status == &quot;Failed&quot; or Status == &quot;Canceled&quot;\n<\/code><\/pre>\n<p>You can refer to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/monitor-azure-machine-learning#analyzing-logs\" rel=\"nofollow noreferrer\">Monitor Azure Machine Learning<\/a>, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Log &amp; view metrics and log files<\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-pipelines\" rel=\"nofollow noreferrer\">Troubleshooting machine learning pipelines<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1658464027763,
        "Solution_link_count":5.0,
        "Solution_readability":19.3,
        "Solution_reading_time":20.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":111.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.6887980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What kind of collaboration do we need among the data scientists or developers who need to share these notebooks? What kind of compute does these notebooks require? Is it all single node? <\/p>",
        "Challenge_closed_time":1591962208856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591956129183,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on collaboration and compute requirements for sharing Azure ML notebooks among data scientists and developers, specifically whether single node compute is sufficient.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/35432\/azure-ml-notebooks-sharing-and-compute-selection",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.9,
        "Challenge_reading_time":3.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.6887980556,
        "Challenge_title":"Azure ml notebooks sharing and compute selection.",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a>@azureml056-5112<\/a> Please follow the below for managing compute instances. <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#managing-a-compute-instance\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#managing-a-compute-instance<\/a> All data scientists or developers need is access to the AzureML Workspace and they will have access to a shared file share where everyone\u2019s notebooks can be accessed.<\/p>\n<p>All notebook require a Compute Instance(CI). CI is a managed VM that exists in AzureML.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.6,
        "Solution_reading_time":7.77,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1545311054088,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":170.0,
        "Answerer_view_count":33.0,
        "Challenge_adjusted_solved_time":8.4162855556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Kedro pipeline, nodes (something like python functions) are declared sequentially. In some cases, the input of one node is the output of the previous node. However, sometimes, when kedro run API is called in the commandline, the nodes are not run sequentially.<\/p>\n\n<p>In kedro documentation, it says that by default the nodes are ran in sequence. <\/p>\n\n<p>My run.py code:<\/p>\n\n<pre><code>def main(\ntags: Iterable[str] = None,\nenv: str = None,\nrunner: Type[AbstractRunner] = None,\nnode_names: Iterable[str] = None,\nfrom_nodes: Iterable[str] = None,\nto_nodes: Iterable[str] = None,\nfrom_inputs: Iterable[str] = None,\n):\n\nproject_context = ProjectContext(Path.cwd(), env=env)\nproject_context.run(\n    tags=tags,\n    runner=runner,\n    node_names=node_names,\n    from_nodes=from_nodes,\n    to_nodes=to_nodes,\n    from_inputs=from_inputs,\n)\n<\/code><\/pre>\n\n<p>Currently my last node is sometimes ran before my first few nodes.<\/p>",
        "Challenge_closed_time":1572835437910,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572835098980,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with running nodes in sequence as declared in Kedro pipeline. Despite the default setting of running nodes sequentially, the nodes are not being executed in order when the Kedro run API is called in the command line. The user's last node is sometimes being executed before the first few nodes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58686533",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":12.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.0941472223,
        "Challenge_title":"How to run the nodes in sequence as declared in kedro pipeline?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1741.0,
        "Challenge_word_count":118,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545311054088,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":170.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>The answer that I recieved from Kedro github:<\/p>\n\n<blockquote>\n  <p>Pipeline determines the node execution order exclusively based on\n  dataset dependencies (node inputs and outputs) at the moment. So the\n  only option to dictate that the node A should run before node B is to\n  put a dummy dataset as an output of node A and an input of node B.<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1572865397608,
        "Solution_link_count":0.0,
        "Solution_readability":12.8,
        "Solution_reading_time":4.38,
        "Solution_score_count":5.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":61.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0377777778,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nHi, I have a quick question, is it possible to filter all jobs that requested\/accessed a specific connection?\n\nTo explain my use-case, we detected an issue with some data, and we would like to assess how many jobs and how far in the past that data was used in our training jobs.",
        "Challenge_closed_time":1649676909000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649676773000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a way to filter all jobs that accessed a specific connection in order to assess how many jobs and how far in the past that data was used in their training jobs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1487",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":9.6,
        "Challenge_reading_time":4.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0377777778,
        "Challenge_title":"How to filter all jobs that accessed a connection, a dataset, or an artifact",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":67,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"V1.18 the following filters, or any combination, will be possible:\n\nBy connection name connections.name: CONNECTION1 | CONNECTION2\nBy connection tag connections.tags: TAG1 | TAG2\nBy connection kind connections.kind: git or connections.kind: KIND1 | KIND2\nBy artifact name artifacts.name: LINEAGE1 | LINEAGE2\nBy artifact kind artifacts.kind: model or artifacts.kind: KIND1 | KIND2\nBy artifact path artifacts.path: foo\/bar\nBy artifact state artifacts.state: STATE",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":5.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":56.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":156.2246444445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In the sagemaker pipeline example shown here<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html<\/a><\/p>\n<p>I see two lines joining from AbaloneProcess to AbaloneTrain and AbaloneEval respectively. However, based on the code, I would expect it to actually be connected from AbaloneTrain only, then to AbaloneEval in a single path. Can somebody explain to me what is actually happening here because I am struggling to wrap my head around this. Much appreciated and apologies for the inconvenience in advance<\/p>",
        "Challenge_closed_time":1661363764683,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660801355963,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the sagemaker pipeline example on AWS, specifically regarding the connection between AbaloneProcess, AbaloneTrain, and AbaloneEval. The user is confused about the two lines joining from AbaloneProcess and is seeking an explanation for the discrepancy between the code and the diagram.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73397959",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":9.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":156.2246444445,
        "Challenge_title":"Question regarding sagemaker pipeline example on AWS?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":16.0,
        "Challenge_word_count":81,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644981356940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The connections are indicated data dependencies between steps, not only the order of execution. This is what you see AbaloneProcess connected to AbaloneEval, since the output of AbaloneProcess is used in AbaloneEval.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":2.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1510527902860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Zurich, Switzerland",
        "Answerer_reputation_count":1078.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":1.3906147222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a pipeline in Kedro that looks like this:<\/p>\n<pre><code>from kedro.pipeline import Pipeline, node\nfrom .nodes import *\n\ndef foo():\n    return Pipeline([\n        node(a, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=dict(bar_a=&quot;bar_a&quot;), name=&quot;A&quot;),\n        node(b, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=dict(bar_b=&quot;bar_b&quot;), name=&quot;B&quot;),\n        node(c, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=dict(bar_c=&quot;bar_c&quot;), name=&quot;C&quot;),\n        node(d, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=dict(bar_d=&quot;bar_d&quot;), name=&quot;D&quot;),\n        \n    ])\n<\/code><\/pre>\n<p>The nodes A, B, and C are not very resource-intensive, but they take a while so I want to run them in parallel, node D, on the other hand, uses pretty much all my memory, and it will fail if it's executed alongside the other nodes. Is there a way that I can tell Kedro to wait for A, B, and C to finish before executing node D and keep the code organized?<\/p>",
        "Challenge_closed_time":1626713371123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626707886057,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has a pipeline in Kedro with four nodes, where nodes A, B, and C are not resource-intensive but take a while to run, and node D is memory-intensive and will fail if executed alongside the other nodes. The user is looking for a way to make Kedro wait for nodes A, B, and C to finish before executing node D while keeping the code organized.",
        "Challenge_last_edit_time":1626708364910,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68442999",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":13.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.5236294445,
        "Challenge_title":"Waiting for nodes to finish in Kedro",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":245.0,
        "Challenge_word_count":122,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423164285360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Itabira, Brazil",
        "Poster_reputation_count":856.0,
        "Poster_view_count":106.0,
        "Solution_body":"<p>Kedro determines the execution order based on the interdependencies between the inputs\/outputs of different nodes. In your case, node D doesn't depend on any of the other nodes, so execution order cannot be guaranteed. Similarly, it cannot be ensured that node D will <em>not<\/em> run in parallel to A, B and C if using a parallel runner.<\/p>\n<p>That said, there are a couple of workarounds one could use achieve a particular execution order.<\/p>\n<h5 id=\"preferred-run-the-nodes-separately-62tl\">1 [Preferred] Run the nodes separately<\/h5>\n<p>Instead of doing <code>kedro run --parallel<\/code>, you could do:<\/p>\n<pre><code>kedro run --pipeline foo --node A --node B --node C --parallel; kedro run --pipeline foo --node D\n<\/code><\/pre>\n<p>This is arguably the preferred solution because it requires no code changes (which is good in case you ever run the same pipeline on a different machine). You could do <code>&amp;&amp;<\/code> instead of <code>;<\/code> if you want node D to run only if A, B and C succeded. If the running logic gets more complex, you could store it in a Makefile\/bash script.<\/p>\n<h5 id=\"using-dummy-inputsoutputs-j7un\">2 Using dummy inputs\/outputs<\/h5>\n<p>You could also force the execution order by introducing dummy datasets. Something like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def foo():\n    return Pipeline([\n        node(a, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=[dict(bar_a=&quot;bar_a&quot;), &quot;a_done&quot;], name=&quot;A&quot;),\n        node(b, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=[dict(bar_b=&quot;bar_b&quot;), &quot;b_done&quot;], name=&quot;B&quot;),\n        node(c, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=[dict(bar_c=&quot;bar_c&quot;), &quot;c_done&quot;], name=&quot;C&quot;),\n        node(d, inputs=[&quot;train_x&quot;, &quot;test_x&quot;, &quot;a_done&quot;, &quot;b_done&quot;, &quot;c_done&quot;], outputs=dict(bar_d=&quot;bar_d&quot;), name=&quot;D&quot;),     \n    ])\n<\/code><\/pre>\n<p>Empty lists could do for the dummy datasets. The underlying functions would also have to return\/take the additional arguments.<\/p>\n<p>The advantage of this approach is that <code>kedro run --parallel<\/code> will immediately result in the desired execution logic. The disadvantage is that it pollutes the definition of nodes and underlying functions.<\/p>\n<p>If you go down this road, you'll also have to decide whether you want to store the dummy datasets in the data catalog (pollutes even more, but allows to run node D on its own) or not (node D cannot run on its own).<\/p>\n<hr \/>\n<p>Related discussions [<a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/issues\/132\" rel=\"nofollow noreferrer\">1<\/a>, <a href=\"https:\/\/stackoverflow.com\/questions\/58686533\/how-to-run-the-nodes-in-sequence-as-declared-in-kedro-pipeline\">2<\/a>]<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.1,
        "Solution_reading_time":36.36,
        "Solution_score_count":3.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":324.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.9341666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\nStep Functions can be used to create ML workflows. What is the best practice to version the code creating those workflows? boto3 code in CodeCommit? Something else?\n\nCheers\nOlivier",
        "Challenge_closed_time":1549478317000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1549456954000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking advice on the best practice for versioning the code used to create ML workflows using Step Functions, specifically whether to use boto3 code in CodeCommit or another method.",
        "Challenge_last_edit_time":1668624402176,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUdG1tanW-TXy-vY0YrCsVeg\/how-to-version-step-functions-for-ml",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":5.9341666667,
        "Challenge_title":"how to version step functions for ML?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":555.0,
        "Challenge_word_count":36,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"A Step Functions state machine usually doesn't come alone and typically relies on other resources such as Lambda, EC2, DynamoDB, etc. You might want to package these dependent artifacts\/resources altogether within a version otherwise you might have a state machine that doesn't fully work (eg, state machine version doesn't match Lambda version). I guess the simplest way to achieve this is to provision these resources together as code (eg, CDK or CloudFormation) and store them in a Git repo. You could then use Git tags for versioning.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1601285800631,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":6.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1663183171310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":16.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":535.9962822222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Sorry for long post, I need to explain it properly for people to undertsand.<\/p>\n<p>I have a pipeline in datafctory that triggers a published AML endpoint:\n<a href=\"https:\/\/i.stack.imgur.com\/mKIeU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mKIeU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I am trying to parametrize this ADF pipeline so that I can deploy to test and prod, but on test and prod the aml endpoints are different.<\/p>\n<p>Therefore, I have tried to edit the <strong>parameter configuration<\/strong> in ADF as shows here:\n<a href=\"https:\/\/i.stack.imgur.com\/c4g7x.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/c4g7x.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Here in the section <code>Microsoft.DataFactory\/factories\/pipelines<\/code> I add <code>&quot;*&quot;:&quot;=&quot;<\/code> so that all the pipeline parameters are parametrized:<\/p>\n<pre><code> &quot;Microsoft.DataFactory\/factories\/pipelines&quot;: {\n        &quot;*&quot;: &quot;=&quot;\n    }\n<\/code><\/pre>\n<p>After this I export the template to see which parameters are there in json, there are lot of them but I do not see any paramter that has aml endpoint name as value, but I see the endpint ID is parametrized.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/7WRUL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7WRUL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>My question is: Is it possible to parametrize the AML endpoint by name? So that, when deploying ADF to test I can just provide the AML endpoint name and it can pick the id automatically:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0Fu1g.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0Fu1g.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1663188036943,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661258450327,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in parametrizing the Azure Machine Learning (AML) pipeline endpoint name in Azure Data Factory (ADF). They have tried to edit the parameter configuration in ADF to parametrize all pipeline parameters, but they cannot find any parameter that has the AML endpoint name as a value. The user is seeking a solution to parametrize the AML endpoint by name so that they can deploy ADF to test and provide the AML endpoint name to pick the ID automatically.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73458933",
        "Challenge_link_count":8,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":23.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":535.9962822222,
        "Challenge_title":"Unable to parametrize ML pipeline endpoint name - Azure Data Factory",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":67.0,
        "Challenge_word_count":213,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443017464707,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sweden",
        "Poster_reputation_count":644.0,
        "Poster_view_count":126.0,
        "Solution_body":"<p>i faced the similar issue when deploying adf pipelines with ml between environments. Unfortunately, As of now, adf parameter file do not have ml pipeline name as parameter value. only turn around solution is modifiying the parameter file(json) file with aligns with your pipeline design. For example, i am triggering ml pipeline endpoint inside foreach activity--&gt;if condition--&gt;ml pipeline<\/p>\n<p>Here is my parameter file values:<\/p>\n<pre><code>&quot;Microsoft.DataFactory\/factories\/pipelines&quot;: {\n    &quot;properties&quot;: {\n        &quot;activities&quot;: [\n            {\n                &quot;typeProperties&quot;: {\n                    &quot;mlPipelineEndpointId&quot;: &quot;=&quot;,\n                    &quot;url&quot;: {\n                        &quot;value&quot;: &quot;=&quot;\n                    },\n                    &quot;ifFalseActivities&quot;: [\n                        {\n                            &quot;typeProperties&quot;: {\n                                &quot;mlPipelineEndpointId&quot;: &quot;=&quot;\n                            }\n                        }\n                    ],\n                    &quot;ifTrueActivities&quot;: [\n                        {\n                            &quot;typeProperties&quot;: {\n                                &quot;mlPipelineEndpointId&quot;: &quot;=&quot;\n                            }\n                        }\n                    ],\n                    &quot;activities&quot;: [\n                        {\n                            &quot;typeProperties&quot;: {\n                                &quot;mlPipelineEndpointId&quot;: &quot;=&quot;,\n                                &quot;ifFalseActivities&quot;: [\n                                    {\n                                        &quot;typeProperties&quot;: {\n                                            &quot;mlPipelineEndpointId&quot;: &quot;=&quot;,\n                                            &quot;url&quot;: &quot;=&quot;\n                                        }\n                                    }\n                                ],\n                                &quot;ifTrueActivities&quot;: [\n                                    {\n                                        &quot;typeProperties&quot;: {\n                                            &quot;mlPipelineEndpointId&quot;: &quot;=&quot;,\n                                            &quot;url&quot;: &quot;=&quot;\n                                        }\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    }\n}\n<\/code><\/pre>\n<p>after you export the ARM template, the json file has records for your ml endpoints<\/p>\n<pre><code>&quot;ADFPIPELINE_NAME_properties_1_typeProperties_1_typeProperties_0_typeProperties_mlPipelineEndpointId&quot;: {\n        &quot;value&quot;: &quot;445xxxxx-xxxx-xxxxx-xxxxx&quot;\n<\/code><\/pre>\n<p>it is lot of manual effort to maintain if design is frequently changing so far worked for me. Hope this answers your question.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.6,
        "Solution_reading_time":23.75,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":140.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":4389.5575297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Sagemaker pipelines are rather unclear to me, I'm not experienced in the field of ML but I'm working on figuring out the pipeline definitions.<\/p>\n<p>I have a few questions:<\/p>\n<ul>\n<li><p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/li>\n<li><p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/li>\n<li><p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/li>\n<\/ul>\n<p>I can't seem to find any examples besides the Python SDK usage, how come?<\/p>\n<p>The docs and workshops seem only to properly describe the Python SDK usage,it would be really helpful if someone could clear this up for me!<\/p>",
        "Challenge_closed_time":1638396070903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638395443060,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is confused about SageMaker pipelines and has several questions about them. They are unsure if it is a stand-alone service, how it integrates with codepipeline, and how the Python SDK differs from CDK and CloudFormation. They are also having trouble finding examples besides the Python SDK usage and are seeking clarification.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70191668",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":10.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.1744008334,
        "Challenge_title":"What are SageMaker pipelines actually?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":716.0,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578250359256,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam",
        "Poster_reputation_count":197.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>SageMaker has two things called Pipelines: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pipelines.html\" rel=\"nofollow noreferrer\">Model Building Pipelines<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>. I believe you're referring to the former<\/p>\n<p>A model building pipeline defines steps in a machine learning workflow, such as pre-processing, hyperparameter tuning, batch transformations, and setting up endpoints<\/p>\n<p>A serial inference pipeline is two or more SageMaker models run one after the other<\/p>\n<p>A model building pipeline is defined in JSON, and is hosted\/run in some sort of proprietary, serverless fashion by SageMaker<\/p>\n<blockquote>\n<p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/blockquote>\n<p>You can create\/modify them using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePipeline.html\" rel=\"nofollow noreferrer\">API<\/a>, which can also be called via the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-pipeline.html\" rel=\"nofollow noreferrer\">CLI<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline.Pipeline.create\" rel=\"nofollow noreferrer\">Python SDK<\/a>, or <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html\" rel=\"nofollow noreferrer\">CloudFormation<\/a>. These all use the AWS API under the hood<\/p>\n<p>You can start\/stop\/view them in SageMaker Studio:<\/p>\n<pre><code>Left-side Navigation bar &gt; SageMaker resources &gt; Drop-down menu &gt; Pipelines\n<\/code><\/pre>\n<blockquote>\n<p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/blockquote>\n<p>Unlikely. CodePipeline is more for building and deploying code, not specific to SageMaker. There is no direct integration as far as I can tell, other than that you can start a SM pipeline with CP<\/p>\n<blockquote>\n<p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/blockquote>\n<p>The Python SDK is a stand-alone library to interact with SageMaker in a developer-friendly fashion. It's more dynamic than CloudFormation. Let's you build pipelines using code. Whereas CloudFormation takes a static JSON string<\/p>\n<p>A very simple example of Python SageMaker SDK usage:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>processor = SKLearnProcessor(\n    framework_version=&quot;0.23-1&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    role=&quot;role-arn&quot;,\n)\n\nprocessing_step = ProcessingStep(\n    name=&quot;processing&quot;,\n    processor=processor,\n    code=&quot;preprocessor.py&quot;\n)\n\npipeline = Pipeline(name=&quot;foo&quot;, steps=[processing_step])\npipeline.upsert(role_arn = ...)\npipeline.start()\n<\/code><\/pre>\n<p><code>pipeline.definition()<\/code> produces rather verbose JSON like this:<\/p>\n\n<pre class=\"lang-json prettyprint-override\"><code>{\n&quot;Version&quot;: &quot;2020-12-01&quot;,\n&quot;Metadata&quot;: {},\n&quot;Parameters&quot;: [],\n&quot;PipelineExperimentConfig&quot;: {\n    &quot;ExperimentName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineName&quot;\n    },\n    &quot;TrialName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineExecutionId&quot;\n    }\n},\n&quot;Steps&quot;: [\n    {\n        &quot;Name&quot;: &quot;processing&quot;,\n        &quot;Type&quot;: &quot;Processing&quot;,\n        &quot;Arguments&quot;: {\n            &quot;ProcessingResources&quot;: {\n                &quot;ClusterConfig&quot;: {\n                    &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n                    &quot;InstanceCount&quot;: 1,\n                    &quot;VolumeSizeInGB&quot;: 30\n                }\n            },\n            &quot;AppSpecification&quot;: {\n                &quot;ImageUri&quot;: &quot;246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3&quot;,\n                &quot;ContainerEntrypoint&quot;: [\n                    &quot;python3&quot;,\n                    &quot;\/opt\/ml\/processing\/input\/code\/preprocessor.py&quot;\n                ]\n            },\n            &quot;RoleArn&quot;: &quot;arn:aws:iam::123456789012:role\/foo&quot;,\n            &quot;ProcessingInputs&quot;: [\n                {\n                    &quot;InputName&quot;: &quot;code&quot;,\n                    &quot;AppManaged&quot;: false,\n                    &quot;S3Input&quot;: {\n                        &quot;S3Uri&quot;: &quot;s3:\/\/bucket\/preprocessor.py&quot;,\n                        &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/input\/code&quot;,\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3InputMode&quot;: &quot;File&quot;,\n                        &quot;S3DataDistributionType&quot;: &quot;FullyReplicated&quot;,\n                        &quot;S3CompressionType&quot;: &quot;None&quot;\n                    }\n                }\n            ]\n        }\n    }\n  ]\n}\n<\/code><\/pre>\n<p>You could <em>use<\/em> the above JSON with CloudFormation\/CDK, but you <em>build<\/em> the JSON with the SageMaker SDK<\/p>\n<p>You can also define model building workflows using Step Function State Machines, using the <a href=\"https:\/\/aws-step-functions-data-science-sdk.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Data Science SDK<\/a>, or <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/airflow\/index.html\" rel=\"nofollow noreferrer\">Airflow<\/a><\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1654197850167,
        "Solution_link_count":8.0,
        "Solution_readability":18.8,
        "Solution_reading_time":68.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":32.0,
        "Solution_word_count":402.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1567164934556,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":15.1199388889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using \"studio (preview)\" from Microsoft Azure Machine Learning to create a pipeline that applies machine learning to a dataset in a blob storage that is connected to our data warehouse.<\/p>\n\n<p>In the \"Designer\", an \"Exectue R Script\" action can be added to the pipeline. I'm using this functionality to execute some of my own machine learning algorithms.<\/p>\n\n<p>I've got a 'hello world' version of this script working (including using the \"script bundle\" to load the functions in my own R files). It applies a very simple manipulation (compute the days difference with the date in the date column and 'today'), and stores the output as a new file. Given that the exported file has the correct information, I know that the R script works well.<\/p>\n\n<p>The script looks like this:<\/p>\n\n<pre><code># R version: 3.5.1\n# The script MUST contain a function named azureml_main\n# which is the entry point for this module.\n\n# The entry point function can contain up to two input arguments:\n#   Param&lt;medals&gt;: a R DataFrame\n#   Param&lt;matches&gt;: a R DataFrame\n\nazureml_main &lt;- function(dataframe1, dataframe2){\n\n  message(\"STARTING R script run.\")\n\n  # If a zip file is connected to the third input port, it is\n  # unzipped under \".\/Script Bundle\". This directory is added\n  # to sys.path.\n\n  message('Adding functions as source...')\n\n  if (FALSE) {\n    # This works...\n      source(\".\/Script Bundle\/first_function_for_script_bundle.R\")\n  } else {\n    # And this works as well!\n    message('Sourcing all available functions...')\n    functions_folder = '.\/Script Bundle'\n\n    list.files(path = functions_folder)\n    list_of_R_functions &lt;- list.files(path = functions_folder, pattern = \"^.*[Rr]$\", include.dirs = FALSE, full.names = TRUE)\n    for (fun in list_of_R_functions) {\n\n      message(sprintf('Sourcing &lt;%s&gt;...', fun))\n\n      source(fun)\n\n    }\n  }\n\n  message('Executing R pipeline...')\n  dataframe1 = calculate_days_difference(dataframe = dataframe1)\n\n  # Return datasets as a Named List\n  return(list(dataset1=dataframe1, dataset2=dataframe2))\n}\n<\/code><\/pre>\n\n<p>And although I do print some messages in the R Script, I haven't been able to find the \"stdoutlogs\" nor the \"stderrlogs\" that should contain these printed messages.<\/p>\n\n<p>I need the printed messages for 1) information on how the analysis went and -most importantly- 2) debugging in case the code failed.<\/p>\n\n<p>Now, I have found (on multiple locations) the files \"stdoutlogs.txt\" and \"stderrlogs.txt\". These can be found under \"Logs\" when I click on \"Exectue R Script\" in the \"Designer\".\nI can also find \"stdoutlogs.txt\" and \"stderrlogs.txt\" files under \"Experiments\" when I click on a finished \"Run\" and then both under the tab \"Outputs\" and under the tab \"Logs\".\nHowever... all of these files are empty.<\/p>\n\n<p>Can anyone tell me how I can print messages from my R Script and help me locate where I can find the printed information?<\/p>",
        "Challenge_closed_time":1579829908723,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579792546180,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using \"studio (preview)\" from Microsoft Azure Machine Learning to create a pipeline that applies machine learning to a dataset in a blob storage that is connected to their data warehouse. They are using the \"Execute R Script\" action to execute their own machine learning algorithms. Although they print some messages in the R Script, they cannot find the \"stdoutlogs\" nor the \"stderrlogs\" that should contain these printed messages. They need the printed messages for debugging in case the code failed. The user has found the files \"stdoutlogs.txt\" and \"stderrlogs.txt\" in multiple locations, but all of these files are empty. They are seeking help to locate where they can find the printed information.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59881727",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":36.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":10.3784841667,
        "Challenge_title":"Debugging R Scripts in azure-ml: Where can stdout and stderr logs be found? (or why are they empty?)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":287.0,
        "Challenge_word_count":415,
        "Platform":"Stack Overflow",
        "Poster_created_time":1534511592567,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherlands",
        "Poster_reputation_count":423.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>Can you please click on the \"Execute R module\" and download the 70_driver.log? I tried message(\"STARTING R script run.\") in an R sample and can found the output there.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z7s7h.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z7s7h.png\" alt=\"view logs for a execute R script module\"><\/a><\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1579846977960,
        "Solution_link_count":2.0,
        "Solution_readability":7.2,
        "Solution_reading_time":4.55,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1280505139752,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bangalore, India",
        "Answerer_reputation_count":4265.0,
        "Answerer_view_count":403.0,
        "Challenge_adjusted_solved_time":24.7636894444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was trying Azure Machine Learning Services following this tutorial (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/quickstart-installation\" rel=\"nofollow noreferrer\">Link<\/a>). After successfully creating the Azure Machine Learning services accounts, I successfully installed the Workbench on my Windows 10 Laptop (Behind Proxy; Proxy has been configured at the WorkBench). Next, I was trying to create project following this section (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/quickstart-installation#create-a-project-in-workbench\" rel=\"nofollow noreferrer\">Link<\/a>). Once I click on the Create button, it goes to \"Creating\" state and stays there for ever. The errors displayed at Errors.log is the following. Any suggestion will be appreciated. <\/p>\n\n<pre><code>[2018-07-09 09:47:08.437] [ERROR] HttpService - {\"event\":\"HttpService\",\"task\":\"Failed\",\"data\":{\"url\":\"http:\/\/localhost:54240\/projects\/v1.0\/create\/template\",\"status\":500,\"statusText\":\"INKApi Error\",\"jsonError\":null,\"requestId\":null,\"sessionType\":\"Workbench\"},\"sid\":\"365395c0-832b-11e8-b4ce-e5d7046c6143\"}\n\n[2018-07-09 09:47:08.960] [ERROR] CreateProjectForm - {\"event\":\"CreateProject\",\"task\":\"Error\",\"data\":{\"_body\":null,\"status\":500,\"ok\":false,\"statusText\":\"INKApi Error\",\"headers\":{\"Date\":[\"Mon\",\" 09 Jul 2018 04:17:06 GMT\"],\"Via\":[\"1.1 localhost.localdomain\"],\"Proxy-Connection\":[\"close\"],\"Content-Length\":[\"0\"],\"Content-Type\":[\"text\/html\"]},\"type\":2,\"url\":\"http:\/\/localhost:54240\/projects\/v1.0\/create\/template\"},\"sid\":\"365395c0-832b-11e8-b4ce-e5d7046c6143\"}\n\n[2018-07-09 09:47:08.963] [FATAL] ExceptionLogger - {\"event\":\"exception\",\"task\":\"\",\"data\":{\"message\":\"Cannot read property 'error' of null\",\"name\":\"TypeError\",\"stack\":\"TypeError: Cannot read property 'error' of null\\n    at SafeSubscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:61476:58)\\n    at SafeSubscriber.__tryOrUnsub (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212279:20)\\n    at SafeSubscriber.error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212241:30)\\n    at Subscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212172:30)\\n    at Subscriber.error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212146:22)\\n    at MergeMapSubscriber.OuterSubscriber.notifyError (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:210968:30)\\n    at InnerSubscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:211072:25)\\n    at InnerSubscriber.Subscriber.error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212146:22)\\n    at DeferSubscriber.OuterSubscriber.notifyError (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:210968:30)\\n    at InnerSubscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:211072:25)\"},\"sid\":\"365395c0-832b-11e8-b4ce-e5d7046c6143\"}\n<\/code><\/pre>",
        "Challenge_closed_time":1531201276972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531112127690,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an issue while creating a new project in Azure Machine Learning Workbench. After clicking on the Create button, the project goes into a \"Creating\" state and stays there indefinitely. The errors displayed in the log indicate a 500 error and a null property error. The user is seeking suggestions to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51238413",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":29.4,
        "Challenge_reading_time":46.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":24.7636894444,
        "Challenge_title":"Azure Machine Learning Workbench hangs while creating new project",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":67.0,
        "Challenge_word_count":160,
        "Platform":"Stack Overflow",
        "Poster_created_time":1280505139752,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":4265.0,
        "Poster_view_count":403.0,
        "Solution_body":"<p>It was happening because of the Proxy (although I have configured the Proxy on the Workbench). When I am connected to internet directly, everything works fine (Able to create project, train, compare models etc). However the Workbench should return meaningful error instead of hanging or simply waiting while creating the project.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":4.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1565129860212,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":4.0860186111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created an Azure Machine Learning Service Pipeline which i am invoking externally using its rest endpoint.\nBut i also need to monitor its run , whether it got completed or failed, periodically.\n<strong>Is there a methodinside a machine learning pipeline's rest endpoint, which i can hit to check its run status?<\/strong>\nI have tried the steps mentioned in the link here \n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb<\/a><\/p>",
        "Challenge_closed_time":1569516992870,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569502183603,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an Azure Machine Learning Service Pipeline and is invoking it externally using its rest endpoint. However, they need to monitor its run status periodically and are looking for a method inside the pipeline's rest endpoint to check its status. The user has tried the steps mentioned in a GitHub link but is still facing the issue.",
        "Challenge_last_edit_time":1569502283203,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58117200",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":15.5,
        "Challenge_reading_time":10.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":4.1136852778,
        "Challenge_title":"How to get status of Azure machine learning service pipeline run using Rest Api?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":898.0,
        "Challenge_word_count":79,
        "Platform":"Stack Overflow",
        "Poster_created_time":1508663110972,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>For getting status of run, you can use REST APIs described here <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/data-plane\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/data-plane<\/a> <\/p>\n\n<p>Specifically you need <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/blob\/master\/specification\/machinelearningservices\/data-plane\/Microsoft.MachineLearningServices\/preview\/2019-08-01\/runHistory.json\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/blob\/master\/specification\/machinelearningservices\/data-plane\/Microsoft.MachineLearningServices\/preview\/2019-08-01\/runHistory.json<\/a><\/p>\n\n<p>use this call to get run information including status:<\/p>\n\n<blockquote>\n  <p>\/history\/v1.0\/subscriptions\/{subscriptionId}\/resourceGroups\/{resourceGroupName}\/providers\/Microsoft.MachineLearningServices\/workspaces\/{workspaceName}\/experiments\/{experimentName}\/runs\/{runId}\/details<\/p>\n<\/blockquote>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":54.5,
        "Solution_reading_time":14.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":5.3960230556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am developing machine learning repositories that require fairly large trained model files to run. These files are not part of the git remote but is tracked by DVC and is saved in a separate remote storage. I am running into issues when I am trying to run unit tests in the CI pipeline for functions that require these model files to make their prediction. Since I don't have access them in the git remote, I can't test them.<\/p>\n<p>What is the best practice that people usually do in this situation? I can think of couple of options -<\/p>\n<ul>\n<li>Pull the models from the DVC remote inside the CI pipeline. I don't want to do this becasue downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/li>\n<li>Use <code>unittest.mock<\/code> to simulate the output of from the model prediction and test other parts of my code. This is what I am doing now but it's sort of a pain with unittest's mock functionalities. That module wasn't really developed with ML in mind from what I can tell. It's missing (or is hard to find) some functionalities that I would have really liked. Are there any good tools for doing this geared specifically towards ML?<\/li>\n<li>Do weird reformatting of the function definition that allows me to essentially do option 2 but without a mock module. That is, just test the surrounding logic and don't worry about the model output.<\/li>\n<li>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/li>\n<\/ul>\n<p>What do people usually do in this situation?<\/p>",
        "Challenge_closed_time":1603314290768,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603252093370,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while running unit tests in the CI pipeline for functions that require large trained model files, which are not part of the git remote but are tracked by DVC and saved in a separate remote storage. The user is considering different options, including pulling the models from the DVC remote inside the CI pipeline, using unittest.mock to simulate the output of the model prediction, or putting the model files in the git remote. The user is seeking advice on the best practice in this situation.",
        "Challenge_last_edit_time":1603305923907,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64456396",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":20.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":17.277055,
        "Challenge_title":"How do I unit test a function in the CI pipeline that uses model files that are not part of the git remote?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":289.0,
        "Challenge_word_count":304,
        "Platform":"Stack Overflow",
        "Poster_created_time":1446746840592,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2545.0,
        "Poster_view_count":382.0,
        "Solution_body":"<p>If we talk about unit tests, I think it's indeed better to do a mock. It's best to have unit tests small, testing actual logic of the unit, etc. It's good to have other tests though that would pull the model and run some logic on top of that - I would call them integration tests.<\/p>\n<p>It's not black and white though. If you for some reason see that it's easier to use an actual model (e.g. it changes a lot and it is easier to use it instead of maintaining and updating stubs\/fixtures), you could potentially cache it.<\/p>\n<p>I think, to help you with the mock, you would need to share some technical details- how does the function look like, what have you tried, what breaks, etc.<\/p>\n<blockquote>\n<p>to do this because downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/p>\n<\/blockquote>\n<p>I think you can potentially utilize CI systems cache to avoid downloading it over and over again. This is the GitHub Actions related <a href=\"https:\/\/github.com\/actions\/cache#cache-limits\" rel=\"nofollow noreferrer\">repository<\/a>, this is <a href=\"https:\/\/circleci.com\/docs\/2.0\/caching\" rel=\"nofollow noreferrer\">CircleCI<\/a>. The idea is the same across all common CI providers. Which one are considering to use, btw?<\/p>\n<blockquote>\n<p>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/p>\n<\/blockquote>\n<p>This can be the way, but if models are large enough you will pollute Git history significantly. On some CI systems it can become even slower since they will be fetching this with regular <code>git clone<\/code>. Effectively, downloading models anyway.<\/p>\n<p>Btw, if you use DVC or not take a look at another open-source project that is made specifically to do CI\/CD for ML - <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">CML<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1603325349590,
        "Solution_link_count":3.0,
        "Solution_readability":7.6,
        "Solution_reading_time":23.14,
        "Solution_score_count":3.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":297.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1619204963587,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1442.0,
        "Answerer_view_count":879.0,
        "Challenge_adjusted_solved_time":478.5142619445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have successfully run the following pipeline that creates a dataset, trains a model, and deploys to an endpoint using VertexAIs pipeline tool when everything is based in us-central1. Now, when I change the region to europe-west2, I get the following error:<\/p>\n<pre><code>debug_error_string = &quot;{&quot;created&quot;:&quot;@1647430410.324290053&quot;,&quot;description&quot;:&quot;Error received from peer\nipv4:172.217.169.74:443&quot;,&quot;file&quot;:&quot;src\/core\/lib\/surface\/call.cc&quot;,&quot;file_line&quot;:1066,\n&quot;grpc_message&quot;:&quot;List of found errors:\\t1.Field: name; Message: \nThe provided location ID doesn't match the endpoint. The valid location ID is `us-central1`.\\t&quot;,&quot;grpc_status&quot;:3}&quot;\n<\/code><\/pre>\n<p>This error occurs after the dataset is created in europe-west2, and before the model starts to train. Here is my code:<\/p>\n<pre><code>#import libraries\nfrom typing import NamedTuple\nimport kfp\nfrom kfp import dsl\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import (Artifact, Input, InputPath, Model, Output, \n                        OutputPath, ClassificationMetrics, Metrics, component)\nfrom kfp.v2.components.types.artifact_types import Dataset\nfrom kfp.v2.google.client import AIPlatformClient\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom google.api_core.exceptions import NotFound\n\n@kfp.dsl.pipeline(name=f&quot;lookalike-model-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = f&quot;bq:\/\/{PROJECT_ID}.{DATASET_ID}.{TABLE_NAME}&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;europe-west2&quot;,\n    api_endpoint: str = &quot;europe-west2-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auPrc&quot;: 0.5}',\n):\n            \n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project,\n        display_name=display_name, \n        bq_source=bq_source,\n        location = gcp_region\n    )\n\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        location=gcp_region,\n        predefined_split_column_name=&quot;set&quot;,\n        column_transformations=[\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;agentId&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;postcode&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;isMobile&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;gender&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;timeOfDay&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;set&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;sale&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;sale&quot;,\n    )\n\ncompiler.Compiler().compile(\n    pipeline_func=pipeline, package_path=&quot;tab_classif_pipeline.json&quot;\n)\n\nml_pipeline_job = aiplatform.PipelineJob(\n    display_name=f&quot;{MODEL_PREFIX}_training&quot;,\n    template_path=&quot;tab_classif_pipeline.json&quot;,\n    pipeline_root=PIPELINE_ROOT,\n    parameter_values={&quot;project&quot;: PROJECT_ID, &quot;display_name&quot;: DISPLAY_NAME},\n    enable_caching=True,\n    location=&quot;europe-west2&quot;\n)\nml_pipeline_job.submit()\n<\/code><\/pre>\n<p>As previously mentioned, the dataset gets created so I suspect that the issue must lie in <code>training_op = gcc_aip.AutoMLTabularTrainingJobRunOp<\/code><\/p>\n<p>I tried providing another endpoint: <code>eu-aiplatform.googleapis.com<\/code> which yielded the following error:<\/p>\n<pre><code>google.api_core.exceptions.InvalidArgument: 400 List of found errors: 1.Field: name; Message: \nThe provided location ID doesn't match the endpoint. The valid location ID is `us-central1`.\n\nFail to send metric: [rpc error: code = PermissionDenied desc = Permission monitoring.metricDescriptors.create \ndenied (or the resource may not exist).; rpc error: code = PermissionDenied desc = Permission monitoring.timeSeries.create\n denied (or the resource may not exist).]\n<\/code><\/pre>\n<p>I understand that I am not passing api-endpoint to any of the methods above, but I thought I'd highlight that the error changed slightly.<\/p>\n<p>Does anyone know what the issue may be? Or how I can run <code>gcc_aip.AutoMLTabularTrainingJobRunOp<\/code> in europe-west2 (or EU in general)?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1649155832383,
        "Challenge_comment_count":1,
        "Challenge_created_time":1647433181040,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to run a pipeline using VertexAI in the europe-west2 region. The error message states that the provided location ID does not match the endpoint, and suggests that the valid location ID is us-central1. The error occurs after the dataset is created and before the model starts to train. The user has tried changing the API endpoint to eu-aiplatform.googleapis.com, but the error persists. The user is seeking help to resolve the issue and run the pipeline in europe-west2 or EU in general.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71496966",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":17.4,
        "Challenge_reading_time":59.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":478.5142619445,
        "Challenge_title":"VertexAI Pipelines: The provided location ID doesn't match the endpoint",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":220.0,
        "Challenge_word_count":357,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562706291280,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>Try Updating the pipeline component using the command:<\/p>\n<p><code>pip3 install --force-reinstall google_cloud_pipeline_components==0.1.3<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.2,
        "Solution_reading_time":2.07,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":12.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2575583333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am a new learner of machine learning and computer science, I wonder the difference between these two terms. I am confused on the concept, can someone answer this question?  <\/p>",
        "Challenge_closed_time":1623787840780,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623786913570,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is a new learner of machine learning and computer science and is confused about the difference between online learning and offline learning. They are seeking clarification on the concept.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/437505\/what-is-the-difference-between-online-learning-and",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":3.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.2575583333,
        "Challenge_title":"What is the difference between online learning and offline learning",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":40,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,  <\/p>\n<p>Thanks for reaching out to us here. They are both machine learning methods for training. online machine learning is a method of machine learning in which data becomes available in <strong>a sequential order and is used to update the best predictor<\/strong> for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the <strong>entire training data set at once.<\/strong>   <\/p>\n<p>Like, one more data coming in, the <strong>predictor moves<\/strong> once. This method is good for scenario like stock prediction, optimization...  <\/p>\n<p>Linear least square is a very good example to understand.   <br \/>\n<a href=\"https:\/\/en.wikipedia.org\/wiki\/Linear_least_squares\">https:\/\/en.wikipedia.org\/wiki\/Linear_least_squares<\/a>  <\/p>\n<p>For Machine Learning beginner, Machine Learning Designer is a very good point to start. You can try any algorithms to see the difference.  <br \/>\n<a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning\/designer\/\">https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning\/designer\/<\/a>  <\/p>\n<p>Please feel free to let us know if you have more questions.  <\/p>\n<p>Regards,  <br \/>\nYutong  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.3,
        "Solution_reading_time":15.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":150.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":48.2963777778,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi there,\n\nIs it possible to use R model training and serving in SageMaker ML Pipelines? Looked in examples [here](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/r_examples).  And it doesn't look that R is fully supported currently by ML Pipelines.  Any examples and success stories are very welcome. \n\nThanks.",
        "Challenge_closed_time":1643404063708,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643230196748,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of using R model training and serving in SageMaker ML Pipelines. They have looked at examples but it doesn't seem like R is fully supported currently by ML Pipelines. They are seeking examples and success stories.",
        "Challenge_last_edit_time":1668030995400,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU17aS4s7uSRqmiLuveuchBw\/using-r-model-in-sagemaker-ml-pipelines",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":4.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":48.2963777778,
        "Challenge_title":"Using R model in SageMaker ML pipelines",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":244.0,
        "Challenge_word_count":48,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"In general it is possible to use the SageMaker python SDK and boto3  using the reticulate package in R, However do not have direct examples of SageMaker Pipelines using R.\n\nIt is possible to orchestrate the production pipeline using the R Containers for training and serving and setting up the DAG can be done with reticulate and SageMaker Python SDK and can be achieved using the AWS Step Functions. Please refer to the following example for reference.\n\n\nhttps:\/\/github.com\/aws-samples\/reinvent2020-aim404-productionize-r-using-amazon-sagemaker\nhttps:\/\/www.youtube.com\/watch?v=Zpp0nfvqDCA",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1643404063708,
        "Solution_link_count":2.0,
        "Solution_readability":15.9,
        "Solution_reading_time":7.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":44.4,
        "Challenge_answer_count":3,
        "Challenge_body":"I have been using the the autoML regression pipeline templates for a while successfully.\u00a0 Three days ago overnight they broke.\u00a0 The same job that worked the night before hangs with the error message: \"Unable to create pipeline run due to the following error: Input parameter type mismatch. PipelineSpec.root.input_definitions.parameters['dataflow_use_public_ips'] is defined as BOOLEAN that parses BOOL_VALUE type, but the default value is provided as NUMBER_VALUE type.\"\n\nIf I want to re-run a clone of a previously successful training the same happens.\u00a0 I have tried to set public IP setting to different values -- no success.\u00a0 I have downloaded and edited the yaml according to the error-- no luck either!\n\nAnybody encountered the same?\u00a0 Is there a workaround?",
        "Challenge_closed_time":1683696000000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683536160000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with VertexAI autoML regression pipeline templates. The error message \"Input parameter type mismatch\" is displayed while creating a pipeline run due to a mismatch in the default value provided for the parameter 'dataflow_use_public_ips'. The user has tried different settings and editing the yaml file but has not been successful in resolving the issue. The user is seeking help for a workaround.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VertexAI-autoML-pipeline-template-error-tabular-regression\/m-p\/551367#M1828",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":44.4,
        "Challenge_title":"VertexAI autoML pipeline template error (tabular-regression)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":80.0,
        "Challenge_word_count":120,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"OK!\u00a0 The original Google provided template is now fixed and running!\n\nThanks for the help!\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.7,
        "Solution_reading_time":1.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":20.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":147.3688888889,
        "Challenge_answer_count":0,
        "Challenge_body":"### What steps did you take:\r\nAttempted to run the Sagemaker training operator using a custom image that is not hosted on ECR\r\n\r\n### What happened:\r\nI got the following error:\r\n```\r\nException: Invalid training image. Please provide a valid Amazon Elastic Container Registry path of the Docker image to run.\r\n```\r\n\r\n### What did you expect to happen:\r\nOur CI\/CD pipeline is set up to push images to our own personal registry that is not hosted on ECR - ideally, I would want to run Sagemaker training jobs using images hosted from our personal registry instead of having to also push our images to ECR (much more error-prone + having to maintain two container registries ...)\r\n\r\n\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\nDeployed kubeflow piplines as part of kubeflow deployment on AWS EKS",
        "Challenge_closed_time":1589522860000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1588992332000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running a custom image using the sagemaker training operator. The error occurred when the user used boto3 to manually download an object from s3, resulting in an \"Unable to locate credentials\" error. Although the boto credentials were found in environment variables, they did not make their way to the boto3 client instantiated inside the custom image. The user expected the credentials to be passed to the image that the training operator is running, but it did not happen.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3728",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":12.5,
        "Challenge_reading_time":10.43,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":147.3688888889,
        "Challenge_title":"Sagemaker Training Operator throws an error if custom image is not hosted on ECR",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":141,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thank you @marwan116  for trying out the operator. Currently SageMaker has support for images hosted in ECR only. \r\nSageMaker has support for various frameworks like TensorFlow, XGBoost, PyTorch etc as well as some [in-built algorithms](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html).\r\n\r\nIf you have custom image, [here](https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/docker-push-ecr-image.html) is the instruction to put them into ECR.\r\n  https:\/\/github.com\/kubeflow\/pipelines\/issues\/3670 @marwan116 looks like question answered, I'm going to close this issue.\r\nBut feel free to reopen with `\/reopen` comment.\r\n\r\n\/close @Bobgy: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/3728#issuecomment-629047584):\n\n>@marwan116 looks like question answered, I'm going to close this issue.\r\n>But feel free to reopen with `\/reopen` comment.\r\n>\r\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":13.1,
        "Solution_reading_time":16.5,
        "Solution_score_count":null,
        "Solution_sentence_count":13.0,
        "Solution_word_count":129.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4308333333,
        "Challenge_answer_count":0,
        "Challenge_body":"We encountered an issue where an older Sagemaker instance (>2 months) was turned on. After starting, one of the two study folders associated were not syncing any of the files. In the system logs there's this error: `Nov 11 16:21:45 <ip redacted> \/usr\/local\/bin\/goofys[9204]: main.ERROR Unable to access '<bucket A, name redacted>': permission denied`\r\n\r\nComparing the S3mounts parameter for the Sagemaker stack of the older instance that fails to sync, and a newer instance (with the same studies), I see that the FS role number for the private workspace study that wouldn't sync is different.\r\n\r\nOld stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1662735997814\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nNew stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1668521384862\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nSome additional context, this bucket (and the associated SWB data source) that the two studies are a part of gets updated every couple months to add new study folders\/ids.\r\n\r\nMy question is: What could cause the fs role number to change for a study?\r\n\r\n**Versions (please complete the following information):**\r\n - SWB 4.3.1\r\n",
        "Challenge_closed_time":1668634670000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1668633119000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue with the Sagemaker autostop script, which caused instances to hang around for days. The cron job was failing due to a syntax error in the autostop.py script, which was not present in the file on the repo or the S3 bucket. The error was caused by a line introduced in a recent commit, and it is unclear how it got into the Sagemaker notebook and why it was not overridden by the custom config start. The expected behavior is for the autostop script in the S3 bucket to be used for SWB Sagemaker instances.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1066",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":26.21,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.4308333333,
        "Challenge_title":"[Bug] SWB Sagemaker Study permission denied",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":231,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I'm elevating this to a bug.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.5,
        "Solution_reading_time":0.34,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":6.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1327588060552,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":802.0,
        "Answerer_view_count":91.0,
        "Challenge_adjusted_solved_time":599.2239861111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In my pipeline multiple steps are independent and so I would like them to run in parallel based on input dependencies.<\/p>\n<p>As the compute I use has multiple nodes I would have expected this to be the default.<\/p>\n<p>For example:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Iye85.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iye85.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>All 3 upper steps should run in parallel, then both <code>finetune<\/code> steps in parallel as soon as their inputs are satisfied and the same for <code>rgb_test<\/code>.<\/p>\n<p>Currently only 1 step runs at a time, the other are <code>Queued<\/code>.<\/p>",
        "Challenge_closed_time":1632769372630,
        "Challenge_comment_count":2,
        "Challenge_created_time":1630612166280,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to run multiple independent `PythonScriptStep` steps in parallel based on input dependencies in their pipeline. They expected this to be the default as their compute has multiple nodes, but currently only one step runs at a time and the others are queued.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69036277",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":599.2239861111,
        "Challenge_title":"Run independent `PythonScriptStep` steps in parallel",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":156.0,
        "Challenge_word_count":93,
        "Platform":"Stack Overflow",
        "Poster_created_time":1327588060552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":802.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p>It ended up being because of vCPU quota.<\/p>\n<p>After increasing the quota, parallel tasks can run at the same time as expected.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":1.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1286966860487,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Australia",
        "Answerer_reputation_count":55279.0,
        "Answerer_view_count":5321.0,
        "Challenge_adjusted_solved_time":6.2338886111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have just started to get myself acquainted with parallelism in R. <\/p>\n\n<p>As I am planning to use <a href=\"https:\/\/studio.azureml.net\/\" rel=\"nofollow\">Microsoft Azure Machine Learning Studio<\/a> for my project, I have started investigating what <a href=\"https:\/\/mran.revolutionanalytics.com\/documents\/rro\/multithread\/\" rel=\"nofollow\">Microsoft R Open<\/a> offers for parallelism, and thus, I found <a href=\"https:\/\/mran.revolutionanalytics.com\/documents\/rro\/multithread\/\" rel=\"nofollow\">this<\/a>, in which it says that parallelism is done under the hood that leverages the benefit of all available cores, without changing the R code. The article also shows some performance benchmarks, however, most of them demonstrate the performance benefit in doing mathematical operations.<\/p>\n\n<p>This was good so far. In addition, I am also interested to know whether it also parallelize the <code>*apply<\/code> functions under the hood or not. I also found these 2 articles that describes how to parallelize <code>*apply<\/code> functions in general:<\/p>\n\n<ol>\n<li><a href=\"https:\/\/www.r-bloggers.com\/quick-guide-to-parallel-r-with-snow\/\" rel=\"nofollow\">Quick guide to parallel R with snow<\/a>: describes facilitating parallelism using <a href=\"https:\/\/cran.r-project.org\/web\/packages\/snow\/snow.pdf\" rel=\"nofollow\"><code>snow<\/code><\/a> package, <code>par*apply<\/code> function family, and <code>clusterExport<\/code>.<\/li>\n<li><a href=\"http:\/\/www.win-vector.com\/blog\/2016\/01\/parallel-computing-in-r\/\" rel=\"nofollow\">A gentle introduction to parallel computing in R<\/a>: using <code>parallel<\/code> package, <code>par*apply<\/code> function family, and binding values to environment.<\/li>\n<\/ol>\n\n<p>So my question is when I will be using <code>*apply<\/code> functions in Microsoft Azure Machine Learning Studio, will that be parallelized under the hood by default, or I need to make use of packages like <code>parallel<\/code>, <code>snow<\/code> etc.?<\/p>",
        "Challenge_closed_time":1476024698776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1476002256777,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is exploring parallelism in R and is planning to use Microsoft Azure Machine Learning Studio for their project. They found that Microsoft R Open offers parallelism under the hood, but they are unsure if the *apply functions will also be parallelized by default or if they need to use packages like parallel or snow. They found two articles that describe how to parallelize *apply functions in general.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39941622",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":14.9,
        "Challenge_reading_time":26.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":6.2338886111,
        "Challenge_title":"Parallel *apply in Azure Machine Learning Studio",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":501.0,
        "Challenge_word_count":221,
        "Platform":"Stack Overflow",
        "Poster_created_time":1365684640140,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Paderborn, Germany",
        "Poster_reputation_count":4588.0,
        "Poster_view_count":453.0,
        "Solution_body":"<p>Personally, I think we could have marketed MRO a bit differently, without making such a big deal about parallelism\/multithreading. Ah well.<\/p>\n\n<p>R comes with an Rblas.dll\/.so which implements the routines used for linear algebra computations. These routines are used in various places, but one common use case is for fitting regression models. With MRO, we replace the standard Rblas with one that uses the <a href=\"https:\/\/software.intel.com\/en-us\/intel-mkl\" rel=\"noreferrer\">Intel Math Kernel Library<\/a>. When you call a function like <code>lm<\/code> or <code>glm<\/code>, MRO will use multiple threads and optimized CPU instructions to fit the model, which can get you dramatic speedups over the standard implementation.<\/p>\n\n<p>MRO isn't the only way you can get this sort of speedup; you can also compile\/download other BLAS implementations that are similarly optimized. We just make it an easy one-step download.<\/p>\n\n<p>Note that the MKL only affects code that involves linear algebra. It isn't a general-purpose speedup tool; any R code that doesn't do matrix computations won't see a performance improvement. In particular, it won't speed up any code that involves <em>explicit<\/em> parallelism, such as code using the parallel package, SNOW, or other cluster computing tools.<\/p>\n\n<p>On the other hand, it won't <em>degrade<\/em> them either. You can still use packages like parallel, SNOW, etc to create compute clusters and distribute your code across multiple processes. MRO works just like regular CRAN R in this respect. (One thing you might want to do, though, if you're creating a cluster of nodes on the one machine, is reduce the number of MKL threads. Otherwise you risk contention between the nodes for CPU cores, which will degrade performance.)<\/p>\n\n<p>Disclosure: I work for Microsoft.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.2,
        "Solution_reading_time":22.65,
        "Solution_score_count":5.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":272.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1548390570396,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":124.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":26.2102008333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm doing following tutorial. I failed to run &quot;Create a control script&quot;.<\/p>\n<p>What could be wrong?<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-hello-world\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-hello-world<\/a><\/p>\n<pre><code>azureuser@kensmlcompute:~\/cloudfiles\/code\/Users\/my.name\/get-started$ python run-hello.py \nFailure while loading azureml_run_type_providers. Failed to load entrypoint automl = \nazureml.train.automl.run:AutoMLRun._from_run_dto with exception (pyarrow 4.0.0 \n(\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages), \nRequirement.parse('pyarrow&lt;4.0.0,&gt;=0.17.0'), {'azureml-dataset-runtime'}).\nhttps:\/\/ml.azure.com\/runs\/day1-experiment-hello_1623766747_073126f5? \nwsid=\/subscriptions\/1679753a-501e-4e46-9bff- \n6120ed5694cf\/resourcegroups\/kensazuremlrg\/workspaces\/kensazuremlws&amp;tid=94fe1041-ba47-4f49- \n866b- \n06c297c116cc\nazureuser@kensmlcompute:~\/cloudfiles\/code\/Users\/my.name\/get-started$\n<\/code><\/pre>",
        "Challenge_closed_time":1623861331176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623766974453,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an issue while running the \"Create a control script\" step in the Azure ML tutorial. The error message indicates a failure to load the entry point automl, with a specific exception related to pyarrow. The user has provided a link to the tutorial and the error message for reference.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67988138",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":26.4,
        "Challenge_reading_time":15.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":26.2102008333,
        "Challenge_title":"Azure ML Tutorial - Failed to load entrypoint automl",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1241.0,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_created_time":1478251050692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Finland",
        "Poster_reputation_count":1519.0,
        "Poster_view_count":375.0,
        "Solution_body":"<p>I think the error indicates that your environment is using pyarrow package which is of version 4.0.0 whereas azureml-dataset-runtime requires the package to be &gt;=0.17.0 but &lt;4.0.0<\/p>\n<p>It would be easier for you to uninstall the package and install a specific version. The list of releases of pyarrow are available here.<\/p>\n<p>Since you are using a notebook create new cells and run these commands.<\/p>\n<pre><code> !pip uninstall pyarrow\n !pip install -y pyarrow==3.0.0\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":6.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":100.7332322222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to implement the best estimator from a hyperparameter tuning job into a pipeline object to deploy an endpoint.<\/p>\n\n<p>I've read the docs in a best effort to include the results from the tuning job in the pipeline, but I'm having trouble creating the Model() class object.<\/p>\n\n<pre><code># This is the hyperparameter tuning job\ntuner.fit({'train': s3_train, 'validation': s3_val}, \ninclude_cls_metadata=False)\n\n\n#With a standard Model (Not from the tuner) the process was as follows:\nscikit_learn_inferencee_model_name = sklearn_preprocessor.create_model()\nxgb_model_name = Model(model_data=xgb_model.model_data, image=xgb_image)\n\n\nmodel_name = 'xgb-inference-pipeline-' + timestamp_prefix\nendpoint_name = 'xgb-inference-pipeline-ep-' + timestamp_prefix\nsm_model = PipelineModel(\n    name=model_name, \n    role=role, \n    models=[\n        scikit_learn_inferencee_model_name, \n        xgb_model_name])\n\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', \nendpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I would like to be able to cleanly instantiate a model object using my results from the tuning job and pass it into the PipelineModel object. Any guidance is appreciated.<\/p>",
        "Challenge_closed_time":1558879357103,
        "Challenge_comment_count":0,
        "Challenge_created_time":1558813444520,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble creating a Model() class object to deploy an endpoint using the best estimator from a hyperparameter tuning job. They have read the documentation but are struggling to include the results from the tuning job in the pipeline. The user is seeking guidance on how to instantiate a model object using the tuning job results and pass it into the PipelineModel object.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56308169",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":16.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":18.3090508334,
        "Challenge_title":"Creating a model for use in a pipeline from a hyperparameter tuning job",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":455.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558812981692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>I think you are on the right track. Do you get any error? Refer this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/e9c295c8538d29cc9fea2f73a29649126628064a\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a>  for instantiating the model from the tuner and use in inference pipeline.<\/p>\n\n<p>Editing previous response based on the comment. To create model from the best training job of the hyperparameter tuning job, you can use below snippet<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tuner import HyperparameterTuner\nfrom sagemaker.estimator import Estimator\nfrom sagemaker.model import Model\n\n# Attach to an existing hyperparameter tuning job.\nxgb_tuning_job_name = 'my_xgb_hpo_tuning_job_name'\nxgb_tuner = HyperparameterTuner.attach(xgb_tuning_job_name)\n\n# Get the best XGBoost training job name from the HPO job\nxgb_best_training_job = xgb_tuner.best_training_job()\nprint(xgb_best_training_job)\n\n# Attach estimator to the best training job name\nxgb_best_estimator = Estimator.attach(xgb_best_training_job)\n\n# Create model to be passed to the inference pipeline\nxgb_model = Model(model_data=xgb_best_estimator.model_data,\n                  role=sagemaker.get_execution_role(),\n                  image=xgb_best_estimator.image_name)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1559176084156,
        "Solution_link_count":1.0,
        "Solution_readability":18.7,
        "Solution_reading_time":18.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":119.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1457261731840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, BC",
        "Answerer_reputation_count":584.0,
        "Answerer_view_count":270.0,
        "Challenge_adjusted_solved_time":11057.4897425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use the AWS sagemaker cli to run the create-training-job command. Here is my command:<\/p>\n\n<pre><code>aws sagemaker create-training-job \\\n--training-job-name $(DEPLOYMENT_NAME)-$(BUILD_ID) \\\n--hyper-parameters file:\/\/sagemaker\/hyperparameters.json \\\n--algorithm-specification TrainingImage=$(IMAGE_NAME),\\\nTrainingInputMode=\"File\" \\\n--role-arn $(ROLE) \\\n--input-data-config ChannelName=training,DataSource={S3DataSource={S3DataType=S3Prefix,S3Uri=$(S3_INPUT),S3DataDistributionType=FullyReplicated}},ContentType=string,CompressionType=None,RecordWrapperType=None \\\n--output-data-config S3OutputPath=$(S3_OUTPUT) \\\n--resource-config file:\/\/sagemaker\/train-resource-config.json \\\n--stopping-condition file:\/\/sagemaker\/stopping-conditions.json \n<\/code><\/pre>\n\n<p>and here is the error:<\/p>\n\n<pre><code>Parameter validation failed:\nInvalid type for parameter InputDataConfig[0].DataSource.S3DataSource, value: S3DataType=S3Prefix, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nInvalid type for parameter InputDataConfig[1].DataSource.S3DataSource, value: S3Uri=s3:\/\/hs-machine-learning-processed-production\/inbound-autotag\/data, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nInvalid type for parameter InputDataConfig[2].DataSource.S3DataSource, value: S3DataDistributionType=FullyReplicated, type: &lt;type 'unicode'&gt;, valid types: &lt;type 'dict'&gt;\nmake: *** [train] Error 255\n<\/code><\/pre>\n\n<p>The error is happening with the <code>--input-data-config<\/code> flag. I'm trying to use the Shorthand Syntax so I can inject some variables (the capitalized words). Haalp!<\/p>",
        "Challenge_closed_time":1567514274983,
        "Challenge_comment_count":1,
        "Challenge_created_time":1527707311910,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while using the AWS SageMaker create-training-job command with the --input-data-config flag. The error is related to the invalid type for parameter InputDataConfig and is happening with the Shorthand Syntax used to inject variables.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50611864",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":24.7,
        "Challenge_reading_time":22.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":11057.4897425,
        "Challenge_title":"Using the AWS SageMaker create-training-job command: type Error",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1130.0,
        "Challenge_word_count":125,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443201378360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":749.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>So, your input config is not correctly formatted. \nCheckout the sample json here:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html<\/a><\/p>\n\n<pre><code># look at the format of input-data-config, it is a dictionary\n  \"InputDataConfig\": [ \n      { \n         \"ChannelName\": \"string\",\n         \"CompressionType\": \"string\",\n         \"ContentType\": \"string\",\n         \"DataSource\": { \n            \"FileSystemDataSource\": { \n               \"DirectoryPath\": \"string\",\n               \"FileSystemAccessMode\": \"string\",\n               \"FileSystemId\": \"string\",\n               \"FileSystemType\": \"string\"\n            },\n            \"S3DataSource\": { \n               \"AttributeNames\": [ \"string\" ],\n               \"S3DataDistributionType\": \"string\",\n               \"S3DataType\": \"string\",\n               \"S3Uri\": \"string\"\n            }\n         },\n         \"InputMode\": \"string\",\n         \"RecordWrapperType\": \"string\",\n         \"ShuffleConfig\": { \n            \"Seed\": number\n         }\n      }\n   ]\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.1,
        "Solution_reading_time":11.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":146.8910691667,
        "Challenge_answer_count":9,
        "Challenge_body":"<p>I have a compute environment where I was running wandb offline for quite a while. I am now hoping to use it online (to get automatic syncing), however I seem to be unable to set this up now. The following is a minimal reproducible example:<\/p>\n<pre><code class=\"lang-auto\">&gt;&gt; import wandb\n&gt;&gt; test = wandb.init(mode='online')\nTraceback (most recent call last):\n  File \"[path]\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_init.py\", line 867, in init\n    wi.setup(kwargs)\n  File \"[path]\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_init.py\", line 182, in setup\n    user_settings = self._wl._load_user_settings()\n  File \"[path]\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_setup.py\", line 183, in _load_user_settings\n    flags = self._server._flags\nAttributeError: 'NoneType' object has no attribute '_flags'\nwandb: ERROR Abnormal program exit\n<\/code><\/pre>\n<p>I have tried<\/p>\n<ul>\n<li>running wandb online in the terminal<\/li>\n<li>setting the wandb mode environment variable to be online<\/li>\n<li>uninstalling and reinstalling wandb<\/li>\n<\/ul>\n<p>Is there any way I can run this online?<\/p>",
        "Challenge_closed_time":1637034448659,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636505640810,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user was previously running wandb offline but now wants to switch to online mode for automatic syncing. However, they are encountering an error when trying to initialize wandb in online mode. They have tried various solutions such as setting the wandb mode environment variable and reinstalling wandb, but the issue persists. The user is seeking help to resolve the problem and run wandb online.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/unable-to-run-wandb-online-after-running-offline\/1252",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":10.4,
        "Challenge_reading_time":14.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":146.8910691667,
        "Challenge_title":"Unable to run wandb online after running offline",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":390.0,
        "Challenge_word_count":133,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/dimaduev\">@dimaduev<\/a> , thanks so much for the fixes! I think I was able to resolve this through looking at the different WANDB_DIR locations\u2026 I had several in different bashrc\/zshrc files and I suspect this was causing an issue. It seems to be resolved now!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":3.72,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":46.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1543778671427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tokyo",
        "Answerer_reputation_count":99.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":0.9683091666,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want suggest ratio in Optuna.<\/p>\n<p>The ratio is <code>X_1, X_2, ..., X_k<\/code> bounded to <code>\u2211X_i = 1<\/code> and <code>0 &lt;= X_i &lt;= 1<\/code> for all <code>i<\/code>.<\/p>\n<p>Optuna doesn't offer Dirichlet distribution.<\/p>\n<p>I tried this but it doesn't work.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    k = 10\n    ratios = np.zeros(k)\n    \n    residual = 1\n    for i in range(k - 1):\n        ratios[i] = trial.suggest_float(f'ratio_{i}', 0, residual)\n        residual -= ratios[i]\n        \n#     ratios[k - 1] = trial.suggest_float(f'ratio_{k - 1}', residual, residual)\n    ratios[k - 1] = residual\n    return np.log(ratios).sum()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=20)\n<\/code><\/pre>\n<p>And I tried this and finished without any errors. However, this is inconsistent because degree of freedom is <code>k - 1<\/code> for the bound but suggest <code>k<\/code> times.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    k = 10\n    ratios = np.zeros(k)\n    \n    for i in range(k):\n        ratios[i] = trial.suggest_float(f'ratio_{i}', 0, 1)\n    \n    ratios \/= ratios.sum()\n    return np.log(ratios).sum()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=20)\n<\/code><\/pre>\n<p>How can I suggest ratio or multiple variables with bound?<\/p>\n<p>This is a simple example so it's differentiable but I need variables in more complex objective.<\/p>",
        "Challenge_closed_time":1610599709396,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610595386710,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to suggest a ratio in Optuna that is bounded to the sum of the variables being equal to 1 and each variable being between 0 and 1. However, Optuna does not offer Dirichlet distribution. The user has tried two methods, but one is inconsistent and the other does not work for more complex objectives. The user is seeking a solution to suggest ratios or multiple variables with bounds.",
        "Challenge_last_edit_time":1610596223483,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65713063",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":18.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":1.2007461111,
        "Challenge_title":"How to suggest multivariate of ratio (with bound) in Optuna?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":183.0,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1543778671427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tokyo",
        "Poster_reputation_count":99.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>This worked.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class Objective:\n    def __init__(self):\n        self.max = 1\n    def __call__(self, trial):\n        k = 10\n        ratios = np.zeros(k)\n\n        for i in range(k):\n            ratios[i] = trial.suggest_float(f'ratio_{i}', 0, self.max)\n\n        ratios \/= ratios.sum()\n        self.max = (self.max + ratios.max()) \/ 2\n        return np.log(ratios).sum()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(Objective(), n_trials=100)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":5.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":38.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1611181716003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":119.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":1654.0729555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a published Azure ML Pipeline that I am trying to trigger from an Automate Flow I have that triggers when users edit a document. Since I have the REST Endpoint for the Published Pipeline, I figured I should be able to make a POST request using the HTTP module available in Power Automate to trigger the pipeline.<\/p>\n<p>However, when I actually try this, I get an authentication error. I assume this is because I need to include some access token with the REST Endpoint, but I can't find any documentation that will tell me where to get that token from. Please note that I do not need to pass any data to the Pipeline, it handles its own data collection, I literally just need a way to trigger it.<\/p>\n<p>Does anybody know how to trigger a Published Azure ML Pipeline using the REST Endpoint? Does it make sense to use the HTTP module, or is there a better way to achieve this?<\/p>",
        "Challenge_closed_time":1630892629443,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624934847563,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to trigger an Azure ML Pipeline from a Power Automate Flow using the REST Endpoint, but is encountering an authentication error. They are unsure of how to obtain the necessary access token and are seeking advice on the best way to trigger the pipeline.",
        "Challenge_last_edit_time":1624937966803,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68172002",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":11.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1654.9394111111,
        "Challenge_title":"How to trigger Azure ML Pipeline from Power Automate",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":305.0,
        "Challenge_word_count":172,
        "Platform":"Stack Overflow",
        "Poster_created_time":1611181716003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":119.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>So I figured out how to do it by following the directions contained within this piece of Microsoft Documentation:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-rest\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-rest<\/a><\/p>\n<p>Specifically, it required performing two of the calls in the documentation;<\/p>\n<ul>\n<li>The first to get an AAD token using an Azure Service Principle that is authorised to access the Machine Learning Instance.<\/li>\n<\/ul>\n<blockquote>\n<p>curl -X POST <a href=\"https:\/\/login.microsoftonline.com\/\" rel=\"nofollow noreferrer\">https:\/\/login.microsoftonline.com\/<\/a>\/oauth2\/token -d &quot;grant_type=client_credentials&amp;resource=https%3A%2F%2Fmanagement.azure.com%2F&amp;client_id=&amp;client_secret=&quot;<\/p>\n<\/blockquote>\n<ul>\n<li>The second to use this token to trigger your pipeline from its rest endpoint. This one I had to figure out myself a little, but below is the basic structure I used.<\/li>\n<\/ul>\n<blockquote>\n<p>curl -X POST {PIPELINE_REST_ENDPOINT} -H &quot;Authorisation:Bearer {AAD_TOKEN}&quot; -H &quot;Content-Type: application\/json&quot; -d &quot;{&quot;ExperimentName&quot;: &quot;{EXPERIMENT_NAME}&quot;,&quot;ParameterAssignments&quot;: {}}&quot;<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":17.8,
        "Solution_reading_time":17.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":118.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":70.4262202778,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>I am inspecting and analysing my best runs. I expected that <code>group<\/code> and <code>job_type<\/code> would be populated with the resumed run\u2019s values after running the code below.<\/p>\n<pre><code class=\"lang-python\">run_id = input(\"id=\")\nwith wandb.init(entity=wandb_entity, project=wandb_project, id=run_id, resume=\"must\") as wandb_r:\n    config = wandb_r.config\n    group = wandb_r.group\n    job_type = wandb_r.job_type\n<\/code><\/pre>\n<p>Even though <code>config<\/code> is successfully recovered, <code>group<\/code> and <code>job_type<\/code> are just empty strings. How do I retrieve group and job_type values from WandB? Thanks.<\/p>",
        "Challenge_closed_time":1662128318611,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661874784218,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to retrieve the `group` and `job_type` values of a resumed run using the code provided, but even though the `config` value is successfully recovered, the `group` and `job_type` values are empty strings. The user is seeking help on how to retrieve these values from WandB.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-retrieve-the-group-and-job-type-of-a-resumed-run\/3031",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":10.2,
        "Challenge_reading_time":8.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":70.4262202778,
        "Challenge_title":"How to retrieve the `group` and `job_type` of a resumed run?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":115.0,
        "Challenge_word_count":78,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/avm21\">@avm21<\/a>, it looks like we don\u2019t download these on resumed runs but rather we don\u2019t update them unless you explicitly change them on a resumed run. If you need to get group\/job_type you can use the public API like this to access anything you may need:<\/p>\n<pre><code class=\"lang-auto\">import wandb\nfrom wandb import Api\n\napi = Api()\n\nwith wandb.init(entity=wandb_entity, project=wandb_project, id=run_id, resume=\"must\") as wandb_r:\n    config = wandb_r.config\n\n    # A resumed run will still have the path attribute which can be used to access the run via the API\n    api_run = api.run(wandb_r.path)\n\n    # This will correctly print the group of the run\n    print(api_run.group)\n<\/code><\/pre>\n<p>Let me know if you have any questions around this.<\/p>\n<p>Thank you,<br>\nNate<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":9.97,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":113.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":1.1426305556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>while running pipeline creation python script facing the following error.\n&quot;AzureMLCompute job failed. JobConfigurationMaxSizeExceeded: The specified job configuration exceeds the max allowed size of 32768 characters. Please reduce the size of the job's command line arguments and environment settings&quot;<\/p>",
        "Challenge_closed_time":1632803094283,
        "Challenge_comment_count":1,
        "Challenge_created_time":1632798980813,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while running a pipeline creation python script in Azure, stating that the job configuration exceeds the maximum allowed size of 32768 characters. The user needs to reduce the size of the job's command line arguments and environment settings.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69355385",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.7,
        "Challenge_reading_time":4.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.1426305556,
        "Challenge_title":"Size of the input \/ output parameters in the pipeline step in Azure",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":208.0,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Poster_created_time":1632461310820,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":107.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>When we tried to pass a quite lengthy content as argument value to a Pipeline. You can try to upload file to blob, optionally create a dataset, then pass on dataset name or file path to AML pipeline as parameter. The pipeline step will read content of the file from the blob.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":3.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1278090607540,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":30.0,
        "Challenge_adjusted_solved_time":21.0488663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Not sure if this is possible or not, but this is what I am trying to do: -<\/p>\n\n<p>I want to extract out portions (steps) of a function as individual nodes (ok so far), but the catch is I have an iterator on top of steps, which is dependent on some logic on dataset i.e. repeating the same operation (which are independent) on logical partitions of a dataset. <\/p>\n\n<h3>Example code<\/h3>\n\n<pre class=\"lang-py prettyprint-override\"><code>def single_node(list_of_numbers):\n   modified_list = [] # to store all output\n   for x in list_of_numbers: # iteration logic\n      x+=1 # Step 1\n      x=str(x) # Step 2\n      x+=\"_suffix\" # Step 3\n      modified_list.append(x) # append to final output\n   return modified_list # return\n<\/code><\/pre>\n\n<h3>Context<\/h3>\n\n<ol>\n<li>In the provided example, suppose currently I have a single node which performs all of the steps.<\/li>\n<li>So the current pipeline has one node which takes 1 input and returns 1 output.<\/li>\n<li>As the complexity of my steps increases, I want to expose them as individual nodes. So I create another pipeline with these 3 steps as individual nodes and connected them together. (their input and output)<\/li>\n<li>But my overall requirement is unchanged, I want to iterate over all values in <code>list_of_numbers<\/code>, and for each element in this list I want to call this new pipeline. Finally I want to merge the output of all run's and generate one output.<\/li>\n<\/ol>\n\n<p>Seems somewhat similar to dynamic graph (multiple dynamic instance of a pipeline) which expands based on the dataset. <\/p>\n\n<h3>Additional points to consider,<\/h3>\n\n<ol>\n<li>My input is a single file. Say I do the partition of dataset based on some logic defined as a node. So this node could have multiple outputs. (the exact count totally depends on the dataset, here the size of list) <\/li>\n<li>For each output of the data iterator node, I need to \"spawn\" one pipeline.<\/li>\n<li>Finally, merge the outputs of all \"spawned\" pipelines. (this logic could again be defined in a merge node with multiple dynamic inputs).<\/li>\n<\/ol>\n\n<p>Is there a way to do this? Thank you! <\/p>",
        "Challenge_closed_time":1591785478972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591709703053,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a dynamic instance of a pipeline execution based on dataset partition\/iterator logic. They want to extract portions of a function as individual nodes, but have an iterator on top of the steps that is dependent on some logic on the dataset. The user wants to iterate over all values in a list and call a new pipeline for each element in the list, then merge the output of all runs to generate one output. They need to \"spawn\" one pipeline for each output of the data iterator node and merge the outputs of all \"spawned\" pipelines.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62283931",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":26.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":21.0488663889,
        "Challenge_title":"Dynamic instance of pipeline execution based on dataset partition\/iterator logic",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":561.0,
        "Challenge_word_count":339,
        "Platform":"Stack Overflow",
        "Poster_created_time":1338197185852,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":1025.0,
        "Poster_view_count":228.0,
        "Solution_body":"<p>This looks like the PartitionedDataSet or IncrementalDataSet might be of use to you.<\/p>\n\n<p>They allow you to segregate your similar data into separate chunks, determined by files, and repeat operations on those chunks as you see fit.<\/p>\n\n<p>So, rather than kick of x pipelines containing y nodes, you would have one pipeline that contains y nodes which processes x chunks of your data.<\/p>\n\n<p>More on IncrementalDataSet in this video: <a href=\"https:\/\/www.youtube.com\/watch?v=v7JSSiYgqpg\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=v7JSSiYgqpg<\/a><\/p>\n\n<pre><code># nodes.py\n\nfrom typing import Any, Dict, Callable\n\ndef _dict_mapper(dictionary: Dict[str, Any], fun: Callable):\n  # Apply your function to the dictionary mapping\n  return {k: fun(v) for k, v in dictionary.items()}\n\ndef node_0(list_of_strings: Dict[str, str]):\n  return _dict_mapper(list_of_strings, lambda x: int(x))\n\ndef node_1(list_of_numbers: Dict[str, int]):\n  return _dict_mapper(list_of_numbers, lambda x: x+1)\n\ndef node_2(list_of_numbers: Dict[str, int]):\n  return _dict_mapper(list_of_numbers, lambda x: str(x))\n\ndef node_3(list_of_strings: Dict[str, str]):\n  return _dict_mapper(list_of_strings, lambda x: f'{x}_suffix')\n\n\n# catalog.yml\ndata:\n  type: IncrementalDataSet\n  dataset: text.TextDataSet\n  path: folder\/with\/text_files\/each\/containing\/single\/number\/\n  filename_suffix: .txt\n\n# pipeline.py\n\nPipeline([\n  node(node_0, inputs='data', outputs='0'),\n  node(node_1, inputs='0', outputs='1'),\n  node(node_2, inputs='1', outputs='2'),\n  node(node_3, inputs='2', outputs='final_output'),\n])\n\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.6,
        "Solution_reading_time":20.46,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":163.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1588516515763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"UK",
        "Answerer_reputation_count":29087.0,
        "Answerer_view_count":3080.0,
        "Challenge_adjusted_solved_time":0.0864069444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm experimenting with <a href=\"https:\/\/aws.amazon.com\/sagemaker\/\" rel=\"nofollow noreferrer\">AWS Sagemaker<\/a> using a Free Tier account. According to the <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">Sagemaker pricing<\/a>, I can use 50 hours of m4.xlarge and m5.xlarge instances for training in the free tier. (I am safely within the two-month limit.) But when I attempt to train an algorithm with the XGBoost container using m5.xlarge, I get the error shown below the code.<\/p>\n<p>Are the ml-type and non-ml-type instances the same with just a fancy prefix for those that one would use with Sagemaker or are they entirely different? The <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/\" rel=\"nofollow noreferrer\">EC2 page<\/a> doesn't even list the ml instances.<\/p>\n<pre><code>sess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(container,\n                                    role, \n                                    instance_count=1, \n                                    instance_type='m5.xlarge',\n                                    output_path=output_location,\n                                    sagemaker_session=sess)\n<\/code><\/pre>\n<blockquote>\n<p>ClientError: An error occurred (ValidationException) when calling the\nCreateTrainingJob operation: 1 validation error detected: Value\n'm5.xlarge' at 'resourceConfig.instanceType' failed to satisfy\nconstraint: Member must satisfy enum value set: [ml.p2.xlarge,\nml.m5.4xlarge, ml.m4.16xlarge, ml.p4d.24xlarge, ml.c5n.xlarge,\nml.p3.16xlarge, ml.m5.large, ml.p2.16xlarge, ml.c4.2xlarge,\nml.c5.2xlarge, ml.c4.4xlarge, ml.c5.4xlarge, ml.c5n.18xlarge,\nml.g4dn.xlarge, ml.g4dn.12xlarge, ml.c4.8xlarge, ml.g4dn.2xlarge,\nml.c5.9xlarge, ml.g4dn.4xlarge, ml.c5.xlarge, ml.g4dn.16xlarge,\nml.c4.xlarge, ml.g4dn.8xlarge, ml.c5n.2xlarge, ml.c5n.4xlarge,\nml.c5.18xlarge, ml.p3dn.24xlarge, ml.p3.2xlarge, ml.m5.xlarge,\nml.m4.10xlarge, ml.c5n.9xlarge, ml.m5.12xlarge, ml.m4.xlarge,\nml.m5.24xlarge, ml.m4.2xlarge, ml.p2.8xlarge, ml.m5.2xlarge,\nml.p3.8xlarge, ml.m4.4xlarge]<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1607865492452,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607865181387,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to use an m5.xlarge instance for training an algorithm with the XGBoost container in AWS Sagemaker using a Free Tier account. However, the user is encountering an error that indicates that the instance type is not valid. The user is questioning whether the ml-type and non-ml-type instances are the same or different, as the EC2 page does not list the ml instances.",
        "Challenge_last_edit_time":1608039265823,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65276017",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":11.9,
        "Challenge_reading_time":25.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":0.0864069444,
        "Challenge_title":"What's the difference between regular and ml AWS EC2 instances?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":7690.0,
        "Challenge_word_count":192,
        "Platform":"Stack Overflow",
        "Poster_created_time":1493109317327,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1152.0,
        "Poster_view_count":113.0,
        "Solution_body":"<p>The instances with the <code>ml<\/code> prefix are instance classes specifically for use in Sagemaker.<\/p>\n<p>In addition to being used within the Sagemaker service, the instance will be running an AMI with all the necessary libraries and packages such as Jupyter.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.39,
        "Solution_score_count":12.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":12.2413888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI am a little confused about whether S3 Shard key would work when using PIPE mode, here is a example:\n\nAssume I have:\n\n2 instance, each instance have 4 worker;\n\ndata: total 8 files with total size 8GB, each file is 1GB. Put them into 4 different S3 path, that means, each path has 2 files (2GB in total)\n\nIf I use PIPE mode, and s3_input using  distribution='ShardedByS3Key', and create 4 channel (each channel mapping a s3 path, 2 files)\n\ntrain_s3_input_1 = sagemaker.inputs.s3_input(channel_1, distribution='ShardedByS3Key')\n\nQuestion:\n\nHow much data of each worker get to train, 1 file or 2 files? thanks",
        "Challenge_closed_time":1589407880000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589363811000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is confused about whether S3 Shard key would work when using PIPE mode. They have 2 instances, each with 4 workers, and a total of 8 files with a total size of 8GB. They have put the files into 4 different S3 paths, with each path having 2 files. The user wants to know how much data each worker will get to train if they use PIPE mode and s3_input using distribution='ShardedByS3Key' with 4 channels mapping to each S3 path.",
        "Challenge_last_edit_time":1667925675584,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU31DdUqtuQziixKTkPasZKw\/confusion-about-pipe-mode-when-using-s3-shard-key",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":8.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":12.2413888889,
        "Challenge_title":"confusion about PIPE mode when using S3 shard key",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":46.0,
        "Challenge_word_count":108,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\nSageMaker will replicate a subset of data (1\/n ML compute instances) on each ML compute instance that is launched for model training when you specify *ShardedByS3Key*. If there are n ML compute instances launched for a training job, each instance gets approximately 1\/n of the number of S3 objects. This applies in both File and Pipe modes. Keep this in mind when developing algorithms.\n\nTo answer your question:\nHow much data of each worker get to train, 1 file or 2 files? 1 file each from the training channel.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925572408,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":6.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":90.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1266513014020,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2280.0,
        "Answerer_view_count":136.0,
        "Challenge_adjusted_solved_time":487.0128825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to cache part of the stream executed successfully (marked as tick) in Azure ML so that next time run will start from the same point onwards.\nAny help is appreciable.<\/p>",
        "Challenge_closed_time":1429645020147,
        "Challenge_comment_count":0,
        "Challenge_created_time":1427891773770,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking assistance in caching a portion of a successful stream in Azure ML to allow for the next run to start from the same point onwards.",
        "Challenge_last_edit_time":1488123478287,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/29391016",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":2.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":487.0128825,
        "Challenge_title":"Cache part of experiment in AzureML same as SPSS modeler?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":60.0,
        "Challenge_word_count":41,
        "Platform":"Stack Overflow",
        "Poster_created_time":1427358932900,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I believe Azure ML already does this.  When you run it the second time, if nothing upstream from that tick has changed it should just load the results from the previous run.  It may take a few seconds for Azure ML to recognize that it is cached and reload it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":3.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.4025,
        "Challenge_answer_count":0,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nWhen we open a pull request, chart-testing (lint) step in [release.yaml](https:\/\/github.com\/community-charts\/helm-charts\/blob\/main\/.github\/workflows\/release.yml#L60) file getting the following error.\r\n\r\n```\r\nError: Error linting charts: Error processing charts\r\n------------------------------------------------------------------------------------------------------------------------\r\n \u2716\ufe0e mlflow => (version: \"0.1.47\", path: \"charts\/mlflow\") > Error validating maintainer 'Burak Ince': 404 Not Found\r\n------------------------------------------------------------------------------------------------------------------------\r\n```\r\n\r\nBecause of maintainer name for the `ct lint` command must be a GitHub username rather than a real name.\n\n### What's your helm version?\n\nv3.9.0\n\n### What's your kubectl version?\n\nv1.24.2\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.1.47\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nct lint --debug --config .\/.github\/configs\/ct-lint.yaml --lint-conf .\/.github\/configs\/lintconf.yaml\n\n### Anything else we need to know?\n\n_No response_",
        "Challenge_closed_time":1656583953000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656578904000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue with MLFlow active run not being reported correctly in the MLFlow UI while using the `TrainingArguments` with `report_to=['mlflow']` and `run_name=\"run0\"`. The cause of the issue was identified as an incorrect check for the MLFlow active run in `src\/transformers\/integrations.py`. The expected behavior was for the MLFlow UI to report a run with a Run Name of `run0`.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/2",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":7.8,
        "Challenge_reading_time":18.48,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":1.4025,
        "Challenge_title":"[mlflow] Run chart-testing (lint) step returns Error validating maintainer 404 Not Found error",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":147,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6409055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi There,   <br \/>\nI am running a very simple pipeline that contains a dataset and a SQL transformation task. When i run the two tasks i get an error : 2021\/09\/07 17:49:47 Wrapper cmd failed with err: exit status 143 which i can't seem to find anywhere. I am running a compute VM DS1.   <br \/>\nany direction?  <br \/>\nThanks,  <\/p>",
        "Challenge_closed_time":1631040059280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631037752020,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error with exit code 143 while running a simple pipeline containing a dataset and a SQL transformation task on Azure Machine Learning. The user is seeking guidance to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543071\/azure-machine-learning-exit-code-143",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.9,
        "Challenge_reading_time":4.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.6409055556,
        "Challenge_title":"Azure Machine Learning Exit Code 143",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":64,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Incase anyone is wondering, you must increase the compute with more memory to avoid this...<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":1.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":15.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":788.8893794444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello experts,  <\/p>\n<p>I would like to attach a managed disk to my machine learning compute instance. Is that possible?  <\/p>\n<p>There is a possible overlap to the question <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/2ee51daa-2ec8-430f-a4ca-ec50a30d0321\/attach-disk-to-virtual-machine?forum=WAVirtualMachinesforWindows\">Attach Disk to Virtual Machine<\/a>, but steps doesn't seem to apply to ML compute instances.  <\/p>\n<p>Thanks in advance,  <\/p>",
        "Challenge_closed_time":1604477155916,
        "Challenge_comment_count":2,
        "Challenge_created_time":1601637154150,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to attach a managed disk to a machine learning compute instance, but is unsure if the steps for attaching a disk to a virtual machine apply to ML compute instances.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/115201\/how-can-i-attach-a-managed-disk-to-a-machine-learn",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.8,
        "Challenge_reading_time":6.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":788.8893794444,
        "Challenge_title":"How can I attach a managed disk to a Machine  Learning Compute instance?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":62,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,    <\/p>\n<p>You can attach your managed disk by following steps in Azure portal:    <br \/>\n<img src=\"\/answers\/storage\/temp\/37296-image.png\" alt=\"37296-image.png\" \/>    <\/p>\n<p>More details and limitation please see:    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-target#azure-machine-learning-compute-managed\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-target#azure-machine-learning-compute-managed<\/a>    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":25.3,
        "Solution_reading_time":6.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1305851487736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Challenge_adjusted_solved_time":80.7456638889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In my workflow, I do the following:<\/p>\n<ol>\n<li>Acquire raw data (e.g. a video containing people)<\/li>\n<li>Transform it (e.g. automatically extract all crops with faces)<\/li>\n<li>Manually label them (e.g. identify the person in each crop). The labels are stored in json files along with the crops.<\/li>\n<li>Train a model on these data.<\/li>\n<\/ol>\n<p><strong>How should I track this pipeline with DVC?<\/strong><\/p>\n<p>My concerns:<\/p>\n<ol>\n<li>If stage 2 is changed (e.g. crops are extracted with a different size), the manual data should be invalidated (and so should the final model).<\/li>\n<li>The 3rd step is manual and therefore not precisely reproducible. But I do need its input to be reproducible.<\/li>\n<li>Stage 4 has an element of randomness, so it's not precisely reproducible either.<\/li>\n<\/ol>",
        "Challenge_closed_time":1655523571800,
        "Challenge_comment_count":2,
        "Challenge_created_time":1655411665790,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to track a pipeline with DVC that involves acquiring raw data, transforming it, manually labeling it, and training a model. The user is concerned about how to handle changes in the pipeline, the manual labeling step, and the randomness in the final stage.",
        "Challenge_last_edit_time":1655415427550,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72651603",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.1,
        "Challenge_reading_time":10.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":31.0850027778,
        "Challenge_title":"Adding files that rely on pipeline outputs",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":128,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>Stage 3 is manual so you can't really codify it or automate it, nor guarantee its reproducibility (due to possible human error). But there's a way to get you as close as possible:<\/p>\n<p>You could replace it with a helper script that just checks whether all the labels are annotated. If so, output a text file with content &quot;green&quot;, otherwise &quot;red&quot; (for example) and error out.<\/p>\n<p>Stage 4 should depend on both the inputs from stages 2 and 3, so it will only run if BOTH the face crops changed AND if they are thoroughly annotated.\nInternally, it first checks the semaphore file (from 3) and dies on red. On green, it trains the model :)<\/p>\n<p>The <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">DAG<\/a> looks like this:<\/p>\n<pre><code>          +-----------+       \n          | 1-acquire |       \n          +-----------+       \n                *          \n                *          \n                *          \n          +---------+       \n          | 2-xform |       \n          +---------+       \n you      **        **     \n   --&gt;  **            **   \n       *                ** \n+---------+               *\n| 3-check |             ** \n+---------+           **   \n          **        **     \n            **    **       \n              *  *         \n          +---------+      \n          | 4-train |      \n          +---------+      \n<\/code><\/pre>\n<blockquote>\n<p>re randomness: while not ideal, non-determinism technically only <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#avoiding-unexpected-behavior\" rel=\"nofollow noreferrer\">affects intermediate stages<\/a> of the pipeline, because it causes everything after that to always run. In this case, since it's in the last stage, it won't affect DVC's job.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1655706111940,
        "Solution_link_count":2.0,
        "Solution_readability":12.0,
        "Solution_reading_time":17.8,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":174.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":473.2511111111,
        "Challenge_answer_count":0,
        "Challenge_body":"One can either define a DVC option with default values in the init, which could be considered a constant, or change a DVC option that has no default values in the call method.\r\n\r\nIf a pre-intialized DVC option is being changed within the call that can lead to issues and should either raise an exception or at least log that it can lead to not supported problems",
        "Challenge_closed_time":1634716886000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633013182000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue where dvc is attempting to launch an updater using an asv script, resulting in an error message indicating an unknown mode. The user suggests setting the CI or DVC_TEST environment variable as a workaround to prevent dvc from launching the updater.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/76",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":10.7,
        "Challenge_reading_time":5.05,
        "Challenge_repo_contributor_count":3.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":459.0,
        "Challenge_repo_star_count":32.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":473.2511111111,
        "Challenge_title":"raise Error if pre-initialized DVC option is being changed",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":75,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6132516667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,    <\/p>\n<p>I tried to use the pipe operation %&gt;% of R in an azure notebook without success ...    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/16866-image.png?platform=QnA\" alt=\"16866-image.png\" \/>    <\/p>\n<p>is possible to use it or it is a limitation in azure notebooks ?<\/p>",
        "Challenge_closed_time":1597097474163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597095266457,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges while using the pipe operation %>% of R in an azure notebook and is seeking clarification on whether it is possible to use it or if it is a limitation in azure notebooks.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/63863\/pipe-gt-for-r-is-not-working-in-azure-notebooks",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":4.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.6132516667,
        "Challenge_title":"Pipe %&gt;% for R is not working in azure notebooks",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":46,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,  <\/p>\n<p>Fixed.  <\/p>\n<p>Azure Notebook release the session after some time of inactivity, therefore the dplyr package wasn\u00b4t loaded in the session<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":1.97,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6197222222,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to create simple templates for scientists so that they can fit their models easily into a continuous integration\/continuous delivery (CI\/CD) pipeline. I want to know about success stories of AWS customers performing CI\/CD on machine learning pipelines. ",
        "Challenge_closed_time":1592579742000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592577511000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking success stories of AWS customers who have implemented CI\/CD on machine learning pipelines to create templates for scientists to easily fit their models into a CI\/CD pipeline.",
        "Challenge_last_edit_time":1667926686451,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUwLq6HNRZSOK7ODKKc_lC3Q\/can-you-share-success-stories-of-aws-customers-performing-ml-ci-cd",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.6197222222,
        "Challenge_title":"Can you share success stories of AWS customers performing ML CI\/CD?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":49,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Amazon has released the [Amazon SageMaker Pipelines][1] that are the first purpose-built CI\/CD service for machine learning: \n[1]: https:\/\/aws.amazon.com\/sagemaker\/pipelines\/\n\nFor more information, see [New \u2013 Amazon SageMaker Pipelines brings DevOps capabilities to your machine learning projects] [2]\n[2]: https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-pipelines-brings-devops-to-machine-learning-projects\/\n\nAdditionally, we have a case-study where a customer created one on their own for model development using Airflow.  For more information, see [NerdWallet uses machine learning on AWS to power recommendations platform][3] and \n[Using Amazon SageMaker to build a machine learning platform with just three engineers][4].\n[3]: https:\/\/aws.amazon.com\/solutions\/case-studies\/nerdwallet-case-study\/\n[4]: https:\/\/www.nerdwallet.com\/blog\/engineering\/machine-learning-platform-amazon-sagemaker\/\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925550032,
        "Solution_link_count":4.0,
        "Solution_readability":20.8,
        "Solution_reading_time":11.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":519.1605555556,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi,\r\n\r\nI have installed the azure ml using below environment yml, installation happened without any issues but when I import the azureml.core I am getting exception.\r\n\r\n**conda environment yml**\r\n```\r\nname: ati_reranking_automl_py36\r\ndependencies:\r\n  # The python interpreter version.\r\n  # Currently Azure ML only supports 3.5.2 and later.\r\n- pip==20.2.4\r\n- python==3.6.13\r\n- nb_conda\r\n- matplotlib==2.1.0\r\n- numpy==1.18.5\r\n- seaborn==0.9.0\r\n- urllib3<1.24\r\n- scipy>=1.4.1,<=1.5.2\r\n- scikit-learn==0.22.1\r\n- pandas==0.25.1\r\n- py-xgboost<=1.3.3\r\n- jupyterlab==1.0.2\r\n- ipykernel==5.3.4\r\n- pytorch::pytorch=1.4.0\r\n\r\n- pip:\r\n  # Base AzureML SDK\r\n  - azureml-sdk\r\n      \r\n  - pytorch-transformers==1.0.0\r\n\r\n  # Scoring deps\r\n  - inference-schema[numpy-support]\r\n```\r\n\r\n\r\n**Exception**\r\nimport azureml.core\r\n`Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (cryptography 2.3.1 (c:\\miniconda\\envs\\ati_reranking_automl_py36\\lib\\site-packages), Requirement.parse('cryptography<4.0.0,>=3.3.1; extra == \"crypto\"'), {'PyJWT'}).`\r\n\r\nAzure ML SDK Version:  1.31.0\r\n\r\n\r\nPlease help.\r\nThanks",
        "Challenge_closed_time":1626388114000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1624519136000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while running a pipeline example in AzureML, which is yielding the warning \"Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\" The user is also getting the same warning in other pipelines and is unable to identify the cause of the issue. The user has provided a slightly reduced MWE for clarity.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1523",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.6,
        "Challenge_reading_time":14.93,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":519.1605555556,
        "Challenge_title":"Warning while loading the azureml.core",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":101,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Can you try installing [azureml-core](https:\/\/pypi.org\/project\/azureml-core\/) instead? This is unfortunately a package conflict issue. I was able to create the conda environment by removing all the version pinning on `matplotlib`, `numpy`, ... all the way to `pytorch`, and changed `azureml-sdk` to `azureml-core`.  \r\n\r\n```yml\r\nname: ati_reranking_automl_py36\r\ndependencies:\r\n  # The python interpreter version.\r\n  # Currently Azure ML only supports 3.5.2 and later.\r\n- pip==20.2.4\r\n- python==3.6.13\r\n- nb_conda\r\n- matplotlib\r\n- numpy\r\n- seaborn\r\n- urllib3\r\n- scipy\r\n- scikit-learn\r\n- pandas\r\n- py-xgboost\r\n- jupyterlab\r\n- ipykernel\r\n- pytorch\r\n\r\n- pip:\r\n  # Base AzureML SDK\r\n  - azureml-core\r\n\r\n  - pytorch-transformers==1.0.0\r\n\r\n  # Scoring deps\r\n  - inference-schema[numpy-support]                               \r\n```\r\n\r\n```bash\r\n(base) \u279c  jiazho_playground \u2717 conda activate ati_reranking_automl_py36\r\n(ati_reranking_automl_py36) \u279c  jiazho_playground git:(split-merge) \u2717 python\r\nPython 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59)\r\n[GCC 7.5.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import azureml.core\r\n>>>\r\n\r\n\r\n(ati_reranking_automl_py36) \u279c  jiazho_playground \u2717 pip freeze | grep azureml.core\r\nazureml-core==1.32.0\r\n```\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.4,
        "Solution_reading_time":15.12,
        "Solution_score_count":null,
        "Solution_sentence_count":13.0,
        "Solution_word_count":120.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1526004205792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"China",
        "Answerer_reputation_count":28087.0,
        "Answerer_view_count":3298.0,
        "Challenge_adjusted_solved_time":109.1988627778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have azure ml , I created compute for learning.\nCost for instance is 2-5usd with my use. But cost for p10(premium SSD) Disk 17usd.<\/p>\n<p>I don't know how change it because its not appear in azure Disk and in ML studio i cant find option for manage storage type for compute.<\/p>\n<p>Some one know how change it ?<\/p>",
        "Challenge_closed_time":1617778657403,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617385541497,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing difficulty in changing the disk type for Azure ML as the option to manage storage type for compute is not available in ML studio. The cost for the premium SSD disk is significantly higher than the regular disk, and the user is seeking assistance in changing it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66923216",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":2.6,
        "Challenge_reading_time":4.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":109.1988627778,
        "Challenge_title":"Change Disk Type Azure ML",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":150.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1561015783552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":97.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>There is no possible way to change the compute disk type if you use the Azure ML compute cluster and compute instance. Only when you use the extra computer, you can manage the separate resources such as the disk, network, and so on. For example, you attach a VM as the target computer to the Azure ML. Then when you create the VM you can set the disk type with HDD.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":4.44,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":71.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.1780152778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am building an Azure ML pipeline with the azureml Python SDK. The pipeline calls a PythonScriptStep which stores data on the workspaceblobstore of the AML workspace. <\/p>\n\n<p>I would like to extend the pipeline to export the pipeline data to an Azure Data Lake (Gen 1). Connecting the output of the PythonScriptStep directly to Azure Data Lake (Gen 1) is not supported by Azure ML as far as I understand. Therefore, I added an extra DataTransferStep to the pipeline, which takes the output from the PythonScriptStep as input directly into the DataTransferStep. According to the Microsoft documentation this should be possible.<\/p>\n\n<p>So far I have built this solution, only this results in a file of 0 bytes on the Gen 1 Data Lake. I think the output_export_blob PipelineData does not correctly references the test.csv, and therefore the DataTransferStep cannot find the input. How can I connect the DataTransferStep correctly with the PipelineData output from the PythonScriptStep?<\/p>\n\n<p>Example I followed:\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-with-data-dependency-steps.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-with-data-dependency-steps.ipynb<\/a><\/p>\n\n<p>pipeline.py<\/p>\n\n<pre><code>input_dataset = delimited_dataset(\n    datastore=prdadls_datastore,\n    folderpath=FOLDER_PATH_INPUT,\n    filepath=INPUT_PATH\n)\n\noutput_export_blob = PipelineData(\n    'export_blob',\n    datastore=workspaceblobstore_datastore,\n)\n\ntest_step = PythonScriptStep(\n    script_name=\"test_upload_stackoverflow.py\",\n    arguments=[\n        \"--output_extract\", output_export_blob,\n    ],\n    inputs=[\n        input_dataset.as_named_input('input'),\n    ],\n    outputs=[output_export_blob],\n    compute_target=aml_compute,\n    source_directory=\".\"\n)\n\noutput_export_adls = DataReference(\n    datastore=prdadls_datastore, \n    path_on_datastore=os.path.join(FOLDER_PATH_OUTPUT, 'test.csv'),\n    data_reference_name='export_adls'        \n)\n\nexport_to_adls = DataTransferStep(\n    name='export_output_to_adls',\n    source_data_reference=output_export_blob,\n    source_reference_type='file',\n    destination_data_reference=output_export_adls,\n    compute_target=adf_compute\n)\n\npipeline = Pipeline(\n    workspace=aml_workspace, \n    steps=[\n        test_step, \n        export_to_adls\n    ]\n)\n<\/code><\/pre>\n\n<p>test_upload_stackoverflow.py<\/p>\n\n<pre><code>import os\nimport pathlib\nfrom azureml.core import Datastore, Run\n\nparser = argparse.ArgumentParser(\"train\")\nparser.add_argument(\"--output_extract\", type=str)\nargs = parser.parse_args() \n\nrun = Run.get_context()\ndf_data_all = (\n    run\n    .input_datasets[\"input\"]\n    .to_pandas_dataframe()\n)\n\nos.makedirs(args.output_extract, exist_ok=True)\ndf_data_all.to_csv(\n    os.path.join(args.output_extract, \"test.csv\"), \n    index=False\n)\n<\/code><\/pre>",
        "Challenge_closed_time":1591813300647,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591811660073,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is building an Azure ML pipeline with the azureml Python SDK and wants to export pipeline data to an Azure Data Lake (Gen 1). They added a DataTransferStep to the pipeline, but it results in a file of 0 bytes on the Gen 1 Data Lake. The user suspects that the output_export_blob PipelineData does not correctly reference the test.csv, and therefore the DataTransferStep cannot find the input. They are seeking guidance on how to connect the DataTransferStep correctly with the PipelineData output from the PythonScriptStep.",
        "Challenge_last_edit_time":1591812659792,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62310010",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.5,
        "Challenge_reading_time":38.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":0.455715,
        "Challenge_title":"Azure ML PipelineData with DataTransferStep results in 0 bytes file",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":917.0,
        "Challenge_word_count":243,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509387489888,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":73.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>The code example is immensely helpful. Thanks for that. You're right that it can be confusing to get <code>PythonScriptStep -&gt; PipelineData<\/code>. Working initially even without the <code>DataTransferStep<\/code>.<\/p>\n\n<p>I don't know 100% what's going on, but I thought I'd spitball some ideas:<\/p>\n\n<ol>\n<li>Does your <code>PipelineData<\/code>,  <code>export_blob<\/code>, contain the \"test.csv\" file? I would verify that before troubleshooting the <code>DataTransferStep<\/code>. You can verify this using the SDK, or more easily with the UI.\n\n<ol>\n<li>Go to the PipelineRun page, click on the <code>PythonScriptStep<\/code> in question.<\/li>\n<li>On \"Outputs + Logs\" page, there's a \"Data Outputs\" Section (that is slow to load initially)<\/li>\n<li>Open it and you'll see the output PipelineDatas then click on \"View Output\"<\/li>\n<li>Navigate to given path either in the Azure Portal or Azure Storage Explorer.\n<a href=\"https:\/\/i.stack.imgur.com\/9LaEq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9LaEq.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/XbnhC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XbnhC.png\" alt=\"enter image description here\"><\/a><\/li>\n<\/ol><\/li>\n<li>In <code>test_upload_stackoverflow.py<\/code> you are treating the <code>PipelineData<\/code> as a directory when call <code>.to_csv()<\/code> as opposed to a file which would be you just calling <code>df_data_all.to_csv(args.output_extract, index=False)<\/code>. Perhaps try defining the <code>PipelineData<\/code> with <code>is_directory=True<\/code>. Not sure if this is required though.<\/li>\n<\/ol>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.0,
        "Solution_reading_time":21.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":184.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":77.8924658334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I deployed my Training pipeline and my Real-time inference pipeline.  <br \/>\nWith the REST-Api of my training pipeline I'm able to retrain my ML model. Is it possible to use that retrained model automated in my real inference pipeline?  <br \/>\nWhen i trigger the pipeline in ML studio I have to update my real inference pipeline manually. Since I want to trigger my retraining external that is not possible.  <br \/>\nThanks in advance.<\/p>",
        "Challenge_closed_time":1615579149187,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615298736310,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has deployed a training pipeline and a real-time inference pipeline. They are able to retrain their ML model using the REST-API of the training pipeline, but are unable to automate the use of the retrained model in the real inference pipeline. They currently have to update the real inference pipeline manually and are seeking a solution to trigger the retraining externally.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/305899\/update-real-interference-pipeline",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.5,
        "Challenge_reading_time":5.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":77.8924658334,
        "Challenge_title":"update real interference pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":76,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, here's a reference on which <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-ml-pipelines#which-azure-pipeline-technology-should-i-use\">technology<\/a> to use based on a given scenario. For your scenario, you should be able to create an Azure Machine Learning pipeline using the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-trigger-published-pipeline\">SDK to trigger a pipeline<\/a> based on a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-trigger-published-pipeline#create-a-schedule\">time\/change based schedule<\/a> and then <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-update-web-service\">update the web service<\/a> accordingly. Depending on the complexity of your triggers or data prep needs, you can leverage other technologies such as <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-trigger-published-pipeline#use-azure-logic-apps-for-complex-triggers\">Logic Apps<\/a> or <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-trigger-published-pipeline#call-machine-learning-pipelines-from-azure-data-factory-pipelines\">Azure Data Factory<\/a> to trigger your Azure Machine Learning pipeline. Currently, you can only use the Azure Machine Learning SDK to automatically update the web service. Hope this helps.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":17.6,
        "Solution_reading_time":18.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":105.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":110.2481841667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I\u2019m having an issue importing the data into my Machine Learning Studio. It shows me a Red Cross with the error 0030 - which means that there\u2019s an issue in downloading the data. For background, I\u2019m importing data from the Web URL via HTTP option. I looked up the issue on the troubleshooting page, followed the advice, which shows I\u2019ve done everything correctly. My data link works perfectly fine in my browser. When I enter the http link into my browser, it immediately downloads the csv file. However, my studio is not downloading the data. Importing the data is the first step in my experiment, and I can\u2019t move forward without it. Immediate help would be greatly appreciated! I\u2019ve attached pictures for reference. [1]: \/api\/attachments\/72499-0ebb78a4-4805-46e8-a7f1-fbf99682af5f.png?platform=QnA <\/p>",
        "Challenge_closed_time":1614759574360,
        "Challenge_comment_count":2,
        "Challenge_created_time":1614362680897,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while importing data into Azure ML Studio experiment. The error 0030 is displayed, indicating a problem in downloading the data. The user is importing data from a Web URL via HTTP option, and the data link works fine in the browser but not in the studio. This is the first step in the experiment, and the user needs immediate help to move forward.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/291213\/importing-data-in-azure-ml-studio-experiment",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":10.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":110.2481841667,
        "Challenge_title":"Importing Data in Azure ML Studio Experiment",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":131,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,  <\/p>\n<p>This exception in Azure Machine Learning occurs when it is not possible to download a file. You will receive this exception when an attempted read from an HTTP source has failed after three (3) retry attempts.  <\/p>\n<p>Resolution: Verify that the URI to the HTTP source is correct and that the site is currently accessible via the Internet.  <\/p>\n<p>Is this file on any place need authentication?   <\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":5.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":59.1833333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Hey,\n\ni'm trying to understand the MLOps Pipeline with the CI\/CD-Automation (Stage 2 Maturity Level) and struggle with the Feature Store as the component feeding the Automated Pipeline with data. What i found out in the internet was, that Feature Stores extract data from different sources, transform them and create training data which can be used to train the model (retraining with new data). But in the pipeline the steps like Data preperation and Data extraction come after the Feature Store.\n\nCan somebody explain to me, whats the output of the Feature Store and how it is used to serve the data for the Automated Pipeline and the Prediction Service?\n\nThanks in advance",
        "Challenge_closed_time":1678111680000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677898620000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is struggling to understand how the Feature Store component works in the MLOps Pipeline with CI\/CD-Automation. They have researched that Feature Stores extract and transform data to create training data for model retraining, but are confused about how it fits into the pipeline as data preparation and extraction come after the Feature Store. The user is seeking an explanation of the output of the Feature Store and how it serves data for the Automated Pipeline and Prediction Service.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Feature-Store-MLOps-Pipeline\/m-p\/528764#M1375",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":8.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":59.1833333333,
        "Challenge_title":"Feature Store MLOps Pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":104.0,
        "Challenge_word_count":116,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The Feature Store is just a centralized repository of features. By that, its output is just a set of features typically used to train an ML model. Depending on your specific needs, you can serve the ingested data in the Feature Store to the model right away (in what is called feature serving) or export feature values and do further preparation of data.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":4.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":68.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4421.2883333333,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nOccasionally after starting a Sagemaker workspace, clicking 'Connect' gives an error in the bottom right-hand corner of the screen:\r\n\r\n> We have a problem!\r\n> null is not an object (evaluating 'l.location=s') \r\n\r\nin a little red box on the bottom-right of the screen. The notebook window is not opened after clicking on 'Connect'.\r\n\r\n**To Reproduce**\r\nThe error is intermittent. I *think* it may happen after the SW window has been open a while, because I noticed that the SW window automatically logged me out shortly after seeing this error.\r\n\r\n1. Click 'Start' for Sagemaker workspace and wait for the status to change to 'Available'. \r\n2. Click 'Connections', then 'Connect'\r\n3. See error\r\n\r\nWhen I logged out and back into Service Workbench, and was able to connect to the workspace successfully. \r\n\r\n**Expected behavior**\r\nA new window should open with a Jupyter\/Sagemaker notebook in a new window. \r\n\r\n**Versions (please complete the following information):**\r\n - 3.2.0\r\n",
        "Challenge_closed_time":1643923114000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1628006476000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"Users are experiencing an intermittent issue when connecting to Sagemaker workspaces where a blank browser launches instead of Sagemaker. This issue occurs for both newly created workspaces and workspaces that were already created but were stopped and restarted. The issue is experienced approximately once a week, and sometimes clearing the cache solves the issue, while other times it does not. Users receive a \"Something Went Wrong\" general error in SWB at Step 6.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.5,
        "Challenge_reading_time":12.78,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":4421.2883333333,
        "Challenge_title":"\"null is not an object\" while trying to connect to Sagemaker notebook.",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":164,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @tom-christie, we believe the issue mentioned is due to access token getting expired. Please feel free to use the latest version with the fix ([v3.3.1](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v3.3.1)). We're seeing this issue on 4.1.1 as well. However, it appears to be persistent (i.e. it happens every time we connect to a SageMaker workspace). So far, we've only tested a single workspace config, but the error consistently shows up when we try to connect to different workspace instances using the same config. The workspace instances are new and running, at least as shown in the SWB UI. We haven't verified if the instances are available in the SageMaker console, however. Is it possible this is related to a popup blocker as reported in GALI-1224? It creates a similar error message.\r\nhttps:\/\/sim.amazon.com\/issues\/CHAMDOC-17 Yeah, I've seen the error happen because popups are disabled for the SWB domain. Once you enable popup for the SWB domain, it should allow you to connect to Sagemaker. Feel free to reopen this ticket if enabling popups didn't resolve your issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.0,
        "Solution_reading_time":13.74,
        "Solution_score_count":null,
        "Solution_sentence_count":14.0,
        "Solution_word_count":171.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1465222092252,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Z\u00fcrich, Switzerland",
        "Answerer_reputation_count":1414.0,
        "Answerer_view_count":478.0,
        "Challenge_adjusted_solved_time":649.8259375,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Kubeflow pipelines (KFP) with GCP Vertex AI pipelines. I am using <code>kfp==1.8.5<\/code> (kfp SDK) and <code>google-cloud-pipeline-components==0.1.7<\/code>. Not sure if I can find which version of Kubeflow is used on GCP.<\/p>\n<p>I am bulding a component (yaml) using python inspired form this <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3748#issuecomment-627698554\" rel=\"nofollow noreferrer\">Github issue<\/a>. I am defining an output like:<\/p>\n<pre><code>outputs=[(OutputSpec(name='drt_model', type='Model'))]\n<\/code><\/pre>\n<p>This will be a base output directory to store few artifacts on Cloud Storage like model checkpoints and model.<\/p>\n<p>I would to keep one base output directory but add sub directories depending of the artifact:<\/p>\n<ul>\n<li>&lt;output_dir_base&gt;\/model<\/li>\n<li>&lt;output_dir_base&gt;\/checkpoints<\/li>\n<li>&lt;output_dir_base&gt;\/tensorboard<\/li>\n<\/ul>\n<p>but I didn't find how to concatenate the <strong>OutputPathPlaceholder('drt_model')<\/strong> with a string like <strong>'\/model'<\/strong>.<\/p>\n<p>How can append extra folder structure like \/model or \/tensorboard to the OutputPathPlaceholder that KFP will set during run time ?<\/p>",
        "Challenge_closed_time":1637263566352,
        "Challenge_comment_count":2,
        "Challenge_created_time":1634924192977,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using Kubeflow pipelines with GCP Vertex AI pipelines and is trying to concatenate the OutputPathPlaceholder with a string to add subdirectories for storing artifacts on Cloud Storage. They are building a component using python and defining an output directory to store model checkpoints and models. However, they are unable to add subdirectories like \/model or \/tensorboard to the OutputPathPlaceholder. The user is seeking help to append extra folder structure to the OutputPathPlaceholder set by KFP during runtime.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69681031",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.4,
        "Challenge_reading_time":16.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":649.8259375,
        "Challenge_title":"how to concatenate the OutputPathPlaceholder with a string with Kubeflow pipelines?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":312.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1465222092252,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Z\u00fcrich, Switzerland",
        "Poster_reputation_count":1414.0,
        "Poster_view_count":478.0,
        "Solution_body":"<p>I didn't realized in the first place that <code>ConcatPlaceholder<\/code> accept both Artifact and string. This is exactly what I wanted to achieve:<\/p>\n<pre><code>ConcatPlaceholder([OutputPathPlaceholder('drt_model'), '\/model'])\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.2,
        "Solution_reading_time":3.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1426093220648,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, India",
        "Answerer_reputation_count":1861.0,
        "Answerer_view_count":294.0,
        "Challenge_adjusted_solved_time":11.1307344444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any way to access the kedro pipeline environment name? Actually below is my problem.<\/p>\n<p>I am loading the config paths as below<\/p>\n<pre><code>conf_paths = [&quot;conf\/base&quot;, &quot;conf\/local&quot;]  \nconf_loader = ConfigLoader(conf_paths)\nparameters = conf_loader.get(&quot;parameters*&quot;, &quot;parameters*\/**&quot;)\ncatalog = conf_loader.get(&quot;catalog*&quot;)\n\n<\/code><\/pre>\n<p>But  I have few environments like  <code>&quot;conf\/server&quot; <\/code>, <code>&quot;conf\/test&quot;<\/code> etc, So if I have env name available I can add it to conf_paths as <code>&quot;conf\/&lt;env_name&gt;&quot;<\/code>  so that kedro will read the files from the respective env folder.\nBut now if the env path is not added to conf_paths, the files are not being read by kedro even if i specify the env name while I  run kedro like    <code>kedro run --env=server <\/code>\nI searched for all the docs but was not able to find any solution.<\/p>\n<p>EDIT:\nElaborating more on the problem.\nI am using the above-given parameters and catalog dicts in the nodes. I only have keys that are common for all runs in <code>conf\/base\/parameters.yml<\/code> and the environment specific keys in <code>conf\/server\/parameters.yml<\/code> but when i do <code>kedro run --env=server<\/code> I am getting <code>keyerror<\/code> which means the keys in <code>conf\/server\/parameters.yml<\/code> is not available in the parameters dict. If I add  <code>conf\/server<\/code> to config_paths kedro is running well without keyerror.<\/p>",
        "Challenge_closed_time":1639600021140,
        "Challenge_comment_count":8,
        "Challenge_created_time":1639518159733,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a problem in accessing the kedro pipeline environment name. They are trying to load config paths for different environments but are unable to do so without adding the environment path to conf_paths. Even after specifying the environment name while running kedro, the files are not being read. The user is getting a KeyError as the keys in the environment-specific parameters.yml file are not available in the parameters dict.",
        "Challenge_last_edit_time":1639559950496,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70355869",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":11.3,
        "Challenge_reading_time":19.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":22.7392797222,
        "Challenge_title":"How to access environment name in kedro pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":712.0,
        "Challenge_word_count":201,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495105930728,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>You don't need to define config paths, config loader etc unless you are trying to override something.<\/p>\n<p>If you are using kedro 0.17.x, the hooks.py will look something like this.<\/p>\n<p>Kedro will pass, base, local and the env you specified during runtime in <code>conf_paths<\/code> into <code>ConfigLoader<\/code>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class ProjectHooks:\n    @hook_impl\n    def register_config_loader(\n        self, conf_paths: Iterable[str], env: str, extra_params: Dict[str, Any]\n    ) -&gt; ConfigLoader:\n        return ConfigLoader(conf_paths)\n\n    @hook_impl\n    def register_catalog(\n        self,\n        catalog: Optional[Dict[str, Dict[str, Any]]],\n        credentials: Dict[str, Dict[str, Any]],\n        load_versions: Dict[str, str],\n        save_version: str,\n        journal: Journal,\n    ) -&gt; DataCatalog:\n        return DataCatalog.from_config(\n            catalog, credentials, load_versions, save_version, journal\n        )\n<\/code><\/pre>\n<p>In question, I can see you have defined <code>conf_paths<\/code> and <code>conf_loader<\/code> and the env path is not present. So kedro will ignore the env passed during runtime.<\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.7,
        "Solution_reading_time":13.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":121.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":34.5506816667,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hello,<\/p>\n<p>We use Pytorch Lightning for training and we use Kubeflow Pipelines and are thinking about using wandb to track and visualize the training and test metrics.<\/p>\n<p>Kubeflow pipelines offers the possibility to view a static html page (see <a href=\"https:\/\/www.kubeflow.org\/docs\/components\/pipelines\/sdk\/output-viewer\/#single-html-file\" rel=\"noopener nofollow ugc\">this link<\/a> ).<br>\nI was wondering if it would be possible via the wandb python sdk to get a read-only embeded code (iframe) that I could then simply pass to Kubeflow pipeline sdk to show the html ?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1643930787864,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643806405410,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking a way to visualize HTML run in Kubeflow Pipeline using wandb to track and visualize training and test metrics. They are wondering if it is possible to get a read-only embedded code (iframe) via the wandb python sdk to pass to Kubeflow pipeline sdk to show the HTML.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-visualize-html-run-in-kubeflow-pipeline\/1862",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":11.4,
        "Challenge_reading_time":8.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":34.5506816667,
        "Challenge_title":"How to visualize HTML run in Kubeflow Pipeline?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":890.0,
        "Challenge_word_count":89,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Ok I found the solution.<br>\nKubeflow Pipelines also support markdown visualization therefore instead of using kubeflow HTML output I used markdown and since markdown supports html inline I was able to directly use the wandb run html.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/21d1b9f84b7948659b75b981b04f21235e528615.png\" data-download-href=\"\/uploads\/short-url\/4Pb5MStVV77kGP5bCR1nBTvaK7H.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/21d1b9f84b7948659b75b981b04f21235e528615_2_690x333.png\" alt=\"image\" data-base62-sha1=\"4Pb5MStVV77kGP5bCR1nBTvaK7H\" width=\"690\" height=\"333\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/21d1b9f84b7948659b75b981b04f21235e528615_2_690x333.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/21d1b9f84b7948659b75b981b04f21235e528615_2_1035x499.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/21d1b9f84b7948659b75b981b04f21235e528615_2_1380x666.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/21d1b9f84b7948659b75b981b04f21235e528615_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1678\u00d7812 57.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Here is the code if someone is interested :<\/p>\n<pre><code class=\"lang-python\">import kfp\nfrom kfp.v2.dsl import component, Output, Markdown, pipeline\n\n@component(packages_to_install=['wandb'])\ndef wandb_visualization(markdown_artifact: Output[Markdown]):\n    import wandb\n    wandb.login(key=\"you_key\")\n\n    run = wandb.init(project=\"your-project\", entity=\"your-entity\")\n\n    wandb.log({\"train\/loss\" : 5.0})\n    wandb.log({\"train\/loss\" : 4.0})\n    wandb.log({\"train\/loss\" : 3.0})\n    wandb.log({\"train\/loss\" : 2.0})\n    wandb.log({\"train\/loss\" : 1.0})\n\n    wandb.finish()\n    with open(markdown_artifact.path, 'w') as f:\n        f.write(f\"&lt;iframe src=\\\"{run.get_url()}\\\" width=\\\"100%\\\" height=\\\"700\\\"\/&gt;\")\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":23.3,
        "Solution_reading_time":32.05,
        "Solution_score_count":null,
        "Solution_sentence_count":14.0,
        "Solution_word_count":126.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1466260908296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":71.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":149.6007655556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In the Azure Recommendation API sample there is a snippet like this:<\/p>\n\n<pre><code>     if (itemSets.RecommendedItemSetInfo != null)\n        {\n            ...\n        }\n        else\n        {\n            Console.WriteLine(\"No recommendations found.\");\n        }\n<\/code><\/pre>\n\n<p>So I assume that nullable recommended set means no recommendations. But what is the case with this set being not nullable but still empty ( as I am having it running the example)?<\/p>\n\n<p>I provided my own usages and catalog files. I have not too many entries there however for i2i recommendations I have results and for u2i there is an empty set.\nAllowColdItemPlacement doesn't change a think here.<\/p>",
        "Challenge_closed_time":1478186851716,
        "Challenge_comment_count":0,
        "Challenge_created_time":1477648288960,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the difference between null and empty results in the Azure Recommendation API sample. They have provided their own usage and catalog files and are experiencing an empty set for u2i recommendations, despite the set being not nullable. They are unsure if an empty set means no recommendations.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40302499",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":8.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":149.6007655556,
        "Challenge_title":"Recommendation API: what is the difference between null results and empty results",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":130.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1354118434116,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Wroc\u0142aw, Poland",
        "Poster_reputation_count":393.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>We did not mean to convey a difference in meaning between null recommendations and empty recommendations. I will check why we may be sending two different types of results. Either way, don't treat those two cases as different cases. <\/p>\n\n<p>If you are not getting results for user-to-item recommendations, most likely there was no data for that user when the build was created or the items that the user interacted with do not have enough co-occurrences with other items in the usage.<\/p>\n\n<p>What to do when you get empty recommendations is up to you, you may decide to not show any recommendations, or back-fill with popular items you may want to promote.<\/p>\n\n<p>Thanks!<\/p>\n\n<p>Luis Cabrera\nProgram Manager - Recommendations API.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":9.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":119.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":0.5489747222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I haven't used sagemaker for a while and today I started a training job (with the same old settings I always used before), but this time I noticed that a processing job has been automatically created and it's running while my training job run (I presume for debugging purpose).\nI'm sure that this is the first time that it happens.. Is that a new feature introduced by sagemaker? I didn't find any related in documentation, but it's important to know because I don't want extra costs..<\/p>\n<p>This is the image used by the processing job, with a instance type of <code>ml.m5.2xlarge<\/code> which I didn't set anywhere..<\/p>\n<blockquote>\n<p>929884845733.dkr.ecr.eu-west-1.amazonaws.com\/sagemaker-debugger-rules:latest<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1611573127772,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611569494177,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user started a training job on AWS Sagemaker and noticed that a processing job was automatically created and running alongside the training job, which they presume is for debugging purposes. The user is concerned about incurring extra costs and is unsure if this is a new feature introduced by Sagemaker as they did not find any related documentation. The processing job is using an image with an instance type of ml.m5.2xlarge, which the user did not set anywhere.",
        "Challenge_last_edit_time":1611571151463,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65882686",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.0093319444,
        "Challenge_title":"AWS Sagemaker processing Job automatically created?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":301.0,
        "Challenge_word_count":114,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416346350292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation_count":2302.0,
        "Poster_view_count":227.0,
        "Solution_body":"<p>I can answer my question.. it seems to be a new feature as highlighted <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/use-debugger-built-in-rules.html\" rel=\"nofollow noreferrer\">here<\/a>. You can turn it off as suggested in the doc:<\/p>\n<pre><code>To disable both monitoring and profiling, include the disable_profiler parameter to your estimator and set it to True. \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.6,
        "Solution_reading_time":5.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":47.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":30.0475236111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was wondering if it was possible to delete particular runs using the Python SDK.   <br \/>\nthis would be rather useful to delete old failed runs.  <br \/>\nit already has functions such as cancel(), fail(), submit(). <\/p>",
        "Challenge_closed_time":1638494197368,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638386026283,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a way to delete specific runs in AzureML using the Python SDK, which would be helpful in removing old failed runs. The SDK currently has functions like cancel(), fail(), and submit().",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/648089\/is-there-a-way-to-delete-azureml-runs-using-the-py",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":3.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":30.0475236111,
        "Challenge_title":"is there a way to delete azureml runs using the python sdk?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":48,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=2dc066be-691a-47bd-9f7a-67e426d994d9\">@Antara Das  <\/a>  Thanks, Run history documents, which may contain personal user information, are stored in the storage account in blob storage, in subfolders of \/azureml. You can download and delete the data from the portal.    <\/p>\n<p> Here is the document to delete workspace data.     <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-export-delete-data\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-export-delete-data<\/a>    <\/p>\n<p>There is a Private Preview for deleting an experiment, however such functionality does not delete the intermediate data generated for the run or any child run.    <br \/>\n\u2022 Not deleted:    <br \/>\no Files in azureml-blobstore-GUID\/azureml\/{run_id}    <br \/>\no Code snapshot (zip files)    <br \/>\no Pipeline intermediate data and child runs    <br \/>\no Metric data    <\/p>\n<p>\u2022 Deleted    <br \/>\no The output folder content    <br \/>\no Log files<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.2,
        "Solution_reading_time":12.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":116.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":1.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":76.9123686111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi. \n\nIs it possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance every time which is somewhat time consuming? I'm familiar with local mode, but I understand this is not supported when using AWS SageMaker machine learning estimators.\n\nAppreciate any suggestions for how to make the model training process in SageMaker go faster when using AWS SageMaker machine learning estimators.\n\nThanks,\nStefan",
        "Challenge_closed_time":1642148033228,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641871148701,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is looking for ways to speed up the model training process in SageMaker by using a reserved instance that is already running instead of provisioning a new instance every time. They are seeking suggestions for making the process faster when using AWS SageMaker machine learning estimators.",
        "Challenge_last_edit_time":1668610348310,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUsy3vkTMkSA2ojA1bmafDSA\/train-machine-learning-model-using-reserved-instance",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":6.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":76.9123686111,
        "Challenge_title":"Train machine learning model using reserved instance",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":354.0,
        "Challenge_word_count":84,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As of today, it's not possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance. The service team is currently working on it, unfortunately I don't have an ETA as to when the feature will be released.\n\nLocal Mode is supported for frameworks images (TensorFlow, MXNet, Chainer, PyTorch, and Scikit-Learn) and images you supply yourself.\n\n[Using the SageMaker Python SDK \u2014 sagemaker 2.72.3 documentation](https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode)\n\nIf you want to train Built-in algorithm models simply faster, you should check the recommendation in the SageMaker document.\n\nExample [Blazingtext-instances](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html#blazingtext-instances), [Deepar-instances](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar.html#deepar-instances)\n\nIf the algorithm supports it, one can also try using Pipe mode or FastFile mode. These offer some fast training job startup time. [Accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker](https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/)",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1642148033228,
        "Solution_link_count":4.0,
        "Solution_readability":18.5,
        "Solution_reading_time":16.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":124.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1518707555920,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":1664.0,
        "Answerer_view_count":560.0,
        "Challenge_adjusted_solved_time":9.0124052778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using nested parameters in my <code>parameters.yml<\/code> and would like to override these using runtime parameters for the <code>kedro run<\/code> CLI command:<\/p>\n<pre><code>train:\n    batch_size: 32\n    train_ratio: 0.9\n    epochs: 5\n<\/code><\/pre>\n<p>The following doesn't seem to work:<\/p>\n<pre><code>kedro run --params  train.batch_size:64,train.epochs:50 \n<\/code><\/pre>\n<p>the values for epoch and batch_size are those from the <code>parameters.yml<\/code>. How can I override these parameters with the cli command?<\/p>",
        "Challenge_closed_time":1596531353283,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596500312837,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in overriding nested parameters using runtime parameters for the kedro run CLI command. The user has tried to override the values for epoch and batch_size using the kedro run CLI command, but it did not work. The user is seeking guidance on how to override these parameters with the CLI command.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63238607",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.2,
        "Challenge_reading_time":7.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":8.6223461111,
        "Challenge_title":"Override nested parameters using kedro run CLI command",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":549.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1362514672823,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Vorarlberg, Austria",
        "Poster_reputation_count":1570.0,
        "Poster_view_count":159.0,
        "Solution_body":"<p>The additional parameters get passed into the <code>KedroContext<\/code> object via <code>load_context(Path.cwd(), env=env, extra_params=params)<\/code> in <code>kedro_cli.py<\/code>. Here you can see that there's a callback (protected) function called <code>_split_params<\/code> which splits the key-value pairs on <code>:<\/code>.<\/p>\n<p>This <code>_split_params<\/code> first splits string on commas (to get multiple params) and then on colons. Actually adding a print\/logging statement of what gets passed into <code>extra_params<\/code> will show you something like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>{'train.batch_size': 64, 'train.epochs': 50}\n<\/code><\/pre>\n<p>I think you have a couple options:<\/p>\n<ol>\n<li>Un-nesting the params. That way you will override them correctly.<\/li>\n<li>Adding custom logic to <code>_split_params<\/code> in <code>kedro_cli.py<\/code> to create a nested dictionary on <code>.<\/code> characters which gets passed into the func mentioned above. I think you can reuse a lot of the existing logic.<\/li>\n<\/ol>\n<p>NB: This was tested on <code>kedro==0.16.2<\/code>.<\/p>\n<p>NB2: The way <code>kedro<\/code> splits out nested params is using the <code>_get_feed_dict<\/code> and <code>_add_param_to_feed_dict<\/code> functions in <code>context.py<\/code>. Specifically, <code>_add_param_to_feed_dict<\/code> is a recursive function that unpacks a dictionary and formats as <code>&quot;{}.{}&quot;.format(key, value)<\/code>. IMO you can use the logic from here.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1596532757496,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":19.54,
        "Solution_score_count":3.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":171.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.8236111111,
        "Challenge_answer_count":2,
        "Challenge_body":"What is the difference between SageMaker Pipelines and SageMaker Step Function SDK?\n\nThe [official Pipelines notebook][1] is basically only doing a workflow - pretty much a copy cat of what step functions has been doing for years. In the nice video from [Julien Simon][2] I see CICD capacities mentioned, where are those? any demos?\n\n\n  [1]: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-pipelines\/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.ipynb\n  [2]: https:\/\/www.youtube.com\/watch?v=Hvz2GGU3Z8g",
        "Challenge_closed_time":1607015065000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606994100000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the difference between SageMaker Pipelines and SageMaker Step Function SDK, particularly in regards to the CICD capabilities mentioned in a video by Julien Simon. The user notes that the official Pipelines notebook appears to only offer workflow capabilities similar to Step Functions and is looking for demos of the CICD features.",
        "Challenge_last_edit_time":1668621791288,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU2iheeTzhSTmWw4aqVEeqOQ\/what-is-the-difference-between-sagemaker-pipelines-and-sagemaker-step-function-sdk",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":13.3,
        "Challenge_reading_time":8.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":5.8236111111,
        "Challenge_title":"What is the difference between SageMaker Pipelines and SageMaker Step Function SDK?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1344.0,
        "Challenge_word_count":68,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey, that demo is missing the project part of Pipelines and therefore the SM provided project templates. Go to SM studio and on the Studio summary hit edit settings and then enable access and provisioning of Service Catalog Portfolio of products in SM Studio. Then check your service catalog portfolios.\nHaven't tried it out yet though.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1610011923648,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":4.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1412669622830,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":7.0974458333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using the SageMaker HuggingFace Processor to create a custom tokenizer on a large volume of text data.\nIs there a way to make this job data distributed - meaning read partitions of data across nodes and train the tokenizer leveraging multiple CPUs\/GPUs.<\/p>\n<p>At the moment, providing more nodes to the processing cluster merely replicates the tokenization process (basically duplicates the process of creation), which is redundant. You can primarily only scale vertically.<\/p>\n<p>Any insights into this?<\/p>",
        "Challenge_closed_time":1662653309532,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662621424647,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using SageMaker HuggingFace Processor to create a custom tokenizer on a large volume of text data. They are looking for a way to distribute the job data across nodes and train the tokenizer leveraging multiple CPUs\/GPUs, but adding more nodes to the processing cluster only duplicates the process of creation. The user is seeking insights on how to scale the process horizontally.",
        "Challenge_last_edit_time":1662627758727,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73645084",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":7.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":8.8569125,
        "Challenge_title":"Create Hugging Face Transformers Tokenizer using Amazon SageMaker in a distributed way",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":27.0,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_created_time":1662621266503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":48.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Considering the following example code for\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-job-frameworks-hugging-face.html\" rel=\"nofollow noreferrer\">HuggingFaceProcessor<\/a>:<\/p>\n<p>If you have 100 large files in S3 and use a ProcessingInput with\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ProcessingS3Input.html#:%7E:text=S3DataDistributionType\" rel=\"nofollow noreferrer\">s3_data_distribution_type<\/a>=&quot;ShardedByS3Key&quot; (instead of FullyReplicated), the objects in your S3 prefix will be sharded and distributed to your instances.<\/p>\n<p>For example, if you have 100 large files and want to filter records from them using HuggingFace on 5 instances, the s3_data_distribution_type=&quot;ShardedByS3Key&quot; will put 20 objects on each instance, and each instance can read the files from its own path, filter out records, and write (uniquely named) files to the output paths, and SageMaker Processing will put the filtered files in S3.<\/p>\n<p>However, if your filtering criteria is stateful or depends on doing a full pass over the dataset first (such as: filtering outliers based on mean and standard deviation on a feature - in case of using SKLean Processor for example): you'll need to pass that information in to the job so each instance can know how to filter. To send information to the instances launched, you have to use the\u00a0<code>\/opt\/ml\/config\/resourceconfig.json<\/code>\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-your-own-processing-container.html#byoc-config\" rel=\"nofollow noreferrer\">file<\/a>:<\/p>\n<p><code>{ &quot;current_host&quot;: &quot;algo-1&quot;, &quot;hosts&quot;: [&quot;algo-1&quot;,&quot;algo-2&quot;,&quot;algo-3&quot;] }<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":17.1,
        "Solution_reading_time":22.86,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":186.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1601729162436,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":887.0,
        "Answerer_view_count":130.0,
        "Challenge_adjusted_solved_time":326.8268652778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>The ParallelRunStep Documentation suggests the following:<\/p>\n<p>A named input Dataset (<code>DatasetConsumptionConfig<\/code> class)<\/p>\n<pre><code>path_on_datastore = iris_data.path('iris\/')\ninput_iris_ds = Dataset.Tabular.from_delimited_files(path=path_on_datastore, validate=False)\nnamed_iris_ds = input_iris_ds.as_named_input(iris_ds_name)\n<\/code><\/pre>\n<p>Which is just passed as an Input:<\/p>\n<pre><code>distributed_csv_iris_step = ParallelRunStep(\n    name='example-iris',\n    inputs=[named_iris_ds],\n    output=output_folder,\n    parallel_run_config=parallel_run_config,\n    arguments=['--model_name', 'iris-prs'],\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p>The Documentation to submit Dataset Inputs as Parameters suggests the following:\nThe Input is a <code>DatasetConsumptionConfig<\/code> class element<\/p>\n<pre><code>tabular_dataset = Dataset.Tabular.from_delimited_files('https:\/\/dprepdata.blob.core.windows.net\/demo\/Titanic.csv')\ntabular_pipeline_param = PipelineParameter(name=&quot;tabular_ds_param&quot;, default_value=tabular_dataset)\ntabular_ds_consumption = DatasetConsumptionConfig(&quot;tabular_dataset&quot;, tabular_pipeline_param)\n<\/code><\/pre>\n<p>Which is passed in <code>arguments<\/code> as well in <code>inputs<\/code><\/p>\n<pre><code>train_step = PythonScriptStep(\n    name=&quot;train_step&quot;,\n    script_name=&quot;train_with_dataset.py&quot;,\n    arguments=[&quot;--param2&quot;, tabular_ds_consumption],\n    inputs=[tabular_ds_consumption],\n    compute_target=compute_target,\n    source_directory=source_directory)\n<\/code><\/pre>\n<p>While submitting with new parameter we create a new <code>Dataset<\/code> class:<\/p>\n<pre><code>iris_tabular_ds = Dataset.Tabular.from_delimited_files('some_link')\n<\/code><\/pre>\n<p>And submit it like this:<\/p>\n<pre><code>pipeline_run_with_params = experiment.submit(pipeline, pipeline_parameters={'tabular_ds_param': iris_tabular_ds})\n<\/code><\/pre>\n<p>However, how do we combine this: How do we pass a Dataset Input as a Parameter to the ParallelRunStep?<\/p>\n<p>If we create a <code>DatasetConsumptionConfig<\/code> class element like so:<\/p>\n<pre><code>tabular_dataset = Dataset.Tabular.from_delimited_files('https:\/\/dprepdata.blob.core.windows.net\/demo\/Titanic.csv')\ntabular_pipeline_param = PipelineParameter(name=&quot;tabular_ds_param&quot;, default_value=tabular_dataset)\ntabular_ds_consumption = DatasetConsumptionConfig(&quot;tabular_dataset&quot;, tabular_pipeline_param)\n<\/code><\/pre>\n<p>And pass it as an argument in the ParallelRunStep, it will throw an error.<\/p>\n<p>References:<\/p>\n<ol>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-showcasing-dataset-and-pipelineparameter.ipynb\" rel=\"nofollow noreferrer\">Notebook with Dataset Input Parameter<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/parallel-run\/tabular-dataset-inference-iris.ipynb\" rel=\"nofollow noreferrer\">ParallelRunStep Notebook<\/a><\/li>\n<\/ol>",
        "Challenge_closed_time":1617345688808,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616169112093,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to pass a dataset input as a parameter to the ParallelRunStep in Azure ML Service. The documentation suggests creating a DatasetConsumptionConfig class element and passing it as an argument in the ParallelRunStep. However, when the user tries to do so, it throws an error. The user is looking for a solution to combine the two methods.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66711458",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":29.8,
        "Challenge_reading_time":42.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":326.8268652778,
        "Challenge_title":"How do we do Batch Inferencing on Azure ML Service with Parameterized Dataset\/DataPath input?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":664.0,
        "Challenge_word_count":187,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>For the inputs we create Dataset class instances:<\/p>\n<pre><code>tabular_ds1 = Dataset.Tabular.from_delimited_files('some_link')\ntabular_ds2 = Dataset.Tabular.from_delimited_files('some_link')\n<\/code><\/pre>\n<p>ParallelRunStep produces an output file, we use the PipelineData class to create a folder which will store this output:<\/p>\n<pre><code>from azureml.pipeline.core import Pipeline, PipelineData\n\noutput_dir = PipelineData(name=&quot;inferences&quot;, datastore=def_data_store)\n<\/code><\/pre>\n<p>The ParallelRunStep depends on ParallelRunConfig Class to include details about the environment, entry script, output file name and other necessary definitions:<\/p>\n<pre><code>from azureml.pipeline.core import PipelineParameter\nfrom azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n\nparallel_run_config = ParallelRunConfig(\n    source_directory=scripts_folder,\n    entry_script=script_file,\n    mini_batch_size=PipelineParameter(name=&quot;batch_size_param&quot;, default_value=&quot;5&quot;),\n    error_threshold=10,\n    output_action=&quot;append_row&quot;,\n    append_row_file_name=&quot;mnist_outputs.txt&quot;,\n    environment=batch_env,\n    compute_target=compute_target,\n    process_count_per_node=PipelineParameter(name=&quot;process_count_param&quot;, default_value=2),\n    node_count=2\n)\n<\/code><\/pre>\n<p>The input to ParallelRunStep is created using the following code<\/p>\n<pre><code>tabular_pipeline_param = PipelineParameter(name=&quot;tabular_ds_param&quot;, default_value=tabular_ds1)\ntabular_ds_consumption = DatasetConsumptionConfig(&quot;tabular_dataset&quot;, tabular_pipeline_param)\n<\/code><\/pre>\n<p>The PipelineParameter helps us run the pipeline for different datasets.\nParallelRunStep consumes this as an input:<\/p>\n<pre><code>parallelrun_step = ParallelRunStep(\n    name=&quot;some-name&quot;,\n    parallel_run_config=parallel_run_config,\n    inputs=[ tabular_ds_consumption ],\n    output=output_dir,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p>To consume with another dataset:<\/p>\n<pre><code>pipeline_run_2 = experiment.submit(pipeline, \n                                   pipeline_parameters={&quot;tabular_ds_param&quot;: tabular_ds2}\n)\n<\/code><\/pre>\n<p>There is an error currently: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1312\" rel=\"nofollow noreferrer\">DatasetConsumptionConfig and PipelineParameter cannot be reused<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":28.1,
        "Solution_reading_time":31.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":152.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":6.7810055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am submitting the training through a script file. Following is the content of the <code>train.py<\/code> script. Azure ML is treating all these as one run (instead of run per alpha value as coded below) as <code>Run.get_context()<\/code> is returning the same Run id.<\/p>\n<p><strong>train.py<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.opendatasets import Diabetes\nfrom azureml.core import Run\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.externals import joblib\n\nimport math\nimport os\nimport logging\n\n# Load dataset\ndataset = Diabetes.get_tabular_dataset()\nprint(dataset.take(1))\n\ndf = dataset.to_pandas_dataframe()\ndf.describe()\n\n# Split X (independent variables) &amp; Y (target variable)\nx_df = df.dropna()      # Remove rows that have missing values\ny_df = x_df.pop(&quot;Y&quot;)    # Y is the label\/target variable\n\nx_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=66)\nprint('Original dataset size:', df.size)\nprint(&quot;Size after dropping 'na':&quot;, x_df.size)\nprint(&quot;Training split size: &quot;, x_train.size)\nprint(&quot;Test split size: &quot;, x_test.size)\n\n# Training\nalphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters\n\n# Create and log interactive runs\n\noutput_dir = os.path.join(os.getcwd(), 'outputs')\n\nfor hyperparam_alpha in alphas:\n    # Get the experiment run context\n    run = Run.get_context()\n    print(&quot;Started run: &quot;, run.id)\n    run.log(&quot;train_split_size&quot;, x_train.size)\n    run.log(&quot;test_split_size&quot;, x_train.size)\n    run.log(&quot;alpha_value&quot;, hyperparam_alpha)\n\n    # Train\n    print(&quot;Train ...&quot;)\n    model = Ridge(hyperparam_alpha)\n    model.fit(X = x_train, y = y_train)\n    \n    # Predict\n    print(&quot;Predict ...&quot;)\n    y_pred = model.predict(X = x_test)\n\n    # Calculate &amp; log error\n    rmse = math.sqrt(mean_squared_error(y_true = y_test, y_pred = y_pred))\n    run.log(&quot;rmse&quot;, rmse)\n    print(&quot;rmse&quot;, rmse)\n\n    # Serialize the model to local directory\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir, exist_ok=True) \n\n    print(&quot;Save model ...&quot;)\n    model_name = &quot;model_alpha_&quot; + str(hyperparam_alpha) + &quot;.pkl&quot; # Pickle file\n    file_path = os.path.join(output_dir, model_name)\n    joblib.dump(value = model, filename = file_path)\n\n    # Upload the model\n    run.upload_file(name = model_name, path_or_stream = file_path)\n\n    # Complete the run\n    run.complete()\n<\/code><\/pre>\n<p><strong>Experiments view<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/E6xAG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E6xAG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Authoring code (i.e. control plane)<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom azureml.core import Workspace, Experiment, RunConfiguration, ScriptRunConfig, VERSION, Run\n\nws = Workspace.from_config()\nexp = Experiment(workspace = ws, name = &quot;diabetes-local-script-file&quot;)\n\n# Create new run config obj\nrun_local_config = RunConfiguration()\n\n# This means that when we run locally, all dependencies are already provided.\nrun_local_config.environment.python.user_managed_dependencies = True\n\n# Create new script config\nscript_run_cfg = ScriptRunConfig(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    script = 'train.py',\n    run_config = run_local_config) \n\nrun = exp.submit(script_run_cfg)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>",
        "Challenge_closed_time":1599436122230,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599411710610,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is submitting a training through a script file, but Azure ML is treating all the runs as one run instead of a run per alpha value as coded, because Run.get_context() is returning the same Run id.",
        "Challenge_last_edit_time":1599438192360,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63766714",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":46.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":6.7810055556,
        "Challenge_title":"Run.get_context() gives the same run id",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":2523.0,
        "Challenge_word_count":324,
        "Platform":"Stack Overflow",
        "Poster_created_time":1245726715288,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cumming, GA",
        "Poster_reputation_count":77230.0,
        "Poster_view_count":6359.0,
        "Solution_body":"<h2>Short Answer<\/h2>\n<h3>Option 1: create child runs within run<\/h3>\n<p><code>run = Run.get_context()<\/code> assigns the run object of the run that you're currently in to <code>run<\/code>. So in every iteration of the hyperparameter search, you're logging to the same run. To solve this, you need to create child (or sub-) runs for each hyperparameter value. You can do this with <code>run.child_run()<\/code>. Below is the template for making this happen.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>run = Run.get_context()\n\nfor hyperparam_alpha in alphas:\n    # Get the experiment run context\n    run_child = run.child_run()\n    print(&quot;Started run: &quot;, run_child.id)\n    run_child.log(&quot;train_split_size&quot;, x_train.size)\n<\/code><\/pre>\n<p>On the <code>diabetes-local-script-file<\/code> Experiment page, you can see that Run <code>9<\/code> was the parent run and Runs <code>10-19<\/code> were the child runs if you click &quot;Include child runs&quot; page. There is also a &quot;Child runs&quot; tab on Run 9 details page.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5IMcz.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5IMcz.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Long answer<\/h2>\n<p>I highly recommend abstracting the hyperparameter search away from the data plane (i.e. <code>train.py<\/code>) and into the control plane (i.e. &quot;authoring code&quot;). This becomes especially valuable as training time increases and you can arbitrarily parallelize and also choose Hyperparameters more intelligently by using Azure ML's <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters\" rel=\"noreferrer\"><code>Hyperdrive<\/code><\/a>.<\/p>\n<h3>Option 2 Create runs from control plane<\/h3>\n<p>Remove the loop from your code, add the code like below (<a href=\"https:\/\/gist.github.com\/swanderz\/f5c0dc1aefc796d37f6bc3600f2ae3cd\" rel=\"noreferrer\">full data and control here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import argparse\nfrom pprint import pprint\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--alpha', type=float, default=0.5)\nargs = parser.parse_args()\nprint(&quot;all args:&quot;)\npprint(vars(args))\n\n# use the variable like this\nmodel = Ridge(args.alpha)\n<\/code><\/pre>\n<p>below is how to submit a single run using a script argument. To submit multiple runs, just use a loop in the control plane.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters\n\nlist_rcs = [ScriptRunConfig(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    script = 'train.py',\n    arguments=['--alpha',a],\n    run_config = run_local_config) for a in alphas]\n\nlist_runs = [exp.submit(rc) for rc in list_rcs]\n\n<\/code><\/pre>\n<h3>Option 3 Hyperdrive (IMHO the recommended approach)<\/h3>\n<p>In this way you outsource the hyperparameter source to <code>Hyperdrive<\/code>. The UI will also report results exactly how you want them, and via the API you can easily download the best model.  Note you can't use this locally anymore and must use <code>AMLCompute<\/code>, but to me it is a worthwhile trade-off.<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters#configure-experiment\" rel=\"noreferrer\">This is a great overview<\/a>. Excerpt below (<a href=\"https:\/\/gist.github.com\/swanderz\/f5c0dc1aefc796d37f6bc3600f2ae3cd#file-hyperdrive-ipynb\" rel=\"noreferrer\">full code here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>param_sampling = GridParameterSampling( {\n        &quot;alpha&quot;: choice(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)\n    }\n)\n\nestimator = Estimator(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    entry_script = 'train.py',\n    compute_target=cpu_cluster,\n    environment_definition=Environment.get(workspace=ws, name=&quot;AzureML-Tutorial&quot;)\n)\n\nhyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n                          hyperparameter_sampling=param_sampling, \n                          policy=None,\n                          primary_metric_name=&quot;rmse&quot;, \n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                          max_total_runs=10,\n                          max_concurrent_runs=4)\n\nrun = exp.submit(hyperdrive_run_config)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q1AhJ.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q1AhJ.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":14.3,
        "Solution_reading_time":57.41,
        "Solution_score_count":7.0,
        "Solution_sentence_count":41.0,
        "Solution_word_count":417.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":4257.2200897222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a Sklearn processing job in Amazon Sagemekar to perform some data transformation of my input data before I do model training.<\/p>\n<p>I wrote a custom python script <code>preprocessing.py<\/code> which does the needful. I use some python package in this script. <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">Here is the Sagemaker example I followed<\/a>.<\/p>\n<p>When I try to submit the Processing Job I get an error -<\/p>\n<pre><code>............................Traceback (most recent call last):\n  File &quot;\/opt\/ml\/processing\/input\/code\/preprocessing.py&quot;, line 6, in &lt;module&gt;\n    import snowflake.connector\nModuleNotFoundError: No module named 'snowflake.connector'\n<\/code><\/pre>\n<p>I understand that my processing job is unable to find this package and I need to install it. My question is how can I accomplish this using Sagemaker Processing Job API? Ideally there should be a way to define a <code>requirements.txt<\/code> in the API call, but I don't see such functionality in the docs.<\/p>\n<p>I know I can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-container-run-scripts.html\" rel=\"nofollow noreferrer\">create a custom Image with relevant packages<\/a> and later use this image in the Processing Job, but this seems too much work for something that should be built-in?<\/p>\n<p>Is there an easier\/elegant way to install packages needed in Sagemaker Processing Job ?<\/p>",
        "Challenge_closed_time":1638909512683,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638871133180,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to create a Sklearn processing job in Amazon Sagemaker to perform data transformation using a custom python script that requires a specific python package. However, when trying to submit the processing job, the user encounters an error indicating that the package is not found. The user is seeking an easier and more elegant way to install the required package within the Sagemaker Processing Job API.",
        "Challenge_last_edit_time":1638872002100,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70258080",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":21.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":10.6609730556,
        "Challenge_title":"How to install python packages within Amazon Sagemaker Processing Job?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":2087.0,
        "Challenge_word_count":199,
        "Platform":"Stack Overflow",
        "Poster_created_time":1410888611067,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":435.0,
        "Poster_view_count":32.0,
        "Solution_body":"<p>One way would be to <a href=\"https:\/\/stackoverflow.com\/questions\/12332975\/installing-python-module-within-code\">call pip from Python<\/a>:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>subprocess.check_call([sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, package])\n<\/code><\/pre>\n<p>Another way would be to use an <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/sagemaker.sklearn.html\" rel=\"nofollow noreferrer\">SKLearn Estimator<\/a> (training job) instead, to do the same thing. You can provide the <code>source_dir<\/code>, which can include a <code>requirements.txt<\/code> file, and these requirements will be installed for you<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>estimator = SKLearn(\n    entry_point=&quot;foo.py&quot;,\n    source_dir=&quot;.\/foo&quot;, # no trailing slash! put requirements.txt here\n    framework_version=&quot;0.23-1&quot;,\n    role = ...,\n    instance_count = 1,\n    instance_type = &quot;ml.m5.large&quot;\n)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1654197994423,
        "Solution_link_count":2.0,
        "Solution_readability":14.8,
        "Solution_reading_time":13.46,
        "Solution_score_count":4.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":76.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":170.4254455556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/run-pipeline.html#run-pipeline-prereq\" rel=\"nofollow noreferrer\">SageMaker documentatin<\/a> explains how to run a pipeline, but it assumes I have just <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html\" rel=\"nofollow noreferrer\">defined it<\/a> and I have the object <code>pipeline<\/code> available.<\/p>\n<p>How can I run an <strong>existing<\/strong> pipeline with <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">Python SDK<\/a>?<\/p>\n<p>I know how to read a pipeline with AWS CLI (i.e. <code>aws sagemaker describe-pipeline --pipeline-name foo<\/code>). Can the same be done with Python code? Then I would have <code>pipeline<\/code> object ready to use.<\/p>",
        "Challenge_closed_time":1659628256872,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659598635513,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to execute an existing SageMaker pipeline using Python SDK, as the documentation assumes that the user has only defined the pipeline and has the object 'pipeline' available. The user is also looking for a way to obtain the 'pipeline' object using Python code, similar to how it can be done using AWS CLI.",
        "Challenge_last_edit_time":1659598939403,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73232032",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":11.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":8.2281552778,
        "Challenge_title":"Start execution of existing SageMaker pipeline using Python SDK",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":106.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1217615304816,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Poland",
        "Poster_reputation_count":16694.0,
        "Poster_view_count":3155.0,
        "Solution_body":"<p>If the Pipeline has been created, you can use the Python Boto3 SDK to make the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.start_pipeline_execution\" rel=\"nofollow noreferrer\"><code>StartPipelineExecution<\/code><\/a> API call.<\/p>\n<pre><code>response = client.start_pipeline_execution(\n    PipelineName='string',\n    PipelineExecutionDisplayName='string',\n    PipelineParameters=[\n        {\n            'Name': 'string',\n            'Value': 'string'\n        },\n    ],\n    PipelineExecutionDescription='string',\n    ClientRequestToken='string',\n    ParallelismConfiguration={\n        'MaxParallelExecutionSteps': 123\n    }\n)\n<\/code><\/pre>\n<p>If you prefer AWS CLI, the most basic call is:<\/p>\n<pre><code>aws sagemaker start-pipeline-execution --pipeline-name &lt;name-of-the-pipeline&gt;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1660212471007,
        "Solution_link_count":1.0,
        "Solution_readability":25.0,
        "Solution_reading_time":10.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":53.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1322.0291666667,
        "Challenge_answer_count":0,
        "Challenge_body":"<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\nI started to use amundsen metadata with Neptune database. Initially I used the metadata docker image to interact with the database, but every tested route gave me a 500 internal server error. So I tested it locally, using a VPN to connect to neptune db, and I found 2 problems. I'll do a PR linked to the issue that solves the problems\r\n## Expected Behavior\r\n<!--- Tell us what should happen -->\r\nWhen calling a route of the metadata api for the neptune service, the server should respond without problem\r\n## Current Behavior\r\n<!--- Tell us what happens instead of the expected behavior -->\r\n1. When calling the api to retrieve (for example) a table description, there's an error `got an unexpected keyword argument 'read_timeout'`. This error has already be identified in https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1382\r\n2. After the correction of 1, another error during the same request\r\n```json\r\n{\r\n    \"detailedMessage\": \"Failed to interpret Gremlin query: Query parsing failed at line 1, character position at 208, error message : token recognition error at: 'dec'\",\r\n    \"code\": \"MalformedQueryException\",\r\n    \"requestId\": \"25542307-96bb-40d2-9585-5a340b8d868c\"\r\n}\r\n```\r\n## Possible Solution\r\n<!--- Not obligatory, but suggest a fix\/reason for the bug -->\r\n1. Initialize `TornadoTransport` class properly, removing `read_timeout` and `write_timeout` in  `gremlin_proxy.py` file\r\n2. Move `Order.decr`to `Order.desc` for `_get_table_columns` and `_get_popular_tables_uris` functions in `gremlin_proxy.py` file. The Order.decr and Order.incr are deprecated and don't work with neptune\r\n## Steps to Reproduce\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\r\n1. Call the `\/table\/{table_uri}` metadata route using the gremlin metadata service with AWS Neptune db\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\n<!--- How has this issue affected you? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Amunsen version used: last (metadata-3.10.0)\r\n* Data warehouse stores: snowflake\r\n* Deployment (k8s or native):\r\n* Link to your fork or repository: https:\/\/github.com\/ggirodda\/amundsen\/tree\/main",
        "Challenge_closed_time":1664069854000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1659310549000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the limit parameter in their code, which is not working and throwing an error. The code is related to the introduction to Node Classification Gremlin, and the error message suggests that there may be an issue with the service configuration or query. The user is seeking suggestions on whether there is something wrong with the code mentioned in the document.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1946",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":10.1,
        "Challenge_reading_time":31.58,
        "Challenge_repo_contributor_count":207.0,
        "Challenge_repo_fork_count":890.0,
        "Challenge_repo_issue_count":2023.0,
        "Challenge_repo_star_count":3674.0,
        "Challenge_repo_watch_count":245.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":1322.0291666667,
        "Challenge_title":"Neptune MalformedQueryException",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":345,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for opening your first issue here!\n The PR that solves the issue in my case https:\/\/github.com\/amundsen-io\/amundsen\/pull\/1947\/files This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.\n This issue has been automatically closed for inactivity. If you still wish to make these changes, please open a new pull request or reopen this one.\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.7,
        "Solution_reading_time":5.27,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":67.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.7709561111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi all,\n\nI am asking if it's possible to use `framework processor` inside a `sagemaker pipeline`.\n\nI am asking because the to submit the source_dir for the framework processor, we have to do so when calling the .run() method, when wrapping the processor inside a `sagemaker.workflow.steps.ProcessingStep`, there isn't an available argument to specify the `source_dir`.\n\nThank you!\nBest,\nRuoy",
        "Challenge_closed_time":1652383066859,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652344291417,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with using the framework processor inside a SageMaker pipeline as there is no available argument to specify the source directory when wrapping the processor inside a processing step.",
        "Challenge_last_edit_time":1668580240092,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUbY_u2lSORnmomHzZsGOZAA\/sagemaker-framework-processor-compatibility-with-sagemaker-pipelines",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":5.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":10.7709561111,
        "Challenge_title":"SageMaker framework processor compatibility with sagemaker pipelines",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":383.0,
        "Challenge_word_count":65,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You can do this with the latest version of the sagemaker sdk 2.89.0\n\n```\nfrom sagemaker.workflow.pipeline_context import PipelineSession\n\nsession = PipelineSession()\n\ninputs = [\n    ProcessingInput(\n    source=\"s3:\/\/my-bucket\/sourcefile\", \n    destination=\"\/opt\/ml\/processing\/inputs\/\",),\n]\n\nprocessor = FrameworkProcessor(...)\n\nstep_args = processor.run(inputs=inputs, source_dir=\"...\")\n\nstep_sklearn = ProcessingStep(\n    name=\"MyProcessingStep\",\n    step_args=step_args,\n)\n```",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1652383066859,
        "Solution_link_count":0.0,
        "Solution_readability":18.2,
        "Solution_reading_time":6.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":32.0,
        "Tool":"Amazon SageMaker"
    }
]