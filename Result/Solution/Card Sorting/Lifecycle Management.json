[
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"We encountered an issue where an older Sagemaker instance (>2 months) was turned on. After starting, one of the two study folders associated were not syncing any of the files. In the system logs there's this error: `Nov 11 16:21:45 <ip redacted> \/usr\/local\/bin\/goofys[9204]: main.ERROR Unable to access '<bucket A, name redacted>': permission denied`\r\n\r\nComparing the S3mounts parameter for the Sagemaker stack of the older instance that fails to sync, and a newer instance (with the same studies), I see that the FS role number for the private workspace study that wouldn't sync is different.\r\n\r\nOld stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1662735997814\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nNew stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1668521384862\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nSome additional context, this bucket (and the associated SWB data source) that the two studies are a part of gets updated every couple months to add new study folders\/ids.\r\n\r\nMy question is: What could cause the fs role number to change for a study?\r\n\r\n**Versions (please complete the following information):**\r\n - SWB 4.3.1\r\n",
        "Challenge_closed_time":1668634.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668633119000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1066",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":26.21,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"[Bug] SWB Sagemaker Study permission denied",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":231,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I'm elevating this to a bug.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.5,
        "Solution_reading_time":0.34,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":6.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am a new learner of machine learning and computer science, I wonder the difference between these two terms. I am confused on the concept, can someone answer this question?  <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623786913570,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/437505\/what-is-the-difference-between-online-learning-and",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":3.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"What is the difference between online learning and offline learning",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":40,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,  <\/p>\n<p>Thanks for reaching out to us here. They are both machine learning methods for training. online machine learning is a method of machine learning in which data becomes available in <strong>a sequential order and is used to update the best predictor<\/strong> for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the <strong>entire training data set at once.<\/strong>   <\/p>\n<p>Like, one more data coming in, the <strong>predictor moves<\/strong> once. This method is good for scenario like stock prediction, optimization...  <\/p>\n<p>Linear least square is a very good example to understand.   <br \/>\n<a href=\"https:\/\/en.wikipedia.org\/wiki\/Linear_least_squares\">https:\/\/en.wikipedia.org\/wiki\/Linear_least_squares<\/a>  <\/p>\n<p>For Machine Learning beginner, Machine Learning Designer is a very good point to start. You can try any algorithms to see the difference.  <br \/>\n<a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning\/designer\/\">https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning\/designer\/<\/a>  <\/p>\n<p>Please feel free to let us know if you have more questions.  <\/p>\n<p>Regards,  <br \/>\nYutong  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.3,
        "Solution_reading_time":15.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":150.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1386491614716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2778.0,
        "Answerer_view_count":352.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Recently, I have changed account on AWS and faced with weird error in Sagemaker.<\/p>\n<p>Basically, I'm just checking <code>xgboost<\/code> algo with some toy dataset in this manner:<\/p>\n<pre><code>from sagemaker import image_uris\n\nxgb_image_uri = image_uris.retrieve(&quot;xgboost&quot;, boto3.Session().region_name, &quot;1&quot;)\n\nclf = sagemaker.estimator.Estimator(xgb_image_uri,\n                   role, 1, 'ml.c4.2xlarge',\n                   output_path=&quot;s3:\/\/{}\/output&quot;.format(session.default_bucket()),\n                   sagemaker_session=session)\n\nclf.fit(location_data)\n<\/code><\/pre>\n<p>Then the training job is starting to be executed but for some reason, on downloading data step it stops the training job and displays the following message:<\/p>\n<pre><code>2021-10-21 17:33:27 Downloading - Downloading input data\n2021-10-21 17:33:27 Stopping - Stopping the training job\n2021-10-21 17:33:27 Stopped - Training job stopped\nProfilerReport-1634837444: Stopping\n..\nJob ended with status 'Stopped' rather than 'Completed'. This could mean the job timed out or stopped early for some other reason: Consider checking whether it completed as you expect.\n<\/code><\/pre>\n<p>Also, when I'm trying to go back to training jobs section and check for logs in cloudwatch there is nothing to be displayed. Is it common issue and who had faced with that? Are there any workarounds?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634838048463,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69666500",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":17.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Training Job is Stopping in Sagemaker",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":361.0,
        "Challenge_word_count":165,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1386491614716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2778.0,
        "Poster_view_count":352.0,
        "Solution_body":"<p>The problem was most likely with templates for sagemaker that was runned before creating the instance.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":1.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer wants to use SageMaker, but doesn't know how to get started with instance sizes or how to forecast the cost for it. I've looked at the SageMaker TCO PDF we have online, but that appears more marketing than helpful, i.e. more price *comparison* than guidance.\n\nI know that the SageMaker cost is really the underlying EC2 and storage pieces, not SageMaker itself. However, I feel it is incorrect to say that they start with (say) t3.medium and see if that fits and scale up if they need more power behind it. As well, that doesn't help them to forecast either.\n\nAny thoughts here?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603285551000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667926579047,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUq-Kaj1bLStK6Bs2gCUZ1Iw\/where-can-i-find-guidance-for-getting-a-customer-started-with-sagemaker-sizing-and-cost",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":8.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Where can I find guidance for getting a customer started with SageMaker sizing and cost?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":82.0,
        "Challenge_word_count":119,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"See the **performance efficiency** and **cost optimization** pillars in [Machine Learning Lens][1].\nAdditionally this is an [EC2 based right sizing best practices guide][2].  \nOverall, it's better to start small, then increase instance size as needed (as those that start large, never bother reduce the size), or apply auto scaling for SageMaker hosting.  \nAssuming a CPU ML predictions: When choosing ml.t2.medium instances the customer will need to keep an eye on the instance CPU credits. If they lack the knowledge, just start with M5.\n\n\n  [1]: https:\/\/d1.awsstatic.com\/whitepapers\/architecture\/wellarchitected-Machine-Learning-Lens.pdf\n  [2]: https:\/\/docs.aws.amazon.com\/whitepapers\/latest\/cost-optimization-right-sizing\/tips-for-right-sizing-your-workloads.html",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1667925560088,
        "Solution_link_count":2.0,
        "Solution_readability":11.7,
        "Solution_reading_time":9.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created an Azure ML pipeline with different steps (data preprocess, train, validation ...). And for pass data from one step to the next I have used the PipelineData object.<\/p>\n<p><em>Example passing the model from train step to validate one:<\/em><\/p>\n<pre><code>    # Create a PipelineData to pass model from train to register\n    model_path = PipelineData('model')\n\n    # Step 2\n    train_step = PythonScriptStep(\n        name = 'Train the Model',\n        script_name = 'TrainStepScript.py',\n        source_directory = source_folder,\n        arguments = ['--training_data', prepped_data,\n                     '--model_name', model_name,\n                     '--model_path', model_path],\n        outputs = [model_path],\n        compute_target = compute_target,\n        runconfig = aml_run_config,\n        allow_reuse = True\n    )\n\n    # Step 3\n    third_step = PythonScriptStep(\n        name = 'Evaluate &amp; register the Model',\n        script_name = 'ValidateStepScript.py',\n        source_directory = source_folder,\n        arguments = ['--model_name', model_name,\n                     '--model_path', model_path],\n        inputs = [model_path],\n        compute_target = compute_target,\n        runconfig = aml_run_config,\n        allow_reuse = True\n    )\n<\/code><\/pre>\n<p>Now for debugging and development purposes I want to create a script to run separately the different steps using a ScriptRunConfig (with the same environment and arguments of the StepScript in the pipeline). But the problem is I don't know how to simulate the data input\/output of each step, because the DataPipeline object is not working for this purpose.<\/p>\n<p>Just for clarification, my goal is to NOT modify the original pipeline StepScripts, so I can use them after debugging in the final pipeline. To sum up, my question is: how can I emulate the DataPipeline object (if possible) in this case?<\/p>\n<p><em>Example of what I'm trying to build:<\/em><\/p>\n<pre><code># Passing in some way the model path (from local)\nmodel_path = PipelineData('model')\n\n# Create a script config for validate step\nvalidate_script_config = ScriptRunConfig(\n    source_directory = source_folder,\n    script = 'ValidateStepScript.py',\n    arguments = ['--model_name', model_name,\n                 '--model_path', model_path],\n    environment = experiment_env,\n    docker_runtime_config = DockerConfiguration(use_docker=True)\n)\n\nexperiment = Experiment(workspace=ws, name=experiment_name)\ndata_run = experiment.submit(config=data_script_config)\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663071829967,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73702976",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.4,
        "Challenge_reading_time":29.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"How to split Azure ML pipeline steps to debug",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":41.0,
        "Challenge_word_count":264,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1661942018832,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>We can download the output of the model in a repository and make them as the source file for the later steps as required. The below code block can be incorporated in the same pipeline which is being used now.<\/p>\n<p>To download the model output:<\/p>\n<pre><code>train_step = pipeline_run1.find_step_run('train.py')\n\nif train_step:\n    train_step_obj = train_step[0] \n    train_step_obj.get_output_data('processed_data1').download(&quot;.\/outputs&quot;) # download the output to current directory\n<\/code><\/pre>\n<p>after downloading the model, then use that as the parent source directory in source_directory<\/p>\n<pre><code>from azureml.core import ScriptRunConfig, Experiment\n   # create or load an experiment\n   experiment = Experiment(workspace, 'MyExperiment')\n   # create or retrieve a compute target\n   cluster = workspace.compute_targets['MyCluster']\n   # create or retrieve an environment\n   env = Environment.get(ws, name='MyEnvironment')\n   # configure and submit your training run\n   config = ScriptRunConfig(source_directory='.',\n                            command=['python', 'train.py'],\n                            compute_target=cluster,\n                            environment=env)\n   script_run = experiment.submit(config)\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.3,
        "Solution_reading_time":14.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":117.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Trying to download a report as latex causes an instrument.js error, and the waiting symbol turns forever. I use chrome on MacOS.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77.jpeg\" data-download-href=\"\/uploads\/short-url\/aV3jmQ0drwgzx2rJ9iyEpD7TJQj.jpeg?dl=1\" title=\"Bildschirmfoto 2023-02-13 um 17.42.14\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77_2_690x307.jpeg\" alt=\"Bildschirmfoto 2023-02-13 um 17.42.14\" data-base62-sha1=\"aV3jmQ0drwgzx2rJ9iyEpD7TJQj\" width=\"690\" height=\"307\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77_2_690x307.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77_2_1035x460.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/4\/4c88c4ac7b283a2fef287aa3cd58a712426c2f77_2_1380x614.jpeg 2x\" data-dominant-color=\"959190\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Bildschirmfoto 2023-02-13 um 17.42.14<\/span><span class=\"informations\">1886\u00d7841 173 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676306917404,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/download-report-as-latex-causes-js-errors\/3872",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":24.9,
        "Challenge_reading_time":21.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Download report as latex causes js errors",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":218.0,
        "Challenge_word_count":76,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Found a solution: When carefully loading each graph by scrolling slowly over the whole page, the download finally works.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":1.6,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":19.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>&quot;We want  the model  to automatically register model every time there is a new model. we created the model in the process and write it out to a pipeline data set.To persist it then we upload and read it for registration.    <\/p>\n<p>We are using .\/output to send the file to output. The issue is that it cannot find it in the file path . How can we validate its existence?  &quot;  <\/p>\n<p>[Note: As we migrate from MSDN, this question has been posted by an\u202fAzure Cloud Engineer\u202fas a frequently asked question] Source: <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/1369621f-ebc2-4e79-abe9-9f1165aea6c6\/model-file-is-not-found-for-registration-of-model-in-training-pipeline?forum=MachineLearning\">MSDN<\/a>  <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589329342560,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/26470\/model-file-is-not-found-for-registration-of-model",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Model file is not found for Registration of model in training Pipeline.",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":108,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Can you verify that the script that is actually writing the model file to the location you expect:<\/p>\n<pre><code>with open(model_name, 'wb') as file:\n       joblib.dump(value = model, filename = os.path.join('.\/outputs\/', model_name))\n<\/code><\/pre>\n<p>Inside in your train python script, you just need to do something like this:<\/p>\n<h1 id=\"persist-the-model-to-the-local-machine\">persist the model to the local machine<\/h1>\n<pre><code>tf.saved_model.save(model,'.\/outputs\/model\/')\n<\/code><\/pre>\n<h1 id=\"register-the-model-with-run-object\">register the model with run object<\/h1>\n<pre><code>run.register_model(model_name,'.\/outputs\/model\/')\n<\/code><\/pre>\n<p>Source: <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/1369621f-ebc2-4e79-abe9-9f1165aea6c6\/model-file-is-not-found-for-registration-of-model-in-training-pipeline?forum=MachineLearning\">MSDN<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":21.3,
        "Solution_reading_time":11.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":65.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":170.4254455556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/run-pipeline.html#run-pipeline-prereq\" rel=\"nofollow noreferrer\">SageMaker documentatin<\/a> explains how to run a pipeline, but it assumes I have just <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html\" rel=\"nofollow noreferrer\">defined it<\/a> and I have the object <code>pipeline<\/code> available.<\/p>\n<p>How can I run an <strong>existing<\/strong> pipeline with <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">Python SDK<\/a>?<\/p>\n<p>I know how to read a pipeline with AWS CLI (i.e. <code>aws sagemaker describe-pipeline --pipeline-name foo<\/code>). Can the same be done with Python code? Then I would have <code>pipeline<\/code> object ready to use.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659598635513,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1659598939403,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73232032",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":11.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Start execution of existing SageMaker pipeline using Python SDK",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":106.0,
        "Challenge_word_count":83,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1217615304816,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Poland",
        "Poster_reputation_count":16694.0,
        "Poster_view_count":3155.0,
        "Solution_body":"<p>If the Pipeline has been created, you can use the Python Boto3 SDK to make the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.start_pipeline_execution\" rel=\"nofollow noreferrer\"><code>StartPipelineExecution<\/code><\/a> API call.<\/p>\n<pre><code>response = client.start_pipeline_execution(\n    PipelineName='string',\n    PipelineExecutionDisplayName='string',\n    PipelineParameters=[\n        {\n            'Name': 'string',\n            'Value': 'string'\n        },\n    ],\n    PipelineExecutionDescription='string',\n    ClientRequestToken='string',\n    ParallelismConfiguration={\n        'MaxParallelExecutionSteps': 123\n    }\n)\n<\/code><\/pre>\n<p>If you prefer AWS CLI, the most basic call is:<\/p>\n<pre><code>aws sagemaker start-pipeline-execution --pipeline-name &lt;name-of-the-pipeline&gt;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1660212471007,
        "Solution_link_count":1.0,
        "Solution_readability":25.0,
        "Solution_reading_time":10.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":53.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458548318740,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":1568.0,
        "Answerer_view_count":266.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a DAG in Airflow with SageMakerOperators and I have not been able to make them work. The title is the error that appears in the airflow GUI. For solving it, I have made the following tries:<\/p>\n\n<pre><code>sudo pip3 uninstall urllib3 &amp;&amp; sudo pip3 install urllib3==1.22 \nsudo pip3 install urllib3==1.22 --upgrade\nsudo pip3 install urllib3==1.22 -t \/home\/ubuntu\/.local\/lib\/python3.7\/site-packages -upgrade\n<\/code><\/pre>\n\n<p>But I am still getting the error in the GUI. Plus, in the console of the webserver I am getting:<\/p>\n\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages\/urllib3-1.22.dist-info\/METADATA'\n<\/code><\/pre>\n\n<p>The thing is that if I make <code>pip3 show urllib3<\/code> I get the version 1.22:\n<a href=\"https:\/\/i.stack.imgur.com\/i5y8i.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/i5y8i.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>However, it says dist-packages instead of site-packages. In addition, trying to go to <code>\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages\/urllib3-1.22.dist-info\/<\/code> for trying to solve the metadata file not found error, the directory does not exists. \n<a href=\"https:\/\/i.stack.imgur.com\/44H4I.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/44H4I.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/2CnJl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2CnJl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am totally lost at this point. How could I solve this problem?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1568045843843,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57857726",
        "Challenge_link_count":6,
        "Challenge_participation_count":4,
        "Challenge_readability":10.4,
        "Challenge_reading_time":22.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"Broken DAG: urllib3 1.25.3 (\/home\/ubuntu\/.local\/lib\/python3.7\/site-packages), Requirement.parse('urllib3<1.25,>=1.21'), {'sagemaker'}",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":343.0,
        "Challenge_word_count":181,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1523298968403,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1754.0,
        "Poster_view_count":197.0,
        "Solution_body":"<p>Here you go.<\/p>\n\n<p>Airflow is looking in the local (user) Python installation for the library but <code>urllib3<\/code> is installed for all users. It's weird but try doing <code>pip3 install --user urllib3==1.22<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":2.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1409041899323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":324.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a team where many member has permission to submit Spark tasks to YARN (the resource management) by command line. It's hard to track who is using how much cores, who is using how much memory...e.g. Now I'm looking for a software, framework or something could help me monitor the parameters that each member used. It will be a bridge between client and YARN. Then I could used it to filter the submit commands.<\/p>\n\n<p>I did take a look at <a href=\"http:\/\/www.mlflow.org\" rel=\"nofollow noreferrer\">mlflow<\/a> and I really like the MLFlow Tracking but it was designed for ML training process. I wonder if there is an alternative for my purpose? Or there is any other solution for the problem.<\/p>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562750084187,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56967364",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":9.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Keep track of all the parameters of spark-submit",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":93.0,
        "Challenge_word_count":128,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1413431014112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>My recommendation would be to build such a tool yourself as its not too complicated,\nhave a wrapper script to spark submit which logs the usage in a DB and after the spark job finishes the wrapper will know to release information. could be done really easily.\nIn addition you can even block new spark submits if your team already asked for too much information.<\/p>\n\n<p>And as you build it your self its really flexible as you can even create \"sub teams\" or anything you want.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":5.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":86.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm training a large-ish model, trying to use for the purpose <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#create-an-estimator\" rel=\"nofollow noreferrer\">Azure Machine Learning service<\/a> in Azure notebooks.<\/p>\n\n<p>I thus create an <code>Estimator<\/code> to train locally:<\/p>\n\n<pre><code>from azureml.train.estimator import Estimator\n\nestimator = Estimator(source_directory='.\/source_dir',\n                      compute_target='local',\n                      entry_script='train.py')\n<\/code><\/pre>\n\n<p>(my <code>train.py<\/code> should load and train starting from a large word vector file).<\/p>\n\n<p>When running with <\/p>\n\n<pre><code>run = experiment.submit(config=estimator)\n<\/code><\/pre>\n\n<p>I get <\/p>\n\n<blockquote>\n  <p>TrainingException: <\/p>\n  \n  <p>====================================================================<\/p>\n  \n  <p>While attempting to take snapshot of\n  \/data\/home\/username\/notebooks\/source_dir Your total\n  snapshot size exceeds the limit of 300.0 MB. Please see\n  <a href=\"http:\/\/aka.ms\/aml-largefiles\" rel=\"nofollow noreferrer\">http:\/\/aka.ms\/aml-largefiles<\/a> on how to work with large files.<\/p>\n  \n  <p>====================================================================<\/p>\n<\/blockquote>\n\n<p>The link provided in the error is likely <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/26076\" rel=\"nofollow noreferrer\">broken<\/a>. \nContents in my <code>.\/source_dir<\/code> indeed exceed 300 MB.<br>\nHow can I solve this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554414642303,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1554642637940,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55525445",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":13.3,
        "Challenge_reading_time":20.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"How to overcome TrainingException when training a large model with Azure Machine Learning service?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1127.0,
        "Challenge_word_count":135,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>You can place the training files outside <code>source_dir<\/code> so that they don't get uploaded as part of submitting the experiment, and then upload them separately to the data store (which is basically using the Azure storage associated with your workspace). All you need to do then is reference the training files from <code>train.py<\/code>. <\/p>\n\n<p>See the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml\" rel=\"nofollow noreferrer\">Train model tutorial<\/a> for an example of how to upload data to the data store and then access it from the training file.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1554449103928,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":7.95,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nI was wondering if there is CLI option to \"Update the Job status\" similar to:\n\nI have found polyaxon ops update --help\n\nUsage: polyaxon ops update [OPTIONS]\n\n  Update run.\n\n  Uses \/docs\/core\/cli\/#caching\n\n  Examples:\n\n  $ polyaxon ops update --uid 8aac02e3a62a4f0aaa257c59da5eab80\n  --description=\"new description for my runs\"\n\n  $ polyaxon ops update --project=cats-vs-dogs -uid 8aac02e3a62a4f0aaa257c59da5eab80 --tags=\"foo, bar\" --name=\"unique-name\"\n\nOptions:\n  -p, --project TEXT  The project name, e.g. 'mnist' or 'acme\/mnist'.\n  -uid, --uid TEXT    The run uuid.\n  -n, --name TEXT     Name of the run (optional).\n  --description TEXT  Description of the run (optional).\n  --tags TEXT         Tags of the run (comma separated values).\n  --help              Show this message and exit.\n\nLong story short is that lot of my jobs finished working with status \"Running\". Don't know why, but in logs I see they finished.\nSo I decided to stop them manually, and I see \"Stopping\".\nSome jobs are being stopped but did not change status\nSo I want to clean that and \"update the status\" manually --> let's mark them as stopped.",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649332613000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1478",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":7.1,
        "Challenge_reading_time":13.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Update jobs status from CLI or Client",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":167,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The first thing is the check the release notes with this kind of problems, as the issue could have been solved in subsequent releases.\n\nNow going back to the solution, It's better to use client to iterate over the runs and mark them as finished:\n\nfrom polyaxon.client import ProjectClient\n\npclient = ProjectClient(project=\"PROJECT_NAME\")\nfor r in pclient.list_runs(query=\"status: stopping\").results:\n    print(\"cleaning {}\".format(r.uuid)\n    RunClient(\"PROJECT_NAME\", run_uuid=r.uuid).log_stopped(message=\"manual cleaning\")",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.6,
        "Solution_reading_time":6.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":61.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1324129118823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11756.0,
        "Answerer_view_count":517.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created an EventBridge rule that triggers a Sagemaker Pipeline when someone uploads a new file to an S3 bucket. As new input files become available, they will be uploaded to the bucket for processing. I'd like the pipeline to process only the uploaded file, and so thought to pass in the S3 URL of the file as a parameter to the Pipeline. Since the full URL doesn't exist as a single field value in the S3 event, I was wondering if there is some way to concatenate multiple field values into a single parameter value that EventBridge will pass on to the target.<\/p>\n<p>For example, I know the name of the uploaded file can be sent from EventBridge using <code>$.detail.object.key<\/code> and the bucket name can be sent using <code>$.detail.bucket.name<\/code>, so I'm wondering if I can send both somehow to get something like this to the Sagemaker Pipeline <code>s3:\/\/my-bucket\/path\/to\/file.csv<\/code><\/p>\n<p>For what it's worth, I tried splitting the parameter into two (one being <code>s3:\/\/bucket-name\/<\/code> and the other being <code>default_file.csv<\/code>) when defining the pipeline, but got an error saying <code>Pipeline variables do not support concatenation<\/code> when combining the two into one.<\/p>\n<p>The relevant pipeline step is<\/p>\n<p><code>step_transform = TransformStep(name = &quot;Name&quot;, transformer=transformer,inputs=TransformInput(data=variable_of_s3_path)<\/code><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651705997730,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72120382",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":18.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS EventBridge: Add multiple event details to a target parameter",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":809.0,
        "Challenge_word_count":207,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324682328743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":969.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/eventbridge\/latest\/userguide\/eb-transform-target-input.html\" rel=\"nofollow noreferrer\">Input transformers<\/a> manipulate the event payload that EventBridge sends to the target.  Transforms consist of (1) an &quot;input path&quot; that maps substitution variable names to JSON-paths in the event and (2) a &quot;template&quot; that references the substitution variables.<\/p>\n<p>Input path:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;detail-bucket-name&quot;: &quot;$.detail.bucket.name&quot;,\n  &quot;detail-object-key&quot;: &quot;$.detail.object.key&quot;\n}\n<\/code><\/pre>\n<p>Input template that concatenates the s3 url and outputs it along with the original event payload:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;s3Url&quot;: &quot;s3:\/\/&lt;detail-bucket-name&gt;\/&lt;detail-object-key&gt;&quot;,\n  &quot;original&quot;: &quot;$&quot;\n}\n<\/code><\/pre>\n<p>Define the transform in the EventBridge console by editing the rule: <code>Rule &gt; Select Targets &gt; Additional Settings<\/code>.<\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.8,
        "Solution_reading_time":14.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":93.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello!  <\/p>\n<p>I created an Azure ML pipeline in Python and used multiple PythonScriptSteps for each of my tasks. For example, I have three training steps running in parallel, so I create three PythonScriptSteps in a for loop with my train.py script and different data. Later, I came across the <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-how-to-use-modulestep.ipynb\">ModuleStep<\/a>, which seems to do exactly this, but with an extra layer of (seemingly pointless) abstraction. What does the ModuleStep add to a PythonScriptStep?  <\/p>\n<p>Also, I imagined the ModuleStep might make it possible to use a custom PythonScriptStep in the pipeline designer (by creating a new drag and drop module), however this doesn't seem to be the case. Is there any way of doing this?   <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627306015463,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/489671\/what-is-the-point-of-azureml-modules",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":9.9,
        "Challenge_reading_time":11.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"What is the point of AzureML modules?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":124,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Just to close this question, I have since discovered that the ModuleStep <strong>does<\/strong> create a custom drag-and-drop module in the designer. I don't know if I'd missed this (I imagine so) or if this is a new feature. Either way, that's the answer. <a href=\"\/users\/na\/?userid=ad870133-9538-4d77-adc8-2b5ffc5c1b45\">@YutongTie-MSFT  <\/a> can you confirm if this was recently added?    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.5,
        "Solution_reading_time":4.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"In **Moon_Classification_Solution.ipynb**, the original code below would cause an error `ValueError: framework_version or py_version was None, yet image_uri was also None. Either specify both framework_version and py_version, or specify image_uri.` So I specified `py_version='py3'`, cause the framework version only supports `py2` and `py3`, which fixed the problem. Or I guess just add `!pip install sagemaker==1.72.0` like notebooks in another [**repo**](https:\/\/github.com\/udacity\/sagemaker-deployment\/blob\/master\/Mini-Projects\/IMDB%20Sentiment%20Analysis%20-%20XGBoost%20(Batch%20Transform)%20-%20Solution.ipynb) would also solve the issue.\r\n\r\n```\r\n# import a PyTorch wrapper\r\nfrom sagemaker.pytorch import PyTorch\r\n\r\n# specify an output path\r\n# prefix is specified above\r\noutput_path = 's3:\/\/{}\/{}'.format(bucket, prefix)\r\n\r\n# instantiate a pytorch estimator\r\nestimator = PyTorch(entry_point='train.py',\r\n                    source_dir='source_solution', # this should be just \"source\" for your code\r\n                    role=role,\r\n                    framework_version='1.0',\r\n                    py_version='py3', ### <------------------------ added a line here\r\n                    train_instance_count=1,\r\n                    train_instance_type='ml.c4.xlarge',\r\n                    output_path=output_path,\r\n                    sagemaker_session=sagemaker_session,\r\n                    hyperparameters={\r\n                        'input_dim': 2,  # num of features\r\n                        'hidden_dim': 20,\r\n                        'output_dim': 1,\r\n                        'epochs': 80 # could change to higher\r\n                    })\r\n```",
        "Challenge_closed_time":1623053.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619388470000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/udacity\/ML_SageMaker_Studies\/issues\/15",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":18.94,
        "Challenge_repo_contributor_count":8.0,
        "Challenge_repo_fork_count":428.0,
        "Challenge_repo_issue_count":16.0,
        "Challenge_repo_star_count":350.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"With \"sagemaker 2.31.1\", \"sagemaker.pytorch.PyTorch\" needs to specify both \"framework_version\" and \"py_version\"",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":136,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Resolved by fixing Sagemaker's version to 1.72.0.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":0.63,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":7.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Microsoft Azure Support,<\/p>\n<p>I am currently learning to use Azure Cloud, specifically Microsoft Azure Machine Learning Studio. I am using the example dataset &quot;Automobile price data&quot; and have successfully completed a classic training pipeline, which includes the following blocks: &quot;Select Columns in Dataset&quot;, &quot;Clean Missing Data&quot;, &quot;Normalize Data&quot;, &quot;Split Data&quot;, &quot;Linear Regression&quot;, &quot;Train Model&quot;, &quot;Score Model&quot;, and &quot;Evaluate Model&quot;. I have also successfully created a real-time inference pipeline, which includes an &quot;Enter Data Manually&quot; block, a &quot;Web Service Input&quot; block, an &quot;Execute Python Script&quot; block code, and a &quot;Web Service Output&quot; block.<\/p>\n<p>However, I am encountering an issue when I try to deploy the pipeline for inference using the &quot;Set up real-time endpoint&quot; window. Specifically, when I click on the &quot;Deploy&quot; button, nothing happens and the same window remains open. My setup is &quot;Deploy new real-time endpoint&quot; with the following details: Name: predict-auto-price, Description: Auto Price Regression, Compute type: Azure Container Instance.<\/p>\n<p>I have tried creating a new Workspace, as well as checking the access policies and permissions on the resources used in the pipeline and deployment, but the issue persists. I am using an Azure Free Account with 200 USD of credit, and I still have more than 190 USD of credit remaining.<\/p>\n<p>I would greatly appreciate your help in resolving this issue, as my goal is to finish the Coursera DP-100 specialization and apply for the Data Science Azure Certification.<\/p>\n<p>Thank you for your attention to this matter.<\/p>\n<p>--<\/p>\n<p>Carlos Alanis<\/p>\n<p>Attach some images: <\/p>\n<p><strong>Pipeline Completed without errors:<\/strong><\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/85dc896d-a94e-4372-8249-0b2de8dc9438?platform=QnA\" alt=\"real time inference pipeline completed\" \/><\/p>\n<p><strong>Coursera Instructions:<\/strong><\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/e1482fa9-bca7-44c0-a85a-4482807cbb95?platform=QnA\" alt=\"Coursera instructions\" \/><\/p>\n<p><strong>Set up real-time endpoint WINDOW:<\/strong> <\/p>\n<p><strong>DEPLOY BUTTON UNPRESSED:<\/strong> <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/75d73443-c414-4064-9a55-3589092a87c1?platform=QnA\" alt=\"set up real time endpoint - unpressed button\" \/><\/p>\n<p><strong>DEPLOY BUTTON PRESSED:<\/strong><\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/3f429b37-2bd9-4012-8d6d-2afdbcdf6b14?platform=QnA\" alt=\"set up real time endpoint - PRESSED DEPLOY button\" \/><\/p>\n<p><strong>BUT NOTHING HAPPENS!!!<\/strong> <\/p>\n<p><strong>ALSO I SHARE YOU THE WORKSPACE OVERVIEW:<\/strong> <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/cfd21661-f06c-4cd5-97ab-1ad2b953230e?platform=QnA\" alt=\"workspace overview\" \/><\/p>\n<p>I hope that some people of the expert team can Help me, Thank you! <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":11,
        "Challenge_created_time":1683692664093,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1281429\/issues-for-implementing-a-real-time-inference-pipe",
        "Challenge_link_count":5,
        "Challenge_participation_count":12,
        "Challenge_readability":13.9,
        "Challenge_reading_time":41.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"Issues for implementing a real-time inference pipeline! The DEPLOY button does nothing!",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":343,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=8f9f275d-8a35-4900-94e8-03effa4fecb8\">@Carlos Alanis  <\/a> <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=d8849772-db6e-4cbc-aac4-7e2bbe39dc8a\">Rania Boukhriss<\/a> <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=c23a88da-a1fe-4209-9289-829c28dfa73b\">Carlos Garc\u00eda Gonz\u00e1lez<\/a> <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=259d2834-c6dc-4038-b068-fe422681470d\">anzilparviz<\/a> <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=f1b6c4a4-3330-4115-9b7a-eb63e63a4faa\">SpaceBuddha<\/a> <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=c057ba5f-2c81-43c2-a518-cd25ef60490f\">jaime reinoso<\/a> <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=a5ece606-8fef-4154-982f-69ded88859d4\">Cesar Olivares Espinosa<\/a> The hotfix deployment is complete. Could you please retry the Deployment action? If you still face any issues please do let us know the region of your workspace. Thanks!! <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/3343ee39-8aff-4859-9a73-1afd64140fad?platform=QnA\" alt=\"test_deploy_hotfix2\" \/><\/p>\n<p>If this answers your query, do click <code>Accept Answer<\/code> and <code>Yes<\/code> for was this answer helpful. And, if you have any further query do let us know.<\/p>\n",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":14.2,
        "Solution_reading_time":17.89,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to perform some data transformations using the Python script module in Designer for which i would need to access some pipeline parameters. How can i get those values?  <\/p>\n<p>What would be the equivalent for an R script?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":4,
        "Challenge_created_time":1620522230407,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/387875\/how-can-i-access-azure-ml-pipeline-parameters-from",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":7.5,
        "Challenge_reading_time":4.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How can i access azure ml pipeline parameters from a python script running in designer?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=1b597528-b52c-41fc-932a-baf4d96cb15b\">@javier  <\/a>  Thanks, Currently passing a pipeline parameter to the script of Execute Python\/R Module is not supported. We have a new feature custom module which is in private preview. you can write your own module and use in Designer. If it's a common case, it might be better to use custom module.<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.4,
        "Solution_reading_time":4.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've got a basic ScriptStep in my AML Pipeline and it's just trying to read an attached dataset. When i execute this simple example, the pipeline fails with the following in the driver log:<\/p>\n\n<blockquote>\n  <p>ImportError: azureml-dataprep is not installed. Dataset cannot be used\n  without azureml-dataprep. Please make sure\n  azureml-dataprep[fuse,pandas] is installed by specifying it in the\n  conda dependencies. pandas is optional and should be only installed if\n  you intend to create a pandas DataFrame from the dataset.<\/p>\n<\/blockquote>\n\n<p>I then modified my step to include the conda package but then the driver fails with \"ResolvePackageNotFound: azureml-dataprep\". The entire log file can be accessed <a href=\"https:\/\/www.dropbox.com\/s\/372ht6jkvzu9loo\/conda.err.txt?dl=0\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<pre><code># create a new runconfig object\nrun_config = RunConfiguration()\nrun_config.environment.docker.enabled = True\nrun_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\nrun_config.environment.python.user_managed_dependencies = False\nrun_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['azureml-dataprep[pandas,fuse]'])\n\nsource_directory = '.\/read-step'\nprint('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))\nstep2 = PythonScriptStep(name=\"read_step\",\n                         script_name=\"Read.py\", \n                         arguments=[\"--dataFilePath\", dataset.as_named_input('local_ds').as_mount() ],\n                         compute_target=aml_compute, \n                         source_directory=source_directory,\n                         runconfig=run_config,\n                         allow_reuse=False)\n<\/code><\/pre>\n\n<p>I'm out of ideas, would deeply appreciate any help here!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587154334353,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1591825691630,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61279914",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":22.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureML: ResolvePackageNotFound azureml-dataprep",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1244.0,
        "Challenge_word_count":155,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330016065408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1704.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>The <code>azureml-sdk<\/code> isn't available on conda, you need to install it with <code>pip<\/code>.<\/p>\n\n<pre><code>myenv = Environment(name=\"myenv\")\nconda_dep = CondaDependencies().add_pip_package(\"azureml-dataprep[pandas,fuse]\")\nmyenv.python.conda_dependencies=conda_dep\nrun_config.environment = myenv\n<\/code><\/pre>\n\n<p>For more information, about this error, the logs tab has a log named <code>20_image_build_log.txt<\/code> which Docker build logs. It contains the error where <code>conda<\/code> failed to failed to find <code>azureml-dataprep<\/code><\/p>\n\n<p>EDIT:<\/p>\n\n<p>Soon, you won't have to specify this dependency anymore. the Azure Data4ML team says <code>azureml-dataprep[pandas,fuse]<\/code> is getting added as a dependency for <code>azureml-defaults<\/code> which is automatically installed on all images. <\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1587416360076,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":10.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":8,
        "Challenge_body":"<p>Hi all,    <\/p>\n<p>I'm trying to create an inference pipeline with the AML designer.     <br \/>\nI clicked on the &quot;Create inference pipeline&quot; button:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/205064-image.png?platform=QnA\" alt=\"205064-image.png\" \/>    <\/p>\n<p>and now I want to do some changes in the pipeline. I added at the end two more steps and linked the Webservice output component to the last step:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/205028-image.png?platform=QnA\" alt=\"205028-image.png\" \/>    <\/p>\n<p>I clicked on save and submit it.     <br \/>\nThe result is the following:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/204999-image.png?platform=QnA\" alt=\"204999-image.png\" \/>    <\/p>\n<p>The two new steps are present and executed, but the webservice output step is disappeared! I've tried multiple time with the same result.     <br \/>\nThe webservice input step is correctly present at the beginning of the pipeline.    <\/p>\n<p>Also, after making the change and saving correctly, if I exit and reopen the pipeline the step &quot;Web Service Output&quot; is no longer there    <\/p>\n<p>Can you help me?    <\/p>\n<p>Thanks!    <\/p>\n<p>G    <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1653385681273,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/861882\/azure-machine-learning-designer-webservice-input-o",
        "Challenge_link_count":3,
        "Challenge_participation_count":9,
        "Challenge_readability":9.6,
        "Challenge_reading_time":16.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Designer - Webservice input\/output disappear",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":158,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=061108cd-43c2-45e6-aefa-0aaa3ab2e335\">@Antonio  <\/a>,     <\/p>\n<p>Sorry for the inconvenience caused.    <br \/>\nThis is a known bug and we've fixed. Could you please retry to see if you can still repro? I tried from my side either manually build an inference pipeline or modify the auto-gen inference pipeline, the web service input\/output components are still there.     <\/p>\n<p>If you can still repro, could you please provide following info for us to investigate?    <\/p>\n<ul>\n<li> your inference pipeline draft URL    <\/li>\n<li>  inference pipeline job URL of which the webservice input\/output components disappear    <\/li>\n<li> Is your workspace in Vnet?    <\/li>\n<\/ul>\n<p>We're also happy to set up a call to investigate, could you please send me an email so that I can send the meeting request?     <br \/>\nWe're based in Beijing (UTC+8).    <\/p>\n<p>Thanks!     <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":10.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":135.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\ndoing\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 61`\r\n\r\nas described here https:\/\/aimstack.readthedocs.io\/en\/latest\/quick_start\/convert_data.html#show-mlflow-logs-in-aim\r\n\r\nfails with the following error\r\n\r\n![Screenshot from 2022-02-27 02-33-17](https:\/\/user-images.githubusercontent.com\/26168435\/155864827-dc7f3acb-0c79-4fab-9c79-a599f1a954ab.png)\r\n\r\nusing the experiment name instead of the experiment id\r\n\r\n![Screenshot from 2022-02-27 02-33-55](https:\/\/user-images.githubusercontent.com\/26168435\/155864887-63c19423-865e-4540-bfb7-c034e123af80.png)\r\n\r\ni.e.\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 'ai-vengers-collab'` \r\n\r\nworks:\r\n\r\n![Screenshot from 2022-02-27 02-31-46](https:\/\/user-images.githubusercontent.com\/26168435\/155864881-03434a11-68f8-47e3-90e3-13465cbe86b4.png)\r\n\r\n### To reproduce\r\n\r\nsee above\r\n\r\n### Expected behavior\r\n\r\nconvert the experiment by ID\r\n\r\n### Environment\r\n\r\n- Aim Version 3.6\r\n- Python 3.8.1\r\n- pip3\r\n- Ubuntu 20.04.3 LTS\r\n",
        "Challenge_closed_time":1649939.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645926561000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aimhubio\/aim\/issues\/1415",
        "Challenge_link_count":4,
        "Challenge_participation_count":6,
        "Challenge_readability":13.3,
        "Challenge_reading_time":14.53,
        "Challenge_repo_contributor_count":47.0,
        "Challenge_repo_fork_count":181.0,
        "Challenge_repo_issue_count":2399.0,
        "Challenge_repo_star_count":2909.0,
        "Challenge_repo_watch_count":35.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"aim convert mlflow --experiment fails for experiment id, works for experiment name",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey @luisoala, thanks for reporting the issue!\r\n@devfox-se could you please take a look at this? Thanks for reporting this @luisoala, will take a look soon! Hey @luisoala! We've released `v3.6.2` containing the fix for mlflow converter. Please check it out and let me know if there are any issues. thanks @alberttorosyan working through a few other deadlines atm, aiming for a test ~ next tuesday, will share result here @luisoala Hi, have you had a chance to test this?:) Closing due to inactivity, feel free to reopen in case this still persists.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":6.68,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":92.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1431605411263,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In Batch execution API help page of Azure Machine Learning there are three different URI\u2019s <\/p>\n\n<ul>\n<li>Submit Job (Response is Job ID)<\/li>\n<li>Start Job ( we need to use the above Job ID in this URI)<\/li>\n<li>Get Status or Result (we need to use the above Job ID in this URI)<\/li>\n<\/ul>\n\n<p>How do I automate these jobs in the Azure Scheduler? (i.e. if I want to execute the BES on the particular date<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1431519915817,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1431588219032,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30214698",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.9,
        "Challenge_reading_time":6.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How to schedule Batch Execution Service of Azure Machine Learning in Azure Scheduler",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":759.0,
        "Challenge_word_count":86,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403541426412,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, India",
        "Poster_reputation_count":191.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>You would use Azure Data Factory instead of the scheduler. This would allow you to schedule the BES call into the future while identifying where the result file will end up. <\/p>\n\n<p>There are lots of examples online on how to do that.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":2.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1286966860487,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Australia",
        "Answerer_reputation_count":55279.0,
        "Answerer_view_count":5321.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have just started to get myself acquainted with parallelism in R. <\/p>\n\n<p>As I am planning to use <a href=\"https:\/\/studio.azureml.net\/\" rel=\"nofollow\">Microsoft Azure Machine Learning Studio<\/a> for my project, I have started investigating what <a href=\"https:\/\/mran.revolutionanalytics.com\/documents\/rro\/multithread\/\" rel=\"nofollow\">Microsoft R Open<\/a> offers for parallelism, and thus, I found <a href=\"https:\/\/mran.revolutionanalytics.com\/documents\/rro\/multithread\/\" rel=\"nofollow\">this<\/a>, in which it says that parallelism is done under the hood that leverages the benefit of all available cores, without changing the R code. The article also shows some performance benchmarks, however, most of them demonstrate the performance benefit in doing mathematical operations.<\/p>\n\n<p>This was good so far. In addition, I am also interested to know whether it also parallelize the <code>*apply<\/code> functions under the hood or not. I also found these 2 articles that describes how to parallelize <code>*apply<\/code> functions in general:<\/p>\n\n<ol>\n<li><a href=\"https:\/\/www.r-bloggers.com\/quick-guide-to-parallel-r-with-snow\/\" rel=\"nofollow\">Quick guide to parallel R with snow<\/a>: describes facilitating parallelism using <a href=\"https:\/\/cran.r-project.org\/web\/packages\/snow\/snow.pdf\" rel=\"nofollow\"><code>snow<\/code><\/a> package, <code>par*apply<\/code> function family, and <code>clusterExport<\/code>.<\/li>\n<li><a href=\"http:\/\/www.win-vector.com\/blog\/2016\/01\/parallel-computing-in-r\/\" rel=\"nofollow\">A gentle introduction to parallel computing in R<\/a>: using <code>parallel<\/code> package, <code>par*apply<\/code> function family, and binding values to environment.<\/li>\n<\/ol>\n\n<p>So my question is when I will be using <code>*apply<\/code> functions in Microsoft Azure Machine Learning Studio, will that be parallelized under the hood by default, or I need to make use of packages like <code>parallel<\/code>, <code>snow<\/code> etc.?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1476002256777,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39941622",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":14.9,
        "Challenge_reading_time":26.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Parallel *apply in Azure Machine Learning Studio",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":501.0,
        "Challenge_word_count":221,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1365684640140,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Paderborn, Germany",
        "Poster_reputation_count":4588.0,
        "Poster_view_count":453.0,
        "Solution_body":"<p>Personally, I think we could have marketed MRO a bit differently, without making such a big deal about parallelism\/multithreading. Ah well.<\/p>\n\n<p>R comes with an Rblas.dll\/.so which implements the routines used for linear algebra computations. These routines are used in various places, but one common use case is for fitting regression models. With MRO, we replace the standard Rblas with one that uses the <a href=\"https:\/\/software.intel.com\/en-us\/intel-mkl\" rel=\"noreferrer\">Intel Math Kernel Library<\/a>. When you call a function like <code>lm<\/code> or <code>glm<\/code>, MRO will use multiple threads and optimized CPU instructions to fit the model, which can get you dramatic speedups over the standard implementation.<\/p>\n\n<p>MRO isn't the only way you can get this sort of speedup; you can also compile\/download other BLAS implementations that are similarly optimized. We just make it an easy one-step download.<\/p>\n\n<p>Note that the MKL only affects code that involves linear algebra. It isn't a general-purpose speedup tool; any R code that doesn't do matrix computations won't see a performance improvement. In particular, it won't speed up any code that involves <em>explicit<\/em> parallelism, such as code using the parallel package, SNOW, or other cluster computing tools.<\/p>\n\n<p>On the other hand, it won't <em>degrade<\/em> them either. You can still use packages like parallel, SNOW, etc to create compute clusters and distribute your code across multiple processes. MRO works just like regular CRAN R in this respect. (One thing you might want to do, though, if you're creating a cluster of nodes on the one machine, is reduce the number of MKL threads. Otherwise you risk contention between the nodes for CPU cores, which will degrade performance.)<\/p>\n\n<p>Disclosure: I work for Microsoft.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.2,
        "Solution_reading_time":22.65,
        "Solution_score_count":5.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":272.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":3528.4438297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a <code>sagemaker.workflow.pipeline.Pipeline<\/code> object, in which, there are couple of processing step where I am trying to reference to an s3 file path rather than a local file path, so that it won't upload files to s3 everytime the pipeline runs.<\/p>\n<p>My question is, can I modify the <code>step<\/code> or <code>scriptprocessor<\/code> or <code>pipeline<\/code> object so that I can reference a code from artifact created from AWS Codebuild?<\/p>\n<p>If not, can I use codebuild to first copy my local file to a specific S3 position (I am having permission issue so far) and then run the pipeline?<\/p>\n<p>As your reference<\/p>\n<pre><code>...\nstep_data_ingest = ProcessingStep(\n        name=&quot;DataIngestion&quot;,\n        processor=sklearn_data_ingest_processor,\n        inputs=[\n            ProcessingInput(\n                input_name=&quot;input_train_data&quot;,\n                source=input_data, \n                destination=&quot;\/opt\/ml\/processing\/input\/data\/train&quot;\n            ),\n            ProcessingInput(\n                input_name=&quot;input_test_data&quot;,\n                source=test_data, \n                destination=&quot;\/opt\/ml\/processing\/input\/data\/test&quot;\n            ),\n            ProcessingInput(\n                input_name=&quot;requirement_file&quot;,\n                source=os.path.join(code_dir, &quot;requirements.txt&quot;), \n                destination=&quot;\/opt\/ml\/processing\/input\/requirement&quot;\n            ),\n        ],\n        outputs=[\n            ProcessingOutput(\n                output_name=&quot;train&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/train&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/train&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;validation&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/validation&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/validation&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;test&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/test&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/test&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;sample&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/sample&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/sample&quot;)\n            ),\n        ],\n        code=os.path.join(code_dir, &quot;data_ingestion.py&quot;),\n        # something like s3:\/\/some_code_dir\/data_ingestion.py\n        job_arguments = [&quot;-c&quot;, country, \n                         &quot;-v&quot;, train_val_split_percentage],\n    )\n...\n<\/code><\/pre>\n<p>What I expect to do is something like:<\/p>\n<pre><code># in processing step or processor\nProcessingStep(\n    ...\n    code=&quot;data_ingestion.py&quot;\n    code_location=&quot;s3:\/\/some_artifact_bucket\/buildartifact\/fdskz.zip&quot;\n\n    ...\n)\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code># in processing step or processor\nProcessingStep(\n    ...\n    code=&quot;s3:\/\/some_artifact_bucket\/buildartifact\/fdsix\/data_ingestion.py&quot;\n    ...\n)\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code># in buildspec.yml for codebuild\naws s3 sync .\/code_dir\/ s3:\/\/some_code_dir\/\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615324569760,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66554893",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":21.6,
        "Challenge_reading_time":38.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker Workflow pIpeline use the code stored in artifact created from Codebuild",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":156.0,
        "Challenge_word_count":204,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1575485255683,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>When using the <code>ProcessingStep<\/code>, you can use an <code>S3 URI<\/code> as the code location, take a look on <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/9fc57555bba4fc1d33064478dc209a84a6726c57\/src\/sagemaker\/workflow\/steps.py#L374\" rel=\"nofollow noreferrer\">this<\/a> for reference.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1628026967547,
        "Solution_link_count":1.0,
        "Solution_readability":18.6,
        "Solution_reading_time":4.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":24.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>There is very less components compared to classical mode, how can I use prebuilt components in custom mode? Is that possible? How should I get it? <\/p>\n<p>Can I get some help here? Much appreciated.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1680044853140,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1194061\/can-i-use-prebuilt-component-in-custom-pipeline-mo",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.5,
        "Challenge_reading_time":3.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"can I use prebuilt component in custom pipeline mode?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":43,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello @merten<\/p>\n<p>Thanks for reaching out to us, as you know Designer supports two type of components, classic prebuilt components and custom components. These two types of components <strong>are not compatible. So a quick answer for your question is you can not use it together.<\/strong><\/p>\n<p>Classic prebuilt components provides prebuilt components majorly for data processing and traditional machine learning tasks like regression and classification. This type of component continues to be supported but will not have any new components added.<\/p>\n<p>Custom components allow you to provide your own code as a component. It supports sharing across workspaces and seamless authoring across Studio, CLI, and SDK interfaces.<\/p>\n<p>I am sorry for all inconveniences. If you can share more details about your scenario, we are happy to discuss with product team.<\/p>\n<p>Regards,<\/p>\n<p>Yutong<\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":12.56,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":147.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was following the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-python-sdk\" rel=\"nofollow noreferrer\">SDK v2 Python tutorial<\/a> in order to create a pipeline job with my own assets. I notice that in this tutorial they let you use a csv file that can be downloaded but Im trying to use a registered dataset that I already registered by my own. The problem that I facing is that I dont know where I need to specify the dataset.<\/p>\n<p>The funny part is that at the beginning they create this dataset like this:<\/p>\n<pre><code>credit_data = ml_client.data.create_or_update(credit_data)\nprint(\n    f&quot;Dataset with name {credit_data.name} was registered to workspace, the dataset version is {credit_data.version}&quot;\n)\n<\/code><\/pre>\n<p>But the only part where they refer to this dataset is on the last part where they # the line:<\/p>\n<pre><code>registered_model_name = &quot;credit_defaults_model&quot;\n\n# Let's instantiate the pipeline with the parameters of our choice\npipeline = credit_defaults_pipeline(\n    # pipeline_job_data_input=credit_data,\n    pipeline_job_data_input=Input(type=&quot;uri_file&quot;, path=web_path),\n    pipeline_job_test_train_ratio=0.2,\n    pipeline_job_learning_rate=0.25,\n    pipeline_job_registered_model_name=registered_model_name,\n)\n<\/code><\/pre>\n<p>For me this means that I can use this data like this (a already registered dataset), the problem is that I don't know where I need to do the changes (I know that in the data_prep.py and in the code below but I don\u00b4t know where else) and I don't know how to set this:<\/p>\n<pre><code>%%writefile {data_prep_src_dir}\/data_prep.py\n...\n\ndef main():\n    &quot;&quot;&quot;Main function of the script.&quot;&quot;&quot;\n\n    # input and output arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(&quot;--data&quot;, type=str, help=&quot;path to input data&quot;) # &lt;=== Here, but I don\u00b4t know how\n    parser.add_argument(&quot;--test_train_ratio&quot;, type=float, required=False, default=0.25)\n    parser.add_argument(&quot;--train_data&quot;, type=str, help=&quot;path to train data&quot;)\n    parser.add_argument(&quot;--test_data&quot;, type=str, help=&quot;path to test data&quot;)\n    args = parser.parse_args()\n\n...\n<\/code><\/pre>\n<p>Does anyone have experience working as registered datasets?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656688618547,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1657482591390,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72831360",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":30.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Use dataset registed in on pipelines in AML",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":85.0,
        "Challenge_word_count":259,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1636569000947,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":9.0,
        "Poster_view_count":2.0,
        "Solution_body":"<blockquote>\n<p>parser.add_argument(&quot;--data&quot;, type=str, help=&quot;path to input data&quot;) # &lt;=== Here, but I don\u00b4t know how<\/p>\n<\/blockquote>\n<p>To get the path to input data, according to <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/how-to-train-with-datasets.md\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n<ul>\n<li><p>You can get <code>--input-data<\/code> by ID which you can access in your training script.<\/p>\n<\/li>\n<li><p>Use it as <code>argument<\/code> on <code>mounted_input_path<\/code><\/p>\n<\/li>\n<\/ul>\n<p>For example, try the following three code snippets taken from the <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/how-to-train-with-datasets.md\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n<p><strong>Access dataset in training script:<\/strong><\/p>\n<pre><code>parser = argparse.ArgumentParser()\nparser.add_argument(&quot;--input-data&quot;, type=str)\nargs = parser.parse_args()\n\nrun = Run.get_context()\nws = run.experiment.workspace\n\n# get the input dataset by ID\ndataset = Dataset.get_by_id(ws, id=args.input_data)\n<\/code><\/pre>\n<p><strong>Configure the training run:<\/strong><\/p>\n<pre><code>src = ScriptRunConfig(source_directory=script_folder,\n                      script='train_titanic.py',\n                      # pass dataset as an input with friendly name 'titanic'\n                      arguments=['--input-data', titanic_ds.as_named_input('titanic')],\n                      compute_target=compute_target,\n                      environment=myenv)\n<\/code><\/pre>\n<p><strong>Pass <code>mounted_input_path<\/code> as argument:<\/strong><\/p>\n<pre><code>mounted_input_path = sys.argv[1]\nmounted_output_path = sys.argv[2]\n\nprint(&quot;Argument 1: %s&quot; % mounted_input_path)\nprint(&quot;Argument 2: %s&quot; % mounted_output_path)\n<\/code><\/pre>\n<p>References: <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/v1\/how-to-create-register-datasets.md\" rel=\"nofollow noreferrer\">How to create register dataset<\/a> and <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/datasets-tutorial\/scriptrun-with-data-input-output\/how-to-use-scriptrun.ipynb\" rel=\"nofollow noreferrer\">How to use configure a training run with data input and output<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":21.5,
        "Solution_reading_time":30.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":155.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1657058369727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If we use multiple instances for training will the built-in algorithm automatically exploit it? For example, what if we used 2 instances for training using built-in XGBoost container and we used the same customer churn example? Will one instance be ignored?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662834395330,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1662834724687,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73674278",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":4.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use multiple instances with the SageMaker XGBoost built-in algorithm?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":19.0,
        "Challenge_word_count":51,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1507661294190,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":98.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Yes SageMaker XGBoost supports distributed training. If you set instance count &gt; 1, SageMaker XGBoost will distribute the files from S3 to individual instances and perform distributed training. This, however, requires number of files on S3 &gt;= number of instances. Otherwise, you will be charged for using two training instances without the benefit of using distributed training.<\/p>\n<p>You can find an example here<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.6,
        "Solution_reading_time":10.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1579718832727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":149.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I can see experiment 2 is in deleted, but when it will be deleted actually?<\/p>\n\n<pre><code>2   test    hdfs:\/\/\/1234\/mlflow deleted\n<\/code><\/pre>\n\n<p>If the experiment is not deleted automatically, how can I delete it?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579189296547,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59773167",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":3.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"When will experiment be deleted with lifecycle_stage is set as deleted",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":412.0,
        "Challenge_word_count":42,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408370821672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":2521.0,
        "Poster_view_count":197.0,
        "Solution_body":"<p>I am assuming you use sql store?<\/p>\n\n<p>Currently there is no way to tell mlflow to hard-delete experiments. We are working with open source contributors to add a cli command that would perform garbage-collection of deleted experiments. This should be added soon in one of the upcoming mlflow releases. In the meantime, you can connect to your sql store and delete the experiments manually.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":4.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":64.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1465222092252,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Z\u00fcrich, Switzerland",
        "Answerer_reputation_count":1414.0,
        "Answerer_view_count":478.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Kubeflow pipelines (KFP) with GCP Vertex AI pipelines. I am using <code>kfp==1.8.5<\/code> (kfp SDK) and <code>google-cloud-pipeline-components==0.1.7<\/code>. Not sure if I can find which version of Kubeflow is used on GCP.<\/p>\n<p>I am bulding a component (yaml) using python inspired form this <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3748#issuecomment-627698554\" rel=\"nofollow noreferrer\">Github issue<\/a>. I am defining an output like:<\/p>\n<pre><code>outputs=[(OutputSpec(name='drt_model', type='Model'))]\n<\/code><\/pre>\n<p>This will be a base output directory to store few artifacts on Cloud Storage like model checkpoints and model.<\/p>\n<p>I would to keep one base output directory but add sub directories depending of the artifact:<\/p>\n<ul>\n<li>&lt;output_dir_base&gt;\/model<\/li>\n<li>&lt;output_dir_base&gt;\/checkpoints<\/li>\n<li>&lt;output_dir_base&gt;\/tensorboard<\/li>\n<\/ul>\n<p>but I didn't find how to concatenate the <strong>OutputPathPlaceholder('drt_model')<\/strong> with a string like <strong>'\/model'<\/strong>.<\/p>\n<p>How can append extra folder structure like \/model or \/tensorboard to the OutputPathPlaceholder that KFP will set during run time ?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1634924192977,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69681031",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.4,
        "Challenge_reading_time":16.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"how to concatenate the OutputPathPlaceholder with a string with Kubeflow pipelines?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":312.0,
        "Challenge_word_count":143,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1465222092252,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Z\u00fcrich, Switzerland",
        "Poster_reputation_count":1414.0,
        "Poster_view_count":478.0,
        "Solution_body":"<p>I didn't realized in the first place that <code>ConcatPlaceholder<\/code> accept both Artifact and string. This is exactly what I wanted to achieve:<\/p>\n<pre><code>ConcatPlaceholder([OutputPathPlaceholder('drt_model'), '\/model'])\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.2,
        "Solution_reading_time":3.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi is it possible to implement Deep Reinforcement Learning for structured data frames? If son can someone help me with an example?",
        "Challenge_closed_time":1650461.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649832960000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Deep-Reinforcement-Learning\/m-p\/413277#M269",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":1.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Deep Reinforcement Learning",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":91.0,
        "Challenge_word_count":24,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Deep Learning delivers a seamless notebook experience with integrated support for JupyterLab[1], so you can load data frames as a normal notebook. Additionally, it depends on what instance you are using Deep Learning.\u00a0\n\nIf you are using TensorFlow, you can see this[2] to know how to load a data frame to TensorFlow.\n\nIf you are using Pytorch tensor, you can see this[3] example of how to load the data frame.\n\n\u00a0\n\n[1] https:\/\/cloud.google.com\/deep-learning-vm\/docs\/jupyter\u00a0\n\n[2] https:\/\/www.tensorflow.org\/tutorials\/load_data\/pandas_dataframe\u00a0\n\n[3] https:\/\/stackoverflow.com\/a\/50308132\/16929358\u00a0\n\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.2,
        "Solution_reading_time":7.84,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":81.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1546882781360,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":106.0,
        "Answerer_view_count":41.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The output folder of an annotation job contains the following file structure: <\/p>\n\n<ul>\n<li><p>active learning<\/p><\/li>\n<li><p>annotation-tools<\/p><\/li>\n<li><p>annotations<\/p><\/li>\n<li><p>intermediate<\/p><\/li>\n<li><p>manifests<\/p><\/li>\n<\/ul>\n\n<p>Each line of the manifests\/output\/output.manifest file is a dictionary, where the key 'jobname' contains information about the annotations, and the key 'jobname-metadata' contains confidence score and other information about each of the bounding box annotations. There is also another folder called annotations which contain json files which contain information about annotations and associated worker ids. How are the two annotation informations related to each other? Is there any blogs\/tutorials which discuss how to interpret the data received from amazon sagemaker ground-truth service? Thanks in advance. <\/p>\n\n<p>Links I referred to: \n1. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-output.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-output.html<\/a>\n2. <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb<\/a> <\/p>\n\n<p>I have displayed the annotations received using the code available in the link 2 <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, which treats consolidated annotations and worker response separately.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562114234297,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56861525",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":22.5,
        "Challenge_reading_time":25.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Interpretation of output of bounding box annotation job",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":199.0,
        "Challenge_word_count":155,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562111269163,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Thank you for your question. I\u2019m the product manager for Amazon SageMaker Ground Truth and am happy to answer your question here.<\/p>\n\n<p>We have a feature called annotation consolidation that takes the responses from multiple workers for a single image and then consolidates those responses into a single set of bounding boxes for the image. The bounding boxes referenced in the manifest file are the consolidated responses whereas what you see in the annotations folders are the raw annotations (which is why you have the respective worker IDs). <\/p>\n\n<p>You can find out more about the annotation consolidation feature here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-annotation-consolidation.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-annotation-consolidation.html<\/a><\/p>\n\n<p>Please let us know if you have any further questions.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.4,
        "Solution_reading_time":11.52,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":114.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Azure ML the input data has to be defined as a Dataset (to create a pipeline). In my code I am passing datasets with the following syntax: input_data = Dataset.File.from_files(datapath)  <\/p>\n<p>I would like to change this datapath as an input parameter from Data Factory (for example via PipelineParamater), so I can apply the same Data Factory pipeline for different datasets. However, in Data Factory you can only pass string as a parameter, not a DataPath.  <\/p>\n<p>What is the solution around this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1626444050490,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/479034\/passing-data-from-azure-data-factory-into-azure-ml",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.2,
        "Challenge_reading_time":6.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Passing data from Azure Data Factory into Azure ML",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":91,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=db220306-9cc7-45e6-8626-e4713f3f5baf\">@Ilze Amanda   <\/a> ,    <\/p>\n<p>Thank you for posting your query on Microsoft Q&amp;A Portal and sharing clarifications on ask.    <\/p>\n<blockquote>\n<p>Unfortunately, we cannot create user defined types in Azure data factory at this moment.    <\/p>\n<\/blockquote>\n<p>But, I will encourage you to log your feedback using below link. Product team will actively monitor feedback there and consider them for future releases. Thank you.    <br \/>\n<a href=\"https:\/\/feedback.azure.com\/forums\/270578-data-factory\">https:\/\/feedback.azure.com\/forums\/270578-data-factory<\/a>     <\/p>\n<p>Hope this will help.    <\/p>\n<p>--------------------------------------    <\/p>\n<ul>\n<li> Please <code>accept an answer<\/code> if correct. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>.    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>.     <\/li>\n<\/ul>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.0,
        "Solution_reading_time":15.35,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":124.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":1.6854797222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am deploying a trained model to an ACI endpoint on Azure Machine Learning, using the Python SDK.\nI have created my score.py file, but I would like that file to be called with an argument being passed (just like with a training file) that I can interpret using <code>argparse<\/code>.\nHowever, I don't seem to find how I can pass arguments\nThis is the code I have to create the InferenceConfig environment and which obviously does not work.  Should I fall back on using the extra Docker file steps or so?<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment('my_hosted_environment')\nenv.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['scikit-learn'],\n    pip_packages=['azureml-defaults'])\nscoring_script = 'score.py --model_name ' + model_name\ninference_config = InferenceConfig(entry_script=scoring_script, environment=env)\n<\/code><\/pre>\n\n<p>Adding the score.py for reference on how I'd love to use the arguments in that script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#removed imports\nimport argparse\n\ndef init():\n    global model\n\n    parser = argparse.ArgumentParser(description=\"Load sklearn model\")\n    parser.add_argument('--model_name', dest=\"model_name\", required=True)\n    args, _ = parser.parse_known_args()\n\n    model_path = Model.get_model_path(model_name=args.model_name)\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    try:\n        data = json.loads(raw_data)['data']\n        data = np.array(data)\n        result = model.predict(data)\n        return result.tolist()\n\n    except Exception as e:\n        result = str(e)\n        return result\n<\/code><\/pre>\n\n<p>Interested to hear your thoughts<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1583933260433,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1584005920356,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60637170",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":12.7,
        "Challenge_reading_time":23.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"How to pass arguments to scoring file when deploying a Model in AzureML",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1681.0,
        "Challenge_word_count":196,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360655430743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":2947.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>How to deploy using environments can be found here <a href=\"https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2FAzure%2FMachineLearningNotebooks%2Fblob%2Fmaster%2Fhow-to-use-azureml%2Fdeployment%2Fdeploy-to-cloud%2Fmodel-register-and-deploy.ipynb&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7Ce06d310b0447416ab46b08d7bc836a81%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637185146436156499&amp;sdata=uQo332dpjuiNqWFCguvs3Kgg7UUMN8MBEzLxTPyH4MM%3D&amp;reserved=0\" rel=\"nofollow noreferrer\">model-register-and-deploy.ipynb<\/a> .  InferenceConfig class accepts  source_directory and entry_script <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py#parameters\" rel=\"nofollow noreferrer\">parameters<\/a>, where source_directory  is a path to the folder that contains all files(score.py and any other additional files) to create the image. <\/p>\n\n<p>This <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">multi-model-register-and-deploy.ipynb<\/a> has code snippets on how to create InferenceConfig with source_directory and entry_script.<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\n\nmyenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\ninference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                       name='sklearn-mnist-svc',\n                       models=[model], \n                       inference_config=inference_config,\n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n\nprint(service.scoring_uri)\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1584011988083,
        "Solution_link_count":3.0,
        "Solution_readability":35.1,
        "Solution_reading_time":25.25,
        "Solution_score_count":-2.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":89.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1327570314367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation_count":2854.0,
        "Answerer_view_count":324.0,
        "Challenge_adjusted_solved_time":30.6574666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are deploying a data consortium between more than 10 companies. Wi will deploy several machine learning models (in general advanced analytics models) for all the companies and we will administrate all the models. We are looking for a solution that administrates several servers, clusters and data science pipelines. I love kedro, but not sure what is the best option to administrate all while using kedro.<\/p>\n<p>In summary, we are looking for the best solution to administrate several models, tasks and pipelines in different servers and possibly Spark clusters. Our current options are:<\/p>\n<ul>\n<li><p>AWS as our data warehouse and Databricks for administrating servers, clusters and tasks. I don't feel that the notebooks of databricks are a good solution for building pipelines and to work collaboratively, so I would like to connect kedro to databricks (is it good? is it easy to schedule the run of the kedro pipelines using databricks?)<\/p>\n<\/li>\n<li><p>Using GCP for data warehouse and use kubeflow (iin GCP) for deploying models and the administration and the schedule of the pipelines and the needed resources<\/p>\n<\/li>\n<li><p>Setting up servers from ASW or GCP, install kedro and schedule the pipelines with airflow (I see a big problem administrating 20 servers and 40 pipelines)<\/p>\n<\/li>\n<\/ul>\n<p>I would like to know if someone knows what is the best option between these alternatives, their  downsides and advantages, or if there are more possibilities.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605830422657,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1605836750283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64921833",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":19.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"DataBricks + Kedro Vs GCP + Kubeflow Vs Server + Kedro + Airflow",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":967.0,
        "Challenge_word_count":241,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605828724552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":59.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I'll try and summarise what I know, but be aware that I've not been part of a KubeFlow project.<\/p>\n<h2>Kedro on Databricks<\/h2>\n<p>Our approach was to build our project with CI and then execute the pipeline from a notebook. We <em>did not<\/em> use the <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/11_tools_integration\/03_databricks.html\" rel=\"nofollow noreferrer\">kedro recommended approach<\/a> of using databricks-connect due to the <a href=\"https:\/\/databricks.com\/product\/aws-pricing\" rel=\"nofollow noreferrer\">large price difference<\/a> between Jobs and Interactive Clusters (which are needed for DB-connect). If you're working on several TB's of data, this quickly becomes relevant.<\/p>\n<p>As a DS, this approach may feel natural, as a SWE though it does not. Running pipelines in notebooks feels hacky. It works but it feels non-industrialised. Databricks performs well in automatically spinning up and down clusters &amp; taking care of the runtime for you. So their value add is abstracting IaaS away from you (more on that later).<\/p>\n<h2>GCP &amp; &quot;Cloud Native&quot;<\/h2>\n<p><strong>Pro<\/strong>: GCP's main selling point is BigQuery. It is an incredibly powerful platform, simply because you can be productive from day 0. I've seen people build entire web API's on top of it. KubeFlow isn't tied to GCP so you could port this somewhere else later on. Kubernetes will also allow you to run anything else you wish on the cluster, API's, streaming, web services, websites, you name it.<\/p>\n<p><strong>Con<\/strong>: Kubernetes is complex. If you have 10+ engineers to run this project long-term, you should be OK. But don't underestimate the complexity of Kubernetes. It is to the cloud what Linux is to the OS world. Think log management, noisy neighbours (one cluster for web APIs + batch spark jobs), multi-cluster management (one cluster per department\/project), security, resource access etc.<\/p>\n<h2>IaaS server approach<\/h2>\n<p>Your last alternative, the manual installation of servers is one I would recommend only if you have a large team, extremely large data and are building a long-term product who's revenue can sustain the large maintenance costs.<\/p>\n<h2>The people behind it<\/h2>\n<p>How does the talent market look like in your region? If you can hire experienced engineers with GCP knowledge, I'd go for the 2nd solution. GCP is a mature, &quot;native&quot; platform in the sense that it abstracts a lot away for customers. If your market has mainly AWS engineers, that may be a better road to take. If you have a number of kedro engineers, that also has relevance. Note that kedro is agnostic enough to run anywhere. It's really just python code.<\/p>\n<p><strong>Subjective advise<\/strong>:<\/p>\n<p>Having worked mostly on AWS projects and a few GCP projects, I'd go for GCP. I'd use the platform's components (BigQuery, Cloud Run, PubSub, Functions, K8S) as a toolbox to choose from and build an organisation around that. Kedro can run in any of these contexts, as a triggered job by the Scheduler, as a container on Kubernetes or as a ETL pipeline bringing data into (or out of) BigQuery.<\/p>\n<p>While Databricks is &quot;less management&quot; than raw AWS, it's still servers to think about and VPC networking charges to worry over. BigQuery is simply GB queried. Functions are simply invocation count. These high level components will allow you to quickly show value to customers and you only need to go deeper (RaaS -&gt; PaaS -&gt; IaaS) as you scale.<\/p>\n<p>AWS also has these higher level abstractions over IaaS but in general, it appears (to me) that Google's offering is the most mature. Mainly because they have published tools they've been using internally for almost a decade whereas AWS has built new tools for the market. AWS is the king of IaaS though.<\/p>\n<p>Finally, a bit of content, <a href=\"https:\/\/youtu.be\/kjhXMTOLtac?t=618\" rel=\"nofollow noreferrer\">two former colleagues have discussed ML industrialisation frameworks earlier this fall<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1605947117163,
        "Solution_link_count":3.0,
        "Solution_readability":7.9,
        "Solution_reading_time":49.9,
        "Solution_score_count":4.0,
        "Solution_sentence_count":41.0,
        "Solution_word_count":606.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1305851487736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Challenge_adjusted_solved_time":80.7456638889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In my workflow, I do the following:<\/p>\n<ol>\n<li>Acquire raw data (e.g. a video containing people)<\/li>\n<li>Transform it (e.g. automatically extract all crops with faces)<\/li>\n<li>Manually label them (e.g. identify the person in each crop). The labels are stored in json files along with the crops.<\/li>\n<li>Train a model on these data.<\/li>\n<\/ol>\n<p><strong>How should I track this pipeline with DVC?<\/strong><\/p>\n<p>My concerns:<\/p>\n<ol>\n<li>If stage 2 is changed (e.g. crops are extracted with a different size), the manual data should be invalidated (and so should the final model).<\/li>\n<li>The 3rd step is manual and therefore not precisely reproducible. But I do need its input to be reproducible.<\/li>\n<li>Stage 4 has an element of randomness, so it's not precisely reproducible either.<\/li>\n<\/ol>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1655411665790,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1655415427550,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72651603",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.1,
        "Challenge_reading_time":10.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Adding files that rely on pipeline outputs",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":128,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>Stage 3 is manual so you can't really codify it or automate it, nor guarantee its reproducibility (due to possible human error). But there's a way to get you as close as possible:<\/p>\n<p>You could replace it with a helper script that just checks whether all the labels are annotated. If so, output a text file with content &quot;green&quot;, otherwise &quot;red&quot; (for example) and error out.<\/p>\n<p>Stage 4 should depend on both the inputs from stages 2 and 3, so it will only run if BOTH the face crops changed AND if they are thoroughly annotated.\nInternally, it first checks the semaphore file (from 3) and dies on red. On green, it trains the model :)<\/p>\n<p>The <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">DAG<\/a> looks like this:<\/p>\n<pre><code>          +-----------+       \n          | 1-acquire |       \n          +-----------+       \n                *          \n                *          \n                *          \n          +---------+       \n          | 2-xform |       \n          +---------+       \n you      **        **     \n   --&gt;  **            **   \n       *                ** \n+---------+               *\n| 3-check |             ** \n+---------+           **   \n          **        **     \n            **    **       \n              *  *         \n          +---------+      \n          | 4-train |      \n          +---------+      \n<\/code><\/pre>\n<blockquote>\n<p>re randomness: while not ideal, non-determinism technically only <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#avoiding-unexpected-behavior\" rel=\"nofollow noreferrer\">affects intermediate stages<\/a> of the pipeline, because it causes everything after that to always run. In this case, since it's in the last stage, it won't affect DVC's job.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1655706111940,
        "Solution_link_count":2.0,
        "Solution_readability":12.0,
        "Solution_reading_time":17.8,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":174.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1619402503747,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am currently working on creating a Sagemaker Pipeline to train a Tensorflow model. I'm new to this area and I have been following <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-pipelines\/tabular\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint.ipynb\" rel=\"nofollow noreferrer\">this guide<\/a> created by AWS as well as the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html\" rel=\"nofollow noreferrer\">standard pipeline workflow<\/a> listed in the Sagemaker developer guide.<\/p>\n<p>I have a pipeline that runs without error when I only include the preprocessing, training, evaluation, and condition steps. When I add the register step:<\/p>\n<pre><code># Package evaluation metrics into an evaluation report `PropertyFile`\nevaluation_report = PropertyFile(\n        name=&quot;EvaluationReport&quot;, output_name=&quot;evaluation&quot;, path=&quot;evaluation_report.json&quot;\n)\n\n# Create ModelMetrics object using the evaluation report from the evaluation step\n# A ModelMetrics object contains metrics captured from a model.\nmodel_metrics = ModelMetrics(model_statistics=evaluation_report)\n\n# Create a RegisterModel step, which registers the model with Sagemaker Model Registry.\nregister_step = RegisterModel(\n    name=&quot;Foo&quot;,\n    estimator=estimator,\n    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n    content_types=[&quot;text\/csv&quot;],\n    response_types=[&quot;text\/csv&quot;],\n    inference_instances=config[&quot;instance&quot;][&quot;inference&quot;],\n    transform_instances=config[&quot;instance&quot;][&quot;transform&quot;],\n    model_package_group_name=&quot;Bar&quot;,\n    model_metrics=model_metrics,\n    approval_status=&quot;approved&quot;,\n)\n<\/code><\/pre>\n<p>to the condition step's <code>if_steps<\/code>:<\/p>\n<pre><code># Create a Sagemaker Pipelines ConditionStep, using the condition above.\n# Enter the steps to perform if the condition returns True \/ False.\ncond_step = ConditionStep(\n    name=&quot;MSE-Lower-Than-Threshold-Condition&quot;,\n    conditions=[cond_lte],\n    if_steps=[register_step],\n    else_steps=[],\n)\n<\/code><\/pre>\n<p>I get the following trace:<\/p>\n<pre><code>PropertyFile(name='EvaluationReport', output_name='evaluation', path='evaluation_report.json')\nNo finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\nTraceback (most recent call last):\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 474, in &lt;module&gt;\n    main()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 466, in main\n    pipeline = define_pipeline()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 457, in define_pipeline\n    print(json.loads(pipeline.definition()))\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/pipeline.py&quot;, line 257, in definition\n    request_dict = self.to_request()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/pipeline.py&quot;, line 89, in to_request\n    &quot;Steps&quot;: list_to_request(self.steps),\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/utilities.py&quot;, line 37, in list_to_request\n    request_dicts.append(entity.to_request())\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 99, in to_request\n    &quot;Arguments&quot;: self.arguments,\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/condition_step.py&quot;, line 87, in arguments\n    IfSteps=list_to_request(self.if_steps),\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/utilities.py&quot;, line 39, in list_to_request\n    request_dicts.extend(entity.request_dicts())\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/step_collections.py&quot;, line 50, in request_dicts\n    return [step.to_request() for step in self.steps]\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/step_collections.py&quot;, line 50, in &lt;listcomp&gt;\n    return [step.to_request() for step in self.steps]\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 209, in to_request\n    step_dict = super().to_request()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 99, in to_request\n    &quot;Arguments&quot;: self.arguments,\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/_utils.py&quot;, line 423, in arguments\n    model_package_args = get_model_package_args(\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/session.py&quot;, line 4217, in get_model_package_args\n    model_package_args[&quot;model_metrics&quot;] = model_metrics._to_request_dict()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/model_metrics.py&quot;, line 66, in _to_request_dict\n    model_quality[&quot;Statistics&quot;] = self.model_statistics._to_request_dict()\nAttributeError: 'PropertyFile' object has no attribute '_to_request_dict'\n<\/code><\/pre>\n<p>From this trace I see two, potentially related, issues. The immediate issue is the <code>AttributeError: 'PropertyFile' object has no attribute '_to_request_dict'<\/code>. I haven't been able to find any information on why we might be receiving it between forums and Sagemaker documentation.<\/p>\n<p>I also see a sneaky issue towards the top of the trace that has plagued me all day. The line <code>No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config<\/code> tells me that the register step is using our estimator when it should be waiting until after the training step has run. I can't seem to find any reference to this error, besides a somewhat-similar <a href=\"https:\/\/datascience.stackexchange.com\/questions\/100113\/how-to-fix-sagemakers-no-finished-training-job-found-associated-with-this-esti\">stack exchange post<\/a>.<\/p>\n<p>I've compared my code to the AWS-published examples many times and I'm confident that I'm not doing anything taboo. Would anyone be able to shine some light on what these errors are suggesting? Is there any more information or code that would be relevant?<\/p>\n<p>Thanks so much!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640211128263,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70455676",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":19.9,
        "Challenge_reading_time":93.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":57,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker Pipelines throws a \"No finished training job found associated with this estimator\" after introducing a register step",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1006.0,
        "Challenge_word_count":526,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1619402503747,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":61.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I was able to work through the issue! Here is the code for the register step that ended up working with my Tensorflow model:<\/p>\n<pre><code># Package the model\npipeline_model = PipelineModel(models=[model], role=params[&quot;role&quot;].default_value, sagemaker_session=session)\n\n# Create a RegisterModel step, which registers the model with Sagemaker Model Registry.\nregister_step = RegisterModel(\n    name=&quot;Bar&quot;,\n    model=pipeline_model,\n    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n    content_types=[&quot;application\/json&quot;],\n    response_types=[&quot;application\/json&quot;],\n    inference_instances=[config[&quot;instance&quot;][&quot;inference&quot;]],\n    transform_instances=[config[&quot;instance&quot;][&quot;transform&quot;]],\n    model_package_group_name=&quot;Foo&quot;,\n    approval_status=&quot;Approved&quot;,\n)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":26.5,
        "Solution_reading_time":11.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"What is the difference between SageMaker Pipelines and SageMaker Step Function SDK?\n\nThe [official Pipelines notebook][1] is basically only doing a workflow - pretty much a copy cat of what step functions has been doing for years. In the nice video from [Julien Simon][2] I see CICD capacities mentioned, where are those? any demos?\n\n\n  [1]: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-pipelines\/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.ipynb\n  [2]: https:\/\/www.youtube.com\/watch?v=Hvz2GGU3Z8g",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606994100000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668621791288,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU2iheeTzhSTmWw4aqVEeqOQ\/what-is-the-difference-between-sagemaker-pipelines-and-sagemaker-step-function-sdk",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":13.3,
        "Challenge_reading_time":8.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"What is the difference between SageMaker Pipelines and SageMaker Step Function SDK?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1344.0,
        "Challenge_word_count":68,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey, that demo is missing the project part of Pipelines and therefore the SM provided project templates. Go to SM studio and on the Studio summary hit edit settings and then enable access and provisioning of Service Catalog Portfolio of products in SM Studio. Then check your service catalog portfolios.\nHaven't tried it out yet though.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1610011923648,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":4.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1604146329127,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":95.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Idea:<\/p>\n<ul>\n<li>To use experiments and trials to log the training parameters and artifacts in sagemaker while using MWAA as the pipeline orchestrator<\/li>\n<\/ul>\n<p>I am using the training_config to create the dict to pass the training configuration to the Tensorflow estimator, but there is no parameter to pass the experiment configuration<\/p>\n<pre><code>tf_estimator = TensorFlow(entry_point='train_model.py',\n                                      source_dir= source\n                                      role=sagemaker.get_execution_role(),\n                                      instance_count=1,\n                                      framework_version='2.3.0',\n                                      instance_type=instance_type,\n                                      py_version='py37',\n                                      script_mode=True,\n                                      enable_sagemaker_metrics = True,\n                                      metric_definitions=metric_definitions,\n                                      output_path=output\n\nmodel_training_config = training_config(\n                    estimator=tf_estimator,\n                    inputs=input\n                    job_name=training_jobname,\n                )\n    \n\n\n\ntraining_task = SageMakerTrainingOperator(\n                    task_id=test_id,\n                    config=model_training_config,\n                    aws_conn_id=&quot;airflow-sagemaker&quot;,  \n                    print_log=True,\n                    wait_for_completion=True,\n                    check_interval=60  \n                )\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645378809553,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71197045",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":25.7,
        "Challenge_reading_time":14.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to pass the experiment configuration to a SagemakerTrainingOperator while training?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":128.0,
        "Challenge_word_count":90,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604146329127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>The only way that i found right now is to use the CreateTrainigJob API (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html#sagemaker-CreateTrainingJob-request-RoleArn\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html#sagemaker-CreateTrainingJob-request-RoleArn<\/a>). The following steps are needed:<\/p>\n<ul>\n<li>I am not sure if this will work with Bring your own script method for E.g with a Tensorflow estimator<\/li>\n<li>it works with a build your own container approach<\/li>\n<li>Using the CreateTrainigJob API i created the configs which in turn includes all the needed configs like - training, experiment, algporthm etc and passed that to SagemakerTrainingOperator<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.2,
        "Solution_reading_time":10.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1479159384132,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Illinois, United States",
        "Answerer_reputation_count":513.0,
        "Answerer_view_count":113.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I want to run a pipeline for different files, but some of them don't need all of the defined nodes. How can I pass them?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1572974635783,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1572982969067,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58716474",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":3.0,
        "Challenge_reading_time":2.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How to run a pipeline except for a few nodes?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":859.0,
        "Challenge_word_count":34,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572973807452,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>To filter out a few lines of a pipeline you can simply filter the pipeline list from inside of python, my favorite way is to use a list comprehension.<\/p>\n\n<p><strong>by name<\/strong><\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>nodes_to_run = [node for node in pipeline.nodes if 'dont_run_me' not in node.name]\nrun(nodes_to_run, io)\n<\/code><\/pre>\n\n<p><strong>by tag<\/strong><\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>nodes_to_run = [node for node in pipeline.nodes if 'dont_run_tag' not in node.tags]\nrun(nodes_to_run, io)\n<\/code><\/pre>\n\n<p>It's possible to filter by any attribute tied to the pipeline node, (name, inputs, outputs, short_name, tags)<\/p>\n\n<p>If you need to run your pipeline this way in production or from the command line, you can either tag your pipeline to run with tags, or add a custom <code>click.option<\/code> to your <code>run<\/code> function inside of <code>kedro_cli.py<\/code> then run this filter when the flag is <code>True<\/code>.<\/p>\n\n<p><strong>Note<\/strong>\nThis assumes that you have your pipeline loaded into memory as <code>pipeline<\/code> and catalog loaded in as <code>io<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":14.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":148.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"I am wondering if training jobs on vertex AI run in parallel, based on my tests it seems they do but wondering if anyone can confirm this is true as the number of concurrent jobs grows past say 1000.\n\n\u00a0\n\nThanks!",
        "Challenge_closed_time":1668513.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668498960000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Do-Training-Jobs-Run-in-Parallel-VERTEX-AI\/m-p\/489639#M786",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.1,
        "Challenge_reading_time":3.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Do Training Jobs Run in Parallel? (VERTEX AI)",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":156.0,
        "Challenge_word_count":47,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes training jobs run in parallel but the concurrency is subject to quota. See Vertex AI quota document.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.5,
        "Solution_reading_time":1.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1567164934556,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":15.1199388889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using \"studio (preview)\" from Microsoft Azure Machine Learning to create a pipeline that applies machine learning to a dataset in a blob storage that is connected to our data warehouse.<\/p>\n\n<p>In the \"Designer\", an \"Exectue R Script\" action can be added to the pipeline. I'm using this functionality to execute some of my own machine learning algorithms.<\/p>\n\n<p>I've got a 'hello world' version of this script working (including using the \"script bundle\" to load the functions in my own R files). It applies a very simple manipulation (compute the days difference with the date in the date column and 'today'), and stores the output as a new file. Given that the exported file has the correct information, I know that the R script works well.<\/p>\n\n<p>The script looks like this:<\/p>\n\n<pre><code># R version: 3.5.1\n# The script MUST contain a function named azureml_main\n# which is the entry point for this module.\n\n# The entry point function can contain up to two input arguments:\n#   Param&lt;medals&gt;: a R DataFrame\n#   Param&lt;matches&gt;: a R DataFrame\n\nazureml_main &lt;- function(dataframe1, dataframe2){\n\n  message(\"STARTING R script run.\")\n\n  # If a zip file is connected to the third input port, it is\n  # unzipped under \".\/Script Bundle\". This directory is added\n  # to sys.path.\n\n  message('Adding functions as source...')\n\n  if (FALSE) {\n    # This works...\n      source(\".\/Script Bundle\/first_function_for_script_bundle.R\")\n  } else {\n    # And this works as well!\n    message('Sourcing all available functions...')\n    functions_folder = '.\/Script Bundle'\n\n    list.files(path = functions_folder)\n    list_of_R_functions &lt;- list.files(path = functions_folder, pattern = \"^.*[Rr]$\", include.dirs = FALSE, full.names = TRUE)\n    for (fun in list_of_R_functions) {\n\n      message(sprintf('Sourcing &lt;%s&gt;...', fun))\n\n      source(fun)\n\n    }\n  }\n\n  message('Executing R pipeline...')\n  dataframe1 = calculate_days_difference(dataframe = dataframe1)\n\n  # Return datasets as a Named List\n  return(list(dataset1=dataframe1, dataset2=dataframe2))\n}\n<\/code><\/pre>\n\n<p>And although I do print some messages in the R Script, I haven't been able to find the \"stdoutlogs\" nor the \"stderrlogs\" that should contain these printed messages.<\/p>\n\n<p>I need the printed messages for 1) information on how the analysis went and -most importantly- 2) debugging in case the code failed.<\/p>\n\n<p>Now, I have found (on multiple locations) the files \"stdoutlogs.txt\" and \"stderrlogs.txt\". These can be found under \"Logs\" when I click on \"Exectue R Script\" in the \"Designer\".\nI can also find \"stdoutlogs.txt\" and \"stderrlogs.txt\" files under \"Experiments\" when I click on a finished \"Run\" and then both under the tab \"Outputs\" and under the tab \"Logs\".\nHowever... all of these files are empty.<\/p>\n\n<p>Can anyone tell me how I can print messages from my R Script and help me locate where I can find the printed information?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579792546180,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59881727",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":36.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"Debugging R Scripts in azure-ml: Where can stdout and stderr logs be found? (or why are they empty?)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":287.0,
        "Challenge_word_count":415,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1534511592567,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherlands",
        "Poster_reputation_count":423.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>Can you please click on the \"Execute R module\" and download the 70_driver.log? I tried message(\"STARTING R script run.\") in an R sample and can found the output there.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z7s7h.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z7s7h.png\" alt=\"view logs for a execute R script module\"><\/a><\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1579846977960,
        "Solution_link_count":2.0,
        "Solution_readability":7.2,
        "Solution_reading_time":4.55,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"# Context\r\nToday, you can execute a kedro pipeline interactively. The logic would be to load the context, and then to run the pipeline.\r\n\r\n```python\r\nfrom kedro.context import load_context\r\nlocal_context = load_context(\".\")\r\nlocal_context.run(pipeline=local_context.pipelines[PIPELINE_NAME],\r\n                             catalog=local_context.catalog)\r\n```\r\n\r\n# Description\r\nIf the execution fails for some reason (bug in the pipeline), the mlflow run is not closed. This creates unintended side effects: for instance, if you rerun the pipeline, the new run will be nested in the failing runs and the mllflow database will become very messy.\r\n\r\nThis bug does not occur when running from the command line since the mlflow run is automatically closed when exiting.\r\n\r\n# Possible Implementation \r\nImplement a [``on_pipeline_error`` kedro ``Hook``](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/15_hooks.html?highlight=on_pipeline_error#hook-specification) to close the mlflow run when the pipeline fails.",
        "Challenge_closed_time":1598336.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591562037000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/10",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":10.6,
        "Challenge_reading_time":13.06,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Close mlflow run when a pipeline fails in interactive mode",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":126,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nKedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). We claim to support both, but the CLI commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## Steps to Reproduce\r\n\r\nCall ``kedro mlflow init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## Expected Result\r\n\r\nThe cli commands should be available (``init``)\r\n\r\n## Actual Result\r\nOnly the ``new`` command is available. This is not considered as a kedro project.\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): kedro==16.6, kedro-mlflow==0.4.1\r\n* Python version used (`python -V`): 3.7.9\r\n* Operating system and version: Windows 7\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nThe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Challenge_closed_time":1615716.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610404594000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/157",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":14.22,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"kedro mlflow cli is broken if configuration is declared in pyproject.toml",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":167,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This will wait the migration to `kedro>=0.17.0` (cf. #144) in milestone 0.6.0 because kedro has bradnd new utilities to handle this part. This will remove boilerplate code from the plugin and ensure consistency with future kedro changes.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":2.95,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nIf I create a PipelineML objects  and I return it in the `hooks.py`:\r\n\r\n\r\n```python\r\nclass ProjectHooks:\r\n    @hook_impl\r\n    def register_pipelines(self) -> Dict[str, Pipeline]:\r\n        \"\"\"Register the project's pipeline.\r\n        Returns:\r\n            A mapping from a pipeline name to a ``Pipeline`` object.\r\n        \"\"\"\r\n       ml_pipeline=create_ml_pipeline()\r\n        training_pipeline = pipeline_ml_factory(training=ml_pipeline.only_nodes_with_tags(\"training\"), inference=ml_pipeline.only_nodes_with_tags(\"inference\"), input_name=\"instances\")\r\n\r\n        return {\r\n            \"training\": training_pipeline,\r\n            \"__default__\": other_pipeline\r\n        }\r\n````\r\n\r\n`kedro run` command works fine, but `kedro viz` and `kedro pipeline list` fail.\r\n\r\n## Context\r\n\r\nI was trying to visualise a pipeline with kedro-viz==3.7.0 (I also tried 3.4.0 and 3.0.0), and kedro==0.16.6\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create a PipelineMl object with pipeline_ml_factory in `hooks;py`\r\n2. Launch `kedro viz` in terminal\r\n\r\n## Expected Result\r\nKedro viz should be launched on localhost:5000\r\n\r\n## Actual Result\r\nTell us what happens instead.\r\n\r\n```\r\n-- If you received an error, place it here.\r\n```\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`):\r\n* Python version used (`python -V`):\r\n* Operating system and version:\r\n\r\n*Note: everything works fine with the older template (`kedro<=0.16.4`) and the `pipeline.py` file instead of `hooks.py`*\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Potential solution: \r\n\r\nIt seems the `__add__` method of the `PipelineML` class must be implemented.",
        "Challenge_closed_time":1605720.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605718283000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/119",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":22.28,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"PipelineML objects in `hooks.py` breaks all kedro-viz versions with kedro template>=0.16.5",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":218,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The issue is not confirmed and was due to adding a Pipeline and a PipelineML object.\r\nI close it.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.3,
        "Solution_reading_time":1.15,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1393524211332,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":745.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm training in SageMaker using TensorFlow + Script Mode and currently using 'File' input mode for my data.<\/p>\n\n<p>Has anyone figured out how to stream data using 'Pipe' data format in conjunction with Script Mode training?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1549915827960,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1549923138163,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54638364",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker Script Mode + Pipe Mode",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1385.0,
        "Challenge_word_count":39,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>You can import <code>sagemaker_tensorflow<\/code> from the training script as follows:<\/p>\n\n<pre><code>from sagemaker_tensorflow import PipeModeDataset\nfrom tensorflow.contrib.data import map_and_batch\n\nchannel = 'my-pipe-channel-name'\n\nds = PipeModeDataset(channel)\nds = ds.repeat(EPOCHS)\nds = ds.prefetch(PREFETCH_SIZE)\nds = ds.apply(map_and_batch(parse, batch_size=BATCH_SIZE,\n                            num_parallel_batches=NUM_PARALLEL_BATCHES))\n<\/code><\/pre>\n\n<p>You can find the full example here: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_pipemode_example\/pipemode.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_pipemode_example\/pipemode.py<\/a><\/p>\n\n<p>You can find documentation about sagemaker_tensorflow here <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset<\/a><\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":34.2,
        "Solution_reading_time":14.43,
        "Solution_score_count":4.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":53.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>It was written in the Microsoft AzureML documentation, &quot;A run represents a single trial of an experiment. Runs are used to monitor the asynchronous execution of a trial&quot; and A Run object is also created when you submit or start_logging with the Experiment class.&quot;<\/p>\n<p>Related to <code>start_logging<\/code>, as far as I know, when we have simply started the run by executing this <code>start logging<\/code> method. We have to stop, or complete by <code>complete<\/code> method when the run is completed. This is because  <code>start_logging<\/code> is a synchronized way of creating an experiment. However, Run object created from <code>start_logging<\/code> is to monitor the asynchronous execution of a trial.<\/p>\n<p>Can anyone clarify whether <code>start_logging<\/code> will start asynchronous execution or synchronous execution?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659981330190,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1660267366787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73282180",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":11.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"In AzureML, start_logging will start asynchronous execution or synchronous execution?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":127,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525287643000,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Ottawa, ON, Canada",
        "Poster_reputation_count":27.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p><strong>start_logging<\/strong> will be considered as <strong>asynchronous<\/strong> execution as this generates the multiple interactive run sessions. In a specific experiment, there is a chance of multiple interactive sessions, that work parallelly and there will be no scenario to be followed in sequential.<\/p>\n<p>The individual operation can be performed and recognized based on the parameters like <strong>args<\/strong>  and <strong>kwargs<\/strong>.<\/p>\n<p>When the start_logging is called, then an interactive run like <strong>jupyter notebook<\/strong> was created. The complete metrics and components which are created when the start_logging was called will be utilized. When the output directory was mentioned for each interactive run, based on the args value, the output folder will be called seamlessly.<\/p>\n<p>The following code block will help to define the operation of start_logging<\/p>\n<pre><code>experiment = Experiment(your_workspace, &quot;your_experiment_name&quot;)\n   run = experiment.start_logging(outputs=None, snapshot_directory=&quot;.&quot;, display_name=&quot;test&quot;)\n   ...\n   run.log_metric(&quot;Accuracy_Value&quot;, accuracy)\n   run.complete()\n<\/code><\/pre>\n<p>the below code block will be defining the basic syntax of start_logging<\/p>\n<pre><code>start_logging(*args, **kwargs)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.4,
        "Solution_reading_time":17.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":147.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi all,\n\nI have raised a ticket for multiple issues we've been having with SageMaker lately, the ticket was created more than 36 hours  ago, and I have not had any response, in fact the ticket hasn't even been assigned yet.\n\nThe case ID is 10300240931.\n\nI thought AWS guarantee a response under 12 hours for \"system impaired\" issues, does anyone know what I can do to accelerate this?\n\nthank you!\nRuoy",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656580942341,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667931210104,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUFgnjt9J3T0iXhE0axG10vQ\/how-long-does-it-take-for-aws-tech-support-team-to-respond-to-a-system-impaired-issue",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":5.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How long does it take for AWS tech support team to respond to a \"system impaired\" issue?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":128.0,
        "Challenge_word_count":88,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Ruoy!\nMy advice here is to scale this issue via your account team, they will have the mechanisms to scale this concern.\nIf you are on basic or developer support, you could look into upgrading to business support for a day and open a live chat with support!\nHope this helps",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1656585271688,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":3.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":52.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi, there was a website for machine learning studio. You can publish your pipelines and you can also get others publish. But it not works for designer. Is there any plan for the platform migration? <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1669766733727,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1109523\/ai-gallery-for-designer",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.0,
        "Challenge_reading_time":2.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"AI gallery for Designer",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=662ea9f1-8d5c-4484-9ff1-c27d48858639\">@Markswift  <\/a>     <\/p>\n<p>I am sorry the AI gallery currently is not supporting Azure Machine Learning Service Designer at this moment, but I do see there are some sample shared by offcial product team. Currently you can refer to it for general style project.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/265941-image.png?platform=QnA\" alt=\"265941-image.png\" \/>    <\/p>\n<p>I will bring this feedback to product team, but at this moment we still need to wait for the next step plan. I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to suppor the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.8,
        "Solution_reading_time":9.3,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":99.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi, I manage to run a azure ml trainning pipeline in adf. Then I can see that I can create\/update a batch inference pipeline from the Designer. But can I update the batch inference pipeline from adf?  <\/p>\n<p>thanks<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619624088887,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/375702\/how-to-update-azure-ml-model-from-adf",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.4,
        "Challenge_reading_time":3.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"how to update azure ml model from adf?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":46,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=1b597528-b52c-41fc-932a-baf4d96cb15b\">@javier  <\/a>,    <\/p>\n<p>Thanks for using Microsoft Q&amp;A !!    <\/p>\n<p>Unfortunately this is not supported using Azure Data Factory and you can only update the scoring web service using <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/update-machine-learning-models\">Azure Machine Learning Studio (classic) update resource activity<\/a> Can you please provide your scenario\/use case in details so that I can check internally.  I also suggest you to please post this as a feedback at <a href=\"https:\/\/feedback.azure.com\/forums\/270578-data-factory\">ADDF UserVoice<\/a>. This will allow the community to upvote and for the product team to include into their plans    <\/p>\n<p>----------    <\/p>\n<p><em>Please do not forget to &quot;Accept the answer&quot; wherever the information provided helps you to help others in the community.<\/em>    <\/p>\n<p>Thanks    <br \/>\nSaurabh    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.0,
        "Solution_reading_time":12.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":114.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1613062428296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":141.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Having issues with kedro. The 'register_pipelines' function doesn't seem to be running or creating the <strong>default<\/strong> Pipeline that I'm returning from it.<\/p>\n<p>The error is<\/p>\n<pre><code>(kedro-environment) C:\\Users\\cc667216\\OneDrive\\DCS_Pipeline\\dcs_files&gt;kedro run\n2021-03-22 13:30:28,201 - kedro.framework.session.store - INFO - `read()` not implemented for `BaseSessionStore`. Assuming empty store.\nfatal: not a git repository (or any of the parent directories): .git\n2021-03-22 13:30:28,447 - kedro.framework.session.session - WARNING - Unable to git describe C:\\Users\\cc667216\\OneDrive\\DCS_Pipeline\\dcs_files\n2021-03-22 13:30:28,476 - root - INFO - ** Kedro project dcs_files\n2021-03-22 13:30:28,486 - kedro.framework.session.store - INFO - `save()` not implemented for `BaseSessionStore`. Skipping the step.\nTraceback (most recent call last):\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\kedro\\framework\\context\\context.py&quot;, line 304, in _get_pipeline\n    return pipelines[name]\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\dynaconf\\utils\\functional.py&quot;, line 17, in inner\n    return func(self._wrapped, *args)\nKeyError: '__default__'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\Scripts\\kedro-script.py&quot;, line 9, in &lt;module&gt;\n    sys.exit(main())\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\kedro\\framework\\cli\\cli.py&quot;, line 228, in main\n    cli_collection(**cli_context)\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\click\\core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\click\\core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\click\\core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\click\\core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\click\\core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;C:\\Users\\cc667216\\OneDrive\\DCS_Pipeline\\dcs_files\\src\\dcs_package\\cli.py&quot;, line 240, in run\n    pipeline_name=pipeline,\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\kedro\\framework\\session\\session.py&quot;, line 344, in run\n    pipeline = context._get_pipeline(name=pipeline_name)\n  File &quot;C:\\Anaconda3\\envs\\kedro-environment\\lib\\site-packages\\kedro\\framework\\context\\context.py&quot;, line 310, in _get_pipeline\n    ) from exc\nkedro.framework.context.context.KedroContextError: Failed to find the pipeline named '__default__'. It needs to be generated and returned by the 'register_pipelines' function.\n<\/code><\/pre>\n<p>My src\\dcs_package\\pipeline_registry.py looks like this:<\/p>\n<pre><code># Copyright 2021 QuantumBlack Visual Analytics Limited\n#\n# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\n# OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND\n# NONINFRINGEMENT. IN NO EVENT WILL THE LICENSOR OR OTHER CONTRIBUTORS\n# BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY, WHETHER IN AN\n# ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF, OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n#\n# The QuantumBlack Visual Analytics Limited (&quot;QuantumBlack&quot;) name and logo\n# (either separately or in combination, &quot;QuantumBlack Trademarks&quot;) are\n# trademarks of QuantumBlack. The License does not grant you any right or\n# license to the QuantumBlack Trademarks. You may not use the QuantumBlack\n# Trademarks or any confusingly similar mark as a trademark for your product,\n# or use the QuantumBlack Trademarks in any other manner that might cause\n# confusion in the marketplace, including but not limited to in advertising,\n# on websites, or on software.\n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n&quot;&quot;&quot;Project pipelines.&quot;&quot;&quot;\nfrom typing import Dict\nfrom kedro.pipeline import Pipeline, node\nfrom .pipelines.data_processing.pipeline import create_pipeline\nimport logging\n\ndef register_pipelines() -&gt; Dict[str, Pipeline]:\n    &quot;&quot;&quot;Register the project's pipelines.\n\n    Returns:\n        A mapping from a pipeline name to a ``Pipeline`` object.\n    &quot;&quot;&quot;\n    log = logging.getLogger(__name__)\n    log.info(&quot;Start register_pipelines&quot;) \n    data_processing_pipeline = create_pipeline()\n    log.info(&quot;create pipeline done&quot;) \n    \n\n    return {\n        &quot;__default__&quot;: data_processing_pipeline,\n        &quot;dp&quot;: data_processing_pipeline\n    }\n\n<\/code><\/pre>\n<p>Then I have a &quot;src\\dcs_package\\pipelines\\data_processing\\pipeline.py&quot; file with a real simple function that outputs &quot;test string&quot; and nothing else.<\/p>\n<p>I was able to read a few items from my catalog (a csv and a xlsx) so I think all the dependencies are working fine.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616435385817,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66751310",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":72.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":52,
        "Challenge_solved_time":null,
        "Challenge_title":"Kedro : Failed to find the pipeline named '__default__'",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1486.0,
        "Challenge_word_count":536,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1398987181710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":75.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>What version of kedro are you on? There is a bit of a problem with kedro 0.17.2 where the true error is masked and will return the exception that you're seeing instead. It's possible that the root cause of the error is actually some other <code>ModuleNotFoundError<\/code> or <code>AttributeError<\/code>. Try doing a <code>kedro install<\/code> before <code>kedro run<\/code> and see if that fixes it.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":5.05,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to create simple templates for scientists so that they can fit their models easily into a continuous integration\/continuous delivery (CI\/CD) pipeline. I want to know about success stories of AWS customers performing CI\/CD on machine learning pipelines. ",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592577511000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667926686451,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUwLq6HNRZSOK7ODKKc_lC3Q\/can-you-share-success-stories-of-aws-customers-performing-ml-ci-cd",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Can you share success stories of AWS customers performing ML CI\/CD?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":49,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Amazon has released the [Amazon SageMaker Pipelines][1] that are the first purpose-built CI\/CD service for machine learning: \n[1]: https:\/\/aws.amazon.com\/sagemaker\/pipelines\/\n\nFor more information, see [New \u2013 Amazon SageMaker Pipelines brings DevOps capabilities to your machine learning projects] [2]\n[2]: https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-pipelines-brings-devops-to-machine-learning-projects\/\n\nAdditionally, we have a case-study where a customer created one on their own for model development using Airflow.  For more information, see [NerdWallet uses machine learning on AWS to power recommendations platform][3] and \n[Using Amazon SageMaker to build a machine learning platform with just three engineers][4].\n[3]: https:\/\/aws.amazon.com\/solutions\/case-studies\/nerdwallet-case-study\/\n[4]: https:\/\/www.nerdwallet.com\/blog\/engineering\/machine-learning-platform-amazon-sagemaker\/\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925550032,
        "Solution_link_count":4.0,
        "Solution_readability":20.8,
        "Solution_reading_time":11.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently in the process of setting up a custom sagemaker container to run training jobs on Sagemaker and have succeeded in doing so. However, I am a bit confused over this question which is currently bugging me and is definitely something that I need to consider in the future<\/p>\n<ol>\n<li>Is it possible to run custom scripts on a custom container when declaring a sagemaker training job?<\/li>\n<\/ol>\n<p>My current understanding when it comes to creating a custom sagemaker image is that I create a train file that gets executed when running a training job, but I could never find documentation on whether is it possible to overwrite this and run a training script (but using the same custom container), like how we run training jobs using in-built algorithms. Is it the case that for custom algorithms we are restricted by this limitation?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656086681097,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72746701",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.5,
        "Challenge_reading_time":11.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Running custom scripts in a custom container while running a sagemaker training job",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":26.0,
        "Challenge_word_count":157,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644981356940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I don't fully understand your question (can you make an example please?), specially when you say<\/p>\n<blockquote>\n<p>like how we run training jobs using in-built algorithms<\/p>\n<\/blockquote>\n<p>But basically you can do whatever you want in your container, as probably you already did, you have a proper <code>train<\/code> file in your container, which is the one that sagemaker calls as the entrypoint. In that file you can call external script (which are in your container too) and also pass parameters to your container (see how for example hyperparameters are passed). There is a quite clear documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/docker-containers-create.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":9.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":100.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":5.3960230556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am developing machine learning repositories that require fairly large trained model files to run. These files are not part of the git remote but is tracked by DVC and is saved in a separate remote storage. I am running into issues when I am trying to run unit tests in the CI pipeline for functions that require these model files to make their prediction. Since I don't have access them in the git remote, I can't test them.<\/p>\n<p>What is the best practice that people usually do in this situation? I can think of couple of options -<\/p>\n<ul>\n<li>Pull the models from the DVC remote inside the CI pipeline. I don't want to do this becasue downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/li>\n<li>Use <code>unittest.mock<\/code> to simulate the output of from the model prediction and test other parts of my code. This is what I am doing now but it's sort of a pain with unittest's mock functionalities. That module wasn't really developed with ML in mind from what I can tell. It's missing (or is hard to find) some functionalities that I would have really liked. Are there any good tools for doing this geared specifically towards ML?<\/li>\n<li>Do weird reformatting of the function definition that allows me to essentially do option 2 but without a mock module. That is, just test the surrounding logic and don't worry about the model output.<\/li>\n<li>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/li>\n<\/ul>\n<p>What do people usually do in this situation?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603252093370,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1603305923907,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64456396",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":20.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I unit test a function in the CI pipeline that uses model files that are not part of the git remote?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":289.0,
        "Challenge_word_count":304,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1446746840592,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2545.0,
        "Poster_view_count":382.0,
        "Solution_body":"<p>If we talk about unit tests, I think it's indeed better to do a mock. It's best to have unit tests small, testing actual logic of the unit, etc. It's good to have other tests though that would pull the model and run some logic on top of that - I would call them integration tests.<\/p>\n<p>It's not black and white though. If you for some reason see that it's easier to use an actual model (e.g. it changes a lot and it is easier to use it instead of maintaining and updating stubs\/fixtures), you could potentially cache it.<\/p>\n<p>I think, to help you with the mock, you would need to share some technical details- how does the function look like, what have you tried, what breaks, etc.<\/p>\n<blockquote>\n<p>to do this because downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.<\/p>\n<\/blockquote>\n<p>I think you can potentially utilize CI systems cache to avoid downloading it over and over again. This is the GitHub Actions related <a href=\"https:\/\/github.com\/actions\/cache#cache-limits\" rel=\"nofollow noreferrer\">repository<\/a>, this is <a href=\"https:\/\/circleci.com\/docs\/2.0\/caching\" rel=\"nofollow noreferrer\">CircleCI<\/a>. The idea is the same across all common CI providers. Which one are considering to use, btw?<\/p>\n<blockquote>\n<p>Just put the model files in the git remote and be done with it. Only use DVC to track data.<\/p>\n<\/blockquote>\n<p>This can be the way, but if models are large enough you will pollute Git history significantly. On some CI systems it can become even slower since they will be fetching this with regular <code>git clone<\/code>. Effectively, downloading models anyway.<\/p>\n<p>Btw, if you use DVC or not take a look at another open-source project that is made specifically to do CI\/CD for ML - <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">CML<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1603325349590,
        "Solution_link_count":3.0,
        "Solution_readability":7.6,
        "Solution_reading_time":23.14,
        "Solution_score_count":3.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":297.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1384730587840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"France",
        "Answerer_reputation_count":717.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<pre><code>pins 1.0.1\nAzureStor 3.7.0\n<\/code><\/pre>\n<p>I'm getting this error<\/p>\n<pre><code>Error in withr::local_options(azure_storage_progress_bar = progress, .local_envir = env) : \n  unused argument (azure_storage_progress_bar = progress)\nCalls: %&gt;% ... pin_meta.pins_board_azure -&gt; azure_download -&gt; local_azure_progress\nExecution halted\n<\/code><\/pre>\n<p>when running <code>pin_read()<\/code> in the following code (<code>pin_list()<\/code> works fine)<\/p>\n<pre><code>bl_endp_key &lt;- storage_endpoint(endpoint = &lt;endpoint URL&gt;, key =&lt;endpoint key&gt;&quot;)\ncontainer &lt;- storage_container(endpoint = bl_endp_key, name = &lt;blob name&gt;)\nboard &lt;- board_azure(container = container, path = &quot;accidentsdata&quot;)\ncat(&quot;Testing pins:\\n&quot;)\nprint(board %&gt;% pin_list())\naccidents2 &lt;- board %&gt;% pins::pin_read('accidents') %&gt;% as_tibble()\n<\/code><\/pre>\n<p>My goal is to &quot;pin_read&quot; a dataset located on a Azure Blob Storage from an R script being run from <strong>pipelineJoB (YAML)<\/strong> including a <code>command: Rscript script.R ...<\/code> and an <code>environment:<\/code> based on a dockerfile installing <strong>R version 4.0.0<\/strong> (2020-04-24) -- &quot;Arbor Day&quot;<\/p>\n<p>The pipelineJob is being called from an Azure DevOps Pipeline task with <code>az ml job create &lt;pipelineJob YAML&gt; &lt;resource grp&gt; &lt;aml workspace name&gt;<\/code>.<\/p>\n<p>Note: the R script runs fine on my Windows RStudio desktop, with R version 4.1.3 (2022-03-10) -- &quot;One Push-Up&quot;.<\/p>\n<p>I've already tried with<\/p>\n<p><code>options(azure_storage_progress_bar=FALSE)<\/code> or<\/p>\n<p><code>withr::local_options(azure_storage_progress_bar=FALSE)<\/code><\/p>\n<p>but I'm getting the same <code>unused argument (azure_storage_progress_bar ...<\/code> error.<\/p>\n<p>FYI: <code>local_azure_progress<\/code> is defined here <a href=\"https:\/\/rdrr.io\/github\/rstudio\/pins\/src\/R\/board_azure.R#sym-local_azure_progress\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1656349974407,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1656680662488,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72775967",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":27.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"R, pins and AzureStor: unused argument (azure_storage_progress_bar = progress)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":80.0,
        "Challenge_word_count":189,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384730587840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"France",
        "Poster_reputation_count":717.0,
        "Poster_view_count":112.0,
        "Solution_body":"<p>Issue has been filed in <a href=\"https:\/\/github.com\/rstudio\/pins\/issues\/624\" rel=\"nofollow noreferrer\">pins<\/a>, it seems that is not an AzureStor issue.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.0,
        "Solution_reading_time":2.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"### What steps did you take:\r\nAttempted to run the Sagemaker training operator using a custom image that is not hosted on ECR\r\n\r\n### What happened:\r\nI got the following error:\r\n```\r\nException: Invalid training image. Please provide a valid Amazon Elastic Container Registry path of the Docker image to run.\r\n```\r\n\r\n### What did you expect to happen:\r\nOur CI\/CD pipeline is set up to push images to our own personal registry that is not hosted on ECR - ideally, I would want to run Sagemaker training jobs using images hosted from our personal registry instead of having to also push our images to ECR (much more error-prone + having to maintain two container registries ...)\r\n\r\n\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\nDeployed kubeflow piplines as part of kubeflow deployment on AWS EKS",
        "Challenge_closed_time":1589522.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588992332000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3728",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":12.5,
        "Challenge_reading_time":10.43,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker Training Operator throws an error if custom image is not hosted on ECR",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":141,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thank you @marwan116  for trying out the operator. Currently SageMaker has support for images hosted in ECR only. \r\nSageMaker has support for various frameworks like TensorFlow, XGBoost, PyTorch etc as well as some [in-built algorithms](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html).\r\n\r\nIf you have custom image, [here](https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/docker-push-ecr-image.html) is the instruction to put them into ECR.\r\n  https:\/\/github.com\/kubeflow\/pipelines\/issues\/3670 @marwan116 looks like question answered, I'm going to close this issue.\r\nBut feel free to reopen with `\/reopen` comment.\r\n\r\n\/close @Bobgy: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/3728#issuecomment-629047584):\n\n>@marwan116 looks like question answered, I'm going to close this issue.\r\n>But feel free to reopen with `\/reopen` comment.\r\n>\r\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":13.1,
        "Solution_reading_time":16.5,
        "Solution_score_count":null,
        "Solution_sentence_count":13.0,
        "Solution_word_count":129.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1606390824848,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using the ML-pipeline designer in MS Azure it is possible to clean missing data, namely by replacing them by means or constant values.<\/p>\n<p>In my dataset I have gaps, when the measured value did not change enough, thus I should want to replace the missing data with the last existing entry.\nSo from<\/p>\n<pre><code>VALUE A\n2\nNONE\nNONE\nNONE\n3\nNONE\nNONE\n<\/code><\/pre>\n<p>I would like to get<\/p>\n<pre><code>VALUE A\n2\n2\n2\n2\n3\n3\n3\n<\/code><\/pre>\n<p>This option is not available in the pipeline designer as far as I know. Can I manipulate the dataset somehow else within Azure, before training?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646036588140,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71292240",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":8.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to replace missing datapoints with prior in MS Azure?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":13.0,
        "Challenge_word_count":113,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606390824848,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>I figured it out, by using the Notebooks (do not work in Firefox for me, only on Chrome).\nThere it is possible to handle the dataset in python, transform it to pandas, manipulate it and save it to the datastore.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":2.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nHi, I have a quick question, is it possible to filter all jobs that requested\/accessed a specific connection?\n\nTo explain my use-case, we detected an issue with some data, and we would like to assess how many jobs and how far in the past that data was used in our training jobs.",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649676773000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1487",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":9.6,
        "Challenge_reading_time":4.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How to filter all jobs that accessed a connection, a dataset, or an artifact",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":67,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"V1.18 the following filters, or any combination, will be possible:\n\nBy connection name connections.name: CONNECTION1 | CONNECTION2\nBy connection tag connections.tags: TAG1 | TAG2\nBy connection kind connections.kind: git or connections.kind: KIND1 | KIND2\nBy artifact name artifacts.name: LINEAGE1 | LINEAGE2\nBy artifact kind artifacts.kind: model or artifacts.kind: KIND1 | KIND2\nBy artifact path artifacts.path: foo\/bar\nBy artifact state artifacts.state: STATE",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":5.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":56.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":1796.2886094445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>My <code>API_ENDPOINT<\/code> is set to <code>europe-west1-aiplatform.googleapis.com<\/code>.<\/p>\n<p>I define a pipeline:<\/p>\n<pre><code>def pipeline(project: str = PROJECT_ID, region: str = REGION, api_endpoint: str = API_ENDPOINT):\n<\/code><\/pre>\n<p>when I run it:<\/p>\n<pre><code>job = aip.PipelineJob(\ndisplay_name=DISPLAY_NAME,\ntemplate_path=&quot;image classification_pipeline.json&quot;.replace(&quot; &quot;, &quot;_&quot;),)\njob.run()\n<\/code><\/pre>\n<p>it is always created in USandA:<\/p>\n<pre><code>INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. \nResource name: projects\/my_proj_id\/locations\/us-central1\/pipelineJobs\/automl-image-training-v2-anumber\n<\/code><\/pre>\n<p>How do I get it into Europe?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641293452093,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70577610",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":14.9,
        "Challenge_reading_time":10.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Why is my GCP Vertex pipeline api_endpoint not right?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":92.0,
        "Challenge_word_count":61,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>The <code>location<\/code> parameter in the <code>aip.PipelineJob()<\/code> class can be used to specify in which region the pipeline will be deployed. Refer to this <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/aiplatform#class-googlecloudaiplatformpipelinejobdisplayname-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-templatepath-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-jobid-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-pipelineroot-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-parametervalues-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-any--none-enablecaching-optionalboolhttpspythonreadthedocsioenlatestlibraryfunctionshtmlbool--none-encryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-labels-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-credentials-optionalgoogleauthcredentialscredentialshttpsgoogleapisdevpythongoogle-authlatestreferencegoogleauthcredentialshtmlgoogleauthcredentialscredentials--none-project-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-location-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none\" rel=\"nofollow noreferrer\">documentation<\/a> for more information about the <code>PipelineJob()<\/code> method.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>REGION = &quot;europe-west1&quot;\n\njob = aip.PipelineJob(\n          display_name=DISPLAY_NAME,\n          template_path=&quot;image classification_pipeline.json&quot;.replace(&quot; &quot;, &quot;_&quot;),\n          location=REGION)\n\njob.run()\n<\/code><\/pre>\n<p>The above code will deploy a pipeline in the <code>europe-west1<\/code> region. The code returns the following output. The job is now deployed in the specified region.<\/p>\n<pre><code>INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\nINFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects\/&lt;project-id&gt;\/locations\/europe-west1\/pipelineJobs\/hello-world-pipeline\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1647760091087,
        "Solution_link_count":1.0,
        "Solution_readability":50.8,
        "Solution_reading_time":30.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":82.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi there,\n\nIs it possible to use R model training and serving in SageMaker ML Pipelines? Looked in examples [here](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/r_examples).  And it doesn't look that R is fully supported currently by ML Pipelines.  Any examples and success stories are very welcome. \n\nThanks.",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643230196748,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668030995400,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU17aS4s7uSRqmiLuveuchBw\/using-r-model-in-sagemaker-ml-pipelines",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":4.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Using R model in SageMaker ML pipelines",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":244.0,
        "Challenge_word_count":48,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"In general it is possible to use the SageMaker python SDK and boto3  using the reticulate package in R, However do not have direct examples of SageMaker Pipelines using R.\n\nIt is possible to orchestrate the production pipeline using the R Containers for training and serving and setting up the DAG can be done with reticulate and SageMaker Python SDK and can be achieved using the AWS Step Functions. Please refer to the following example for reference.\n\n\nhttps:\/\/github.com\/aws-samples\/reinvent2020-aim404-productionize-r-using-amazon-sagemaker\nhttps:\/\/www.youtube.com\/watch?v=Zpp0nfvqDCA",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1643404063708,
        "Solution_link_count":2.0,
        "Solution_readability":15.9,
        "Solution_reading_time":7.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\nKedro Telemetry installed alongside a packaged and installed Kedro project breaks the project by assuming that the `pyproject.toml` file exists. The `pyproject.toml` is only a recipe for building the project and should not be assumed to be existing in the current folder in all cases.\r\n\r\nThe problem was introduced with https:\/\/github.com\/kedro-org\/kedro-plugins\/pull\/62\r\n\r\n## Context\r\nWhen deploying Kedro projects and if you have installed Kedro Telemetry, it breaks your project.\r\n\r\n## Steps to Reproduce\r\n1. Create a Kedro project\r\n2. Add a dependency on kedro-telemetry\r\n3. Package it through `kedro package`\r\n4. Install it in a different environment\r\n5. Run the project through `.\/<project>` in a folder where only the `conf\/` is\r\n\r\n## Expected Result\r\nThe project should run.\r\n\r\n## Actual Result\r\nAn exception is thrown.\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* Kedro version used (`pip show kedro` or `kedro -V`): 0.18.x\r\n* Kedro plugin and kedro plugin version used (`pip show kedro-telemetry`): 0.2.2 \r\n* Python version used (`python -V`): Not relevant\r\n* Operating system and version: Not relevant\r\n",
        "Challenge_closed_time":1670415.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669645759000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/83",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":7.5,
        "Challenge_reading_time":15.64,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":13.0,
        "Challenge_repo_issue_count":97.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"Kedro Telemetry breaks packaged projects due to wrongly assuming `pyproject.toml` exists",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":182,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1645110475503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":15.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":104.3447397222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I follow the official tutotial from microsoft: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool<\/a><\/p>\n<p>When I execute:<\/p>\n<pre><code>#Bind model within Spark session\n model = pcontext.bind_model(\n     return_types=RETURN_TYPES, \n     runtime=RUNTIME, \n     model_alias=&quot;Sales&quot;, #This alias will be used in PREDICT call to refer  this   model\n     model_uri=AML_MODEL_URI, #In case of AML, it will be AML_MODEL_URI\n     aml_workspace=ws #This is only for AML. In case of ADLS, this parameter can be removed\n ).register()\n<\/code><\/pre>\n<p>I got : No module named 'azureml.automl'<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/g0UCX.png\" rel=\"nofollow noreferrer\">My Notebook<\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648558433353,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71662401",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":17.4,
        "Challenge_reading_time":12.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Synapse Analytics Auto ML Predict No module named 'azureml.automl'",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":271.0,
        "Challenge_word_count":81,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1645110475503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I solved it. In my case it works best like this:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/JKEmr.png\" rel=\"nofollow noreferrer\">Imports<\/a><\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>#Import libraries\nfrom pyspark.sql.functions import col, pandas_udf,udf,lit\nfrom notebookutils.mssparkutils import azureML\nfrom azureml.core import Workspace, Model\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nfrom azureml.core.model import Model\nimport joblib\nimport pandas as pd\n\nws = azureML.getWorkspace(\"AzureMLService\")\nspark.conf.set(\"spark.synapse.ml.predict.enabled\",\"true\")<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ij760.png\" rel=\"nofollow noreferrer\">Predict function<\/a><\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>def forecastModel():\n    model_path = Model.get_model_path(model_name=\"modelName\", _workspace=ws)\n    modeljob = joblib.load(model_path + \"\/model.pkl\")\n\n    validation_data = spark.read.format(\"csv\") \\\n                            .option(\"header\", True) \\\n                            .option(\"inferSchema\",True) \\\n                            .option(\"sep\", \";\") \\\n                            .load(\"abfss:\/\/....csv\")\n\n    validation_data_pd = validation_data.toPandas()\n\n\n    predict = modeljob.forecast(validation_data_pd)\n\n    return predict<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1648934074416,
        "Solution_link_count":2.0,
        "Solution_readability":20.1,
        "Solution_reading_time":20.46,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is a fundamental AWS Sagemaker question. When I run training with one of Sagemaker's built in algorithms I am able to take advantage of the massive speedup from distributing the job to many instances by increasing the instance_count argument of the training algorithm. However, when I package my own custom algorithm then increasing the instance count seems to just duplicate the training on every instance, leading to no speedup. <\/p>\n\n<p>I suspect that when I am packaging my own algorithm there is something special I need to do to control how it handles the training differently for a particular instance inside of the my custom train() function (otherwise, how would it know how the job should be distributed?), but I have not been able to find any discussion of how to do this online. <\/p>\n\n<p>Does anyone know how to handle this? Thank you very much in advance.<\/p>\n\n<p>Specific examples:\n=> It works well in a standard algorithm: I verified that increasing train_instance_count in the first documented sagemaker example speeds things up here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a><\/p>\n\n<p>=> It does not work in my custom algorithm. I tried taking the standard sklearn build-your-own-model example and adding a few extra sklearn variants inside of the training and then printing out results to compare. When I increase the train_instance_count that is passed to the Estimator object, it runs the same training on every instance, so the output gets duplicated across each instance (the printouts of the results are duplicated) and there is no speedup.\nThis is the sklearn example base: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a> . The third argument of the Estimator object partway down in this notebook is what lets you control the number of training instances.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1517249392277,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48507471",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":14.0,
        "Challenge_reading_time":29.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker custom user algorithms: how to take advantage of extra instances",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1369.0,
        "Challenge_word_count":295,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1368093601223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Distributed training requires having a way to sync the results of the training between the training workers. Most of the traditional libraries, such as scikit-learn are designed to work with a single worker, and can't just be used in a distributed environment. Amazon SageMaker is distributing the data across the workers, but it is up to you to make sure that the algorithm can benefit from the multiple workers. Some algorithms, such as Random Forest, are easier to take advantage of the distribution, as each worker can build a different part of the forest, but other algorithms need more help. <\/p>\n\n<p>Spark MLLib has distributed implementations of popular algorithms such as k-means, logistic regression, or PCA, but these implementations are not good enough for some cases. Most of them were too slow and some even crushed when a lot of data was used for the training. The Amazon SageMaker team reimplemented many of these algorithms from scratch to benefit from the scale and economics of the cloud (20 hours of one instance costs the same as 1 hour of 20 instances, just 20 times faster). Many of these algorithms are now more stable and much faster beyond the linear scalability. See more details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a><\/p>\n\n<p>For the deep learning frameworks (TensorFlow and MXNet) SageMaker is using the built-in parameters server that each one is using, but it is taking the heavy lifting of the building the cluster and configuring the instances to communicate with it. <\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.1,
        "Solution_reading_time":20.32,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":249.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to <code>azure-ml<\/code>, and have been tasked to make some integration tests for a couple of pipeline steps. I have prepared some input test data and some expected output data, which I store on a <code>'test_datastore'<\/code>. The following example code is a simplified version of what I want to do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ws = Workspace.from_config('blabla\/config.json')\nds = Datastore.get(ws, datastore_name='test_datastore')\n\nmain_ref = DataReference(datastore=ds,\n                            data_reference_name='main_ref'\n                            )\n\ndata_ref = DataReference(datastore=ds,\n                            data_reference_name='main_ref',\n                            path_on_datastore='\/data'\n                            )\n\n\ndata_prep_step = PythonScriptStep(\n            name='data_prep',\n            script_name='pipeline_steps\/data_prep.py',\n            source_directory='\/.',\n            arguments=['--main_path', main_ref,\n                        '--data_ref_folder', data_ref\n                        ],\n            inputs=[main_ref, data_ref],\n            outputs=[data_ref],\n            runconfig=arbitrary_run_config,\n            allow_reuse=False\n            )\n<\/code><\/pre>\n<p>I would like:<\/p>\n<ul>\n<li>my <code>data_prep_step<\/code> to run,<\/li>\n<li>have it store some data on the path to my <code>data_ref<\/code>), and<\/li>\n<li>I would then like to access this stored data afterwards outside of the pipeline<\/li>\n<\/ul>\n<p>But, I can't find a useful function in the documentation. Any guidance would be much appreciated.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":7,
        "Challenge_created_time":1616603138180,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1616961515960,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66785273",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":13.6,
        "Challenge_reading_time":17.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"How to acces output folder from a PythonScriptStep?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1327.0,
        "Challenge_word_count":138,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459511191443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":445.0,
        "Poster_view_count":96.0,
        "Solution_body":"<p>two big ideas here -- let's start with the main one.<\/p>\n<h2>main ask<\/h2>\n<blockquote>\n<p>With an Azure ML Pipeline, how can I access the output data of a <code>PythonScriptStep<\/code> outside of the context of the pipeline?<\/p>\n<\/blockquote>\n<h3>short answer<\/h3>\n<p>Consider using <code>OutputFileDatasetConfig<\/code> (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.output_dataset_config.outputdatasetconfig?view=azure-ml-py&amp;viewFallbackFrom=experimental&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">docs<\/a> <a href=\"http:\/\/%20https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-data-in-out-of-pipelines?WT.mc_id=AI-MVP-5003930#use-outputfiledatasetconfig-for-intermediate-data\" rel=\"nofollow noreferrer\">example<\/a>), instead of <code>DataReference<\/code>.<\/p>\n<p>To your example above, I would just change your last two definitions.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>data_ref = OutputFileDatasetConfig(\n    name='data_ref',\n    destination=(ds, '\/data')\n).as_upload()\n\n\ndata_prep_step = PythonScriptStep(\n    name='data_prep',\n    script_name='pipeline_steps\/data_prep.py',\n    source_directory='\/.',\n    arguments=[\n        '--main_path', main_ref,\n        '--data_ref_folder', data_ref\n                ],\n    inputs=[main_ref, data_ref],\n    outputs=[data_ref],\n    runconfig=arbitrary_run_config,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p>some notes:<\/p>\n<ul>\n<li>be sure to check out how <code>DataPath<\/code>s work. Can be tricky at first glance.<\/li>\n<li>set <code>overwrite=False<\/code> in the `.as_upload() method if you don't want future runs to overwrite the first run's data.<\/li>\n<\/ul>\n<h3>more context<\/h3>\n<p><code>PipelineData<\/code> used to be the defacto object to pass data ephemerally between pipeline steps. The idea was to make it easy to:<\/p>\n<ol>\n<li>stitch steps together<\/li>\n<li>get the data after the pipeline runs if need be (<code>datastore\/azureml\/{run_id}\/data_ref<\/code>)<\/li>\n<\/ol>\n<p>The downside was that you have no control over <em>where<\/em> the pipeline is saved. If you wanted to data for more than just as a baton that gets passed between steps, you could have a <code>DataTransferStep<\/code> to land the <code>PipelineData<\/code> wherever you please after the <code>PythonScriptStep<\/code> finishes.<\/p>\n<p>This downside is what motivated <code>OutputFileDatasetConfig<\/code><\/p>\n<h2>auxilary ask<\/h2>\n<blockquote>\n<p>how might I programmatically test the functionality of my Azure ML pipeline?<\/p>\n<\/blockquote>\n<p>there are not enough people talking about data pipeline testing, IMHO.<\/p>\n<p>There are three areas of data pipeline testing:<\/p>\n<ol>\n<li>unit testing (the code in the step works?<\/li>\n<li>integration testing (the code works when submitted to the Azure ML service)<\/li>\n<li>data expectation testing (the data coming out of the meets my expectations)<\/li>\n<\/ol>\n<p>For #1, I think it should be done outside of the pipeline perhaps as part of a package of helper functions\nFor #2, Why not just see if the whole pipeline completes, I think get more information that way. That's how we run our CI.<\/p>\n<p>#3 is the juiciest, and we do this in our pipelines with the <a href=\"https:\/\/greatexpectations.io\/\" rel=\"nofollow noreferrer\">Great Expectations (GE)<\/a> Python library. The GE community calls these &quot;expectation tests&quot;. To me you have two options for including expectation tests in your Azure ML pipeline:<\/p>\n<ol>\n<li>within the <code>PythonScriptStep<\/code> itself, i.e.\n<ol>\n<li>run whatever code you have<\/li>\n<li>test the outputs with GE before writing them out; or,<\/li>\n<\/ol>\n<\/li>\n<li>for each functional <code>PythonScriptStep<\/code>, hang a downstream <code>PythonScriptStep<\/code> off of it in which you run your expectations against the output data.<\/li>\n<\/ol>\n<p>Our team does #1, but either strategy should work. What's great about this approach is that you can run your expectation tests by just running your pipeline (which also makes integration testing easy).<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1616626562700,
        "Solution_link_count":3.0,
        "Solution_readability":12.2,
        "Solution_reading_time":51.55,
        "Solution_score_count":3.0,
        "Solution_sentence_count":27.0,
        "Solution_word_count":453.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was wondering if it was possible to delete particular runs using the Python SDK.   <br \/>\nthis would be rather useful to delete old failed runs.  <br \/>\nit already has functions such as cancel(), fail(), submit(). <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638386026283,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/648089\/is-there-a-way-to-delete-azureml-runs-using-the-py",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":3.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"is there a way to delete azureml runs using the python sdk?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":48,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=2dc066be-691a-47bd-9f7a-67e426d994d9\">@Antara Das  <\/a>  Thanks, Run history documents, which may contain personal user information, are stored in the storage account in blob storage, in subfolders of \/azureml. You can download and delete the data from the portal.    <\/p>\n<p> Here is the document to delete workspace data.     <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-export-delete-data\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-export-delete-data<\/a>    <\/p>\n<p>There is a Private Preview for deleting an experiment, however such functionality does not delete the intermediate data generated for the run or any child run.    <br \/>\n\u2022 Not deleted:    <br \/>\no Files in azureml-blobstore-GUID\/azureml\/{run_id}    <br \/>\no Code snapshot (zip files)    <br \/>\no Pipeline intermediate data and child runs    <br \/>\no Metric data    <\/p>\n<p>\u2022 Deleted    <br \/>\no The output folder content    <br \/>\no Log files<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.2,
        "Solution_reading_time":12.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":116.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"It seems they both assume that the current working dir is where they can find the `.git` and `.dvc` dirs.\r\nWe should correctly detect those paths, as it affects all our logic to e.g. automatically dvc init on behalf of the user.\r\n\r\nRelevant resources:\r\n1. https:\/\/stackoverflow.com\/a\/957978\r\n2. https:\/\/dvc.org\/doc\/command-reference\/root",
        "Challenge_closed_time":1630227.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629832721000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/92",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":9.0,
        "Challenge_reading_time":5.02,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"DVC and Git services don't correctly detect the repo root directory",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":58,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1389700864876,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sydney, New South Wales, Australia",
        "Answerer_reputation_count":193.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am working on a Kedro 0.17.2 project that is running on out-of-memory issues and I'm trying to reduce the memory footprint.<\/p>\n<p>I'm doing the profiling by using <code>mprof<\/code> from the <code>memory-profiler<\/code> library and I noticed that there is always a child process and memory seems to duplicate in the main process after the first computation in the node that is running. Is it possible that Kedro is duplicating the dataframes in memory? And, if so, is there a way to avoid this?<\/p>\n<p>Notes:<\/p>\n<ul>\n<li>I'm using the <code>SequentialRunner<\/code><\/li>\n<li>I'm not using the <code>is_async<\/code> cli option<\/li>\n<li>I'm not using either multithreading or multiprocessing in the node execution<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/87hhM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/87hhM.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634349281137,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69592125",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":10.8,
        "Challenge_reading_time":11.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Kedro - Memory management",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":308.0,
        "Challenge_word_count":120,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389700864876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney, New South Wales, Australia",
        "Poster_reputation_count":193.0,
        "Poster_view_count":42.0,
        "Solution_body":"<p>It turns out this issue is caused by a possible bug in the <code>memory-profiler<\/code> library that is used in the <code>kedro.extras.decorators.memory_profiler.mem_profile<\/code> decorator.<\/p>\n<p>The kedro decorator makes use of the <code>memory_usage<\/code> function in the <code>memory-profiler<\/code> module. It is used to sample the total memory being used by the running function from within the python process.<\/p>\n<p>There is an open issue about this problem but with no solution yet.\n<a href=\"https:\/\/github.com\/pythonprofilers\/memory_profiler\/issues\/332\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pythonprofilers\/memory_profiler\/issues\/332<\/a><\/p>\n<p>For the moment I have just removed the decorator.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.4,
        "Solution_reading_time":9.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":80.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1341842709088,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jo\u00e3o Pessoa - PB, Brasil",
        "Answerer_reputation_count":314.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":38.6383375,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to build an autoencoder, which I'm sure I'm doing something wrong. I tried separating the creation of the model from the actual training but this is not really working out for me and is giving me the following error.<\/p>\n<pre><code>AssertionError: Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 310), dtype=tf.float32, name=None), name='dense_7\/Sigmoid:0', description=&quot;created by layer 'dense_7'&quot;)\n<\/code><\/pre>\n<p>I'm doing this all using the Kedro framework. I have a pipeline.py file with the pipeline definition and a nodes.py with the functions that I want to use. So far, this is my project structure:<\/p>\n<p>pipelines.py:<\/p>\n<pre><code>from kedro.pipeline import Pipeline, node\nfrom .nodes.autoencoder_nodes import *\n\ndef train_autoencoder_pipeline():\n    return Pipeline([\n        # Build neural network\n        node(\n            build_models, \n            inputs=[\n                &quot;train_x&quot;, \n                &quot;params:autoencoder_n_hidden_layers&quot;,\n                &quot;params:autoencoder_latent_space_size&quot;,\n                &quot;params:autoencoder_regularization_strength&quot;,\n                &quot;params:seed&quot;\n                ],\n            outputs=dict(\n                pre_train_autoencoder=&quot;pre_train_autoencoder&quot;,\n                pre_train_encoder=&quot;pre_train_encoder&quot;,\n                pre_train_decoder=&quot;pre_train_decoder&quot;\n            ), name=&quot;autoencoder-create-models&quot;\n        ),\n        # Scale features\n        node(fit_scaler, inputs=&quot;train_x&quot;, outputs=&quot;autoencoder_scaler&quot;, name=&quot;autoencoder-fit-scaler&quot;),\n        node(tranform_scaler, inputs=[&quot;autoencoder_scaler&quot;, &quot;train_x&quot;], outputs=&quot;autoencoder_scaled_train_x&quot;, name=&quot;autoencoder-scale-train&quot;),\n        node(tranform_scaler, inputs=[&quot;autoencoder_scaler&quot;, &quot;test_x&quot;], outputs=&quot;autoencoder_scaled_test_x&quot;, name=&quot;autoencoder-scale-test&quot;),\n\n        # Train autoencoder\n        node(\n            train_autoencoder, \n            inputs=[\n                &quot;autoencoder_scaled_train_x&quot;,\n                &quot;autoencoder_scaled_test_x&quot;,\n                &quot;pre_train_autoencoder&quot;, \n                &quot;pre_train_encoder&quot;, \n                &quot;pre_train_decoder&quot;,\n                &quot;params:autoencoder_epochs&quot;,\n                &quot;params:autoencoder_batch_size&quot;,\n                &quot;params:seed&quot;\n            ],\n            outputs= dict(\n                autoencoder=&quot;autoencoder&quot;,\n                encoder=&quot;encoder&quot;,\n                decoder=&quot;decoder&quot;,\n                autoencoder_history=&quot;autoencoder_history&quot;,\n            ),\n            name=&quot;autoencoder-train-model&quot;\n        )])\n<\/code><\/pre>\n<p>nodes.py:<\/p>\n<pre><code>from sklearn.preprocessing import MinMaxScaler\nfrom tensorflow import keras\nimport tensorflow as tf\n\nfrom typing import Dict, Any, Tuple\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport logging\n\n\ndef build_models(data: pd.DataFrame, n_hidden_layers: int, latent_space_size: int, retularization_stregth: float, seed: int) -&gt; Tuple[keras.Model, keras.Model, keras.Model]:\n    assert n_hidden_layers &gt;= 1, &quot;There must be at least 1 hidden layer for the autoencoder&quot;\n    \n    n_features = data.shape[1]\n    tf.random.set_seed(seed)\n    input_layer = keras.Input(shape=(n_features,))\n    \n    hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(input_layer)\n    hidden = keras.layers.LeakyReLU()(hidden)\n    \n    for _ in range(n_hidden_layers - 1):\n        hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(hidden)\n        hidden = keras.layers.LeakyReLU()(hidden)\n    \n    encoded = keras.layers.Dense(latent_space_size, activation=&quot;sigmoid&quot;)(hidden)\n\n    hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(encoded)\n    hidden = keras.layers.LeakyReLU()(hidden)\n    \n    for _ in range(n_hidden_layers - 1):\n        hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(hidden)\n        hidden = keras.layers.LeakyReLU()(hidden)\n    \n\n    decoded = keras.layers.Dense(n_features, activation=&quot;sigmoid&quot;)(hidden)\n\n    # Defines the neural networks\n    autoencoder = keras.models.Model(inputs=input_layer, outputs=decoded)\n    encoder = keras.models.Model(inputs=input_layer, outputs=encoded)\n    decoder = keras.models.Model(inputs=input_layer, outputs=decoded)\n    autoencoder.compile(optimizer=&quot;adam&quot;, loss=&quot;mean_absolute_error&quot;)\n\n    return dict(\n        pre_train_autoencoder=autoencoder,\n        pre_train_encoder=encoder,\n        pre_train_decoder=decoder\n    )\n\ndef fit_scaler(data: pd.DataFrame) -&gt; MinMaxScaler:\n    scaler = MinMaxScaler()\n    scaler.fit(data)\n    return scaler\n\ndef tranform_scaler(scaler: MinMaxScaler, data: pd.DataFrame) -&gt; np.array:\n    return scaler.transform(data)\n\ndef train_autoencoder(\n    train_x: pd.DataFrame, test_x: pd.DataFrame, \n    autoencoder: keras.Model, encoder: keras.Model, decoder: keras.Model, \n    epochs: int, batch_size: int, seed: int) -&gt; Dict[str, Any]:\n\n    tf.random.set_seed(seed)\n    callbacks = [\n        keras.callbacks.History(),\n        keras.callbacks.EarlyStopping(patience=3)\n    ]\n    logging.info(train_x.shape)\n    logging.info(test_x.shape)\n\n    history = autoencoder.fit(\n        train_x, train_x,\n        validation_data=(test_x, test_x),\n        callbacks=callbacks, \n        epochs=epochs,\n        batch_size=batch_size\n    )\n\n    return dict(\n        autoencoder=autoencoder,\n        encoder=encoder,\n        decoder=decoder,\n        autoencoder_history=history,\n    )\n<\/code><\/pre>\n<p>catalog.yaml:<\/p>\n<pre><code>autoencoder_scaler:\n  type: pickle.PickleDataSet\n  filepath: data\/06_models\/autoencoder_scaler.pkl\n\nautoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/autoencoder.h5\n\nencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/encoder.h5\n\ndecoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/decoder.h5\n\nautoencoder_train_x:\n  type: pandas.CSVDataSet\n  filepath: data\/04_feature\/autoencoder_train_x.csv\n\nautoencoder_test_x:\n  type: pandas.CSVDataSet\n  filepath: data\/04_feature\/autoencoder_test_x.csv\n<\/code><\/pre>\n<p>And finally parameters.yaml:<\/p>\n<pre><code>seed: 200\n# Autoencoder\nautoencoder_n_hidden_layers: 3\nautoencoder_latent_space_size: 15\nautoencoder_epochs: 100\nautoencoder_batch_size: 32\nautoencoder_regularization_strength: 0.001\n<\/code><\/pre>\n<p>I believe that Keras is not seeing the whole graph since they will be out of the scope for the buld_models function, but I'm not sure whether this is the case, or how to fix it. Any help would be appreciated.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629078707297,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68796641",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":19.0,
        "Challenge_reading_time":85.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":62,
        "Challenge_solved_time":null,
        "Challenge_title":"Building an autoencoder with Keras and Kedro",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":245.0,
        "Challenge_word_count":439,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423164285360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Itabira, Brazil",
        "Poster_reputation_count":856.0,
        "Poster_view_count":106.0,
        "Solution_body":"<p>I was able to set up your project locally and reproduce the error. To fix it, I had to add the <code>pre_train_*<\/code> outputs to the catalog as well. Therefore, it's my <code>catalog.yaml<\/code> file:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>autoencoder_scaler:\n  type: pickle.PickleDataSet\n  filepath: data\/06_models\/autoencoder_scaler.pkl\n\npre_train_autoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_autoencoder.h5\n\npre_train_encoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_encoder.h5\n\npre_train_decoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_decoder.h5\n\nautoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/autoencoder.h5\n\nencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/encoder.h5\n\ndecoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/decoder.h5\n<\/code><\/pre>\n<p>Also, I changed the return of <code>train_autoencoder<\/code> node to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>return dict(\n    autoencoder=autoencoder,\n    autoencoder_history=history.history,\n)\n<\/code><\/pre>\n<p>Note that I changed the <code>autoencoder_history<\/code> to return <code>history.history<\/code> since <code>MemoryDataset<\/code> can't pickle the object <code>history<\/code> by itself. The <code>history.history<\/code> is a dictionary with losses of train and validation sets.<\/p>\n<p>You can find the complete code <a href=\"https:\/\/gist.github.com\/arnaldog12\/a3f7801fe3910c02f4bcea8be61b910c\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1629217805312,
        "Solution_link_count":1.0,
        "Solution_readability":19.6,
        "Solution_reading_time":23.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":127.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":15,
        "Challenge_body":"This guidance results in an error:\r\n\r\n\"To install the default packages in an environment without a previous version of the package installed, run the following command.\" \r\n\r\nPS C:\\> pip install azureml-sdk\r\n\r\n`ERROR: Could not find a version that satisfies the requirement azureml-sdk (from versions: none)\r\nERROR: No matching distribution found for azureml-sdk`\r\n\r\nWhat am I missing?\r\n\r\nThanks,\r\nclaw\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 8e0e12a4-b363-2726-06b4-9db2015efb32\r\n* Version Independent ID: e39a91ac-375b-a2cc-350d-a82cb7b0b035\r\n* Content: [Install the Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/install.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/install.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @harneetvirk\r\n* Microsoft Alias: **harnvir**",
        "Challenge_closed_time":1637097.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609957275000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1285",
        "Challenge_link_count":2,
        "Challenge_participation_count":15,
        "Challenge_readability":13.9,
        "Challenge_reading_time":14.88,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Error Installing Azureml. (Python 3.9 support)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":110,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@klawrawkz :  What is your OS? What is the python and pip version? \r\n\r\nazureml-sdk only supports Python 3.5 to 3.8. So, if you're using an out-of-range version of Python (older or newer), then you'll need to use a different version. Thanks for the reply @harneetvirk. I'm pretty sure it's not a python version issue.\r\n```\r\npy --version\r\nPython 3.9.1\r\n```\r\nCould be a Win 10 version issue?\r\n![image](https:\/\/user-images.githubusercontent.com\/48074223\/103943498-2f478c00-5100-11eb-9bfd-43443a4cb582.png)\r\n\r\nI ran this command and got farther. \r\n```\r\npip install --upgrade --upgrade-strategy eager azureml-sdk\r\n```\r\nI am stuck at this point now.\r\n```\r\n...\r\nINFO: pip is looking at multiple versions of azure-core to determine which version is compatible with other requirements. This could take a while.\r\nINFO: pip is looking at multiple versions of azure-mgmt-containerregistry to determine which version is compatible with other requirements. This could take a while.\r\nCollecting azure-mgmt-containerregistry>=2.0.0\r\n  Downloading azure_mgmt_containerregistry-2.7.0-py2.py3-none-any.whl (509 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 509 kB ...\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https:\/\/pip.pypa.io\/surveys\/backtracking\r\n  Downloading azure_mgmt_containerregistry-2.6.0-py2.py3-none-any.whl (501 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501 kB 1.6 MB\/s\r\nINFO: pip is looking at multiple versions of azure-mgmt-core to determine which version is compatible with other requirements. This could take a while.\r\n  Downloading azure_mgmt_containerregistry-2.5.0-py2.py3-none-any.whl (494 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 494 kB 6.4 MB\/s\r\n  Downloading azure_mgmt_containerregistry-2.4.0-py2.py3-none-any.whl (482 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 482 kB 6.4 MB\/s\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https:\/\/pip.pypa.io\/surveys\/backtracking\r\n  Downloading azure_mgmt_containerregistry-2.3.0-py2.py3-none-any.whl (481 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 481 kB 6.8 MB\/s\r\n```\r\n\r\nWhat's your advice on commands to provide \"stricter constraints to reduce runtime?\" The command (above) has been \"running\" for ~24 hours, so I'm guessing that it's dead in the H20.\r\n\r\nKlaw azureml-sdk only supports Python 3.5 to 3.8, but you are having python 3.9.1 installed in the environment.  Please change the python version between 3.5 to 3.8.\r\n\r\nAlso, the latest pip 20.3 has a new dependency resolver which is resulting in this long running dependency resolutions. If you switch to older version of pip (<20.3), you will notice the difference in the performance. Gotcha, thanks for the info. I'll make the change.\r\n\r\nKlaw If azureml-sdk does not support Python 3.9, then the metadata should be updated from:\r\n```\r\nRequires-Python: >=3.5,<4\r\n```\r\nto:\r\n```\r\nRequires-Python: >=3.5,<3.9\r\n```\r\nIs this also true for the hundreds of subpackages that azureml-sdk depends on? When is Python 3.9 support coming? when will azureml-core be compatible with python 3.9? I am currently using azureml-sdk under Python 3.9 by installing with pip's `--ignore-requires-python` option, and everything I am using seems to work fine. But there are probably some other parts that don't work... @johan12345 is this in production environment? you are using it like this? or in your local env? In my local development environment.  `azureml-core` now supports Python 3.9. unfortunately although `azureml-core` might install w\/o errors in 3.9, `azureml-sdk` still creates errors. Installed w\/o errors in 3.8.12   azureml-sdk is a meta package.  azureml-core is one of the upstream that supports python 3.9 but there are some other AutoML dependencies in azureml-sdk  which do not support python 3.9.\r\n I have just updated azureml-sdk to allow Python 3.9.\r\nThis should be included in the next Azure ML SDK release, 1.45.0. What about 3.10? 3.11 is coming out soon too. @adamjstewart Python 3.10 is already supported in the new SDK V2 preview: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-v2\r\nI expect that we will support 3.10 in SDK V1 as well but I don't have a date for that.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":5.2,
        "Solution_reading_time":55.84,
        "Solution_score_count":null,
        "Solution_sentence_count":77.0,
        "Solution_word_count":608.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"**Describe the bug**\r\nOccasionally after starting a Sagemaker workspace, clicking 'Connect' gives an error in the bottom right-hand corner of the screen:\r\n\r\n> We have a problem!\r\n> null is not an object (evaluating 'l.location=s') \r\n\r\nin a little red box on the bottom-right of the screen. The notebook window is not opened after clicking on 'Connect'.\r\n\r\n**To Reproduce**\r\nThe error is intermittent. I *think* it may happen after the SW window has been open a while, because I noticed that the SW window automatically logged me out shortly after seeing this error.\r\n\r\n1. Click 'Start' for Sagemaker workspace and wait for the status to change to 'Available'. \r\n2. Click 'Connections', then 'Connect'\r\n3. See error\r\n\r\nWhen I logged out and back into Service Workbench, and was able to connect to the workspace successfully. \r\n\r\n**Expected behavior**\r\nA new window should open with a Jupyter\/Sagemaker notebook in a new window. \r\n\r\n**Versions (please complete the following information):**\r\n - 3.2.0\r\n",
        "Challenge_closed_time":1643923.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628006476000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.5,
        "Challenge_reading_time":12.78,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"\"null is not an object\" while trying to connect to Sagemaker notebook.",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":164,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @tom-christie, we believe the issue mentioned is due to access token getting expired. Please feel free to use the latest version with the fix ([v3.3.1](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v3.3.1)). We're seeing this issue on 4.1.1 as well. However, it appears to be persistent (i.e. it happens every time we connect to a SageMaker workspace). So far, we've only tested a single workspace config, but the error consistently shows up when we try to connect to different workspace instances using the same config. The workspace instances are new and running, at least as shown in the SWB UI. We haven't verified if the instances are available in the SageMaker console, however. Is it possible this is related to a popup blocker as reported in GALI-1224? It creates a similar error message.\r\nhttps:\/\/sim.amazon.com\/issues\/CHAMDOC-17 Yeah, I've seen the error happen because popups are disabled for the SWB domain. Once you enable popup for the SWB domain, it should allow you to connect to Sagemaker. Feel free to reopen this ticket if enabling popups didn't resolve your issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.0,
        "Solution_reading_time":13.74,
        "Solution_score_count":null,
        "Solution_sentence_count":14.0,
        "Solution_word_count":171.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1547219189528,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've created an AML Pipeline with a single DatabricksStep. I've need to pass a parameter to the Databricks notebook when I run the published pipeline.<\/p>\n<p>When I run the published pipeline, the Databricks steps always take the default value of the PipelineParameter, no matter what value I choose when I submit the pipeline.<\/p>\n<p>Here the code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>start_date_param = PipelineParameter(name=&quot;StartDate&quot;, default_value='2022-01-19')\n\n# Define data ingestion step\ndata_loading_step = DatabricksStep(\n        name=&quot;Data Loading&quot;,\n        existing_cluster_id=db_cluster_id,\n        notebook_path=data_loading_path,\n        run_name=&quot;Loading raw data&quot;,\n        notebook_params={\n            'StartDate': start_date_param,\n        },\n        compute_target=dbricks_compute,\n        instance_pool_id=instance_pool_id,\n        num_workers=num_workers,\n        allow_reuse=False\n    )\n<\/code><\/pre>\n<p>And this is the Databricks notebook:<\/p>\n<pre><code>dbutils.widgets.text(&quot;StartDate&quot;, &quot;&quot;, &quot;StartDate(YYYY-MM-DD)&quot;)\n<\/code><\/pre>\n<p>The default value of StartDate is 2022-01-19.Even though I set the StartDate parameter to '2021-01-19' the Databricks notebook still takes 2022-01-19 as StartDate.<\/p>\n<p>What am I doing wrong?<\/p>\n<p>Thanks for any help,<\/p>\n<p>G<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642600664140,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70771911",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":17.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use PipelineParameter in DatabricksStep (Python)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":175.0,
        "Challenge_word_count":131,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1547219189528,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I've found the solution of the problem. I hope it can be of help to someone.\nIt works if you set all the parameter name and widget name in lower case.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>start_date_param = PipelineParameter(name=&quot;start_date&quot;, default_value='2022-01-19')\n\n# Define data ingestion step\ndata_loading_step = DatabricksStep(\n        name=&quot;Data Loading&quot;,\n        existing_cluster_id=db_cluster_id,\n        notebook_path=data_loading_path,\n        run_name=&quot;Loading raw data&quot;,\n        notebook_params={\n            'start_date': start_date_param,\n        },\n        compute_target=dbricks_compute,\n        instance_pool_id=instance_pool_id,\n        num_workers=num_workers,\n        allow_reuse=False\n    )\n<\/code><\/pre>\n<pre><code>dbutils.widgets.text(&quot;start_date&quot;, &quot;&quot;, &quot;start_date(YYYY-MM-DD)&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":10.81,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"#### Environment details\r\n\r\n  - OS: Mac M1 Pro\r\n  - Node.js version: v16.16.0\r\n  - npm version: 8.11.0\r\n  - `@google-cloud\/aiplatform` version: ^2.3.0\r\n\r\n#### Steps to reproduce\r\n\r\n  1. I've run this demo on my local computer: https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-text-classification.js\r\n  2. The process paused and shows `4 DEADLINE_EXCEEDED: Deadline exceeded` in the line: `await predictionServiceClient.predict(request);`\r\n\r\n\r\nThanks!\r\n",
        "Challenge_closed_time":1664935.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664933940000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/issues\/453",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":6.83,
        "Challenge_repo_contributor_count":20.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":558.0,
        "Challenge_repo_star_count":29.0,
        "Challenge_repo_watch_count":42.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"vertex AI endpoint prediction error, 4 DEADLINE_EXCEEDED: Deadline exceeded",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":53,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"> \r\n\r\nWhen I upgrade the nodejs to v16.17.1 and add a call_option\r\n`\r\n      const call_options = {\r\n        timeout: 200000 \/\/ millis\r\n      }\r\n`\r\nproblem solved.\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.59,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"In particular:\r\n- Experiments needs to be renamed to Jobs\r\n- Datasets needs to be renamed to Data\r\n\r\nFurther changes probably aren't absolutely necessary right now, but should be considered as well. See #616.",
        "Challenge_closed_time":1663956.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661895451000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1691",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":3.19,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Update Treeview asset labels to match Azure ML Studio.",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":40,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1562706291280,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have been following this video:\n<a href=\"https:\/\/www.youtube.com\/watch?v=1ykDWsnL2LE&amp;t=310s\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=1ykDWsnL2LE&amp;t=310s<\/a><\/p>\n<p>Code located at:\n<a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5\" rel=\"nofollow noreferrer\">https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5<\/a>\n(I have done the last two steps as per the video which isn't an issue for google_cloud_pipeline_components version: 0.1.1)<\/p>\n<p>I have created a pipeline in vertex ai which ran and used the following code to create the pipeline (from video not code extract in link above):<\/p>\n<pre><code>#run pipeline\nresponse = api_client.create_run_from_job_spec(\n    &quot;tab_classif_pipeline.json&quot;, pipeline_root = PIPELINE_ROOT,\n    parameter_values = {\n    &quot;project&quot; : PROJECT_ID,\n    &quot;display_name&quot; : DISPLAY_NAME\n    }\n)\n    \n<\/code><\/pre>\n<p>and in the GCP logs I get the following error:<\/p>\n<pre><code>&quot;google.api_core.exceptions.FailedPrecondition: 400 BigQuery Dataset location `eu` must be in the same location as the service location `us-central1`.\n<\/code><\/pre>\n<p>I get the error at the dataset_create_op stage:<\/p>\n<pre><code>    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n    project = project, display_name = display_name, bq_source = bq_source\n)\n<\/code><\/pre>\n<p>My dataset is configured in EU (the whole region) so I don't understand where us-central1 is coming from (or what the service location is?).<\/p>\n<p>Here is the all the code I have used:<\/p>\n<pre><code> PROJECT_ID = &quot;marketingtown&quot;\n BUCKET_NAME = f&quot;gs:\/\/lookalike_model&quot;\n from typing import NamedTuple\n import kfp\n from kfp import dsl\n from kfp.v2 import compiler\n from kfp.v2.dsl import (Artifact, Input, InputPath, Model, Output, \n                            OutputPath, ClassificationMetrics, \n Metrics, component)\n from kfp.v2.components.types.artifact_types import Dataset\n from kfp.v2.google.client import AIPlatformClient\n from google.cloud import aiplatform\n from google_cloud_pipeline_components import aiplatform as gcc_aip\n\n #set environment variables\n PATH = %env PATH\n %env PATH = (PATH):\/\/home\/jupyter\/.local\/bin\n REGION = &quot;europe-west2&quot;\n    \n #cloud storage path where artifact is created by pipeline\n PIPELINE_ROOT = f&quot;{BUCKET_NAME}\/pipeline_root\/&quot;\n PIPELINE_ROOT\n import time\n DISPLAY_NAME = f&quot;lookalike_model_pipeline_{str(int(time.time()))}&quot;\n print(DISPLAY_NAME)\n \n@kfp.dsl.pipeline(name = &quot;lookalike-model-training-v2&quot;, \npipeline_root = PIPELINE_ROOT)\n\ndef pipeline(\n    bq_source : str = f&quot;bq:\/\/{PROJECT_ID}.MLOp_pipeline_temp.lookalike_training_set&quot;,\n    display_name : str = DISPLAY_NAME,\n    project : str = PROJECT_ID,\n    gcp_region : str = &quot;europe-west2&quot;,\n    api_endpoint : str = &quot;europe-west2-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str : str = '{&quot;auPrc&quot; : 0.3}'\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project = project, display_name = display_name, bq_source = bq_source\n    )\n    \n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=[\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;agentId&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;postcode&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;isMobile&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;gender&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;timeOfDay&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;sale&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;], #dataset from previous step\n        target_column=&quot;sale&quot;,\n    )\n    \n    #outputted evaluation metrics\n    model_eval_task = classification_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n    )\n    \n    #if deployment threshold is mean, deploy\n    with dsl.Condition(\n        model_eval_task.outputs[&quot;dep_decision&quot;] == &quot;true&quot;,\n        name=&quot;deploy_decision&quot;,\n    ):\n        \n    endpoint_op = gcc_aip.EndpointCreateOp(\n        project=project,\n        location=gcp_region,\n        display_name=&quot;train-automl-beans&quot;,\n    )\n        \n    #deploys model to an endpoint\n    gcc_aip.ModelDeployOp(\n        model=training_op.outputs[&quot;model&quot;],\n        endpoint=endpoint_op.outputs[&quot;endpoint&quot;],\n        min_replica_count=1,\n        max_replica_count=1,\n        machine_type=&quot;n1-standard-4&quot;,\n        )\n   \n\n     compiler.Compiler().compile(\n        pipeline_func = pipeline, package_path = &quot;tab_classif_pipeline.json&quot;\n    )\n\n    #run pipeline\n    response = api_client.create_run_from_job_spec(\n        &quot;tab_classif_pipeline.json&quot;, pipeline_root = PIPELINE_ROOT,\n        parameter_values = {\n        &quot;project&quot; : PROJECT_ID,\n        &quot;display_name&quot; : DISPLAY_NAME\n        }\n    )\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":5,
        "Challenge_created_time":1645657042940,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1645697298212,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71245000",
        "Challenge_link_count":4,
        "Challenge_participation_count":7,
        "Challenge_readability":20.5,
        "Challenge_reading_time":65.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"Vertex AI Pipeline Failed Precondition",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":420.0,
        "Challenge_word_count":355,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562706291280,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>I solved this issue by adding the location to the TabularDatasetCreateJob:<\/p>\n<pre><code>    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n    project=project,\n    display_name=display_name, \n    bq_source=bq_source,\n    location = gcp_region\n)\n<\/code><\/pre>\n<p>I now have the same issue with the model training job but I have learnt that a lot of the functions in the above code take a location parameter, or default to us-central1. I will update if I get any further.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.4,
        "Solution_reading_time":5.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":61.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi    <br \/>\nI am unable to create a VM in Compute. Status is at Creating for an hour and then it fails.     <br \/>\nI tried several times without luck.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/78763-image.png?platform=QnA\" alt=\"78763-image.png\" \/>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/78781-image.png?platform=QnA\" alt=\"78781-image.png\" \/>    <\/p>\n<p>Does anyone know how to solve this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615983139490,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/318685\/cant-create-a-vm-in-compute-creation-failed",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":10.9,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Can't create a VM in compute - Creation failed",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":52,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I was able to delete all the failed VMs and create one today.  <br \/>\nThe solution in this case was to wait it out.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":1.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In the sagemaker pipeline example shown here<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html<\/a><\/p>\n<p>I see two lines joining from AbaloneProcess to AbaloneTrain and AbaloneEval respectively. However, based on the code, I would expect it to actually be connected from AbaloneTrain only, then to AbaloneEval in a single path. Can somebody explain to me what is actually happening here because I am struggling to wrap my head around this. Much appreciated and apologies for the inconvenience in advance<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660801355963,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73397959",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":9.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Question regarding sagemaker pipeline example on AWS?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":16.0,
        "Challenge_word_count":81,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644981356940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The connections are indicated data dependencies between steps, not only the order of execution. This is what you see AbaloneProcess connected to AbaloneEval, since the output of AbaloneProcess is used in AbaloneEval.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":2.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1393579668636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":1450.0,
        "Answerer_view_count":162.0,
        "Challenge_adjusted_solved_time":0.1505822222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Kedro, we can pipeline different nodes and partially run some nodes. When we are partially running some nodes, we need to save some inputs from the nodes somewhere so that when another node is run it can access the data that the previous node has generated. However, in which file do we write the code for this - pipeline.py, run.py or nodes.py?<\/p>\n\n<p>For instance, I am trying to save a dir path directly to the DataCatalog under a variable name 'model_path'. <\/p>\n\n<p>Snippet from pipeline.py:<\/p>\n\n<pre><code>    # A mapping from a pipeline name to a ``Pipeline`` object.\ndef create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\nio = DataCatalog(dict(\n    model_path=MemoryDataSet()\n))\n\nio.save('model_path', \"data\/06_models\/model_test\")\nprint('****', io.exists('model_path'))\n\npipeline = Pipeline([\n    node(\n        split_files,\n        [\"data_csv\", \"parameters\"],\n        [\"train_filenames\", \"val_filenames\", \"train_labels\", \"val_labels\"],\n        name=\"splitting filenames\"\n    ),\n    # node(\n    #     create_and_train,\n    #     [\"train_filenames\", \"val_filenames\", \"train_labels\", \"val_labels\", \"parameters\"],\n    #     \"model_path\",\n    #     name=\"Create Dataset, Train and Save Model\"\n    # ),\n    node(\n        validate_model,\n        [\"val_filenames\", \"val_labels\", \"model_path\"],\n        None,\n        name=\"Validate Model\",\n    )\n\n]).decorate(decorators.log_time, decorators.mem_profile)\n\nreturn {\n    \"__default__\": pipeline\n}\n<\/code><\/pre>\n\n<p>However, I get the following error when I Kedro run:<\/p>\n\n<pre><code>ValueError: Pipeline input(s) {'model_path'} not found in the DataCatalog\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1571371429583,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1571671894436,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58443788",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":19.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Where to perform the saving of an nodeoutput in Kedro?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1606.0,
        "Challenge_word_count":177,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545311054088,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":170.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>Node inputs are automatically loaded by Kedro from the <code>DataCatalog<\/code> before being passed to the node function. Node outputs are consequently saved to the DataCatalog after the node successfully produces some data. DataCatalog configuration by default is taken from <code>conf\/base\/catalog.yml<\/code>. <\/p>\n\n<p>In your example <code>model_path<\/code> is produced by <code>Create Dataset, Train and Save Model<\/code> node and then consumed by <code>Validate Model<\/code>. If required dataset definition is not found in the <code>conf\/base\/catalog.yml<\/code>, Kedro will try to store this dataset in memory using <code>MemoryDataSet<\/code>. This will work if you run the pipeline that contains both <code>Create Dataset...<\/code> and <code>Validate Model<\/code> nodes (given no other issues arise). However, when you are trying to run <code>Validate Model<\/code> node alone, Kedro attempts to read <code>model_path<\/code> dataset from memory, which doesn't exist there.<\/p>\n\n<p>So, <strong>TLDR<\/strong>:<\/p>\n\n<p>To mitigate this, you need to:<\/p>\n\n<p>a) persist <code>model_path<\/code> by adding something like the following to your <code>conf\/base\/catalog.yml<\/code>:<\/p>\n\n<pre><code>model_path:\n  type: TextLocalDataSet\n  filepath: data\/02_intermediate\/model_path.txt\n<\/code><\/pre>\n\n<p>b) run <code>Create Dataset, Train and Save Model<\/code> node (and its dependencies) at least once<\/p>\n\n<p>After completing a) and b) you should be able to start running <code>Validate Model<\/code> separately.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1571672436532,
        "Solution_link_count":0.0,
        "Solution_readability":12.8,
        "Solution_reading_time":19.39,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":183.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":1.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi. \n\nIs it possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance every time which is somewhat time consuming? I'm familiar with local mode, but I understand this is not supported when using AWS SageMaker machine learning estimators.\n\nAppreciate any suggestions for how to make the model training process in SageMaker go faster when using AWS SageMaker machine learning estimators.\n\nThanks,\nStefan",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641871148701,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1668610348310,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUsy3vkTMkSA2ojA1bmafDSA\/train-machine-learning-model-using-reserved-instance",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":6.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Train machine learning model using reserved instance",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":354.0,
        "Challenge_word_count":84,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"As of today, it's not possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance. The service team is currently working on it, unfortunately I don't have an ETA as to when the feature will be released.\n\nLocal Mode is supported for frameworks images (TensorFlow, MXNet, Chainer, PyTorch, and Scikit-Learn) and images you supply yourself.\n\n[Using the SageMaker Python SDK \u2014 sagemaker 2.72.3 documentation](https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode)\n\nIf you want to train Built-in algorithm models simply faster, you should check the recommendation in the SageMaker document.\n\nExample [Blazingtext-instances](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html#blazingtext-instances), [Deepar-instances](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar.html#deepar-instances)\n\nIf the algorithm supports it, one can also try using Pipe mode or FastFile mode. These offer some fast training job startup time. [Accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker](https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/)",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1642148033228,
        "Solution_link_count":4.0,
        "Solution_readability":18.5,
        "Solution_reading_time":16.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":124.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1280505139752,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bangalore, India",
        "Answerer_reputation_count":4265.0,
        "Answerer_view_count":403.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was trying Azure Machine Learning Services following this tutorial (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/quickstart-installation\" rel=\"nofollow noreferrer\">Link<\/a>). After successfully creating the Azure Machine Learning services accounts, I successfully installed the Workbench on my Windows 10 Laptop (Behind Proxy; Proxy has been configured at the WorkBench). Next, I was trying to create project following this section (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/quickstart-installation#create-a-project-in-workbench\" rel=\"nofollow noreferrer\">Link<\/a>). Once I click on the Create button, it goes to \"Creating\" state and stays there for ever. The errors displayed at Errors.log is the following. Any suggestion will be appreciated. <\/p>\n\n<pre><code>[2018-07-09 09:47:08.437] [ERROR] HttpService - {\"event\":\"HttpService\",\"task\":\"Failed\",\"data\":{\"url\":\"http:\/\/localhost:54240\/projects\/v1.0\/create\/template\",\"status\":500,\"statusText\":\"INKApi Error\",\"jsonError\":null,\"requestId\":null,\"sessionType\":\"Workbench\"},\"sid\":\"365395c0-832b-11e8-b4ce-e5d7046c6143\"}\n\n[2018-07-09 09:47:08.960] [ERROR] CreateProjectForm - {\"event\":\"CreateProject\",\"task\":\"Error\",\"data\":{\"_body\":null,\"status\":500,\"ok\":false,\"statusText\":\"INKApi Error\",\"headers\":{\"Date\":[\"Mon\",\" 09 Jul 2018 04:17:06 GMT\"],\"Via\":[\"1.1 localhost.localdomain\"],\"Proxy-Connection\":[\"close\"],\"Content-Length\":[\"0\"],\"Content-Type\":[\"text\/html\"]},\"type\":2,\"url\":\"http:\/\/localhost:54240\/projects\/v1.0\/create\/template\"},\"sid\":\"365395c0-832b-11e8-b4ce-e5d7046c6143\"}\n\n[2018-07-09 09:47:08.963] [FATAL] ExceptionLogger - {\"event\":\"exception\",\"task\":\"\",\"data\":{\"message\":\"Cannot read property 'error' of null\",\"name\":\"TypeError\",\"stack\":\"TypeError: Cannot read property 'error' of null\\n    at SafeSubscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:61476:58)\\n    at SafeSubscriber.__tryOrUnsub (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212279:20)\\n    at SafeSubscriber.error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212241:30)\\n    at Subscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212172:30)\\n    at Subscriber.error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212146:22)\\n    at MergeMapSubscriber.OuterSubscriber.notifyError (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:210968:30)\\n    at InnerSubscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:211072:25)\\n    at InnerSubscriber.Subscriber.error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:212146:22)\\n    at DeferSubscriber.OuterSubscriber.notifyError (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:210968:30)\\n    at InnerSubscriber._error (file:\/\/\/C:\/Users\/MyUser\/AppData\/Local\/AmlWorkbench\/resources\/app.asar\/src\/App\/main.bundle.js:211072:25)\"},\"sid\":\"365395c0-832b-11e8-b4ce-e5d7046c6143\"}\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531112127690,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51238413",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":29.4,
        "Challenge_reading_time":46.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Workbench hangs while creating new project",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":67.0,
        "Challenge_word_count":160,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1280505139752,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":4265.0,
        "Poster_view_count":403.0,
        "Solution_body":"<p>It was happening because of the Proxy (although I have configured the Proxy on the Workbench). When I am connected to internet directly, everything works fine (Able to create project, train, compare models etc). However the Workbench should return meaningful error instead of hanging or simply waiting while creating the project.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":4.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1334851920780,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":158.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I write some training job on AWS-SageMaker framework.<\/p>\n\n<p>For some it's requirements, it needs know the job-name of which current running on.<\/p>\n\n<p>I know this code works for it ...<\/p>\n\n<pre><code>import sagemaker_containers\nenv = sagemaker_containers.training_env()\njob_name = env['job_name']\n<\/code><\/pre>\n\n<p>But <code>sagemaker_containers<\/code> package has been deprecated. (I read that on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"nofollow noreferrer\">it's GitHub<\/a>)<\/p>\n\n<p>What should i do?<\/p>\n\n<p>I just started learning about this platform last month. I would appreciate any advice. Thank you.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592537326547,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62462790",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":9.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I get current job-name in SageMaker training job script?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":753.0,
        "Challenge_word_count":82,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1401925028590,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":113.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>For older containers using the deprecated <code>sagemaker_containers<\/code>, the approach you described is correct.<\/p>\n<p>For newer containers that use <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\" rel=\"nofollow noreferrer\"><code>sagemaker-training-toolkit<\/code><\/a>, this is how you retrieve information about the environment: <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit#get-information-about-the-container-environment\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit#get-information-about-the-container-environment<\/a><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker_training import environment\n\nenv = environment.Environment()\n\njob_name = env[&quot;job_name&quot;]\n<\/code><\/pre>\n<p>You can check the <a href=\"https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/dlc-release-notes.html\" rel=\"nofollow noreferrer\">DLC Release Notes<\/a> to see what's installed in each version.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":26.9,
        "Solution_reading_time":13.4,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I have been successfully building and operating machine learning pipelines with Azure SDK, but there is something I fail to fully understand, and I'm wondering if my code can be simplified in some way.<\/p>\n<p>Let's say I have a simple pipeline with two steps: the first step processes data located at 'training_data_path' in Blob storage and then saves it to the same location, and the second step reads that processed data to do something else. So my code is as follows:<\/p>\n<pre><code>def_data_store = ws.get_default_datastore()\ntraining_data_path = (def_data_store, 'training_data')\n\nstep_1_config = OutputFileDatasetConfig(destination = training_data_path)\nstep_2_config = OutputFileDatasetConfig(destination = training_data_path)\n\nstep_1 = PythonScriptStep(\n    name=&quot;Step 1&quot;,\n    script_name=&quot;step_1.py&quot;,\n    source_directory=&quot;.\/&quot;,\n    outputs=[step_1_config],\n    arguments = [\n        &quot;--training-data-path&quot;, step_1_config\n        ],    \n    compute_target=compute_target,\n    runconfig=aml_run_config,\n    allow_reuse=False\n)\n\nstep_2 = PythonScriptStep(\n    name=&quot;Step 2&quot;,\n    script_name=&quot;step_2.py&quot;,\n    source_directory=&quot;.\/&quot;,\n    inputs=[step_1_config.as_input('training_data')],\n    arguments = [\n        &quot;--training-data-path&quot;, step_2_config\n        ],    \n    compute_target=compute_target,\n    runconfig=aml_run_config,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p>I have two questions about that:<\/p>\n<p>1) Even though the path to the data is the same in each step, it seems like I have to create a separate OutputFileDatasetConfig object for each step. So if my pipeline has 10 steps, I will create step_1_config, step_2_config, step_3_config... Isn't there a way to reuse the same OutputFileDatasetConfig object for multiple steps?<\/p>\n<p>2) As far as I know, in step 2, I could delete the 'inputs' parameter and modify the 'arguments' parameter as follows, the result would be the same.<\/p>\n<pre><code>step_2 = PythonScriptStep(\n    name=&quot;Step 2&quot;,\n    script_name=&quot;step_2.py&quot;,\n    source_directory=&quot;.\/&quot;,\n    arguments = [\n        &quot;--training-data-path&quot;, step_1_config.as_input('training_data')\n        ],    \n    compute_target=compute_target,\n    runconfig=aml_run_config,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p>My question is: is there any difference when specifying the input using both the 'inputs' and 'arguments' parameters Vs. using only the 'arguments' parameter?<\/p>\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654156643450,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/874019\/passing-data-between-azureml-pipeline-steps-with-o",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":32.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Passing data between AzureML pipeline steps with OutputFileDatasetConfig: difference between 'inputs\/outputs' and 'arguments'?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":258,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@ThierryL-3166 I think the recommendation to use separate OutputFileDatasetConfig objects for different steps is to avoid concurrent writes to a single object. As stated in a note in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-data-in-out-of-pipelines#use-outputfiledatasetconfig-for-intermediate-data\">documentation<\/a>:    <\/p>\n<pre><code>Concurrent writes to a OutputFileDatasetConfig will fail. Do not attempt to use a single OutputFileDatasetConfig concurrently. Do not share a single OutputFileDatasetConfig in a multiprocessing situation, such as when using distributed training.  \n<\/code><\/pre>\n<p>If your steps do not run in parallel then you can try to use a single object and check though.    <\/p>\n<p>With respect to using inputs or arguments, If you are using the same for the same operation then arguments would pass the same as input to the script used in the same step and you would need to use an argparser to retrieve the value in the script. Whereas, inputs would provide the same value as the run objects context in the same script. The section <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-data-in-out-of-pipelines#access-datasets-within-your-script\">access datasets within script<\/a> provides an example here for a train and test dataset where train is passed with arguments and test with inputs.    <\/p>\n<pre><code>smaller_dataset = iris_dataset.take_sample(0.1, seed=seed) # 10%  \ntrain, test = smaller_dataset.random_split(percentage=0.8, seed=seed)  \n  \n# In pipeline definition script:  \n# Code for demonstration only: It would be very confusing to split datasets between `arguments` and `inputs`  \ntrain_step = PythonScriptStep(  \n    name=&quot;train_data&quot;,  \n    script_name=&quot;train.py&quot;,  \n    compute_target=cluster,  \n    arguments=['--training-folder', train.as_named_input('train').as_download()],  \n    inputs=[test.as_named_input('test').as_download()]  \n)  \n  \n# In pipeline script  \nparser = argparse.ArgumentParser()  \nparser.add_argument('--training-folder', type=str, dest='train_folder', help='training data folder mounting point')  \nargs = parser.parse_args()  \ntraining_data_folder = args.train_folder  \n  \ntesting_data_folder = Run.get_context().input_datasets['test']  \n<\/code><\/pre>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.2,
        "Solution_reading_time":29.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":233.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I don't know where else to ask this question so would appreciate any help or feedback. I've been reading the SDK documentation for azure machine learning service (in particular <code>azureml.core<\/code>). There's a class called <code>Pipeline<\/code> that has methdods <code>validate()<\/code> and <code>publish()<\/code>. Here are the docs for this:<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py<\/a><\/p>\n<p>When I call <code>validate()<\/code>, everything validates and I call publish but it seems to only create an API endpoint in the workspace, it doesn't register my pipeline under Pipelines and there's obviously nothing in the designer.<\/p>\n<p>My question: I want to publish my pipeline so I just have to launch from the workspace with one click. I've built it already using the SDK (Python code). I don't want to work with an API. Is there any way to do this or would I have to rebuild the entire pipeline using the designer (drag and drop)?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1595466856670,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1595544102140,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63045395",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":16.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Machine learning in Azure: How do I publish a pipeline to the workspace once I've already built it in Python using the SDK?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":739.0,
        "Challenge_word_count":168,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1595292020127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Totally empathize with your confusion. Our team has been working with Azure ML pipelines for quite some time but <code>PublishedPipelines<\/code> still confused me initially because:<\/p>\n<ul>\n<li>what the SDK calls a <code>PublishedPipeline<\/code> is called as a <code>Pipeline Endpoint<\/code> in the Studio UI, and<\/li>\n<li>it is semi-related to <code>Dataset<\/code> and <code>Model<\/code>'s <code>.register()<\/code> method, but fundamentally different.<\/li>\n<\/ul>\n<p><code>TL;DR<\/code>: all <code>Pipeline.publish()<\/code> does is create an endpoint that you can use to:<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb\" rel=\"nofollow noreferrer\">schedule<\/a> and <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb\" rel=\"nofollow noreferrer\">version<\/a> Pipelines, and<\/li>\n<li>re-run the pipeline from other services via a REST API call (e.g. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/transform-data-machine-learning-service\" rel=\"nofollow noreferrer\">via Azure Data Factory<\/a>).<\/li>\n<\/ul>\n<p>You can see <code>PublishedPipelines<\/code> in the Studio UI in two places:<\/p>\n<ul>\n<li>Pipelines page :: Pipeline Endpoints tab<\/li>\n<li>Endpoints page :: Pipeline Endpoints tab<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/UQ6RS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/UQ6RS.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1595468750207,
        "Solution_link_count":5.0,
        "Solution_readability":17.5,
        "Solution_reading_time":22.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug description\r\n\r\nThe pipeline `sentence_embedding\/dvc.yaml` is not correctly defined for `evaluation:deps`.\r\n\r\nThis creates the following issues:\r\n  - The evaluation stage does not know how to pull the model `biobert_nli_sts_cord19_v1\/`.\r\n  - The training stage does not know it has to run before the evaluation stage for the models `tf_idf\/` and `count\/`.\r\n\r\n## To reproduce\r\n\r\n```\r\ngit checkout 12988ef564dd4e6373a7455f5ee30c0608e2e972\r\nexport PIPELINE=data_and_models\/pipelines\/sentence_embedding\/dvc.yaml\r\ndvc pull -d $PIPELINE\r\ndvc repro -f $PIPELINE\r\n```\r\n\r\nThis will give the error:\r\n```\r\nRunning stage 'data_and_models\/pipelines\/sentence_embedding\/dvc.yaml:evaluation@biobert_nli_sts_cord19_v1':\r\n...\r\nAttributeError: Path ..\/..\/models\/sentence_embedding\/biobert_nli_sts_cord19_v1\/ not found\r\n```\r\n\r\nAfter manually pulling `biobert_nli_sts_cord19_v1`, this will give the error:\r\n```\r\nRunning stage 'data_and_models\/pipelines\/sentence_embedding\/dvc.yaml:evaluation@tf_idf':\r\n...\r\nFileNotFoundError: [Errno 2] No such file or directory: '..\/..\/models\/sentence_embedding\/tf_idf\/model.pkl'\r\n```\r\n\r\n## Expected behavior\r\n\r\n`dvc pull -d` and `dvc repro -f` should run without errors about missing files.",
        "Challenge_closed_time":1626683.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625148874000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/BlueBrain\/Search\/issues\/396",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":13.8,
        "Challenge_reading_time":16.11,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":7.0,
        "Challenge_repo_issue_count":644.0,
        "Challenge_repo_star_count":29.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Fix the definition of pipelines\/sentence_embedding\/dvc.yaml",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":118,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1359957233372,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":86.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>When running an ML training job in Amazon SageMaker, the training script is \"deployed\" and given an ML training instance, which takes about 10 minutes to spin up and get the data it needs. <\/p>\n\n<p>I can only get one error message from the training job, then it dies and the instance is killed along with it. <\/p>\n\n<p>After I make a change to the training script to fix it, I need to deploy and run it which takes another 10 minutes or so.<\/p>\n\n<p>How can I accomplish this faster, or keep the training instance running?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548272148563,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54334462",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I quickly debug a SageMaker training script?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1902.0,
        "Challenge_word_count":104,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416193017423,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gensokyo",
        "Poster_reputation_count":880.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>It seems that you are running a training job using one of the SageMaker frameworks. Given that, you can use the \"local mode\" feature of SageMaker, which will run your training job (specifically the container) locally in your notebook instance. That way, you can iterate on your script until it works. Then you can move on to the remote training cluster to train the model against the whole dataset if needed. To use local mode, you just set the instance type to \"local\". More details about local mode can be found at <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview<\/a> and the blog post: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.7,
        "Solution_reading_time":13.03,
        "Solution_score_count":5.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":102.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<br>\nI deleted a run by accident and tried to recover it. However, I noticed there is undelete all runs option available from the dot menu on the overview page of the project.<br>\nIs there a way to recover a deleted run?<br>\nThank you.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668549759237,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/undelete-runs-no-longer-available\/3420",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.9,
        "Challenge_reading_time":3.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Undelete runs no longer available?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":329.0,
        "Challenge_word_count":48,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/summer5e\">@summer5e<\/a> thank you for reporting this. The option to undelete runs should be available when you go to the Project\u2019s overview page, and not in the Runs overview level. I have attached a screenshot of an example, the option can be found when you click on the three vertical dots:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/b\/b4766106065aa247f2d9930f131e3dab402136a6.png\" data-download-href=\"\/uploads\/short-url\/pKrzn3Dflv0FWE8GGBfCE4JTMhM.png?dl=1\" title=\"Screenshot 2022-11-16 at 12.02.29\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/b\/b4766106065aa247f2d9930f131e3dab402136a6_2_690x174.png\" alt=\"Screenshot 2022-11-16 at 12.02.29\" data-base62-sha1=\"pKrzn3Dflv0FWE8GGBfCE4JTMhM\" width=\"690\" height=\"174\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/b\/b4766106065aa247f2d9930f131e3dab402136a6_2_690x174.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/b\/b4766106065aa247f2d9930f131e3dab402136a6_2_1035x261.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/b\/b4766106065aa247f2d9930f131e3dab402136a6_2_1380x348.png 2x\" data-dominant-color=\"212122\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2022-11-16 at 12.02.29<\/span><span class=\"informations\">2845\u00d7719 123 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Could you please let me know the name of your project to look further into this?<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":19.0,
        "Solution_reading_time":24.62,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":117.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1412669622830,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using the SageMaker HuggingFace Processor to create a custom tokenizer on a large volume of text data.\nIs there a way to make this job data distributed - meaning read partitions of data across nodes and train the tokenizer leveraging multiple CPUs\/GPUs.<\/p>\n<p>At the moment, providing more nodes to the processing cluster merely replicates the tokenization process (basically duplicates the process of creation), which is redundant. You can primarily only scale vertically.<\/p>\n<p>Any insights into this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662621424647,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1662627758727,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73645084",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":7.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Create Hugging Face Transformers Tokenizer using Amazon SageMaker in a distributed way",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":27.0,
        "Challenge_word_count":88,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1662621266503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":48.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Considering the following example code for\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-job-frameworks-hugging-face.html\" rel=\"nofollow noreferrer\">HuggingFaceProcessor<\/a>:<\/p>\n<p>If you have 100 large files in S3 and use a ProcessingInput with\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ProcessingS3Input.html#:%7E:text=S3DataDistributionType\" rel=\"nofollow noreferrer\">s3_data_distribution_type<\/a>=&quot;ShardedByS3Key&quot; (instead of FullyReplicated), the objects in your S3 prefix will be sharded and distributed to your instances.<\/p>\n<p>For example, if you have 100 large files and want to filter records from them using HuggingFace on 5 instances, the s3_data_distribution_type=&quot;ShardedByS3Key&quot; will put 20 objects on each instance, and each instance can read the files from its own path, filter out records, and write (uniquely named) files to the output paths, and SageMaker Processing will put the filtered files in S3.<\/p>\n<p>However, if your filtering criteria is stateful or depends on doing a full pass over the dataset first (such as: filtering outliers based on mean and standard deviation on a feature - in case of using SKLean Processor for example): you'll need to pass that information in to the job so each instance can know how to filter. To send information to the instances launched, you have to use the\u00a0<code>\/opt\/ml\/config\/resourceconfig.json<\/code>\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-your-own-processing-container.html#byoc-config\" rel=\"nofollow noreferrer\">file<\/a>:<\/p>\n<p><code>{ &quot;current_host&quot;: &quot;algo-1&quot;, &quot;hosts&quot;: [&quot;algo-1&quot;,&quot;algo-2&quot;,&quot;algo-3&quot;] }<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":17.1,
        "Solution_reading_time":22.86,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":186.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I haven't used sagemaker for a while and today I started a training job (with the same old settings I always used before), but this time I noticed that a processing job has been automatically created and it's running while my training job run (I presume for debugging purpose).\nI'm sure that this is the first time that it happens.. Is that a new feature introduced by sagemaker? I didn't find any related in documentation, but it's important to know because I don't want extra costs..<\/p>\n<p>This is the image used by the processing job, with a instance type of <code>ml.m5.2xlarge<\/code> which I didn't set anywhere..<\/p>\n<blockquote>\n<p>929884845733.dkr.ecr.eu-west-1.amazonaws.com\/sagemaker-debugger-rules:latest<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611569494177,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1611571151463,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65882686",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker processing Job automatically created?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":301.0,
        "Challenge_word_count":114,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416346350292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation_count":2302.0,
        "Poster_view_count":227.0,
        "Solution_body":"<p>I can answer my question.. it seems to be a new feature as highlighted <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/use-debugger-built-in-rules.html\" rel=\"nofollow noreferrer\">here<\/a>. You can turn it off as suggested in the doc:<\/p>\n<pre><code>To disable both monitoring and profiling, include the disable_profiler parameter to your estimator and set it to True. \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.6,
        "Solution_reading_time":5.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":47.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I run several experiments, both singularly and using sweeps. Now I want to look at the results from the UI and I want to filter out runs that came from a sweep.<\/p>\n<p>I tried to filter out by name, but I can\u2019t get it to work. I manage to match all the sweep runs with the regex <code>.*sweep*<\/code>, but I don\u2019t know how to get the opposite. Any suggestion would be much appreciated, thanks! <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1674201525952,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-filter-out-sweep-runs-from-ui\/3725",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":6.4,
        "Challenge_reading_time":7.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"How to filter out sweep runs from UI",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":178.0,
        "Challenge_word_count":89,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/lclissa\">@lclissa<\/a> thanks for your question! You can go to the runs Table view and then perform the following actions: <code>Filter<\/code> &gt; <code>Add Filter<\/code> &gt; <code>Sweeps in \"&lt;null&gt;\"<\/code> as in the attached screenshot below.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/e\/ef99184af5047a63dcc478a1480f31d9bcebc6ce.png\" data-download-href=\"\/uploads\/short-url\/ybA7qg4vIknsHb3jlIldWMRDplY.png?dl=1\" title=\"Screenshot 2023-01-20 at 11.25.10\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/e\/ef99184af5047a63dcc478a1480f31d9bcebc6ce_2_690x244.png\" alt=\"Screenshot 2023-01-20 at 11.25.10\" data-base62-sha1=\"ybA7qg4vIknsHb3jlIldWMRDplY\" width=\"690\" height=\"244\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/e\/ef99184af5047a63dcc478a1480f31d9bcebc6ce_2_690x244.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/e\/ef99184af5047a63dcc478a1480f31d9bcebc6ce_2_1035x366.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/e\/ef99184af5047a63dcc478a1480f31d9bcebc6ce_2_1380x488.png 2x\" data-dominant-color=\"262728\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2023-01-20 at 11.25.10<\/span><span class=\"informations\">1836\u00d7651 72.5 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>In this way you will end up with the runs that weren\u2019t part of the Sweeps. Would this work for you?<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":21.2,
        "Solution_reading_time":24.52,
        "Solution_score_count":null,
        "Solution_sentence_count":11.0,
        "Solution_word_count":105.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to figure out how viable Azure ML in production; I would like to accomplish the following:<\/p>\n\n<ol>\n<li>Specify <em>custom environments<\/em> for my pipelines using a <em>pip file<\/em> and use them in a pipeline<\/li>\n<li><em>Declaratively<\/em> specify my workspace, environments and pipelines in an <em>Azure DevOps repo<\/em><\/li>\n<li><em>Reproducibly<\/em> deploy my Azure ML workspace to my subscription using an <em>Azure DevOps pipeline<\/em><\/li>\n<\/ol>\n\n<p>I found an <a href=\"https:\/\/stackoverflow.com\/questions\/60506398\/how-do-i-use-an-environment-in-an-ml-azure-pipeline\">explanation of how to specify environments using notebooks<\/a> but this seems ill-suited for the second and third requirements I have.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583316022363,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1583348292107,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60523435",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.4,
        "Challenge_reading_time":10.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I version control Azure ML workspaces with custom environments and pipelines?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":659.0,
        "Challenge_word_count":98,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1317398821727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1263.0,
        "Poster_view_count":44.0,
        "Solution_body":"<p>Currently, we have a python script, <code>pipeline.py<\/code> that uses the <code>azureml-sdk<\/code>to create, register and run all of our ML artifacts (envs, pipelines, models). We call this script in our Azure DevOps CI pipeline with a Python Script task after building the right pip env from the requirements file in our repo.<\/p>\n\n<p>However, it is worth noting there is YAML support for ML artifact definition. Though I don't know if the existing support will cover all of your bases (though that is the plan).<\/p>\n\n<p>Here's some great docs from MSFT to get you started:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/microsoft\/MLOpsPython\" rel=\"nofollow noreferrer\">GitHub Template repo of an end-to-end example of ML pipeline + deployment<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">How to define\/create an environment (using Pip or Conda) and use it in a remote compute context<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/targets\/azure-machine-learning?context=azure%2Fmachine-learning%2Fservice%2Fcontext%2Fml-context&amp;view=azure-devops&amp;tabs=yaml\" rel=\"nofollow noreferrer\">Azure Pipelines guidance on CI\/CD for ML Service<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-pipeline-yaml\" rel=\"nofollow noreferrer\">Defining ML pipelines in YAML<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1583340824232,
        "Solution_link_count":4.0,
        "Solution_readability":13.8,
        "Solution_reading_time":18.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":148.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":4389.5575297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Sagemaker pipelines are rather unclear to me, I'm not experienced in the field of ML but I'm working on figuring out the pipeline definitions.<\/p>\n<p>I have a few questions:<\/p>\n<ul>\n<li><p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/li>\n<li><p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/li>\n<li><p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/li>\n<\/ul>\n<p>I can't seem to find any examples besides the Python SDK usage, how come?<\/p>\n<p>The docs and workshops seem only to properly describe the Python SDK usage,it would be really helpful if someone could clear this up for me!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638395443060,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70191668",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":10.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"What are SageMaker pipelines actually?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":716.0,
        "Challenge_word_count":131,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578250359256,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam",
        "Poster_reputation_count":197.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>SageMaker has two things called Pipelines: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pipelines.html\" rel=\"nofollow noreferrer\">Model Building Pipelines<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>. I believe you're referring to the former<\/p>\n<p>A model building pipeline defines steps in a machine learning workflow, such as pre-processing, hyperparameter tuning, batch transformations, and setting up endpoints<\/p>\n<p>A serial inference pipeline is two or more SageMaker models run one after the other<\/p>\n<p>A model building pipeline is defined in JSON, and is hosted\/run in some sort of proprietary, serverless fashion by SageMaker<\/p>\n<blockquote>\n<p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/blockquote>\n<p>You can create\/modify them using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePipeline.html\" rel=\"nofollow noreferrer\">API<\/a>, which can also be called via the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-pipeline.html\" rel=\"nofollow noreferrer\">CLI<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline.Pipeline.create\" rel=\"nofollow noreferrer\">Python SDK<\/a>, or <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html\" rel=\"nofollow noreferrer\">CloudFormation<\/a>. These all use the AWS API under the hood<\/p>\n<p>You can start\/stop\/view them in SageMaker Studio:<\/p>\n<pre><code>Left-side Navigation bar &gt; SageMaker resources &gt; Drop-down menu &gt; Pipelines\n<\/code><\/pre>\n<blockquote>\n<p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/blockquote>\n<p>Unlikely. CodePipeline is more for building and deploying code, not specific to SageMaker. There is no direct integration as far as I can tell, other than that you can start a SM pipeline with CP<\/p>\n<blockquote>\n<p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/blockquote>\n<p>The Python SDK is a stand-alone library to interact with SageMaker in a developer-friendly fashion. It's more dynamic than CloudFormation. Let's you build pipelines using code. Whereas CloudFormation takes a static JSON string<\/p>\n<p>A very simple example of Python SageMaker SDK usage:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>processor = SKLearnProcessor(\n    framework_version=&quot;0.23-1&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    role=&quot;role-arn&quot;,\n)\n\nprocessing_step = ProcessingStep(\n    name=&quot;processing&quot;,\n    processor=processor,\n    code=&quot;preprocessor.py&quot;\n)\n\npipeline = Pipeline(name=&quot;foo&quot;, steps=[processing_step])\npipeline.upsert(role_arn = ...)\npipeline.start()\n<\/code><\/pre>\n<p><code>pipeline.definition()<\/code> produces rather verbose JSON like this:<\/p>\n\n<pre class=\"lang-json prettyprint-override\"><code>{\n&quot;Version&quot;: &quot;2020-12-01&quot;,\n&quot;Metadata&quot;: {},\n&quot;Parameters&quot;: [],\n&quot;PipelineExperimentConfig&quot;: {\n    &quot;ExperimentName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineName&quot;\n    },\n    &quot;TrialName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineExecutionId&quot;\n    }\n},\n&quot;Steps&quot;: [\n    {\n        &quot;Name&quot;: &quot;processing&quot;,\n        &quot;Type&quot;: &quot;Processing&quot;,\n        &quot;Arguments&quot;: {\n            &quot;ProcessingResources&quot;: {\n                &quot;ClusterConfig&quot;: {\n                    &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n                    &quot;InstanceCount&quot;: 1,\n                    &quot;VolumeSizeInGB&quot;: 30\n                }\n            },\n            &quot;AppSpecification&quot;: {\n                &quot;ImageUri&quot;: &quot;246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3&quot;,\n                &quot;ContainerEntrypoint&quot;: [\n                    &quot;python3&quot;,\n                    &quot;\/opt\/ml\/processing\/input\/code\/preprocessor.py&quot;\n                ]\n            },\n            &quot;RoleArn&quot;: &quot;arn:aws:iam::123456789012:role\/foo&quot;,\n            &quot;ProcessingInputs&quot;: [\n                {\n                    &quot;InputName&quot;: &quot;code&quot;,\n                    &quot;AppManaged&quot;: false,\n                    &quot;S3Input&quot;: {\n                        &quot;S3Uri&quot;: &quot;s3:\/\/bucket\/preprocessor.py&quot;,\n                        &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/input\/code&quot;,\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3InputMode&quot;: &quot;File&quot;,\n                        &quot;S3DataDistributionType&quot;: &quot;FullyReplicated&quot;,\n                        &quot;S3CompressionType&quot;: &quot;None&quot;\n                    }\n                }\n            ]\n        }\n    }\n  ]\n}\n<\/code><\/pre>\n<p>You could <em>use<\/em> the above JSON with CloudFormation\/CDK, but you <em>build<\/em> the JSON with the SageMaker SDK<\/p>\n<p>You can also define model building workflows using Step Function State Machines, using the <a href=\"https:\/\/aws-step-functions-data-science-sdk.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Data Science SDK<\/a>, or <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/airflow\/index.html\" rel=\"nofollow noreferrer\">Airflow<\/a><\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1654197850167,
        "Solution_link_count":8.0,
        "Solution_readability":18.8,
        "Solution_reading_time":68.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":32.0,
        "Solution_word_count":402.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1611181716003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":119.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a published Azure ML Pipeline that I am trying to trigger from an Automate Flow I have that triggers when users edit a document. Since I have the REST Endpoint for the Published Pipeline, I figured I should be able to make a POST request using the HTTP module available in Power Automate to trigger the pipeline.<\/p>\n<p>However, when I actually try this, I get an authentication error. I assume this is because I need to include some access token with the REST Endpoint, but I can't find any documentation that will tell me where to get that token from. Please note that I do not need to pass any data to the Pipeline, it handles its own data collection, I literally just need a way to trigger it.<\/p>\n<p>Does anybody know how to trigger a Published Azure ML Pipeline using the REST Endpoint? Does it make sense to use the HTTP module, or is there a better way to achieve this?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624934847563,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1624937966803,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68172002",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":11.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"How to trigger Azure ML Pipeline from Power Automate",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":305.0,
        "Challenge_word_count":172,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1611181716003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":119.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>So I figured out how to do it by following the directions contained within this piece of Microsoft Documentation:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-rest\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-rest<\/a><\/p>\n<p>Specifically, it required performing two of the calls in the documentation;<\/p>\n<ul>\n<li>The first to get an AAD token using an Azure Service Principle that is authorised to access the Machine Learning Instance.<\/li>\n<\/ul>\n<blockquote>\n<p>curl -X POST <a href=\"https:\/\/login.microsoftonline.com\/\" rel=\"nofollow noreferrer\">https:\/\/login.microsoftonline.com\/<\/a>\/oauth2\/token -d &quot;grant_type=client_credentials&amp;resource=https%3A%2F%2Fmanagement.azure.com%2F&amp;client_id=&amp;client_secret=&quot;<\/p>\n<\/blockquote>\n<ul>\n<li>The second to use this token to trigger your pipeline from its rest endpoint. This one I had to figure out myself a little, but below is the basic structure I used.<\/li>\n<\/ul>\n<blockquote>\n<p>curl -X POST {PIPELINE_REST_ENDPOINT} -H &quot;Authorisation:Bearer {AAD_TOKEN}&quot; -H &quot;Content-Type: application\/json&quot; -d &quot;{&quot;ExperimentName&quot;: &quot;{EXPERIMENT_NAME}&quot;,&quot;ParameterAssignments&quot;: {}}&quot;<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":17.8,
        "Solution_reading_time":17.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":118.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I got this error when running a job in Azure Machine Learning Studio. Any ideas about how to fix it?<\/p>\n<pre><code>Error Code: ScriptExecution.StreamAccess.Unexpected\nNative Error: error in streaming from input data sources\n\tStreamError(Unknown(&quot;unsuccessful status code 409 Conflict, body &quot;, None))\n=&gt; unsuccessful status code 409 Conflict, body \n\tUnknown(&quot;unsuccessful status code 409 Conflict, body &quot;, None)\nError Message: Got unexpected error: unsuccessful status code 409 Conflict, body . | session_id=96032e2f-c1e6-423c-8225-c1c460b3192f\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676589961420,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1181563\/error-code-scriptexecution-streamaccess-unexpected",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":8.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Error Code: ScriptExecution.StreamAccess.Unexpected when running a job in AzureML Studio",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":77,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=2506b2a8-0b14-4610-bdd6-41ac619af16a\">@Maria Rivera Araya  <\/a>The error message &quot;unsuccessful status code 409 Conflict, body&quot; suggests that there is a conflict with the input data sources. This error can occur when the input data sources are being modified while the job is running.&lt;sup&gt;<a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/component-reference\/designer-error-codes.md\">[2]<\/a>&lt;\/sup&gt;<\/p>\n<p>You can try the following steps to resolve the issue: Wait for the input data sources to finish being modified.<\/p>\n<ol>\n<li> If the input data sources are not being modified, try restarting the job.<\/li>\n<li> If the issue persists, try using a different input data source.<\/li>\n<\/ol>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":10.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1619204963587,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1442.0,
        "Answerer_view_count":879.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have successfully run the following pipeline that creates a dataset, trains a model, and deploys to an endpoint using VertexAIs pipeline tool when everything is based in us-central1. Now, when I change the region to europe-west2, I get the following error:<\/p>\n<pre><code>debug_error_string = &quot;{&quot;created&quot;:&quot;@1647430410.324290053&quot;,&quot;description&quot;:&quot;Error received from peer\nipv4:172.217.169.74:443&quot;,&quot;file&quot;:&quot;src\/core\/lib\/surface\/call.cc&quot;,&quot;file_line&quot;:1066,\n&quot;grpc_message&quot;:&quot;List of found errors:\\t1.Field: name; Message: \nThe provided location ID doesn't match the endpoint. The valid location ID is `us-central1`.\\t&quot;,&quot;grpc_status&quot;:3}&quot;\n<\/code><\/pre>\n<p>This error occurs after the dataset is created in europe-west2, and before the model starts to train. Here is my code:<\/p>\n<pre><code>#import libraries\nfrom typing import NamedTuple\nimport kfp\nfrom kfp import dsl\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import (Artifact, Input, InputPath, Model, Output, \n                        OutputPath, ClassificationMetrics, Metrics, component)\nfrom kfp.v2.components.types.artifact_types import Dataset\nfrom kfp.v2.google.client import AIPlatformClient\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom google.api_core.exceptions import NotFound\n\n@kfp.dsl.pipeline(name=f&quot;lookalike-model-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = f&quot;bq:\/\/{PROJECT_ID}.{DATASET_ID}.{TABLE_NAME}&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;europe-west2&quot;,\n    api_endpoint: str = &quot;europe-west2-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auPrc&quot;: 0.5}',\n):\n            \n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project,\n        display_name=display_name, \n        bq_source=bq_source,\n        location = gcp_region\n    )\n\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        location=gcp_region,\n        predefined_split_column_name=&quot;set&quot;,\n        column_transformations=[\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;agentId&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;postcode&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;isMobile&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;gender&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;timeOfDay&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;set&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;sale&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;sale&quot;,\n    )\n\ncompiler.Compiler().compile(\n    pipeline_func=pipeline, package_path=&quot;tab_classif_pipeline.json&quot;\n)\n\nml_pipeline_job = aiplatform.PipelineJob(\n    display_name=f&quot;{MODEL_PREFIX}_training&quot;,\n    template_path=&quot;tab_classif_pipeline.json&quot;,\n    pipeline_root=PIPELINE_ROOT,\n    parameter_values={&quot;project&quot;: PROJECT_ID, &quot;display_name&quot;: DISPLAY_NAME},\n    enable_caching=True,\n    location=&quot;europe-west2&quot;\n)\nml_pipeline_job.submit()\n<\/code><\/pre>\n<p>As previously mentioned, the dataset gets created so I suspect that the issue must lie in <code>training_op = gcc_aip.AutoMLTabularTrainingJobRunOp<\/code><\/p>\n<p>I tried providing another endpoint: <code>eu-aiplatform.googleapis.com<\/code> which yielded the following error:<\/p>\n<pre><code>google.api_core.exceptions.InvalidArgument: 400 List of found errors: 1.Field: name; Message: \nThe provided location ID doesn't match the endpoint. The valid location ID is `us-central1`.\n\nFail to send metric: [rpc error: code = PermissionDenied desc = Permission monitoring.metricDescriptors.create \ndenied (or the resource may not exist).; rpc error: code = PermissionDenied desc = Permission monitoring.timeSeries.create\n denied (or the resource may not exist).]\n<\/code><\/pre>\n<p>I understand that I am not passing api-endpoint to any of the methods above, but I thought I'd highlight that the error changed slightly.<\/p>\n<p>Does anyone know what the issue may be? Or how I can run <code>gcc_aip.AutoMLTabularTrainingJobRunOp<\/code> in europe-west2 (or EU in general)?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1647433181040,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71496966",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":17.4,
        "Challenge_reading_time":59.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":null,
        "Challenge_title":"VertexAI Pipelines: The provided location ID doesn't match the endpoint",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":220.0,
        "Challenge_word_count":357,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562706291280,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>Try Updating the pipeline component using the command:<\/p>\n<p><code>pip3 install --force-reinstall google_cloud_pipeline_components==0.1.3<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.2,
        "Solution_reading_time":2.07,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":12.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I have been working with ML Studio (classic) and facing a problem with &quot;Execute Python&quot; scripts. I have noticed that it takes time to perform some internal tasks after which it starts executing the actual Python code in ML Studio. This delay has caused an increased time of 40-60 seconds per module which is aggregating and causing a delay of 400-500 seconds per execution when consumed through Batch Execution System or on running the experiments manually. (I've multiple Modules of &quot;Execute Python&quot; scripts)  <\/p>\n<p>Can you please help understand the reason behind this or any optimization that can be done?  <\/p>\n<p>Regards,  <br \/>\nAnant<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1593687947923,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/42134\/why-does-azure-machine-learning-studio-(classic)-t",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.0,
        "Challenge_reading_time":9.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Why does Azure Machine Learning Studio (classic) take additonal time to execute python scripts?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":119,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I completely understand, sorry for the inconvenience, however, this is a known limitation as optimizing performance in Azure ML Studio (Classic) isn't supported. We recommend that customers use Designer for advanced capabilities and active updates\/improvements for the service. Thanks.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.6,
        "Solution_reading_time":3.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1388584380832,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":396.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I could not find a way yet of setting the runs name after the first start_run for that run (we can pass a name there). <\/p>\n\n<p>I Know we can use tags but that is not the same thing. I would like to add a run relevant name, but very often we know the name only after run evaluation or while we're running the run interactively in notebook for example.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1564049391490,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1564533527096,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57199472",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.7,
        "Challenge_reading_time":5.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":11.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Is it possible to set\/change mlflow run name after run initial creation?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":7689.0,
        "Challenge_word_count":81,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1393062808772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Portugal",
        "Poster_reputation_count":133.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>It is possible to edit run names from the MLflow UI. First, click into the run whose name you'd like to edit.<\/p>\n\n<p>Then, edit the run name by clicking the dropdown next the run name (i.e. the downward-pointing caret in this image):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sl6Qs.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sl6Qs.png\" alt=\"Rename run dropdown\"><\/a><\/p>\n\n<p>There's currently no stable public API for setting run names - however, you can programmatically set\/edit run names by setting the tag with key <code>mlflow.runName<\/code>, which is what the UI (currently) does under the hood.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.9,
        "Solution_reading_time":7.92,
        "Solution_score_count":9.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":83.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":1807.7083702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to add an alert if Azure ML pipeline fails. It looks that one of the ways is to create a monitor in the Azure Portal. The problem is that I cannot find a correct signal name (required when setting up condition), which would identify pipeline fail. What signal name should I use? Or is there another way to send an email if Azure pipeline fails?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651478303433,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1651956277630,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72083832",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.5,
        "Challenge_reading_time":4.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Send alert if Azure ML pipeline fails",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":158.0,
        "Challenge_word_count":74,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1313536247312,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Vilnius, Lithuania",
        "Poster_reputation_count":563.0,
        "Poster_view_count":108.0,
        "Solution_body":"<blockquote>\n<p>What signal name should I use?<\/p>\n<\/blockquote>\n<p>You can use <code>PipelineChangeEvent<\/code> category of <code>AmlPipelineEvent<\/code> table to view events when ML pipeline draft or endpoint or module are accessed (read, created, or deleted).<\/p>\n<p>For example, according to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/monitor-azure-machine-learning#analyzing-logs\" rel=\"nofollow noreferrer\">documentation<\/a>, use <code>AmlComputeJobEvent<\/code> to get failed jobs in the last five days:<\/p>\n<pre><code>AmlComputeJobEvent\n| where TimeGenerated &gt; ago(5d) and EventType == &quot;JobFailed&quot;\n| project  TimeGenerated , ClusterId , EventType , ExecutionState , ToolType\n<\/code><\/pre>\n<p><strong>Updated answer:<\/strong><\/p>\n<p>According to <a href=\"https:\/\/stackoverflow.com\/users\/897665\/laurynas-g\">Laurynas G<\/a>:<\/p>\n<pre><code>AmlRunStatusChangedEvent \n| where Status == &quot;Failed&quot; or Status == &quot;Canceled&quot;\n<\/code><\/pre>\n<p>You can refer to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/monitor-azure-machine-learning#analyzing-logs\" rel=\"nofollow noreferrer\">Monitor Azure Machine Learning<\/a>, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Log &amp; view metrics and log files<\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-pipelines\" rel=\"nofollow noreferrer\">Troubleshooting machine learning pipelines<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1658464027763,
        "Solution_link_count":5.0,
        "Solution_readability":19.3,
        "Solution_reading_time":20.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":111.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hello,<\/p>\n<p>We use Pytorch Lightning for training and we use Kubeflow Pipelines and are thinking about using wandb to track and visualize the training and test metrics.<\/p>\n<p>Kubeflow pipelines offers the possibility to view a static html page (see <a href=\"https:\/\/www.kubeflow.org\/docs\/components\/pipelines\/sdk\/output-viewer\/#single-html-file\" rel=\"noopener nofollow ugc\">this link<\/a> ).<br>\nI was wondering if it would be possible via the wandb python sdk to get a read-only embeded code (iframe) that I could then simply pass to Kubeflow pipeline sdk to show the html ?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643806405410,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-visualize-html-run-in-kubeflow-pipeline\/1862",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":11.4,
        "Challenge_reading_time":8.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to visualize HTML run in Kubeflow Pipeline?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":890.0,
        "Challenge_word_count":89,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Ok I found the solution.<br>\nKubeflow Pipelines also support markdown visualization therefore instead of using kubeflow HTML output I used markdown and since markdown supports html inline I was able to directly use the wandb run html.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/21d1b9f84b7948659b75b981b04f21235e528615.png\" data-download-href=\"\/uploads\/short-url\/4Pb5MStVV77kGP5bCR1nBTvaK7H.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/21d1b9f84b7948659b75b981b04f21235e528615_2_690x333.png\" alt=\"image\" data-base62-sha1=\"4Pb5MStVV77kGP5bCR1nBTvaK7H\" width=\"690\" height=\"333\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/21d1b9f84b7948659b75b981b04f21235e528615_2_690x333.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/21d1b9f84b7948659b75b981b04f21235e528615_2_1035x499.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/21d1b9f84b7948659b75b981b04f21235e528615_2_1380x666.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/21d1b9f84b7948659b75b981b04f21235e528615_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1678\u00d7812 57.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Here is the code if someone is interested :<\/p>\n<pre><code class=\"lang-python\">import kfp\nfrom kfp.v2.dsl import component, Output, Markdown, pipeline\n\n@component(packages_to_install=['wandb'])\ndef wandb_visualization(markdown_artifact: Output[Markdown]):\n    import wandb\n    wandb.login(key=\"you_key\")\n\n    run = wandb.init(project=\"your-project\", entity=\"your-entity\")\n\n    wandb.log({\"train\/loss\" : 5.0})\n    wandb.log({\"train\/loss\" : 4.0})\n    wandb.log({\"train\/loss\" : 3.0})\n    wandb.log({\"train\/loss\" : 2.0})\n    wandb.log({\"train\/loss\" : 1.0})\n\n    wandb.finish()\n    with open(markdown_artifact.path, 'w') as f:\n        f.write(f\"&lt;iframe src=\\\"{run.get_url()}\\\" width=\\\"100%\\\" height=\\\"700\\\"\/&gt;\")\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":23.3,
        "Solution_reading_time":32.05,
        "Solution_score_count":null,
        "Solution_sentence_count":14.0,
        "Solution_word_count":126.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\nSageMaker supports training data streaming via [PIPE mode][1], and also reading from [FSx][2] distributed file system.\nThose options seem to provide same value: low latency, high throughput.\n\n - What are the reasons for using one or the other?\n - Do we have any benchmark of PIPE vs FSx for SageMaker, in terms of costs and speed?\n\n\n  [1]: https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\n  [2]: https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/08\/amazon-sagemaker-works-with-amazon-fsx-lustre-amazon-efs-model-training\/",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579692074000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668074106092,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUyS6bjxG4R4qtnrXzA3uSeg\/sagemaker-pipe-mode-vs-fsx",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":7.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker PIPE Mode vs FSx ?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":261.0,
        "Challenge_word_count":64,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I can think of the following scenarios \n\nPipemode cons\n\n** UPDATED**\n\n1.  Data Shuffling -  In pipe mode you are working with streaming data and hence you cannot perform data shuffle operations unless you are prepared to shuffle within batches (as in wait to read a batch of records and shuffle within the batch in Pipe mode). Of if your data is distributed across multiples files, then you could use [Sagemaker data shuffle][1] to perform file level shuffle\n\n2.  Data readers -   There are default data readers for pipemode that come with Tensorflow for formats like csv, tfrecord etc. But if you have custom data formats or using a  different deep leaning  framework, yYou would have to use custom data readers to deal with the raw bytes and understand the logical end of record. You could also use [ml-io][2] to see if any of the built-in pipe mode readers work for your usecase\n\n3. PIPE mode streams the data for each epoch from S3 and hence will be slower than FSX when you run a few epochs\n\n\nFSX:\n\n1. FSX works by lazy loading the  s3 file and hence it has a start up delay but gets faster during repeated training. \n\n2. There is no  dependency on the framework and your existing code will work as is..\n\n3. The only con of using FSX is the additional storage costs, but I would almost prefer FSX to pipe mode in most cases.\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ShuffleConfig.html\n  [2]: https:\/\/github.com\/awslabs\/ml-io#Python",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925565015,
        "Solution_link_count":2.0,
        "Solution_readability":9.5,
        "Solution_reading_time":17.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":242.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have defined an Azure Machine Learning Pipeline with three steps:<\/p>\n<pre><code>e2e_steps=[etl_model_step, train_model_step, evaluate_model_step]\ne2e_pipeline = Pipeline(workspace=ws, steps = e2e_steps)\n<\/code><\/pre>\n<p>The idea is to run the Pipeline in the given sequence:<\/p>\n<ol>\n<li>etl_model_step<\/li>\n<li>train_model_step<\/li>\n<li>evaluate_model_step<\/li>\n<\/ol>\n<p>However, my experiment is failing because it is trying to execute evaluate_model_step before train_model_step:\n<a href=\"https:\/\/i.stack.imgur.com\/0fO0t.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0fO0t.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>How do I enforce the sequence of execution?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624536441163,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68115476",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.6,
        "Challenge_reading_time":10.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to organize one step after another in Azure Machine Learning Pipelines?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":515.0,
        "Challenge_word_count":78,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p><code>azureml.pipeline.core.StepSequence<\/code> lets you do exactly that.<\/p>\n<blockquote>\n<p>A StepSequence can be used to easily run steps in a specific order, without needing to specify data dependencies through the use of PipelineData.<\/p>\n<\/blockquote>\n<p>See <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.stepsequence?view=azure-ml-py\" rel=\"nofollow noreferrer\">the docs<\/a> to read more.<\/p>\n<p>However, the preferable way to have steps run in order is stitching them together via <code>PipelineData<\/code> or <code>OutputFileDatasetConfig<\/code>. In your example, does the <code>train_step<\/code> depend on outputs from the <code>etl step<\/code>? If so, consider having that be the way that steps are run in sequence. For more info see <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-data-in-out-of-pipelines\" rel=\"nofollow noreferrer\">this tutorial<\/a> for more info<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":12.71,
        "Solution_score_count":3.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":98.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":100.7332322222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to implement the best estimator from a hyperparameter tuning job into a pipeline object to deploy an endpoint.<\/p>\n\n<p>I've read the docs in a best effort to include the results from the tuning job in the pipeline, but I'm having trouble creating the Model() class object.<\/p>\n\n<pre><code># This is the hyperparameter tuning job\ntuner.fit({'train': s3_train, 'validation': s3_val}, \ninclude_cls_metadata=False)\n\n\n#With a standard Model (Not from the tuner) the process was as follows:\nscikit_learn_inferencee_model_name = sklearn_preprocessor.create_model()\nxgb_model_name = Model(model_data=xgb_model.model_data, image=xgb_image)\n\n\nmodel_name = 'xgb-inference-pipeline-' + timestamp_prefix\nendpoint_name = 'xgb-inference-pipeline-ep-' + timestamp_prefix\nsm_model = PipelineModel(\n    name=model_name, \n    role=role, \n    models=[\n        scikit_learn_inferencee_model_name, \n        xgb_model_name])\n\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', \nendpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I would like to be able to cleanly instantiate a model object using my results from the tuning job and pass it into the PipelineModel object. Any guidance is appreciated.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1558813444520,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56308169",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":16.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Creating a model for use in a pipeline from a hyperparameter tuning job",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":455.0,
        "Challenge_word_count":137,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558812981692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>I think you are on the right track. Do you get any error? Refer this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/e9c295c8538d29cc9fea2f73a29649126628064a\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a>  for instantiating the model from the tuner and use in inference pipeline.<\/p>\n\n<p>Editing previous response based on the comment. To create model from the best training job of the hyperparameter tuning job, you can use below snippet<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tuner import HyperparameterTuner\nfrom sagemaker.estimator import Estimator\nfrom sagemaker.model import Model\n\n# Attach to an existing hyperparameter tuning job.\nxgb_tuning_job_name = 'my_xgb_hpo_tuning_job_name'\nxgb_tuner = HyperparameterTuner.attach(xgb_tuning_job_name)\n\n# Get the best XGBoost training job name from the HPO job\nxgb_best_training_job = xgb_tuner.best_training_job()\nprint(xgb_best_training_job)\n\n# Attach estimator to the best training job name\nxgb_best_estimator = Estimator.attach(xgb_best_training_job)\n\n# Create model to be passed to the inference pipeline\nxgb_model = Model(model_data=xgb_best_estimator.model_data,\n                  role=sagemaker.get_execution_role(),\n                  image=xgb_best_estimator.image_name)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1559176084156,
        "Solution_link_count":1.0,
        "Solution_readability":18.7,
        "Solution_reading_time":18.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":119.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\r\n\r\nI have local minikube cluster. I installed the helm chart with some changed settings. See below for the changed values. Everthing else is same as per default values yaml file. For db backend I am using `bitnami\/postgresql` and for s3 storage minio instance. I also have created a initial bucket named \"mlflow\" in minio. \r\n\r\nAnd then I created a simple k8s pod to run the simple training example from mlflow docs. This pod has env variables set as : `MLFLOW_TRACKING_URI=http:\/\/mlflow.airflow.svc.cluster.local:5000` [Here ](https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/master\/examples\/sklearn_elasticnet_wine\/train.py) is the link to that code. I can see the metadata about the model in UI however , artifact section in UI is empty and also the bucket is empty. \r\n\r\n### What's your helm version?\r\n\r\nversion.BuildInfo{Version:\"v3.9.0\", GitCommit:\"7ceeda6c585217a19a1131663d8cd1f7d641b2a7\", GitTreeState:\"clean\", GoVersion:\"go1.17.5\"}\r\n\r\n### What's your kubectl version?\r\n\r\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.3\", GitCommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T21:19:12Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n\r\n### Which chart?\r\n\r\nmlflow\r\n\r\n### What's the chart version?\r\n\r\nlatest\r\n\r\n### What happened?\r\n\r\n_No response_\r\n\r\n### What you expected to happen?\r\n\r\nI would expect the artifacts in minio bucket.\r\n\r\n### How to reproduce it?\r\n\r\ninstall the helm chart with minio and postgresql config. Run a simple exmple frpom docs. \r\n\r\n### Enter the changed values of values.yaml?\r\n\r\n```\r\nbackendStore:\r\n    databaseMigration: true\r\n    databaseConnectionCheck: true\r\n    postgres:\r\n      enabled: true\r\n      host: mlflow-postgres-postgresql.airflow.svc.cluster.local\r\n      database: mlflow_db\r\n      user: mlflow\r\n      password: mlflow\r\nartifactRoot:\r\n  proxiedArtifactStorage: true\r\n  s3:\r\n    enabled: true\r\n    bucket: mlflow\r\n    awsAccessKeyId: {{ requiredEnv \"MINIO_USERNAME\" }}\r\n    awsSecretAccessKey: {{ requiredEnv \"MINIO_PASSWORD\" }}\r\nextraEnvVars:\r\n  MLFLOW_S3_ENDPOINT_URL: minio.airflow.svc.cluster.local\r\n```\r\n\r\n### Enter the command that you execute and failing\/misfunctioning.\r\n\r\nhelm install mlflow-release community-charts\/mlflow --values values.yaml\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_",
        "Challenge_closed_time":1660345.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660152345000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/32",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":8.7,
        "Challenge_reading_time":30.0,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"[mlflow] model artifacts not saved in remote s3 artifact store",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":261,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @mohittalele ,\r\n\r\nThank you very much for reporting the error. Could you please share your mlflow pod and training pod logs with me?\r\n\r\nBest,\r\nBurak Hi @mohittalele \r\n\r\nI think you have a misconfiguration. I added a [full example to here](https:\/\/github.com\/community-charts\/examples\/tree\/main\/mlflow-examples\/bitnami-postgresql-and-bitnami-minio-sklearn-training-example). Simply, your `MLFLOW_S3_ENDPOINT_URL` configuration is wrong. URL must be `http:\/\/minio.airflow.svc.cluster.local:9000`. Could you please fix your configuration and try again?\r\n\r\nBest,\r\nBurak @burakince  Here is the log of the training container. Somehow the trining container does not \"know\" of the s3 endpoint and it is using the local path. \r\n\r\n```\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - 2022\/08\/11 11:53:26 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - The git executable must be specified in one of the following ways:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - be included in your $PATH\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - be set via $GIT_PYTHON_GIT_EXECUTABLE\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - explicitly set via git.refresh()\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - All git commands will error until this is rectified.\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - This initial warning can be silenced or aggravated in the future by setting the\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - quiet|q|silence|s|none|n|0: for no warning or exception\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - warn|w|warning|1: for a printed warning\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - error|e|raise|r|2: for a raised exception\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - Example:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     export GIT_PYTHON_REFRESH=quiet\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - Elasticnet model (alpha=0.500000, l1_ratio=0.500000):\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   RMSE: 0.793164022927685\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   MAE: 0.6271946374319586\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   R2: 0.10862644997792636\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_artifact_uri ::  .\/mlruns\/0\/3b376331bcaa4894a0723fe4b690658f\/artifacts\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_registry_uri ::  http:\/\/mlflow.airflow.svc.cluster.local:5000\/\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_tracking_uri ::  http:\/\/mlflow.airflow.svc.cluster.local:5000\/\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - Registered model 'ElasticnetWineModel' already exists. Creating a new version of this model...\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - 2022\/08\/11 11:53:29 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: ElasticnetWineModel, version 11\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - Created version '11' of model 'ElasticnetWineModel'.\r\n[2022-08-11, 13:53:30 CEST] {kubernetes_pod.py:453} INFO - Deleting pod: mlflow-example-1-7e89c5e6540645e3822fbf34410c6b99\r\n[2022-08-11, 13:53:30 CEST] {taskinstance.py:1420} INFO - Marking task as SUCCESS. dag_id=mlflow, task_id=mlflow_example_1, execution_date=20220811T115225, start_date=20220811T115226, end_date=20220811T115330\r\n[2022-08-11, 13:53:30 CEST] {local_task_job.py:156} INFO - Task exited with return code 0\r\n[2022-08-11, 13:53:30 CEST] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check\r\n```\r\n\r\n\r\n\r\nmlflow logs are quite, There is nothing logged there. I will try out the example. Thanks for the example. \r\n\r\nedit : with 9000 port number specifie, there is no improvement @burakince The setup now works. Actually there was problem with VPN setting since I was deploying mlflow behind mlflow. We can close the issue :) Hi @mohittalele,\r\n\r\nI'm glad to hear the problem was resolved. If you need anything else, please don't hesitate to open a new issue.\r\n\r\nBest,\r\nBurak",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":7.5,
        "Solution_reading_time":60.05,
        "Solution_score_count":null,
        "Solution_sentence_count":66.0,
        "Solution_word_count":507.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>Please will you tell me how to cancel a dataset upload?  <\/p>\n<p>I tried to upload a small (321kb) CSV to Azure Machine Learning Studio.  The upload has been running for more than 1 hour, but it still says uploading.  <\/p>\n<p>I tried to upload other files (different names), and they are hanging too....same symptoms.  <\/p>\n<p>It seems the first problem is blocking all subsequent upload attempts.  <\/p>\n<p>Until today uploads worked perfectly.....multiple file types, multiple sizes, multiple dates, were all OK.  <\/p>\n<p>I have plenty of space left in my environment.  <\/p>\n<p>I tried closing and restarting my browser....same problem.  I accessed my AMLS via a different computer...same problem.  <\/p>\n<p>Thanks in advance for any advice you can give.  <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1592921687557,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/39201\/how-to-cancel-upload-azure-machine-learning-studio",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":4.8,
        "Challenge_reading_time":10.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"How to Cancel Upload? - Azure Machine Learning Studio",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":130,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Well, I don't know if you (<a href=\"\/users\/na\/?userid=d67fe4ea-5ec6-4e6b-b93c-404092429abd\">@GiftA-MSFT  <\/a>) did something to help, but it's solved!  Thanks if you did take that initiative, I appreciate it.  :-)    <\/p>\n<p>...or it could be that after 4-5 hours the upload just completed.  My internet connection was fine (it's a 40Mb line), so I'm not sure what the solution was.  Perhaps patience alone is the answer.    <\/p>\n<p>Anyway, thanks for taking an interest in my problem either way.      <\/p>\n<p>Best wishes.  :-)<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":6.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":78.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1579718832727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":149.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":11.8299888889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Mlflow project that raises an exception. I execute that function using <code>mlflow.run<\/code>, but I get <code>mlflow.exceptions.ExecutionException(\"Run (ID '&lt;run_id&gt;') failed\")<\/code>. <\/p>\n\n<p>Is there any way I could get the exception that is being raised where I am executing <code>mlflow.run<\/code>? <\/p>\n\n<p>Or is it possible to send an <code>mlflow.exceptions.ExecutionException<\/code> with custom message set from within the project?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579686060327,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59856641",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":6.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I throw an exception from within an MLflow project?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":428.0,
        "Challenge_word_count":68,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1472932425400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Unfortunately not at the moment. mlflow run starts a new process and there is no protocol for exception passing right now. In general the other project does not even have to be in the same language. <\/p>\n\n<p>One workaround I can think of is to pass the exception via mlflow by setting run tag. E.g.:<\/p>\n\n<pre><code>try:\n    ...\nexcept Exception as ex:\n    mlflow.set_tag(\"exception\", str(ex))\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1579728648287,
        "Solution_link_count":0.0,
        "Solution_readability":5.0,
        "Solution_reading_time":4.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":63.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"According to [Sagemaker's Pipeline Python SDK documenation](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html), looks like there is no specific pipeline step for model deployment. \n\nCan you please confirm this and, also, if there is a plan to have such a step? \n\nWhat is the recommended way to add a pipeline step to deploy the trained model, resulting in an enpoint being created?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669850852340,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1670198562116,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUiGdmBa_oQJuiiewjUhe9OA\/sagemaker-pipeline-deploy-model-step",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":5.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker Pipeline Deploy Model Step",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":312.0,
        "Challenge_word_count":60,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, there is indeed no specific pipeline step for model deployment. The idea is that SageMaker Pipelines is more about \"batch mode\", but customers do ask for this feature, so it might be added. \n\nYou can implement it quite easily using Lambda Step.\n\n1st create a Lambda function to deploy\/update the model:\n```\n%%writefile deploy_model_lambda.py\n\n\n\"\"\"\nThis Lambda function deploys the model to SageMaker Endpoint. \nIf Endpoint exists, then Endpoint will be updated with new Endpoint Config.\n\"\"\"\n\nimport json\nimport boto3\nimport time\n\n\nsm_client = boto3.client(\"sagemaker\")\n\n\ndef lambda_handler(event, context):\n\n    print(f\"Received Event: {event}\")\n\n    current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())\n    endpoint_instance_type = event[\"endpoint_instance_type\"]\n    model_name = event[\"model_name\"]\n    endpoint_config_name = \"{}-{}\".format(event[\"endpoint_config_name\"], current_time)\n    endpoint_name = event[\"endpoint_name\"]\n\n    # Create Endpoint Configuration\n    create_endpoint_config_response = sm_client.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=[\n            {\n                \"InstanceType\": endpoint_instance_type,\n                \"InitialVariantWeight\": 1,\n                \"InitialInstanceCount\": 1,\n                \"ModelName\": model_name,\n                \"VariantName\": \"AllTraffic\",\n            }\n        ],\n    )\n    print(f\"create_endpoint_config_response: {create_endpoint_config_response}\")\n\n    # Check if an endpoint exists. If no - Create new endpoint, if yes - Update existing endpoint\n    list_endpoints_response = sm_client.list_endpoints(\n        SortBy=\"CreationTime\",\n        SortOrder=\"Descending\",\n        NameContains=endpoint_name,\n    )\n    print(f\"list_endpoints_response: {list_endpoints_response}\")\n\n    if len(list_endpoints_response[\"Endpoints\"]) > 0:\n        print(\"Updating Endpoint with new Endpoint Configuration\")\n        update_endpoint_response = sm_client.update_endpoint(\n            EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n        )\n        print(f\"update_endpoint_response: {update_endpoint_response}\")\n    else:\n        print(\"Creating Endpoint\")\n        create_endpoint_response = sm_client.create_endpoint(\n            EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n        )\n        print(f\"create_endpoint_response: {create_endpoint_response}\")\n\n    return {\"statusCode\": 200, \"body\": json.dumps(\"Endpoint Created Successfully\")}\n```\n\nThen create the Lambda step:\n```\ndeploy_model_lambda_function_name = \"sagemaker-deploy-model-lambda-\" + current_time\n\ndeploy_model_lambda_function = Lambda(\n    function_name=deploy_model_lambda_function_name,\n    execution_role_arn=lambda_role,\n    script=\"deploy_model_lambda.py\",\n    handler=\"deploy_model_lambda.lambda_handler\",\n)\n```\n\nYou can see a full working example in [this notebook](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint.ipynb).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1669881962643,
        "Solution_link_count":1.0,
        "Solution_readability":23.2,
        "Solution_reading_time":37.71,
        "Solution_score_count":2.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":197.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I\u2019ve had some unexpected behavior using <code>pipenv<\/code> in 0.9.0 versus 0.8.2 - using guild check yields the following:<\/p>\n<ul>\n<li>in 0.9.0<\/li>\n<\/ul>\n<pre><code class=\"lang-plaintext\">\u27a4 pipenv run guild check\nguild_version:             0.9.0\nguild_install_location:    \/home\/neelav\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/lib\/python3.11\/site-packages\/guild\nguild_home:                \/home\/neelav\/.guild\nguild_resource_cache:      \/home\/neelav\/.guild\/cache\/resources\ninstalled_plugins:         config_flags, cpu, dask, disk, dvc, exec_script, gpu, ipynb, keras, memory, perf, python_script, quarto_document, queue, r_script, resource_flags, skopt\npython_version:            3.11.3 (main, Apr  5 2023, 15:52:25) [GCC 12.2.1 20230201]\npython_exe:                \/home\/neelav\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/bin\/python\nplatform:                  Linux 6.3.1-arch2-1 x86_64\npsutil_version:            5.9.5\ntensorboard_version:       2.13.0\ncuda_version:              12.1\nnvidia_smi_version:        530.41.03\nlatest_guild_version:      0.9.0\n<\/code><\/pre>\n<ul>\n<li>in 0.8.2:<\/li>\n<\/ul>\n<pre><code class=\"lang-plaintext\">\u27a4 pipenv run guild check\nguild_version:             0.8.2\nguild_install_location:    \/home\/neelav\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/lib\/python3.11\/site-packages\/guild\nguild_home:                \/home\/neelav\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/.guild\nguild_resource_cache:      \/home\/neelav\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/.guild\/cache\/resources\ninstalled_plugins:         config_flags, cpu, dask, disk, dvc, exec_script, gpu, ipynb, keras, memory, perf, python_script, quarto_document, queue, r_script, skopt\npython_version:            3.11.3 (main, Apr  5 2023, 15:52:25) [GCC 12.2.1 20230201]\npython_exe:                \/home\/neelav\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/bin\/python\nplatform:                  Linux 6.3.1-arch2-1 x86_64\npsutil_version:            5.9.5\ntensorboard_version:       2.13.0\ncuda_version:              12.1\nnvidia_smi_version:        530.41.03\nlatest_guild_version:      0.9.0\nA newer version of Guild AI is available. Run 'pip install guildai --upgrade' to install it.\n<\/code><\/pre>\n<p>so in 0.9.0 the <code>guild_home<\/code> variable doesn\u2019t seem to point to the virtual environment like it does in 0.8.2, instead pointing at my user\u2019s <code>.guild<\/code> directory. Moreover, I seem to run into issues with guild moving my sourcecode files into the run directory. I have a small example of this with the following guild file:<\/p>\n<pre><code class=\"lang-plaintext\">train:\n    exec: python .guild\/sourcecode\/train.py\n    output-scalars: False \n    sourcecode:\n        - exclude: '*'\n        - '*.py'\n    requires:\n        -\n            file: guild.yml\n            target-type: copy                            \n<\/code><\/pre>\n<p>and a Hello World script.<\/p>\n<pre><code class=\"lang-plaintext\">def main():\n    print(\"Hello World\")\n\nif __name__ == '__main__':\n    main()\n<\/code><\/pre>\n<p>The result:<\/p>\n<pre><code class=\"lang-plaintext\">\u27a4 pipenv shell\nLaunching subshell in virtual environment...\nWelcome to fish, the friendly interactive shell\nType help for instructions on how to use fish\n\u27a4  source \/home\/neelav\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/bin\/activate.fish\n\u27a4 guild run train\nYou are about to run train\nContinue? (Y\/n) \nResolving file:guild.yml\npython: can't open file '\/home\/neelav\/.guild\/runs\/d6d109936802465493d5ea91a3439f98\/.guild\/sourcecode\/train.py': [Errno 2] No such file or directory\n<\/code><\/pre>\n<p>and <code>ls<\/code> on the run directory gives:<\/p>\n<pre><code class=\"lang-plaintext\">\u27a4 ls -a ~\/.guild\/runs\/d6d109936802465493d5ea91a3439f98\/.guild\/\n.\/  ..\/  attrs\/  manifest  opref  output  output.index\n<\/code><\/pre>\n<p>In contrast, using version 0.8.2 with this configuration:<\/p>\n<pre><code class=\"lang-plaintext\">\u27a4 guild run train\nYou are about to run train\nContinue? (Y\/n) \nResolving file:guild.yml\nHello World\n\u27a4 guild runs\n[1:4c264831]  train  2023-05-15 17:34:11  completed  \n\u27a4 guild ls 1\n~\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/.guild\/runs\/4c264831b090451d86592dae5d359f3a:\n  guild.yml\n\u27a4 ls -a ~\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/.guild\/runs\/4c264831b090451d86592dae5d359f3a\n.\/  ..\/  .guild\/  guild.yml\n\u27a4 ls -a ~\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/.guild\/runs\/4c264831b090451d86592dae5d359f3a\/.guild\/\n.\/  ..\/  attrs\/  manifest  opref  output  output.index  sourcecode\/\n<\/code><\/pre>\n<p>So something seems to have changed regarding how guild interacts with <code>pipenv<\/code> between 0.8.2 and 0.9.0. Are there changes to the guild file that I need to make in 0.9.0 to get that functionality from 0.8.2?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684186807091,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/my.guild.ai\/t\/pipenv-compatibility-in-0-9-0\/1035",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":58.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":59,
        "Challenge_solved_time":null,
        "Challenge_title":"Pipenv compatibility in 0.9.0",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":35.0,
        "Challenge_word_count":408,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello! My apologies for the late reply here\u2026<\/p>\n<p>You\u2019re running into two changes in 0.9:<\/p>\n<ol>\n<li>In 0.9, Guild treats <code>.guild<\/code> directories in the current directory as \u2018Guild home\u2019 by default. To use an explicit location, set <code>GUILD_HOME<\/code>. Alternatively, modify <code>~\/.guild\/config.yml<\/code> to include:<\/li>\n<\/ol>\n<pre><code class=\"lang-yaml\"># ~\/.guild\/config.yml\n\nlegacy:\n  guild-home: pre-0.9\n<\/code><\/pre>\n<p>If this legacy behavior should be applied to specific projects, create <code>guild-config.yml<\/code> in your project directory (i.e. along side <code>guild.yml<\/code>). This file provides the same config as ~\/.guild\/config.yml` but applies to your project rather than to your system.<\/p>\n<ol start=\"2\">\n<li>In 0.9, Guild copies source code to the run directory root, rather than to <code>.guild\/sourcecode<\/code>. You can change this in your Guild file (<code>guild.yml<\/code>) using the <code>sourcecode<\/code> attribute for your operation.<\/li>\n<\/ol>\n<pre><code class=\"lang-yaml\"># guild.yml\n\ntrain:\n  sourcecode:\n    dest: .guild\/sourcecode\n    ...\n<\/code><\/pre>\n<p>However, in your case, consider changing <code>exec<\/code> simply to <code>python train.py<\/code>.<\/p>\n<p>I apologize for the breakage here! Guild should have done a better job here at surfacing the problem and pointing you to a solution. I\u2019ve opened an <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/502\">issue<\/a> to address this in 0.9.1 with additional help.<\/p>\n<p>We\u2019ll also confirm that the <code>legacy<\/code> switch above is properly documented.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.3,
        "Solution_reading_time":20.21,
        "Solution_score_count":null,
        "Solution_sentence_count":26.0,
        "Solution_word_count":190.0,
        "Tool":"Guild AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am running through the tutorial at ..https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/create-clustering-model-azure-machine-learning-designer\/explore-data    <\/p>\n<p>When I submit my pipeline I am seeing the error ...    <\/p>\n<p>An error occurred while submitting pipeline run    <br \/>\nGraphDatasetNotFound: Request failed with status code 400    <\/p>\n<p>This is an incredibly unhelpful message. I believe I have followed the steps as per the tutorial.     <\/p>\n<p>Any idea what is the cause of this error?    <\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":3,
        "Challenge_created_time":1619957038450,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/379678\/receive-error-graphdatasetnotfound-request-failed",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":10.6,
        "Challenge_reading_time":7.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Receive error GraphDatasetNotFound: Request failed with status code 400 when submitting pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":76,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>In dataset Version change from &quot;Always use latest&quot; to 1 or anyother version, worked for me<\/p>\n",
        "Solution_comment_count":9.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":1.35,
        "Solution_score_count":14.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1545311054088,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":170.0,
        "Answerer_view_count":33.0,
        "Challenge_adjusted_solved_time":8.4162855556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Kedro pipeline, nodes (something like python functions) are declared sequentially. In some cases, the input of one node is the output of the previous node. However, sometimes, when kedro run API is called in the commandline, the nodes are not run sequentially.<\/p>\n\n<p>In kedro documentation, it says that by default the nodes are ran in sequence. <\/p>\n\n<p>My run.py code:<\/p>\n\n<pre><code>def main(\ntags: Iterable[str] = None,\nenv: str = None,\nrunner: Type[AbstractRunner] = None,\nnode_names: Iterable[str] = None,\nfrom_nodes: Iterable[str] = None,\nto_nodes: Iterable[str] = None,\nfrom_inputs: Iterable[str] = None,\n):\n\nproject_context = ProjectContext(Path.cwd(), env=env)\nproject_context.run(\n    tags=tags,\n    runner=runner,\n    node_names=node_names,\n    from_nodes=from_nodes,\n    to_nodes=to_nodes,\n    from_inputs=from_inputs,\n)\n<\/code><\/pre>\n\n<p>Currently my last node is sometimes ran before my first few nodes.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572835098980,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58686533",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":12.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How to run the nodes in sequence as declared in kedro pipeline?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1741.0,
        "Challenge_word_count":118,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545311054088,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":170.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>The answer that I recieved from Kedro github:<\/p>\n\n<blockquote>\n  <p>Pipeline determines the node execution order exclusively based on\n  dataset dependencies (node inputs and outputs) at the moment. So the\n  only option to dictate that the node A should run before node B is to\n  put a dummy dataset as an output of node A and an input of node B.<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1572865397608,
        "Solution_link_count":0.0,
        "Solution_readability":12.8,
        "Solution_reading_time":4.38,
        "Solution_score_count":5.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":61.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hey,    <br \/>\nIs there any way to export the ML Pipeline as Template\/PNG\/Code ?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668149568060,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1085047\/azure-ml-pipeline-designer-export",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":1.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML pipeline designer export",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":18,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=979eeb5c-297b-4af8-af81-bb25ddabe5d5\">@Kumar Shanu  <\/a> The designer pipelines cannot be exported to code or a template currently.     <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.9,
        "Solution_reading_time":7.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1393579668636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":1450.0,
        "Answerer_view_count":162.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p><a href=\"https:\/\/stackoverflow.com\/questions\/tagged\/kedro\"><code>kedro<\/code><\/a> recommends storing parameters in <code>conf\/base\/parameters.yml<\/code>. Let's assume it looks like this:<\/p>\n\n<pre><code>step_size: 1\nmodel_params:\n    learning_rate: 0.01\n    test_data_ratio: 0.2\n    num_train_steps: 10000\n<\/code><\/pre>\n\n<p>And now imagine I have some <code>data_engineering<\/code> pipeline whose <code>nodes.py<\/code> has a function that looks something like this:<\/p>\n\n<pre><code>def some_pipeline_step(num_train_steps):\n    \"\"\"\n    Takes the parameter `num_train_steps` as argument.\n    \"\"\"\n    pass\n<\/code><\/pre>\n\n<p>How would I go about and pass that nested parameters straight to this function in <code>data_engineering\/pipeline.py<\/code>? I unsuccessfully tried:<\/p>\n\n<pre><code>from kedro.pipeline import Pipeline, node\n\nfrom .nodes import split_data\n\n\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                some_pipeline_step,\n                [\"params:model_params.num_train_steps\"],\n                dict(\n                    train_x=\"train_x\",\n                    train_y=\"train_y\",\n                ),\n            )\n        ]\n    )\n<\/code><\/pre>\n\n<p>I know that I could just pass all parameters into the function by using <code>['parameters']<\/code> or just pass all <code>model_params<\/code> parameters with <code>['params:model_params']<\/code> but it seems unelegant and I feel like there must be a way. Would appreciate any input!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587965065520,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61452211",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.3,
        "Challenge_reading_time":17.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Kedro - how to pass nested parameters directly to node",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1403.0,
        "Challenge_word_count":138,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525290575943,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":143.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>(Disclaimer: I'm part of the Kedro team)<\/p>\n\n<p>Thank you for your question. Current version of Kedro, unfortunately, does not support nested parameters. The interim solution would be to use top-level keys inside the node (as you already pointed out) or decorate your node function with some sort of a parameter filter, which is not elegant either.<\/p>\n\n<p>Probably the most viable solution would be to customise your <code>ProjectContext<\/code> (in <code>src\/&lt;package_name&gt;\/run.py<\/code>) class by overwriting <code>_get_feed_dict<\/code> method as follows:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>class ProjectContext(KedroContext):\n    # ...\n\n\n    def _get_feed_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Get parameters and return the feed dictionary.\"\"\"\n        params = self.params\n        feed_dict = {\"parameters\": params}\n\n        def _add_param_to_feed_dict(param_name, param_value):\n            \"\"\"This recursively adds parameter paths to the `feed_dict`,\n            whenever `param_value` is a dictionary itself, so that users can\n            specify specific nested parameters in their node inputs.\n\n            Example:\n\n                &gt;&gt;&gt; param_name = \"a\"\n                &gt;&gt;&gt; param_value = {\"b\": 1}\n                &gt;&gt;&gt; _add_param_to_feed_dict(param_name, param_value)\n                &gt;&gt;&gt; assert feed_dict[\"params:a\"] == {\"b\": 1}\n                &gt;&gt;&gt; assert feed_dict[\"params:a.b\"] == 1\n            \"\"\"\n            key = \"params:{}\".format(param_name)\n            feed_dict[key] = param_value\n\n            if isinstance(param_value, dict):\n                for key, val in param_value.items():\n                    _add_param_to_feed_dict(\"{}.{}\".format(param_name, key), val)\n\n        for param_name, param_value in params.items():\n            _add_param_to_feed_dict(param_name, param_value)\n\n        return feed_dict\n<\/code><\/pre>\n\n<p>Please also note that this issue has already been <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/commit\/529606273e201a736f10338ada73ac6206081730\" rel=\"nofollow noreferrer\">addressed on develop<\/a> and will become available in the next release. The fix uses the approach from the snippet above.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.7,
        "Solution_reading_time":25.18,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":203.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to write the output of batch scoring into datalake:<\/p>\n<pre><code>    parallel_step_name = &quot;batchscoring-&quot; + datetime.now().strftime(&quot;%Y%m%d%H%M&quot;)\n    \n    output_dir = PipelineData(name=&quot;scores&quot;, \n                              datastore=def_ADL_store,\n                              output_mode=&quot;upload&quot;,\n                              output_path_on_compute=&quot;path in data lake&quot;)\n\nparallel_run_config = ParallelRunConfig(\n    environment=curated_environment,\n    entry_script=&quot;use_model.py&quot;,\n    source_directory=&quot;.\/&quot;,\n    output_action=&quot;append_row&quot;,\n    mini_batch_size=&quot;20&quot;,\n    error_threshold=1,\n    compute_target=compute_target,\n    process_count_per_node=2,\n    node_count=2\n)\n    \n    batch_score_step = ParallelRunStep(\n        name=parallel_step_name,\n        inputs=[test_data.as_named_input(&quot;test_data&quot;)],\n        output=output_dir,\n        parallel_run_config=parallel_run_config,\n        allow_reuse=False\n    )\n<\/code><\/pre>\n<p>However I meet the error: &quot;code&quot;: &quot;UserError&quot;,\n&quot;message&quot;: &quot;User program failed with Exception: Missing argument --output or its value is empty.&quot;<\/p>\n<p>How can I write results of batch score to data lake?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596780750297,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63296185",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":21.8,
        "Challenge_reading_time":16.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to write Azure machine learning batch scoring results to data lake?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":277.0,
        "Challenge_word_count":85,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1513841518107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"China",
        "Poster_reputation_count":71.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>I don\u2019t think ADLS is supported for <code>PipelineData<\/code>. My suggestion is to use the workspace\u2019s default blob store for the <code>PipelineData<\/code>, then use a <code>DataTransferStep<\/code> for after the <code>ParallelRunStep<\/code> is completed.<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.9,
        "Solution_reading_time":3.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am setting up deployment pipelines for our models and I wanted to support this scenario:<\/p>\n\n<ol>\n<li>User registers model in <code>test<\/code> AML workspace in test subscription, checks in deployment code\/configs that references the model version (there is a <code>requirements.txt<\/code>-like file that specifies the model ID - name and version)<\/li>\n<li>Azure DevOps CI is triggered after code checkin to run <code>az ml model deploy<\/code> to a test environment.<\/li>\n<li>User decides after that endpoint works well, wants to deploy to prod. In Azure DevOps, manually invokes a prod pipeline that will use the same checked-in code\/configs (with the same referenced model):\n\n<ul>\n<li>copy the model from the <code>test<\/code> AML workspace to a new registered model in the <code>prod<\/code> AML workspace in a different subscription, <em>with the same version<\/em><\/li>\n<li>run <code>az ml model deploy<\/code> with different variables corresponding to the <code>prod<\/code> env, but using the same checked-in AML code\/configs<\/li>\n<\/ul><\/li>\n<\/ol>\n\n<p>I've looked at the MLOps references but can't seem to figure out how to support step 3 in the above scenario. <\/p>\n\n<p>I thought I could do an <code>az ml model download<\/code> to download the model from the <code>test<\/code> env and register it in the <code>prod<\/code> env. The registration process automatically sets the version number so, e.g. the config that references <code>myModel:12<\/code> is no longer valid since in <code>prod<\/code> the ID is <code>myModel:1<\/code><\/p>\n\n<p>How can I copy the model from one workspace in one subscription to another and preserve the ID?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1567105610797,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57716459",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":21.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Copying models between workspaces",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":515.0,
        "Challenge_word_count":242,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1254279877887,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA",
        "Poster_reputation_count":153.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>You could use model tags to set up your own identifiers that are shared across workspace, and query models with specific tags:<\/p>\n\n<pre><code>az ml model update --add-tag\naz ml model list --tag\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":2.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1604093818187,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Krakow, Poland",
        "Answerer_reputation_count":1200.0,
        "Answerer_view_count":263.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run a simple Ada-boosted Decision Tree regressor on GCP Vertex AI. To parse hyperparams and other arguments I use Click for Python, a very simple CLI library. Here's the setup for my task function:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@click.command()\n@click.argument(&quot;input_path&quot;, type=str)\n@click.option(&quot;--output-path&quot;, type=str, envvar='AIP_MODEL_DIR')\n@click.option('--gcloud', is_flag=True, help='Run as if in Google Cloud Vertex AI Pipeline')\n@click.option('--grid', is_flag=True, help='Perform a grid search instead of a single run. Ignored with --gcloud')\n@click.option(&quot;--max_depth&quot;, type=int, default=4, help='Max depth of decision tree', show_default=True)\n@click.option(&quot;--n_estimators&quot;, type=int, default=50, help='Number of AdaBoost boosts', show_default=True)\ndef click_main(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators)\n\n\ndef train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    print(input_path, output_path, gcloud)\n    logger = logging.getLogger(__name__)\n    logger.info(&quot;training models from processed data&quot;)\n    ...\n<\/code><\/pre>\n<p>When I run it locally like below, Click correctly grabs the params both from console and environment and proceeds with model training (<code>AIP_MODEL_DIR<\/code> is <code>gs:\/\/(BUCKET_NAME)\/models<\/code>)<\/p>\n<pre><code>\u276f python3 -m src.models.train_model gs:\/\/(BUCKET_NAME)\/data\/processed --gcloud\n\ngs:\/\/(BUCKET_NAME)\/data\/processed gs:\/\/(BUCKET_NAME)\/models True\n\n<\/code><\/pre>\n<p>However, when I put this code on the Vertex AI Pipeline, it throws an error, namely<\/p>\n<pre><code>FileNotFoundError: b\/(BUCKET_NAME)\/o\/data%2Fprocessed%20%20--gcloud%2Fprocessed_features.csv\n<\/code><\/pre>\n<p>As it is clearly seen, Click grabs both the parameter and the <code>--gcloud<\/code> option and assigns it to <code>input_path<\/code>. The print statement before that confirms it, both by having one too many spaces and <code>--gcloud<\/code> being parsed as false.<\/p>\n<pre><code>gs:\/\/(BUCKET_NAME)\/data\/processed  --gcloud gs:\/\/(BUCKET_NAME)\/models\/1\/model\/ False\n<\/code><\/pre>\n<p>Has anyone here encountered this issue or have any idea how to solve it?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641797901177,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1641896853596,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70648776",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.5,
        "Challenge_reading_time":31.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"Python click incorrectly parses arguments when called in Vertex AI Pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":129.0,
        "Challenge_word_count":240,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1579801831103,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tempe, AZ, USA",
        "Poster_reputation_count":71.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p>I think is due the nature of <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/arguments\/?highlight=arguments\" rel=\"nofollow noreferrer\">arguments<\/a> and <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/options\/?highlight=options\" rel=\"nofollow noreferrer\">options<\/a>, you are mixing arguments and options although is not implicit stated in the documentation but argument will eat up the options that follow. If nargs is not allocated it will default to 1 considering everything after it follows as string which it looks like this is the case.<\/p>\n<blockquote>\n<p>nargs \u2013 the number of arguments to match. If not 1 the return value is a tuple instead of single value. The default for nargs is 1 (except if the type is a tuple, then it\u2019s the arity of the tuple).<\/p>\n<\/blockquote>\n<p>I think you should first use options followed by the argument as display on the <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/documentation\/?highlight=arguments\" rel=\"nofollow noreferrer\">documentation page<\/a>. Other approach is to group it under a command as show on this <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/commands\/\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.9,
        "Solution_reading_time":15.04,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":143.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1655446100500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Previously, using Kubeflow Pipelines SDK v1, the status of a pipeline could be inferred during pipeline execution by passing an Argo placeholder, <code>{{workflow.status}}<\/code>, to the component, as shown below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import kfp.dsl as dsl\n\ncomponent_1 = dsl.ContainerOp(\n    name='An example component',\n    image='eu.gcr.io\/...\/my-component-img',\n    arguments=[\n               'python3', 'main.py',\n               '--status', &quot;{{workflow.status}}&quot;\n              ]\n)\n<\/code><\/pre>\n<p>This placeholder would take the value <code>Succeeded<\/code> or <code>Failed<\/code> when passed to the component. One use-case for this would be to send a failure-warning to eg. Slack, in combination with <code>dsl.ExitHandler<\/code>.<\/p>\n<p>However, when using Pipeline SDK version 2, <code>kfp.v2<\/code>, together with Vertex AI to compile and run the pipeline the Argo placeholders no longer work, as described by <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/7614\" rel=\"nofollow noreferrer\">this open issue<\/a>. Because of this, I would need another way to check the status of the pipeline within the component. I was thinking I could use the <code>kfp.Client<\/code> <a href=\"https:\/\/kubeflow-pipelines.readthedocs.io\/en\/latest\/source\/kfp.client.html\" rel=\"nofollow noreferrer\">class<\/a>, but I'm assuming this won't work using Vertex AI, since there is no &quot;host&quot; really. Also, there seems to be supported placeholders for to pass the run id (<code>dsl.PIPELINE_JOB_ID_PLACEHOLDER<\/code>) as a placeholder, as per <a href=\"https:\/\/stackoverflow.com\/questions\/68348026\/run-id-in-kubeflow-pipelines-on-vertex-ai\">this SO post<\/a>, but I can't find anything around <code>status<\/code>.<\/p>\n<p>Any ideas how to get the status of a pipeline run within a component, running on Vertex AI?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1651551486133,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1654867910903,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72094768",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":10.9,
        "Challenge_reading_time":24.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"How to get the status of a pipeline run within a component, running on Vertex AI?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":520.0,
        "Challenge_word_count":219,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562750927332,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Stockholm, Sverige",
        "Poster_reputation_count":803.0,
        "Poster_view_count":73.0,
        "Solution_body":"<p>Each pipeline run is automatically logged to Google Logging, and so are also the failed pipeline runs.\nThe error logs also contain information about the pipeline and the component that failed.<\/p>\n<p>We can use this information to monitor our logs and set up an alert via email for example.<\/p>\n<p>The logs for our Vertex AI Pipeline runs we get with the following filter<\/p>\n<p>resource.type=\u201daiplatform.googleapis.com\/PipelineJob\u201d\nseverity=(ERROR OR CRITICAL OR ALERT OR EMERGENCY)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/e4jFR.png\" rel=\"nofollow noreferrer\">Vertex AI Pipeline Logs<\/a><\/p>\n<p>Based on those logs you can set up log-based alerts <a href=\"https:\/\/cloud.google.com\/logging\/docs\/alerting\/log-based-alerts\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/logging\/docs\/alerting\/log-based-alerts<\/a>. Notifications via email, Slack, SMS, and many more are possible.<\/p>\n<p>source:\n<a href=\"https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":13.4,
        "Solution_reading_time":15.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":107.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1656061360900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":78.065125,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have built a custom container which use my managed dataset on vertex to run my training code, it worked successfully when I create the training job on the Vertex AI website interface.<\/p>\n<p>But now I'm trying to create the training job from a python script using<\/p>\n<pre><code>class google.cloud.aiplatform.CustomContainerTrainingJob\n<\/code><\/pre>\n<p>I load a managed dataset that I have on vertex AI with<\/p>\n<pre><code>dataset = aiplatform.ImageDataset(dataset_id) if dataset_id else None\n<\/code><\/pre>\n<p>But when I try to run the following code:<\/p>\n<pre><code>model = job.run(\n        dataset=dataset,\n        model_display_name=model_display_name,\n        args=args,\n        replica_count=replica_count,\n        machine_type=machine_type,\n        accelerator_type=accelerator_type,\n        accelerator_count=accelerator_count,\n        training_fraction_split=training_fraction_split,\n        validation_fraction_split=validation_fraction_split,\n        test_fraction_split=test_fraction_split,\n        sync=sync,\n    )\n\n    model.wait()\n\n    print(model.display_name)\n    print(model.resource_name)\n    print(model.uri)\n    return model\n<\/code><\/pre>\n<p>I got the following error:<\/p>\n<pre><code>google.api_core.exceptions.FailedPrecondition: 400 'annotation_schema_uri' should be set in the TrainingPipeline.input_data_config for custom training or hyperparameter tuning with managed dataset.\n<\/code><\/pre>\n<p>I feel like something is wrong because when I create the job on the website I specify an export directory for the managed dataset, but I have not found where to do it here.<\/p>\n<p>Any ideas?<\/p>\n<p>Thank you<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656062302777,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72741757",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":21.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Vertex AI Custom Container Training Job python SDK - google.api_core.exceptions.FailedPrecondition: 400 '",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":170.0,
        "Challenge_word_count":170,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1656061360900,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Well I found the answer in the <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/aiplatform?hl=fr#class-googlecloudaiplatformcustomcontainertrainingjobdisplayname-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-containeruri-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-command-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerimageuri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerpredictroute-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerhealthroute-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainercommand-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerargs-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerenvironmentvariables-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerports-optionalsequenceinthttpspythonreadthedocsioenlatestlibraryfunctionshtmlint--none-modeldescription-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelinstanceschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelparametersschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelpredictionschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-project-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-location-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-credentials-optionalgoogleauthcredentialscredentialshttpsgoogleapisdevpythongoogle-authlatestreferencegoogleauthcredentialshtmlgoogleauthcredentialscredentials--none-labels-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-trainingencryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelencryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-stagingbucket-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none\" rel=\"nofollow noreferrer\">documentation<\/a>, data are automatically exported to the provided bucket thus it was not the issue. The issue was in the error ( obviously ).\nTo provide a good annotation URI, it is enough to just add a parameter to run():<\/p>\n<pre><code>annotation_schema_uri=aiplatform.schema.dataset.annotation.image.classification\n<\/code><\/pre>\n<p>image.classification was what I needed here but can be replaced by text.extraction if you do text extraction for example.<\/p>\n<p>This will pass as string value the following value which is the asked gs uri:<\/p>\n<pre><code>gs:\/\/google-cloud-aiplatform\/schema\/dataset\/annotation\/image_classification_1.0.0.yaml\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1656343337227,
        "Solution_link_count":1.0,
        "Solution_readability":80.7,
        "Solution_reading_time":43.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":86.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\nshould highlight `instance type` field\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/843303\/182809305-2d25c565-18f8-4da0-ad9e-847c28cc62b0.png)\r\n\r\nthe field `AutoStopIdleTimeInMinutes` also is required.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1662449.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659604135000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/70",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":10.42,
        "Challenge_repo_contributor_count":38.0,
        "Challenge_repo_fork_count":5.0,
        "Challenge_repo_issue_count":119.0,
        "Challenge_repo_star_count":8.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"[Bug] highlight incorrect field in screenshot of importing Sagemaker workspace",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":98,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Already fixed",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":0.18,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":2.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1457555855467,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":86.0,
        "Answerer_view_count":23.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a big pipeline, taking a few hours to run. A small part of it needs to run quite often, how do I run it without triggering the entire pipeline?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574848206347,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59067349",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.8,
        "Challenge_reading_time":2.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How to run parts of your Kedro pipeline conditionally?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":4724.0,
        "Challenge_word_count":39,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457555855467,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":86.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>There are multiple ways to specify which nodes or parts of your pipeline to run. <\/p>\n\n<ol>\n<li><p>Use <code>kedro run<\/code> parameters like <code>--to-nodes<\/code>\/<code>--from-nodes<\/code>\/<code>--node<\/code> to explicitly define what needs to be run.<\/p><\/li>\n<li><p>In <code>kedro&gt;=0.15.2<\/code> you can define multiple pipelines, and then run only one of them with <code>kedro run --pipeline &lt;name&gt;<\/code>. If no <code>--pipeline<\/code> parameter is specified, the default pipeline is run. The default pipeline might combine several other pipelines. More information about using modular pipelines: <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/06_pipelines.html#modular-pipelines\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/06_pipelines.html#modular-pipelines<\/a><\/p><\/li>\n<li><p>Use tags. Tag a small portion of your pipeline with something like \"small\", and then do <code>kedro run --tag small<\/code>. Read more here: <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/05_nodes.html#tagging-nodes\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/05_nodes.html#tagging-nodes<\/a><\/p><\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.7,
        "Solution_reading_time":16.14,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":107.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on a prediction model and am about to use the azure machine learning studio resources. The main operation is to create a workspace on azure ML studio through Powershell. I would like to operate my workspace through the command line. Is there any way to develop and operate the ML Studio workspace through Powershell?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652332894457,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72210450",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":5.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Is there any way to create or delete workspaces in AML studio using powershell?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":104.0,
        "Challenge_word_count":70,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1652331444420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>According to the requirements, there is no procedure developed to create\/delete workspaces through PowerShell in machine learning studio. For reference of creation of workspaces, you can check the below link and the point to be noted is we can create\/delete workspaces using <em><strong>Az<\/strong><\/em><\/p>\n<p>Here is the table link to check PowerShell support table<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/previous-versions\/azure\/machine-learning\/classic\/powershell-module\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/previous-versions\/azure\/machine-learning\/classic\/powershell-module<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PGfhb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PGfhb.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.9,
        "Solution_reading_time":10.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1592673441292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":38.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been working with ML Studio (classic) and facing a problem with &quot;Execute Python&quot; scripts. I have noticed that it takes additional time to perform some internal tasks after which it starts executing the actual Python code in ML Studio. This delay has caused an increased time of 40-60 seconds per module which is aggregating and causing a delay of 400-500 seconds per execution when consumed through Batch Execution System or on running the experiments manually. (I've multiple Modules of &quot;Execute Python&quot; scripts)<\/p>\n<p>For instance - If I run a code in my local system, suppose it takes 2-3 seconds. The same would consume 50-60 seconds in Azure ML Studio.<\/p>\n<p>Can you please help understand the reason behind this or any optimization that can be done?<\/p>\n<p>Regards,\nAnant<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593694819477,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62696966",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":11.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Why does Azure ML Studio (classic) take additional time to execute Python Scripts?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":166.0,
        "Challenge_word_count":140,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582179684312,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":601.0,
        "Poster_view_count":94.0,
        "Solution_body":"<p>The known limitations of Machine Learning Studio (classic) are:<\/p>\n<p>The Python runtime is sandboxed and does not allow access to the network or to the local file system in a persistent manner.<\/p>\n<p>All files saved locally are isolated and deleted once the module finishes. The Python code cannot access most directories on the machine it runs on, the exception being the current directory and its subdirectories.<\/p>\n<p>When you provide a zipped file as a resource, the files are copied from your workspace to the experiment execution space, unpacked, and then used. Copying and unpacking resources can consume memory.<\/p>\n<p>The module can output a single data frame. It's not possible to return arbitrary Python objects such as trained models directly back to the Studio (classic) runtime. However, you can write objects to storage or to the workspace. Another option is to use pickle to serialize multiple objects into a byte array and then return the array inside a data frame.<\/p>\n<p>Hope this helps!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.6,
        "Solution_reading_time":12.57,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":162.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"based on the docs here https:\/\/github.com\/aws-samples\/sagemaker-pipelines-callback-step-for-batch-transform\/blob\/main\/batch_transform_with_callback.ipynb, a separate pipeline is created to perform a batch transform within sagemaker pipeline. the example utilizes a lambda and sqs to achieve this.  couldn't the batch transform job can simply be part of the training pipeline? once the model is trained, and added to model registry, one should be able to query the registry and get the latest model and run a batch transformation job on that, without the callback set up in the docs, right? any examples of running a batch transform job directly from a training pipeline?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671660466669,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1672008224227,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUFQCzo_y3TdiQ5iWO4sFR-Q\/how-to-run-an-inference-in-sagemaker-pipeline",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.2,
        "Challenge_reading_time":9.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"how to run an inference in sagemaker pipeline?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":459.0,
        "Challenge_word_count":102,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\n\nThe solution can be various and it depends on what you are trying to achieve. In general, I believe it is a good idea to build a generic pipeline and utilize parameters for different jobs. The image below shows a typical ML pattern with stages.\n\n\n![Enter image description here](\/media\/postImages\/original\/IMgAGV22YXQ16wVlctBVCnwg)\n\nYou can use condition steps to orchestrate Sagemaker jobs, more information with code examples are below:\n\nhttps:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker-pipelines\/tabular\/abalone_build_train_deploy\/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.html\n\nHope it helps,",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1671913943536,
        "Solution_link_count":1.0,
        "Solution_readability":15.8,
        "Solution_reading_time":8.21,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1470228490790,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":504.0,
        "Answerer_view_count":82.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to solve ImageClassification task. I have prepared a code to train, evaluate and deploy tensorflow model in SageMaker Notebook. I'm new with SageMaker and SageMaker Pipeline too. Currently, I'm trying to split my code and create SageMaker pipeline to solve Image Classification task.\nIn reference to AWS documentation there is <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html#step-type-processing\" rel=\"nofollow noreferrer\">Processing steps<\/a>. I have a code which read data from S3 and use <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator\" rel=\"nofollow noreferrer\">ImageGenerator<\/a> to generate augmented images on the fly while tensorflow model is still in the training stage.<\/p>\n<p>I don't find anything of how I can use <code>ImageGenerator<\/code> inside of Processing step in SageMaker Pipeline.<\/p>\n<p>My Code of <code>ImageGenerator<\/code>:<\/p>\n<pre><code>def load_data(mode):\n    if mode == 'TRAIN':\n        datagen = ImageDataGenerator(\n            rescale=1. \/ 255,\n            rotation_range = 0.5,\n            shear_range=0.2,\n            zoom_range=0.2,\n            width_shift_range = 0.2,\n            height_shift_range = 0.2,\n            fill_mode = 'nearest',\n            horizontal_flip=True)\n    else:\n        datagen = ImageDataGenerator(rescale=1. \/ 255)\n    return datagen\n\n\ndef get_flow_from_directory(datagen,\n                            data_dir,\n                            batch_size,\n                            shuffle=True):\n    assert os.path.exists(data_dir), (&quot;Unable to find images resources for input&quot;)\n    generator = datagen.flow_from_directory(data_dir,\n                                            class_mode = &quot;categorical&quot;,\n                                            target_size=(HEIGHT, WIDTH),\n                                            batch_size=batch_size,\n                                            shuffle=shuffle\n                                            )\n    print('Labels are: ', generator.class_indices)\n    return generator\n<\/code><\/pre>\n<p>Question is - does it possible to use <code>ImageGenerator<\/code> inside of <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html#step-type-processing\" rel=\"nofollow noreferrer\">Processing step<\/a> of SageMaker Pipeline?\nI'd appreciate for any ideas, Thanks.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655888089677,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72712449",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":26.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker Pipeline - Processing step for ImageClassification model",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":193,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1470228490790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":504.0,
        "Poster_view_count":82.0,
        "Solution_body":"<p>So, <code>ImageGenerator<\/code> and <code>flow_from_directory<\/code> I continue use inside of Training step. Processing step I skip at all, just use Training, Evaluating and Register model.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.7,
        "Solution_reading_time":2.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":24.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1663183171310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":16.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Sorry for long post, I need to explain it properly for people to undertsand.<\/p>\n<p>I have a pipeline in datafctory that triggers a published AML endpoint:\n<a href=\"https:\/\/i.stack.imgur.com\/mKIeU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mKIeU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I am trying to parametrize this ADF pipeline so that I can deploy to test and prod, but on test and prod the aml endpoints are different.<\/p>\n<p>Therefore, I have tried to edit the <strong>parameter configuration<\/strong> in ADF as shows here:\n<a href=\"https:\/\/i.stack.imgur.com\/c4g7x.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/c4g7x.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Here in the section <code>Microsoft.DataFactory\/factories\/pipelines<\/code> I add <code>&quot;*&quot;:&quot;=&quot;<\/code> so that all the pipeline parameters are parametrized:<\/p>\n<pre><code> &quot;Microsoft.DataFactory\/factories\/pipelines&quot;: {\n        &quot;*&quot;: &quot;=&quot;\n    }\n<\/code><\/pre>\n<p>After this I export the template to see which parameters are there in json, there are lot of them but I do not see any paramter that has aml endpoint name as value, but I see the endpint ID is parametrized.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/7WRUL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7WRUL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>My question is: Is it possible to parametrize the AML endpoint by name? So that, when deploying ADF to test I can just provide the AML endpoint name and it can pick the id automatically:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0Fu1g.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0Fu1g.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661258450327,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73458933",
        "Challenge_link_count":8,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":23.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to parametrize ML pipeline endpoint name - Azure Data Factory",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":67.0,
        "Challenge_word_count":213,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443017464707,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sweden",
        "Poster_reputation_count":644.0,
        "Poster_view_count":126.0,
        "Solution_body":"<p>i faced the similar issue when deploying adf pipelines with ml between environments. Unfortunately, As of now, adf parameter file do not have ml pipeline name as parameter value. only turn around solution is modifiying the parameter file(json) file with aligns with your pipeline design. For example, i am triggering ml pipeline endpoint inside foreach activity--&gt;if condition--&gt;ml pipeline<\/p>\n<p>Here is my parameter file values:<\/p>\n<pre><code>&quot;Microsoft.DataFactory\/factories\/pipelines&quot;: {\n    &quot;properties&quot;: {\n        &quot;activities&quot;: [\n            {\n                &quot;typeProperties&quot;: {\n                    &quot;mlPipelineEndpointId&quot;: &quot;=&quot;,\n                    &quot;url&quot;: {\n                        &quot;value&quot;: &quot;=&quot;\n                    },\n                    &quot;ifFalseActivities&quot;: [\n                        {\n                            &quot;typeProperties&quot;: {\n                                &quot;mlPipelineEndpointId&quot;: &quot;=&quot;\n                            }\n                        }\n                    ],\n                    &quot;ifTrueActivities&quot;: [\n                        {\n                            &quot;typeProperties&quot;: {\n                                &quot;mlPipelineEndpointId&quot;: &quot;=&quot;\n                            }\n                        }\n                    ],\n                    &quot;activities&quot;: [\n                        {\n                            &quot;typeProperties&quot;: {\n                                &quot;mlPipelineEndpointId&quot;: &quot;=&quot;,\n                                &quot;ifFalseActivities&quot;: [\n                                    {\n                                        &quot;typeProperties&quot;: {\n                                            &quot;mlPipelineEndpointId&quot;: &quot;=&quot;,\n                                            &quot;url&quot;: &quot;=&quot;\n                                        }\n                                    }\n                                ],\n                                &quot;ifTrueActivities&quot;: [\n                                    {\n                                        &quot;typeProperties&quot;: {\n                                            &quot;mlPipelineEndpointId&quot;: &quot;=&quot;,\n                                            &quot;url&quot;: &quot;=&quot;\n                                        }\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    }\n}\n<\/code><\/pre>\n<p>after you export the ARM template, the json file has records for your ml endpoints<\/p>\n<pre><code>&quot;ADFPIPELINE_NAME_properties_1_typeProperties_1_typeProperties_0_typeProperties_mlPipelineEndpointId&quot;: {\n        &quot;value&quot;: &quot;445xxxxx-xxxx-xxxxx-xxxxx&quot;\n<\/code><\/pre>\n<p>it is lot of manual effort to maintain if design is frequently changing so far worked for me. Hope this answers your question.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.6,
        "Solution_reading_time":23.75,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":140.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1377156004256,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to get a a simple Azure ML pipeline with the dogs vs cats data set following the steps  - <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">documented here<\/a><\/p>\n\n<p>My notebook contains the following -<\/p>\n\n<pre><code>import azureml.core\nfrom azureml.core import Workspace, Datastore\nfrom azureml.core import Environment\nfrom azureml.core.environment import CondaDependencies\nfrom azureml.pipeline.steps import PythonScriptStep\n\nws = Workspace.from_config()\n\nmyenv = Environment(name=\"myenv\")\nconda_dep = CondaDependencies()\nconda_dep.add_conda_package(\"keras\")\nconda_dep.add_conda_package(\"PIL\")\nmyenv.python.conda_dependencies=conda_dep\nmyenv.register(workspace=ws)\n<\/code><\/pre>\n\n<p>After setting up the data reference and the compute, here's how I am creating the pipeline -<\/p>\n\n<pre><code>trainStep = PythonScriptStep(\n    script_name=\"dogs_vs_cats.py\",\n    arguments=[\"--input\", blob_input_data, \"--output\", output_data1],\n    inputs=[blob_input_data],\n    outputs=[output_data1],\n    compute_target=compute_target,\n    source_directory=\"..\/dogs-vs-cats\"\n)\n\nSteps = [trainStep]\n\nfrom azureml.pipeline.core import Pipeline\npipeline1 = Pipeline(workspace=ws, steps=[Steps])\n\nfrom azureml.core import Experiment\n\npipeline_run1 = Experiment(ws, 'dogs_vs_cats_exp').submit(pipeline1)\npipeline_run1.wait_for_completion()\n<\/code><\/pre>\n\n<p>Once this steps is executed, the experiment fails and I get the following error after a bunch of information -<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"dogs_vs_cats.py\", line 30, in &lt;module&gt;\n    import keras\nModuleNotFoundError: No module named 'keras'\n<\/code><\/pre>\n\n<p>The terminal shows my conda environment set to azureml_py36 and Keras seems be listed in the output of <code>conda list<\/code>.<\/p>\n\n<p>Am I setting up the environment correctly? What is mising <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570082438523,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1570094136667,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58213125",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":25.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"ModuleNotFoundError: No module named 'keras' in Azure ML Pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":697.0,
        "Challenge_word_count":186,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1570078371343,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":3.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>From the way you have specified your environment, it's hard to see if it's a proper RunConfiguration object. If it is, it should be a matter of adding it to you PythonScriptStep.<\/p>\n\n<pre><code>trainStep = PythonScriptStep(\n    script_name=\"dogs_vs_cats.py\",\n    arguments=[\"--input\", blob_input_data, \"--output\", output_data1],\n    inputs=[blob_input_data],\n    outputs=[output_data1],\n    compute_target=compute_target,\n    source_directory=\"..\/dogs-vs-cats\",\n    runconfig=myenv\n)\n<\/code><\/pre>\n\n<p>Right now you're defining the environment, but no using it anywhere it seems. If your trouble persists maybe try defining your RunConfiguration like they do under the \"Specify the environment to run the script\" step in this notebook:<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.7,
        "Solution_reading_time":14.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1324129118823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11756.0,
        "Answerer_view_count":517.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm looking to deploy the SageMaker pipeline using CDK (<a href=\"https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html<\/a>) but could not find any code examples. Any pointers?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652282728443,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72203674",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.3,
        "Challenge_reading_time":5.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Deploy AWS SageMaker pipeline using the cloud development kit (CDK)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":31,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1489685785576,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Canada",
        "Poster_reputation_count":65.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>CDK L1 Constructs correspond 1:1 to a CloudFormation resource of the same name. The construct props match the resouce properties.  The go-to source is therefore the CloudFormation docs.<\/p>\n<p>The <code>AWS::SageMaker::Pipeline<\/code> <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html#aws-resource-sagemaker-pipeline--examples\" rel=\"nofollow noreferrer\">docs have a more complete example<\/a>.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.8,
        "Solution_reading_time":6.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":39.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":9,
        "Challenge_body":"<p>I am loving <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a> and all it can do for me. I have a question whose answer I cannot find anywhere.<br>\nAmong the various fields in the wandb.config file are a few that wandb generates automatically. One of them is <code>Description<\/code>. I tried setting it from a Python program via my configuration file, but to no avail. So I am wondering how to set the Description field programmatically. This will allow me to \u201cdescribe\u201d several hundred simulations for easy retrieval. Thanks,<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660092658434,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/description-field\/2881",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":6.9,
        "Challenge_reading_time":6.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Description field",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":131.0,
        "Challenge_word_count":83,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/erlebacher\">@erlebacher<\/a> , It\u2019s pretty expensive to do pattern filtering in MySQL, especially on a large column like <code>notes<\/code> . The engineering team decided this feature will not be implemented. I will mark this resolved but please let me know if there is anything else I can answer for you.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":4.27,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":50.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a output from previous step and want to use it as input to multiple steps.    <br \/>\nBut, when I run the experiment, the pipeline looks like this    <\/p>\n<p>Here is my code    <\/p>\n<pre><code>source_directory=&quot;.\/test&quot;  \ndesigner1_config = ScriptRunConfig(source_directory=source_directory,  \n                                 command=[&quot;python&quot;, &quot;designer1.py&quot;,   \n                                          &quot;--output_test1&quot;, output_test1], #   \n                                 compute_target=aml_compute,  \n                                 environment=env_py)  \n  \ndesigner1_step = CommandStep(name=&quot;designer_step&quot;,   \n                           inputs=[input_dept_fun_d],  \n                           outputs=[output_test1], #  \n                           runconfig=designer1_config,  \n                           allow_reuse=True)  \n\n\nsource_directory=&quot;.\/test&quot;  \ndesigner2_config = ScriptRunConfig(source_directory=source_directory,  \n                                 command=[&quot;python&quot;, &quot;designer2.py&quot;], #   \n                                 compute_target=aml_compute,  \n                                 environment=env_py)  \n  \ndesigner2_step = CommandStep(name=&quot;designer2_step&quot;,   \n                           inputs=[input_dept_fun_d, output_test1],  \n                           outputs=[], #  \n                           runconfig=designer2_config,  \n                           allow_reuse=True)  \n<\/code><\/pre>\n<p>Use 2 steps, it shows picture 1    <\/p>\n<pre><code>step_sequence = [designer1_step, designer2_step]  \n<\/code><\/pre>\n<p>Use 1 step, it shows picture 2    <\/p>\n<pre><code>step_sequence = [designer2_step]  \n  \n<\/code><\/pre>\n<p>Submit the experiement    <\/p>\n<pre><code>pipeline = Pipeline(workspace=ws, steps=step_sequence)  \npipeline_run = Experiment(workspace=ws, name='pipeline').submit(config=pipeline, regenerate_outputs=True)  \n<\/code><\/pre>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/190374-image.png?platform=QnA\" alt=\"190374-image.png\" \/>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/190373-image.png?platform=QnA\" alt=\"190373-image.png\" \/>    <\/p>\n<p>Here's what I want    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/190392-image.png?platform=QnA\" alt=\"190392-image.png\" \/>    <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1649214311427,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/801687\/azure-sdk-previous-step-output-to-multiple-steps-a",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":22.4,
        "Challenge_reading_time":25.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure SDK previous step output to multiple steps as input",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":126,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@MiaZhangWHQWistron-2092 Based on the setup for designer2_step the inputs are the original input of step1 and the output of step1. The second screen shot seems appropriate and the designer has just replicated the original input dataset for step2. The connection that you are referring to is irrelevant because the same dataset is used, and designer only displays it for simplicity.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.7,
        "Solution_reading_time":9.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1403112997127,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":142.9204211111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Am I missing something but how can I cancel a run in my workspace from <a href=\"https:\/\/ms.portal.azure.com\/\" rel=\"nofollow noreferrer\">https:\/\/ms.portal.azure.com\/<\/a> ? The cancel button is always greyed out.<\/p>\n\n<p>I know I can use use the sdk to cancel a run using:<\/p>\n\n<pre><code>run = [ r for r in Experiment(ws, 'myExp').get_runs() if r.id == '899b8314-26b6-458f-9f5c-539ffbf01b91'].pop()\nrun.cancel()\n<\/code><\/pre>\n\n<p>But it would be more convenient to be able to do it from the UI<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568993659347,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58031370",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to cancel a running job from the UI?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":204.0,
        "Challenge_word_count":74,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565794118448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":113.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>What kind of run is this? Canceling is not currently enabled for pipeline runs in the UI, but is supported for other run types.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1569508172863,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":1.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi everyone!<\/p>\n<p>I am trying to retrieve  filtered runs from a project using the WandB API and a filter dictionary.<\/p>\n<p>I try to do the following:<\/p>\n<pre><code class=\"lang-auto\">api = wandb.Api()\nfilter_dict = {\"job_type\":  \"my_job_type\"}\nruns = api.runs(\"my_entity\/my_project\", filters=filter_dict)\nfor run in runs:\n    print(run)\n<\/code><\/pre>\n<p>When I do this, I get the following error message:<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\apis\\public.py\", line 980, in __next__\n    if not self._load_page():\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\apis\\public.py\", line 965, in _load_page\n    self.last_response = self.client.execute(\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\retry.py\", line 168, in wrapped_fn\n    return retrier(*args, **kargs)\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\retry.py\", line 108, in __call__\n    result = self._call_fn(*args, **kwargs)\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\apis\\public.py\", line 207, in execute\n    return self._client.execute(*args, **kwargs)\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\wandb_gql\\client.py\", line 52, in execute\n    result = self._get_result(document, *args, **kwargs)\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\wandb_gql\\client.py\", line 60, in _get_result\n    return self.transport.execute(document, *args, **kwargs)\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\wandb_gql\\transport\\requests.py\", line 39, in execute\n    request.raise_for_status()\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\requests\\models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https:\/\/api.wandb.ai\/graphql\n<\/code><\/pre>\n<p>However, other filters do work. For example I can do the above described procedure with<\/p>\n<pre><code class=\"lang-auto\">filter_dict = {\"group\":  \"my_group\"}\n<\/code><\/pre>\n<p>and it yields the correctly filtered jobs.<\/p>\n<p>What I am currently doing as a workaround is this:<\/p>\n<pre><code class=\"lang-auto\">api = wandb.Api()\nruns = api.runs(\"my_entity\/my_project\")\nfor run in runs:\n    if run.job_type == \"my_job_type\":\n        print(run)\n<\/code><\/pre>\n<p>However, I would prefer to directly filter the runs with the API call. Any idea what I am doing wrong?<\/p>\n<p>Thanks in advance!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667921959400,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/filtering-runs-by-job-type-using-api-is-not-working\/3390",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":13.4,
        "Challenge_reading_time":34.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":null,
        "Challenge_title":"Filtering runs by job_type using API is not working",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":183.0,
        "Challenge_word_count":235,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kolja\">@kolja<\/a> thank you for writing in! Could you please try if the following would work for you?<\/p>\n<pre><code class=\"lang-auto\">api = wandb.Api()\nfilter_dict = {\"jobType\":  \"my_job_type\"}\nruns = api.runs(\"my_entity\/my_project\", filters=filter_dict)\nfor run in runs:\n    print(run)\n<\/code><\/pre>\n<p>The queries in <code>filters<\/code> are using the MongoDB query language. Hope this helps!<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":5.55,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":48.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am interested in knowing how can I integrate a repository with Azure Machine Learning Workspace.<\/p>\n<h2>What have I tried ?<\/h2>\n<p>I have some experience with Azure Data Factory and usually I have setup workflows where<\/p>\n<ol>\n<li><p>I have a <code>dev<\/code> azure data factory instance that is linked to azure repository.<\/p>\n<\/li>\n<li><p>Changes made to the repository using the code editor.<\/p>\n<\/li>\n<li><p>These changes are published via the <code>adf_publish<\/code> branch to the live <code>dev<\/code> instance<\/p>\n<\/li>\n<li><p>I use CI \/ CD pipeline and the AzureRMTemplate task to deploy the templates in the publish branch to release the changes to <code>production<\/code> environment<\/p>\n<\/li>\n<\/ol>\n<h2>Question:<\/h2>\n<ul>\n<li>How can I achieve the same \/ similar workflow with Azure Machine Learning Workspace ?<\/li>\n<li>How is CI \/ CD done with Azure ML Workspace<\/li>\n<\/ul>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655471491697,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72659937",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":11.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"CI \/ CD and repository integration for Azure ML Workspace",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":136,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1271093246887,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United States",
        "Poster_reputation_count":9826.0,
        "Poster_view_count":1238.0,
        "Solution_body":"<p>The following workflow is the official practice to be followed to achieve the task required.<\/p>\n<ol>\n<li>Starting with the architecture mentioned below<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KdRUa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KdRUa.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ul>\n<li>we need to have a specific data store to handle the dataset<\/li>\n<li>Perform the regular code modifications using the IDE like Jupyter Notebook or VS Code<\/li>\n<li>Train and test the model<\/li>\n<li>To register and operate on the model, deploy the model image as a web service and operate the rest.<\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>Configure the CI Pipeline:<\/strong><\/li>\n<\/ol>\n<ul>\n<li><p>Follow the below steps to complete the procedure<\/p>\n<p><strong>Before implementation:<\/strong><\/p>\n<pre><code>- We need azure subscription enabled account\n- DevOps activation must be activated.\n<\/code><\/pre>\n<\/li>\n<li><p>Open DevOps portal with enabled SSO<\/p>\n<\/li>\n<li><p>Navigate to <strong>Pipeline -&gt; Builds -&gt; Choose the model which was created -&gt; Click on EDIT<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/yUVZl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/yUVZl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>Build pipeline will be looking like below screen\n<a href=\"https:\/\/i.stack.imgur.com\/VSKJq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VSKJq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>We need to use Anaconda distribution for this example to get all the dependencies.<\/p>\n<\/li>\n<li><p>To install environment dependencies, check the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/tasks\/package\/conda-environment?view=azure-devops&amp;viewFallbackFrom=azdevops\" rel=\"nofollow noreferrer\">link<\/a><\/p>\n<\/li>\n<li><p>Use the python environment, under <strong>Install Requirements<\/strong> in user setup.<\/p>\n<\/li>\n<li><p>Select <strong>create or get workspace<\/strong> select your account subscription as mentioned in below screen<\/p>\n<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/vt0el.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vt0el.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ul>\n<li>Save the changes happened in other tasks and all those muse be in same subscription.\n<a href=\"https:\/\/i.stack.imgur.com\/WJxCL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WJxCL.png\" alt=\"enter image description here\" \/><\/a><\/li>\n<\/ul>\n<p>The entire CI\/CD procedure and solution was documented in <a href=\"https:\/\/www.azuredevopslabs.com\/labs\/vstsextend\/aml\/#author-praneet-singh-solanki\" rel=\"nofollow noreferrer\">link<\/a><\/p>\n<p><strong>Document Credit: Praneet Singh Solanki<\/strong><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":12.0,
        "Solution_readability":14.0,
        "Solution_reading_time":36.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":278.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I am new to Azure Machine Learning, I have some questions related to cost<\/p>\n<p>1) What will be the cost breakup? (Workspace charges, Network charges, Disk Charges etc.)<\/p>\n<p>2) I saw that there were some charges for Bandwidth and Load balancer. Why is it getting charged and I am not able to see the details in Azure ML?<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/46057-azure-ml-cost.png?platform=QnA\" alt=\"46057-azure-ml-cost.png\" \/><\/p>\n<p>3) I have a SQL Sever which is inside a Vnet (VPN Gateway) and I want to integrate it with Azure Machine Learning and use the data for my analysis. What all will be the charges for it?<\/p>\n<p>4) Is there a way to stop the computes automatically when i am not using it, as i dont want to be charged when i am not using the Azure Machine learning?<\/p>\n<p>5) Will there be any charges even though i am not using the Azure Machine learning? (Static Charges)<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607408839580,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/190053\/azure-machine-learning-cost",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":7.6,
        "Challenge_reading_time":12.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning cost",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":153,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=c0af539e-5b19-433b-85a1-2ca81772cd40\">@Srinivasan G  <\/a> Here are the charges that you could incur for the above setup:<\/p>\n<blockquote>\n<p>1) What will be the cost breakup? (Workspace charges, Network charges, Disk Charges etc.)<\/p>\n<\/blockquote>\n<p>For Azure Machine learning the <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/machine-learning\/\">charges<\/a> of ML services are nil for enterprise edition workspaces. Currently all the workspaces are using enterprise edition and if there are any basic edition workspaces they can be migrated with no down time. All other charges while using Azure ML workspace depends on the compute used and the setup of the compute. If they are enabled in a vnet then you could incur data charges according to vnet's <a href=\"https:\/\/azure.microsoft.com\/en-in\/pricing\/details\/virtual-network\/\">pricing<\/a>.<\/p>\n<blockquote>\n<p>2) I saw that there were some charges for Bandwidth and Load balancer. Why is it getting charged and I am not able to see the details in Azure ML?<\/p>\n<\/blockquote>\n<p>The charges incurred for LB and bandwidth could be based on your setup and how the compute was setup to run the experiments. If you have also setup your designer to use virtual network then there could be charges on how the compute was setup with respect to the region. More details of the setup of workspace with private networks are detailed <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-network-security-overview\">here<\/a>. For the breakup of charges mentioned above you can raise a support request through the azure portal for billing which does not require any support plan to raise a ticket from the Help+Support tab on Azure portal.<\/p>\n<blockquote>\n<p>3) I have a SQL Sever which is inside a Vnet (VPN Gateway) and I want to integrate it with Azure Machine Learning and use the data for my analysis. What all will be the charges for it?<\/p>\n<\/blockquote>\n<p>Once you are able to confirm if your vnet connection is successful you can get the data from Azure SQL database using the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/import-data\">import data<\/a> module from Azure ML designer. If you are planning to setup your own SQL server then this functionality is not supported with the available options, you might need to use the option URL via HTTP to get the required data in your experiments.<\/p>\n<p>If you are attaching storage from different region than workspace region, it can result in higher latency and additional network usage costs.<\/p>\n<blockquote>\n<p>4) Is there a way to stop the computes automatically when i am not using it, as i dont want to be charged when i am not using the Azure Machine learning?<\/p>\n<\/blockquote>\n<p>The option to shutdown compute instance automatically that is not in use is not available currently. You can stop the compute instances that are not required, for compute clusters you can set the minimum no. of nodes to 0 to ensure no compute is running when not in use.<\/p>\n<blockquote>\n<p>5) Will there be any charges even though i am not using the Azure Machine learning? (Static Charges)<\/p>\n<\/blockquote>\n<p>Azure ML learning enterprise edition workspaces do not have a surcharge i.e charges for using Azure ML. If there are any associated compute or storage or virtual networks there could be charges if used.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":9.5,
        "Solution_reading_time":42.67,
        "Solution_score_count":2.0,
        "Solution_sentence_count":27.0,
        "Solution_word_count":510.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"The warning claims that the project is not initialised yet, and that you must call ``kedro mlflow init`` before calling any command while you are calling ``kedro mlflow init``. It can be safely ignored because the command works as intended. This bug is due to the dynamic creation of command.",
        "Challenge_closed_time":1600718.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593379921000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/14",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.33,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Warning message appears when calling ``kedro mlflow init``",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created and published a Azure ML pipeline. I want to trigger the ML pipeline from Azure Data Factory.  <\/p>\n<p>In ADF, i have chosen Machine learning execute pipeline and created the linked service to azure machine learning and able to choose the published pipeline endpoint. However while running, i am getting the below error. I couldn't find much information how to resolve the error.   <\/p>\n<p>&quot;Convert Failed. The value type 'System.String', in key 'azureCloudType' is not expected type 'Microsoft.DataTransfer.Common.Models.AzureCloudType&quot;<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":10,
        "Challenge_created_time":1645081864137,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/739240\/trigger-azure-ml-pipeline-from-azure-data-factory",
        "Challenge_link_count":0,
        "Challenge_participation_count":12,
        "Challenge_readability":8.3,
        "Challenge_reading_time":7.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Trigger Azure ML Pipeline from Azure Data Factory",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":88,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=0cab9490-181d-4799-9dfb-834a723c261c\">@Vinoth Kumar K  <\/a> ,    <br \/>\nWelcome to Microsoft Q&amp;A platform and thankyou for posting your query.     <br \/>\nAs per the details you have shared in the query, it looks like a product bug. I have raised this issue with the internal Product team. Once I hear back from them, I will keep everyone posted on this. Thanks for your patience!<\/p>\n",
        "Solution_comment_count":14.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.9,
        "Solution_reading_time":5.11,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":62.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI am a little confused about whether S3 Shard key would work when using PIPE mode, here is a example:\n\nAssume I have:\n\n2 instance, each instance have 4 worker;\n\ndata: total 8 files with total size 8GB, each file is 1GB. Put them into 4 different S3 path, that means, each path has 2 files (2GB in total)\n\nIf I use PIPE mode, and s3_input using  distribution='ShardedByS3Key', and create 4 channel (each channel mapping a s3 path, 2 files)\n\ntrain_s3_input_1 = sagemaker.inputs.s3_input(channel_1, distribution='ShardedByS3Key')\n\nQuestion:\n\nHow much data of each worker get to train, 1 file or 2 files? thanks",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589363811000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667925675584,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU31DdUqtuQziixKTkPasZKw\/confusion-about-pipe-mode-when-using-s3-shard-key",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":8.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"confusion about PIPE mode when using S3 shard key",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":46.0,
        "Challenge_word_count":108,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\nSageMaker will replicate a subset of data (1\/n ML compute instances) on each ML compute instance that is launched for model training when you specify *ShardedByS3Key*. If there are n ML compute instances launched for a training job, each instance gets approximately 1\/n of the number of S3 objects. This applies in both File and Pipe modes. Keep this in mind when developing algorithms.\n\nTo answer your question:\nHow much data of each worker get to train, 1 file or 2 files? 1 file each from the training channel.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925572408,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":6.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":90.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I deployed my Training pipeline and my Real-time inference pipeline.  <br \/>\nWith the REST-Api of my training pipeline I'm able to retrain my ML model. Is it possible to use that retrained model automated in my real inference pipeline?  <br \/>\nWhen i trigger the pipeline in ML studio I have to update my real inference pipeline manually. Since I want to trigger my retraining external that is not possible.  <br \/>\nThanks in advance.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615298736310,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/305899\/update-real-interference-pipeline",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.5,
        "Challenge_reading_time":5.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"update real interference pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":76,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, here's a reference on which <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-ml-pipelines#which-azure-pipeline-technology-should-i-use\">technology<\/a> to use based on a given scenario. For your scenario, you should be able to create an Azure Machine Learning pipeline using the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-trigger-published-pipeline\">SDK to trigger a pipeline<\/a> based on a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-trigger-published-pipeline#create-a-schedule\">time\/change based schedule<\/a> and then <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-update-web-service\">update the web service<\/a> accordingly. Depending on the complexity of your triggers or data prep needs, you can leverage other technologies such as <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-trigger-published-pipeline#use-azure-logic-apps-for-complex-triggers\">Logic Apps<\/a> or <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-trigger-published-pipeline#call-machine-learning-pipelines-from-azure-data-factory-pipelines\">Azure Data Factory<\/a> to trigger your Azure Machine Learning pipeline. Currently, you can only use the Azure Machine Learning SDK to automatically update the web service. Hope this helps.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":17.6,
        "Solution_reading_time":18.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":105.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1510344022336,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Moscow, Russia",
        "Answerer_reputation_count":1182.0,
        "Answerer_view_count":117.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using Mlflow as a work orchestration tool. I have a Machine Learning pipeline. In this pipeline, I have real-time data. I'm listening this data with Apache Kafka. Also, I'm doing this: Whenever 250 message comes to this topic, I'm gathering them, and I'm appending this message my previous data. After that, my training function is triggered. Thus, I am able to making new training in every 250 new data. With Mlflow, I can show the results, metrics and any other parameters of trained models. But After training occurred one time, the second one doesn't occurs, and It throws me this error which I have shown in title. Here it is my consumer:<\/p>\n<pre><code>topic_name = 'twitterdata'\ntrain_every = 250\n\n\ndef consume_tweets():\n    consumer = KafkaConsumer(\n        topic_name,\n        bootstrap_servers=['localhost:9093'],\n        auto_offset_reset='latest',\n        enable_auto_commit=True,\n        auto_commit_interval_ms=5000,\n        fetch_max_bytes=128,\n        max_poll_records=100,\n        value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n\n    tweet_counter = 0\n    for message in consumer:\n        tweets = json.loads(json.dumps(message.value))\n        # print(tweets['text'])\n        tweet_sentiment = make_prediction(tweets['text'])\n\n        if tweet_counter == train_every:\n            update_df()\n            data_path = 'data\/updated_tweets.csv'\n            train(data_path)\n            print(&quot;\\nTraining with new data is completed!\\n&quot;)\n            tweet_counter = 0\n\n        else:\n            tweet_counter += 1\n\n        publish_prediction(tweet_sentiment, tweets['text'])\n\n<\/code><\/pre>\n<p>And here it is my train.py:<\/p>\n<pre><code>train_tweets = pd.read_csv(DATA_PATH)\n    # train_tweets = train_tweets[:20000]\n\n    tweets = train_tweets.tweet.values\n    labels = train_tweets.label.values\n\n    # Log data params\n    mlflow.log_param('input_rows', train_tweets.shape[0])\n\n    # Do preprocessing and return vectorizer with it\n    vectorizer, processed_features = embedding(tweets)\n\n    # Saving vectorizer\n    save_vectorizer(vectorizer)\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)\n\n    # Handle imbalanced data by using 'Smote' and log to Mlflow\n    smote = SMOTE('minority')\n    mlflow.log_param(&quot;over-sampling&quot;, smote)\n\n    X_train, y_train = smote.fit_sample(X_train, y_train)\n\n    # text_classifier = MultinomialNB()\n    text_classifier = LogisticRegression(max_iter=10000)\n    text_classifier.fit(X_train, y_train)\n    predictions = text_classifier.predict(X_test)\n\n    # Model metrics\n    (rmse, mae, r2) = eval_metrics(y_test, predictions)\n\n    mlflow.log_param('os-row-Xtrain', X_train.shape[0])\n    mlflow.log_param('os-row-ytrain', y_train.shape[0])\n    mlflow.log_param(&quot;model_name&quot;, text_classifier)\n    mlflow.log_metric(&quot;rmse&quot;, rmse)\n    mlflow.log_metric(&quot;r2&quot;, r2)\n    mlflow.log_metric(&quot;mae&quot;, mae)\n    mlflow.log_metric('acc_score', accuracy_score(y_test, predictions))\n\n    mlflow.sklearn.log_model(text_classifier, &quot;model&quot;)\n<\/code><\/pre>\n<p>I couldn't solve the problem. MLflow is one of the newest tool, so issues and examples of Mlflow are very few.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614014592930,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1614015098323,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66320435",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.8,
        "Challenge_reading_time":40.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"mlflow.exceptions.MlflowException: Changing param values is not allowed. Param with key='input_rows' was already logged with value='32205'",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":2716.0,
        "Challenge_word_count":292,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1613130263950,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Turkey",
        "Poster_reputation_count":44.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>I think you need an MLflow &quot;run&quot; for every new batch of data, so that your parameters are logged independently for each new training.<\/p>\n<p>So, try the following in your consumer:<\/p>\n<pre><code>if tweet_counter == train_every:\n            update_df()\n            data_path = 'data\/updated_tweets.csv'\n            with mlflow.start_run() as mlrun:\n               train(data_path)\n            print(&quot;\\nTraining with new data is completed!\\n&quot;)\n            tweet_counter = 0\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":5.6,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":51.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Amazon SageMaker has <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">inference pipelines<\/a> that process requests for inferences on data. It sounds as though inferences are similar (or perhaps identical) to predictions. Are there any differences between inferences and predictions? If so, what? If not, why not just call it a prediction pipeline?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561555658777,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56773989",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":6.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"In Amazon SageMaker, what (if any) is the difference between an inference and prediction?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":42.0,
        "Challenge_word_count":61,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1336973807643,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Minneapolis, MN, United States",
        "Poster_reputation_count":1907.0,
        "Poster_view_count":174.0,
        "Solution_body":"<p>Inference usually refers to applying a learned transformation to input data. That learned transformation could be something else than a prediction (eg dim reduction, clustering, entity extraction etc). So calling that process a prediction would be a bit too restrictive in my opinion<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":3.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm deploying a SageMaker inference pipeline composed of two PyTorch models (<code>model_1<\/code> and <code>model_2<\/code>), and I am wondering if it's possible to pass the same input to both the models composing the pipeline.<\/p>\n<p>What I have in mind would work more or less as follows<\/p>\n<ol>\n<li><p>Invoke the endpoint sending a binary encoded payload (namely <code>payload_ser<\/code>), for example:<\/p>\n<pre><code>client.invoke_endpoint(EndpointName=ENDPOINT,\n                       ContentType='application\/x-npy',\n                       Body=payload_ser)\n<\/code><\/pre>\n<\/li>\n<li><p>The first model parses the payload with <code>inut_fn<\/code> function, runs the predictor on it, and returns the output of the predictor. As a simplified example:<\/p>\n<pre><code>def input_fn(request_body, request_content_type):\n    if request_content_type == &quot;application\/x-npy&quot;:\n        input = some_function_to_parse_input(request_body)\n    return input\n\ndef predict_fn(input_object, predictor):\n    outputs = predictor(input_object)\n    return outputs\n\ndef output_fn(predictions, response_content_type):\n    return json.dumps(predictions)\n<\/code><\/pre>\n<\/li>\n<li><p>The second model gets as payload both the original payload (<code>payload_ser<\/code>) and the output of the previous model (predictions). Possibly, the <code>input_fn<\/code> function would be used to parse the output of model_1 (as in the &quot;standard case&quot;), but I'd need some way to also make the original payload available to model_2.  In this way, model_2 will use both the original payload and the output of model_1 to make the final prediction and return it to whoever invoked the endpoint.<\/p>\n<\/li>\n<\/ol>\n<p>Any idea if this is achievable?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638808672663,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70248817",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.7,
        "Challenge_reading_time":22.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Shared input in Sagemaker inference pipeline models",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":210.0,
        "Challenge_word_count":205,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1508881117760,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":157.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>Sounds like you need an inference DAG. Amazon SageMaker Inference pipelines currently supports only a chain of handlers, where the output of handler N is the input for handler N+1.<\/p>\n<p>You could change model1's predict_fn() to return both (input_object, outputs), and output_fn(). output_fn() will receive these two objects as the predictions, and will handle serializing both as json. model2's input_fn() will need to know how to parse this pair input.<\/p>\n<p>Consider implementing this as a generic pipeline handling mechanism that adds the input to the model's output. This way you could reuse it for all models and pipelines.<\/p>\n<p>You could allow the model to be deployed as a standalone model, and as a part of a pipeline, and apply the relevant input\/output handling behavior that will be triggered by the presence of an environment variable (<code>Environment<\/code> dict), which you can specify when <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model\" rel=\"nofollow noreferrer\">creating<\/a> the inference pipelines model.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":14.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":150.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\nStep Functions can be used to create ML workflows. What is the best practice to version the code creating those workflows? boto3 code in CodeCommit? Something else?\n\nCheers\nOlivier",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1549456954000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668624402176,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUdG1tanW-TXy-vY0YrCsVeg\/how-to-version-step-functions-for-ml",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"how to version step functions for ML?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":555.0,
        "Challenge_word_count":36,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"A Step Functions state machine usually doesn't come alone and typically relies on other resources such as Lambda, EC2, DynamoDB, etc. You might want to package these dependent artifacts\/resources altogether within a version otherwise you might have a state machine that doesn't fully work (eg, state machine version doesn't match Lambda version). I guess the simplest way to achieve this is to provision these resources together as code (eg, CDK or CloudFormation) and store them in a Git repo. You could then use Git tags for versioning.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1601285800631,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":6.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1390832932187,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation_count":249.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was following <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/04_create_pipelines.html\" rel=\"nofollow noreferrer\">pipelines tutorial<\/a>, create all needed files, started the kedro with <code>kedro run --node=preprocessing_data<\/code> but got stuck with such error message:<\/p>\n\n<pre><code>ValueError: Pipeline does not contain nodes named ['preprocessing_data'].\n<\/code><\/pre>\n\n<p>If I run kedro without <code>node<\/code> parameter, I receive<\/p>\n\n<pre><code>kedro.context.context.KedroContextError: Pipeline contains no nodes\n<\/code><\/pre>\n\n<p>Contents of the files:<\/p>\n\n<pre><code>src\/project\/pipelines\/data_engineering\/nodes.py\ndef preprocess_data(data: SparkDataSet) -&gt; None:\n    print(data)\n    return\n<\/code><\/pre>\n\n<pre><code>src\/project\/pipelines\/data_engineering\/pipeline.py\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                func=preprocess_data,\n                inputs=\"data\",\n                outputs=\"preprocessed_data\",\n                name=\"preprocessing_data\",\n            ),\n        ]\n    )\n<\/code><\/pre>\n\n<pre><code>src\/project\/pipeline.py\ndef create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\n    de_pipeline = de.create_pipeline()\n    return {\n        \"de\": de_pipeline,\n        \"__default__\": Pipeline([])\n    }\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582395101757,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1583175186660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60355240",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":17.9,
        "Challenge_reading_time":16.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Pipeline can't find nodes in kedro",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":2155.0,
        "Challenge_word_count":93,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324477592580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1315.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p>I think it looks like you need to have the pipeline in <code>__default__<\/code>.\ne.g.<\/p>\n\n<pre><code>def create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\n    de_pipeline = de.create_pipeline()\n    return {\n        \"de\": data_engineering_pipeline,\n        \"__default__\": data_engineering_pipeline\n    }\n<\/code><\/pre>\n\n<p>Then <code>kedro run --node=preprocessing_data<\/code> works for me.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":4.91,
        "Solution_score_count":8.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1559910246180,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":2046.0,
        "Answerer_view_count":369.0,
        "Challenge_adjusted_solved_time":3949.4255963889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to schedule a data-quality monitoring job in AWS SageMaker by following steps mentioned in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-data-quality.html\" rel=\"nofollow noreferrer\">this AWS documentation page<\/a>. I have enabled data-capture for my endpoint. Then, trained a baseline on my training csv file and statistics and constraints are available in S3 like this:<\/p>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker import image_uris\nfrom sagemaker.model_monitor.dataset_format import DatasetFormat\n\nmy_data_monitor = DefaultModelMonitor(\n    role=get_execution_role(),\n    instance_count=1,\n    instance_type='ml.m5.large',\n    volume_size_in_gb=30,\n    max_runtime_in_seconds=3_600)\n\n# base s3 directory\nbaseline_dir_uri = 's3:\/\/api-trial\/data_quality_no_headers\/'\n# train data, that I have used to generate baseline\nbaseline_data_uri = baseline_dir_uri + 'ch_train_no_target.csv'\n# directory in s3 bucket that I have stored my baseline results to \nbaseline_results_uri = baseline_dir_uri + 'baseline_results_try17\/'\n\n\nmy_data_monitor.suggest_baseline(\n    baseline_dataset=baseline_data_uri,\n    dataset_format=DatasetFormat.csv(header=True),\n    output_s3_uri=baseline_results_uri,\n    wait=True, logs=False, job_name='ch-dq-baseline-try21'\n)\n<\/code><\/pre>\n<p>and data is available in S3:\n<a href=\"https:\/\/i.stack.imgur.com\/rNwZ0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rNwZ0.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Then I tried scheduling a monitoring job by following <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_model_monitor\/model_quality\/model_quality_churn_sdk.ipynb\" rel=\"nofollow noreferrer\">this example notebook for model-quality-monitoring in sagemaker-examples github repo<\/a>, to schedule my data-quality-monitoring job by making necessary modifications with feedback from error messages.<\/p>\n<p>Here's how tried to schedule the data-quality monitoring job from SageMaker Studio:<\/p>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker.model_monitor import EndpointInput\nfrom sagemaker import image_uris\nfrom sagemaker.model_monitor import CronExpressionGenerator\nfrom sagemaker.model_monitor import DefaultModelMonitor\nfrom sagemaker.model_monitor.dataset_format import DatasetFormat\n\n# base s3 directory\nbaseline_dir_uri = 's3:\/\/api-trial\/data_quality_no_headers\/'\n\n# train data, that I have used to generate baseline\nbaseline_data_uri = baseline_dir_uri + 'ch_train_no_target.csv'\n\n# directory in s3 bucket that I have stored my baseline results to \nbaseline_results_uri = baseline_dir_uri + 'baseline_results_try17\/'\n# s3 locations of baseline job outputs\nbaseline_statistics = baseline_results_uri + 'statistics.json'\nbaseline_constraints = baseline_results_uri + 'constraints.json'\n\n# directory in s3 bucket that I would like to store results of monitoring schedules in\nmonitoring_outputs = baseline_dir_uri + 'monitoring_results_try17\/'\n\nch_dq_ep = EndpointInput(endpoint_name=myendpoint_name,\n                         destination=&quot;\/opt\/ml\/processing\/input_data&quot;,\n                         s3_input_mode=&quot;File&quot;,\n                         s3_data_distribution_type=&quot;FullyReplicated&quot;)\n\nmonitor_schedule_name='ch-dq-monitor-schdl-try21'\n\nmy_data_monitor.create_monitoring_schedule(endpoint_input=ch_dq_ep,\n                                           monitor_schedule_name=monitor_schedule_name,\n                                           output_s3_uri=baseline_dir_uri,\n                                           constraints=baseline_constraints,\n                                           statistics=baseline_statistics,\n                                           schedule_cron_expression=CronExpressionGenerator.hourly(),\n                                           enable_cloudwatch_metrics=True)\n<\/code><\/pre>\n<p>after an hour or so, when I check the status of the schedule like this:<\/p>\n<pre><code>import boto3\nboto3_sm_client = boto3.client('sagemaker')\nboto3_sm_client.describe_monitoring_schedule(MonitoringScheduleName='ch-dq-monitor-schdl-try17')\n<\/code><\/pre>\n<p>I get failed status like below:<\/p>\n<pre><code>'MonitoringExecutionStatus': 'Failed',\n  ...\n  'FailureReason': 'Job inputs had no data'},\n<\/code><\/pre>\n<p>Entire Message:<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"true\" data-console=\"false\" data-babel=\"false\">\n<div class=\"snippet-code snippet-currently-hidden\">\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>```\n{'MonitoringScheduleArn': 'arn:aws:sagemaker:ap-south-1:&lt;my-account-id&gt;:monitoring-schedule\/ch-dq-monitor-schdl-try21',\n 'MonitoringScheduleName': 'ch-dq-monitor-schdl-try21',\n 'MonitoringScheduleStatus': 'Scheduled',\n 'MonitoringType': 'DataQuality',\n 'CreationTime': datetime.datetime(2021, 9, 14, 13, 7, 31, 899000, tzinfo=tzlocal()),\n 'LastModifiedTime': datetime.datetime(2021, 9, 14, 14, 1, 13, 247000, tzinfo=tzlocal()),\n 'MonitoringScheduleConfig': {'ScheduleConfig': {'ScheduleExpression': 'cron(0 * ? * * *)'},\n  'MonitoringJobDefinitionName': 'data-quality-job-definition-2021-09-14-13-07-31-483',\n  'MonitoringType': 'DataQuality'},\n 'EndpointName': 'ch-dq-nh-try21',\n 'LastMonitoringExecutionSummary': {'MonitoringScheduleName': 'ch-dq-monitor-schdl-try21',\n  'ScheduledTime': datetime.datetime(2021, 9, 14, 14, 0, tzinfo=tzlocal()),\n  'CreationTime': datetime.datetime(2021, 9, 14, 14, 1, 9, 405000, tzinfo=tzlocal()),\n  'LastModifiedTime': datetime.datetime(2021, 9, 14, 14, 1, 13, 236000, tzinfo=tzlocal()),\n  'MonitoringExecutionStatus': 'Failed',\n  'EndpointName': 'ch-dq-nh-try21',\n  'FailureReason': 'Job inputs had no data'},\n 'ResponseMetadata': {'RequestId': 'dd729244-fde9-44b5-9904-066eea3a49bb',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': 'dd729244-fde9-44b5-9904-066eea3a49bb',\n   'content-type': 'application\/x-amz-json-1.1',\n   'content-length': '835',\n   'date': 'Tue, 14 Sep 2021 14:27:53 GMT'},\n  'RetryAttempts': 0}}\n```<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p>Possible things you might think to have gone wrong at my side or might help me fix my issue:<\/p>\n<ol>\n<li>dataset used for baseline: I have tried to create a baseline with the dataset with and without my target-variable(or dependent variable or y) and the error persisted both times. So, I think the error has originated because of a different reason.<\/li>\n<li>there are no log groups created for these jobs for me to look at and try debug the issue. baseline jobs have log-groups, so i presume there is no problem with roles being used for monitoring-schedule-jobs  not having permissions to create a log group or stream.<\/li>\n<li>role: the role I have attached is defined by <code>get_execution_role()<\/code>, which points to a role with full access to sagemaker, cloudwatch, S3 and some other services.<\/li>\n<li>the data collected from my endpoint during my inference: here's how a line of data of .jsonl file saved to S3, which contains data collected during inference, looks like:<\/li>\n<\/ol>\n<pre><code>{&quot;captureData&quot;:{&quot;endpointInput&quot;:{&quot;observedContentType&quot;:&quot;application\/json&quot;,&quot;mode&quot;:&quot;INPUT&quot;,&quot;data&quot;:&quot;{\\&quot;longitude\\&quot;: [-122.32, -117.58], \\&quot;latitude\\&quot;: [37.55, 33.6], \\&quot;housing_median_age\\&quot;: [50.0, 5.0], \\&quot;total_rooms\\&quot;: [2501.0, 5348.0], \\&quot;total_bedrooms\\&quot;: [433.0, 659.0], \\&quot;population\\&quot;: [1050.0, 1862.0], \\&quot;households\\&quot;: [410.0, 555.0], \\&quot;median_income\\&quot;: [4.6406, 11.0567]}&quot;,&quot;encoding&quot;:&quot;JSON&quot;},&quot;endpointOutput&quot;:{&quot;observedContentType&quot;:&quot;text\/html; charset=utf-8&quot;,&quot;mode&quot;:&quot;OUTPUT&quot;,&quot;data&quot;:&quot;eyJtZWRpYW5faG91c2VfdmFsdWUiOiBbNDUyOTU3LjY5LCA0NjcyMTQuNF19&quot;,&quot;encoding&quot;:&quot;BASE64&quot;}},&quot;eventMetadata&quot;:{&quot;eventId&quot;:&quot;9804d438-eb4c-4cb4-8f1b-d0c832b641aa&quot;,&quot;inferenceId&quot;:&quot;ef07163d-ea2d-4730-92f3-d755bc04ae0d&quot;,&quot;inferenceTime&quot;:&quot;2021-09-14T13:59:03Z&quot;},&quot;eventVersion&quot;:&quot;0&quot;}\n<\/code><\/pre>\n<p>I would like to know what has gone wrong in this entire process, that led to data not being fed to my monitoring job.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":4,
        "Challenge_created_time":1631630733197,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1631632366336,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69179914",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":19.4,
        "Challenge_reading_time":107.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":null,
        "Challenge_title":"How to fix SageMaker data-quality monitoring-schedule job that fails with 'FailureReason': 'Job inputs had no data'",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":565.0,
        "Challenge_word_count":650,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559910246180,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":2046.0,
        "Poster_view_count":369.0,
        "Solution_body":"<p>This happens, during the ground-truth-merge job, when the spark can't find any data either in '\/opt\/ml\/processing\/groundtruth\/' or '\/opt\/ml\/processing\/input_data\/' directories. And that can happen when either you haven't sent any requests to the sagemaker endpoint or there are no ground truths.<\/p>\n<p>I got this error because, the folder <code>\/opt\/ml\/processing\/input_data\/<\/code> of the docker volume mapped to the monitoring container had no data to process. And that happened because, the thing that facilitates entire process, including fetching data couldn't find any in S3. and that happened because, there was an extra slash(<code>\/<\/code>) in the directory to which endpoint's captured-data will be saved. to elaborate, while creating the endpoint, I had mentioned the directory as <code>s3:\/\/&lt;bucket-name&gt;\/&lt;folder-1&gt;\/<\/code>, while it should have just been <code>s3:\/\/&lt;bucket-name&gt;\/&lt;folder-1&gt;<\/code>. so, while the thing that copies data from S3 to docker volume tried to fetch data of that hour, the directory it tried to extract the data from was <code>s3:\/\/&lt;bucket-name&gt;\/&lt;folder-1&gt;\/\/&lt;endpoint-name&gt;\/&lt;variant-name&gt;\/&lt;year&gt;\/&lt;month&gt;\/&lt;date&gt;\/&lt;hour&gt;<\/code>(notice the two slashes). So, when I created the endpoint-configuration again with the slash removed in S3 directory, this error wasn't present and ground-truth-merge operation was successful as part of model-quality-monitoring.<\/p>\n<p><em>I am answering this question because, someone read the question and upvoted it. meaning, someone else has faced this problem too. so, I have mentioned what worked for me. And I wrote this, so that StackExchange doesn't think I am spamming the forum with questions.<\/em><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1645850298483,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":22.46,
        "Solution_score_count":4.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":224.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1467237684900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":613.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to use my own sickit-learn ML model with SageMaker using the github example.<\/p>\n\n<p>The python code is below : <\/p>\n\n<pre><code># Define IAM role import boto3 \nimport re \nimport os \nimport numpy as np \nimport pandas as pd \nfrom sagemaker import get_execution_role \nimport sagemaker as sage from time \nimport gmtime, strftime \nrole = get_execution_role()\n\ness =  sage.Session()\naccount = sess.boto_session.client('sts').get_caller_identity()['Account']\nregion = sess.boto_session.region_name\nimage = '{}.dkr.ecr.{}.amazonaws.com\/decision-trees-sample:latest'.format(account, region)\n\n\noutput_path=\"s3:\/\/output\"\n\nsess\n\ntree = sage.estimator.Estimator(image,\n                      role, 1, 'ml.c4.2xlarge',\n                     output_path='s3-eu-west-1.amazonaws.com\/output',\n                    sagemaker_session=sess)\n\ntree.fit(\"s3:\/\/output\/iris.csv\")\n<\/code><\/pre>\n\n<p>But I get this error : <\/p>\n\n<blockquote>\n  <p>INFO:sagemaker:Creating training-job with name:\n  decision-trees-sample-2018-04-24-13-13-38-281<\/p>\n  \n  <p>--------------------------------------------------------------------------- ClientError                               Traceback (most recent call\n  last)  in ()\n       14                     sagemaker_session=sess)\n       15 \n  ---> 16 tree.fit(\"s3:\/\/inteldatastore-cyrine\/iris.csv\")<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py\n  in fit(self, inputs, wait, logs, job_name)\n      161             self.output_path = 's3:\/\/{}\/'.format(self.sagemaker_session.default_bucket())\n      162 \n  --> 163         self.latest_training_job = _TrainingJob.start_new(self, inputs)\n      164         if wait:\n      165             self.latest_training_job.wait(logs=logs)<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py\n  in start_new(cls, estimator, inputs)\n      336                                           input_config=input_config, role=role,\n  job_name=estimator._current_job_name,\n      337                                           output_config=output_config, resource_config=resource_config,\n  --> 338                                           hyperparameters=hyperparameters, stop_condition=stop_condition)\n      339 \n      340         return cls(estimator.sagemaker_session, estimator._current_job_name)<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py\n  in train(self, image, input_mode, input_config, role, job_name,\n  output_config, resource_config, hyperparameters, stop_condition)\n      242         LOGGER.info('Creating training-job with name: {}'.format(job_name))\n      243         LOGGER.debug('train request: {}'.format(json.dumps(train_request, indent=4)))\n  --> 244         self.sagemaker_client.create_training_job(**train_request)\n      245 \n      246     def create_model(self, name, role, primary_container):<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py\n  in _api_call(self, *args, **kwargs)\n      312                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n      313             # The \"self\" in this scope is referring to the BaseClient.\n  --> 314             return self._make_api_call(operation_name, kwargs)\n      315 \n      316         _api_call.<strong>name<\/strong> = str(py_operation_name)<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py\n  in _make_api_call(self, operation_name, api_params)\n      610             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n      611             error_class = self.exceptions.from_code(error_code)\n  --> 612             raise error_class(parsed_response, operation_name)\n      613         else:\n      614             return parsed_response<\/p>\n  \n  <p>ClientError: An error occurred (AccessDeniedException) when calling\n  the CreateTrainingJob operation: User:\n  arn:aws:sts::307504647302:assumed-role\/default\/SageMaker is\n  not authorized to perform: sagemaker:CreateTrainingJob on resource:\n  arn:aws:sagemaker:eu-west-1:307504647302:training-job\/decision-trees-sample-2018-04-24-13-13-38-281<\/p>\n<\/blockquote>\n\n<p>Can you help me to resolve the problem?<\/p>\n\n<p>Thank you<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1524576827563,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50003050",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":19.9,
        "Challenge_reading_time":48.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":null,
        "Challenge_title":"error to run training job with aws Sagemaker",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":4369.0,
        "Challenge_word_count":263,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518617852856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":495.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>Looks like you don't have access to the resource <\/p>\n\n<pre><code>arn:aws:sagemaker:eu-west-1:307504647302:training-job\/decision-trees-sample-2018-04-24-13-13-38-281\n<\/code><\/pre>\n\n<p>Can you check if the resource url is correct and the proper permissions are set in the security group. <\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.0,
        "Solution_reading_time":3.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":32.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"based on sagemaker python sdk  , i can add custom\/customer metadata via (sample code below). i am planning to use sagemaker model step and register the model in a model package group and create an end to end pipeline. is it possible to add these metadata at the time of model creation or registering model ?\n\n```\nboto3.client('sagemaker').update_model_package( ... \nCustomerMetadataProperties = {'key1': 'value1', 'key2': 'value2'})\n```",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682544198301,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1682891672336,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU26CweYmuQP2BNVjdC8zTUg\/how-to-add-custom-metadata-to-a-model-in-model-registry-via-sagemaker-pipeline-step",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":6.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to add custom metadata to a model in model registry via sagemaker pipeline step?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":74,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes, it is possible. You can add custom metadata when you use the [register](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.Model.register) method of the Model object in the SageMaker SDK. The [Model step](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html#step-type-model) from SageMaker Pipelines can then be used together with this register method. \n\nBelow is a snippet of code copied from the example provided on [this page](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html#step-type-model), where I have added the `custom_metadata_properties` parameter with your example input. \n \n```\nregister_model_step_args = pipeline_model.register(\n    content_types=[\"application\/json\"],\n   response_types=[\"application\/json\"],\n   inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n   transform_instances=[\"ml.m5.xlarge\"],\n   model_package_group_name=\"sipgroup\",\n   customer_metadata_properties={\"key1\": \"value1\", \"key2\": \"value2\"}\n)\n\nstep_model_registration = ModelStep(\n   name=\"AbaloneRegisterModel\",\n   step_args=register_model_step_args,\n)\n```",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1682634634328,
        "Solution_link_count":3.0,
        "Solution_readability":23.2,
        "Solution_reading_time":14.97,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nCurrently the example DAG for sagemaker just uses access key and secret key. We need to use a temporary  access token\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. iOS]\r\n - Browser [e.g. chrome, safari]\r\n - Version [e.g. 22]\r\n\r\n**Smartphone (please complete the following information):**\r\n - Device: [e.g. iPhone6]\r\n - OS: [e.g. iOS8.1]\r\n - Browser [e.g. stock browser, safari]\r\n - Version [e.g. 22]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1666960.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666950742000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/738",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":6.6,
        "Challenge_reading_time":10.27,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":22.0,
        "Challenge_repo_issue_count":807.0,
        "Challenge_repo_star_count":97.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker example DAG to use aws session token",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":120,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### System Info\r\n\r\n- `transformers` version: 4.21.0.dev0\r\n- Platform: Linux-5.8.0-43-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.7.0\r\n- PyTorch version (GPU?): 1.11.0+cu113 (True)\r\n- Tensorflow version (GPU?): 2.9.1 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): 0.5.0 (cpu)\r\n- Jax version: 0.3.6\r\n- JaxLib version: 0.3.5\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n\r\n\r\n### Who can help?\r\n\r\n@sgugger \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1.enable sigopt HPO in example and run.\r\n2. work log like\"UserWarning: You're currently using the old SigOpt Experience. Try out the new and improved SigOpt experience by getting started with the docs today. You have until July 2022 to migrate over without experiencing breaking changes.\"\r\n\r\n### Expected behavior\r\n\r\nHPO with sigopt backend could work correctly without warning",
        "Challenge_closed_time":1658150.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657875526000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18145",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":6.1,
        "Challenge_reading_time":14.47,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17206.0,
        "Challenge_repo_issue_count":20680.0,
        "Challenge_repo_star_count":76088.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"the Sigopt api is outdated in transformers trainer.py, the old api could not work",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":153,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"SigOpt"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"When I register a dataset in the catalog.yml\r\n\r\n```yaml\r\nmy_dataset:\r\n  type : kedro_mlflow.io.MlflowDataSet \r\n  data_set : \r\n    type: pickle.PickleDataSet\r\n    filepath: data\/02_intermediate\/my_dataset.pkl\r\n```\r\n\r\nand I run `kedro run` I got a `expected string or bytes-like object` when **the local path is linux AND the `mlflow_tracking_uri` is an Azure blob storage (it works locally)**. I don't know really why this append, but it can be fied by replacing `self._filepath` by `self._filepath.as_posix()` in these 2 locations: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/kedro_mlflow\/io\/mlflow_dataset.py#L51\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/kedro_mlflow\/io\/mlflow_dataset.py#L55\r\n\r\n@kaemo @akruszewski did you experience some issues with S3 too?\r\n\r\n**EDIT**: @akruszewski it is [the very same issue you encountered here](https:\/\/github.com\/akruszewski\/kedro-mlflow\/commit\/41e9e3fdd2c54a774cca69e1cb52e26cadf50b1e)",
        "Challenge_closed_time":1602278.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601476316000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/74",
        "Challenge_link_count":3,
        "Challenge_participation_count":0,
        "Challenge_readability":10.6,
        "Challenge_reading_time":14.69,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"MlflowDataSet fails to log on remote storage when underlying dataset filepath is converted as a PurePosixPath",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":106,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have 3 main process to perform using Amazon SageMaker.<\/p>\n\n<ol>\n<li>Using own training python script, (not using sagemaker container, inbuilt algorithm) [Train.py]<\/li>\n<\/ol>\n\n<p>-> For this, I have referred to this link:<br>\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/train-and-host-scikit-learn-models-in-amazon-sagemaker-by-building-a-scikit-docker-container\/\" rel=\"nofollow noreferrer\">Bring own algorithm to AWS sagemaker<\/a>\nand it seems that we can bring our own training script to sagemaker managed training setup, and model artifacts can be uploaded to s3 etc.\nNote: I am using Light GBM model for training.<\/p>\n\n<ol start=\"2\">\n<li>Writing forecast to AWS RDS DB:<\/li>\n<\/ol>\n\n<p>-> There is no need to deploy model and create endpoint, because training will happen everyday, and will create forecast as soon as training completes. (Need to generate forecast in train.py itself)<\/p>\n\n<p>-> <strong>Challenge is how can I write forecast in AWS RDS DB from train.py script. (Given that script is running in Private VPC)<\/strong><\/p>\n\n<ol start=\"3\">\n<li>Scheduling this process as daily job:<\/li>\n<\/ol>\n\n<p>--> I have gone through AWS step functions and seems to be the way to trigger daily training and write forecast to RDS.<\/p>\n\n<p>--> <strong>Challenge is how to use step function for time based trigger and not event based.<\/strong><\/p>\n\n<p>Any suggestions on how to do this? Any best practices to follow? Thank you in advance.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566547602673,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1566550066240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57622122",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":19.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Custom sagemaker container for training, write forecast to AWS RDS, on a daily basis",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":466.0,
        "Challenge_word_count":217,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1469183608840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gurugram, Haryana, India",
        "Poster_reputation_count":624.0,
        "Poster_view_count":67.0,
        "Solution_body":"<p>The way to trigger Step Functions on schedule is by using CloudWatch Events (sort of cron). Check out this tutorial: <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/tutorial-cloudwatch-events-target.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/tutorial-cloudwatch-events-target.html<\/a><\/p>\n\n<p>Don't write to the RDS from your Python code! It is better to write the output to S3 and then \"copy\" the files from S3 into the RDS. Decoupling these batches will make a more reliable and scalable process. You can trigger the bulk copy into the RDS when the files are written to S3 or to a later time when your DB is not too busy. <\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.6,
        "Solution_reading_time":8.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":92.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,    <\/p>\n<p>Is there any way to access the X_tain, y_train, X_test, y_test data that was used in azure automl pipeline step during training?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1671429879837,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1133620\/how-to-access-the-data-used-during-the-azure-autom",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.8,
        "Challenge_reading_time":2.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"How to access the data used during the azure automl pipeline training?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":35,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>You can access the data that was used during the training of an Azure AutoML model by using the TrainingData property of the Model object in the Azure Machine Learning SDK.    <\/p>\n<p>Here's an example of how you can do this:    <\/p>\n<pre><code>from azureml.core import Workspace, Dataset, Experiment, Model  \n  \n# Load the workspace  \nws = Workspace.from_config()  \n  \n# Get the experiment that contains the model  \nexperiment = Experiment(workspace=ws, name='my-experiment')  \n  \n# Get the model  \nmodel = Model(workspace=ws, name='my-model', version='1')  \n  \n# Get the training data  \ntraining_data = model.training_data  \n  \n# Access the X_train and y_train data  \nX_train = training_data.split['train']['X']  \ny_train = training_data.split['train']['y']  \n  \n# Access the X_test and y_test data  \nX_test = training_data.split['test']['X']  \ny_test = training_data.split['test']['y']  \n<\/code><\/pre>\n<p>Note that the training_data object is a TabularDataset object, which represents a dataset in tabular format (i.e., rows and columns). The split property of this object is a dictionary that contains the training and test data splits. The keys of this dictionary are 'train' and 'test', and the values are dictionaries containing the 'X' and 'y' data for each split.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":15.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":159.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":6.6429136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/schedule-cloud-scheduler\" rel=\"nofollow noreferrer\">this<\/a> tutorial to create my first scheduled Vertex AI Pipeline to run every minute. The only thing it does is prints <code>&quot;Hello, &lt;any-greet-string&gt;&quot;<\/code> and also returns this same string. I can see that it is running because the last run time updates and the last run result is &quot;Success&quot; every time.<\/p>\n<p>My question is very simple: Where can I see this string printed and the output of my pipeline?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1639659363547,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1639726542407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70379395",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.6,
        "Challenge_reading_time":7.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Vertex AI - Viewing Pipeline Output",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":346.0,
        "Challenge_word_count":79,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1407245826703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":745.0,
        "Poster_view_count":168.0,
        "Solution_body":"<p>The output of the <code>print()<\/code> statements in the pipeline can be found in &quot;Cloud Logging&quot; with the appropriate filters. To check logs for each component in the pipeline, click on the respective component in the console and click &quot;<strong>VIEW LOGS<\/strong>&quot; in the right pane. A new pane with the logs will open in the pipeline page which will allow us to see the output of the component. Refer to the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I ran a sample <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#4\" rel=\"nofollow noreferrer\">pipeline<\/a> from this codelab, <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#0\" rel=\"nofollow noreferrer\">Intro to Vertex Pipelines<\/a> and below is the output for one of the <code>print()<\/code> statements in the pipeline.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<br \/>\n<h4>UPDATE:<\/h4>\n<p>Every component in a pipeline run is deployed as an individual Vertex AI custom job. Corresponding to the sample pipeline consisting 3 components, there are 3 entries in the &quot;CUSTOM JOBS&quot; section as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Therefore, to view the logs on the run level, we would need to query the log entries with the respective <code>job_id<\/code>s of the pipeline components and the <code>job_id<\/code> of the Cloud Scheduler job. The query would look like this.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.labels.job_id=(&quot;JOB_ID_1&quot; OR &quot;JOB_ID_2&quot; [OR &quot;JOB_ID_N&quot;...])\nseverity&gt;=DEFAULT\n<\/code><\/pre>\n<p>If there are no simultaneous pipeline runs, a simpler query like below can be used.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.type=(&quot;cloud_scheduler_job&quot; OR &quot;ml_job&quot;)\nseverity&gt;=DEFAULT\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1639750456896,
        "Solution_link_count":8.0,
        "Solution_readability":11.1,
        "Solution_reading_time":29.64,
        "Solution_score_count":2.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":241.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1555488556820,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Baillargues, France",
        "Answerer_reputation_count":56447.0,
        "Answerer_view_count":9158.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a <code>Vertex AI custom training job<\/code> (machine learnin training using custom container) on <code>GCP<\/code>. I would like to create a <code>Pub\/Sub<\/code> message when the job failed so I can post a message on some chat like Slack. Logfile (<code>Cloud Logging)<\/code> is looking like that:<\/p>\n<pre><code>{\ninsertId: &quot;xxxxx&quot;\nlabels: {\nml.googleapis.com\/endpoint: &quot;&quot;\nml.googleapis.com\/job_state: &quot;FAILED&quot;\n}\nlogName: &quot;projects\/xxx\/logs\/ml.googleapis.com%2F1113875647681265664&quot;\nreceiveTimestamp: &quot;2021-07-09T15:05:52.702295640Z&quot;\nresource: {\nlabels: {\njob_id: &quot;1113875647681265664&quot;\nproject_id: &quot;xxx&quot;\ntask_name: &quot;service&quot;\n}\ntype: &quot;ml_job&quot;\n}\nseverity: &quot;INFO&quot;\ntextPayload: &quot;Job failed.&quot;\ntimestamp: &quot;2021-07-09T15:05:52.187968162Z&quot;\n}\n<\/code><\/pre>\n<p>I am creating a Logs Router Sink with the following query:<\/p>\n<pre><code>resource.type=&quot;ml_job&quot; AND textPayload:&quot;Job failed&quot; AND labels.&quot;ml.googleapis.com\/job_state&quot;:&quot;FAILED&quot;\n<\/code><\/pre>\n<p>The issue I am facing is that Vertex AI will retry the job 3 times before declaring the job as a failure but in the logfile the message is identical. Below you have 3 examples, only the last one that failed 3 times really failed at the end.\n<a href=\"https:\/\/i.stack.imgur.com\/NSGPb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NSGPb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>In the logfile, I don't have any count id for example. Any idea how to solve this ? Creating a BigQuery table to keep track of the number of failure per <code>resource.labels.job_id<\/code> seems to be an overkill if I need to do that in all my project. Is there a way to do a group by <code>resource.labels.job_id<\/code> and count within Logs Router Sink ?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1627312838200,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1627313437400,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68532457",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":25.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"How to create a Logs Router Sink when a Vertex AI training job failed (after 3 attempts)?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":216.0,
        "Challenge_word_count":232,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1465222092252,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Z\u00fcrich, Switzerland",
        "Poster_reputation_count":1414.0,
        "Poster_view_count":478.0,
        "Solution_body":"<p>The log sink is quite simple: provide a filter, it will publish in a PubSub topic each entry which match this filter. No group by, no count, nothing!!<\/p>\n<p>I propose you to use a combination of log-based metrics and Cloud monitoring.<\/p>\n<ol>\n<li>Firstly, create a <a href=\"https:\/\/cloud.google.com\/logging\/docs\/logs-based-metrics\" rel=\"nofollow noreferrer\">log based metrics<\/a> on your job failed log entry<\/li>\n<li>Create an <a href=\"https:\/\/cloud.google.com\/logging\/docs\/logs-based-metrics\/charts-and-alerts\" rel=\"nofollow noreferrer\">alert on this log based metrics<\/a> with the following key values<\/li>\n<\/ol>\n<ul>\n<li>Set the group by that you want, for example, the jobID (i don't know what is the relevant value for VertexAI job)<\/li>\n<li>Set an alert when the threshold is equal or above 3<\/li>\n<li>Add a notification channel and set a PubSub notification (still in beta)<\/li>\n<\/ul>\n<p>With this configuration, the alert will be posted only once in PubSub when 3 occurrences of the same jobID will occur.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.7,
        "Solution_reading_time":12.96,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":142.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1513106638900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":276.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":1546.0056783333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am building a time series usecase to automate the preprocess and retrain tasks.At first the data is preprocessed using numpy, pandas, statsmodels etc &amp; later a machine learning algorithm is applied to make predictions.\nThe reason for using inference pipeline is that it reuses the same preprocess code for training and inference. I have checked the examples given by AWS sagemaker team with spark and sci-kit learn. In both the examples they use a sci-kit learn container to fit &amp; transform their preprocess code. Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code? <\/p>\n\n<p>Can someone give me a custom example of using these pipelines? Any help is appreciated!<\/p>\n\n<p><strong>Sources looked into:<\/strong><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline<\/a>\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia<\/a><\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574408498230,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58989610",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":19.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"How to custom code an inference pipeline in AWS sagemaker?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1186.0,
        "Challenge_word_count":145,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553712330910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":103.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Apologies for the late response.<\/p>\n\n<p>Below is some documentation on inference pipelines:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html<\/a>\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-real-time.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-real-time.html<\/a><\/p>\n\n<blockquote>\n  <p>Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code?<\/p>\n<\/blockquote>\n\n<p>Your container is an encapsulation of the environment needed for your custom code needed to run properly. Based on the requirements listed above, <code>numpy, pandas, statsmodels etc &amp; later a machine learning algorithm<\/code>, I would create a container if you wish to isolate your dependencies or modify an existing predefined SageMaker container, such as the scikit-learn one, and add your dependencies into that.<\/p>\n\n<blockquote>\n  <p>Can someone give me a custom example of using these pipelines? Any help is appreciated!<\/p>\n<\/blockquote>\n\n<p>Unfortunately, the two example notebooks referenced above are the only examples utilizing inference pipelines. The biggest hurdle most likely is creating containers that fulfill the preprocessing and prediction task you are seeking and then combining those two together into the inference pipeline.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1579974118672,
        "Solution_link_count":4.0,
        "Solution_readability":15.8,
        "Solution_reading_time":19.52,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":168.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to write a script which can download the outputs from an Azure ML experiment Run after the fact.<\/p>\n<p>Essentially, I want to know how I can get a Run by its <code>runId<\/code> property (or some other identifier).<\/p>\n<p>I am aware that I have access to the Run object when I create it for the purposes of training. What I want is a way to recreate this Run object later in a separate script, possibly from a completely different environment.<\/p>\n<p>What I've found so far is a way to get a list of ScriptRun objects from an experiment via the <code>get_runs()<\/code> function. But I don't see a way to use one of these ScriptRun objects to create a Run object representing the original Run and allowing me to download the outputs.<\/p>\n<p>Any help appreciated.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1612316926667,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66020144",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":10.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"How can one download the outputs of historical Azure ML experiment Runs via the python API",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1402.0,
        "Challenge_word_count":150,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1612316384927,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>I agree that this could probably be better documented, but fortunately, it's a simple implementation.<\/p>\n<p>this is how you get a run object for an already submitted run for <code>azureml-sdk&gt;=1.16.0<\/code> (for the older approach <a href=\"https:\/\/stackoverflow.com\/questions\/62949488\/amls-experiment-run-stuck-in-status-running\/62958369#62958369\">see my answer here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace\n\nws = Workspace.from_config()\nrun = ws.get_run('YOUR_RUN_ID')\n<\/code><\/pre>\n<p>once you have the <code>run<\/code> object, you can call methods like<\/p>\n<ul>\n<li><code>.get_file_names()<\/code> to see what files are available (the logs in <code>azureml-logs\/<\/code> and <code>logs\/azureml\/<\/code> will also be listed)<\/li>\n<li><code>.download_file()<\/code> to download an individual file<\/li>\n<li><code>.download_files()<\/code> to download all files that match a given prefix (or all the files)<\/li>\n<\/ul>\n<p>See the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py&amp;WT.mc_id=AI-MVP-5003930\" rel=\"noreferrer\">Run object docs<\/a> for more details.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.9,
        "Solution_reading_time":15.62,
        "Solution_score_count":7.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":110.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I start a training job from a sagemaker notebook using boto3.client():\n\nclient = boto3.client(service_name='sagemaker')\nclient.create_training_job(**training_params)\n\nI see the training job in progress on the console. Is it then an error to stop and delete the notebook even if the model has not finished training?",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683739235526,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1684085641956,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUT7JnzaUJRs-s13xiawMEFA\/keep-sagemaker-notebook-up-during-model-training",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":4.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Keep sagemaker notebook up during model training?",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":46.0,
        "Challenge_word_count":49,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @fascani, no, once the training job has been created, you don't need to keep the notebook running. You can continue other explorations\/model building on the notebook, check the progress of the training job using APIs, but that's not required.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1683838814256,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":3.03,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1288806614312,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dallas, TX, United States",
        "Answerer_reputation_count":14103.0,
        "Answerer_view_count":2018.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to execute a R script every time an azure function is triggered. The R script executes perfectly on Azure machine learning Studio. But I am failing to execute through azure function.\nIs there any way to execute it?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1612522682103,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66062015",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.6,
        "Challenge_reading_time":3.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Executing R script from Azure function",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":640.0,
        "Challenge_word_count":45,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1475153705707,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":79.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>AFAIK you'll have to create your own Runtime as <code>R<\/code> isn't supported natively.<\/p>\n<p>Have you already tried <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/functions-create-function-linux-custom-image?tabs=bash%2Cportal&amp;pivots=programming-language-other\" rel=\"nofollow noreferrer\">&quot;Create a function on Linux using a custom container&quot;<\/a>? Interestingly they have given <code>R<\/code> as the example of custom runtime, so hopefully that answers your question.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.2,
        "Solution_reading_time":6.79,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,    <\/p>\n<p>Is there a way to specify the disk storage type for Compute instances?     <br \/>\nBoth the Azure portal and ARM templates do not have an option to define the disk storage type, which defaults to the P10 disks (Premium SSD).     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/148883-azureml-compute.png?platform=QnA\" alt=\"148883-azureml-compute.png\" \/>Thanks    <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1636713894743,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/625035\/azure-machine-learning-specify-disk-storage-type",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":5.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning - Specify disk storage type",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":54,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=41b4924d-c8f5-4ca4-9844-0c0af46eb5d5\">@Simon Magrin  <\/a>  Thanks, Currently There's no way to change the disk storage type for CIs or compute clusters. We have added this to our product backlog item to support in the near future.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":13.8,
        "Solution_reading_time":17.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":130.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hey,\n\ni'm trying to understand the MLOps Pipeline with the CI\/CD-Automation (Stage 2 Maturity Level) and struggle with the Feature Store as the component feeding the Automated Pipeline with data. What i found out in the internet was, that Feature Stores extract data from different sources, transform them and create training data which can be used to train the model (retraining with new data). But in the pipeline the steps like Data preperation and Data extraction come after the Feature Store.\n\nCan somebody explain to me, whats the output of the Feature Store and how it is used to serve the data for the Automated Pipeline and the Prediction Service?\n\nThanks in advance",
        "Challenge_closed_time":1678111.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677898620000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Feature-Store-MLOps-Pipeline\/m-p\/528764#M1375",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":8.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Feature Store MLOps Pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":104.0,
        "Challenge_word_count":116,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The Feature Store is just a centralized repository of features. By that, its output is just a set of features typically used to train an ML model. Depending on your specific needs, you can serve the ingested data in the Feature Store to the model right away (in what is called feature serving) or export feature values and do further preparation of data.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.7,
        "Solution_reading_time":4.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":68.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1313441158323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":319.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like run SparkJarProcessor within Sagemaker Pipeline.  After I create an instance of SparkJarProcessor, when I just <code>run<\/code> the processor, I can specify the jar and the class you I want to execute with the <code>submit_app<\/code> and <code>submit_class<\/code> parameters to the <code>run<\/code> method. e.g.,<\/p>\n<pre><code>processor.run(\n    submit_app=&quot;my.jar&quot;,\n    submit_class=&quot;program.to.run&quot;,\n    arguments=['--my_arg', &quot;my_arg&quot;],\n    configuration=my_config,\n    spark_event_logs_s3_uri=log_path\n)\n<\/code><\/pre>\n<p>If I want to run it as a step in the pipeline, what arguments can I give to ProcessingStep? According to <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.spark.processing.SparkJarProcessor.get_run_args\" rel=\"nofollow noreferrer\">this documentation<\/a>, you can call get_run_args on the processor to &quot;<em>get the normalized inputs, outputs and arguments needed when using a SparkJarProcessor in a ProcessingStep<\/em>&quot;, but when I run it like this,<\/p>\n<pre><code>processor.get_run_args(\n    submit_app=&quot;my.jar&quot;, \n    submit_class=&quot;program.to.run&quot;,\n    arguments=['--my_arg', &quot;my_arg&quot;],\n    configuration=my_config,\n    spark_event_logs_s3_uri=log_path\n)\n<\/code><\/pre>\n<p>My output looks like this:<\/p>\n<pre><code>RunArgs(code='my.jar', inputs=[&lt;sagemaker.processing.ProcessingInput object at 0x7fc53284a090&gt;], outputs=[&lt;sagemaker.processing.ProcessingOutput object at 0x7fc532845ed0&gt;], arguments=['--my_arg', 'my_arg'])\n<\/code><\/pre>\n<p>&quot;program.to.run&quot; is not part of the output.  So, assuming <code>code<\/code> is to specify the jar, what's the normalized version of <code>submit_class<\/code>?<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645838816427,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1646242607940,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71273364",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.2,
        "Challenge_reading_time":23.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"SparkJarProcessor in Sagemaker Pipeline",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":197.0,
        "Challenge_word_count":158,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1336278258636,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Roseville, MN",
        "Poster_reputation_count":543.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>When <code>get_run_args<\/code> or <code>run<\/code> is called on a SparkJarProcessor, the <code>submit_class<\/code> <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/dev\/src\/sagemaker\/spark\/processing.py#L1143\" rel=\"nofollow noreferrer\">is used to set a property on the processor itself<\/a> which is why you don't see it in the <code>get_run_args<\/code> output.<\/p>\n<p>That processor property will be used during pipeline definition generation to set the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_AppSpecification.html#sagemaker-Type-AppSpecification-ContainerEntrypoint\" rel=\"nofollow noreferrer\">ContainerEntrypoint<\/a> argument to <code>CreateProcessingJob<\/code>.<\/p>\n<p>Example:<\/p>\n<pre><code>run_args = spark_processor.get_run_args(\n    submit_app=&quot;my.jar&quot;,\n    submit_class=&quot;program.to.run&quot;,\n    arguments=[]\n)\n\nstep_process = ProcessingStep(\n    name=&quot;SparkJarProcessStep&quot;,\n    processor=spark_processor,\n    inputs=run_args.inputs,\n    outputs=run_args.outputs,\n    code=run_args.code\n)\n\npipeline = Pipeline(\n    name=&quot;myPipeline&quot;,\n    parameters=[],\n    steps=[step_process],\n)\n\ndefinition = json.loads(pipeline.definition())\ndefinition\n<\/code><\/pre>\n<p>The output of <code>definition<\/code>:<\/p>\n<pre><code>...\n'Steps': [{'Name': 'SparkJarProcessStep',\n   'Type': 'Processing',\n   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n      'InstanceCount': 2,\n      'VolumeSizeInGB': 30}},\n    'AppSpecification': {'ImageUri': '153931337802.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-spark-processing:2.4-cpu',\n     'ContainerEntrypoint': ['smspark-submit',\n      '--class',\n      'program.to.run',\n      '--local-spark-event-logs-dir',\n      '\/opt\/ml\/processing\/spark-events\/',\n      '\/opt\/ml\/processing\/input\/code\/my.jar']},\n...\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":23.3,
        "Solution_reading_time":24.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":107.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\nInstalling `kedro-datasets[option]` installs a different set of dependencies than `kedro[option]`. It appears that `kedro-datasets[option]` is installing the superset of requirements for all datasets.\n\n## Context\nThis is currently blocking https:\/\/github.com\/kedro-org\/kedro\/issues\/1495\n\n## Steps to Reproduce\n1. `pip install \"kedro[pandas.CSVDataSet]\"; pip freeze > requirements-kedro.txt`\n2. `pip install \"kedro-datasets[pandas.CSVDataSet]\"; pip freeze > requirements-kedro-datasets.txt`\n3. Compare the requirements\n",
        "Challenge_closed_time":1667929.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666866011000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/64",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":12.7,
        "Challenge_reading_time":8.27,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":13.0,
        "Challenge_repo_issue_count":97.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"pip installing kedro-datasets[option] causes different dependencies to installing kedro[option]",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Issue_self_closed":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi there.<\/p>\n<p>We currently use wandb artifacts for model versioning during experiments. We\u2019d also like to integrate this into our production pipeline so that we can automatically pull specific model versions during builds.<\/p>\n<p>I am wondering if it\u2019s possible to get credentials that are not tied to a specific wandb user so that they don\u2019t expire if the team member that implements this happens to leave our company.<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667918249037,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/ci-credentials-not-tied-to-user\/3388",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.5,
        "Challenge_reading_time":5.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"CI Credentials Not Tied to User",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":243.0,
        "Challenge_word_count":74,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/willjstone\">@willjstone<\/a> thank you for writing in! You can do this using a <code>service account<\/code>, the steps to add this account type to your team are explained in our documentation <a href=\"https:\/\/docs.wandb.ai\/guides\/technical-faq\/general#what-is-a-service-account-and-why-is-it-useful\">here<\/a>. Would this work for your use case?<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.4,
        "Solution_reading_time":5.01,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":30.6442191667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>On the AWS developer docs for Sagemaker, they recommend us to use PIPE mode to directly stream large datasets from S3 to the model training containers (since it's faster, uses less disk storage, reduces training time, etc.).<\/p>\n\n<p>However, they don't include information on whether this data streaming transfer is charged for (they only include data transfer pricing for their model building &amp; deployment stages, not training).<\/p>\n\n<p>So, I wanted to ask if anyone knew whether this data transfer in PIPE mode is charged for, since if it is, I don't get how this would be recommended for large datasets, since streaming a few epochs for each model iteration can get prohibitively expensive for large datasets (my dataset, for example, is 6.3TB on S3).<\/p>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1564927931767,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57347278",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.9,
        "Challenge_reading_time":10.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Does AWS Sagemaker charge for S3 streamed data in PIPE mode (for model training)?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":828.0,
        "Challenge_word_count":138,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1401475981032,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":440.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>You are charged for the S3 GET calls that you do similarly to what you would be charged if you used the FILE option of the training. However, these charges are usually marginal compared to the alternatives. <\/p>\n\n<p>When you are using the FILE mode, you need to pay for the local EBS on the instances, and for the extra time that your instances are up and only copying the data from S3. If you are running multiple epochs, you will not benefit much from the PIPE mode, however, when you have so much data (6.3 TB), you don't really need to run multiple epochs. <\/p>\n\n<p>The best usage of PIPE mode is when you can use a <strong>single pass<\/strong> over the data. In the era of big data, this is a better model of operation, as you can't retrain your models often. In SageMaker, you can point to your \"old\" model in the \"model\" channel, and your \"new\" data in the \"train\" channel and benefit from the PIPE mode to the maximum. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1565038250956,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":11.15,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":174.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to train Model in the AML Designer and on the Train Model component, I am receiving the following error when submitting it for a pipeline run\u2026    <\/p>\n<p>AmlExceptionMessage:AzureMLCompute job failed.    <br \/>\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.    <\/p>\n<p>ModuleExceptionMessage:ColumnUniqueValuesExceeded: Number of unique values in column: &quot;MessageID&quot; is greater than allowed.    <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1661308367600,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/979208\/receiving-error-while-submitting-the-pipeline-run",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.5,
        "Challenge_reading_time":6.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Receiving error while submitting the pipeline run",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":70,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=71c0cf97-895c-43f1-ac54-98e1e9833ae4\">@A-4824  <\/a> Thanks for the question. Here is the troubleshooting <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/component-reference\/designer-error-codes#error-0014\">document<\/a> for this issue.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.5,
        "Solution_reading_time":3.97,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_closed_time":1632465.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615292808000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/174",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.3,
        "Challenge_reading_time":3.66,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":23,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for using amazon-sagemaker-operator-for-k8s. Please help us with the steps to replicate the issue, especially the installation\r\n\r\nOfficial documentation for reference: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-operators-for-kubernetes.html I ran into this issue while myself and was resolved by making sure the SageMaker operator was applied and running by verifying with kubectl -n sagemaker-k8s-operator-system get pods Closing since there has been no activity in 90+ days",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.2,
        "Solution_reading_time":6.49,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi, I'm using Azure ML Designer to run a pipeline. The pipeline performs a few steps and then it cancels the work throwing an error message with no further details.  <\/p>\n<p>If I re-submit the pipeline it completes the previously failed step but fails on the next step. If I re-submit the same thing happens (completes previously failed step to then fail the next step)... until it gets stuck in a specific sql transform step (see log below)  <\/p>\n<p>Here is a sequence of  run ids related with the issue:  <br \/>\nd33d23a2-2e60-4198-a6b6-f47e6e27ef4e  <br \/>\n57e04c1e-73e8-4ddf-91a8-c407cd1ad5ef  <br \/>\nad7dc826-6549-4eb3-9536-9a801d8e8c0b  <br \/>\ne6623f6f-b7b9-4f19-9501-c8c28f53ab23  <\/p>\n<p>It may be due to the way my pipeline is built but seems like JOIN, SQL Transform and SELECT Column operations tend to fail the most.  <\/p>\n<p>Would much appreciate any help on this.  <\/p>\n<pre><code>2021\/05\/11 01:57:24 Starting App Insight Logger for task:  runTaskLet\n2021\/05\/11 01:57:24 Attempt 1 of http call to http:\/\/10.0.0.6:16384\/sendlogstoartifacts\/info\n2021\/05\/11 01:57:24 Attempt 1 of http call to http:\/\/10.0.0.6:16384\/sendlogstoartifacts\/status\n[2021-05-11T01:57:24.912444] Entering context manager injector.\n[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['urldecode_invoker.py', 'python', '-m', 'azureml.designer.modules.datatransform.invoker', 'ApplySqlTransModule', '--dataset', 'DatasetOutputConfig:Result_dataset', '--t1=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr', '--t2=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy', '--t3=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji', '--sqlquery=%22select+b.*%2cc.*%0d%0afrom+(%0d%0a++++select+a.customer_id%2c+a.sku_id%0d%0a++++from+(%0d%0a++++++++select+*+from+t1+cross+join+t2%0d%0a++++)+a%0d%0a++++where+exists+(%0d%0a++++++++select+t3.top_skus%0d%0a++++++++from+t3%0d%0a++++++++where+t3.sku_id+%3d+a.sku_id%0d%0a++++)%0d%0a)+b%0d%0ainner+join+(%0d%0a++++select+distinct+sku_id%2c+top_skus%0d%0a++++from+t3%0d%0a)+c%0d%0aon+c.sku_id+%3d+b.sku_id%22'])\nScript type = None\n[2021-05-11T01:57:26.142183] Entering Run History Context Manager.\n[2021-05-11T01:57:26.734197] Current directory: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/mounts\/workspaceblobstore\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\n[2021-05-11T01:57:26.734493] Preparing to call script [urldecode_invoker.py] with arguments:['python', '-m', 'azureml.designer.modules.datatransform.invoker', 'ApplySqlTransModule', '--dataset', '$Result_dataset', '--t1=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr', '--t2=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy', '--t3=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji', '--sqlquery=%22select+b.*%2cc.*%0d%0afrom+(%0d%0a++++select+a.customer_id%2c+a.sku_id%0d%0a++++from+(%0d%0a++++++++select+*+from+t1+cross+join+t2%0d%0a++++)+a%0d%0a++++where+exists+(%0d%0a++++++++select+t3.top_skus%0d%0a++++++++from+t3%0d%0a++++++++where+t3.sku_id+%3d+a.sku_id%0d%0a++++)%0d%0a)+b%0d%0ainner+join+(%0d%0a++++select+distinct+sku_id%2c+top_skus%0d%0a++++from+t3%0d%0a)+c%0d%0aon+c.sku_id+%3d+b.sku_id%22']\n[2021-05-11T01:57:26.734551] After variable expansion, calling script [urldecode_invoker.py] with arguments:['python', '-m', 'azureml.designer.modules.datatransform.invoker', 'ApplySqlTransModule', '--dataset', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpnbybe4mu', '--t1=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr', '--t2=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy', '--t3=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji', '--sqlquery=%22select+b.*%2cc.*%0d%0afrom+(%0d%0a++++select+a.customer_id%2c+a.sku_id%0d%0a++++from+(%0d%0a++++++++select+*+from+t1+cross+join+t2%0d%0a++++)+a%0d%0a++++where+exists+(%0d%0a++++++++select+t3.top_skus%0d%0a++++++++from+t3%0d%0a++++++++where+t3.sku_id+%3d+a.sku_id%0d%0a++++)%0d%0a)+b%0d%0ainner+join+(%0d%0a++++select+distinct+sku_id%2c+top_skus%0d%0a++++from+t3%0d%0a)+c%0d%0aon+c.sku_id+%3d+b.sku_id%22']\n\nSession_id = 4b5b4c29-cfda-4ab6-a715-47fee287c468\nInvoking module by urldecode_invoker 0.0.8.\n\nModule type: custom module.\n\nUsing runpy to invoke module 'azureml.designer.modules.datatransform.invoker'.\n\n\/azureml-envs\/azureml_7c975cabc8bb1dc19c3de94457d707fd\/lib\/python3.6\/site-packages\/azureml\/designer\/modules\/datatransform\/tools\/dataframe_utils.py:2: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n  from pandas.util.testing import assert_frame_equal\n2021-05-11 01:57:27,324 [             invoker] [    INFO] .[main] Start custom modules\n2021-05-11 01:57:27,337 [             invoker] [    INFO] .[main] Module version: 0.0.74\n2021-05-11 01:57:27,344 [             invoker] [    INFO] .[main] args: azureml.designer.modules.datatransform.invoker, ApplySqlTransModule, --dataset, \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpnbybe4mu, --t1=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr, --t2=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy, --t3=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji, --sqlquery=select b.*,c.*\nfrom (\n    select a.customer_id, a.sku_id\n    from (\n        select * from t1 cross join t2\n    ) a\n    where exists (\n        select t3.top_skus\n        from t3\n        where t3.sku_id = a.sku_id\n    )\n) b\ninner join (\n    select distinct sku_id, top_skus\n    from t3\n) c\non c.sku_id = b.sku_id\n2021-05-11 01:57:27,352 [             invoker] [    INFO] .[main] &quot;transform_module_class_name&quot;: ApplySqlTransModule\n2021-05-11 01:57:27,444 [         module_base] [    INFO] ...[get_arg_parser] Construct arg parser\n2021-05-11 01:57:27,460 [         module_base] [    INFO] ...[get_arg_parser] arg: t1\n2021-05-11 01:57:27,468 [         module_base] [    INFO] ...[get_arg_parser] arg: t2\n2021-05-11 01:57:27,476 [         module_base] [    INFO] ...[get_arg_parser] arg: t3\n2021-05-11 01:57:27,484 [         module_base] [    INFO] ...[get_arg_parser] arg: dataset\n2021-05-11 01:57:27,492 [         module_base] [    INFO] ...[get_arg_parser] arg: sqlquery\n2021-05-11 01:57:27,500 [         module_base] [    INFO] ..[parse_and_insert_args] invoker args:\n module_classname = ApplySqlTransModule\n t1 = \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr\n t2 = \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy\n t3 = \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji\n dataset = \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpnbybe4mu\n sqlquery = select b.*,c.*\nfrom (\n    select a.customer_id, a.sku_id\n    from (\n        select * from t1 cross join t2\n    ) a\n    where exists (\n        select t3.top_skus\n        from t3\n        where t3.sku_id = a.sku_id\n    )\n) b\ninner join (\n    select distinct sku_id, top_skus\n    from t3\n) c\non c.sku_id = b.sku_id\n\n2021-05-11 01:57:27,508 [             invoker] [    INFO] .[main] start to run custom module: ApplySqlTransModule\n2021-05-11 01:57:27,516 [apply_sql_trans_module] [    INFO] ...[run] Construct SQLite Server\n2021-05-11 01:57:27,530 [    module_parameter] [    INFO] ......[data] Read data from \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr\n2021-05-11 01:57:29,215 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t1 with only column names\n2021-05-11 01:57:29,227 [    module_parameter] [    INFO] ......[data] Read data from \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy\n2021\/05\/11 01:57:29 Not exporting to RunHistory as the exporter is either stopped or there is no data.\nStopped: false\nOriginalData: 1\nFilteredData: 0.\n2021-05-11 01:57:30,093 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t2 with only column names\n2021-05-11 01:57:30,106 [    module_parameter] [    INFO] ......[data] Read data from \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji\n2021-05-11 01:57:30,876 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t3 with only column names\n2021-05-11 01:57:30,888 [apply_sql_trans_module] [    INFO] ...[run] Read SQL script query\n2021-05-11 01:57:30,895 [apply_sql_trans_module] [    INFO] ...[run] Validate SQL script query\n2021-05-11 01:57:30,912 [apply_sql_trans_module] [    INFO] ...[run] Insert data to SQLite Server\n2021-05-11 01:57:30,919 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t1\n2021-05-11 01:57:30,930 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t2\n2021-05-11 01:57:30,970 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t3\n2021-05-11 01:57:31,053 [apply_sql_trans_module] [    INFO] ...[run] Generate SQL query result from SQLite Server\n<\/code><\/pre>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1620697549357,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/390003\/azure-ml-pipeline-fails-at-sql-transform-task",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":17.3,
        "Challenge_reading_time":130.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":69,
        "Challenge_solved_time":null,
        "Challenge_title":"azure ml pipeline fails at sql transform task",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":609,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Found the problem.   <\/p>\n<p>There was a task failing but due to the size of the canvas I wasn't able to spot it at first (working late hours didn't help also).   <\/p>\n<p>However it certainly didn't help the fact that the error message didn't provide any info regarding which task failed, so maybe the AML team would like to add more descriptive messages in cases like this one.  <\/p>\n<p>thanks<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":4.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":70.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645796339972,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have thousands of training jobs that I want to run on sagemaker. Basically I have a list of hyperparameters and I want to train the model for <em>all<\/em> of those hyperparmeters in parallel (not a standard hyperparameter tuning where we just want to optimize the hyperparameter, here we want to train for all of the hyperparameters). I have searched the docs quite extensively but it surprises me that I couldn't find any info about this, even though it seems like a pretty basic functionality.<\/p>\n<p>For example, let's say I have 10,000 training jobs, and my quota is 20 instances, what is the best way to run these jobs utilizing all my available instances? In particular,<\/p>\n<ul>\n<li>Is there a &quot;queue manager&quot; functionality that takes the list of hyperparameters and runs the training jobs in batches of 20 until they are all done (even better if it could keep track of failed\/completed jobs).<\/li>\n<li>Is it best practice to run a single training job per instance? If that's the case do I need to ask for a much higher quota on the number of instance?<\/li>\n<li>If this functionality does not exist in sagemaker, is it worth using EC2 instead since it's a bit cheaper?<\/li>\n<\/ul>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1650506611153,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1650508898300,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71948090",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.4,
        "Challenge_reading_time":15.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Best way to run 1000s of training jobs on sagemaker",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":269.0,
        "Challenge_word_count":213,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1622388181327,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":103.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Your question is very broad and the best way forward would depend on other details of your use-case, so we will have to make some assumptions.<\/p>\n<p>[Queue manager]\nSageMaker does <em>not<\/em> have a queue manager. If at the end you decide you need a queue manager, I would suggest looking towards AWS Batch.<\/p>\n<p>[Single vs multiple training jobs]\nSince you need to run 10s of thousands job I assume you are training fairly lightweight models, so to save on time, you would be better off reusing instances for multiple training jobs. (Otherwise, with 20 instances limit, you need 500 rounds of training, with a 3 min start time - depending on instance type - you need 25 hours just for the wait time. Depending on the complexity of each individual model, this 25hours might be significant or totally acceptable).<\/p>\n<p>[Instance limit increase]\nYou can always ask for a limit increase, but going from a limit of 20 to 10k at once is likely that will not be accepted by the AWS support team, unless you are part of an organisation with a track record of usage on AWS, in which case this might be fine.<\/p>\n<p>[One possible option] (Assuming multiple lightweight models)\nYou could create a single training job, with instance count, the number of instances available to you.\nInside the training job, your code can run a for loop and perform all the individual training jobs you need.<\/p>\n<p>In this case, you will need to know which which instance is which so you can make the split of the HPOs. SageMaker writes this information on the file: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-dist-training\" rel=\"nofollow noreferrer\">\/opt\/ml\/input\/config\/resourceconfig.json<\/a> so using that you can easily have each instance run a subset of the trainings required.<\/p>\n<p>Another thing to think of, is if you need to save the generated models (which you probably need). You can either save everything in the output model directory - standard SM approach- but this would zip all models in a model.tar.gz file.\nIf you don't want this, and prefer to have each model individually saved, I'd suggest using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">checkpoints<\/a> directory that will sync anything written there to your s3 location.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.5,
        "Solution_reading_time":30.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":359.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi, the UI is very confused, not straightforward as Studio. Can you guide me to the next step to use the pipeline?     <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661266129010,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/978610\/whats-the-next-step-after-creating-a-pipeline",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.7,
        "Challenge_reading_time":2.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"What's the next step after creating a pipeline?",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":30,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=a1c46131-fcca-467d-9bfb-d1c7cdf021ad\">@Nichole\u2019s  <\/a>     <\/p>\n<p>Thanks for using Microsft Q&amp;A platform. I think you are on the stage of designing your pipeline and running it.     <\/p>\n<p>The next step should be submit your pipeline and evaluate your model - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score#submit-the-pipeline\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score#submit-the-pipeline<\/a>    <\/p>\n<p>When you feel good with your model, you can then deploy your pipeline as this guidance - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy<\/a>    <\/p>\n<p>You may then want to test and update your endpoint as above guidance.     <\/p>\n<p>Each time you run a pipeline, the configuration of the pipeline and its results are stored in your workspace as a pipeline job. You can go back to any pipeline job to inspect it for troubleshooting or auditing. Clone a pipeline job to create a new pipeline draft for you to edit.    <\/p>\n<p>Pipeline jobs are grouped into experiments to organize job history. You can set the experiment for every pipeline job.    <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.0,
        "Solution_reading_time":19.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":172.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi, While trying authentication in AzureML SDK v2 the DefaultAzureCredential failed to retrieve a token from the included credentials. Attempted credentials: \tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured. <\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":1,
        "Challenge_created_time":1671076439927,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1129620\/defaultazurecredential-failed-to-retrieve-a-token",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":16.7,
        "Challenge_reading_time":4.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"DefaultAzureCredential failed to retrieve a token from the included credentials.",
        "Challenge_topic":"Job Management",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":41,
        "Issue_self_closed":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=71c0cf97-895c-43f1-ac54-98e1e9833ae4\">@A-4824  <\/a>  Thanks for the question. Hers is the <a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-python\/blob\/main\/sdk\/identity\/azure-identity\/TROUBLESHOOTING.md#troubleshoot-environmentcredential-authentication-issues\">Troubleshooting guide<\/a> for Default Azure credential errors.    <\/p>\n<p> from azure.core.exceptions import ClientAuthenticationError    <br \/>\n    from azure.identity import DefaultAzureCredential  <br \/>\n    from azure.keyvault.secrets import SecretClient  <\/p>\n<pre><code># Create a secret client using the DefaultAzureCredential  \nclient = SecretClient(&quot;https:\/\/myvault.vault.azure.net\/&quot;, DefaultAzureCredential())  \ntry:  \n    secret = client.get_secret(&quot;secret1&quot;)  \nexcept ClientAuthenticationError as ex:  \n    print(ex.message)  \n<\/code><\/pre>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.8,
        "Solution_reading_time":11.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":55.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1327588060552,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":802.0,
        "Answerer_view_count":91.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In my pipeline multiple steps are independent and so I would like them to run in parallel based on input dependencies.<\/p>\n<p>As the compute I use has multiple nodes I would have expected this to be the default.<\/p>\n<p>For example:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Iye85.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iye85.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>All 3 upper steps should run in parallel, then both <code>finetune<\/code> steps in parallel as soon as their inputs are satisfied and the same for <code>rgb_test<\/code>.<\/p>\n<p>Currently only 1 step runs at a time, the other are <code>Queued<\/code>.<\/p>",
        "Challenge_closed_time":1.0,
        "Challenge_comment_count":2,
        "Challenge_created_time":1630612166280,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69036277",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Run independent `PythonScriptStep` steps in parallel",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":156.0,
        "Challenge_word_count":93,
        "Issue_self_closed":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1327588060552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":802.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p>It ended up being because of vCPU quota.<\/p>\n<p>After increasing the quota, parallel tasks can run at the same time as expected.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":1.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":8,
        "Challenge_body":"Hi,\r\n\r\nI created a custom docker container to deploy my model on Vertex AI. The model uses LightGBM, so I can't use the pre-built container images available for TF\/SKL\/XGBoost. I was able to deploy the model and get predictions, but I get errors while trying to get **explainable** predictions from the model. I have tried to follow the Vertex AI guidelines to configure the model for explanations.\r\nThe example below shows a simplified version of the model that still reproduces the issue, with only two input features 'A' and 'B'.\r\n\r\nPlease take a look and tell me if the explanation metadata is supposed to be set differently, or if there is something wrong with this approach.\r\n\r\n\r\n#### Environment details\r\n\r\n  - Google Cloud Notebook\r\n  - Python version: 3.7.12\r\n  - pip version: 21.3.1\r\n  - `google-cloud-aiplatform` version: 1.15.0\r\n\r\n#### Reference\r\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/explainable-ai\/configuring-explanations#custom-container\r\n\r\n#### explanation-metadata.json\r\n(_Model output is unkeyed. The Vertex AI guide suggests using any memorable string for output key._)\r\n```\r\n{\r\n    \"inputs\": {\r\n        \"A\": {},\r\n        \"B\": {}\r\n    },\r\n    \"outputs\": {\r\n        \"Y\": {}\r\n    }\r\n}\r\n```\r\n#### Model upload with explanation parameters and metadata\r\n```\r\n! gcloud ai models upload \\\r\n  --region=$REGION \\\r\n  --display-name=$MODEL_NAME \\\r\n  --container-image-uri=$PRED_IMAGE_URI \\\r\n  --artifact-uri=$ARTIFACT_LOCATION_GCS \\\r\n  --explanation-method=sampled-shapley \\\r\n  --explanation-path-count=10 \\\r\n  --explanation-metadata-file=explanation-metadata.json\r\n```\r\n\r\n#### Prediction\/Explanation Input\r\n```\r\ninstances = [{\"A\": 1.1, \"B\": 20}, {\"A\": 2.2, \"B\": 21}]\r\n# Prediction (works fine):\r\nendpoint.predict(instances=instances)\r\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\r\nendpoint.explain(instances=instances) # Returns error (1) shown in stack trace below\r\n\r\n# Another example\r\ninstances_2 = [[1.1,20], [2.2,21]]\r\n# Prediction (works fine):\r\nendpoint.predict(instances=instances_2)\r\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\r\nendpoint.explain(instances=instances_2) # Returns error\r\n# Error: Nameless inputs are allowed only if there is a single input in the explanation metadata.\r\n```\r\n#### Prediction Server (Flask)\r\n```python\r\n# Custom Flask server to serve online predictions\r\n# Input for prediction\r\nraw_input = request.get_json()\r\ninput = raw_input['instances']\r\ndf = pd.DataFrame(input, columns = ['A', 'B'])\r\n# Prediction from model (loaded from GCP bucket)\r\npredictions = model.predict(df).tolist() # [0, 1]\r\nresponse = jsonify({\"predictions\": predictions})\r\nreturn response\r\n```\r\n\r\n#### Stack trace of error (1)\r\n```\r\n---------------------------------------------------------------------------\r\n_InactiveRpcError                         Traceback (most recent call last)\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\r\n     49         try:\r\n---> 50             return callable_(*args, **kwargs)\r\n     51         except grpc.RpcError as exc:\r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\r\n    945                                       wait_for_ready, compression)\r\n--> 946         return _end_unary_response_blocking(state, call, False, None)\r\n    947 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\r\n    848     else:\r\n--> 849         raise _InactiveRpcError(state)\r\n    850 \r\n\r\n_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\r\n\tstatus = StatusCode.INVALID_ARGUMENT\r\n\tdetails = \"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\"\r\n\tdebug_error_string = \"{\"created\":\"@1658310559.755090975\",\"description\":\"Error received from peer ipv4:74.125.133.95:443\",\"file\":\"src\/core\/lib\/surface\/call.cc\",\"file_line\":1069,\"grpc_message\":\"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\",\"grpc_status\":3}\"\r\n>\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nInvalidArgument                           Traceback (most recent call last)\r\n\/tmp\/ipykernel_2590\/4024017963.py in <module>\r\n----> 3 print(endpoint.explain(instances=instances, parameters={}))\r\n\r\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in explain(self, instances, parameters, deployed_model_id, timeout)\r\n   1563             parameters=parameters,\r\n   1564             deployed_model_id=deployed_model_id,\r\n-> 1565             timeout=timeout,\r\n   1566         )\r\n   1567 \r\n\r\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1\/services\/prediction_service\/client.py in explain(self, request, endpoint, instances, parameters, deployed_model_id, retry, timeout, metadata)\r\n    917             retry=retry,\r\n    918             timeout=timeout,\r\n--> 919             metadata=metadata,\r\n    920         )\r\n    921 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method.py in __call__(self, timeout, retry, *args, **kwargs)\r\n    152             kwargs[\"metadata\"] = metadata\r\n    153 \r\n--> 154         return wrapped_func(*args, **kwargs)\r\n    155 \r\n    156 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\r\n     50             return callable_(*args, **kwargs)\r\n     51         except grpc.RpcError as exc:\r\n---> 52             raise exceptions.from_grpc_error(exc) from exc\r\n     53 \r\n     54     return error_remapped_callable\r\n\r\nInvalidArgument: 400 {\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\r\n---------------------------------------------------------------------------\r\n```",
        "Challenge_closed_time":1659550.0,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658320562000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/googleapis\/python-aiplatform\/issues\/1526",
        "Challenge_link_count":1,
        "Challenge_participation_count":8,
        "Challenge_readability":14.2,
        "Challenge_reading_time":79.22,
        "Challenge_repo_contributor_count":75.0,
        "Challenge_repo_fork_count":188.0,
        "Challenge_repo_issue_count":1846.0,
        "Challenge_repo_star_count":283.0,
        "Challenge_repo_watch_count":53.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":null,
        "Challenge_title":"Error while trying to get explanation from (custom container) model deployed on Vertex AI (Prediction without explanation works fine)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":null,
        "Challenge_word_count":578,
        "Issue_self_closed":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @jaycee-li,\r\nAny update on this? Would really appreciate your inputs! Hi @pankajrsingla, sorry for the late reply!\r\n\r\nSince instance_2 prediction works for your model, looks like your model takes unkeyed input. Could you please try this metadata setting:\r\n```\r\n{\r\n    \"inputs\": {\r\n        \"X\": {},\r\n    },\r\n    \"outputs\": {\r\n        \"Y\": {}\r\n    }\r\n}\r\n```\r\nThen update the model, endpoint, and try:\r\n```\r\ninstances = [[1.1,20], [2.2,21]]\r\nendpoint.explain(instances=instances)\r\n```\r\n\r\nPlease let me know if this works for you. Hi @jaycee-li,\r\nThank you so much for your response.\r\nI tried your suggestion, but I got the same error as before.\r\n`Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\"`\r\n\r\nIf you see the code for my prediction server, it can take both unkeyed as well as keyed input (prediction works fine for both cases), since it converts the input to a dataframe. The output is definitely unkeyed. However, I am still confused as to what should be the contents of the explanation-metadata.json file.\r\n\r\nAlso, just to be sure - the same API (predict) in the flask server is supposed to work for both predictions and explanations, right? Or do I need to create a separate API for 'explain'?\r\n\r\nIf you have any other suggestions, I would be more than happy to try them out. \r\n(If that would help, I can also send you the full contents of the Jupyter notebook - all code one place - if you share your email id.)\r\n\r\nPlease let me know!\r\n\r\nThank you! It would be helpful if you can share the notebook to jayceeli@google.com\r\n\r\nThank you very much! Done!\r\nThanks! :) Hi @pankajrsingla ,\r\n\r\nI got `AttributeError: 'Blob' object has no attribute 'open'` for `with blob.open(\"wb\") as f:` in your TRAIN_IMAGE_URI. So I was stuck here and didn't reproduce the error you got. \r\n\r\nYou mixed CLI, gapic API, and SDK in your code. Since I'm not familiar with CLI tool, I'm not very sure what the problem is. Maybe it's due to your PRED_IMAGE_URI? I would suggest you to try a pre-built container(`us-docker.pkg.dev\/vertex-ai\/prediction\/sklearn-cpu.1-0:latest`) when uploading the model.\r\n\r\nI drafted a notebook that used SDK only to train, upload, deploy a same model as yours. And it can successfully make predictions and explanations. I've shared the notebook with you for your reference.\r\n\r\nPlease let me know if you still get the error. Thanks! Hi @pankajrsingla ,\r\n\r\nPlease check this [notebook](https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/ml_ops\/stage6\/get_started_with_xai_and_custom_server.ipynb) (Specifically **Create the model server** and **Build a FastAPI HTTP server** sections) for how to use XAI with a custom container. Thanks a lot, @jaycee-li! This is exactly what I was looking for!\r\nI will give this a try for my model, and will update you once I have the results. This should work.\r\n\r\nThank you!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":5.5,
        "Solution_reading_time":35.96,
        "Solution_score_count":null,
        "Solution_sentence_count":38.0,
        "Solution_word_count":449.0,
        "Tool":"Vertex AI"
    }
]